1994.amta-1.11,P91-1022,0,0.565217,"Missing"
1994.amta-1.11,P93-1002,0,0.310201,"Missing"
1994.amta-1.11,P93-1001,0,0.614472,"Missing"
1994.amta-1.11,C94-2178,1,0.869399,"Missing"
1994.amta-1.11,J93-1004,0,0.449974,"Missing"
1994.amta-1.11,1992.tmi-1.7,0,0.436765,"Missing"
1994.amta-1.11,P94-1012,0,0.776427,"Missing"
1994.amta-1.11,1994.amta-1.26,0,0.835444,"Missing"
1995.tmi-1.18,J93-2003,0,0.00798376,"Missing"
1995.tmi-1.18,1994.amta-1.3,0,0.0223545,"Missing"
1995.tmi-1.18,A94-1006,0,0.0292693,"Missing"
1995.tmi-1.18,W93-0301,0,0.0187203,"Missing"
1995.tmi-1.18,P95-1032,1,0.839504,"Missing"
1995.tmi-1.18,1994.amta-1.11,1,0.834197,"Missing"
1995.tmi-1.18,P94-1051,0,0.0217946,"Missing"
1995.tmi-1.18,P94-1012,1,0.859572,"Missing"
1995.tmi-1.18,1995.tmi-1.28,1,0.842079,"Missing"
1995.tmi-1.18,A94-1030,1,0.841127,"Missing"
1995.tmi-1.18,1994.amta-1.26,1,0.843615,"Missing"
2007.tmi-papers.10,I05-2021,1,0.838417,"results in section 5. In section 6, we then demonstrate experimentally how ARG ALIGN outperforms a more conventional method based on semantic role projection, SYN ALIGN. 2 Problem Definition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The pr"
2007.tmi-papers.10,P05-1048,1,0.821015,"results in section 5. In section 6, we then demonstrate experimentally how ARG ALIGN outperforms a more conventional method based on semantic role projection, SYN ALIGN. 2 Problem Definition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The pr"
2007.tmi-papers.10,D07-1007,1,0.811803,"nition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The predicate verb pair “organized /举办” have the operators ARG0 “African Environmental Centre/非洲环境中心”, and the operands ARG1 “Seminar on desertification/沙漠化问题研讨 会”. In the above example, the s"
2007.tmi-papers.10,2006.iwslt-evaluation.5,1,0.813528,"ALIGN. 2 Problem Definition In recent years, researchers have shown that statistical machine translation models can be enhanced by incorporating structural information (Wu and Chiang, 2007). The atten76 tion, though, has thus far been largely focused on chunk or syntactic structures. Researchers only recently began seriously investigating whether incorporating semantic models can enhance statistical machine translation performance (Carpuat and Wu, 2005a; Carpuat and Wu, 2005b), and are only just beginning to show that semantic word sense disambiguation techniques can indeed improve accuracy (Carpuat et al., 2006; Carpuat and Wu, 2007). However, it remains an intriguing open question as to how semantic structures—semantic role mappings in bilingual semantic frames—can also be potentially leveraged to improve machine translation. Thus, in order to overcome the immediate obstacle to exploring this potential, we are interested in learning the bilingual semantic structure given a predicate verb pair in English and Chinese, as in Figure 1. The predicate verb pair “organized /举办” have the operators ARG0 “African Environmental Centre/非洲环境中心”, and the operands ARG1 “Seminar on desertification/沙漠化问题研讨 会”. In t"
2007.tmi-papers.10,P06-1146,0,0.356547,"hich , [ARG0 more than 20 thousand enterprises ] have [T ARGET received] [ARG1 loan support] [ARG2 from the Bank of China] . Chinese [ARGM −T M P 目前] ， 约 有 十五万 家 外商 投资 企业 在 中国 银行 开立 帐 户 ， 其中 [ARG0 二万多 家] [T ARGET 获 得] [ARG1 中国 银行 的 贷款 支持] 。 Gloss currently, about 150 thousand foreign merchant investment enterprise in China Bank open account, of which, 20 thousand more enterprise receive China Bank’s loan support . 6 Role Mapping from Syntactic Constituent Alignment To date, it is often casually assumed that semantic roles can be simply projected across language pairs by constituent alignment (Pado and Lapata, 2006). In such an approach, it is assumed that an English constituent is lexically translated into the Chinese constituent, in which case they must share the same role label. This sort of view is typically inspired by the many structurallybased statistical machine translation models that make use of some kind of syntactic constituent projection (Hwa et al., 2005). Therefore it is worth investigating the possibility of projecting semantic role labels across matching syntactic constituents. To accomplish this, we implement a contrastive SYN ALIGN algorithm that obtains semantic structure mapping base"
2007.tmi-papers.10,W05-0309,0,0.0696953,"Missing"
2007.tmi-papers.10,N04-1030,0,0.0610291,"ng to the unavoidable errors through POS tagging, chunking or syntactic parsing, among the bilingual sentences, some Chinese and English sentences have no identifiable predicate verb, and are eliminated from further processing. Finally, 397 sentence pairs with automatic semantic parsing results are used in our predicate-argument mapping experiment. In our proposed method, Chinese/English shallow semantic parsing is a prerequisite to achieving the task of bilingual semantic frame mapping. In recent years, there has been a lot of research on shallow semantic labeling or parsing both in English (Pradhan et al., 2004; Pradhan et al., 2005) and Chinese (Sun and Jurafsky, 2004; Xue and Palmer, 2005). In our experiments, we use the ASSERT semantic parser (Pradhan, 2005) to carry out the automatic semantic parsing on the English side and a similar SVM-based Chinese semantic parsing system (Wu et al., 2006) on the Chinese side. According to (Pradhan et al., 2005), their English semantic parser achieved 89.40 F-score with gold syntactic parse input, and 79.40 F-score with automatic syntactic parse input. Meanwhile, our SVM-based Chinese semantic parser yielded 89.89 F-score with gold syntactic parse input and 6"
2007.tmi-papers.10,N04-1032,0,0.0590976,"g or syntactic parsing, among the bilingual sentences, some Chinese and English sentences have no identifiable predicate verb, and are eliminated from further processing. Finally, 397 sentence pairs with automatic semantic parsing results are used in our predicate-argument mapping experiment. In our proposed method, Chinese/English shallow semantic parsing is a prerequisite to achieving the task of bilingual semantic frame mapping. In recent years, there has been a lot of research on shallow semantic labeling or parsing both in English (Pradhan et al., 2004; Pradhan et al., 2005) and Chinese (Sun and Jurafsky, 2004; Xue and Palmer, 2005). In our experiments, we use the ASSERT semantic parser (Pradhan, 2005) to carry out the automatic semantic parsing on the English side and a similar SVM-based Chinese semantic parsing system (Wu et al., 2006) on the Chinese side. According to (Pradhan et al., 2005), their English semantic parser achieved 89.40 F-score with gold syntactic parse input, and 79.40 F-score with automatic syntactic parse input. Meanwhile, our SVM-based Chinese semantic parser yielded 89.89 F-score with gold syntactic parse input and 69.12 Fscore with automatic syntactic parse input. Both of t"
2009.eamt-1.30,W05-0909,0,0.0388687,"Missing"
2009.eamt-1.30,P05-1048,1,0.119242,"en increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling. Avoiding the many potential blind alleys calls for careful analysis and evaluation of (1) the frequencies of types of SMT errors where semantic parsing and role labeling could help, (2) if and when semantic roles offer more accurate guidance to SMT than merely syntactic anno"
2009.eamt-1.30,D07-1007,1,0.262102,"l difficulty of making syntactic and semantic models contribute to improving SMT accuracy. The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling. Avoiding the many potential blind alleys calls for careful analysis and evaluation of (1) the frequencies of types of SMT errors where semantic parsing an"
2009.eamt-1.30,P07-1005,0,0.0516711,"syntactic and semantic models contribute to improving SMT accuracy. The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling. Avoiding the many potential blind alleys calls for careful analysis and evaluation of (1) the frequencies of types of SMT errors where semantic parsing and role labeling coul"
2009.eamt-1.30,J02-3001,0,0.00785905,"ntial quantitative impact of realistic semantic role guidance to SMT systems, at least in terms of scores such as BLEU and METEOR. In this paper, we present a series of four experiments designed to address each of these questions, using Chinese-English parallel resources, a typical representative SMT system based on Moses, and shallow semantic parsers for both English and Chinese. (when). For a sentence with multiple verbs, there can be multiple predicate argument structures. Shallow semantic parsing systems are mostly based on classifiers that learn from a manually annotated semantic corpus (Gildea and Jurafsky (2002), Pradhan et al. (2005)). Following the publication of the Proposional Bank (PropBank) (Palmer et al., 2005) first in English, then in Chinese, it has been possible to train these classifiers to perform semantic analysis on news wire type of texts. 2 2.2 Chinese shallow semantic parsing Related work While this is a new avenue of inquiry, the background relevant to the experiments described here includes (1) a broad body of work on shallow semantic parsing and semantic role labeling, the majority of which has been performed on English, (2) a relatively small body of work specific to semantic pa"
2009.eamt-1.30,W07-0738,0,0.0740987,"Missing"
2009.eamt-1.30,W08-0332,0,0.101108,"Missing"
2009.eamt-1.30,P07-2045,0,0.00344519,"Missing"
2009.eamt-1.30,N04-1021,0,0.0287774,"mained consistent cross-lingually across sentence translations. We approach this promise with caution, however, given the painful lessons learned through the historical difficulty of making syntactic and semantic models contribute to improving SMT accuracy. The past decade has at last seen increasing amounts of evidence that SMT accuracy can indeed be improved via tree-structured and syntactic models (e.g., Wu (1997); Chiang and Wu (2008); Wu and Chiang (2009)) despite numerous disappointProceedings of the 13th Annual Conference of the EAMT, pages 218–225, Barcelona, May 2009 218 ing attempts Och et al. (2004). More recently, lexical semantics models for word sense disambiguation have also finally been successfully applied to increasing SMT accuracy (e.g., Carpuat and Wu (2007), Chan et al. (2007); Gim´enez and M`arquez (2007a)) again after surprising initial failures (e.g., Carpuat and Wu (2005)). In both the syntactic and semantic cases, improving SMT accuracy ultimately required making major adaptations to the original linguistic models. We can reasonably expect it to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing an"
2009.eamt-1.30,P02-1040,0,0.103507,"Missing"
2009.eamt-1.30,2007.iwslt-1.12,1,0.846107,".1 Experimental setup To assess the above sorts of phenomena quantitatively, we designed an experiment making use of 745 bi-sentences extracted from the Parallel PropBank with gold standard annotations of both syntactic and semantic roles. We use the Chinese sentences as system input and their corresponding English translations as the reference translations. We use the open source statistical machine translation decoder Moses (?) for the experiments, translating the PropBank Chinese sentences into English with the same model trained for our participation in the IWSLT 2007 evaluation campaign (Shen et al., 2007).The English translations generated by the decoder are the system output. Based on the system input and the reference Table 1: Accuracy of predicate-argument structure in Chinese-English SMT output for data set A. P-A Precision Recall F-measure Structure Predicate 0.98 0.57 0.72 ARG0 0.74 0.38 0.50 ARG1 0.73 0.41 0.53 ARG2 0.82 0.32 0.46 ARG3 1.00 0.67 0.80 ARG4 1.00 0.33 0.51 All ARGs 0.74 0.39 0.51 translation, we intend to investigate whether the predicate verbs are correctly translated and their predicate-argument structures preserved in the system output. We first randomly select 50 bi-se"
2009.eamt-1.30,N04-1032,0,0.0146506,"ho did what to whom, for whom or what, how, where, when, and why.” Shallow semantic parsing extracts the predicateargument structure of verbs in a sentence based on the syntactic tree of that sentence. For example, the predicate argument structure of the verb hold in Figure 1 specifies a “holding” relation between both sides (who) and meeting (what) on Sunday Figure 1: Chinese shallow semantic parsing example. Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank (Sun and Jurafsky (2004), Xue (2006), Fung et al. (2006)). It is interesting to note that, despite the very different characteristics of Chinese verbs (Xue and Palmer, 2005) from those in English, the core algorithm of a shallow semantic parser remains the same. As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task (Fung et al., 2006). The primary difference lies in the feature set chosen to represent semantic information. In experiments carried out on PropBank data using gold standard syntactic parse trees, extended syntactic features such as"
2009.eamt-1.30,J97-3002,1,0.271172,"Missing"
2009.eamt-1.30,N06-1055,0,0.0117212,"whom or what, how, where, when, and why.” Shallow semantic parsing extracts the predicateargument structure of verbs in a sentence based on the syntactic tree of that sentence. For example, the predicate argument structure of the verb hold in Figure 1 specifies a “holding” relation between both sides (who) and meeting (what) on Sunday Figure 1: Chinese shallow semantic parsing example. Systems that perform shallow semantic parsing on Chinese texts are likewise based on classifiers and trained on the Chinese PropBank and the bilingual Chinese-English Parallel PropBank (Sun and Jurafsky (2004), Xue (2006), Fung et al. (2006)). It is interesting to note that, despite the very different characteristics of Chinese verbs (Xue and Palmer, 2005) from those in English, the core algorithm of a shallow semantic parser remains the same. As was found to be the case in English, SVM classifiers have been found to outperform maximum entropy classifiers for this task (Fung et al., 2006). The primary difference lies in the feature set chosen to represent semantic information. In experiments carried out on PropBank data using gold standard syntactic parse trees, extended syntactic features such as Path Trigram"
2009.eamt-1.30,J05-1004,0,\N,Missing
2020.aacl-main.30,D18-1398,0,0.0264085,", and one modality fusion module. For each modality, the input is a sequence of length T . Each modality has a set of emotion embeddings by mapping the GloVe textual emotion embeddings to the other modalities using ft→v and ft→a . The whole architecture is optimized end-to-end. 2.2 Zero/Few-Shot and Continual Learning Zero-shot and few-shot learning methods, which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; K"
2020.aacl-main.30,N18-1193,0,0.0261093,"usion is a widely studied approach. For example, Zadeh et al. (2017) proposed a tensor fusion network that combines three modalities from vectors to a tensor using the Cartesian product. In addition, the attention 269 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 269–280 c December 4 - 7, 2020. 2020 Association for Computational Linguistics mechanism is commonly used to do modality fusion (Zadeh et al., 2018a; Wang et al., 2018; Liang et al., 2018; Hazarika et al., 2018; Pham et al., 2018; Tsai et al., 2019a). Although significant improvements have been made on the multi-modal emotion recognition task, however, the relationship between emotions has not been well modelled, which can lead to sub-optimal performance. Also, the problem of low-resource multi-modal emotion recognition is not adequately studied. Multi-modal emotion recognition data is hard to collect and annotate, especially for low-resource emotions (e.g., surprise) that are rarely seen in daily life, which motivates us to investigate this problem. In this paper, we propose a modality-transferable"
2020.aacl-main.30,W19-1425,0,0.0125299,"e learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al., 2017; Lee et al., 2017), and these were applied to some NLP tasks, such as opinion mining (Shu et al., 2016), document classification (Shu et al., 2017), and dialogue state tracking (Wu et al., 2019). 3 Methodology As shown in Figure 2, our model consists of three parts: intra-modal encoder networks, emotion embedding mapping modules, and an inter-modal fusion module. In"
2020.aacl-main.30,D18-1014,0,0.257303,"mics, and modality fusion is a widely studied approach. For example, Zadeh et al. (2017) proposed a tensor fusion network that combines three modalities from vectors to a tensor using the Cartesian product. In addition, the attention 269 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 269–280 c December 4 - 7, 2020. 2020 Association for Computational Linguistics mechanism is commonly used to do modality fusion (Zadeh et al., 2018a; Wang et al., 2018; Liang et al., 2018; Hazarika et al., 2018; Pham et al., 2018; Tsai et al., 2019a). Although significant improvements have been made on the multi-modal emotion recognition task, however, the relationship between emotions has not been well modelled, which can lead to sub-optimal performance. Also, the problem of low-resource multi-modal emotion recognition is not adequately studied. Multi-modal emotion recognition data is hard to collect and annotate, especially for low-resource emotions (e.g., surprise) that are rarely seen in daily life, which motivates us to investigate this problem. In this paper, we propose"
2020.aacl-main.30,P19-1227,0,0.016783,"et of emotion embeddings by mapping the GloVe textual emotion embeddings to the other modalities using ft→v and ft→a . The whole architecture is optimized end-to-end. 2.2 Zero/Few-Shot and Continual Learning Zero-shot and few-shot learning methods, which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al., 2017; Lee et al., 2017), and these were a"
2020.aacl-main.30,P18-1209,0,0.0213291,". Schuller et al. (2011) proposed the first Audio-Visual Emotion Challenge and Workshop (AVEC), which focused on multimodal emotion analysis for health. In recent years, most achievements in this area aimed to find a better modality fusion method. Zadeh et al. (2017) introduced a tensor fusion network that combined data representation from each modality to a tensor by performing the Cartesian product. In addition, the attention mechanism (Bahdanau et al., 2015) has been widely applied to do modality fusion and emphasis (Zadeh et al., 2018a; Pham et al., 2018; Tsai et al., 2019a). Furthermore, Liu et al. (2018) proposed a low-rank architecture to decrease the problem complexity, and Tsai et al. (2019b) introduced a modality re-construction method to generate occasional missing data in a modality. Although prior works have made progress on this task, the relationship between emotion categories has not been well modelled in previous works, except by Xu et al. (2020), who captured emotion correlations using graph networks for emotion recognition. However, the model is only based on a single textual modality. Additionally, the previous studies have not put much effort toward unseen and lowresource emoti"
2020.aacl-main.30,2020.acl-main.3,1,0.880382,"Missing"
2020.aacl-main.30,P19-1542,1,0.837438,"s a sequence of length T . Each modality has a set of emotion embeddings by mapping the GloVe textual emotion embeddings to the other modalities using ft→v and ft→a . The whole architecture is optimized end-to-end. 2.2 Zero/Few-Shot and Continual Learning Zero-shot and few-shot learning methods, which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al."
2020.aacl-main.30,P17-1135,0,0.061117,"Missing"
2020.aacl-main.30,D14-1162,0,0.0900088,"Missing"
2020.aacl-main.30,D16-1022,0,0.060758,"Missing"
2020.aacl-main.30,D19-1129,1,0.894582,"Missing"
2020.aacl-main.30,D17-1314,0,0.0158079,"speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al., 2017; Lee et al., 2017), and these were applied to some NLP tasks, such as opinion mining (Shu et al., 2016), document classification (Shu et al., 2017), and dialogue state tracking (Wu et al., 2019). 3 Methodology As shown in Figure 2, our model consists of three parts: intra-modal encoder networks, emotion embedding mapping modules, and an inter-modal fusion module. In this section, we first define the problem, and then we introduce the details of our model. 3.1 Problem Definition We define the input multi-modal data samples as X = {(ti , ai , vi )}Ii=1 , in which I denotes the total number of samples, and t, a, v denote the textual, acoustic, and visual modalities, respectively. For each modality, there is a set of emotion embeddings that"
2020.aacl-main.30,P19-1656,0,0.355638,"ample, Zadeh et al. (2017) proposed a tensor fusion network that combines three modalities from vectors to a tensor using the Cartesian product. In addition, the attention 269 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 269–280 c December 4 - 7, 2020. 2020 Association for Computational Linguistics mechanism is commonly used to do modality fusion (Zadeh et al., 2018a; Wang et al., 2018; Liang et al., 2018; Hazarika et al., 2018; Pham et al., 2018; Tsai et al., 2019a). Although significant improvements have been made on the multi-modal emotion recognition task, however, the relationship between emotions has not been well modelled, which can lead to sub-optimal performance. Also, the problem of low-resource multi-modal emotion recognition is not adequately studied. Multi-modal emotion recognition data is hard to collect and annotate, especially for low-resource emotions (e.g., surprise) that are rarely seen in daily life, which motivates us to investigate this problem. In this paper, we propose a modality-transferable network with cross-modality emotion e"
2020.aacl-main.30,P18-1208,0,0.109748,"Missing"
2020.aacl-main.30,D14-1187,0,0.0417856,"Missing"
2020.aacl-main.30,P19-1078,1,0.838945,"dings by mapping the GloVe textual emotion embeddings to the other modalities using ft→v and ft→a . The whole architecture is optimized end-to-end. 2.2 Zero/Few-Shot and Continual Learning Zero-shot and few-shot learning methods, which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al., 2017; Lee et al., 2017), and these were applied to some NLP"
2020.aacl-main.30,D18-1034,0,0.0187982,", which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017; Fernando et al., 2017; Lee et al., 2017), and these were applied to some NLP tasks, such as opinion mining (Shu et al., 2016), document classification (Shu et al., 2017), and dialogue state tracking (Wu et al., 2019). 3 Methodology As shown in Figure 2, our model consists of three parts: intra-modal enco"
2020.aacl-main.30,W18-6243,1,0.580485,"apping functions. In Figure 3, we show the Euclidean distances of emotion embeddings between categories. The relative positions are preserved very well after being transferred from the textual space to the visual and acoustic spaces. This indicates that the learned mapping functions (ft→v and ft→a ) are effective. Although it is not the main focus of this paper, we think improving the pre-trained textual emotion embeddings is an essential direction for future work. It can benefit all modalities and further enhance the overall performance. For example, incorporate semantic emotion information (Xu et al., 2018) to the original word embeddings. 6.3 Unseen emotion Metrics EF-LSTM LF-LSTM MulT Ours (TAV) Ours (TA) Ours (TV) Ours (AV) Ours (T) Ours (A) Ours (V) Zero/Few-Shot Results Benefiting from the pre-trained textual emotion embeddings and learned mapping functions, our model can recognize unseen emotion categories to a certain extent. We evaluate our model’s zero-shot learning ability on the low-resource categories in CMU-MOSEI (shown in Table 4) and IEMOCAP (shown in Table 5). For a fair comparison, we use the same training setting that is used in Table 1. This can ensure that no downgrade happen"
2020.aacl-main.30,W18-5001,0,0.0209188,"ach modality, the input is a sequence of length T . Each modality has a set of emotion embeddings by mapping the GloVe textual emotion embeddings to the other modalities using ft→v and ft→a . The whole architecture is optimized end-to-end. 2.2 Zero/Few-Shot and Continual Learning Zero-shot and few-shot learning methods, which address the data scarcity scenario, have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains, such as machine translation (Johnson et al., 2017; Gu et al., 2018), dialogue generation (Zhao and Eskenazi, 2018; Madotto et al., 2019), dialogue state tracking (Liu et al., 2019c; Wu et al., 2019), slot filling (Bapna et al., 2017; Liu et al., 2019b, 2020), and accented speech recognition (Winata et al., 2020). They have also been adopted in multiple cross-lingual tasks, such as named entity recognition (Xie et al., 2018; Ni et al., 2017), part-ofspeech tagging (Wisniewski et al., 2014; Huck et al., 2019), and question answering (Liu et al., 2019a; Lewis et al., 2019). Recently, several methods have been proposed for continual learning (Rusu et al., 2016; Kirkpatrick et al., 2017; Lopez-Paz and Ranzato"
2020.aacl-main.85,N18-1202,0,0.302824,"with different levels of complexity. The datasets for the tasks lie in different domains and styles to ensure task diversity. We also provide a set of Indonesian pre-trained models (IndoBERT) trained from a large and clean Indonesian dataset (Indo4B) collected from publicly available sources such as social media texts, blogs, news, and websites. We release baseline models for all twelve tasks, as well as the framework for benchmark evaluation, thus enabling everyone to benchmark their system performances. 1 Introduction Following the notable success of contextual pretrained language methods (Peters et al., 2018; Devlin et al., 2019), several benchmarks to gauge the progress of general-purpose NLP research, such as GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019), and CLUE (Xu et al., 2020), have been proposed. These benchmarks cover a large range of tasks to measure how well pre-trained models achieve compared to humans. However, these metrics are limited to high-resource languages, such as English and Chinese, that already have existing datasets available and are accessible to the research community. Most languages, by contrast, suffer from limited data collection and low awareness of ∗ publ"
2020.acl-main.3,Q17-1010,0,0.0209387,"ellaneous). 4.2 200, which would output the same dimension as the concatenated word-level and char-level embeddings. We use Adam optimizer with a learning rate of 0.0005. Cross-entropy loss is leveraged to train the 3-way classification in the first step, and the specific slot type predictions are used in the second step. We split 500 data samples in the target domain as the validation set for choosing the best model and the remainder are used for the test set. We implement the model in CT and RZT and follow the same setting as for our model for a fair comparison. Baselines We use word-level (Bojanowski et al., 2017) and character-level (Hashimoto et al., 2017) embeddings for our model as well as all the following baselines. 5 5.1 Cross-domain Slot Filling Quantitative Analysis As illustrated in Table 1, we can clearly see that our models are able to achieve significantly better performance than the current state-of-the-art approach (RZT). The CT framework suffers from the difficulty of capturing the whole slot entity, while our framework is able to recognize the slot entity tokens by sharing its parameters across all slot types. Based on the CT framework, the performance of RZT is still limited, and Coac"
2020.acl-main.3,P19-1236,0,0.208583,"-shot scenarios. In addition, further experiments show that our framework can be applied to cross-domain named entity recognition, and achieves better adaptation performance than other existing frameworks. Guerini et al., 2018). Coping with low-resource problems where there are zero or few existing training samples has always been an interesting and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an atte"
2020.acl-main.3,N06-1022,0,0.0247609,"sks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in our Coach framework consists of two steps. In the first step, we utilize a BiLSTM-CRF structure (Lample et al., 2016) to learn the general pattern of slot entities by having our model predict whether t"
2020.acl-main.3,N16-1030,0,0.43454,"3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in our Coach framework consists of two steps. In the first step, we utilize a BiLSTM-CRF structure (Lample et al., 2016) to learn the general pattern of slot entities by having our model predict whether tokens are slot entities or not (i.e., 20 3-way classification for each token). In the second step, our model further predicts a specific type for each slot entity based on the similarities with the description representations of all possible slot types. To generate representations of slot entities, we leverage another encoder, BiLSTM (Hochreiter and Schmidhuber, 1997), to encode the hidden states of slot entity tokens and produce representations for each slot entity. We represent the user utterance with n token"
2020.acl-main.3,D17-1169,0,0.0457974,"slot entity based on the similarities with the description representations of all possible slot types. To generate representations of slot entities, we leverage another encoder, BiLSTM (Hochreiter and Schmidhuber, 1997), to encode the hidden states of slot entity tokens and produce representations for each slot entity. We represent the user utterance with n tokens as w = [w1 , w2 , ..., wn ], and E denotes the embedding layer for utterances. The whole process can be formulated as follows: labels to generate correct and incorrect utterance templates. Then, we use BiLSTM and an attention layer (Felbo et al., 2017) to generate the utterance and template representations: [h1 , h2 , ..., hn ] = BiLSTM(E(w)), (1) n X exp(et ) et = ht wa , αt = Pn , R= αt ht , j=1 exp(ej ) t=1 (6) th where ht is the BiLSTM hidden state in the t step, wa is the weight vector in the attention layer and R is the representation for the input utterance or template. We minimize the regularization loss functions for the right and wrong templates, which can be formulated as follows: [p1 , p2 , ..., pn ] = CRF([h1 , h2 , ..., hn ]), (2) Lr = MSE(Ru , Rr ), (7) Lw = −β × MSE(Ru , Rw ), (8) where [p1 , p2 , ..., pn ] are the logits fo"
2020.acl-main.3,J82-2005,0,0.502301,"Missing"
2020.acl-main.3,N18-2118,0,0.104523,"se additional trouble for the final prediction. We emphasize that in order to capture the whole slot entity, it is pivotal for the model to share its parameters for all slot types in the source domains and learn the general pattern of slot entities. Therefore, as depicted in Figure 1b, we propose a new cross-domain slot filling framework called Coach, Introduction Slot filling models identify task-related slot types in certain domains for user utterances, and are an indispensable part of task-oriented dialog systems. Supervised approaches have made great achievements in the slot filling task (Goo et al., 2018; Zhang et al., 2019), where substantial labeled training samples are needed. However, collecting large numbers of training samples is not only expensive but also time-consuming. To cope with the data scarcity issue, we are motivated to investigate cross-domain slot filling methods, which leverage knowledge learned in the source domains and adapt the models to the target domain with a minimum number of target domain labeled training samples. A challenge in cross-domain slot filling is to handle unseen slot types, which prevents general 19 Proceedings of the 58th Annual Meeting of the Associati"
2020.acl-main.3,W18-5036,0,0.0187586,"templates to regularize the utterance representations. By doing so, the model learns to cluster the representations of semantically similar utterances (i.e., in the same or similar templates) into a similar vector space, which further improves the adaptation robustness. Experimental results show that our model surpasses the state-of-the-art methods by a large margin in both zero-shot and few-shot scenarios. In addition, further experiments show that our framework can be applied to cross-domain named entity recognition, and achieves better adaptation performance than other existing frameworks. Guerini et al., 2018). Coping with low-resource problems where there are zero or few existing training samples has always been an interesting and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have t"
2020.acl-main.3,D19-1129,1,0.734398,"Missing"
2020.acl-main.3,D18-1498,0,0.0659828,"Missing"
2020.acl-main.3,2020.repl4nlp-1.1,1,0.826185,"n addition, further experiments show that our framework can be applied to cross-domain named entity recognition, and achieves better adaptation performance than other existing frameworks. Guerini et al., 2018). Coping with low-resource problems where there are zero or few existing training samples has always been an interesting and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to pro"
2020.acl-main.3,D17-1206,0,0.0410278,"Missing"
2020.acl-main.3,P18-1096,0,0.0174431,"g and challenging task (Kingma et al., 2014; Lample et al., 2018; Liu et al., 2019a,b; Lin et al., 2020). Cross-domain adaptation addresses the data scarcity problem in low-resource target domains (Pan et al., 2010; Jaech et al., 2016; Guo et al., 2018; Jia et al., 2019; Liu et al., 2020; Winata et al., 2020). However, most research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parser"
2020.acl-main.3,P19-1547,0,0.128913,"research studying the cross-domain aspect has not focused on predicting unseen label types in the target domain since both source and target domains have the same label types in the considered tasks (Guo et al., 2018). In another line of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in ou"
2020.acl-main.3,P19-1519,0,0.0458009,"ble for the final prediction. We emphasize that in order to capture the whole slot entity, it is pivotal for the model to share its parameters for all slot types in the source domains and learn the general pattern of slot entities. Therefore, as depicted in Figure 1b, we propose a new cross-domain slot filling framework called Coach, Introduction Slot filling models identify task-related slot types in certain domains for user utterances, and are an indispensable part of task-oriented dialog systems. Supervised approaches have made great achievements in the slot filling task (Goo et al., 2018; Zhang et al., 2019), where substantial labeled training samples are needed. However, collecting large numbers of training samples is not only expensive but also time-consuming. To cope with the data scarcity issue, we are motivated to investigate cross-domain slot filling methods, which leverage knowledge learned in the source domains and adapt the models to the target domain with a minimum number of target domain labeled training samples. A challenge in cross-domain slot filling is to handle unseen slot types, which prevents general 19 Proceedings of the 58th Annual Meeting of the Association for Computational"
2020.acl-main.3,D17-1125,0,0.0171551,"e of work, to bypass unseen label types, Ruder and Plank (2018) and Jia et al. (2019) utilized target domain training samples, so that there was no unseen label type in the target domain. Recently, based on the framework proposed by Bapna et al. (2017) (discussed in Section 1), Lee and Jha (2019) added an attention layer to produce slot-aware representations, and Shah et al. (2019) leveraged slot examples to increase the robustness of cross-domain slot filling adaptation. 2 3 Related Work Coarse-to-fine methods in NLP are best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Zhang et al. (2017) reduced the search space of semantic parsers by using coarse macro grammars. Different from the previous work, we apply the idea of coarse-to-fine into cross-domain slot filling to handle unseen slot types by separating the slot filling task into two steps (Zhai et al., 2017; 3.1 Methodology Coach Framework As depicted in Figure 2, the slot filling process in our Coach framework consists of two steps. In the first step, we utilize a BiLSTM-CRF structure (Lample et al., 2016) to learn the general pattern of slot entities by having our model predict whether tokens are slot entities or not (i.e."
2020.acl-main.3,W03-0419,0,\N,Missing
2020.acl-main.348,P13-2037,0,0.0337159,"uces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corp"
2020.acl-main.348,W17-7509,0,0.0228281,"propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee e"
2020.acl-main.348,D18-1346,0,0.0142325,"e ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee et al. (2019) propose to use methods to generate artificial code-switching data using a linguistic constraint. Winata et al. (2018) proposes to leverage syntactic information to improve the identification of the location of code-switching points, and improve the language model performance. Finally Garg et al. (2018) and Winata et al. (2019) propose new neural-based methods using SeqGAN and pointer-generator (Pointer-Gen) to generate diverse synthetic codeswitching sentences that are sampled from the real code-switching data distribution. 3 Meta-Transfer Learning We aim to effectively transfer knowledge from source domains to a specific target domain. We denote our model by fθ with parameters θ. Our model accepts a set of speech inputs X = {x1 , . . . , xn } and generates a set of utterances Y = {y1 , . . . , ym }. The training involves a set of speech datasets in which each dataset is treated as a task T"
2020.acl-main.348,D18-1398,0,0.0637116,"pplicable to other natural language tasks, such as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic f"
2020.acl-main.348,C12-1102,1,0.753256,"h processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameter"
2020.acl-main.348,W19-5508,1,0.781035,"uch as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs)."
2020.acl-main.348,P19-1542,1,0.842392,"veness of our approach in terms of error rate, and that our approach is also faster to converge. We also show that our approach is also applicable to other natural language tasks, such as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorpo"
2020.acl-main.348,P18-1143,0,0.0972363,"ks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a network Algorithm 1 Meta-Transfer Learning Require: Dsrc , Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee et al. (2019) propose to use methods to generate artificial code-switching data using a linguistic constraint. Winata et al. (2018) proposes to leverage syntactic information to improve the identification of the location of code-switching points, and improve the language model performance. Finally Garg et al. (2018) and Winata et al. (2019) propose new neural-based methods using SeqGAN and pointer-generator (Pointer-Gen) to generate diverse synthetic codeswitching sentences that are sampled from the real code-switching data distribution. 3 Meta-Transfer Learning We aim to effectively"
2020.acl-main.348,P19-1253,0,0.0234108,"tural language tasks, such as code-switching language modeling tasks. 2 Related Work Meta-learning Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent ne"
2020.acl-main.348,W18-3207,1,0.87279,"Dtgt Require: α, β: step size hyperparameters 1: Randomly initialize θ 2: while not done do 3: Sample batch data Dtra ∼ (Dsrc , Dtgt ), Dval ∼ Dtgt 4: for all DTtra ∈ Dtra do i 5: Evaluate ∇θ LDTtra (fθ ) using DTtra i i 6: Compute adapted parameters with gradient descent: θT0 i = θ − α∇θ LDTtra (fθ ) i 7: end for P   8: θ ← θ − β i ∇θ LDval fθT0 i 9: end while with monolingual corpora of two languages, and subsequently training on code-switched data. Pratapa et al. (2018) and Lee et al. (2019) propose to use methods to generate artificial code-switching data using a linguistic constraint. Winata et al. (2018) proposes to leverage syntactic information to improve the identification of the location of code-switching points, and improve the language model performance. Finally Garg et al. (2018) and Winata et al. (2019) propose new neural-based methods using SeqGAN and pointer-generator (Pointer-Gen) to generate diverse synthetic codeswitching sentences that are sampled from the real code-switching data distribution. 3 Meta-Transfer Learning We aim to effectively transfer knowledge from source domains to a specific target domain. We denote our model by fθ with parameters θ. Our model accepts a set of"
2020.acl-main.348,K19-1026,1,0.902121,"matrix language. This can occur within a sentence, which is known as intrasentential code-switching or between two matrix language sentences, which is called inter-sentential code-switching (Heredia and Altarriba, 2001). Learning a code-switching automatic speech recognition (ASR) model has been a challenging task for decades due to data scarcity and difficulty in capturing similar phonemes in different These two authors contributed equally. ∇ ∇ Introduction ∗ ) languages. Several approaches have focused on generating synthetic speech data from monolingual resources (Nakayama et al., 2018; Winata et al., 2019). However, these methods are not guaranteed to generate natural code-switching speech or text. Another line of work explores the feasibility of leveraging large monolingual speech data in the pre-training and applying fine-tuning on the model using a limited source of code-switching data, which has been found useful to improve the performance (Li et al., 2011; Winata et al., 2019). However, the transferability of these pretraining approaches is not optimized on extracting useful knowledge from each individual languages in the context of code-switching, and even after the finetuning step, the m"
2020.acl-main.348,2020.acl-main.336,0,0.0335127,"Our idea of learning knowledge transfer from source monolingual resources to a code-switching model comes from MAML (Finn et al., 2017). Probabilistic MAML (Finn et al., 2018) is an extension of MAML, which has better classification coverage. Meta-learning has been applied to natural language and speech processing (Hospedales et al., 2020). Madotto et al. (2019) extends MAML to the personalized text generation domain and successfully produces more personaconsistent dialogue. Gu et al. (2018) and Qian and Yu (2019) and Lin et al. (2019) propose to apply meta-learning on low-resource learning. Yu et al. (2020) applies MAML to hypernym detection. Several applications have been proposed in speech applications, such as cross-lingual speech recognition (Hsu et al., 2019), speaker adaptation (Klejch et al., 2018, 2019), and cross-accent speech recognition (Winata et al., 2020). Code-Switching ASR Li and Fung (2012) introduces a statistical method to incorporate a linguistic theory into a code-switching speech recognition system, and Adel et al. (2013a,b) explore syntactic and semantic features on recurrent neural networks (RNNs). Baheti et al. (2017) adapts effective curriculum learning by training a ne"
2020.cl-2.1,2020.cl-2.3,0,0.0613442,"Missing"
2020.cl-2.1,2020.acl-main.447,0,0.0363097,"he 2010s; red line). The fraction of papers mentioning two or more languages (yellow line) and the average per year (green line) showed increases in the 1990s and 2000s, though these appear to have slowed recently.3 The other trend is a matter of increasing supply: The diversity of computational tools now available—from conceptual definitions of language meaning to operationalizations in downloadable models—has exploded in the past decade. The term “semantic representation” was, not long ago, one that referred to a range of linguistic abstractions. 1 We explored ACL Anthology papers in S2ORC (Lo et al. 2020) with publication years 1980–2019, a total of 40,402 papers. 2 The list is Ethnologue’s list of the 20 most spoken languages in 2019, with Mandarin and Wu Chinese mapped to the string chinese. See https://www.ethnologue.com/guides/ethnologue200. Less dominant languages are, of course, also interesting, but also more sparse in the data. 3 The leveling off of these last two trends is, we speculate, due to the emergence of new representation learning methods that work best with very large data sets. We expect increasing multilinguality of the largest data sets and pretrained representations will"
2020.cl-2.1,2020.cl-2.2,0,0.426794,"cept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adv"
2020.cl-2.1,2020.cl-2.6,0,0.07929,"entary might also be true, and realizations in different languages of the same concept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granulari"
2020.cl-2.1,J82-2005,0,0.392652,"us lines of work that illustrate a range of creative advances exploring natural language meaning, specifically with a multilingual focus. In inviting submissions, we encouraged a broad reading of the term “representations,” in granularity (words, sentences, paragraphs, etc.) and in theoretical assumptions (symbolic, neural, hybrid, etc.). We anticipated breadth as well in the set of motivating applications and evaluation methods. Our deliberate reference to interlingual—not only multilingual—representations evokes recent re-imaginings of interlingual machine translation, a classical approach (Richens 1958). We explicitly encouraged submissions that consider less-commonly studied languages and that go beyond mere projection of representations from text in one language to another. Of particular interest to our editorial team is the potential for multilingual representations (of any kind) to help overcome challenges of polysemy in individual languages. It has been shown that translations into other languages can help at distinguishing senses monolingually (Resnik and Yarowsky 1999). But the complementary might also be true, and realizations in different languages of the same concept may help to ob"
2020.cl-2.1,2020.cl-2.4,0,0.0730249,"ummarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adversarial Translation en, es, de, it, fi, ar, ms, he Linked Data Intrinsic/extrinsic evaluation Word Similarity POS, dependencies, SRL, NER,"
2020.cl-2.1,2020.cl-2.5,0,0.0495885,"Missing"
2020.cl-2.1,D18-1268,0,0.0204012,"ion. The challenges addressed include learning unsupervised representations, introducing priors and linguistic knowledge to compute the representations, and evaluating the quality of these representations, taking into account linguistic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of th"
2020.cl-2.1,D17-1207,0,0.0236204,"tic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of the main contributions is training the adversarial mapping in a latent space, with the hope that this will minimize the effect of a lack of isomorphism between the two original embedding spaces. In addition, the authors combine several l"
2020.cl-2.1,Q17-1010,0,\N,Missing
2020.cl-2.1,Q19-1038,0,\N,Missing
2020.emnlp-main.226,speer-havasi-2012-representing,0,0.0180632,"stafazadeh et al., 2016) for our experiments. It consists of 98,161 stories, where each story contains five sentences. 88,344/4,908/4,909 stories are used for train/validation/test sets, respectively. Following Guan et al. (2020), for each sentence, delexicalization is performed by replacing all the names and entities in stories with special placeholders, [MALE], [FEMALE], and [NEUTRAL] for male, female and unknown names and entities, respectively. Given the first sentence of each story, our model’s task is to generate the rest of the story. For our external knowledge base, we use ConceptNet (Speer and Havasi, 2012), consists of 600k knowledge triples. the same settings. To train our contextual knowledge ranker, we set the margin to 5.0. We set the number of knowledge sentences in Ri to 10. Therefore, for a given story context, the top 10 retrieved knowledge sentences from ConceptNet according to U SE are chosen as the positive samples. We further select 40 negative samples to compute our margin loss. We then randomly sample 50 (positive, negative) pairs for each story context to train our contextual knowledge ranker. In total, we used ∼15 million pairs for training and ∼1 million pairs for validation. A"
2020.emnlp-main.226,P18-1205,0,0.0354557,"ting commonsense knowledge into story generation with attention-based models (Guan et al., 2019; Chen et al., 2019). Recently, pre-trained language models have been used to finetune on both story completion datasets and commonsense knowledge to further improve the quality of story completion (Guan et al., 2020). However, few works concern the controllability of language model generation, especially for the large pre-trained models that are common in today’s literature. Controllable Generation Controllable text generation has a wide range of applications, including controlling through persona (Zhang et al., 2018; Boyd et al., 2020), politeness (Niu and Bansal, 2018), etc. Wiseman et al. (2018) presented controlling generations by learning latent, discrete templates from data. Fu et al. (2019) discovered the importance of pivot words that determines the sentence attributes and presented a lexical analysis framework. To control large pre-trained models, Keskar et al. (2019) demonstrated the ability to control text generation through a wide range of aspects, such as domains and links. Plug-and-play language models Dathathri et al. (2019) also address whole document controllability by adding a linear cla"
2020.emnlp-main.226,P19-1139,0,0.0231779,"able 15 only mentions the keyword “realize” instead of centering around it. This is caused by the RAKE keywords extractor, which does not always extract the keywords that represent the sentence well. One way to mitigate this issue is to leverage longer context information to identify better keywords which is subject of the future work. 6 Related Work Knowledge Incorporation of knowledge into language models has shown promising results for downstream tasks, such as factual correct generation (Logan et al., 2019) , commonsense knowledge graph construction (Bosselut et al., 2019), entity typing (Zhang et al., 2019) and etc. More recently, several works have shown that inclusion of learned mechanisms for explicit or implicit knowledge can lead to the state-of-the-art results in Question Answering (Guu et al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Lewis et al., 2020) and dialogue modeling (Roller et al., 2020). Storytelling There are several different storytelling tasks described throughout the literature. Storytelling can be classified into story completion (Chen et al., 2019), story ending generation (Guan et al., 2019), story generation from prompts (Fan et al., 2018) or titles (Yao et al., 2"
2020.emnlp-main.226,D18-1356,0,0.028812,"an et al., 2019; Chen et al., 2019). Recently, pre-trained language models have been used to finetune on both story completion datasets and commonsense knowledge to further improve the quality of story completion (Guan et al., 2020). However, few works concern the controllability of language model generation, especially for the large pre-trained models that are common in today’s literature. Controllable Generation Controllable text generation has a wide range of applications, including controlling through persona (Zhang et al., 2018; Boyd et al., 2020), politeness (Niu and Bansal, 2018), etc. Wiseman et al. (2018) presented controlling generations by learning latent, discrete templates from data. Fu et al. (2019) discovered the importance of pivot words that determines the sentence attributes and presented a lexical analysis framework. To control large pre-trained models, Keskar et al. (2019) demonstrated the ability to control text generation through a wide range of aspects, such as domains and links. Plug-and-play language models Dathathri et al. (2019) also address whole document controllability by adding a linear classifier on top of G PT-2 to predict whether generated text observes a particular st"
2020.emnlp-main.226,D18-1462,0,0.0447244,"al., 2020; Karpukhin et al., 2020; Lee et al., 2019; Lewis et al., 2020) and dialogue modeling (Roller et al., 2020). Storytelling There are several different storytelling tasks described throughout the literature. Storytelling can be classified into story completion (Chen et al., 2019), story ending generation (Guan et al., 2019), story generation from prompts (Fan et al., 2018) or titles (Yao et al., 2019), and story generation from a given sentence (Guan et al., 2020). Different approaches have been developed to model the structure of stories with storylines (Yao et al., 2019), skeletons (Xu et al., 2018), Conditional Variational AutoEncoders (Wang and Wan, 2019) and a coarse-to-fine framework (Fan et al., 2019). Other works focus on incorporating commonsense knowledge into story generation with attention-based models (Guan et al., 2019; Chen et al., 2019). Recently, pre-trained language models have been used to finetune on both story completion datasets and commonsense knowledge to further improve the quality of story completion (Guan et al., 2020). However, few works concern the controllability of language model generation, especially for the large pre-trained models that are common in today"
2020.emnlp-main.273,D19-5602,0,0.0844866,"Missing"
2020.emnlp-main.273,D18-1547,0,0.426427,"des the Lev for updating the previous dialogue state; then, the updated state is used to search the external knowledge base; and finally, a response decoder decodes response by conditioning on the dialogue context and knowledge base match result. MinTL is easy to set up by using different pretrained seq2seq backbones. We conduct extensive experiments on both DST and end-to-end dialogue response generation tasks with two pre-trained seq2seq models, such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2019). The experimental result on a large-scale task-oriented dialogue benchmark MultiWOZ (Budzianowski et al., 2018; Eric et al., 2019) suggests that our proposed method significantly improves SOTA performance in both the full data and simulated low resource setting. Our contributions are summarized as follows: • We propose the MinTL framework that efficiently leverages pre-trained language models for task-oriented dialogue without any ad hoc module. • We propose the novel Lev for efficiently tracking the dialogue state with the minimal length of generation, which greatly reduces the inference latency. • We instantiate our framework with two different pre-trained backbones, and both of them improve the SOT"
2020.emnlp-main.273,P19-1360,0,0.0321159,"o simplify the system design and reduce human supervision, several end-to-end trainable systems have been proposed (Bordes et al., 2016; Wen et al., 2017; Lei et al., 2018; Neelakantan et al., 2019; Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018). These methods have been shown to achieve promising results in single-domain tasks. However, the recently proposed multi-domain taskoriented dialogue datasets (Budzianowski et al., 2018; Eric et al., 2019) bring new challenges for multi-domain dialogue state tracking and response generation. Several follow up works (Wu et al., 2019a; Chen et al., 2019; Budzianowski and Vuli´c, 3392 Figure 1: Dialogue state tracking with Lev. The model first generates Lev, then updates the dialogue state with new generated slot-values. The updating operations are insertion (blue), deletion (red), and substitution (green). 2019; Mehri et al., 2019; Madotto et al., 2020b) improved on the initial baselines with various methodologies. Zhang et al. (2019b) proposed the domain aware multi-decoder network and augmented the system act labels by leveraging the user act annotation, achieving the SOTA results in MultiWoz. However, the aforementioned works rely on task"
2020.emnlp-main.587,P17-1042,0,0.0235639,"he alignment of cross-lingual representations, which do not require any external resources. • Our model outperforms the previous state-ofthe-art model in both zero-shot and few-shot scenarios on the cross-lingual SLU task. • Extensive analysis and visualizations are made to illustrate the effectiveness of our approaches. 2 Related Work Cross-lingual Transfer Learning Cross-lingual transfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter-connections between the source and the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu"
2020.emnlp-main.587,D18-1038,0,0.0137979,"ffective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. C"
2020.emnlp-main.587,N19-1423,0,0.173683,"ing Cross-lingual transfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter-connections between the source and the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresou"
2020.emnlp-main.587,D17-1169,0,0.0283836,"utterance representations based on the slot labels, which increases the generalization ability in the target language. 3.1.2 Implementation Details Figure 2 (Left) illustrates an utterance encoder and a label encoder that generate the representations for utterances and labels, respectively. We denote the user utterance as w = [w https://www.mathcha.io/editor 1 , w2 , ..., wn ], where n is the length of the utterance. Similarly, we represent the slot label sequences as s = [s1 , s2 , ..., sn ]. We combine a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) and an attention layer (Felbo et al., 2017) to encode and produce the representations for user utterances and slot label sequences. The representation generation process is defined as follows: w w [hw 1 , h2 , ..., hw ] = BiLSTMutter (E(w)), (1) [hs1 , hs2 , ..., hsn ] = BiLSTMlabel (E(s)), (2) exp(mw w w w i ) mw i = hi v , αi = Pn w) , exp(m t t=1 (3) exp(msi ) msi = hsi v s , αis = Pn s , t=1 exp(mt ) (4) u= n X αiw hw i , l = i=1 n X αis hsi , (5) i=1 where the superscript w and s represents utterance and label, respectively, v is a trainable weight vector in the attention layer, αi is the attention score for each token i, E denote"
2020.emnlp-main.587,N18-2118,0,0.0670143,"Missing"
2020.emnlp-main.587,P19-1544,0,0.0187162,"perfect, the sentence-level alignment is still imperfect owing to grammatical and syntactical variances across languages. Therefore, we emphasize that cross-lingual methods should focus on the alignments of word-level and 1 Introduction sentence-level representations, and increase the roData-driven neural-based supervised training ap- bustness for inherent imperfect alignments. proaches have shown effectiveness in spoken lanIn this paper, we concentrate on the cross-lingual guage understanding (SLU) systems (Goo et al., SLU task (as illustrated in Figure 1), and we con2018; Chen et al., 2019; Haihong et al., 2019). sider both few-shot and zero-shot scenarios. To However, collecting large amounts of high-quality improve the quality of cross-lingual alignment, we training data is not only expensive but also timefirst propose a Label Regularization (LR) method, https://www.mathcha.io/editor consuming, which makes these approaches not scal- which utilizes the slot label sequences to regularize able to low-resource languages due to the scarcity the utterance representations. We hypothesize that of training data. Cross-lingual adaptation has natu- if the slot label sequences of user utterances are rally aris"
2020.emnlp-main.587,P82-1020,0,0.669392,"Missing"
2020.emnlp-main.587,D19-1252,0,0.140298,"Missing"
2020.emnlp-main.587,D17-1302,0,0.0161335,"ss-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two ad"
2020.emnlp-main.587,N16-1030,0,0.0603963,"Missing"
2020.emnlp-main.587,D19-1129,1,0.884995,"Missing"
2020.emnlp-main.587,D17-1269,0,0.0607767,"Missing"
2020.emnlp-main.587,Q17-1022,0,0.0419006,"Missing"
2020.emnlp-main.587,P19-1493,0,0.0163719,"nsfer learning is able to circumvent the requirement of enormous training data by leveraging the learned knowledge in the source language and learning inter-connections between the source and the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna e"
2020.emnlp-main.587,N19-1380,0,0.322373,"), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. Chen et al. (2018) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state tracking. Instead of highly relying on extensive bilingual resources, Qin et al. (2020) introduced a data augmentation framework to generate multilingual code-switching data for cross-lingual tasks including the SLU task. Liu et al. (2019b) leveraged a mixed language training framewor"
2020.emnlp-main.587,D18-1270,0,0.0639289,"Missing"
2020.emnlp-main.587,P19-1078,1,0.822312,"., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. Chen et al. (2018) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state"
2020.emnlp-main.587,2020.acl-main.3,1,0.83076,"017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ"
2020.emnlp-main.587,D18-1034,0,0.0188539,"the target language. Artetxe et al. (2017) and Conneau et al. (2018) conducted cross-lingual word embedding mapping with zero or very few supervision signals. Recently, pre-training cross-lingual language models on large amounts of monolingual or bilingual resources have been proved to be effective for the downstream tasks (e.g., natural language inference) (Conneau and Lample, 2019; Devlin et al., 2019; Pires et al., 2019; Huang et al., 2019). Additionally, many cross-lingual transfer algorithms have been proposed to solve specific cross-lingual tasks, for example, named entity recognition (Xie et al., 2018; Mayhew et al., 2017; Liu et al., 2020a), part of speech tagging (Kim et al., 2017; Zhang et al., 2016), entity linking (Zhang et al., 2013; Sil et al., 2018; Upadhyay et al., 2018b), personalized conversations (Lin et al., 2020), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c e"
2020.emnlp-main.587,W18-3023,0,0.128861,"), and dialog systems (Upadhyay et al., 2018a; Chen et al., 2018). Cross-lingual Task-oriented Dialog Systems Deploying task-oriented dialogue systems in lowresource domains (Bapna et al., 2017; Wu et al., 2019; Liu et al., 2020b) or languages (Chen et al., 2018; Liu et al., 2019a,b), where the number of training of samples is limited, is a challenging task. Mrkˇsi´c et al. (2017) expanded Wizard of Oz (WOZ) into multilingual WOZ by annotating two additional languages. Schuster et al. (2019) introduced a multilingual SLU dataset and proposed to leverage bilingual corpus and multilingual CoVe (Yu et al., 2018) to align the representations across languages. Chen et al. (2018) proposed a teacherstudent framework based on a bilingual dictionary or bilingual corpus for building cross-lingual dialog state tracking. Instead of highly relying on extensive bilingual resources, Qin et al. (2020) introduced a data augmentation framework to generate multilingual code-switching data for cross-lingual tasks including the SLU task. Liu et al. (2019b) leveraged a mixed language training framework for cross-lingual task-oriented dialogue systems. And Liu et al. (2019a) proposed to refine the crosslingual word embe"
2020.emnlp-main.587,N16-1156,0,0.0651037,"Missing"
2020.findings-emnlp.215,P19-1470,0,0.0290641,"action. This enables models to learn a better dialogue policy via interaction (Asri et al., 2016; Li et al., 2017; Wu et al., 2019c; Peng et al., 2018), and it is especially useful in scenarios in where few or no data is available (Liu and Lane, 2017; Liu et al., 2017; Shah et al., 2018; Kreyssig et al., 2018; Li et al., 2020). In our work, instead, we use all the possible user goal queries to generate dialogues directly, instead of creating a reinforcement learning loop to train the model. Language Models as Knowledge Bases has been used for encoding common sense knowledge into transformers (Bosselut et al., 2019; Liu et al., 2019a; Xiong et al., 2019; Wang et al., 2020, 2019). (Guan et al., 2020) improved story generation by training a Language Model with knowledge triples converted into sentences using predefined templates (Levy et al., 2017). Differently, we extract templates from real data, and we aim to store the KB into the models parameters to be able to extract knowledge directly, instead of improving common sense generation. Moreover, several studies tried to extract (Petroni et al., 2019; Kassner and Sch¨utze, 2019; Petroni et al., 2020) or use (Roberts et al., 2020) large pre-trained models"
2020.findings-emnlp.215,D18-1547,0,0.27377,", 2017; Eric and Manning, 2017; Madotto et al., 2018; Reddy et al., 2019; Wu et al., 2019b). These systems re2372 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2372–2394 c November 16 - 20, 2020. 2020 Association for Computational Linguistics quire at least the DST annotation for generating the API calls or to select the gold KB. Moreover, even with the most advanced transformer architecture (Kitaev et al., 2020; Lample et al., 2019; Child et al., 2019), end-to-end models struggle when the input becomes too large (Neelakantan et al., 2019). For example, in MWOZ (Budzianowski et al., 2018), there are 22K entities just for one of the domains. Interested readers can refer to Appendix C for an overview of different task-oriented methodologies. On the other hand, Petroni et al. (2019) discovered a simple yet effective way to query factual knowledge from BERT (Devlin et al., 2019). Later on, Roberts et al. (2020) fine-tuned a pre-trained language model, T5 (Raffel et al., 2019), on just question-answers pairs, without letting the model access any external context or knowledge. These results suggest that the actual knowledge is stored in the model parameters. However, in task-oriente"
2020.findings-emnlp.219,D18-1431,0,0.0889668,"20 Association for Computational Linguistics with human annotators in the loop to generate contuining text with positive sentiment. Both of these approaches require learning/fine-tuning all of the models’ parameters, and new desired attributes cannot be easily incorporated into the generation once the models have been trained. Other approaches that do not alter the language model, but modify the decoding procedure for controlled generation include 1) re-weighting the output distribution using discriminators (Holtzman et al., 2018) or bag of words (Ghazvininejad et al., 2017; See et al., 2019; Baheti et al., 2018), and 2) perturbing the models activation with an attribute model (PPLM) (Dathathri et al., 2019). These approaches, instead, are plug-and-play methods in that they can be used on top of any existing pre-trained language model. These methods, do not modify or train the parameters of the original models and they can achieve comparable performance to finetuning methods (Dathathri et al., 2019). Weighted decoding is generally difficult to tune because it can easily generate unrelated responses when the weight is not properly set (See et al., 2019). On the other hand, (Dathathri et al., 2019) incu"
2020.findings-emnlp.219,D19-1165,0,0.0578978,"Missing"
2020.findings-emnlp.219,N19-1423,0,0.0299653,"se using GPT2 (Radford et al., 2019).PP For diversity, we use the distinct n-grams (Li et al., 2016a) (normalized by the length of theAD text) across all the responses generated by a given method. For evaluating the attribute consistency, we train external classifiers using no-overlapping data with the attribute model. For sentiments, we use AMAZON-5 (McAuley and Leskovec, 2013) product reviews. For topics, we use the test-set data of AG-NEWS (Zhang et al., 2015) because we could not find another topic classification dataset with the same classes. For each dataset, we trained a separate BERT (Devlin et al., 2019) (base) classifier with a simple classification head. Table 2 in Appendix B, summarizes the dataset statistics and the performance of the trained scorer. 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 55.4 85.1 76.0 71.6 61.8 HM DG WD PP model comparison and attribute, which amount to 40 a74.0 total of69.0 420061.5 human annotations. Further details are provided in Appendix C. 20 5 1.0 Human Eval. is the most effective way for evaluating open-domain chat-bots. In this paper, we evaluate two aspects from the generated response: Humanness and Attribute Consistency. The first is used for evaluating the fluency"
2020.findings-emnlp.219,W17-4912,0,0.147474,"repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Fina"
2020.findings-emnlp.219,P18-5002,0,0.0341462,"Missing"
2020.findings-emnlp.219,P17-4008,0,0.498709,", pages 2422–2433 c November 16 - 20, 2020. 2020 Association for Computational Linguistics with human annotators in the loop to generate contuining text with positive sentiment. Both of these approaches require learning/fine-tuning all of the models’ parameters, and new desired attributes cannot be easily incorporated into the generation once the models have been trained. Other approaches that do not alter the language model, but modify the decoding procedure for controlled generation include 1) re-weighting the output distribution using discriminators (Holtzman et al., 2018) or bag of words (Ghazvininejad et al., 2017; See et al., 2019; Baheti et al., 2018), and 2) perturbing the models activation with an attribute model (PPLM) (Dathathri et al., 2019). These approaches, instead, are plug-and-play methods in that they can be used on top of any existing pre-trained language model. These methods, do not modify or train the parameters of the original models and they can achieve comparable performance to finetuning methods (Dathathri et al., 2019). Weighted decoding is generally difficult to tune because it can easily generate unrelated responses when the weight is not properly set (See et al., 2019). On the o"
2020.findings-emnlp.219,P18-1152,0,0.291094,"or Computational Linguistics: EMNLP 2020, pages 2422–2433 c November 16 - 20, 2020. 2020 Association for Computational Linguistics with human annotators in the loop to generate contuining text with positive sentiment. Both of these approaches require learning/fine-tuning all of the models’ parameters, and new desired attributes cannot be easily incorporated into the generation once the models have been trained. Other approaches that do not alter the language model, but modify the decoding procedure for controlled generation include 1) re-weighting the output distribution using discriminators (Holtzman et al., 2018) or bag of words (Ghazvininejad et al., 2017; See et al., 2019; Baheti et al., 2018), and 2) perturbing the models activation with an attribute model (PPLM) (Dathathri et al., 2019). These approaches, instead, are plug-and-play methods in that they can be used on top of any existing pre-trained language model. These methods, do not modify or train the parameters of the original models and they can achieve comparable performance to finetuning methods (Dathathri et al., 2019). Weighted decoding is generally difficult to tune because it can easily generate unrelated responses when the weight is n"
2020.findings-emnlp.219,I17-1099,0,0.155669,"of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In thi"
2020.findings-emnlp.219,D16-1140,0,0.0204114,"y of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subr"
2020.findings-emnlp.219,2020.findings-emnlp.41,1,0.835521,"Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Finally, residual adapters (Houlsby et al., 2019) has been used to learn multiple language generation tasks (Lin et al., 2020) without fine-tuning the original models’ parameters. Concurrently to our work, Smith et al. (2020) compare the performance and tradeoffs of three existing controllable language generation methods on 200 possible styles. 3 Methodology A dialogue consists of one or more alternating turns between two speakers. We define the dialogue history at turn t as Dt = {U1 , S1 , . . . , Ut } where Ut is the user utterance and St is the system response. For simplicity, we overload Dt to denote the concatenation of sequences across turns with a special token separating the turns. In this paper, we model the"
2020.findings-emnlp.219,D16-1230,0,0.0777179,"Missing"
2020.findings-emnlp.219,P19-1542,1,0.841346,"neration viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) con"
2020.findings-emnlp.219,D19-1205,0,0.0152924,", ot+1 , Ht+1 = LM(xt , Ht + ∆Ht ). At the beginning of the generation, ∆Ht is initialized to zero and it is updated using the gradients from the attribute model. Following Dathathri et al. 2424 Dataset Task #C SST-5 (Socher et al., 2013) Daily Dialogue (Li et al., 2017) AG NEWS (Zhang et al., 2015) Sentiment Act Topic 5 4 4 Samples Train Test 318,582 2210 92,650 10,295 120,000 7,600 Train 77.68 80.58 90.68 F1-Score Test SotA 47.01 55.50† 80.00 86.10‡ 90.65 95.44§ Table 2: Attribute dataset statistics and performance. State-of-the-Art (SotA) results are taken from † (Munikar et al., 2019), ‡ (Kumar et al., 2019), and § (Yang et al., 2019). (2019), we rewrite the attribute model p(a|X) as p(a|Ht + ∆Ht ) and we define the gradient update for ∆Ht as ∇∆Ht log p(a|Ht + ∆Ht ) k∇∆Ht log p(a|Ht + ∆Ht )kγ (3) where α is the step size, and γ is the scaling coefficient for the normalization term. Equation 3 is repeated p times depending on how strongly we want the response to be conditioned to the attribute. We study the effect of the step-size α and the number of iterations p on the generated text in detail in e t = Ht + ∆Ht Section 6. Subsequently, the new H is computed and a new token is generated using e t"
2020.findings-emnlp.219,N16-1014,0,0.0312371,"taskspecific parameters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininej"
2020.findings-emnlp.219,D18-1255,0,0.0248346,"thods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend th"
2020.findings-emnlp.219,P16-1094,0,0.0341909,"taskspecific parameters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininej"
2020.findings-emnlp.219,D16-1127,0,0.0468992,"taskspecific parameters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininej"
2020.findings-emnlp.219,P02-1040,0,0.111307,"Missing"
2020.findings-emnlp.219,2020.findings-emnlp.17,0,0.0275868,"(Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019"
2020.findings-emnlp.219,N18-1202,0,0.0480166,"omputation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.1 1 HUMAN 2 DGPT NEGATIVE POSITIVE QUESTION BUSINESS SCIENCE & TECH SPORTS HUMAN 1 Table 1: Example of controllable response generation. DGPT is the DialoGPT (Zhang et al., 2019) response, while the others are generated using different attribute models. Introduction Large pre-trained language models (Peters et al., 2018; Radford et al., 2019; Raffel et al., 2019) have greatly improved the state-of-the-art in many downstream tasks. These language models are trained using the simple log-likelihood objective over large amounts of unlabeled data (e.g., Wikipedia articles). This approach results in large powerful language models that produce coherent text and can be ∗ Equal Contribution Work done primarily at the Caltech. 1 Code available at https://github.com/ andreamad8/PPCM † Hmmmm. I really want to try bull riding. Do you have any interest in that? I’d love to try! Can we schedule something for next week? Yes"
2020.findings-emnlp.219,P19-1534,0,0.0260474,"ics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on t"
2020.findings-emnlp.219,N19-1170,0,0.347773,"u guys again in a couple of weeks :D What’s it like? Do you ride on a regular basis or just for fun? Yea sure i can wait until after work. I am off at the end of my shift in the middle east. Sure thing! I’ll PM you my email. We can set up a dropbox or something and just message people together if you’d like. Ok how about Wednesday at 7pm central? I’ll be there with a buddy of mine who is a bull rider. Sure! What does your Saturday look like? used to perform unconditional language generation. However, they provide little control over the text generated. In the context of conversational models, See et al. (2019) show that being able to control the response generation can have a significant impact on the quality of conversations. However, controlled generation from these large conversational models remains a challenge, and is particularly more difficult in the absence of annotated conversational datasets. For large language models, controlled generation has recently received increased attention. In CTRL (Keskar et al., 2019), the language model is trained to generate based on a control code presented to the model at the start of the context. In Ziegler et al. (2019), GPT-2 (Radford et al., 2019) is fi"
2020.findings-emnlp.219,2020.findings-emnlp.291,0,0.0348626,"pics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Finally, residual adapters (Houlsby et al., 2019) has been used to learn multiple language gene"
2020.findings-emnlp.219,D13-1170,0,0.00500261,"oglikelihood of the attribute a under the conditional attribute model p(a|X) and ii) ensuring high loglikelihood of the generated text under the unmodified conversational language model p(X). The gradient updates are restricted to Ht so to preserve the original model parameters. Let ∆Ht be the update to Ht to shift the generated text towards possesing the desired attribute a i.e., ot+1 , Ht+1 = LM(xt , Ht + ∆Ht ). At the beginning of the generation, ∆Ht is initialized to zero and it is updated using the gradients from the attribute model. Following Dathathri et al. 2424 Dataset Task #C SST-5 (Socher et al., 2013) Daily Dialogue (Li et al., 2017) AG NEWS (Zhang et al., 2015) Sentiment Act Topic 5 4 4 Samples Train Test 318,582 2210 92,650 10,295 120,000 7,600 Train 77.68 80.58 90.68 F1-Score Test SotA 47.01 55.50† 80.00 86.10‡ 90.65 95.44§ Table 2: Attribute dataset statistics and performance. State-of-the-Art (SotA) results are taken from † (Munikar et al., 2019), ‡ (Kumar et al., 2019), and § (Yang et al., 2019). (2019), we rewrite the attribute model p(a|X) as p(a|Ht + ∆Ht ) and we define the gradient update for ∆Ht as ∇∆Ht log p(a|Ht + ∆Ht ) k∇∆Ht log p(a|Ht + ∆Ht )kγ (3) where α is the step size,"
2020.findings-emnlp.219,W18-6321,0,0.15583,"the response to be conditioned to the attribute. We study the effect of the step-size α and the number of iterations p on the generated text in detail in e t = Ht + ∆Ht Section 6. Subsequently, the new H is computed and a new token is generated using e t ). The described optimizaoet+1 , Ht+1 = LM(st , H tion process is repeated for every token in the generated sequence. As aforementioned, to ensure fluency we also take a step towards minimizing the Kullback–Leibler (KL) regularization between the perturbed and the original distribution. In addition, we also use the Post-norm Geometric Fusion (Stahlberg et al., 2018; Dathathri et al., 2019) for avoiding adversarial generation (Szegedy et al., 2013). ∆Ht ← ∆Ht + α Attribute Models In PPLM the authors propose two attribute models, such as bag-of-words and discriminators. In this paper, we focus on the latter, since discriminators based attribute models do not require human selected keywords. The discriminator is a linear classifier f trained on an annotated dataset with sentence and label pairs as (x, y) – note that these sentences do not necessarily need to be conversational responses, as in our case. For each sentence x of length t, we compute the set of"
2020.findings-emnlp.219,P18-1205,0,0.0233022,"meters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in c"
2020.findings-emnlp.219,2020.emnlp-main.698,0,0.0695298,"itly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Finally, residual adapters (Houlsby et al., 2019) has been used to learn m"
2020.findings-emnlp.41,D18-1443,0,0.0481438,"Missing"
2020.findings-emnlp.41,P19-1285,0,0.0456936,"Missing"
2020.findings-emnlp.41,W14-3348,0,0.0197859,"ks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More information about task description and the metrics used in each task are reported in Appendix A2. 4.2 Implementation and model comparison We implement VLM based on GPT-2-small (124M) (Wolf et al., 2019a), and experiment with varying adapter bottleneck dimensions in {10, 50, 100, 300} and pick the best one in each task to trade-off the performance with the parameter efficiency. Specifically,"
2020.findings-emnlp.41,N19-1423,0,0.0456527,"t downstream tasks. We adapt the design of the feedforward Transformer sub-layer following Bapna and Firat (2019). To elaborate, the adapter block consists of 1) a layer normalization (Ba et al., 2016) for an efficient adaptation and 2) a following autoencoder (Hinton and Zemel, 1994), with a residual connection. Formally, given the hidden repLM Head Adapter Layers CQA NLG SUM NMT DLG N× GPT-2 Layer Positional Word + Embedding Embedding Segment Embedding Task Embedding 2019; Peters et al., 2018) have shown to be very effective in language generation, whereas, bidirectional pre-trained models (Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019) significantly improve the performance of several down-stream classification tasks. Fine-tuning large pre-trained models has shown positive results in dialogue tasks (Wolf et al., 2019b; Budzianowski and Vuli´c, 2019) and other language generation tasks (Dong et al., 2019). However, all of the previous works only consider fine-tuning on each generation task individually, which requires a separate model for each task. In this work, we use only a single model, for multiple generation tasks. Residual adapters, derived from residual networks (He et al., 2016),"
2020.findings-emnlp.41,D16-1139,0,0.0271,"construct a set of task-specific segment embeddings. For example, in multi-turn dialogue, we alternate between System and User embeddings to help the model to capture the hierarchical structure of dialogues. Figure 1 shows the task embedding for each task, and more details are available in Appendix A2. 442 Knowledge Distillation In tasks with a large distributional shift from the original pre-trained language model (e.g., Machine Translation), we expect a larger performance gap between VLM and full fine-tuning. To cope with this issue, we propose to use sentence-level knowledge distillation (Kim and Rush, 2016), to help the task-specific parameters to better adapt to the task. Specifically, Param. GPT-2 Finetune w/o Pre-Train w/o Task Emb. LM Head VLM MT VLM w/o Task Emb. Reference SOTA 5× 5× 5× 2.55× 1.13× 1.13× 1.13× - Persona (DLG) ppl. ↓ BLEU ↑ 13.13 2.17 37.77 0.99 13.24 0.00 17.58 1.34 13.15 0.84 14.06 1.99 14.31 0.00 38.08¶ † 17.51 - NMT BLEU ↑ 25.45 16.52 0.61 12.05 22.49 24.19* 0.95 29.2§ 35.2‡ SUM ROUGE 2 ↑ 18.1 17.0 15.0 15.8 17.7 18.0* 15.0 17.20 ¶¶ 21.53 §§ CoQA F1 ↑ 67.7 15.1 35.2 47.0 69.3 66.2 32.2 45.4†† 82.5k NLG BLEU ↑ AVG ↑ 66.4 57.77 60.5 53.51 53.1 47.25 65.2 55.25 65.6 57.08 6"
2020.findings-emnlp.41,P04-1077,0,0.0172099,"ring multiple generation tasks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More information about task description and the metrics used in each task are reported in Appendix A2. 4.2 Implementation and model comparison We implement VLM based on GPT-2-small (124M) (Wolf et al., 2019a), and experiment with varying adapter bottleneck dimensions in {10, 50, 100, 300} and pick the best one in each task to trade-off the performance with the"
2020.findings-emnlp.41,2021.ccl-1.108,0,0.0875374,"Missing"
2020.findings-emnlp.41,P19-1542,1,0.903181,"Missing"
2020.findings-emnlp.41,K16-1028,0,0.033511,"of a task (e.g., Machine Translation). Then we replace the gold target (e.g., gold translation) in the training set with the greedy decoded output from the full fine-tuned model. Finally, the new constructed training set is used to fine-tune the student VLM. 4 4.1 Experiments Datasets & Evaluation Metrics We conduct our experiment on five diverse datasets covering multiple generation tasks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More"
2020.findings-emnlp.41,P02-1040,0,0.107492,"duct our experiment on five diverse datasets covering multiple generation tasks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More information about task description and the metrics used in each task are reported in Appendix A2. 4.2 Implementation and model comparison We implement VLM based on GPT-2-small (124M) (Wolf et al., 2019a), and experiment with varying adapter bottleneck dimensions in {10, 50, 100, 300} and pick the best one in eac"
2020.findings-emnlp.41,N18-1202,0,0.0925107,"meters in different colours. Residual Adapters These are trainable modules which steer the pre-trained model to different downstream tasks. We adapt the design of the feedforward Transformer sub-layer following Bapna and Firat (2019). To elaborate, the adapter block consists of 1) a layer normalization (Ba et al., 2016) for an efficient adaptation and 2) a following autoencoder (Hinton and Zemel, 1994), with a residual connection. Formally, given the hidden repLM Head Adapter Layers CQA NLG SUM NMT DLG N× GPT-2 Layer Positional Word + Embedding Embedding Segment Embedding Task Embedding 2019; Peters et al., 2018) have shown to be very effective in language generation, whereas, bidirectional pre-trained models (Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019) significantly improve the performance of several down-stream classification tasks. Fine-tuning large pre-trained models has shown positive results in dialogue tasks (Wolf et al., 2019b; Budzianowski and Vuli´c, 2019) and other language generation tasks (Dong et al., 2019). However, all of the previous works only consider fine-tuning on each generation task individually, which requires a separate model for each task. In this work, we use o"
2020.findings-emnlp.41,W04-1013,0,\N,Missing
2020.findings-emnlp.41,P17-1099,0,\N,Missing
2020.findings-emnlp.41,D19-5602,0,\N,Missing
2020.findings-emnlp.41,P16-1162,0,\N,Missing
2020.findings-emnlp.41,P18-1205,0,\N,Missing
2020.findings-emnlp.41,Q19-1016,0,\N,Missing
2020.findings-emnlp.41,W19-5917,0,\N,Missing
2020.findings-emnlp.41,D19-1165,0,\N,Missing
2020.findings-emnlp.416,P17-1123,0,0.0277364,"Missing"
2020.findings-emnlp.416,N10-1086,0,0.051,"existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems (Heilman and Smith, 2010; Lindberg et al., 2013; Yao et al., 2018), or be applied in intelligent virtual assistant systems (Shum et al., 2018; Pan et al., 2019). It can also combine with question answering (QA) models as dual tasks to boost QA systems with reasoning ability (Tang et al., 2017). Intuitively, there are two main additional challenges needed to be addressed for multi-hop QG. The first challenge is how to effectively identify scattered pieces of evidence that can connect the reasoning path of the answer and question (Chauhan et al., 2020). As the example shown 4636 Findings of the Association for Computat"
2020.findings-emnlp.416,P15-1086,0,0.0192602,"model and the human annotation show almost the same reasoning path. However, we observe that the question generated by MP-GSN model still tends to attend to the entities that are closer to the answer entities. Moreover, for the human annotation in Example I and Example III, the gold questions have a problem with fluency, which is harmful for the QG models, but interestingly, even with training using these labels, our model is still capable of generating relatively fluent outputs. 4 Related Work Question Generation Early single-hop QG use rule-based methods to transform sentences to questions (Labutov et al., 2015; Lindberg et al., 2013). Recently neural network based approaches adopt the sequence-to-sequence (Seq2Seq) based framework, with different types of encoders and decoders have been designed (Zhou et al., 2017; Nema et al., 2019; Zhao et al., 2018). Zhao et al. (2018) proposes to incorporate paragraph level content by using Gated Self Attention and Maxout pointer networks, while Nema et al. (2019) proposes a model which contains two decoders where the second decoder refines the question generated by the first decoder using reinforcement learning. There are different ways to attend answer inform"
2020.findings-emnlp.416,2020.acl-main.703,0,0.0810382,"Missing"
2020.findings-emnlp.416,W04-1013,0,0.0647394,"e is 10. We set the dropout probability for LSTM to 0.2 and 0.3 for GCN. The maximum number of epochs is set to 20. We set the maximum number of entities in each context to 80, and we use a two-layer GCN in our GCN-based answer encoder module. After training the model for 10 epochs, we further fine-tune the MulQG model with the help of BFS loss, where the λ in Eq.14 is set to 0.5. 3.4 Automatic Evaluation 3.4.1 Metrics We use the metrics in previous work on single-hop QG to evaluate the generation performance of our model, with n-gram similarity metrics BLEU1 (Papineni et al., 2002), ROUGE-L (LIN, 2004), and METEOR using the package released in Lavie and Denkowski (2009). We also quantify the QBLEU4 (Nema and Khapra, 2018a) and answerability score of our models, which was shown to correlate significantly better with human judgements (Nema and Khapra, 2018b). 3.4.2 Results and Analysis Table 2 shows the performance of various models on the HotpotQA test set. We report the both results of the experiments on our proposed model before and after fine-tuning with auxiliary BFS loss. As it’s shown in the table, our MulQG model perform much better than the two baselines methods, with 1 https://githu"
2020.findings-emnlp.416,W13-2114,0,0.164372,"ased QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems (Heilman and Smith, 2010; Lindberg et al., 2013; Yao et al., 2018), or be applied in intelligent virtual assistant systems (Shum et al., 2018; Pan et al., 2019). It can also combine with question answering (QA) models as dual tasks to boost QA systems with reasoning ability (Tang et al., 2017). Intuitively, there are two main additional challenges needed to be addressed for multi-hop QG. The first challenge is how to effectively identify scattered pieces of evidence that can connect the reasoning path of the answer and question (Chauhan et al., 2020). As the example shown 4636 Findings of the Association for Computational Linguistics: EMNL"
2020.findings-emnlp.416,D18-1429,0,0.0574183,"o 20. We set the maximum number of entities in each context to 80, and we use a two-layer GCN in our GCN-based answer encoder module. After training the model for 10 epochs, we further fine-tune the MulQG model with the help of BFS loss, where the λ in Eq.14 is set to 0.5. 3.4 Automatic Evaluation 3.4.1 Metrics We use the metrics in previous work on single-hop QG to evaluate the generation performance of our model, with n-gram similarity metrics BLEU1 (Papineni et al., 2002), ROUGE-L (LIN, 2004), and METEOR using the package released in Lavie and Denkowski (2009). We also quantify the QBLEU4 (Nema and Khapra, 2018a) and answerability score of our models, which was shown to correlate significantly better with human judgements (Nema and Khapra, 2018b). 3.4.2 Results and Analysis Table 2 shows the performance of various models on the HotpotQA test set. We report the both results of the experiments on our proposed model before and after fine-tuning with auxiliary BFS loss. As it’s shown in the table, our MulQG model perform much better than the two baselines methods, with 1 https://github.com/Maluuba/nlg-eval regard to all those measuring metrics, which indicates that the multi-hop procedure can significan"
2020.findings-emnlp.416,D19-1326,0,0.179787,"publicly available at https://github.com/HLTCHKUST/MulQG. 1 Table 1: An example of multi-hop QG in the HotpotQA (Yang et al., 2018) dataset. Given the answer is Location H, to ask where is T located, the model needs a bridging evidence to know that T is located in C, and C is located in H (T → C → H). This is done by multi-hop reasoning. Introduction Question Generation (QG) is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG (Du et al., 2017; Zhao et al., 2018; Scialom et al., 2019; Nema et al., 2019; Zhang and Bansal, 2019). Most of the existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an esse"
2020.findings-emnlp.416,P19-1203,0,0.0173871,"om the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems (Heilman and Smith, 2010; Lindberg et al., 2013; Yao et al., 2018), or be applied in intelligent virtual assistant systems (Shum et al., 2018; Pan et al., 2019). It can also combine with question answering (QA) models as dual tasks to boost QA systems with reasoning ability (Tang et al., 2017). Intuitively, there are two main additional challenges needed to be addressed for multi-hop QG. The first challenge is how to effectively identify scattered pieces of evidence that can connect the reasoning path of the answer and question (Chauhan et al., 2020). As the example shown 4636 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4636–4647 c November 16 - 20, 2020. 2020 Association for Computational Linguistics in Table 1, to g"
2020.findings-emnlp.416,P02-1040,0,0.10728,"ork (GCN) on an answer-aware dynamic entity graph, which is constructed from entity mentions in answer and input paragraphs, to aggregate the potential evidence related to the questions. Moreover, we use different attention mechanisms to imitate the reasoning procedures of human beings in multihop generation process, the details are explained in Section 2. We conduct the experiments on the multi-hop QA dataset HotpotQA (Yang et al., 2018) with our model and the baselines. The proposed model outperforms the baselines with a significant improvement on automatic evaluation results, such as BLEU (Papineni et al., 2002). The human evaluation results further validate that our proposed model is more likely to generate multi-hop questions with high quality in terms of Fluency, Answerability and Completeness scores. Our contributions are summarized as follows: • To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information in QG tasks. • We propose a new and effective framework for Multi-hop QG, to do context encoding in multiple hops(steps) with Graph Convolutional Network (GCN). • We show the effectiveness of our method on b"
2020.findings-emnlp.416,D14-1162,0,0.0839141,"ur MulQG framework. The encoding stage is achieved by a novel Multi-hop Encoder. At the decoding stage, we use maxout pointer decoder as proposed in Zhao et al. (2018). The overview of the framework is shown in Figure 1. 2.1 Multi-hop Encoder Our Multi-hop Encoder includes three modules: (1) Answer-aware context encoder (2) GCN-based entity-aware answer encoder (3) Gated encoder reasoning layer. The context and answer are split into word-level tokens and denoted as c = {c1 , c2 , ..., cn } and a = {a1 , a2 , ..., am }, respectively. Each word is represented by the pre-trained GloVe embedding (Pennington et al., 2014). Furthermore, for the words in context, we also append the answer tagging embeddings as described in Zhao et al. (2018). The context and answer embeddings are fed into two bidirectional LSTM-RNNs separately to obtain their initial contextual representations C0 ∈ Rd×n and A0 ∈ Rd×m , in which d is the hidden state dimension in LSTM. 4637 Multi-hop Encoder Decoder Encoder Reasoning Gate Answer-aware Context Encoder Attention Layer Entity-aware Answer Encoder residual Bi-Attn Maxout Pointer Generator Entity Graph Answer-aware Context Encoder LSTM Encoder LSTM Encoder LSTM Decoder Word Embeddings"
2020.findings-emnlp.416,2020.emnlp-main.468,0,0.0160022,"n the answer is Location H, to ask where is T located, the model needs a bridging evidence to know that T is located in C, and C is located in H (T → C → H). This is done by multi-hop reasoning. Introduction Question Generation (QG) is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG (Du et al., 2017; Zhao et al., 2018; Scialom et al., 2019; Nema et al., 2019; Zhang and Bansal, 2019). Most of the existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems (Heilman and Smith, 2010; Lindberg et al., 2013; Yao et al., 2018), or be applied in intelligent virtual assi"
2020.findings-emnlp.416,D16-1264,0,0.260949,"al., 2018) dataset. Given the answer is Location H, to ask where is T located, the model needs a bridging evidence to know that T is located in C, and C is located in H (T → C → H). This is done by multi-hop reasoning. Introduction Question Generation (QG) is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG (Du et al., 2017; Zhao et al., 2018; Scialom et al., 2019; Nema et al., 2019; Zhang and Bansal, 2019). Most of the existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems (Heilman and Smith, 2010; Lindberg et al., 2013; Yao et al., 2018), or be applied in inte"
2020.findings-emnlp.416,P19-1604,0,0.0108768,"aluation. The code is publicly available at https://github.com/HLTCHKUST/MulQG. 1 Table 1: An example of multi-hop QG in the HotpotQA (Yang et al., 2018) dataset. Given the answer is Location H, to ask where is T located, the model needs a bridging evidence to know that T is located in C, and C is located in H (T → C → H). This is done by multi-hop reasoning. Introduction Question Generation (QG) is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG (Du et al., 2017; Zhao et al., 2018; Scialom et al., 2019; Nema et al., 2019; Zhang and Bansal, 2019). Most of the existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It c"
2020.findings-emnlp.416,D18-1259,0,0.13892,"enges for multi-hop QG. First of all, it extends the Seq2Seq QG framework from sing-hop to multi-hop for context encoding. Additionally, it leverages a Graph Convolutional Network (GCN) on an answer-aware dynamic entity graph, which is constructed from entity mentions in answer and input paragraphs, to aggregate the potential evidence related to the questions. Moreover, we use different attention mechanisms to imitate the reasoning procedures of human beings in multihop generation process, the details are explained in Section 2. We conduct the experiments on the multi-hop QA dataset HotpotQA (Yang et al., 2018) with our model and the baselines. The proposed model outperforms the baselines with a significant improvement on automatic evaluation results, such as BLEU (Papineni et al., 2002). The human evaluation results further validate that our proposed model is more likely to generate multi-hop questions with high quality in terms of Fluency, Answerability and Completeness scores. Our contributions are summarized as follows: • To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information in QG tasks. • We propose a"
2020.findings-emnlp.416,D19-1253,0,0.119845,"at https://github.com/HLTCHKUST/MulQG. 1 Table 1: An example of multi-hop QG in the HotpotQA (Yang et al., 2018) dataset. Given the answer is Location H, to ask where is T located, the model needs a bridging evidence to know that T is located in C, and C is located in H (T → C → H). This is done by multi-hop reasoning. Introduction Question Generation (QG) is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG (Du et al., 2017; Zhao et al., 2018; Scialom et al., 2019; Nema et al., 2019; Zhang and Bansal, 2019). Most of the existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in educat"
2020.findings-emnlp.416,D18-1424,0,0.112077,"in the multihop evaluation. The code is publicly available at https://github.com/HLTCHKUST/MulQG. 1 Table 1: An example of multi-hop QG in the HotpotQA (Yang et al., 2018) dataset. Given the answer is Location H, to ask where is T located, the model needs a bridging evidence to know that T is located in C, and C is located in H (T → C → H). This is done by multi-hop reasoning. Introduction Question Generation (QG) is a task to automatically generate a question from a given context and, optionally, an answer. Recently, we have observed an increasing interest in text-based QG (Du et al., 2017; Zhao et al., 2018; Scialom et al., 2019; Nema et al., 2019; Zhang and Bansal, 2019). Most of the existing works on text-based QG focus on generating SQuAD-style (Rajpurkar et al., 2016; Puri et al., 2020) questions, which are generated from the sentence containing the answer or nearby sentences in the same paragraph, via single-hop reasoning (Zhou et al., 2017; Zhao et al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-co"
2020.findings-emnlp.416,P18-1150,0,0.051358,"Missing"
2020.findings-emnlp.416,D17-1090,0,0.0178388,"al., 2018). Little effort has been put in multi-hop QG, which is a more challenging task. Multi-hop QG requires aggregating several scattered evidence spans from multiple paragraphs, and reasoning over them to generate answer-related, factual-coherent questions. It can serve as an essential component in education systems (Heilman and Smith, 2010; Lindberg et al., 2013; Yao et al., 2018), or be applied in intelligent virtual assistant systems (Shum et al., 2018; Pan et al., 2019). It can also combine with question answering (QA) models as dual tasks to boost QA systems with reasoning ability (Tang et al., 2017). Intuitively, there are two main additional challenges needed to be addressed for multi-hop QG. The first challenge is how to effectively identify scattered pieces of evidence that can connect the reasoning path of the answer and question (Chauhan et al., 2020). As the example shown 4636 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4636–4647 c November 16 - 20, 2020. 2020 Association for Computational Linguistics in Table 1, to generate a question asking about “Marine Air Control Group 28” given only the answer “Havelock, North Carolina”, we need the bridging e"
2020.findings-emnlp.416,Q18-1021,0,0.0254447,"e they still focus on the single-hop QG. Multi-hop QA Popular Graph Nueral Network (GNN) frameworks, such as graph convolutional networks (Kipf and Welling, 2016), graph attention network (Veliˇckovi´c et al., 2017), and graph recurrent network (Song et al., 2018) have been explored and showed promising results on multi-hop QA task that requiring reasoning. Xiao et al. (2019) proposes a dynamic fused graph network to work on multi-hop QA on the HotpotQA dataset. De Cao et al. (2018) proposes an entity-GCN method to reason over across multiple documents for multi-hop QA on the WIKIHOP dataset (Welbl et al., 2018). 5 Conclusion Multi-hop QG task is more challenging and worthy of exploration compared to conventional singlehop QG. To address the additional challenges in multi-hop QG, we propose MulQG, which does multi-hop context encoding with Graph Convolutional Network and encoding fusion via a Gated Reasoning module. To the best of our knowledge, 4643 we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentencelevel information. The model performance on HotpotQA dataset demonstrates its effectiveness on aggregating scattered pieces of evidence across the paragra"
2020.findings-emnlp.416,P19-1617,0,0.206388,"M is a binary matrix where Mi,j = 1 if the i-th token in the context is within the span of the j-th entity. Each entity’s encoding will be calculated via a mean-max pooling applied over it’s corresponding context token encoding span. E0 = {e1 , e2 , ..., eg } ∈ R2d×g , where g is the number of entities, and 2d is the dimension since we directly concatenate the meanpooling and max-pooling encoding. Answer-aware GCN First we calculate an answer-aware sub-graph, where irrelevant entities are masked out, only those entity nodes related to answer are allowed to disseminate information. Similar to Xiao et al. (2019), a soft mask M = [m1 , m2 , ..., mg ] is calculated via Eq. 7, where each mi indicate the relatedness of the entity i to the answer, and then apply M on the original graph entities to obtain answer-aware dynamic sub entities graph Esub via Eq. 8. M = σ(aT0 · V · E0 ) ∈ R1×g Esub = M · E0 (7) A1 = BiAttention(A0 , EM ) (9) The computation from Eq. 9 can be repeated for multiple times to obtain multi-hop entity representation EM . Multi-hop Answer Encoding we use biattention mechanism (Seo et al., 2016) regarding (10) 2.1.3 Encoder Reasoning Gate We apply a gated feature fusion module on the an"
2020.lrec-1.73,P15-1034,0,0.0888508,"y masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attribute extraction task. We briefly introduce the baselines: • Seq2Seq is the most common baseline for sequence generation. We use GRUs as a base model to encode a sequence of words and decode a sequence that concatenates (subject, predicate, object) by semicolons. • PG is one of the best gen"
2020.lrec-1.73,P11-1062,0,0.0282575,"et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would"
2020.lrec-1.73,P17-1152,0,0.0256402,"Missing"
2020.lrec-1.73,D13-1114,0,0.0202194,"ution. Lastly, the conversations in the Persona-Chat dataset are not collected naturally, with most of the users tending to ignore what the other said and just talking about themselves. Therefore, it is hard to evaluate whether “understanding your partner” helps agents speak properly. Also, since there is no publicly available data with the same user continually talking to a system, it is hard to evaluate the lifelong setting. 7. Related Work User Attributes Inference Most previous work has treated user attribute inference from social media as a classification task, such as gender prediction (Ciot et al., 2013), age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007"
2020.lrec-1.73,C16-1279,0,0.013158,"Therefore, it is hard to evaluate whether “understanding your partner” helps agents speak properly. Also, since there is no publicly available data with the same user continually talking to a system, it is hard to evaluate the lifelong setting. 7. Related Work User Attributes Inference Most previous work has treated user attribute inference from social media as a classification task, such as gender prediction (Ciot et al., 2013), age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al"
2020.lrec-1.73,Q17-1024,0,0.0174945,"Missing"
2020.lrec-1.73,P14-1016,0,0.051709,"ue systems) of such extracted user attributes, and point out current limitations to cast light on future work. Keywords: Dialogue Systems, Personalization, Information Extraction, Natural Language Processing 1. Introduction User attributes are explicit representations of a person’s identity and characteristics in a structured format. They provide a rich repository of personal information for better user understanding in many applications. High-quality user attributes are, however, hard to obtain since the information in social networks such as Facebook and Twitter is often sparsely populated (Li et al., 2014). Therefore, exploiting unstructured data sources to obtain structured user attributes is a challenging research direction. Meanwhile, there is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either g"
2020.lrec-1.73,P18-1136,1,0.841085,"f words in the utterance and dhdd is the hidden size of the GRU. The last hidden state henc is represented l as the final encoded vector, which will be used to query the predicate classifier and initialize the entity generator. 2 PyTorch version in github.com/huggingface/ pytorch-pretrained-BERT Predicate Classifier We use a multi-hop (K = 3 hops) end-to-end memory network (MN) (Sukhbaatar et al., 2015) as our predicate classifier because we believe its reasoning ability can benefit predicates prediction, as shown in question answering and dialogue tasks (Bordes et al., 2016; Wu et al., 2018; Madotto et al., 2018; Wu et al., 2019b). We assign the memory in the MN as all the predicate words R = {r1 , . . . , rJ }, where J is the total number of possible predicates. The predicate classifier is queried by the encoded vector henc l , and the memory attention at each hop k is computed as αk = Sof tmax(C k (P )q k ) ∈ RJ , (1) where C k and q k are the embedding matrix and query vector at hop k, respectively. Here, αk is a soft memory selector that decides the memory relevance with respect to the query vector q k . The model reads out the memory ok as X ok = αik C k+1 (ri ) ∈ Rdhdd . (2) i Then the query ve"
2020.lrec-1.73,P19-1542,1,0.82919,"der systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches for recommender systems. For dialogue applications, (Lucas et al., 2009) and (Joshi et al., 2017) focus on letting the agent be aware of the human pre-defined profile and so adjust the dialogue accordingly. (Zemlyanskiy and Sha, 2018) define a mutual information discovery score to re-rank system generating responses. (Madotto et al., 2019) uses meta-learning to fast adapt to unseen persona scenarios. 8. Conclusion We utilize conversational data to extract user attributes for better user understanding. Due to lacking a labeled dataset, we apply distant supervision with a natural language inference model to train our proposed two-stage attribute extractor. Our model surpasses several retrieval and generation baselines on human evaluation, and is different from existing open information extraction approaches. In the end, we discuss potential downstream applications and point out current limitations to provide suggestions for futur"
2020.lrec-1.73,D12-1048,0,0.0185552,"reot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas."
2020.lrec-1.73,D18-1298,0,0.106348,"e is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either generating good responses (Serban et al., 2015), or incorporating “system attributes” to generate consistent responses (Zhang et al., 2018; Mazare et al., 2018). However, the whole dialogue history of the same person is ignored, implying that these systems are not gradually getting to know their users by extracting user information through conversations. In this paper, we demonstrate that it is feasible to automatically extract user attributes from dialogues. Given a user utterance, our goal is to predict user information that can be represented as a (Subject, Predicate, Object) triplet format, which is available for any downstream application. For example, in Table 1, (I, live in, Florida) is extracted from the second user utterance. Meanwhile, not"
2020.lrec-1.73,D16-1147,0,0.0139355,"to weight two losses is set to be 0.5. A greedy search decoding strategy is used for our entity generator since the generated phrases are usually short. In addition, to increase model generalization and simulate an out-of-vocabulary setting, a word dropout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attr"
2020.lrec-1.73,P02-1040,0,0.106403,"Missing"
2020.lrec-1.73,D14-1162,0,0.0918919,"Missing"
2020.lrec-1.73,P15-1169,0,0.0659842,"Missing"
2020.lrec-1.73,P19-1363,0,0.0211327,"Missing"
2020.lrec-1.73,P17-1099,0,0.0432375,"d character embeddings (100) (Hashimoto et al., 2016). The λ to weight two losses is set to be 0.5. A greedy search decoding strategy is used for our entity generator since the generated phrases are usually short. In addition, to increase model generalization and simulate an out-of-vocabulary setting, a word dropout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models"
2020.lrec-1.73,N18-1081,0,0.0831064,"opout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attribute extraction task. We briefly introduce the baselines: • Seq2Seq is the most common baseline for sequence generation. We use GRUs as a base model to encode a sequence of words and decode a sequence that concatenates (subject, predicate, object) by se"
2020.lrec-1.73,D18-1157,0,0.011451,"tributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches"
2020.lrec-1.73,P16-1123,0,0.0464071,"Missing"
2020.lrec-1.73,W18-5713,0,0.0202127,"ly introduce these datasets and discuss some of their limitations. Persona-Chat This is a multi-turn chit-chat corpus with annotation of the participants’ personal profiles (e.g., preferences about food, movies). It is collected by asking two crowd-workers to talk to each other freely but conditioned on their artificial personas, which are established by four to six persona sentences. An example from the dataset is provided in Table 2. In total there are 1155 personas with over 5,000 persona sentences, and 162,064 utterances over 10,907 dialogues. Most of the related works using this dataset (Weston et al., 2018; Semih Yavuz, 2018; Wolf et al., 2019; Dinan et al., 2019) focus on adapting systems to a given persona, i.e., learning to generate responses that are consistent with the persona. 1 The code is released at https://github.com/ jasonwu0731/GettingToKnowYou Although the dataset contains pre-defined personas and the corresponding conversations, it cannot be applied directly to the attribute extraction task for the following two reasons: 1) The mapping between utterances and the persona is missing. Which persona sentence is related to which utterance remains unknown. 2) All the personas are writte"
2020.lrec-1.73,P10-1013,0,0.0605996,"age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the pr"
2020.lrec-1.73,P19-1078,1,0.921683,"ce and dhdd is the hidden size of the GRU. The last hidden state henc is represented l as the final encoded vector, which will be used to query the predicate classifier and initialize the entity generator. 2 PyTorch version in github.com/huggingface/ pytorch-pretrained-BERT Predicate Classifier We use a multi-hop (K = 3 hops) end-to-end memory network (MN) (Sukhbaatar et al., 2015) as our predicate classifier because we believe its reasoning ability can benefit predicates prediction, as shown in question answering and dialogue tasks (Bordes et al., 2016; Wu et al., 2018; Madotto et al., 2018; Wu et al., 2019b). We assign the memory in the MN as all the predicate words R = {r1 , . . . , rJ }, where J is the total number of possible predicates. The predicate classifier is queried by the encoded vector henc l , and the memory attention at each hop k is computed as αk = Sof tmax(C k (P )q k ) ∈ RJ , (1) where C k and q k are the embedding matrix and query vector at hop k, respectively. Here, αk is a soft memory selector that decides the memory relevance with respect to the query vector q k . The model reads out the memory ok as X ok = αik C k+1 (ri ) ∈ Rdhdd . (2) i Then the query vector is updated f"
2020.lrec-1.73,D15-1206,0,0.0120966,"Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burk"
2020.lrec-1.73,K18-1053,0,0.0150253,"alogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches for recommender systems. For dialogue applications, (Lucas et al., 2009) and (Joshi et al., 2017) focus on letting the agent be aware of the human pre-defined profile and so adjust the dialogue accordingly. (Zemlyanskiy and Sha, 2018) define a mutual information discovery score to re-rank system generating responses. (Madotto et al., 2019) uses meta-learning to fast adapt to unseen persona scenarios. 8. Conclusion We utilize conversational data to extract user attributes for better user understanding. Due to lacking a labeled dataset, we apply distant supervision with a natural language inference model to train our proposed two-stage attribute extractor. Our model surpasses several retrieval and generation baselines on human evaluation, and is different from existing open information extraction approaches. In the end, we d"
2020.lrec-1.73,C14-1220,0,0.08916,"Missing"
2020.lrec-1.73,P18-1205,0,0.166608,"ion. Meanwhile, there is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either generating good responses (Serban et al., 2015), or incorporating “system attributes” to generate consistent responses (Zhang et al., 2018; Mazare et al., 2018). However, the whole dialogue history of the same person is ignored, implying that these systems are not gradually getting to know their users by extracting user information through conversations. In this paper, we demonstrate that it is feasible to automatically extract user attributes from dialogues. Given a user utterance, our goal is to predict user information that can be represented as a (Subject, Predicate, Object) triplet format, which is available for any downstream application. For example, in Table 1, (I, live in, Florida) is extracted from the second user utte"
2020.nlpcovid19-2.14,N19-1423,0,0.0264676,"Missing"
2020.nlpcovid19-2.14,N19-1246,0,0.042691,"Missing"
2020.nlpcovid19-2.14,P17-1147,0,0.0658621,"Missing"
2020.nlpcovid19-2.14,Q19-1026,0,0.0362958,"Missing"
2020.nlpcovid19-2.14,2020.acl-main.703,0,0.0342798,"Missing"
2020.nlpcovid19-2.14,P19-1613,0,0.0123534,"with the following two sub-modules. 3.1.1 Query Paraphrasing As shorter sentences are generally more easily processed by NLP systems (Narayan et al., 2017), the objective of this sub-module is to break down a user query and rephrase complex question sentences into several shorter and simpler queries that convey the same meaning. Its effectiveness has been proved in our CORD-19 Kaggle tasks, in dealing with the questions that are too long and complicated, and we show examples in Appendix B. Currently, this module has been excluded from our online system, since the automatic solutions we tried (Min et al., 2019; Perez et al., 2020) did not give satisfactory performance improvement for our system. More automatic methods will be explored in the future. 3.1.2 Search Engine We use Anserin (Yang et al., 2018a) to create the search engine for retrieving a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine Lucene8 which is widely used to 6 https://wellai.health/ http://tmcovid.com/ 8 https://lucene.apache.org/ 7 Relevant Snippet Selector The Relevant Snippet Selector outputs a list of the most relevant answer snippets from the re"
2020.nlpcovid19-2.14,D18-1206,0,0.105291,"the corresponding paragraphs and displayed via highlighting: scorere−rank = smatch + αsconf . 3.3 (3) Query-focused Multi-document Summarization To efficiently present pertinent COVID-19 information to the user, we propose a query-focused multi-document summarizer to generate abstractive and extractive summaries related to COVID-19 questions. 3.3.1 Abstractive Summarization BART Fine-tuning Our abstractive summerization model is based on BART (Lewis et al., 2019), which obtained state-of-the-art results on the summarization tasks on the CNN/DailyMail datasets (Hermann et al., 2015) and XSUM (Narayan et al., 2018). We use the BART model fine-tuned on the CNN/DailyMail dataset as the base model since we do not have other COVID-19 related summarization data. Incorporating Answer Relevance In order to generate query-focused summaries, we propose to incorporate answer relevance in the BART-based summarization process in two aspects. First, instead of using the paragraphs list passed by the Document Retriever, we use the top k paragraphs {para1 , para2 , .., parak } passed by the QA module as input to the Multi-document Summarizer, which are re-ranked according to their answer relevance to the query, as sho"
2020.nlpcovid19-2.14,D17-1064,0,0.0135193,"he title and abstract. For each query, the module can return n top paragraphs matching the query. 3.2 3 Methodology Figure 2 illustrates the architecture of the CAiRECOVID system, which consists of three major modules: 1) Document Retriever, 2) Relevant Snippet Selector, and 3) Query-focused Multi-Document Summarizer. 3.1 Document Retrieval To select the most relevant document, i.e. article or paragraph, given a user query, we first apply the Document Retriever with the following two sub-modules. 3.1.1 Query Paraphrasing As shorter sentences are generally more easily processed by NLP systems (Narayan et al., 2017), the objective of this sub-module is to break down a user query and rephrase complex question sentences into several shorter and simpler queries that convey the same meaning. Its effectiveness has been proved in our CORD-19 Kaggle tasks, in dealing with the questions that are too long and complicated, and we show examples in Appendix B. Currently, this module has been excluded from our online system, since the automatic solutions we tried (Min et al., 2019; Perez et al., 2020) did not give satisfactory performance improvement for our system. More automatic methods will be explored in the futu"
2020.nlpcovid19-2.14,P17-1098,0,0.33035,"ralization and domainexpertise capability, we use an ensemble of two QA models in the QA module as the evidence selector. We evaluate the performance on the recently released CovidQA (Tang et al., 2020) dataset, and the results indicate that our QA module even outperforms the T5 model (Raffel et al., 2019) on the recall metric, while for keyword questions, it also marginally outperforms T5 on the precision fraction. The performance of the summerizer module is evaluated on two existing query-focused summarization (QFS) datasets, the DUC datasets (Dang, 2005; Hoa, 2006) and Debatepidia dataset (Nema et al., 2017), since there is no QFS dataset for COVID-19. The DUC datasets are the most widely used for the QFS task, while Debatepedia is the first large-scale abstractive QFS dataset. Previous works on the QFS task incorporate query relevance, either via a query-document relevance score (Baumel et al., 2018) or query attention model (Nema et al., 2017), into a seq2seq model, or concatenate query to documents into a pre-trained transformer architecture (Laskar et al., 2020; Savery et al., 2020). However, none have taken answer relevance into consideration. By incorporating answer relevance from the QA mo"
2020.nlpcovid19-2.14,2020.emnlp-main.713,0,0.0125663,"two sub-modules. 3.1.1 Query Paraphrasing As shorter sentences are generally more easily processed by NLP systems (Narayan et al., 2017), the objective of this sub-module is to break down a user query and rephrase complex question sentences into several shorter and simpler queries that convey the same meaning. Its effectiveness has been proved in our CORD-19 Kaggle tasks, in dealing with the questions that are too long and complicated, and we show examples in Appendix B. Currently, this module has been excluded from our online system, since the automatic solutions we tried (Min et al., 2019; Perez et al., 2020) did not give satisfactory performance improvement for our system. More automatic methods will be explored in the future. 3.1.2 Search Engine We use Anserin (Yang et al., 2018a) to create the search engine for retrieving a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine Lucene8 which is widely used to 6 https://wellai.health/ http://tmcovid.com/ 8 https://lucene.apache.org/ 7 Relevant Snippet Selector The Relevant Snippet Selector outputs a list of the most relevant answer snippets from the retrieved documents whi"
2020.nlpcovid19-2.14,D16-1264,0,0.15103,"Missing"
2020.nlpcovid19-2.14,D19-5827,1,0.92493,"nt Retriever module. It then highlights the answers or evidence (text spans) for the query, given the relevant paragraphs, by a Snippet Selector module via question answering (QA) models. Furthermore, to efficiently present COVID-19 questionrelated information to the user, we propose a queryfocused Multi-Document Summarizer to generate abstractive and extractive summaries related to the question, from multiple retrieved answer-related paragraph snippets. We leverage the power of the generalization ability of pre-trained language models (Lewis et al., 2019; Yang et al., 2019; Lee et al., 2020; Su et al., 2019) by fine-tuning them for QA and summarization, and propose our own adaptation methods for the COVID-19 task. Figure 1: The user interface of our CAiRE-COVID website. The effectiveness of our system has been proved by winning one of the tasks in Round 1 of the Kaggle CORD-19 Challenge,1 in which hundreds of submissions were evaluated with the help of medical researchers. We further conduct a series of experiments to quantitatively show the competency of each module. To enhance both generalization and domainexpertise capability, we use an ensemble of two QA models in the QA module as the evidenc"
2020.nlpcovid19-2.14,W17-2623,0,0.0341668,"Missing"
2020.nlpcovid19-2.14,D18-1259,0,0.0217721,"break down a user query and rephrase complex question sentences into several shorter and simpler queries that convey the same meaning. Its effectiveness has been proved in our CORD-19 Kaggle tasks, in dealing with the questions that are too long and complicated, and we show examples in Appendix B. Currently, this module has been excluded from our online system, since the automatic solutions we tried (Min et al., 2019; Perez et al., 2020) did not give satisfactory performance improvement for our system. More automatic methods will be explored in the future. 3.1.2 Search Engine We use Anserin (Yang et al., 2018a) to create the search engine for retrieving a preliminary candidate set of documents. Anserini is an information retrieval module wrapped around the open source search engine Lucene8 which is widely used to 6 https://wellai.health/ http://tmcovid.com/ 8 https://lucene.apache.org/ 7 Relevant Snippet Selector The Relevant Snippet Selector outputs a list of the most relevant answer snippets from the retrieved documents while highlighting the relevant keywords. To effectively find the snippets of the paragraphs relevant to a query, we build a neural QA system as an evidence selector given the qu"
2020.nlpcovid19-2.14,D14-1159,0,\N,Missing
2020.repl4nlp-1.1,W03-0419,0,\N,Missing
2020.repl4nlp-1.1,N16-1030,0,\N,Missing
2020.repl4nlp-1.1,Q17-1010,0,\N,Missing
2020.repl4nlp-1.1,N18-1001,0,\N,Missing
2020.repl4nlp-1.1,D18-1226,0,\N,Missing
2020.repl4nlp-1.1,S19-2184,1,\N,Missing
2020.repl4nlp-1.1,P16-1101,0,\N,Missing
2020.repl4nlp-1.1,N19-1423,0,\N,Missing
2020.repl4nlp-1.1,Q16-1026,0,\N,Missing
2020.repl4nlp-1.1,W19-5327,1,\N,Missing
2020.repl4nlp-1.1,D19-1129,1,\N,Missing
2020.repl4nlp-1.1,P07-1033,0,\N,Missing
2020.repl4nlp-1.1,L18-1708,0,\N,Missing
2020.sdp-1.35,P19-1204,0,0.0320739,"Missing"
2020.sdp-1.35,2020.acl-main.703,0,0.0580582,"Missing"
2020.sdp-1.35,N03-1020,0,0.502461,"Missing"
2020.sdp-1.35,P17-1171,0,0.0212871,"ntations. After a feedforward neural network, these sentence representations produce a binary distribution about whether they belong to the extractive summary. As for the abstractive summary, it is generated by the autoregressive decoder. The overall loss L is calculated by L = we Le + La . Here Le and La refer to the Cross-Entropy loss of extractive and abstractive summary respectively. 4.3 Data Augmentation Data augmentation has been an effective technique to create new training instances when the training data is not enough, as demonstrated in computer vision as well as for many NLP tasks (Chen et al., 2017; Yang et al., 2019; Yuan et al., 2017). Existing data augmentation approaches in NLP tasks can be categorized into retrieval-based methods (Chen et al., 2017; Yang et al., 2019) and generation-based methods (Yuan et al., 2017; Buck et al., 2017). However, none of these suits our situation, since external sources or auxiliary training data are still required. So we adopted a similar method from (Nema et al., 2017). A pre-defined vocabulary of 24,822 words was used where each word had been associated with a synonym. Then for each training instance, certain ratios (in our case, 1/9) in each docu"
2020.sdp-1.35,N18-2097,0,0.0228042,"neural network-based approaches have reached remarkable performance for news articles summarization (See et al., 2017; Liu and Lapata, 2019; Zhang et al., 2019). Comparing with news articles, scientific papers are typically longer and contain more complex concepts and technical terms. Scientific Paper Summarization Existing approaches for scientific paper summarization include extractive models that perform sentence selection (Qazvinian et al., 2013; Cohan and Goharian, 2017, 2018) and hybrid models that select the salient text first and then summarize it (Subramanian et al., 2019). Besides, Cohan et al. (2018) built the first model for abstractive summarization of single, longer-form documents (e.g., research papers). In order to train neural models for this task, several datasets have been introduced. The arXiv and PubMed datasets (Cohan et al., 2018) were created using open access articles from the corresponding popular repositories. Yasunaga et al. (2019) developed and released the first large-scale manuallyannotated corpus for scientific papers (on computational linguistics). 2 https://ornlcda.github.io/SDProc/index.html https://github.com/TysonYu/Laysumm 303 Proceedings of the First Workshop o"
2020.sdp-1.35,D19-1387,0,0.0236213,"f the sentences in lay summaries have corresponding sentences in original papers. Inspiring by this observation, we think that making binary sentence labels for extractive summarization and utilize them as extra supervision signals can help model generate better summaries. Therefore, we conduct BART (Lewis 1 Related Work Text Summarization Text summarization aims to produce a condensed representation of input text that captures the core meaning of the original text. Recently, neural network-based approaches have reached remarkable performance for news articles summarization (See et al., 2017; Liu and Lapata, 2019; Zhang et al., 2019). Comparing with news articles, scientific papers are typically longer and contain more complex concepts and technical terms. Scientific Paper Summarization Existing approaches for scientific paper summarization include extractive models that perform sentence selection (Qazvinian et al., 2013; Cohan and Goharian, 2017, 2018) and hybrid models that select the salient text first and then summarize it (Subramanian et al., 2019). Besides, Cohan et al. (2018) built the first model for abstractive summarization of single, longer-form documents (e.g., research papers). In order t"
2020.sdp-1.35,2021.ccl-1.108,0,0.105629,"Missing"
2020.sdp-1.35,P17-1098,0,0.137903,"a augmentation has been an effective technique to create new training instances when the training data is not enough, as demonstrated in computer vision as well as for many NLP tasks (Chen et al., 2017; Yang et al., 2019; Yuan et al., 2017). Existing data augmentation approaches in NLP tasks can be categorized into retrieval-based methods (Chen et al., 2017; Yang et al., 2019) and generation-based methods (Yuan et al., 2017; Buck et al., 2017). However, none of these suits our situation, since external sources or auxiliary training data are still required. So we adopted a similar method from (Nema et al., 2017). A pre-defined vocabulary of 24,822 words was used where each word had been associated with a synonym. Then for each training instance, certain ratios (in our case, 1/9) in each document were randomly selected (except stop words and numerical values) and then replaced with their synonyms found in the vocabulary. If a selected word was not found in the vocabulary, it was added there with the most similar word found based on cosine similarity in the GloVe (Pennington et al., 2014) vocabulary. For each training instance, this process is repeated 9 times to create 9 new documents. But the same su"
2020.sdp-1.35,D14-1162,0,0.0839607,"our situation, since external sources or auxiliary training data are still required. So we adopted a similar method from (Nema et al., 2017). A pre-defined vocabulary of 24,822 words was used where each word had been associated with a synonym. Then for each training instance, certain ratios (in our case, 1/9) in each document were randomly selected (except stop words and numerical values) and then replaced with their synonyms found in the vocabulary. If a selected word was not found in the vocabulary, it was added there with the most similar word found based on cosine similarity in the GloVe (Pennington et al., 2014) vocabulary. For each training instance, this process is repeated 9 times to create 9 new documents. But the same summary of the original instance was used in the newly generated instances. 4.4 Two-Stage Fine-tuning To make use of the ScisummNet dataset, we conduct a two-stage fine-tuning method. In the first stage, we fine-tune the pre-trained BART model on the ScisummNet dataset. We use the Abstract and annotators selected citation sentences as the input and the gold summary as the output. The model is fine-tuned with 20000 iterations before saved. As for the second stage, we use the same se"
2020.sdp-1.35,P17-1099,0,0.049199,"bserve that lots of the sentences in lay summaries have corresponding sentences in original papers. Inspiring by this observation, we think that making binary sentence labels for extractive summarization and utilize them as extra supervision signals can help model generate better summaries. Therefore, we conduct BART (Lewis 1 Related Work Text Summarization Text summarization aims to produce a condensed representation of input text that captures the core meaning of the original text. Recently, neural network-based approaches have reached remarkable performance for news articles summarization (See et al., 2017; Liu and Lapata, 2019; Zhang et al., 2019). Comparing with news articles, scientific papers are typically longer and contain more complex concepts and technical terms. Scientific Paper Summarization Existing approaches for scientific paper summarization include extractive models that perform sentence selection (Qazvinian et al., 2013; Cohan and Goharian, 2017, 2018) and hybrid models that select the salient text first and then summarize it (Subramanian et al., 2019). Besides, Cohan et al. (2018) built the first model for abstractive summarization of single, longer-form documents (e.g., resear"
2020.sdp-1.35,2020.emnlp-main.748,0,0.0480707,"Missing"
2020.sdp-1.35,W17-2603,0,0.0561731,"Missing"
2020.semeval-1.272,D19-1371,0,0.019801,"s. Recently, (Zampieri et al., 2019a) introduce an offensive language identification dataset, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic recipe of finetuning such pre-trained models on a certain domain (Azzouza et al., 2019; Lee et al., 2019; Beltagy et al., 2019) or on downstream tasks (Howard and Ruder, 2018; Liu et al., 2019b; Su et al., 2019). 3 Datasets In this project, two datasets are involved, which are the dataset of OffensEval-2019 and OffensEval2020 respectively. In this section, we introduce the details of them and discuss our data pre-processing methods. Table 1 shows the types of labels and how they overlap. Task A Task B Task C OFF TIN IND GRP NOT UNT NULL NULL Table 1: Labels of three subtasks. 3.1 Offensive Language Identification Dataset (OLID) The OLID (Zampieri et al., 2019b) is a hierarchical dataset to identify the type and the ta"
2020.semeval-1.272,D19-1227,0,0.0305275,"model is trained to recover them at the output. In NSP, two sentences are fed into the model and it is trained to predict whether the second sentence is the actual next sentence of the first one. As shown in (Devlin et al., 2018), by fine-tuning, BERT achieves superior results on many NLP downstream tasks. 4.3 Multi-task Offense Detection Model In recent years, multi-task learning (MTL) technique is used in many machine learning fields to improve performance and generalization ability of a model (Kang et al., 2011; Long and Wang, 2015; Kokkinos, 2016; G¨uler et al., 2018; Liu and Zhao, 2018; Dankers et al., 2019). Generally, MTL has three advantages. Firstly, with multiple supervision signals, it can improve the quality of representation learning, because a good representation should have better performance on more tasks. Secondly, MTL can help the model generalize better because multiple tasks introduce more noises and prevent the model from over-fitting. Thirdly, sometimes it is hard to learn features by one task but easier to learn by another task. MTL provides complementary supervisions to one task and makes it possible to eavesdrop other tasks and get more information. For this task, MTL is a ver"
2020.semeval-1.272,W17-3013,0,0.0745047,"Missing"
2020.semeval-1.272,P18-1031,0,0.0234202,"e an offensive language identification dataset, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic recipe of finetuning such pre-trained models on a certain domain (Azzouza et al., 2019; Lee et al., 2019; Beltagy et al., 2019) or on downstream tasks (Howard and Ruder, 2018; Liu et al., 2019b; Su et al., 2019). 3 Datasets In this project, two datasets are involved, which are the dataset of OffensEval-2019 and OffensEval2020 respectively. In this section, we introduce the details of them and discuss our data pre-processing methods. Table 1 shows the types of labels and how they overlap. Task A Task B Task C OFF TIN IND GRP NOT UNT NULL NULL Table 1: Labels of three subtasks. 3.1 Offensive Language Identification Dataset (OLID) The OLID (Zampieri et al., 2019b) is a hierarchical dataset to identify the type and the target of offensive texts in social media. The da"
2020.semeval-1.272,W18-4401,0,0.104607,"Missing"
2020.semeval-1.272,S19-2184,1,0.84492,"detection in tweets. Recently, (Zampieri et al., 2019a) introduce an offensive language identification dataset, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic recipe of finetuning such pre-trained models on a certain domain (Azzouza et al., 2019; Lee et al., 2019; Beltagy et al., 2019) or on downstream tasks (Howard and Ruder, 2018; Liu et al., 2019b; Su et al., 2019). 3 Datasets In this project, two datasets are involved, which are the dataset of OffensEval-2019 and OffensEval2020 respectively. In this section, we introduce the details of them and discuss our data pre-processing methods. Table 1 shows the types of labels and how they overlap. Task A Task B Task C OFF TIN IND GRP NOT UNT NULL NULL Table 1: Labels of three subtasks. 3.1 Offensive Language Identification Dataset (OLID) The OLID (Zampieri et al., 2019b) is a hierarchical dataset to ident"
2020.semeval-1.272,S19-2011,0,0.391718,"identification dataset, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic recipe of finetuning such pre-trained models on a certain domain (Azzouza et al., 2019; Lee et al., 2019; Beltagy et al., 2019) or on downstream tasks (Howard and Ruder, 2018; Liu et al., 2019b; Su et al., 2019). 3 Datasets In this project, two datasets are involved, which are the dataset of OffensEval-2019 and OffensEval2020 respectively. In this section, we introduce the details of them and discuss our data pre-processing methods. Table 1 shows the types of labels and how they overlap. Task A Task B Task C OFF TIN IND GRP NOT UNT NULL NULL Table 1: Labels of three subtasks. 3.1 Offensive Language Identification Dataset (OLID) The OLID (Zampieri et al., 2019b) is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected"
2020.semeval-1.272,malmasi-zampieri-2017-detecting,0,0.0417355,"Missing"
2020.semeval-1.272,W17-3008,0,0.0446731,"Missing"
2020.semeval-1.272,W17-3006,1,0.804901,"nsed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. * These two authors contributed equally. License details: http:// 2060 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2060–2066 Barcelona, Spain (Online), December 12, 2020. and Fung, 2017). (Chen et al., 2012) applied concepts from NLP to exploit the lexical syntactic feature of sentences for offensive language detection. (Huang et al., 2014) integrated the textual features with social network features, which significantly improved cyberbullying detection. (Park and Fung, 2017) and (Gamb¨ack and Sikdar, 2017) used convolutional neural network in the hate-speech detection in tweets. Recently, (Zampieri et al., 2019a) introduce an offensive language identification dataset, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic re"
2020.semeval-1.272,N18-1202,0,0.0321564,"., 2014) integrated the textual features with social network features, which significantly improved cyberbullying detection. (Park and Fung, 2017) and (Gamb¨ack and Sikdar, 2017) used convolutional neural network in the hate-speech detection in tweets. Recently, (Zampieri et al., 2019a) introduce an offensive language identification dataset, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic recipe of finetuning such pre-trained models on a certain domain (Azzouza et al., 2019; Lee et al., 2019; Beltagy et al., 2019) or on downstream tasks (Howard and Ruder, 2018; Liu et al., 2019b; Su et al., 2019). 3 Datasets In this project, two datasets are involved, which are the dataset of OffensEval-2019 and OffensEval2020 respectively. In this section, we introduce the details of them and discuss our data pre-processing methods. Table 1 shows the types of labels and h"
2020.semeval-1.272,D19-5827,1,0.740187,"set, which aims to detect the type and the target of offensive posts in social media. (Zampieri et al., 2020) expanded this dataset into the multilingual version, which advances the multilingual research in this area. Pre-trained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have achieved great performance on a variety of tasks. Many recent papers have used a basic recipe of finetuning such pre-trained models on a certain domain (Azzouza et al., 2019; Lee et al., 2019; Beltagy et al., 2019) or on downstream tasks (Howard and Ruder, 2018; Liu et al., 2019b; Su et al., 2019). 3 Datasets In this project, two datasets are involved, which are the dataset of OffensEval-2019 and OffensEval2020 respectively. In this section, we introduce the details of them and discuss our data pre-processing methods. Table 1 shows the types of labels and how they overlap. Task A Task B Task C OFF TIN IND GRP NOT UNT NULL NULL Table 1: Labels of three subtasks. 3.1 Offensive Language Identification Dataset (OLID) The OLID (Zampieri et al., 2019b) is a hierarchical dataset to identify the type and the target of offensive texts in social media. The dataset is collected on Twitter and pub"
2020.semeval-1.272,N19-1144,0,0.158311,"Missing"
2020.semeval-1.272,S19-2010,0,0.0721015,"Missing"
2021.acl-long.66,N19-1121,0,0.0215805,"optimizes four tasks to perform low-resource translation: (1) denoising autoencoder (2) adversarial training (3) high-resource translation and (4) low-resource backtranslation. We test our proposed method and demonstrate its effectiveness in improving low-resource translation from three distinct families: (1) Iberian languages, (2) Indic languages, and (3) Semitic languages, specifically Arabic dialects. We make our code and resources publicly available.2 2 Related Work Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019). However, while zero-shot translation translates between a language pair with no parallel data, there is an assumption that both languages in the target pair have some parallel data with other languages. As such, the system can learn to process both languages. In one work, Currey and Heafield (2019) improved zero-shot translation using monolingual data on the pivot language. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could he"
2021.acl-long.66,D18-1549,0,0.0199215,"age. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could help zero shot training. We adopt a similar philosophy in our multi-task training to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume tha"
2021.acl-long.66,2020.findings-emnlp.283,0,0.0611464,"es; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to sup"
2021.acl-long.66,P19-1121,0,0.0235806,"?? ??? ??? ??? ???? Figure 1: Illustration of the training tasks for translating from English into a low-resource language (LRL) and from an LRL to English. English to low-resource backtranslation data. The aim of this task is to capture a language-modeling effect in the low-resource language. We describe how we obtain this data using the high-resource translation model to bootstrap backtranslation in Section 3.3. The objective used is, 0 Lbt = LCE (D(ZEn , [LRL]), XLRL ) (discriminators). The critics are recurrent networks to ensure that they can handle variable-length text input. Similar to Gu et al. (2019b), the adversarial component is trained using a Wasserstein loss, which is the difference of expectations between the two types of data. This loss minimizes the earth mover’s distance between the distributions of different languages. We compute the loss function as follows: (3) 0 , where ZEn = E(YEn , [En]). (YEn , XLRL ) is an English to low-resource backtranslation pair. Ladv1 = E[Disc(ZHRL )] − E[Disc(ZLRL )] (4) Task 4: Adversarial Training The final task aims to make the encoder output language-agnostic features. The representation is language agnostic to the noised high and low-resource"
2021.acl-long.66,2020.findings-emnlp.371,0,0.015344,"ranslate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervis"
2021.acl-long.66,2020.tacl-1.47,1,0.896063,"ng to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further im"
2021.acl-long.66,D19-1632,1,0.901445,"Missing"
2021.acl-long.66,D18-1103,0,0.0196006,"to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervised NMT, our work looks at the problem from a domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020)"
2021.acl-long.66,2013.iwslt-papers.2,1,0.837467,"Missing"
2021.acl-long.66,L18-1548,0,0.0979839,"Missing"
2021.acl-long.66,W18-6316,0,0.0238497,"n language could be mapped word by word into the dialect vocabulary, and they calculate the corresponding word for substitution using 803 localized projection. This approach differs from our work in that it relies on the existence of a seed bilingual lexicon to the dialect/similar language. Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to t"
2021.acl-long.66,D17-1266,0,0.0184878,". Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to translating a low-resource language into and out of English without utilizing any low-resource language parallel data. In Section 3.1, we describe how NMT-Adapt leverages a novel multi-task domain adaptation approach to translating English into a low-resource language. In Section 3.2, we then describe"
2021.acl-long.66,N18-2084,0,0.063027,"Missing"
2021.acl-long.66,2020.semeval-1.271,0,0.033269,"Missing"
2021.acl-long.66,P16-1009,0,0.165315,"Missing"
2021.acl-long.66,2020.acl-main.252,0,0.0202162,"s. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been methods proposed that leverage similar languages to improve translation. Hassan et al. (2017) generated synthetic English-dialect parallel data from English-main language corpus. However, this method assumes that the vocabulary in the main language could be mapped word by word into the dialect vocabulary, and they calculate the"
2021.acl-long.66,tiedemann-2012-parallel,0,0.262167,"Missing"
2021.acl-long.66,2020.lrec-1.494,1,0.836882,"Missing"
2021.acl-long.66,P19-1579,0,0.0175642,"domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been me"
2021.acl-long.66,P11-2007,0,0.0962119,"Missing"
2021.calcs-1.20,W18-3219,0,0.032719,"Missing"
2021.calcs-1.20,E14-1049,0,0.0464071,"Missing"
2021.calcs-1.20,2020.lrec-1.223,0,0.241113,"Experiments In this section, we describe the details of the datasets we use and how the models are trained. 3.1 Datasets datasets since the multilingual models are trained with data using that form. Table 1 shows the number of tokens of each language for each dataset. We classify the language with more tokens as the ML and the other as the EL. We replace user hashtags and mentions with <USR&gt;, emoji with <EMOJI&gt;, and URL with <URL&gt; for models that use word-embeddings, similar to Winata et al. (2019a). We evaluate our model with the micro F1 score for NER and accuracy for POS tagging, following Aguilar et al. (2020a). #L1 #L2 ML EL 13,860 163,824 - 11,391 402,923 - HIN ENG MSA ENG SPA EA 12,589 178,135 9,882 92,517 HIN SPA ENG ENG NER HIN-ENG SPA-ENG MSA-EA† POS HIN-ENG SPA-ENG Table 1: Dataset statistics are taken from Aguilar et al. (2020a). We define L1 and L2 as the languages found in the dataset. For example, in HIN-ENG, L1 is HIN and L2 is ENG. † We define MSA as ML and EA as EL. #L1 represents the number of tokens in the first language and #L2 represents the number of tokens in the second language. 3.2 Experimental Setup We describe our experimental details for each model. 3.2.1 Scratch We train"
2021.calcs-1.20,L18-1550,0,0.244155,"rio. 2 Representation Models In this section, we describe multilingual models that we explore in the context of code-switching. Figure 1 shows the architectures for a word embeddings model, a multilingual language model, and the multilingual meta-embeddings (MME), and HME models. 2.1 Word Embeddings 2.1.1 FastText In general, code-switching text contains a primary language the matrix language (ML)) as well as a secondary language (the embedded language (EL)). To represent code-switching text, a straightforward idea is to train the model with the word embeddings of the ML and EL from FastText (Grave et al., 2018). Code-switching text has many noisy tokens and sometimes mixed words in the ML and EL that produce a “new word”, which leads to a high number of out-of-vocabulary (OOV) tokens. To solve this issue, we utilize subword-level embeddings from FastText (Grave et al., 2018) to generate the representations for these OOV tokens. We conduct experiments on two variants of applying the word embeddings to the code-switching tasks: FastText (ML) and FastText (EL), which utilize the word embeddings of ML and EL, respectively. 2.1.2 MUSE To leverage the information from the embeddings of both the ML and EL,"
2021.calcs-1.20,W17-4419,0,0.0329303,"Missing"
2021.calcs-1.20,2020.findings-emnlp.206,0,0.0374655,"Missing"
2021.calcs-1.20,2021.findings-emnlp.141,0,0.0409602,"Missing"
2021.calcs-1.20,N18-2031,0,0.0507836,"Missing"
2021.calcs-1.20,2020.acl-main.747,0,0.189471,"word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. While more advanced multilingual language Learning representation for code-switching has bemodels (Conneau et al., 2020) than multilincome a crucial area of research to support a greater gual BERT (Devlin et al., 2019) have been provariety of language speakers in natural language posed, their effectiveness is still unknown in codeprocessing (NLP) applications, such as dialogue system and natural language understanding (NLU). switching tasks. Thus, we investigate their effecCode-switching is a phenomenon in which a per- tiveness in the code-switching domain and compare son speaks more than one language in a conver- them with the existing works. Here, we would like to answer the following research question, “Whic"
2021.calcs-1.20,N19-1423,0,0.283999,"orm languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. While more advanced multilingual language Learning representation for code-switching has bemodels (Conneau et al., 2020) than multilincome a crucial area of research to support a greater gual BERT (Devlin et al., 2019) have been provariety of language speakers in natural language posed, their effectiveness is still unknown in codeprocessing (NLP) applications, such as dialogue system and natural language understanding (NLU). switching tasks. Thus, we investigate their effecCode-switching is a phenomenon in which a per- tiveness in the code-switching domain and compare son speaks more than one language in a conver- them with the existing works. Here, we would like to answer the following research question, “Which sation, and its usage is prevalent in multilingual models are effective in representing code-swi"
2021.calcs-1.20,L18-1473,0,0.0910692,"Missing"
2021.calcs-1.20,P14-1006,0,0.0144022,"Missing"
2021.calcs-1.20,D18-1330,0,0.0359848,"Missing"
2021.calcs-1.20,2020.acl-main.329,0,0.0877389,"ch as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. While more advanced multilingual language Learning representation for code-switching has bemodels (Conneau et al., 2020) than multilincome a crucial area of research to support a greater gual BERT (Devlin et al., 2019) have been provariety of language speakers in natural language posed, their effectiveness is still unknown in codeprocessing (NLP) applications, such as dialogue system and natural language understanding (NLU). switching tasks. Thus, we investigate their effecCode-switching is a phenomenon in which a per- tiveness in the code-switching domai"
2021.calcs-1.20,D18-1176,0,0.0479456,"Missing"
2021.calcs-1.20,W15-1521,0,0.0284237,"Missing"
2021.calcs-1.20,L18-1008,0,0.0405835,"en size of 768. This setting is important to measure the effectiveness of pre-trained multilingual models. We start the training with a learning rate of 1e-4 and an early stop of 10 epochs. We evaluate our models on five downstream tasks in the LinCE Benchmark (Aguilar et al., 2020a). We choose three named entity recognition (NER) tasks, Hindi-English (HIN-ENG) (Singh et al., 2018a), Spanish-English (SPA-ENG) (Aguilar et al., 2018) 3.2.2 Word Embeddings and Modern Standard Arabic (MSA-EA) (Aguilar We use FastText embeddings (Grave et al., 2018; et al., 2018), and two part-of-speech (POS) tag- Mikolov et al., 2018) to train our transformer modging tasks, Hindi-English (HIN-ENG) (Singh et al., els. The model consists of a 4-layer transformer 2018b) and Spanish-English (SPA-ENG) (Soto encoder with four heads and a hidden size of 200. and Hirschberg, 2017). We apply Roman-to- We train a transformer followed by a Conditional Devanagari transliteration on the Hindi-English Random Field (CRF) layer (Lafferty et al., 2001). 145 The model is trained by starting with a learning rate of 0.1 with a batch size of 32 and an early stop of 10 epochs. We also train our model with only ML and EL embeddings. We freeze al"
2021.calcs-1.20,W17-0212,0,0.0545924,"Missing"
2021.calcs-1.20,D14-1162,0,0.0948378,"Missing"
2021.calcs-1.20,P19-1493,0,0.0219007,"l language models (Devlin et al., 2019; Conneau et al., 2020) possess the ability to produce aligned multilingual representations for semantically similar words and sentences, which brings them advantages to cope with codemixed multilingual text. 2.2.1 Multilingual BERT Multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual version of the BERT model, is pretrained on Wikipedia text across 104 languages with a model size of 110M parameters. It has been shown to possess a surprising multilingual ability and to outperform existing strong models on multiple zero-shot cross-lingual tasks (Pires et al., 2019; Wu and Dredze, 2019). Given its strengths in handling multilingual text, we leverage it for code-switching tasks. 2.2.2 XLM-RoBERTa XLM-RoBERTa (XLM-R) (Conneau et al., 2020) is a multilingual language model that is pre-trained on 100 languages using more than two terabytes of filtered CommonCrawl data. Thanks to the largescale training corpora and enormous model size (XLM-RBASE and XLM-RLARGE have 270M and 550M parameters, respectively), XLM-R is shown to have a better multilingual ability than mBERT, and it can significantly outperform mBERT on a variety of cross-lingual benchmarks. Theref"
2021.calcs-1.20,P18-1143,0,0.0451186,"Missing"
2021.calcs-1.20,D18-1344,0,0.239691,"Missing"
2021.calcs-1.20,P18-3008,0,0.0857479,"Missing"
2021.calcs-1.20,W18-3503,0,0.115728,"in transformer-based models without any pretraining by following the mBERT model structure, and the parameters are randomly initialized, including the subword embeddings. We train transformer models with four and six layers with a hidden size of 768. This setting is important to measure the effectiveness of pre-trained multilingual models. We start the training with a learning rate of 1e-4 and an early stop of 10 epochs. We evaluate our models on five downstream tasks in the LinCE Benchmark (Aguilar et al., 2020a). We choose three named entity recognition (NER) tasks, Hindi-English (HIN-ENG) (Singh et al., 2018a), Spanish-English (SPA-ENG) (Aguilar et al., 2018) 3.2.2 Word Embeddings and Modern Standard Arabic (MSA-EA) (Aguilar We use FastText embeddings (Grave et al., 2018; et al., 2018), and two part-of-speech (POS) tag- Mikolov et al., 2018) to train our transformer modging tasks, Hindi-English (HIN-ENG) (Singh et al., els. The model consists of a 4-layer transformer 2018b) and Spanish-English (SPA-ENG) (Soto encoder with four heads and a hidden size of 200. and Hirschberg, 2017). We apply Roman-to- We train a transformer followed by a Conditional Devanagari transliteration on the Hindi-English R"
2021.calcs-1.20,W18-3220,0,0.0742304,"ions have been utilized cent performance in multilingual and crossto address the out-of-vocabulary issue in codelingual natural language understanding tasks. switched text (Winata et al., 2018c; Wang et al., However, the power of these multilingual mod2018b), while external handcrafted resources such els in code-switching tasks has not been fully explored. In this paper, we study the effectiveas gazetteers list are usually used to mitigate ness of multilingual language models to underthe low-resource issue in code-switching (Aguilar stand their capability and adaptability to the et al., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This"
2021.calcs-1.20,W18-5446,0,0.0696945,"Missing"
2021.calcs-1.20,W18-3221,0,0.0159302,"in multilingual models are effective in representing code-switching communities. Yet, despite the enormous number of text, and why?."" studies in multilingual NLP, only very few focus on code-switching. Recently, contextualized language In this paper, we evaluate the representation models, such as mBERT (Devlin et al., 2019) and quality of monolingual and bilingual word embedXLM-R (Conneau et al., 2020) have achieved state- dings, multilingual meta-embeddings, and multiof-the-art results on monolingual and cross-lingual lingual language models on five downstream tasks tasks in NLU benchmarks (Wang et al., 2018a; Hu on named entity recognition (NER) and part-ofet al., 2020; Wilie et al., 2020; Liu et al., 2020; Lin speech tagging (POS) in Hindi-English, Spanishet al., 2020). However, the effectiveness of these English, and Modern Standard Arabic-Egyptian. multilingual language models on code-switching We study the effectiveness of each model by contasks remains unknown. sidering three criteria: performance, speed, and the 142 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 142–153 June 11, 2021. ©2021 Association for Computational Linguistics https:/"
2021.calcs-1.20,2020.aacl-main.85,1,0.792195,"Yet, despite the enormous number of text, and why?."" studies in multilingual NLP, only very few focus on code-switching. Recently, contextualized language In this paper, we evaluate the representation models, such as mBERT (Devlin et al., 2019) and quality of monolingual and bilingual word embedXLM-R (Conneau et al., 2020) have achieved state- dings, multilingual meta-embeddings, and multiof-the-art results on monolingual and cross-lingual lingual language models on five downstream tasks tasks in NLU benchmarks (Wang et al., 2018a; Hu on named entity recognition (NER) and part-ofet al., 2020; Wilie et al., 2020; Liu et al., 2020; Lin speech tagging (POS) in Hindi-English, Spanishet al., 2020). However, the effectiveness of these English, and Modern Standard Arabic-Egyptian. multilingual language models on code-switching We study the effectiveness of each model by contasks remains unknown. sidering three criteria: performance, speed, and the 142 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 142–153 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_020 Figure 1: Model architectures for code-swit"
2021.calcs-1.20,2020.acl-main.348,1,0.858692,"Missing"
2021.calcs-1.20,W19-4320,1,0.786001,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,D19-1077,0,0.0244203,"evlin et al., 2019; Conneau et al., 2020) possess the ability to produce aligned multilingual representations for semantically similar words and sentences, which brings them advantages to cope with codemixed multilingual text. 2.2.1 Multilingual BERT Multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual version of the BERT model, is pretrained on Wikipedia text across 104 languages with a model size of 110M parameters. It has been shown to possess a surprising multilingual ability and to outperform existing strong models on multiple zero-shot cross-lingual tasks (Pires et al., 2019; Wu and Dredze, 2019). Given its strengths in handling multilingual text, we leverage it for code-switching tasks. 2.2.2 XLM-RoBERTa XLM-RoBERTa (XLM-R) (Conneau et al., 2020) is a multilingual language model that is pre-trained on 100 languages using more than two terabytes of filtered CommonCrawl data. Thanks to the largescale training corpora and enormous model size (XLM-RBASE and XLM-RLARGE have 270M and 550M parameters, respectively), XLM-R is shown to have a better multilingual ability than mBERT, and it can significantly outperform mBERT on a variety of cross-lingual benchmarks. Therefore, we also investiga"
2021.calcs-1.20,P16-1128,0,0.0494507,"Missing"
2021.calcs-1.20,D19-1360,1,0.812612,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,S19-2021,1,0.639343,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,W18-3207,1,0.940907,"al Models Effective in Code-Switching? Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology giwinata@connect.ust.hk Abstract Several approaches have been explored in code-switching representation learning in NLU. Multilingual language models have shown deCharacter-level representations have been utilized cent performance in multilingual and crossto address the out-of-vocabulary issue in codelingual natural language understanding tasks. switched text (Winata et al., 2018c; Wang et al., However, the power of these multilingual mod2018b), while external handcrafted resources such els in code-switching tasks has not been fully explored. In this paper, we study the effectiveas gazetteers list are usually used to mitigate ness of multilingual language models to underthe low-resource issue in code-switching (Aguilar stand their capability and adaptability to the et al., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dicti"
2021.calcs-1.20,K19-1026,1,0.59921,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,W18-3214,1,0.934916,"al Models Effective in Code-Switching? Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology giwinata@connect.ust.hk Abstract Several approaches have been explored in code-switching representation learning in NLU. Multilingual language models have shown deCharacter-level representations have been utilized cent performance in multilingual and crossto address the out-of-vocabulary issue in codelingual natural language understanding tasks. switched text (Winata et al., 2018c; Wang et al., However, the power of these multilingual mod2018b), while external handcrafted resources such els in code-switching tasks has not been fully explored. In this paper, we study the effectiveas gazetteers list are usually used to mitigate ness of multilingual language models to underthe low-resource issue in code-switching (Aguilar stand their capability and adaptability to the et al., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dicti"
2021.dialdoc-1.6,2020.acl-main.703,0,0.0176165,"ng rate # of ckpt Testdev Phase Inference with Knowledge Evidence. During the testdev and test phase, we leverage the predictions from the KI process as the knowledge evidence components for the dialogue queries. The model generates responses based on a concatenation of the knowledge evidence and the dialogue context. Response Generation To obtain natural and relevant responses, we take advantage of the evidence to the query identified from § 3.1 and focusing on paraphrasing the corresponding knowledge sentences based on the dialogue context. We leverage the large pre-trained model BARTlarge (Lewis et al., 2020). The process of training and inference can be summarized as three steps: Post-processing To avoid serious information loss in the generations compared to the knowledge evidence for the OOD data samples, we compare the lengths of the knowledge evidence and the responses (denoted as Lkn and Lresp ). The generated response will be replaced by the raw knowledge evidence as the final output if Lresp ≤ αLkn , where α is set as 0.4. Pre-training on WoW dataset. We first pretrain the BART model on the WoW dataset for better initialization because of its similarity with the RG task. In the training pr"
2021.dialdoc-1.6,2020.emnlp-main.275,0,0.0405936,"nses (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020). In addition to the works to enrich the contents of open-domain conversations by controllable generation (Lin et al., 2020; Madotto et al., 2020b), the knowledge grounded dialogue task aims to offer more informative conversation by leveraging an external knowledge source (Dinan et al., 2018; Xu et al., 2020). Relevant knowledge selection is the key to improving the whole system, and very recently, latent variable models have been attracting more attention for this purpose (Lian et al., 2019; Liu et al., 2019b; Kim et al., 2020; Chen et al., 2020; Xu et al., 2021). Results and Discussion Results The results are shown in Table 3 and Table 4. For both subtasks, we observe gaps between the testdev phase and the test phase. For some of the models in subtask 1, multiple random seeds are applied in the training process. The performance gap may result from the domain difference of the partial data samples in the test phase, where the corresponding documents are unseen in the training set. In Table 3, without post-processing on the predictions, the model performance consistently drops to a certain extent, which indicates that postprocessing i"
2021.dialdoc-1.6,2021.ccl-1.108,0,0.0263752,"Missing"
2021.dialdoc-1.6,D19-1129,1,0.882377,"Missing"
2021.dialdoc-1.6,2020.findings-emnlp.215,1,0.833504,"yxucb,eishii,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract To tackle this problem, we leverage the pretrained language models from Liu et al. (2019a) and Lewis et al. (2020) and explore data augmentation methods with several training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the"
2021.dialdoc-1.6,2020.findings-emnlp.219,1,0.784817,"yxucb,eishii,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract To tackle this problem, we leverage the pretrained language models from Liu et al. (2019a) and Lewis et al. (2020) and explore data augmentation methods with several training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the"
2021.dialdoc-1.6,W18-6319,0,0.0144952,"everal training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our system achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provid"
2021.dialdoc-1.6,P17-1147,0,0.0183514,"QA dataset and CQA datasets using MTL method. For better readability, we summarize the model settings in Table 1. We also explore more combinations of the experimental settings, such as other combinations of the datasets and other pre-trained language models. However, those fail to bring the improvements as much as those we mentioned above. FT Table 1: The combinations of the experimental settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue datase"
2021.dialdoc-1.6,Q19-1016,0,0.062586,"l settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue dataset (Dinan et al., 2018). It aims at providing content-full responses to user utterances based on Wikipedia documents. 3 Knowledge Identification Methodology We utilize a series of data-augmentation approaches to enable the model to obtain better representations on both dialogue context and document context and learn a general pattern of the task with less domain bias. Namely, we have a tw"
2021.dialdoc-1.6,D19-5827,1,0.846457,"erimental settings, such as other combinations of the datasets and other pre-trained language models. However, those fail to bring the improvements as much as those we mentioned above. FT Table 1: The combinations of the experimental settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue dataset (Dinan et al., 2018). It aims at providing content-full responses to user utterances based on Wikipedia documents. 3 Knowledge Identification Methodology"
2021.dialdoc-1.6,2020.emnlp-main.226,1,0.655395,"ly the question but also the previous conversation turns. Various datasets have been introduced in recent years, and many of them restrict answers to be extraction of a span from the reference document, while the others allow free-form responses (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020). In addition to the works to enrich the contents of open-domain conversations by controllable generation (Lin et al., 2020; Madotto et al., 2020b), the knowledge grounded dialogue task aims to offer more informative conversation by leveraging an external knowledge source (Dinan et al., 2018; Xu et al., 2020). Relevant knowledge selection is the key to improving the whole system, and very recently, latent variable models have been attracting more attention for this purpose (Lian et al., 2019; Liu et al., 2019b; Kim et al., 2020; Chen et al., 2020; Xu et al., 2021). Results and Discussion Results The results are shown in Table 3 and Table 4. For both subtasks, we observe gaps between the testdev phase and the test phase. For some of the models in subtask 1, multiple random seeds are applied in the training process. The performance gap may result from the domain difference of the partial data sample"
2021.emnlp-main.326,W05-0909,0,0.0522764,"et al., 2002) are used to calculate the recall and precision of n-gram overlaps, respectively, between the references and the generated summaries. MENTOR (Denkowski and Lavie, 2011) is used to match the word stems, synonyms and paraphrases between the reference and the generated summary. CIDEr (Vedantam et al., 2015) is an image captioning metric to compute the cosine similarity between TF-IDF weighted n-grams. In addition, We use Content F1 (Palaskar et al., 2019) to measure the F1 score of the content words of the generated summary based on a monolingual alignment. Firstly, METEOR toolkit (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) is used to obtain the alignment between the summaries and references. Then, the function words and task-specific stop words are removed from the summaries and references. Finally, the remaining content words from the summaries and references are treated as two bags of words, and the F1 scores are calculated over the alignment. Content F1 focuses more on the content and it can avoid the increase of the ROUGE score from the stop words. We use nlg-eval 3 to compute the BLEU, MENTOR and CIDEr scores, and use rouge 4 to compute ROUGE scores. The implementation of Conten"
2021.emnlp-main.326,2021.naacl-main.417,1,0.842461,"Missing"
2021.emnlp-main.326,2020.aacl-main.30,1,0.750771,"al., 2020b; Yu et al., 2021). Recently, GPLMs (Lewis et al., 2019; Raffel et al., 2019; Zhang et al., 2020a; Qi et al., 2020) have been widely used in abstractive text summarization and have achieved start-of-theart performance. The most significant difference between abstractive text summarization and multimodal abstractive summarization lies in whether the input contains data of more than one modality. 2.2 Multimodal Abstractive Summarization Recently, many studies have been performed on multimodal learning (Mroueh et al., 2015; Antol et al., 2015; Donahue et al., 2015; Zadeh et al., 2017; Dai et al., 2020, 2021). However, only a few have investigated MAS. Li et al. (2017) collected a multimodal corpus of news articles containing 500 videos of English news articles paired with human-annotated summaries. Sanabria et al. (2018) introduced the How2 dataset, which contains about 2,000 hours of short instructional videos, each coming with a summary of two to three sentences. Palaskar et al. (2019) proposed a multi-source Seq2Seq model with hierarchical attention to integrate information from different modalities into a coherent summary. Meanwhile, Liu et al. (2020) proposed a multistage fusion netwo"
2021.emnlp-main.326,W11-2107,0,0.0121913,"Transformer) (Liu et al., 2020). The multistage fusion with forget gate (MFFG) model proposes a cross fusion block with forget gate and a hierarchical fusion decoder to improve multimodal generation. 4.4 Evaluation Metrics Following (Liu et al., 2020), we use ROUGE, BLEU, METEOR, and CIDEr to evaluate the summaries. ROUGE-{1, 2, L} (the standard metrics for abstractive summarization) (Lin and Hovy, 2003) and 4000 BLEU-{1, 2, 3, 4} (Papineni et al., 2002) are used to calculate the recall and precision of n-gram overlaps, respectively, between the references and the generated summaries. MENTOR (Denkowski and Lavie, 2011) is used to match the word stems, synonyms and paraphrases between the reference and the generated summary. CIDEr (Vedantam et al., 2015) is an image captioning metric to compute the cosine similarity between TF-IDF weighted n-grams. In addition, We use Content F1 (Palaskar et al., 2019) to measure the F1 score of the content words of the generated summary based on a monolingual alignment. Firstly, METEOR toolkit (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) is used to obtain the alignment between the summaries and references. Then, the function words and task-specific stop words are r"
2021.emnlp-main.326,W14-3348,0,0.0129591,"calculate the recall and precision of n-gram overlaps, respectively, between the references and the generated summaries. MENTOR (Denkowski and Lavie, 2011) is used to match the word stems, synonyms and paraphrases between the reference and the generated summary. CIDEr (Vedantam et al., 2015) is an image captioning metric to compute the cosine similarity between TF-IDF weighted n-grams. In addition, We use Content F1 (Palaskar et al., 2019) to measure the F1 score of the content words of the generated summary based on a monolingual alignment. Firstly, METEOR toolkit (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) is used to obtain the alignment between the summaries and references. Then, the function words and task-specific stop words are removed from the summaries and references. Finally, the remaining content words from the summaries and references are treated as two bags of words, and the F1 scores are calculated over the alignment. Content F1 focuses more on the content and it can avoid the increase of the ROUGE score from the stop words. We use nlg-eval 3 to compute the BLEU, MENTOR and CIDEr scores, and use rouge 4 to compute ROUGE scores. The implementation of Content F1 scores follows (Palaska"
2021.emnlp-main.326,N19-1423,0,0.194855,"eractions between multi-source modalities. To the best of our knowledge, no previous work has leveraged GPLMs’ generation ability to tackle the MAS task, and we are the first to systematically study multiple multimodal fusion methods based on GPLMs. • We systematically study two research ques2.3 Vision-Language Large Pre-trained tions: 1) how to inject visual information Transformer Models into GPLMs without hurting their generation ability; and 2) where is the optimal place in With the remarkable success of large-scale unsuGPLMs to inject the visual information? pervised pre-training in NLP (Devlin et al., 2019; 3996 Output Probabilities Vision Features ... Add & Norm (1) Softmax ... Linear Cross-modal Dot-Product Attention Add & Norm Linear FFN Text-Vision Fusion ... ... Add & Norm Add & Norm FFN Add & Norm Multi-head Self-Attention Multi-head Enc-Dec Attention (2) ... Add & Norm Cross-modal Multi-head Attention Multi-head Self-Attention Masked Linear Positional Encoding Positional Encoding ... Text Inputs ... ... Linear Linear ... Outputs (shifted right) Figure 2: An overview of our proposed VG GPLMs. It is built based on the Transformer-based Seq2Seq GPLMs (left). To inject visual information, we"
2021.emnlp-main.326,2020.nlpbt-1.7,0,0.046565,"xcept the green dashed block). At the entry of the GPLM, the input text is first tokenized and converted to a sequence of token embeddings ?? ∈ R ? ×?? , in which ? is the sequence length and ?? is the feature dimension. To retain the positional information, positional encodings (Vaswani et al., 2017a) ? ?? ∈ R ? ×?? are added to the token embeddings pointwisely (Eq. 1), which forms the input features ?0??? to the encoder. ?0??? = ?? + ? ?? ????? = LN(FFN(?????0) + ?????0) 3.2 Video Feature Extraction For each video clip, following previous works (Sanabria et al., 2018; Palaskar et al., 2019; Khullar and Arora, 2020), a 2048-dimensional feature representation is extracted for every 16 non-overlapping frames using a 3D ResNeXt-101 model (Hara et al., 2018), which is pre-trained on the Kinetics dataset (Kay et al., 2017). Therefore, each data sample will have a sequence of 2048-? vision feature vectors of length ?. These features can be used directly as the visual input to the text-vision fusion mechanism. In addition, in order to better model the intramodal dynamics and enhance the vision specific temporal information, we further process the extracted sequence of visual features using a Transformer (Vaswan"
2021.emnlp-main.326,2020.acl-main.703,0,0.0881707,"Missing"
2021.emnlp-main.326,D17-1114,0,0.0193076,"Raffel et al., 2019; Zhang et al., 2020a; Qi et al., 2020) have been widely used in abstractive text summarization and have achieved start-of-theart performance. The most significant difference between abstractive text summarization and multimodal abstractive summarization lies in whether the input contains data of more than one modality. 2.2 Multimodal Abstractive Summarization Recently, many studies have been performed on multimodal learning (Mroueh et al., 2015; Antol et al., 2015; Donahue et al., 2015; Zadeh et al., 2017; Dai et al., 2020, 2021). However, only a few have investigated MAS. Li et al. (2017) collected a multimodal corpus of news articles containing 500 videos of English news articles paired with human-annotated summaries. Sanabria et al. (2018) introduced the How2 dataset, which contains about 2,000 hours of short instructional videos, each coming with a summary of two to three sentences. Palaskar et al. (2019) proposed a multi-source Seq2Seq model with hierarchical attention to integrate information from different modalities into a coherent summary. Meanwhile, Liu et al. (2020) proposed a multistage fusion network with the fusion forget gate module, which can model the fine-grai"
2021.emnlp-main.326,P17-2031,0,0.0378986,"Missing"
2021.emnlp-main.326,N03-1020,0,0.0443178,"er) (Palaskar et al., 2019). A multi-source Seq2seq model with hierarchical attention (HA) (Libovick`y and Helcl, 2017) that can integrates information from different modalities into a coherent output. MFFG (RNN/Transformer) (Liu et al., 2020). The multistage fusion with forget gate (MFFG) model proposes a cross fusion block with forget gate and a hierarchical fusion decoder to improve multimodal generation. 4.4 Evaluation Metrics Following (Liu et al., 2020), we use ROUGE, BLEU, METEOR, and CIDEr to evaluate the summaries. ROUGE-{1, 2, L} (the standard metrics for abstractive summarization) (Lin and Hovy, 2003) and 4000 BLEU-{1, 2, 3, 4} (Papineni et al., 2002) are used to calculate the recall and precision of n-gram overlaps, respectively, between the references and the generated summaries. MENTOR (Denkowski and Lavie, 2011) is used to match the word stems, synonyms and paraphrases between the reference and the generated summary. CIDEr (Vedantam et al., 2015) is an image captioning metric to compute the cosine similarity between TF-IDF weighted n-grams. In addition, We use Content F1 (Palaskar et al., 2019) to measure the F1 score of the content words of the generated summary based on a monolingual"
2021.emnlp-main.326,2020.emnlp-main.144,0,0.468077,"modalities: a video and its transcript. Therefore, we emphasize that leveraging a powerful Multimodal abstractive summarization (MAS) aims text generation model and an effective combination to take advantage of data from multiple modalities of the vision and text modalities are key to constructand provides a short, concise and readable textual ing good MAS models. Recently, Transformersummary to let users quickly acquire their essential information (Sanabria et al., 2018; Palaskar et al., based (Vaswani et al., 2017b) sequence-to-sequence (Seq2Seq) large-scale generative pre-trained lan2019; Liu et al., 2020). MAS has become an guage models (GPLMs), such as BART (Lewis et al., increasingly popular research area thanks to the 2019), T5 (Raffel et al., 2019), PEGASUS (Zhang proliferation of online multimedia content and the et al., 2020a) and ProphetNet (Qi et al., 2020), have increasing availability of multimodal data. shown remarkable performance on text generation ∗ The two authors contribute equally. tasks, including abstractive text summarization. 1The code is available at: https://github.com/ HLTCHKUST/VG-GPLMs However, leveraging and adapting GPLMs to MAS 3995 Proceedings of the 2021 Conferen"
2021.emnlp-main.326,D15-1166,0,0.0490075,"TF largely surpasses the previous state-of-the-art model. Software and hardware. We use the deep learning framework PyTorch (Paszke et al., 2019) to implement our code and PyTorch-Lightning2 for the distributed training. We use four Nvidia GeForce RTX 2080 Ti GPUs for all of our experiment. 4.3 Baselines Apart from the text-only GPLMs BART (Lewis et al., 2019) and T5 (Raffel et al., 2019), we use the following baselines to compare with our proposed models, including simple models that only accept text input, as well as prior state-of-the-art models that accept text and vision modalities. S2S (Luong et al., 2015). S2S is a standard Seq2seq model that uses RNNs for both encoder and decoder with a global attention mechanism (Bahdanau et al., 2014). PG (See et al., 2017). The pointer generator (PG) network augments S2S by having a copy module 2https://github.com/PyTorchLightning/ pytorch-lightning to reproduce key information accurately as well as mitigating the out-of-vocabulary issue. TF (Vaswani et al., 2017b). TF is the standard Transformer-based Seq2seq model, which proposes the novel multi-head attention mechanism. HA (RNN/Transformer) (Palaskar et al., 2019). A multi-source Seq2seq model with hier"
2021.emnlp-main.326,P19-1659,0,0.0328311,"Missing"
2021.emnlp-main.326,P02-1040,0,0.111731,"seq model with hierarchical attention (HA) (Libovick`y and Helcl, 2017) that can integrates information from different modalities into a coherent output. MFFG (RNN/Transformer) (Liu et al., 2020). The multistage fusion with forget gate (MFFG) model proposes a cross fusion block with forget gate and a hierarchical fusion decoder to improve multimodal generation. 4.4 Evaluation Metrics Following (Liu et al., 2020), we use ROUGE, BLEU, METEOR, and CIDEr to evaluate the summaries. ROUGE-{1, 2, L} (the standard metrics for abstractive summarization) (Lin and Hovy, 2003) and 4000 BLEU-{1, 2, 3, 4} (Papineni et al., 2002) are used to calculate the recall and precision of n-gram overlaps, respectively, between the references and the generated summaries. MENTOR (Denkowski and Lavie, 2011) is used to match the word stems, synonyms and paraphrases between the reference and the generated summary. CIDEr (Vedantam et al., 2015) is an image captioning metric to compute the cosine similarity between TF-IDF weighted n-grams. In addition, We use Content F1 (Palaskar et al., 2019) to measure the F1 score of the content words of the generated summary based on a monolingual alignment. Firstly, METEOR toolkit (Banerjee and L"
2021.emnlp-main.326,2020.findings-emnlp.217,0,0.0122487,"odalities are key to constructand provides a short, concise and readable textual ing good MAS models. Recently, Transformersummary to let users quickly acquire their essential information (Sanabria et al., 2018; Palaskar et al., based (Vaswani et al., 2017b) sequence-to-sequence (Seq2Seq) large-scale generative pre-trained lan2019; Liu et al., 2020). MAS has become an guage models (GPLMs), such as BART (Lewis et al., increasingly popular research area thanks to the 2019), T5 (Raffel et al., 2019), PEGASUS (Zhang proliferation of online multimedia content and the et al., 2020a) and ProphetNet (Qi et al., 2020), have increasing availability of multimodal data. shown remarkable performance on text generation ∗ The two authors contribute equally. tasks, including abstractive text summarization. 1The code is available at: https://github.com/ HLTCHKUST/VG-GPLMs However, leveraging and adapting GPLMs to MAS 3995 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3995–4007 c November 7–11, 2021. 2021 Association for Computational Linguistics is still an unexplored research direction. To explore this direction, two main questions need to be answered: Firstly, how"
2021.emnlp-main.326,2020.acl-main.214,0,0.0360972,"and generation with video data. However, compared to GPLMs in NLP such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019), their text generation ability is limited as the training data is much smaller. In this paper, we propose to tackle VL tasks and utilize the advantage of pre-training from a different angle by inserting add-on layers to the text-only GPLMs and fine-tuning them on multimodal tasks to incorporate visual information. This takes advantage of GPLMs’ superior generation ability to generate vision-aware texts. Of the very few works that have also considered this direction, Rahman et al. (2020) proposed the multimodal adaptation gate, which fuses data of other modalities to the textual embeddings in BERT. However, their method requires all modalities to have the same sequence 3997 length, which is rare for most datasets. Additionally, they only attempted to address the sentiment analysis task and did not explore text generation. 3 Vision Guided GPLMs To take advantage of the superior text generation ability of the text-only Seq2seq GPLMs and adapt them to the MAS task, we present Vision guided (VG) GPLMs. Specifically, we leverage BART (Lewis et al., 2019) and T5 (Raffel et al., 201"
2021.emnlp-main.326,P17-1099,0,0.047553,"ur code and PyTorch-Lightning2 for the distributed training. We use four Nvidia GeForce RTX 2080 Ti GPUs for all of our experiment. 4.3 Baselines Apart from the text-only GPLMs BART (Lewis et al., 2019) and T5 (Raffel et al., 2019), we use the following baselines to compare with our proposed models, including simple models that only accept text input, as well as prior state-of-the-art models that accept text and vision modalities. S2S (Luong et al., 2015). S2S is a standard Seq2seq model that uses RNNs for both encoder and decoder with a global attention mechanism (Bahdanau et al., 2014). PG (See et al., 2017). The pointer generator (PG) network augments S2S by having a copy module 2https://github.com/PyTorchLightning/ pytorch-lightning to reproduce key information accurately as well as mitigating the out-of-vocabulary issue. TF (Vaswani et al., 2017b). TF is the standard Transformer-based Seq2seq model, which proposes the novel multi-head attention mechanism. HA (RNN/Transformer) (Palaskar et al., 2019). A multi-source Seq2seq model with hierarchical attention (HA) (Libovick`y and Helcl, 2017) that can integrates information from different modalities into a coherent output. MFFG (RNN/Transformer)"
2021.emnlp-main.326,P19-1644,0,0.0244035,"large vision-language (VL) models has also become more and more popular in recent years. Rather than designing task-specific architectures, pre-training results in a general backbone model by feeding it with a large amount of data and then fine-tune it to different downstream tasks. Among the current VL pre-training work, most has been focusing on VL understanding by training BERT-style Transformer models (Sun et al., 2019; Tan and Bansal, 2019; Su et al., 2020; Li et al., 2020; Chen et al., 2020) and finetune them on various VL classification tasks (Goyal et al., 2017; Zellers et al., 2019; Suhr et al., 2019). These models usually receive a pair of text and image as input, where the image is processed into objects (Zhang et al., 2021), patches (Kim et al., 2021), or pixels (Huang et al., 2020) before feeding into the VL model. For VL text generation, Zhou et al. (2020) presented a model for both visual question answering and image captioning (Chen et al., 2015). Additionally, Cho et al. (2021) introduced an encoder-decoder Transformer model that unifies all VL tasks as generative tasks. Although prior work has made much progress on VL pre-training, the problem of generating text given text and vid"
2021.emnlp-main.326,D19-1514,0,0.0187897,"also be placed in the decoder in a similar way. We compare the effects of different injection locations in Section 5. Liu et al., 2019; Radford et al., 2019), pre-training large vision-language (VL) models has also become more and more popular in recent years. Rather than designing task-specific architectures, pre-training results in a general backbone model by feeding it with a large amount of data and then fine-tune it to different downstream tasks. Among the current VL pre-training work, most has been focusing on VL understanding by training BERT-style Transformer models (Sun et al., 2019; Tan and Bansal, 2019; Su et al., 2020; Li et al., 2020; Chen et al., 2020) and finetune them on various VL classification tasks (Goyal et al., 2017; Zellers et al., 2019; Suhr et al., 2019). These models usually receive a pair of text and image as input, where the image is processed into objects (Zhang et al., 2021), patches (Kim et al., 2021), or pixels (Huang et al., 2020) before feeding into the VL model. For VL text generation, Zhou et al. (2020) presented a model for both visual question answering and image captioning (Chen et al., 2015). Additionally, Cho et al. (2021) introduced an encoder-decoder Transfor"
2021.emnlp-main.326,P19-1656,0,0.0253707,"extual features ?? are concatenated with the attention weighted visual features ?? ? and then projected by another linear transformation to output the vision guided textual features ??0 (Eq. 6). ? ?0 = ? ? ?1 , ? ?0 ∈ R ? ×?? ?= ??0 Softmax(?? ? ?0? ), ?∈R (4) ? ×? = Concat(?? , ?? ? )?2 (5) (6) Additionally, we build a variant of this fusion, which uses the linearly transformed visual features ?? ?0 for the concatenation in Eq. 6 instead of the original ?? ? . A comparison of their performance is shown in Section 5. Multi-head Attention Based Fusion. Inspired by prior works (Yu et al., 2019; Tsai et al., 2019), we propose a vision guided multi-head attention mechanism for the text-vision fusion. The query ? is linearly projected from the input textual features, and the key ? and value ? are linearly projected from the visual features (Eq. 7 - 9). Then, a crossmodal multi-head attention (CMA) is applied to get the text queried visual features ? (Eq. 10). Finally, we obtain the vision guided output ??0 by concatenating the input textual features ?? and ?, and linearly project it to the desired dimension (Eq. 11). ? = ?? ?? , ? ∈ R ? ×?? (7) ? = ? ? ? ? , ? ∈ R ? ×?? (8) ? ×?? (9) ? = ? ? ?? , ? ∈ R ?"
2021.emnlp-main.326,P19-1176,0,0.0160816,"ce the vision specific temporal information, we further process the extracted sequence of visual features using a Transformer (Vaswani et al., 2017a) encoder (VTF) with positional encodings. Experiments illustrate that this additional encoding process can further boost the performance of our model (Section 5). (1) As illustrated in Figure 2, the encoder is composed of a stack of ? encoder layers, each containing two sub-layers: 1) Multi-head Self-Attention (MSA, Eq. 2) and 2) Feed-Forward Network (FFN, Eq. 3). In addition, after each sub-layer, there is a residual connection (He et al., 2015; Wang et al., 2019) followed by a layer normalization (LN) (Ba et al., 2016). See Appendix A and B for more details of the MSA and FFN. ??? ??? ?????0 = LN(MSA(??−1 ) + ??−1 ) Similar to the encoder, the decoder also consists of a stack of ? decoder layers, but with two differences. Firstly, the MSA is masked to prevent positions from attending to subsequent positions (keep the decoder in a left-to-right direction). Secondly, there is one more multi-head encoder-decoder attention sub-layer, which uses the decoder embeddings to attend over the output embeddings of the encoder to incorporate the encoded informatio"
2021.emnlp-main.326,D19-1298,0,0.0247356,"Missing"
2021.emnlp-main.326,2021.naacl-main.471,1,0.833391,"Missing"
2021.emnlp-main.590,2020.acl-main.573,0,0.0378034,"observe that by storing only a few samples per task (10-50) the model still greatly suffers from catastrophic forgetting, where with around 500 samples, which is equivalent to a total of 18,500 samples in our setting, the performance is closer to that of the multitask baseline (i.e., a possible upper bound). Similar observations are shown for the other two tasks in Figure 8, 9, and 10 in the Appendix. a fine-tuning step) during inference. Finally, continual learning has been used for sentence encoding (Liu et al., 2019), composition language learning (Li et al., 2019c) and relation learning (Han et al., 2020). However, these methods are specific to particular applications not generalizable to ToDs. 6 Related Work Continual learning methods are usually developed and benchmarked on computer visions tasks. Interested readers may refer to Mundt et al. (2020); Parisi et al. (2019); De Lange et al. (2019) for an overview of the existing approaches, and to Section 2.2 for more details on the three main CL approaches studied in this paper. Continual learning has also been studied in the Long Life Learning (LLL) scenario, where a learner continuously accumulates knowledge and makes use of it in the future"
2021.emnlp-main.622,D18-1547,0,0.416429,"of a list of slot-value pairs. Training a tion model for non-categorical slots and a classifiDST model often requires extensive annotated di- cation model for categorical slots, which hinders alogue data. These data are often collected via a the knowledge sharing from the different types of Wizard-of-Oz (Woz) (Kelley, 1984) setting, where QA datasets. Furthermore, unanswerable questions two workers converse with each other and anno- are not considered during their QA training phase. tate the dialogue states of each utterance (Wen Therefore, in a zero-shot DST setting, the model et al., 2017; Budzianowski et al., 2018; Moon et al., proposed by Gao et al. (2020) is not able to han2020), or with a Machines Talking To Machines dle “none” value slots (e.g., unmentioned slots) ∗ Work done during internship at Facebook that present in the dialogue state. 7890 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7890–7900 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: A high-level representation of the cross-task transfer for zero-shot DST (best viewed in color). During the QA training phase (top figure), the unified generative model (T5) i"
2021.emnlp-main.622,2020.acl-main.12,0,0.0330909,"Missing"
2021.emnlp-main.622,N19-1423,0,0.062187,"Missing"
2021.emnlp-main.622,Q19-1026,0,0.012715,"uncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi dom"
2021.emnlp-main.622,D17-1082,0,0.0876989,"re 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains"
2021.emnlp-main.622,P19-1546,0,0.0155263,"no “none” values are considered as active slots. Then the model gener2 https://github.com/jasonwu0731/ trade-dst 3 https://github.com/google-research/ google-research/tree/master/schema_ guided_dst 4 Source code is available in https://github.com/ facebookresearch/Zero-Shot-DST 7893 Model Joint Goal Accuracy Hotel Restaurant Taxi Train Average 20.06 22.46 22.60 31.25 14.20 16.28 19.80 22.72 12.59 13.56 16.50 26.28 59.21 59.27 59.50 61.87 22.39 22.76 22.50 36.72 25.69 26.87 28.18 35.77 56.81 53.90 56.81 63.22 49.57 56.06 Attraction TRADE† (Wu et al., 2019) MA-DST† (Kumar et al., 2020) SUMBT‡ (Lee et al., 2019) TransferQA (Ours) w/ Oracle Slot Gate SGD-baseline JGA AGA TransferQA JGA AGA Unseen Buses* Messaging* Payment* Trains* Alarm* 9.7 10.2 11.5 13.6 57.7 50.9 20.0 34.8 63.5 1.8 15.9 13.3 24.7 17.4 58.3 63.6 37.9 60.7 64.9 81.7 Seen Table 2: Zero-shot results on MultiWoz 2.1 (Eric et al., 2020). Results marked with † and ‡ are from Kumar et al. (2020) and Campagna et al. (2020). We also report the averaged zero shot joint goal accuracy among five domains. Note that this averaged per-domain accuracy is not comparable to the JGA in full shot setting. RentalCars Music RideSharing Media Homes Restau"
2021.emnlp-main.622,P18-1133,0,0.024592,"Predicted Values hotel-area hotel-pricerange what is the area of the hotel that the user wants? what is the price range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learn"
2021.emnlp-main.622,2021.acl-long.353,0,0.0363165,"Missing"
2021.emnlp-main.622,2021.naacl-main.448,1,0.866589,"Missing"
2021.emnlp-main.622,2020.emnlp-main.273,1,0.761706,"e range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learning has been proposed as an effective solution. Wu et al. (2019) adapt a copy mechanism for transferring prior k"
2021.emnlp-main.622,2021.ccl-1.108,0,0.0535518,"Missing"
2021.emnlp-main.622,P18-2124,0,0.020354,"omain unmentioned slots often appear in the middle of conversations, where some of the in-domain slots have not yet mentioned by the user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level an"
2021.emnlp-main.622,D16-1264,0,0.0515644,"datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains are not presented in the training set. 1 Note that original MRQA-2019 dataset use SQuAD (Rajpurkar et al., 2016), here we also add the unanswerable questions from SQuAD2.0. Dataset Type Train Dev SQuAD2.0 NewsQA TriviaQA SearchQA HotpotQA NaturalQA extractive extractive extractive extractive extractive extractive 130,319 74,160 61,688 117,384 72,928 104,071 11,873 4,212 7,785 16,980 5,904 12,836 multiple-choice multiple-choice 87,866 6,116 4,887 2,040 RACE DREAM Table 1: Datasets used in the QA pre-training. Statistics of extractive datasets (except SQuAD2.0) are taken from MRQA-2019 (Fisch et al., 2019), and that of multiple-choice datasets are from RACE (Lai et al., 2017) and DREAM (Sun et al., 2019)."
2021.emnlp-main.622,N18-2074,0,0.0549602,"Missing"
2021.emnlp-main.622,Q19-1014,0,0.141767,"passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains are not presented in the trai"
2021.emnlp-main.622,W17-2623,0,0.0269985,"in the middle of conversations, where some of the in-domain slots have not yet mentioned by the user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In Mul"
2021.emnlp-main.622,D19-1221,0,0.0383259,"Missing"
2021.emnlp-main.622,E17-1042,0,0.0744009,"Missing"
2021.emnlp-main.622,2020.emnlp-main.66,0,0.0179673,"erange what is the area of the hotel that the user wants? what is the price range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learning has been proposed as an effective"
2021.emnlp-main.622,P19-1078,1,0.83341,"nsuming manual annotations, while M2M requires exhaustive hand-crafted rules for covering various dialogue scenarios. In industrial applications, virtual assistants are required to add new services (domains) frequently based on user’s needs, but collecting extensive data for every new domain is costly and inefficient. Therefore, performing zero-shot prediction of dialogue states is becoming increasingly important since it does not require the expense of data acquisition. There are mainly two lines of work in the zero-shot transfer learning problem. The first is cross-domain transfer learning (Wu et al., 2019; Kumar et al., 2020; Rastogi et al., 2020; Lin et al., 2021a), where the models are first trained on several domains, then zero-shot to new domains. However, these methods rely on a considerable amount 1 Introduction of DST data to cover a broad range of slot types, Virtual assistants are designed to help users per- and it is still challenging for the models to handle form daily activities, such as travel planning, on- new slot types in the unseen domain. The second line shopping and restaurant booking. Dialogue line of work leverages machine reading question state tracking (DST), as an essen"
2021.emnlp-main.622,D18-1259,0,0.0255177,"user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where resta"
2021.emnlp-main.622,2020.nlp4convai-1.13,0,0.0601404,"Missing"
2021.emnlp-main.699,P18-1060,0,0.0391068,"Missing"
2021.emnlp-main.699,2020.lrec-1.293,0,0.0675783,"Missing"
2021.emnlp-main.699,2020.tacl-1.30,0,0.0550978,"Missing"
2021.emnlp-main.699,N19-1423,0,0.0296779,"corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBARTLARGE (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, local languages to achieve more efficient learning and faster inference for very low-resource languages like Javanese and Sundanese.1 developed for high-resource languages such as English, French, and Chinese (Devlin et al., 2019; Martin et al., 2020; Chen et al., 2020). Although the number of datasets, models, and benchmarks has been increasing for low-resource languages such as Indonesian (Wilie et al., 2020; Koto et al., 2020b), Bangla (Bhattacharjee et al., 2021), and Filipino (Cruz and Cheng, 2020), these datasets primarily focus on natural language understanding (NLU) tasks, which only cover a subset of practical NLP systems today. In contrast, much fewer natural language generation (NLG) benchmarks have been developed for low-resource languages; most multilingual NLG resources thus far have primarily focused on"
2021.emnlp-main.699,2020.lrec-1.325,0,0.0256818,"ore efficient learning of low-resource languages. 2 Related Work NLP Benchmarks. Numerous benchmarks have recently emerged, which have catalyzed advances in monolingual and cross-lingual transfer learning. These include NLU benchmarks for low-resource languages including IndoNLU (Wilie et al., 2020), IndoLEM (Koto et al., 2020b), and those focusing on Filipino (Cruz and Cheng, 2020), Bangla (Bhattacharjee et al., 2021), and Thai (Lowphansirikul et al., 2021); neural machine translation (MT) datasets for low-resource scenarios including for Indonesian (Guntara et al., 2020), African languages (Duh et al., 2020; Lakew et al., 2020), and Nepali and Sinhala (Guzmán et al., 2019); and large-scale multilingual benchmarks such as XTREME (Hu et al., 2020), MTOP (Li et al., 2020), and XGLUE (Liang et al., 2020). Winata et al. (2021); Aguilar et al. (2020); Khanuja et al. (2020) further developed multilingual benchmarks to evaluate the effectiveness of pretrained multilingual language models. More recently, GEM (Gehrmann et al., 2021) covers NLG tasks in various languages, together Our contributions are as follows: 1) we curate with automated and human evaluation metrics. Our a multilingual pretraining data"
2021.emnlp-main.699,2020.emnlp-main.393,0,0.0593647,"Missing"
2021.emnlp-main.699,D19-1632,0,0.0215437,"k NLP Benchmarks. Numerous benchmarks have recently emerged, which have catalyzed advances in monolingual and cross-lingual transfer learning. These include NLU benchmarks for low-resource languages including IndoNLU (Wilie et al., 2020), IndoLEM (Koto et al., 2020b), and those focusing on Filipino (Cruz and Cheng, 2020), Bangla (Bhattacharjee et al., 2021), and Thai (Lowphansirikul et al., 2021); neural machine translation (MT) datasets for low-resource scenarios including for Indonesian (Guntara et al., 2020), African languages (Duh et al., 2020; Lakew et al., 2020), and Nepali and Sinhala (Guzmán et al., 2019); and large-scale multilingual benchmarks such as XTREME (Hu et al., 2020), MTOP (Li et al., 2020), and XGLUE (Liang et al., 2020). Winata et al. (2021); Aguilar et al. (2020); Khanuja et al. (2020) further developed multilingual benchmarks to evaluate the effectiveness of pretrained multilingual language models. More recently, GEM (Gehrmann et al., 2021) covers NLG tasks in various languages, together Our contributions are as follows: 1) we curate with automated and human evaluation metrics. Our a multilingual pretraining dataset for Indonesian, benchmark compiles languages and tasks that are"
2021.emnlp-main.699,2020.acl-main.703,0,0.0278531,"84 10,972 3,862 2,810 855 484 Table 2: Task statistics and descriptions. † We create new splits for the train and test. English) MT tasks, Indonesian summarization, and Indonesian chit-chat dialogue. Pretrained NLG Models. Recently, the paradigm of pretraining-then-fine-tuning has achieved remarkable success in NLG, as evidenced by the success of monolingual pretrained NLG models. GPT-2 (Radford et al., 2019), and later GPT-3 (Brown et al., 2020), demonstrated that language models can perform zero-shot transfer to downstream tasks via generation. Other recent state-of-the-art models are BART (Lewis et al., 2020), which maps corrupted documents to their original, and the encoder-decoder T5 (Raffel et al., 2020), which resulted from a thorough investigation of architectures, objectives, datasets, and pretraining strategies. These monolingual models have been generalised to the multilingual case by pretraining the architectures on multiple languages; examples include mBART (Liu et al., 2020) and mT5 (Xue et al., 2020). In this paper, we focus on local, near-monolingual models for the languages of Indonesia, and systematically compare them on our benchmark with such larger multilingual models. 3 3.1 Indo"
2021.emnlp-main.699,W04-1013,0,0.102429,"-5, 1e-5, 5e-6] and report the best results. We report the best hyperparameter settings for each model in Appendix C. 5 Evaluation Procedure For evaluation, we use beam search with a beam width of 5, a length penalty α of 1.0, and limit the maximum sequence length to 512 for all models and all tasks. We conduct both automatic and human evaluations to assess the models. We use a different evaluation metric for each task following the standard evaluation metric on the corresponding task. For machine translation, we report the SacreBLEU (Post, 2018) score. For summarization, we report the ROUGE (Lin, 2004) score. For IndoGPT. We pretrain our IndoGPT model us- QA, the F1 and exact match scores are reported foling an autoregressive language modeling objec- lowing the original SQUAD V2 (Rajpurkar et al., 2018) evaluation metrics. For chit-chat, we report tive (Radford et al., 2019) for 640k iterations on 8 NVIDIA V100 GPUs, with a batch size of 512, both the BLEU and SacreBLEU scores (Papineni et al., 2002). an initial learning rate of 5e-5, and a maximum sequence length of 1024. We apply distributed data We further conduct human evaluation on eight parallelism (DDP) with ZeRO-DP (Rajbhandari task"
2021.emnlp-main.699,2020.tacl-1.47,0,0.228056,"languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBARTLARGE (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, local languages to achieve more efficient learning and faster inference for very low-resource languages like Javanese and Sundanese.1 developed for high-resource languages such as English, French, and Chinese (Devlin et al., 2019; Martin et al., 2020; Chen et al., 2020). Although the number of datasets, models, and benchmarks has been increasing for low-resource languages such as Indonesian (Wilie et al., 2020; Koto et al., 2020b), Bangla (Bhattacharjee et al., 2021), and Filipino (Cruz and Cheng, 2020), these datasets"
2021.emnlp-main.699,D17-1238,0,0.0332864,"Missing"
2021.emnlp-main.699,2020.sltu-1.18,0,0.0370521,"tive since there are only few unlabelled data available for pretraining. In this paper, we explore two approaches. The first approach is to leverage existing pretrained multilingual models, such as mBART (Liu et al., 2020). While this approach is quite effective, we explore a second approach that leverages positive transfer from related languages (Hu et al., 2020; Khanuja et al., 2021), such as pretraining with a corpus of mostly Indonesian text. We justify this approach through the fact that Sundanese, Javanese, and Indonesian all belong to the same Austronesian language family (Blust, 2013; Novitasari et al., 2020), and share various morphological and semantic features as well as common lexical items through the presence of Sundanese and Javanese loanwords in the Indonesian language (Devianty, 2016). We show that pretraining on mostly Indonesian text achieves competitive performance to the larger multilingual models—despite using 5× fewer parameters and smaller pretraining data—and achieves particularly strong performance on tasks involving the very lowresource Javanese and Sundanese languages. languages in Indonesia, IndoBART and IndoGPT; 3) to the best of our knowledge, we develop the first diverse be"
2021.emnlp-main.699,P02-1040,0,0.113803,"luation metric for each task following the standard evaluation metric on the corresponding task. For machine translation, we report the SacreBLEU (Post, 2018) score. For summarization, we report the ROUGE (Lin, 2004) score. For IndoGPT. We pretrain our IndoGPT model us- QA, the F1 and exact match scores are reported foling an autoregressive language modeling objec- lowing the original SQUAD V2 (Rajpurkar et al., 2018) evaluation metrics. For chit-chat, we report tive (Radford et al., 2019) for 640k iterations on 8 NVIDIA V100 GPUs, with a batch size of 512, both the BLEU and SacreBLEU scores (Papineni et al., 2002). an initial learning rate of 5e-5, and a maximum sequence length of 1024. We apply distributed data We further conduct human evaluation on eight parallelism (DDP) with ZeRO-DP (Rajbhandari tasks, i.e., En ↔ Id (News), Su ↔ Id (Bible), Jv ↔ 8880 Model ID→EN (News) ID→SU (Bible) ID→JV (Bible) EN→ID (News) SU→ID (Bible) JV→ID (Bible) Fluency Adequacy Fluency Adequacy Fluency Adequacy Fluency Adequacy Fluency Adequacy Fluency Adequacy Baseline Ground-truth Scratch 4.4±0.8 3.8±0.9 4.2±0.9 2.8±1.0 4.2±0.8 3.1±0.9 3.7±1.2 2.1±1.1 4.5±0.7 3.3±1.0 4.0±0.9 2.2±1.0 4.7±0.5 3.9±0.9 4.4±0.6 2.7±0.9 4.4±0."
2021.emnlp-main.699,W18-6319,0,0.0137132,"search for the learning rate over the range [1e-3, 1e-4, 5e-5, 1e-5, 5e-6] and report the best results. We report the best hyperparameter settings for each model in Appendix C. 5 Evaluation Procedure For evaluation, we use beam search with a beam width of 5, a length penalty α of 1.0, and limit the maximum sequence length to 512 for all models and all tasks. We conduct both automatic and human evaluations to assess the models. We use a different evaluation metric for each task following the standard evaluation metric on the corresponding task. For machine translation, we report the SacreBLEU (Post, 2018) score. For summarization, we report the ROUGE (Lin, 2004) score. For IndoGPT. We pretrain our IndoGPT model us- QA, the F1 and exact match scores are reported foling an autoregressive language modeling objec- lowing the original SQUAD V2 (Rajpurkar et al., 2018) evaluation metrics. For chit-chat, we report tive (Radford et al., 2019) for 640k iterations on 8 NVIDIA V100 GPUs, with a batch size of 512, both the BLEU and SacreBLEU scores (Papineni et al., 2002). an initial learning rate of 5e-5, and a maximum sequence length of 1024. We apply distributed data We further conduct human evaluation"
2021.emnlp-main.699,P17-1099,0,0.106428,"Missing"
2021.emnlp-main.699,2020.acl-main.704,0,0.0348461,"Missing"
2021.emnlp-main.699,P17-1061,0,0.04508,"Missing"
2021.findings-acl.239,2020.acl-main.18,0,0.0610832,"Missing"
2021.findings-acl.239,2020.acl-main.747,0,0.0286152,"Missing"
2021.findings-acl.239,W17-4715,0,0.0204454,"Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of wo"
2021.findings-acl.239,N19-1423,0,0.0190681,"l et al., 2019), and has become a strong backbone for building NMT systems, especially in a low-resource scenario (Liu et al., 2020a; Song et al., 2019; Lin et al., 2020; Yang et al., 2020; Xue et al., 2020; Fan et al., 2020; Tang et al., 2020). Liu et al. (2020a) pre-trained a Seq2Seq multilingual model (mBART) by denoising full texts in 25 languages, while Lin et al. (2020) proposed multilingual random aligned substitution to pre-train an NMT model for many languages based on parallel data. Instead of pre-training models from scratch, Wang et al. (2020) proposed to extend multilingual BERT (Devlin et al., 2019) to an unseen language and evaluate it on the named entity recognition task. Although many studies have focused on pre-training multilingual models, few have investigated how to adapt the pre-trained models to new languages effectively. Also, to the best of our knowledge, we are the first to explore how to adapt a multilingual model pre-trained in a Seq2Seq fashion to unseen languages and evaluate the methods on a generative task (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient"
2021.findings-acl.239,P17-2090,0,0.0188758,"k (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boost"
2021.findings-acl.239,N18-1032,0,0.0214428,"veloping algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied t"
2021.findings-acl.239,D18-1398,0,0.0178247,"veloping algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied t"
2021.findings-acl.239,D19-1632,0,0.0447464,"Missing"
2021.findings-acl.239,D19-1252,0,0.0506751,"Missing"
2021.findings-acl.239,W13-2233,0,0.0123053,"eq fashion to unseen languages and evaluate the methods on a generative task (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018"
2021.findings-acl.239,W18-6325,0,0.0161677,"insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied the meta-learning approach into the lowresource NMT task, and Baziotis et al. (2020) incorpora"
2021.findings-acl.239,J82-2005,0,0.646612,"Missing"
2021.findings-acl.239,2020.emnlp-main.363,0,0.0302761,"Missing"
2021.findings-acl.239,2020.acl-main.703,0,0.0418317,"• We show that our proposed method can consistently surpass strong baselines across all the tested translation pairs. • We conduct in-depth experiments and analyses in terms of different low-resource settings and the effectiveness on the various components of our method. 2 Methodology In this section, we first give a brief overview of the mBART model (Liu et al., 2020a), and then we introduce our proposed method that aims to adapt mBART to unseen languages in the translation task. 2.1 Model: mBART The mBART model follows the sequence-tosequence (Seq2Seq) pre-training scheme of the BART model (Lewis et al., 2020) (i.e., reconstructing the corrupted text) and is pre-trained on largescale monolingual corpora in 25 languages. Two types of noises are used to produce the corrected text. The first is to remove text spans and replace them with a mask token, and the second is to permute the order of sentences within each instance. Thanks to the large-scale pre-training on multiple diverse languages, the mBART model has shown its strength at building low-resource NMT systems by being fine-tuned to the target language pair, and it is also shown to possess a powerful generalization ability to languages that do n"
2021.findings-acl.239,2020.findings-emnlp.17,0,0.0184855,"ew languages effectively. Also, to the best of our knowledge, we are the first to explore how to adapt a multilingual model pre-trained in a Seq2Seq fashion to unseen languages and evaluate the methods on a generative task (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the paral"
2021.findings-acl.239,2020.emnlp-main.210,0,0.313534,"tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART’s pre-training. The code is available at https://github.com/ zliucr/cpt-nmt. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) has a poor generalization ability to lowresource languages where large monolingual and parallel corpora are not available. Recently, leveraging multilingual pre-trained models (Song et al., 2019; Liu et al., 2020a; Lin et al., 2020) as the starting checkpoints has shown to be effective at building low-resource NMT systems. However, the effectiveness of the pre-training will be vastly limited for low-resource languages that are not in the list of pre-training languages. Given the fact that there are more than 7000 languages around the world (Austin and Sallabank, 2011), it is almost impossible for a multilingual model to include all languages. And it is expensive and time-consuming to pre-train another model from scratch so as to include the languages we need. To address this issue, we propose to leverage the advantages o"
2021.findings-acl.239,N18-2083,0,0.0124426,"s that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied the meta-learning approach in"
2021.findings-acl.239,L16-1147,0,0.0145855,"crease the variety of the mixedlanguage text, and given that there will be plenty of frequent words (e.g., stopwords), replacing all of them with the corresponding translations could make the sentences unnatural, and the translations of the frequent words in lang1 would likely not match the context in lang2 . In addition, adding a probability to delete the original token in function h is to inject extra noise and further increase the diversity of the generated mixed-language text. 3 Experimental Settings 3.1 Datasets We conduct experiments on 12 low-resource language pairs from OpenSubtitles (Lison and Tiedemann, 2016), resulting in 24 directed translation pairs in total. Each pair has an unseen language for mBART. Concretely, there are 12 translation pairs (out of 24) containing English and another unseen language (Indonesian (Id), Ukrainian (Uk), Bengali (Bn), Afrikaans (Af), Tamil (Ta), Thai (Th) ↔ English (En)), and the rest of the 12 pairs contain two unseen languages (Id ↔ Ta, Bn ↔ Th, Bulgarian (Bg) ↔ Ta, Id ↔ Bn, Macedonian (Mk) ↔ Th, and Slovak (Sk) ↔ Swedish (Sv)). In addition, we evaluate the translation pairs (En ↔ Gujarati (Gu) and En ↔ Kazakh (Kk) (WMT19)), where both languages are in mBART’s"
2021.findings-acl.239,2020.tacl-1.47,0,0.196713,"elines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART’s pre-training. The code is available at https://github.com/ zliucr/cpt-nmt. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) has a poor generalization ability to lowresource languages where large monolingual and parallel corpora are not available. Recently, leveraging multilingual pre-trained models (Song et al., 2019; Liu et al., 2020a; Lin et al., 2020) as the starting checkpoints has shown to be effective at building low-resource NMT systems. However, the effectiveness of the pre-training will be vastly limited for low-resource languages that are not in the list of pre-training languages. Given the fact that there are more than 7000 languages around the world (Austin and Sallabank, 2011), it is almost impossible for a multilingual model to include all languages. And it is expensive and time-consuming to pre-train another model from scratch so as to include the languages we need. To address this issue, we propose to lever"
2021.findings-acl.239,W19-5327,1,0.845664,"019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied the meta-learning approach into the lowresource NMT task, and Baziotis et al. (2020) incorporated a language model prior to regularize the output distribution of the translation model. Pretraining a"
2021.findings-acl.239,D15-1166,0,0.0397297,"target languages, and then, we continue pretraining mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the finetuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART’s pre-training. The code is available at https://github.com/ zliucr/cpt-nmt. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) has a poor generalization ability to lowresource languages where large monolingual and parallel corpora are not available. Recently, leveraging multilingual pre-trained models (Song et al., 2019; Liu et al., 2020a; Lin et al., 2020) as the starting checkpoints has shown to be effective at building low-resource NMT systems. However, the effectiveness of the pre-training will be vastly limited for low-resource languages that are not in the list of pre-training languages. Given the fact that there are more than 7000 languages around the world (Austin and Sallabank, 2011),"
2021.findings-acl.239,P02-1040,0,0.10908,"Missing"
2021.findings-acl.239,W19-6204,0,0.041946,"Missing"
2021.findings-acl.239,P16-1009,0,0.0354677,"ods on a generative task (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT"
2021.findings-acl.239,2020.acl-main.252,0,0.0201075,"2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied the meta-learning approach into the lowresource NMT task, and Baziotis et al. (2020) incorporated a language model prior to regularize the output distribution of the translation model. Pretraining a multilingual Seq2Seq model (Liu et al., 2020a; Lin et al., 2020) allows it to be directly finet"
2021.findings-acl.239,2020.findings-emnlp.240,0,0.0262751,"Missing"
2021.findings-acl.239,2021.calcs-1.20,1,0.74028,"ledge, we are the first to explore how to adapt a multilingual model pre-trained in a Seq2Seq fashion to unseen languages and evaluate the methods on a generative task (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and"
2021.findings-acl.239,P19-1579,0,0.0128071,"ks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to improve the low-resource NMT performance by boosting the quality of word alignments. Gu et al. (2018b) applied the meta-learning approach into the lowresource NMT task, and Baziotis et al. (2020) incorporated a language model prior to regularize the output distribution of the translation mo"
2021.findings-acl.239,2020.emnlp-main.208,0,0.0610261,"Missing"
2021.findings-acl.239,2021.naacl-main.471,1,0.778791,"Missing"
2021.findings-acl.239,D16-1163,0,0.0280428,"d evaluate the methods on a generative task (the NMT task). 5.2 Low-Resource Machine Translation Recently, developing algorithms that are able to cope with the scenario where the training data are insufficient have become an interesting and popular research topic across a variety of tasks (Chen et al., 2019a,b, 2020; Brown et al., 2020; Liu et al., 2020c; Lauscher et al., 2020; Winata et al., 2020; Liu et al., 2020b; Peng et al., 2020; Liu et al., 2020d; Yu et al., 2021; Winata et al., 2021). Low-resource machine translation systems (Vandeghinste et al., 2007; Irvine and Callison-Burch, 2013; Zoph et al., 2016; Sennrich et al., 2016; Fadaee et al., 2017; Currey et al., 2017; Imankulova et al., 2017; Gu et al., 2018a; Pourdamghani et al., 2018; Gu et al., 2018b; Lample et al., 2018a,c; Kocmi and Bojar, 2018; Artetxe et al., 2018; Lakew et al., 2018; Imankulova et al., 2019a; Xia et al., 2019; Liu et al., 2019; Guzm´an et al., 2019; Imankulova et al., 2019b; Stickland et al., 2020; Siddhant et al., 2020) alleviated the parallel data scarcity issue for low2713 resource languages and improve the models’ generalization ability for low-resource language pairs. Pourdamghani et al. (2018) proposed to impro"
2021.findings-acl.275,D16-1264,0,0.0267479,"raditional interrogations. dataset, and in the second stage, we fine-tune our QFS-BART model with QFS datasets. All the parameters in the model are initialized from the first stage. In order to make the model capture both query relevance and answer relevance, the input text is formatted in the following way: [CLS] document [SEP] query. The answer relevance attention score for the document is generated by the QA model, and we take the maximum number in the document as the attention score for all the words in the query. 4 Experimental Setup Datasets We use multiple QA datasets, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018) and NaturalQuestions (Kwiatkowski et al., 2019) to train HLTCMRQA, following Su et al. (2019). We evaluate our model on the Debatepedia dataset (Nema et al., 2017) and DUC2005-7 dataset (in Appendix). Training Details For all the experiments, we use the BART-large version to implement our models. We use a mini-batch size of 32 and train all the models on one V100 16G. During decoding, we use beam search with the beam size of 4. We decode until an end-of-sequence token is"
2021.findings-acl.275,2020.emnlp-main.296,0,0.17675,"sential information from a source document(s) and organize it into a summary that can answer a query (Dang, 2005). The input can be either a single document that has multiple views or multiple documents that contain multiple topics, and the output summary should be focused on the given query. QFS has various applications (e.g., a personalized search engine that provides the user with an overview summary based on their query (Su et al., 2020b)). Early work on the QFS task mainly focused on generating extractive summaries (Davis et al., 2012; Daum´e III and Marcu, 2006; Feigenblat et al., 2017; Xu and Lapata, 2020b), which may contain unreadable sentence ordering and lack cohesiveness. ∗ The two authors contribute equally. The code is released at: https://github.com/ HLTCHKUST/QFS 1 Other work on abstractive QFS incorporated the query relevance into existing neural summarization models (Nema et al., 2017; Baumel et al., 2018). The closest work to ours was done by (Su et al., 2020a) and (Xu and Lapata, 2020a,b), who leveraged an external question answering (QA) module in a pipeline framework to take into consideration the answer relevance of the generated summary. However, they only used QA as distant s"
2021.findings-acl.275,N19-4013,0,0.0404834,"Missing"
2021.findings-acl.275,D18-1259,0,0.0220876,"s in the model are initialized from the first stage. In order to make the model capture both query relevance and answer relevance, the input text is formatted in the following way: [CLS] document [SEP] query. The answer relevance attention score for the document is generated by the QA model, and we take the maximum number in the document as the attention score for all the words in the query. 4 Experimental Setup Datasets We use multiple QA datasets, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018) and NaturalQuestions (Kwiatkowski et al., 2019) to train HLTCMRQA, following Su et al. (2019). We evaluate our model on the Debatepedia dataset (Nema et al., 2017) and DUC2005-7 dataset (in Appendix). Training Details For all the experiments, we use the BART-large version to implement our models. We use a mini-batch size of 32 and train all the models on one V100 16G. During decoding, we use beam search with the beam size of 4. We decode until an end-of-sequence token is emitted and early stop when the generated summary reaches to 48 tokens. 5 Results & Analysis We compare our proposed QFS-BA"
2021.findings-acl.275,2021.naacl-main.471,1,0.691147,"Missing"
2021.mrl-1.1,Q19-1038,0,0.0200543,"on the few-shot learning, where the source language and target language are German and English, respectively. and the performance increases as the models are given more samples. We conjecture that pre-trained models may be able to adapt to languages that are similar to English. However, for many language tasks, it is difficult to collect a large supervised training dataset as language experts (e.g., linguists or native speakers) are required to annotate the data. Another line of work is to apply cross-lingual transfer on English with the same task as the target languages (Ponti et al., 2018; Artetxe and Schwenk, 2019; Liu et al., 2019b; Lauscher et al., 2020; Liu et al., 2020b, 2021c; Chen et al., 2021). However, such methods still need to apply a fine-tuning step to update the model for fast adaptation, which can be challenging for large pre-trained models – some models require substantial memory capacity – since the models have to be trained on highperforming machines. Different from the aforementioned method, in-context learning using an LM does not allow any parameter updates. Thus, the process does not need to compute and store the gradients for backward propagation. In this work, we investigate the"
2021.mrl-1.1,2020.emnlp-main.697,0,0.0750431,"Missing"
2021.mrl-1.1,2021.acl-long.295,0,0.0505251,"Missing"
2021.mrl-1.1,2021.repl4nlp-1.4,0,0.0340213,"ero-shot experiments on the MTOP and MultiNLU datasets. 7 Figure 3: The results on German (de) MTOP dataset with GPT models. Figure 4: The results on English (en) MTOP dataset with GPT models. Figure 5: The results on Spanish (es) MTOP dataset with GPT models. Figure 6: The results on French (fr) MTOP dataset with GPT models. Figure 7: The results on English (en) multilingual NLU dataset with GPT models. Figure 8: The results on Spanish (es) multilingual NLU dataset with GPT models. 6.2 Pre-trained Language Models et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020; Goyal et al., 2021), decoder-only models, such as GPT models (Radford et al., 2019; Brown et al., 2020) and encoder-decoder models, such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in"
2021.mrl-1.1,2020.acl-main.747,0,0.267705,"oken limit. We utilize an NVIDIA Tesla V100 16GB GPU to run the inference so that the model is ensured to fit in a single GPU, and we use 16-bit precision. Model Name Model details We run experiments on a variety of publicly available models:3 four sizes of GPT-2 models (0.1B, 0.3B, 0.8B and 1.6B), three sizes of GPTNEO models (1.3B, 2.7B, and 6B), and two sizes of T5 models (0.8B and 3B). Table 3 shows the details of each pre-trained model. Baselines We use the same sets of few-shot samples for the baselines. We run fine-tuning on the pre-trained models mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), and also compare our models with the zero-shot cross-task models using pre-trained models XLM-R, fine-tuned on XNLI (Conneau et al., 2018), and BART, finenparams nlayers nhidden nffn GPT-2 GPT-2MEDIUM GPT-2LARGE GPT-2XL GPTNEO GPTNEO GPTNEO-J 0.1B 0.3B 0.8B 1.6B 1.3B 2.7B 6B 12 24 36 48 24 32 28 768 768 1,280 1,600 2,048 2,560 4096 16,384 T5LARGE T53B 0.8B 3B 24 24 1,024 1,024 4,096 16,384 Table 3: Model architecture. 4 The XLM-R model fine-tuned with XNLI data can be accessed at https://huggingface.co/joeddav/ xlm-roberta-large-xnli. The BART model finetuned with MNLI data can be accessed a"
2021.mrl-1.1,P18-1031,0,0.0238954,"h GPT models. Figure 8: The results on Spanish (es) multilingual NLU dataset with GPT models. 6.2 Pre-trained Language Models et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020; Goyal et al., 2021), decoder-only models, such as GPT models (Radford et al., 2019; Brown et al., 2020) and encoder-decoder models, such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffe"
2021.mrl-1.1,2020.emnlp-main.363,0,0.0420681,"Missing"
2021.mrl-1.1,D18-1269,0,0.0228577,"-bit precision. Model Name Model details We run experiments on a variety of publicly available models:3 four sizes of GPT-2 models (0.1B, 0.3B, 0.8B and 1.6B), three sizes of GPTNEO models (1.3B, 2.7B, and 6B), and two sizes of T5 models (0.8B and 3B). Table 3 shows the details of each pre-trained model. Baselines We use the same sets of few-shot samples for the baselines. We run fine-tuning on the pre-trained models mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), and also compare our models with the zero-shot cross-task models using pre-trained models XLM-R, fine-tuned on XNLI (Conneau et al., 2018), and BART, finenparams nlayers nhidden nffn GPT-2 GPT-2MEDIUM GPT-2LARGE GPT-2XL GPTNEO GPTNEO GPTNEO-J 0.1B 0.3B 0.8B 1.6B 1.3B 2.7B 6B 12 24 36 48 24 32 28 768 768 1,280 1,600 2,048 2,560 4096 16,384 T5LARGE T53B 0.8B 3B 24 24 1,024 1,024 4,096 16,384 Table 3: Model architecture. 4 The XLM-R model fine-tuned with XNLI data can be accessed at https://huggingface.co/joeddav/ xlm-roberta-large-xnli. The BART model finetuned with MNLI data can be accessed at https:// huggingface.co/facebook/bart-large-mnli 3 The models except GPTNEO-J are taken from https://huggingface.co/. The GPTNEO-J model i"
2021.mrl-1.1,2020.lrec-1.302,0,0.072522,"rms as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such a"
2021.mrl-1.1,2020.acl-main.703,0,0.225711,"on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility of conducting few-shot learning, that is, learning a new task using a small number of examples without any further training or gradient computation. Few-shot learning alleviates the cost for extensive labeled data, which is beneficial since collecting high-quality labeled data is resource-intensive and expensive. It also reduces the cost for model fine-tuning, which requires tremendous GPU or TPU resources. Fewshot learning can be seen as a one-for-all plugand-play computational model that can be applied to various natural la"
2021.mrl-1.1,N19-1423,0,0.561013,"l., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility of conducting few-shot learning, that is, learning a new task using a small number of examples without any further training or gradient computation. Few-shot learning alleviates the cost for extensive labeled data, which is beneficial since collecting high-quality labeled data is resource-intensive and expensive. It also reduces the cost for model fine-tuning, which requires tremendous GPU or TPU resources. F"
2021.mrl-1.1,2021.ccl-1.108,0,0.0558726,"Missing"
2021.mrl-1.1,2021.eacl-main.257,0,0.0383577,"Missing"
2021.mrl-1.1,D19-1129,1,0.876253,"Missing"
2021.mrl-1.1,2021.acl-long.353,0,0.0468921,"Missing"
2021.mrl-1.1,P19-1301,0,0.0260588,"2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has l"
2021.mrl-1.1,2021.naacl-main.448,1,0.812595,"Missing"
2021.mrl-1.1,2021.repl4nlp-1.13,1,0.918187,"the existing state-of-the-art cross-lingual models and translation models. 1 Figure 1: Accuracy vs. model size on English-Spanish MNLU dataset. Cross-lingual in-context learning with LMs (i.e., context with few English examples tested on Spanish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 202"
2021.mrl-1.1,2020.findings-emnlp.215,1,0.907069,"han random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models. 1 Figure 1: Accuracy vs. model size on English-Spanish MNLU dataset. Cross-lingual in-context learning with LMs (i.e., context with few English examples tested on Spanish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster"
2021.mrl-1.1,2020.emnlp-main.273,1,0.676661,"T5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different languages and explores the possibility of conducting efficient inference on low-resource tasks. We find that LMs can predict samples correctly, significantly better than the random prediction, in cross-lingual tasks with no training examples of the target languages. We would like to investi"
2021.mrl-1.1,2020.tacl-1.47,0,0.287468,"text, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models. 1 Figure 1: Accuracy vs. model size on English-Spanish MNLU dataset. Cross-lingual in-context learning with LMs (i.e., context with few English examples tested on Spanish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Ng"
2021.mrl-1.1,2020.findings-emnlp.92,0,0.0343562,"Missing"
2021.mrl-1.1,W96-0200,0,0.743872,"Missing"
2021.mrl-1.1,2021.naacl-main.185,0,0.261269,"Missing"
2021.mrl-1.1,N19-1380,0,0.379572,"al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et"
2021.mrl-1.1,N18-1202,0,0.449595,"t al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility of conducting few-shot learning, that is, learning a new task using a small number of examples without any further training or gradient computation. Few-shot learning alleviates the cost for extensive labeled data, which is beneficial since collecting high-quality labeled data is resource-intensive and expensive. It also reduces the cost for model fine-tuning, which requires tremendous GP"
2021.mrl-1.1,2020.emnlp-main.346,0,0.0355019,"al., 2020b; Madotto et al., 2020b; Zhao et al., 2021; Schick and Schütze, 2021; Lin et al., 2021a). In this approach, we select the appropriate prompts to trigger the LMs to behave so that they can predict the desired output (Liu et al., 2021b). However, the prompts have to be engineered to allow the LM to generate a text appropriate to solve the task. Learning to calibrate the few-shot results is also essential to reduce the model’s performance variance (Zhao et al., 2021), and the selection criteria in choosing the prompts are also important (Perez et al., 2021). In another stream of work, Shin et al. (2020); Li and Liang (2021) proposed an automated method to create prompts for a diverse set of tasks by gradient-based tuning instead of manually searching for a good prompt. Using such a method, may allow us to find an optimal prompt easier, it is very difficult to discover the optimal prompts for complicated natural language processing tasks, such as semantic parsing (Liu et al., 2021b). Ablation Study To further understand how much data we need for the in-context learning, we conduct experiments with different numbers of few-shot samples, including zero-shot experiments on the MTOP and MultiNLU"
2021.mrl-1.1,D19-1250,0,0.07384,"Missing"
2021.mrl-1.1,2020.aacl-main.85,1,0.749893,"ish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English"
2021.mrl-1.1,2020.emnlp-main.617,0,0.0549595,"Missing"
2021.mrl-1.1,D18-1026,0,0.0444186,"Missing"
2021.mrl-1.1,N18-1101,0,0.0485174,"± 3.82 68.89 ± 2.51 63.33 ± 7.14 38.97 ± 14.80 80.12 ± 3.95 61.51 ± 1.63 63.10 ± 4.46 86.60 ± 2.40 mBERT 0.2B XLM-RBASE 0.3B 88.57 ± 3.14 87.95 ± 1.39 25.21 ± 2.31 27.47 ± 11.90 16.54 ± 5.54 13.8 ± 6.50 84.88 ± 1.59 77.06 ± 3.16 87.87 ± 3.29 74.85 ± 1.53 Zero-shot Cross-Task Prediction 43.41 53.37 36.06 51.67 Few-shot Learning (K-shot) Fine-tuning (40-shot) 41.44 ± 5.59 37.03 ± 5.11 33.82 ± 10.08 27.16 ± 5.51 Table 2: Zero-shot and few-shot results in the monolingual setting. The SOTA results are taken from † Li et al. (2021), ‡ Qin et al. (2019), and ∗ Schuster et al. (2019). tuned on MNLI (Williams et al., 2018);4 a random baseline; and state-of-the-art results reported on each dataset. For the finetuning, we use a learning rate of 5e-5 with a decay of 0.9 for every epoch, and a batch size of 32. We apply an early stopping after 5 epochs without any improvement on the validation set. with the English context (en→XX). In the few-shot in-context learning, we use k-way-few-shot classification, taking k samples. For each model, we take k ∈ [0, 5, K], where K ≤ 40 is the largest number of few-shot samples that can be passed to the model as input and is divisible by 10 without exceeding the maximum input t"
2021.mrl-1.1,D19-1214,0,0.0249661,"± 1.87 50.77 ± 4.41 62.71 ± 6.30 66.97 ± 1.35 50.70 ± 2.47 55.91 ± 3.82 68.89 ± 2.51 63.33 ± 7.14 38.97 ± 14.80 80.12 ± 3.95 61.51 ± 1.63 63.10 ± 4.46 86.60 ± 2.40 mBERT 0.2B XLM-RBASE 0.3B 88.57 ± 3.14 87.95 ± 1.39 25.21 ± 2.31 27.47 ± 11.90 16.54 ± 5.54 13.8 ± 6.50 84.88 ± 1.59 77.06 ± 3.16 87.87 ± 3.29 74.85 ± 1.53 Zero-shot Cross-Task Prediction 43.41 53.37 36.06 51.67 Few-shot Learning (K-shot) Fine-tuning (40-shot) 41.44 ± 5.59 37.03 ± 5.11 33.82 ± 10.08 27.16 ± 5.51 Table 2: Zero-shot and few-shot results in the monolingual setting. The SOTA results are taken from † Li et al. (2021), ‡ Qin et al. (2019), and ∗ Schuster et al. (2019). tuned on MNLI (Williams et al., 2018);4 a random baseline; and state-of-the-art results reported on each dataset. For the finetuning, we use a learning rate of 5e-5 with a decay of 0.9 for every epoch, and a batch size of 32. We apply an early stopping after 5 epochs without any improvement on the validation set. with the English context (en→XX). In the few-shot in-context learning, we use k-way-few-shot classification, taking k samples. For each model, we take k ∈ [0, 5, K], where K ≤ 40 is the largest number of few-shot samples that can be passed to the model"
2021.mrl-1.1,2021.calcs-1.20,1,0.721866,"T (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different langua"
2021.mrl-1.1,K19-1026,1,0.830093,", 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility"
2021.mrl-1.1,2020.emnlp-main.409,0,0.0235976,"2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different languages and explores the possibility of conducting efficient inference on low-resource tasks. We find that LMs can predict samples correctly, significantly better than the random prediction, in cross-lingual tasks with no training examples of the target languages. We would like to investigate further the applicability of this method"
2021.mrl-1.1,2020.emnlp-main.410,0,0.0336126,"Missing"
2021.mrl-1.1,2021.naacl-main.41,0,0.0260138,"U dataset with GPT models. 6.2 Pre-trained Language Models et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020; Goyal et al., 2021), decoder-only models, such as GPT models (Radford et al., 2019; Brown et al., 2020) and encoder-decoder models, such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a;"
2021.mrl-1.1,2020.emnlp-main.272,0,0.0328713,"ems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different languages and explores the possibility of conducting efficient inference on low-resource tasks. We find that LMs can predict samples correctly, significantly better than the random prediction, in cross-lingual tasks with no training examples of the target languages. We would like to investigate further the applicability of this method to other tasks and languages in future work. Acknowledgment We want to thank Bryan Wilie and Samuel Cahyawijaya"
2021.naacl-main.158,W18-5513,0,0.0509333,"Missing"
2021.naacl-main.158,N19-1423,0,0.0121606,"d “Refute”/“Not Enough Info” is mapped into Unsupported to match our task setting. Note that to balance the dataset, we obtain half the data from “Refute” and the other half from “Not Enough Info”. Note that the gold evidence is included in the dataset released by Thorne et al. 5.2 Models Ours We consider one unidirectional LM and one masked LM for our proposed perplexity-based methodology. • PPLGPT2-B – Our single-parameter classifier based on perplexity from GPT2-base (Radford et al., 2019) (unidirectional LM) • PPLBERT-B – Our single-parameter classifier based on perplexity from BERT-base (Devlin et al., 2019) (Masked LM) Baselines We finetune various pre-trained Transformer-based (Vaswani et al., 2017) models to build our baseline classifiers, which is a common approach used to achieve many state-of-the-art results in the literature. • Major Class – A simple majority classifier which always assigns the majority class of the training set to all samples. We provide this for reference because some of our dataset classes are imbalanced. 5.3 Experimental Setup Few-Shot Data Setup Given ND as the size of the dataset D, we do an n-shot experiment with n samples from D as a “validation set” for our perple"
2021.naacl-main.158,2020.acl-main.398,0,0.0228133,"Missing"
2021.naacl-main.158,karadzhov-etal-2017-fully,0,0.0584555,"Missing"
2021.naacl-main.158,2020.fever-1.5,1,0.779876,"knowledge from its training corpus, there are a few limitations to solely relying on the pre-trained weights. First, we cannot easily check and guarantee whether the LM has already seen the evidence that is required for verification, and the LM would definitely not have seen the evidence related to newly emerging events after the LM pretraining. For instance, the event of COVID-19 emerged after the release of the GPT2 pre-trained model. Second, although LMs have shown surprising ability in memorizing some knowledge, they are not perfect, as pointed out by previous works (Poerner et al., 2019; Lee et al., 2020). Therefore, we propose to incorporate evidence into the perplexity calculation by using it as a prefix of the claim. There are two popular kinds of LMs: i) unidirectional LMs that are trained with the conventional next token prediction task, and ii) masked LMs that are trained with the masked token prediction token, resulting in a bidirectional LM. We briefly describe how to obtain the evidence-conditioned perplexity for both types of LM: Unidirectional Language Model Perplexity For a unidirectional LM, first we concatenate the evidence and claim to obtain the input to the LM: X = {xe0 , . ."
2021.naacl-main.158,D18-1003,0,0.0341158,"Missing"
2021.naacl-main.158,2021.ccl-1.108,0,0.0703809,"Missing"
2021.naacl-main.158,P19-1244,0,0.0330818,"Missing"
2021.naacl-main.158,D19-1250,0,0.161028,"Going further, Brown et al. illustrated the impressive potential of LMs as strong zero-shot and fewshot learners across translation, commonsense reasoning and natural language inference (NLI). However, little or no exploration has been made on fewshot learning in the fact-checking domain, which is a timely and important task in which data-scarcity is particularly problematic. Previous works have proposed different ways of leveraging LMs to conduct zero- or few-shot learning. One common approach is to query the LM for the missing token (i.e., “answer”) for the zeroshot question-answering task (Petroni et al., 2019; 1 Introduction Brown et al., 2020) by transforming questions into Few-shot learning is being actively explored to a form of statement. Another approach is to adopt overcome the heavy dependence on large-scale la- an in-context learning approach where the input beled data that serves as a crucial bottleneck to context of the LM is carefully crafted to control machine learning models. Recently, researchers the output. For example, a natural language task have explored few-shot learning that leverages the instruction (e.g., “Translate English to French:”) or powerful transfer learning ability o"
2021.naacl-main.158,2020.emnlp-main.437,0,0.0376932,"Missing"
2021.naacl-main.158,2020.acl-main.240,0,0.0531823,"Missing"
2021.naacl-main.158,N18-1074,0,0.0224053,"Missing"
2021.naacl-main.158,N19-1230,0,0.0223888,"Missing"
2021.naacl-main.158,W18-5515,0,0.0431353,"Missing"
2021.naacl-main.158,2020.acl-main.549,0,0.031193,"Missing"
2021.naacl-main.158,P19-1085,0,0.0247678,"Missing"
2021.naacl-main.417,2020.aacl-main.30,1,0.663994,"ing state-of-the-art models which are based on the two-phase pipeline. Moreover, the incorporation of the sparse cross-modal attention and sparse CNN is able to greatly reduce the computational cost and maintain the performance. We summarize our contributions as follows. • To the best of our knowledge, we are the first to apply a fully end-to-end trainable model for the multimodal emotion recognition task. • We restructure the existing multimodal emotion recognition datasets to enable the end-toend training and cross-modal attention based on the raw data. Zhang and Liu, 2017; Xu et al., 2020; Dai et al., 2020b). In recent years, there is a trend to leverage multimodal information to tackle these research tasks, such as emotion recognition (Busso et al., 2008), sentiment analysis (Zadeh et al., 2016, 2018b), personality trait recognition (Nojavanasghari et al., 2016), etc, have drawn more and more attention. Different methods have been proposed to improve the performance and crossmodal interactions. In earlier works, early fusion (Morency et al., 2011; Pérez-Rosas et al., 2013) and late fusion (Zadeh et al., 2016; Wang et al., 2017) of modalities were widely adopted. Later, more complex approaches"
2021.naacl-main.417,2020.semeval-1.272,1,0.880903,"ing state-of-the-art models which are based on the two-phase pipeline. Moreover, the incorporation of the sparse cross-modal attention and sparse CNN is able to greatly reduce the computational cost and maintain the performance. We summarize our contributions as follows. • To the best of our knowledge, we are the first to apply a fully end-to-end trainable model for the multimodal emotion recognition task. • We restructure the existing multimodal emotion recognition datasets to enable the end-toend training and cross-modal attention based on the raw data. Zhang and Liu, 2017; Xu et al., 2020; Dai et al., 2020b). In recent years, there is a trend to leverage multimodal information to tackle these research tasks, such as emotion recognition (Busso et al., 2008), sentiment analysis (Zadeh et al., 2016, 2018b), personality trait recognition (Nojavanasghari et al., 2016), etc, have drawn more and more attention. Different methods have been proposed to improve the performance and crossmodal interactions. In earlier works, early fusion (Morency et al., 2011; Pérez-Rosas et al., 2013) and late fusion (Zadeh et al., 2016; Wang et al., 2017) of modalities were widely adopted. Later, more complex approaches"
2021.naacl-main.417,D14-1162,0,0.0868421,"2 45.5 47.5 48.3 57.1 62.1 63.3 61.4 20.6 24.2 24.0 25.6 63.1 64.1 64.2 65.4 43.3 44.4 44.2 45.2 FE2E MESM (0.5) 8.65 4.34 67.0 66.8 49.6 49.3 77.7 75.6 57.1 56.4 63.8 65.8 26.8 28.9 65.4 64.1 72.6 72.3 65.2 63.0 49.0 46.6 66.7 65.7 29.1 27.2 67.6 66.8 47.4 46.8 Table 4: The results on the CMU-MOSEI dataset. WAcc stands for weighted accuracy. We report the accuracy and the F1-score on six emotion categories: angry, disgusted, fear, happy, sad and surprised. We re-run the models marked by † , as the data we use is unaligned along the sequence length dimension and the split is different. GloVe (Pennington et al., 2014) word embeddings (glove.840B.300d7 ). Multimodal Learning As different modalities are unaligned in the data, we cannot compare our method with existing works that can only handle aligned input data. We use four multimodal learning models as baselines: the late fusion LSTM (LF-LSTM) model, the late fusion Transformer (LF-TRANS) model, the Emotion Embeddings (EmoEmbs) model (Dai et al., 2020a), and the Multimodal Transformer (MulT) model (Tsai et al., 2019). They receive the hand-crafted features extracted from the first step as input and give the classification decisions. are reported in Append"
2021.naacl-main.417,P13-1096,0,0.0331876,"on datasets to enable the end-toend training and cross-modal attention based on the raw data. Zhang and Liu, 2017; Xu et al., 2020; Dai et al., 2020b). In recent years, there is a trend to leverage multimodal information to tackle these research tasks, such as emotion recognition (Busso et al., 2008), sentiment analysis (Zadeh et al., 2016, 2018b), personality trait recognition (Nojavanasghari et al., 2016), etc, have drawn more and more attention. Different methods have been proposed to improve the performance and crossmodal interactions. In earlier works, early fusion (Morency et al., 2011; Pérez-Rosas et al., 2013) and late fusion (Zadeh et al., 2016; Wang et al., 2017) of modalities were widely adopted. Later, more complex approaches were proposed. For example, Zadeh et al. (2017) introduced the Tensor Fusion Network to model the interactions of the three modalities by performing the Cartesian product, while (Wang et al., 2019) used an attention gate to shift the words using the visual and acoustic features. In addition, based on the Transformer (Vaswani et al., 2017), Tsai et al. (2019) introduced the Multimodal Transformer to improve the performance given unaligned multimodal data, and Rahman et al."
2021.naacl-main.417,2020.acl-main.214,0,0.852411,"utation in the feature extraction part. Hand Crafted Fully End2End Sparse End2End MFCC Prosody Glottal Source ... Figure 1: An illustration of feature extraction from hand-crafted model (left), fully end-to-end model (middle), and sparse end-to-end model (right). The red dots represent the keypoints extracted by hand-crafted models. The areas formed by red lines represent the regions of interest that are processed by (sparse) end-to-end models to extract the features. In the existing works, we discover that a twophase pipeline is generally used (Zadeh et al., 2018a,b; Tsai et al., 2018, 2019; Rahman et al., 2020). In the first phase, given raw input data, feature representations are extracted with hand-crafted algorithms for each modality separately, while in the second phase, end-to-end multimodal learning 1 Introduction is performed using extracted features. However, there are three major defects of this two-phase Humans show their characteristics through not only pipeline: 1) the features are fixed after extraction the words they use, but also the way they speak and and cannot be further fine-tuned on target tasks; 2) their facial expressions. Therefore, in multimodal affective computing tasks, suc"
2021.naacl-main.417,P17-1142,0,0.0153154,"portion of the probability mass in each attention score map (p is a pre-defined hyper-parameter in the range of (0, 1]). In Mns , the points selected by the Nucleus Sampling are set to one and the others are set to zero. Then, we do broadcast point-wise multiplication between Mns and M to generate the output Mo . Therefore, Mo is a sparse tensor with some positions being zero, and the degree of sparsity is controlled by p. 4.3.2 Experiments Sparse CNN Evaluation Metrics Weighted Accuracy Similar to existing works (Zadeh et al., 2018b; Akhtar et al., 2019), we use the weighted accuracy (WAcc) (Tong et al., 2017) to evaluate the CMU-MOSEI dataset, which contains many more negative samples than positive ones on each emotion category. If normal accuracy is used, a model will still get a fine score when predicting all samples to be negative. The formula of the weighted accuracy is WAcc. = T P × N/P + T N , 2N in which P means total positive, TP true positive, N total negative, and TN true negative. 5.2 Baselines For our baselines, we use a two-phase pipeline, which consists of a feature extraction step and an end-to-end learning step. We use the submanifold sparse CNN (Graham and van der Maaten, 2017) af"
2021.naacl-main.417,P19-1656,0,0.473715,"to improve the performance and crossmodal interactions. In earlier works, early fusion (Morency et al., 2011; Pérez-Rosas et al., 2013) and late fusion (Zadeh et al., 2016; Wang et al., 2017) of modalities were widely adopted. Later, more complex approaches were proposed. For example, Zadeh et al. (2017) introduced the Tensor Fusion Network to model the interactions of the three modalities by performing the Cartesian product, while (Wang et al., 2019) used an attention gate to shift the words using the visual and acoustic features. In addition, based on the Transformer (Vaswani et al., 2017), Tsai et al. (2019) introduced the Multimodal Transformer to improve the performance given unaligned multimodal data, and Rahman et al. (2020) introduced a multimodal adaptation gate to integrate visual and acoustic information into a large pre-trained language model. However, unlike some other multimodal tasks (Chen et al., 2017; Yu et al., 2019; Li et al., 2019) using fully end-to-end learning, all of these methods require a feature extraction phase using hand-crafted algorithms (details in Section 5.2), which makes the whole approach a twophase pipeline. 3 Dataset Reorganization The fully end-to-end multimoda"
2021.naacl-main.417,P18-1208,0,0.0276566,"Missing"
2021.naacl-main.432,D19-1565,0,0.0218031,"e sources of data, which improves the robustness of the model to different types of misinformation. Therefore, we carry out experiments to evaluate the generalization ability of U NIFIED M2 representation to unseen misinformation (i) tasks/datasets and (ii) events. The first experiment is about fast adaption ability (few-shot training) to handle a new task/dataset, whereas the second experiment is about the model’s ability to perform well on events unseen during training. 4.1 Unseen Task/Dataset Generalizability Dataset We evaluate using the following four unseen datasets: P ROPAGANDA (Da San Martino et al., 2019), which contains 21,230 propaganda and non-propaganda sentences, with the propaganda sentences annotated by fine-grained propaganda technique labels, such as “Name calling” and “Appeal to fear”; P OLITI FACT (Shu et al., 2019), which contains 91 true and 91 fake news articles collected from PolitiFact’s fact-checking platform; B UZZ F EED (Shu et al., 2019), which contains 120 true and 120 fake news headlines collected from BuzzFeed’s fact-checking platform; and C OVID T WITTER (Alam et al., 2020), which contains 504 COVID-19-related tweets. For our experiment, we use two of the annotations: 1"
2021.naacl-main.432,P19-1441,0,0.0225539,"ion datasets we train on with U NIFIED M2. eralizability of our proposed approach to unseen tasks/datasets and events. This is highly applicable to real-world use cases, where obtaining new misinformation labels is costly and systems often wish to take down misinformation in real time. Our experimental results indicate that our unified representation has better generalization ability over other baselines. are different, we over-sample from the smaller datasets to make the training examples roughly equal. The second step is to fine-tune each taskspecific heads again, similarly to the MT-DNN by Liu et al. (2019a), to obtain the results reported in Table 2 and Table 4. 2 Here, we provide experimental details (dataset, baselines, experimental setups) and results that empirically show the success of the proposed U NIFIED M2 model. U NIFIED M2 In this section, we describe the architecture and the training details for our proposed U NIFIED M2 model. 2.1 Architecture Our proposed model architecture is a hard-parameter sharing multi-task learning model (Ruder, 2017), where a single shared RoBERTa (Liu et al., 2019b) encoder is used across all tasks. RoBERTa is a Transformer encoder pretrained with a masked"
2021.naacl-main.432,2021.ccl-1.108,0,0.0857064,"Missing"
2021.naacl-main.432,I17-2043,0,0.05714,"Missing"
2021.naacl-main.432,D19-1664,0,0.015135,"heads. During inference time, we only use the classification head relevant to the inference-time task. The overall architecture of the model is shown in Figure 1. 2.2 Training 3 3.1 Experiment Misinformation Tasks/Dataset Table 1 lists the four misinformation tasks/datasets we use to train U NIFIED M2. They span various granularities and domains (articles, sentences, headlines and tweets) as well as various objectives (classifying veracity, bias and clickbaity-ness). N EWS B IAS A task to classify whether a given sentence from a news article contains political bias or not. We adapt the BASIL (Fan et al., 2019) dataset, which has bias-span annotations for lexical and informational bias within news articles. Using this dataset, we also include two auxiliary tasks related to political-bias detection: 1) bias type classification – given a biased sentence, the type of the bias (lexical vs informational) is classified; and 2) polarity detection – given a biased sentence, its polarity (positive, negative, neutral) is determined. FAKE N EWS An article-level fake news detection task that leverages the Webis (Potthast et al., 2018) dataset annotated by professional journalists. Our model training process con"
2021.naacl-main.432,P18-1184,0,0.0683527,"Missing"
2021.naacl-main.432,C18-1288,0,0.116892,"ores from leaveone-event-out cross-validation setup for RUMOR task. task-specific models (ST average). As shown in Table 4, our U NIFIED M2 encoder can quickly adapt to new tasks, even with very little in-domain data. While both the single-task models and U NIFIED M2 significantly outperform vanilla RoBERTa, U NIFIED M2 further outperforms the single-task models, indicating that multi-task learning can aid task generalizability. 4.2 Unseen Event Generalizability Dataset We use the previously introduced RU MOR dataset, which includes nine separate events, for this experiment. A group of works (Kochkina et al., 2018; Li et al., 2019; Yu et al., 2020) have used this dataset in a leave-one-event-out crossvalidation setup (eight events for training and one event for testing) to take event generalizability into consideration in their model evaluation. We conduct a supplementary experiment following this evaluation setup for the completeness of our analysis. Experiment First, we train the U NIFIED M2 encoder without RUMOR data, and then fine-tune and evaluate in the leave-one-event-out cross-validation setup. Note that we re-train the U NIFIED M2 encoder to ensure that it has no knowledge of the left-out-even"
2021.naacl-main.432,P19-1113,0,0.0496762,"Missing"
2021.naacl-main.432,D18-1003,0,0.0331863,"Missing"
2021.naacl-main.432,P18-1022,0,0.0234897,"n sentence from a news article contains political bias or not. We adapt the BASIL (Fan et al., 2019) dataset, which has bias-span annotations for lexical and informational bias within news articles. Using this dataset, we also include two auxiliary tasks related to political-bias detection: 1) bias type classification – given a biased sentence, the type of the bias (lexical vs informational) is classified; and 2) polarity detection – given a biased sentence, its polarity (positive, negative, neutral) is determined. FAKE N EWS An article-level fake news detection task that leverages the Webis (Potthast et al., 2018) dataset annotated by professional journalists. Our model training process consists of two steps. The first step is multi-task training of the shared U NIFIED M2 encoder to learn a general misinfor- RUMOR A task to verify the veracity of a rumor mation representation. We jointly optimize for tweet. The PHEME dataset (Zubiaga et al., 2016), all tasks t1 · · · tT by optimizing the sum of their which contains rumor tweets with their correspondtask-specific losses Lt , where Lt refers to the ing reply tweets (social engagement data), is used cross-entropy loss of the task-specific MLP clas- for th"
2021.naacl-main.432,D17-1317,0,0.032803,"Missing"
2021.naacl-main.432,W16-0802,0,0.101982,"Missing"
2021.naacl-main.432,2020.emnlp-main.108,0,0.0654588,"Missing"
2021.naacl-main.432,N18-1074,0,0.0433491,"Missing"
2021.naacl-main.432,P17-2102,0,0.0679395,"Missing"
2021.naacl-main.432,P17-2067,0,0.0819749,"Missing"
2021.naacl-main.432,D19-1471,0,0.0146857,"al., 2018; Nie et al., 2019). Finally, social-data-based approaches use the surrounding social data–such as the credibility of the authors of the information (Long et al., 2017; Kirilin and Strube, 2018; Li et al., 2019) or social engagement data (Derczynski et al., 2017; Ma et al., 2018; Kwon et al., 2013; Volkova et al., 2017). Though prior works have explored multi-task learning within misinformation, they have focused exclusively on one domain. These works try to predict two different labels on the same set of examples from a single (Kochkina et al., 2018) or two closely-related datasets (Wu et al., 2019). In contrast, our proposed approach crosses not just task or dataset boundaries, but also format and domain boundaries. Furthermore, prior works focus on using an auxiliary task to boost the performance of the main task, while we focus on using multitasking to generalize across many domains. Thus, the focus of this work is not the multitask paradigm, but rather the unification of the various domains, using multitasking. 6 Conclusion In this paper, we introduced U NIFIED M2, which unifies multiple domains of misinformation with a single multi-task learning setup. We empirically showed that suc"
2021.naacl-main.471,D10-1044,0,0.0306559,"018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2019) have achieved impressive gains in a wide variety of natural language tasks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization task. Hua and Wang (2017) first studied the adaptation of neural summarization models and showed that the models were able to select salient information from the source domain data. Wang et al. (2019) investigated the domain shift problem for the extractive summarization task. Recently, Magooda and Litman (2020) studied cross-domain transfer"
2021.naacl-main.471,D18-1443,0,0.0164381,"our knowledge, we are the first to systematically study the domain- and task-adaptative pre-training based on the pre-trained generative model in the low-resource abstractive summarization task across multiple diverse domains. Abstractive summarization aims to generate short, concise and readable text that captures the core meaning of the input documents. Neural networks have achieved remarkable results for the abstractive summarization due to the emergence of Seq2Seq models (Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2014). See et al. (2017), Paulus et al. (2017) and Gehrmann et al. (2018) applied a pointer network to solve the outof-vocabulary issue. Further, See et al. (2017) used a coverage mechanism (Tu et al., 2016) to keep 3 AdaptSum track of the already summarized content, which discourages repetition, while Paulus et al. (2017) The goal of AdaptSum is to provide an accessible and Chen and Bansal (2018) combined reinforce- benchmark for the evaluation of low-resource doment learning into an end2end setting. Recently, main adaptation for abstractive summarization on a 5893 Dialog Gliwa et al. (2019) introduced a humanannotated abstractive chat dialog summarization dataset"
2021.naacl-main.471,D19-5409,0,0.517699,"six diverse target domains in resource scenario for the abstractive summarizaa low-resource setting. Specifically, we tion task. To address this research gap, we present investigate the second phase of pre-training on AdaptSum, the first benchmark to simulate the large-scale generative models under three diflow-resource domain Adaptation setting for abferent settings: 1) source domain pre-training; stractive Summarization systems with a combina2) domain-adaptive pre-training; and 3) tasktion of existing datasets across six diverse domains adaptive pre-training. Experiments show that (dialog (Gliwa et al., 2019), email (Zhang and the effectiveness of pre-training is correlated with the similarity between the pre-training Tetreault, 2019), movie review (Wang and Ling, data and the target domain task. Moreover, we 2016), debate (Wang and Ling, 2016), social mefind that continuing pre-training could lead to dia (Kim et al., 2019), and science (Yasunaga et al., the pre-trained model’s catastrophic forgetting, 2019)), and for each domain, we reduce the numand a learning method with less forgetting ber of training samples to a small quantity so as to can alleviate this issue. Furthermore, results create a"
2021.naacl-main.471,W17-4513,0,0.112063,"n et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization task. Hua and Wang (2017) first studied the adaptation of neural summarization models and showed that the models were able to select salient information from the source domain data. Wang et al. (2019) investigated the domain shift problem for the extractive summarization task. Recently, Magooda and Litman (2020) studied cross-domain transfer between two entirely different domains and introduced data synthesis methods. To the best of our knowledge, we are the first to systematically study the domain- and task-adaptative pre-training based on the pre-trained generative model in the low-resource abstractive summarization"
2021.naacl-main.471,P19-1236,0,0.0243681,"ulary overlaps between domains are generally small, which illustrates that the overlaps between domains are comparably small and the chosen domains are diverse. cope with the catastrophic forgetting issue in the second phase of pre-training. 4.1 A Second Phase of Pre-Training We conduct a second pre-training phase based on a pre-trained generative model, BART (Lewis et al., 2019), on three different settings. Then, we finetune it to the summarization task in the target domains. The three settings are described as follows. Source Domain Pre-Training (SDPT) Inspired by the cross-domain setting (Jia et al., 2019; Liu et al., 2020c,d), we leverage substantial training samples from a source (News) domain Science Yasunaga et al. (2019) introduced (XSum (Narayan et al., 2018)), to aid in the fast a human-annotated abstractive summarization adaptation in target domains. We choose the News dataset on computational linguistics. We collect the unlabeled domain corpus from the ACL anthol- domain as the source domain because it is a richresource domain in the summarization task, and ogy (Bird et al., 2008). from Figure 1, the similarity between this domain and target domains is generally low which 4 Methodolog"
2021.naacl-main.471,N19-1260,0,0.0142432,"domain Adaptation setting for abferent settings: 1) source domain pre-training; stractive Summarization systems with a combina2) domain-adaptive pre-training; and 3) tasktion of existing datasets across six diverse domains adaptive pre-training. Experiments show that (dialog (Gliwa et al., 2019), email (Zhang and the effectiveness of pre-training is correlated with the similarity between the pre-training Tetreault, 2019), movie review (Wang and Ling, data and the target domain task. Moreover, we 2016), debate (Wang and Ling, 2016), social mefind that continuing pre-training could lead to dia (Kim et al., 2019), and science (Yasunaga et al., the pre-trained model’s catastrophic forgetting, 2019)), and for each domain, we reduce the numand a learning method with less forgetting ber of training samples to a small quantity so as to can alleviate this issue. Furthermore, results create a low-resource scenario. illustrate that a huge gap still exists between the low-resource and high-resource settings, Recently, conducting a second pre-training step which highlights the need for more advanced on large-scale language models (e.g., BERT (Dedomain adaptation methods for the abstractive vlin et al., 2019), R"
2021.naacl-main.471,N03-1020,0,0.756162,"Missing"
2021.naacl-main.471,D19-1387,0,0.0303968,"Labeled data Train Valid Test 300 818 819 300 1960 1906 300 500 2931 300 956 1003 300 1000 1000 100 350 497 Table 1: Data statistics of AdaptSum for the unlabeled corpus and labeled summarization data across the six domains (“R.” and “M.” are the abbreviations for Review and Media, respectively). pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2019) have achieved impressive gains in a wide variety of natural language tasks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization ta"
2021.naacl-main.471,2021.ccl-1.108,0,0.0828589,"Missing"
2021.naacl-main.471,D19-1129,1,0.889231,"Missing"
2021.naacl-main.471,2020.repl4nlp-1.1,1,0.903591,"sks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization task. Hua and Wang (2017) first studied the adaptation of neural summarization models and showed that the models were able to select salient information from the source domain data. Wang et al. (2019) investigated the domain shift problem for the extractive summarization task. Recently, Magooda and Litman (2020) studied cross-domain transfer between two entirely different domains and introduced data synthesis methods. To the best of our knowledge, we are the first to systematically study the do"
2021.naacl-main.471,2020.acl-main.3,1,0.923531,"sks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization task. Hua and Wang (2017) first studied the adaptation of neural summarization models and showed that the models were able to select salient information from the source domain data. Wang et al. (2019) investigated the domain shift problem for the extractive summarization task. Recently, Magooda and Litman (2020) studied cross-domain transfer between two entirely different domains and introduced data synthesis methods. To the best of our knowledge, we are the first to systematically study the do"
2021.naacl-main.471,P19-1534,0,0.0220299,"ptSum track of the already summarized content, which discourages repetition, while Paulus et al. (2017) The goal of AdaptSum is to provide an accessible and Chen and Bansal (2018) combined reinforce- benchmark for the evaluation of low-resource doment learning into an end2end setting. Recently, main adaptation for abstractive summarization on a 5893 Dialog Gliwa et al. (2019) introduced a humanannotated abstractive chat dialog summarization dataset. The unlabeled dialog corpus from different sources, namely, Reddit conversations,2 personalized dialogs (Zhang et al., 2018), empathetic dialogs (Rashkin et al., 2019), and Wizard of Wikipedia dialogs (Dinan et al., 2019). Email Zhang and Tetreault (2019) introduced an abstractive business and personal email summarization dataset which consists of email and subject pairs. We collect the unlabeled email corpus from the Enron Email Dataset.3 Movie Review Wang and Ling (2016) introduced a human-annotated abstractive movie review summarization dataset. We collect the unlabeled corpus for this domain from IDMB Movie Review (Maas et al., 2011). Debate Wang and Ling (2016) introduced an abstractive debate summarization dataset which consists of arguments and the d"
2021.naacl-main.471,P11-1015,0,0.0642115,"from different sources, namely, Reddit conversations,2 personalized dialogs (Zhang et al., 2018), empathetic dialogs (Rashkin et al., 2019), and Wizard of Wikipedia dialogs (Dinan et al., 2019). Email Zhang and Tetreault (2019) introduced an abstractive business and personal email summarization dataset which consists of email and subject pairs. We collect the unlabeled email corpus from the Enron Email Dataset.3 Movie Review Wang and Ling (2016) introduced a human-annotated abstractive movie review summarization dataset. We collect the unlabeled corpus for this domain from IDMB Movie Review (Maas et al., 2011). Debate Wang and Ling (2016) introduced an abstractive debate summarization dataset which consists of arguments and the debate topic pairs. The unlabeled corpus is from Ajjour et al. (2019). Social Media Kim et al. (2019) introduced an abstractive summarization dataset of Reddit TIFU posts, where the summary for each post come from its title. We collect the unlabeled corpus directly from Reddit TIFU.4 S New Dialo EmaMovie Deba ocial MScienc s g il R. te . e News 100.0 28.1 42.2 39.5 50.5 35.6 27.3 100 90 Dialog 28.1 100.0 27.0 26.8 21.6 31.5 14.1 80 Email 42.2 27.0 100.0 30.8 34.6 30.0 27.2 7"
2021.naacl-main.471,K16-1028,0,0.0667315,"Missing"
2021.naacl-main.471,D18-1206,0,0.0375795,"e. cope with the catastrophic forgetting issue in the second phase of pre-training. 4.1 A Second Phase of Pre-Training We conduct a second pre-training phase based on a pre-trained generative model, BART (Lewis et al., 2019), on three different settings. Then, we finetune it to the summarization task in the target domains. The three settings are described as follows. Source Domain Pre-Training (SDPT) Inspired by the cross-domain setting (Jia et al., 2019; Liu et al., 2020c,d), we leverage substantial training samples from a source (News) domain Science Yasunaga et al. (2019) introduced (XSum (Narayan et al., 2018)), to aid in the fast a human-annotated abstractive summarization adaptation in target domains. We choose the News dataset on computational linguistics. We collect the unlabeled domain corpus from the ACL anthol- domain as the source domain because it is a richresource domain in the summarization task, and ogy (Bird et al., 2008). from Figure 1, the similarity between this domain and target domains is generally low which 4 Methodology increases the challenge of the domain adaptation. In this section, we will first introduce the three Our method to conduct SDPT is straightfordifferent settings"
2021.naacl-main.471,P17-1098,0,0.0485713,"Missing"
2021.naacl-main.471,N18-1202,0,0.0237034,"ation task, which we hope will catalyze research in this area. 2 2.1 Related Work Abstractive Summarization Domain Dialog Email Movie R. Debate Social M. Science Unlabeled Corpus # Tokens Size 44.96M 212MB 117.54M 705MB 11.36M 62MB 122.99M 693MB 153.30M 786MB 41.73M 291MB Labeled data Train Valid Test 300 818 819 300 1960 1906 300 500 2931 300 956 1003 300 1000 1000 100 350 497 Table 1: Data statistics of AdaptSum for the unlabeled corpus and labeled summarization data across the six domains (“R.” and “M.” are the abbreviations for Review and Media, respectively). pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2019) have achieved impressive gains in a wide variety of natural language tasks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Fos"
2021.naacl-main.471,W10-2603,0,0.486913,"ls (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2019) have achieved impressive gains in a wide variety of natural language tasks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization task. Hua and Wang (2017) first studied the adaptation of neural summarization models and showed that the models were able to select salient information from the source domain data. Wang et al. (2019) investigated the domain shift problem for the extractive summarization task. Recently, Magooda and Litman (2020) studied c"
2021.naacl-main.471,P17-1099,0,0.0745635,"duced data synthesis methods. To the best of our knowledge, we are the first to systematically study the domain- and task-adaptative pre-training based on the pre-trained generative model in the low-resource abstractive summarization task across multiple diverse domains. Abstractive summarization aims to generate short, concise and readable text that captures the core meaning of the input documents. Neural networks have achieved remarkable results for the abstractive summarization due to the emergence of Seq2Seq models (Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2014). See et al. (2017), Paulus et al. (2017) and Gehrmann et al. (2018) applied a pointer network to solve the outof-vocabulary issue. Further, See et al. (2017) used a coverage mechanism (Tu et al., 2016) to keep 3 AdaptSum track of the already summarized content, which discourages repetition, while Paulus et al. (2017) The goal of AdaptSum is to provide an accessible and Chen and Bansal (2018) combined reinforce- benchmark for the evaluation of low-resource doment learning into an end2end setting. Recently, main adaptation for abstractive summarization on a 5893 Dialog Gliwa et al. (2019) introduced a humanannota"
2021.naacl-main.471,C16-1038,0,0.0301466,"19; Dong et al., 2019; Lewis et al., 2019) have achieved impressive gains in a wide variety of natural language tasks. Many studies on the use of pre-trained language models in the abstractive summarization task (Liu and Lapata, 2019; Yan et al., 2020; Su et al., 2020; Yu et al., 2020) have been undertaken and have achieved the state-of-the-art performance. 2.2 Domain Adaptation Domain adaption for natural language processing and computer vision tasks is widely studied (Blitzer et al., 2007; Mansour et al., 2008; Daumé III, 2009; Sandu et al., 2010; Foster et al., 2010; Wang and Cardie, 2013; Sun et al., 2016; Liu et al., 2019b, 2020b; Gururangan et al., 2020; Winata et al., 2020; Jadon, 2020; Yin, 2020; Liu et al., 2020a,d). However, little has been done to investigate domain adaption for the abstractive summarization task. Hua and Wang (2017) first studied the adaptation of neural summarization models and showed that the models were able to select salient information from the source domain data. Wang et al. (2019) investigated the domain shift problem for the extractive summarization task. Recently, Magooda and Litman (2020) studied cross-domain transfer between two entirely different domains an"
2021.naacl-main.471,P16-1008,0,0.0703268,"Missing"
2021.naacl-main.471,P19-1043,0,0.0711789,"Paulus et al. (2017) The goal of AdaptSum is to provide an accessible and Chen and Bansal (2018) combined reinforce- benchmark for the evaluation of low-resource doment learning into an end2end setting. Recently, main adaptation for abstractive summarization on a 5893 Dialog Gliwa et al. (2019) introduced a humanannotated abstractive chat dialog summarization dataset. The unlabeled dialog corpus from different sources, namely, Reddit conversations,2 personalized dialogs (Zhang et al., 2018), empathetic dialogs (Rashkin et al., 2019), and Wizard of Wikipedia dialogs (Dinan et al., 2019). Email Zhang and Tetreault (2019) introduced an abstractive business and personal email summarization dataset which consists of email and subject pairs. We collect the unlabeled email corpus from the Enron Email Dataset.3 Movie Review Wang and Ling (2016) introduced a human-annotated abstractive movie review summarization dataset. We collect the unlabeled corpus for this domain from IDMB Movie Review (Maas et al., 2011). Debate Wang and Ling (2016) introduced an abstractive debate summarization dataset which consists of arguments and the debate topic pairs. The unlabeled corpus is from Ajjour et al. (2019). Social Media Kim e"
2021.naacl-main.471,P18-1205,0,0.0607284,"mechanism (Tu et al., 2016) to keep 3 AdaptSum track of the already summarized content, which discourages repetition, while Paulus et al. (2017) The goal of AdaptSum is to provide an accessible and Chen and Bansal (2018) combined reinforce- benchmark for the evaluation of low-resource doment learning into an end2end setting. Recently, main adaptation for abstractive summarization on a 5893 Dialog Gliwa et al. (2019) introduced a humanannotated abstractive chat dialog summarization dataset. The unlabeled dialog corpus from different sources, namely, Reddit conversations,2 personalized dialogs (Zhang et al., 2018), empathetic dialogs (Rashkin et al., 2019), and Wizard of Wikipedia dialogs (Dinan et al., 2019). Email Zhang and Tetreault (2019) introduced an abstractive business and personal email summarization dataset which consists of email and subject pairs. We collect the unlabeled email corpus from the Enron Email Dataset.3 Movie Review Wang and Ling (2016) introduced a human-annotated abstractive movie review summarization dataset. We collect the unlabeled corpus for this domain from IDMB Movie Review (Maas et al., 2011). Debate Wang and Ling (2016) introduced an abstractive debate summarization da"
2021.nlp4convai-1.10,W18-3219,0,0.060773,"Missing"
2021.nlp4convai-1.10,N19-1253,0,0.0286069,"es in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and conalign multiple language representations, achieving state-of-the-art results in many cross-lingual classi- versation consistency. The full annotation instructions are reported in Appendix A. fication tasks. The aforementioned tasks focused on classification and sequence labeling, while inCompared to collecting new persona sentences st"
2021.nlp4convai-1.10,P15-1039,0,0.0629361,"Missing"
2021.nlp4convai-1.10,D18-1038,0,0.0133005,"ong languages and circum- speaker annotators with at least a bachelor degree and a fluent level of English and asked them to vents the requirement of extensive training data in revise the machine-translated dialogues and pertarget languages (Wisniewski et al., 2014; Zhang et al., 2016). Cross-lingual transfer learning meth- sona sentences in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and"
2021.nlp4convai-1.10,P19-4007,0,0.0356572,"glish dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and conalign multiple language representations, achieving state-of-the-art results in many cross-lingual classi- versation consistency. The full annotation instructions are reported in Appendix A. fication tasks. The aforementioned tasks focused on classification and sequence labeling, while inCompared to collecting new persona sentences stead, Chi et al. (2019) proposed to pre-train both and dialogues"
2021.nlp4convai-1.10,D18-1269,0,0.147764,"odels, especially in low resources languages, translation-pipeline models using both autoare only available using costly APIs. matic and human evaluation. Experimental reIn this paper, we analyze two possible sults show that the multilingual trained models workarounds to alleviate the aforementioned chaloutperform the translation-pipeline and that lenges. The first is to build a cross-lingual transthey are on par with the monolingual models, ferable system by aligning cross-lingual represenwith the advantage of having a single model across multiple languages. On the other hand, tations, as in Conneau et al. (2018), in which the the state-of-the-art cross-lingual trained modsystem is trained on one language and zero-shot els achieve inferior performance to the other to another language. The second is to learn a mulmodels, showing that cross-lingual conversatilingual system directly from noisy multilingual tion modeling is a challenging task. We hope data (e.g., translated data), thus getting rid of the that our dataset and baselines 1 will accelerate translation system dependence at inference time. research in multilingual dialogue systems. To evaluate the aforementioned solutions, we propose a dataset"
2021.nlp4convai-1.10,P19-1346,0,0.0277735,"ed as follows: • We present the first multilingual non-goaloriented dialogue benchmark for evaluating multilingual generative chatbots. • We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the pe"
2021.nlp4convai-1.10,P18-5002,0,0.0289758,"Missing"
2021.nlp4convai-1.10,D17-1302,0,0.0223462,"niewski et al., 2014; Zhang et al., 2016). Cross-lingual transfer learning meth- sona sentences in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and conalign multiple language representations, achieving state-of-the-art results in many cross-lingual classi- versation consistency. The full annotation instructions are reported in Appendix A. fication tasks. The aforementioned tasks focused o"
2021.nlp4convai-1.10,P16-1094,0,0.17271,"ns to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona-consistent dialogue (Zhang et al., 2018). Several works have improved on the initial baselines with various methodologies, especially using large pre"
2021.nlp4convai-1.10,D16-1127,0,0.211223,"ns to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona-consistent dialogue (Zhang et al., 2018). Several works have improved on the initial baselines with various methodologies, especially using large pre"
2021.nlp4convai-1.10,D16-1230,0,0.07703,"Missing"
2021.nlp4convai-1.10,P19-1227,0,0.0209796,"et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. 3 Data Collection The proposed XPersona dataset is an extension of the persona-chat"
2021.nlp4convai-1.10,2020.tacl-1.47,0,0.0179185,"e number of dialogues (#Dial.) and utterances (#Utt.) of the validation and test set in six languages. Edit distance per dialogue (Edit) and BLEU score are computed to show the difference between the human-annotated dataset and auto-translated dataset (Training set is reported in Appendix A). The BLEU score also reflects the quality of machine translated dialogues. lation (Johnson et al., 2017), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none"
2021.nlp4convai-1.10,P19-1081,0,0.0202563,"multilingual non-goaloriented dialogue benchmark for evaluating multilingual generative chatbots. • We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is"
2021.nlp4convai-1.10,Q17-1022,0,0.0700587,"Missing"
2021.nlp4convai-1.10,P17-1135,0,0.022254,"Missing"
2021.nlp4convai-1.10,P17-1178,0,0.0121823,"7), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. 3 Data Collection The"
2021.nlp4convai-1.10,P02-1040,0,0.111605,"layer remain frozen. This retains the decoders’ ability to generate multilingual output while still being able to learn new tasks using only the target language. 5 5.1 Experiments Evaluation Metrics Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset. Automatic For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU (Papineni et al., 2002) with reference to the human-annotated responses. Although these automatic measures are not perfect (Liu et al., 2016), they help to roughly estimate the performance of different models under the same test set. More recently, Adiwardana et al. (2020) has shown the correlation between perplexity and human judgment in open-domain chit-chat models. such evaluation, a human interacts with the systems for several turns, and then they assign a score from 1 to 5 based on three questions (Zhang et al., 2018) about fluency, engagingness, and consistency. This evaluation is both expensive to conduct and"
2021.nlp4convai-1.10,P19-1493,0,0.152553,"mputed to show the difference between the human-annotated dataset and auto-translated dataset (Training set is reported in Appendix A). The BLEU score also reflects the quality of machine translated dialogues. lation (Johnson et al., 2017), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation ta"
2021.nlp4convai-1.10,Q19-1016,0,0.0257738,"e present the first multilingual non-goaloriented dialogue benchmark for evaluating multilingual generative chatbots. • We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions,"
2021.nlp4convai-1.10,N19-1380,0,0.0197831,"t al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. 3 Data Collection The proposed XPersona dataset is an extension of the persona-chat dataset (Zhang et al., 2018; Dinan et al., 2019a). Specifically, we extend ConvAI2 (Dinan et al., 2019a) to six languages: Chine"
2021.nlp4convai-1.10,P18-1205,0,0.375821,"tter human-machine interaction. multilingual conversational benchmarks is essenExisting personalized dialogue agents rely tial, yet challenging since it is costly to perform on properly designed conversational datasets, human annotation of data in all languages. which are mostly monolingual (e.g., English), which greatly limits the usage of conversaA possible solution is to use translation systems tional agents in other languages. In this pabefore and after the model inference. This comes per, we propose a multi-lingual extension of with three major problems: 1) amplification of Persona-Chat (Zhang et al., 2018), namely translation errors since the current dialogue sysXPersona. Our dataset includes persona contems are far from perfect, especially with noisy versations in six different languages other than input; 2) the three-stage pipeline system is signifEnglish for evaluating multilingual personalicantly slower in terms of inference speed; and 3) ized agents. We experiment with both mulhigh translation costs since the current state-of-thetilingual and cross-lingual trained baselines, and evaluate them against monolingual and art models, especially in low resources languages, translation-pipeline mo"
2021.nlp4convai-1.10,N19-1170,0,0.0567634,"Missing"
2021.nlp4convai-1.10,N16-1156,0,0.0599374,"Missing"
2021.nlp4convai-1.10,P17-1061,0,0.0187814,"sonaChat training set and early stop based on the perplexity on target language validation set. 5.3 Results and Discussion 5.3.1 Quantitative Analysis Table 3 compares monolingual, multilingual, and cross-lingual models in terms of BLEU and perplexity in the human-translated test set. On both evaluation matrices, the causal decoder models outperform the encoder-decoder models. We observe that the encoder-decoder model tends to overlook dialogue context and generate digressive responses. (Generated samples are available in Appendix D) We hypothesize that this is because the one-tomany problem (Zhao et al., 2017) in open-domain conversation weakens the relation between encoder and decoder; thus the well pre-trained decoder (Bert) easily converges to a local optimum, and learns to ignore the dialogue context from the encoder and generate the response in an unconditional language model way. We leave the investigation of this problem to future work. On the other hand, M-CausalBert achieves a comparable or slightly better performance compared to CausalBert, which suggests that M-CausalBert leverages the data from other languages. As expected, we observe a significant gap between the cross-lingual model an"
2021.nlp4convai-1.10,D14-1187,0,0.0275078,"et of ConvAI2 is hidden, we split the original validation set into a new validation set and test sets. Then, we firstly automatically translate the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native Cross-lingual Cross-lingual adaptation learns the inter-connections among languages and circum- speaker annotators with at least a bachelor degree and a fluent level of English and asked them to vents the requirement of extensive training data in revise the machine-translated dialogues and pertarget languages (Wisniewski et al., 2014; Zhang et al., 2016). Cross-lingual transfer learning meth- sona sentences in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), a"
2021.nlp4convai-1.10,2021.naacl-main.41,0,0.0238846,"cted dataset. We report the number of dialogues (#Dial.) and utterances (#Utt.) of the validation and test set in six languages. Edit distance per dialogue (Edit) and BLEU score are computed to show the difference between the human-annotated dataset and auto-translated dataset (Training set is reported in Appendix A). The BLEU score also reflects the quality of machine translated dialogues. lation (Johnson et al., 2017), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et"
2021.repl4nlp-1.13,2021.naacl-main.236,0,0.0428486,"hat, Rongali et al. (2020) showed that leveraging a sequence-to-sequence model based on a copy mechanism (See et al., 2017) to directly generate the hierarchical representations was effective at parsing the nested queries. Taking this further, Chen et al. (2020) and Li et al. (2020a) extended the TOP dataset into multiple domains and multiple languages, and Li et al. (2020a) conducted zero-shot cross-lingual experiments using the combination of the multilingual pre-trained models (Conneau et al., 2020; Tran et al., 2020) and the copy mechanism method proposed in Rongali et al. (2020). Lately, Babu et al. (2021) and Shrivastava et al. (2021), which are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017),"
2021.repl4nlp-1.13,2020.lrec-1.674,0,0.0380175,"Missing"
2021.repl4nlp-1.13,D18-1038,0,0.0234911,"hich are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sen"
2021.repl4nlp-1.13,2020.emnlp-main.413,0,0.0372946,"8) 113 Figure 3: The architecture of X2Parser. We consider the TCSP task as a combination of the coarse-grained intent classification, fine-grained intent prediction, and slot filling tasks. introduced a new dataset, called TOP, annotated with complex nested intents and slots and proposed to use the hierarchical representations to model the task. After that, Rongali et al. (2020) showed that leveraging a sequence-to-sequence model based on a copy mechanism (See et al., 2017) to directly generate the hierarchical representations was effective at parsing the nested queries. Taking this further, Chen et al. (2020) and Li et al. (2020a) extended the TOP dataset into multiple domains and multiple languages, and Li et al. (2020a) conducted zero-shot cross-lingual experiments using the combination of the multilingual pre-trained models (Conneau et al., 2020; Tran et al., 2020) and the copy mechanism method proposed in Rongali et al. (2020). Lately, Babu et al. (2021) and Shrivastava et al. (2021), which are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several"
2021.repl4nlp-1.13,D19-1252,0,0.0478506,"Missing"
2021.repl4nlp-1.13,P19-1236,0,0.0273528,"em into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b), abusive language detection (Pamungkas and Patti, 2019), and machine reading comprehension (Charlet et al., 2020). To the best of our k"
2021.repl4nlp-1.13,N18-1131,0,0.0583594,"Missing"
2021.repl4nlp-1.13,D19-1129,1,0.836304,"Missing"
2021.repl4nlp-1.13,2020.acl-main.3,1,0.811576,"non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b), abusive languag"
2021.repl4nlp-1.13,P17-1135,0,0.038426,"Missing"
2021.repl4nlp-1.13,N19-1204,0,0.0164537,"-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b), abusive language detection (Pamungkas and Patti, 2019), and machine reading comprehension (Charlet et al., 2020). To the best of our knowledge, we are the first to study the combination of cross-lingual and cross-domain adaptations in the TCSP task. 3 Task Decomposition In this sect"
2021.repl4nlp-1.13,2021.findings-emnlp.161,0,0.0350375,"20) showed that leveraging a sequence-to-sequence model based on a copy mechanism (See et al., 2017) to directly generate the hierarchical representations was effective at parsing the nested queries. Taking this further, Chen et al. (2020) and Li et al. (2020a) extended the TOP dataset into multiple domains and multiple languages, and Li et al. (2020a) conducted zero-shot cross-lingual experiments using the combination of the multilingual pre-trained models (Conneau et al., 2020; Tran et al., 2020) and the copy mechanism method proposed in Rongali et al. (2020). Lately, Babu et al. (2021) and Shrivastava et al. (2021), which are concurrent works of X2Parser, proposed to tackled the TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsin"
2021.repl4nlp-1.13,P19-1078,1,0.676161,"he TCSP task in a non-autoregressive way. Different from them, we propose to flatten the hierarchical representations and cast the problem into several sequence labeling tasks. 2.2 Language and Domain Adaptation Recently, cross-lingual and cross-domain models that aim to tackle low-resource issues have been applied to natural language understanding (Conneau et al., 2018; Huang et al., 2019; Conneau et al., 2020; Gururangan et al., 2020), sentiment analysis (Zhou et al., 2016; Ziser and Reichart, 2017), task-oriented semantic parsing (Chen et al., 2018; Schuster et al., 2019; Liu et al., 2019; Wu et al., 2019; Liu et al., 2020a; Chen et al., 2020; Liu et al., 2020b), named entity recognition (Ni et al., 2017; Xie et al., 2018; Jia et al., 2019; Liu et al., 2020c), speech recognition (Mimura et al., 2017; Winata et al., 2020), abstractive summarization (Zhu et al., 2019; Ouyang et al., 2019; Yu et al., 2021), etc. Despite numerous studies related to the cross-lingual and cross-domain areas, only a few of them have explored how to effectively adapt models to the target languages in target domains, and the investigated tasks are limited to sentiment analysis (Fern´andez et al., 2016; Li et al., 2020b"
2021.repl4nlp-1.8,D14-1187,0,0.0601937,"Missing"
2021.repl4nlp-1.8,D19-1077,0,0.326085,"c. 40 37.72 35.68 34.12 30 26.12 20 15.12 10 0 es → en fr → en it → en el → en Figure 1: Masked language model and cross-lingual sentence retrieval results before and after fine-tuning mBERT to the English part-of-speech tagging task. Introduction Recently, multilingual language models (Devlin et al., 2019; Conneau and Lample, 2019), pretrained on extensive monolingual or bilingual resources across numerous languages, have been shown to enjoy surprising cross-lingual adaptation abilities, and fine-tuning them to downstream crosslingual tasks has achieved promising results (Pires et al., 2019; Wu and Dredze, 2019). Taking this further, better pre-trained language models have been proposed to improve the cross-lingual performance, such as using larger amounts of pre-trained data with larger pre-trained models (Conneau et al., 2019; Liang et al., 2020), and utilizing more tasks in the pre-training stage (Huang et al., 2019). However, we observe that multilingual BERT (mBERT) (Devlin et al., 2019), a pre-trained language model, forgets the masked language model (MLM) task that has been learned and partially loses the cross-lingual ability (from a cross-lingual sentence retrieval (XSR)1 experiment) after b"
2021.sigdial-1.27,2020.sigdial-1.15,1,0.861621,"Missing"
2021.sigdial-1.27,W16-3625,1,0.899776,"sed on the turn: body gestures during the system turn, and nodding during the user turn. Body gestures are intended to show openness to users during ERICA’s utterance, mainly moving her right hand, as shown in Figure 3(d) and (e). We design four versatile movements and play one of them randomly during ERICA’s utterance. During the user turn, ERICA nods to play the role of an active listener until 2.0 s of user silence is detected. To enhance the naturalness of ERICA’s behavior during the conversation, a random gazing model is also introduced. ERICA normally does speaker tracking using Kinect (Inoue et al., 2016, 2020), but since the participant in our case is not on-site, we model gazing behavior as a random uniform sampling of a gaze point nearby the webcam. The gaze point will be randomly changed within a hollow cylinder from the center of the webcam with an outer radius of 0.3 m, inner radius of 0.05 m, and width of 0.2 m. The gaze change decision is taken every 1.5 s. 3 Experiments We conducted a comparative evaluation to see how nonverbal information such as facial expressions and body gestures affect the user experience by asking volunteers to participate in a session with the Nora virtual age"
2021.sigdial-1.57,S19-2184,1,0.883467,"Missing"
2021.sigdial-1.57,W19-3655,1,0.702072,"dence analysis of various chatbots and discuss their behavior from multiple angles through our automatic metric and human evaluation metrics. The testsets and codebase are released to promote research in this area.1 1 Figure 1: Illustration of responses from different chatbots in a political conversation. Abortion law is a topic that often leads to divisive political debates. Introduction With the rise of end-to-end open-domain chatbots, it is increasingly important to ensure their responsible and safe behavior. Chatbot safety has been studied from various aspects including sexism and racism (Lee et al., 2019b; Liu et al., 2020; Xu et al., 2020). However, political prudence of chatbot is an under explored angle. Ensuring responsible behavior when discussing politics deserves more attention, because a hyper-partisan chatbot could be off-putting to the user. Recently, Xu et al. (2020) conducted comprehensive exploration of safety protocols for chatbots. However, political prudence remains an open discussion because a “topic avoidance” strategy – providing canned responses such as the “I’m sorry, I’m not sure what to say. Thank you for sharing and talking to me though”– is adopted for political topic"
2021.sigdial-1.57,2020.coling-main.390,0,0.0265091,"arious chatbots and discuss their behavior from multiple angles through our automatic metric and human evaluation metrics. The testsets and codebase are released to promote research in this area.1 1 Figure 1: Illustration of responses from different chatbots in a political conversation. Abortion law is a topic that often leads to divisive political debates. Introduction With the rise of end-to-end open-domain chatbots, it is increasingly important to ensure their responsible and safe behavior. Chatbot safety has been studied from various aspects including sexism and racism (Lee et al., 2019b; Liu et al., 2020; Xu et al., 2020). However, political prudence of chatbot is an under explored angle. Ensuring responsible behavior when discussing politics deserves more attention, because a hyper-partisan chatbot could be off-putting to the user. Recently, Xu et al. (2020) conducted comprehensive exploration of safety protocols for chatbots. However, political prudence remains an open discussion because a “topic avoidance” strategy – providing canned responses such as the “I’m sorry, I’m not sure what to say. Thank you for sharing and talking to me though”– is adopted for political topics and other sensiti"
2021.sigdial-1.57,2021.ccl-1.108,0,0.0951879,"Missing"
2021.sigdial-1.57,P19-1534,0,0.0230278,"atbot by Lin et al. (2020) fine-tuned on empathetic dialogue (a) Offensiveness vs. Hyper- (b) Slantedness vs. Hyperpartisan in Scenario B partisan in Scenario B Figure 2: Plots of offensiveness and slantedness scores against hyper-partisanship score in Scenario B. No correlation is shown in (a) for offensive vs. hyperpartisan, while in (b), higher slantedness score chatbots tend to have a higher hyper-partisanship score. The chatbot names are written using their abbreviations (DGPT: DialoGPT; EB: EmpatheticBot; PC: PersonaChat; AWiki: AdapterWiki; BB: Blenderbot; BB+Fact: Blenderbot+Fact). by Rashkin et al. (2019); and c) PersonaChat – a personalized chatbot backboned by DialoGPT and finetuned on the Persona dataset by Zhang et al. (2018). The KG chatbots includes d) AdapterWiki – a Wikipedia adapter of AdapterBot (Madotto et al., 2021) trained on Dinan et al. (2018); e) Blenderbot – a publicly available multi-skill chatbot (blenderbot400M-distill) (Roller et al., 2020); f) Blenderbot+Fact – our proposed naive yet safe and neutral chatbot which has a safety layer specialized for political discussion. This chatbot is back-boned by Blenderbot with a safety layer that detects whether the context is politi"
2021.sigdial-1.57,N18-1101,0,0.0177417,"Missing"
2021.sigdial-1.57,P18-1205,0,0.0307416,"enario B partisan in Scenario B Figure 2: Plots of offensiveness and slantedness scores against hyper-partisanship score in Scenario B. No correlation is shown in (a) for offensive vs. hyperpartisan, while in (b), higher slantedness score chatbots tend to have a higher hyper-partisanship score. The chatbot names are written using their abbreviations (DGPT: DialoGPT; EB: EmpatheticBot; PC: PersonaChat; AWiki: AdapterWiki; BB: Blenderbot; BB+Fact: Blenderbot+Fact). by Rashkin et al. (2019); and c) PersonaChat – a personalized chatbot backboned by DialoGPT and finetuned on the Persona dataset by Zhang et al. (2018). The KG chatbots includes d) AdapterWiki – a Wikipedia adapter of AdapterBot (Madotto et al., 2021) trained on Dinan et al. (2018); e) Blenderbot – a publicly available multi-skill chatbot (blenderbot400M-distill) (Roller et al., 2020); f) Blenderbot+Fact – our proposed naive yet safe and neutral chatbot which has a safety layer specialized for political discussion. This chatbot is back-boned by Blenderbot with a safety layer that detects whether the context is political or not using a dialogue context classifier by Xu et al. (2020). When the context is detected as “politics” class, Blenderbo"
A94-1030,W93-0305,0,0.100106,"Missing"
A94-1030,O92-1003,0,0.13173,"Missing"
A94-1030,O93-1004,0,0.144584,"Missing"
A94-1030,P94-1012,1,0.657309,"Missing"
A94-1030,P94-1010,0,\N,Missing
C02-1162,J96-1001,0,\N,Missing
C02-1162,P98-1069,1,\N,Missing
C02-1162,C98-1066,1,\N,Missing
C02-1162,N01-1006,1,\N,Missing
C02-1162,J90-2002,0,\N,Missing
C02-1162,P93-1001,0,\N,Missing
C02-1162,J95-4004,0,\N,Missing
C02-1162,P98-2127,0,\N,Missing
C02-1162,C98-2122,0,\N,Missing
C02-1162,P99-1004,0,\N,Missing
C04-1134,P98-1013,0,0.366896,"Missing"
C04-1134,boas-2002-bilingual,0,0.339637,"Missing"
C04-1134,1985.tmi-1.5,0,0.480569,"Missing"
C04-1134,W03-1007,0,0.12655,"Missing"
C04-1134,P00-1065,0,0.0418219,"Missing"
C04-1134,O02-2003,0,0.0754814,"Missing"
C04-1134,J03-2001,0,0.18018,"entences that have the same semantic roles. We automatically induce Chinese example sentences and their semantic roles, based on semantic structure alignment from the first stage of our work, as well as shallow syntactic structure. In addition to its utility for machine-aided and machine translations, our work is also related to the spatial models proposed by cognitive scientists in the framework of artifactual simulations of the translation process. 1. Consequently, cognitive scientists and computational linguists alike have been interested in the study of semantic mapping between languages (Ploux and Ji, 2003, Dorr et al., 2002, Ngai et al., 2002, Boas 2002, Palmer and Wu, 1995). We propose to automatically construct a bilingual lexical semantic network with word sense and semantic role mapping between English and Chinese, simulating the “concept lexicon”, suggested by cognitive scientists, of a bilingual person. Introduction The merits of translation at the word level or the concept level have long been a cause for debate among linguists. Some linguists suggest that the two languages of a bilingual speaker share a common semantic system (Illes and Francis 1999; Ikeda 1998) and hence translation i"
C04-1134,J97-3002,0,0.0335,"rom the high cost of building two semantic parsers, which itself requires semantically annotated Chinese data; it would be necessary to create artificial links between independent human annotations manually. iii) Mine Chinese sentences from a monolingual corpus that are syntactically similar to the English example sentence, and induce semantic roles from the syntactic transfer function between English and Chinese. This is the approach we take. Inspired by previous work on syntax-driven semantic parsing (Gildea and Jurafsky, 2002; Fleischman et al., 2003), and syntax-based machine translation (Wu, 1997; Cuerzan and Yarowsky, 2002), we postulate that syntactically similar sentences with the same predicate also share similar semantic roles. In this paper, we present our first experiments on inducing semantic roles based on shallow syntactic information. We mine Chinese example sentences from naturally occurring monolingual corpus, and rank them by their syntactic similarity to our English example sentences. A dynamic programming algorithm then annotates the aligned syntactic units with the same semantic roles. The example Chinese sentences are not translations of the English sentences. Theref"
C04-1134,C02-1162,1,\N,Missing
C04-1134,C98-1013,0,\N,Missing
C04-1134,J02-3001,0,\N,Missing
C04-1134,W02-2006,0,\N,Missing
C04-1151,W03-1004,0,0.0881253,"Missing"
C04-1151,W97-0119,1,0.276208,"Missing"
C04-1151,P99-1043,1,0.792923,"ines the algorithm in more detail. In the following sections 3.1-3.4, we describe the four different steps of our algorithm. 3.1 Extract comparable documents The aim of this step is to extract the ChineseEnglish documents pairs that are comparable, and therefore should have similar term distributions. The documents are word segmented with the Language Data Consortium (LDC) ChineseEnglish dictionary 2.0. The Chinese document is then glossed using all the dictionary entries. Multiple translations of a Chinese word is disambiguated by looking at the context of the sentences this word appears in (Fung et al., 1999). Both the glossed Chinese document and the English document are then represented in word vectors, with term weighting. We evaluated different combinations of term weighting of each word in the corpus: term freuency (tf), inverse document frequency (idf), tf.idf, the product of a function of tf and idf. The ”documents” here are sentences. We find that using idf alone gives the best sentence pair rank. This is due to the fact that frequencies of bilingual word pairs are not comparable in a non-parallel, quasi-comparable corpus. Pair-wise similarities are calculated for all possible Chinese-Engl"
C04-1151,N03-1015,0,0.0545338,"Missing"
C04-1151,W02-1037,0,0.0999361,"ntext can still be used to find correspondences between passages, sentences, or words, in non-parallel, comparable texts of the same topic (Fung and McKeown 1995, Rapp 1995, Grefenstette 1998, Fung and Lo 1998, Kikui 1999). More recent works on parallel sentence extraction from comparable data align documents first, before extracting sentences from the aligned documents (Munteanu and Marcu, 2002, Zhao and Vogel, 2002). Both work used a translation model trained from parallel corpus and adaptively extract more parallel sentences and bilingual lexicon in the comparable corpus. In Zhao and Vogel (2002), the comparable corpus consists of Chinese and English versions of new stories from the Xinhua News agency. Munteanu and Marcu (2002) used unaligned segments from the French-English Hansard corpus and finds parallel sentences among them. Zhao and Vogel (2002) used a generative statistical machine translation alignment model, Munteanu and Marcu (2002) used suffix treesbased alignment model, and Munteanu and Marcu (2004) used a maximum entropy based classifier trained from parallel corpus to extract matching sentences from a comparable corpus of Arabic and English news. The comparable corpora u"
C04-1151,N04-1034,0,0.0196009,"Both work used a translation model trained from parallel corpus and adaptively extract more parallel sentences and bilingual lexicon in the comparable corpus. In Zhao and Vogel (2002), the comparable corpus consists of Chinese and English versions of new stories from the Xinhua News agency. Munteanu and Marcu (2002) used unaligned segments from the French-English Hansard corpus and finds parallel sentences among them. Zhao and Vogel (2002) used a generative statistical machine translation alignment model, Munteanu and Marcu (2002) used suffix treesbased alignment model, and Munteanu and Marcu (2004) used a maximum entropy based classifier trained from parallel corpus to extract matching sentences from a comparable corpus of Arabic and English news. The comparable corpora used in all these work consist of documents on the same topic. Our challenge is to find matching bilingual sentences from documents that might or might not be on the same topic. 2 Bilingual Sentence Alignment There have been various definitions of the term “parallel corpora” in the research community. In this paper, we compare and analyze different bilingual corpora, ranging from the parallel, noisy parallel, comparable,"
C04-1151,P95-1050,0,0.322204,"Existing work extract parallel sentences from parallel, noisy parallel or comparable corpora based on the assumption that parallel sentences should be Figure1. Multi-level bootstrapping for parallel sentence extraction Extraction of matching bilingual segments from non-parallel data has remained a challenging task after almost a decade. Previously, the author and other researchers had suggested that bi-lexical information based on context can still be used to find correspondences between passages, sentences, or words, in non-parallel, comparable texts of the same topic (Fung and McKeown 1995, Rapp 1995, Grefenstette 1998, Fung and Lo 1998, Kikui 1999). More recent works on parallel sentence extraction from comparable data align documents first, before extracting sentences from the aligned documents (Munteanu and Marcu, 2002, Zhao and Vogel, 2002). Both work used a translation model trained from parallel corpus and adaptively extract more parallel sentences and bilingual lexicon in the comparable corpus. In Zhao and Vogel (2002), the comparable corpus consists of Chinese and English versions of new stories from the Xinhua News agency. Munteanu and Marcu (2002) used unaligned segments from th"
C04-1151,N04-4010,1,0.808029,"Missing"
C04-1151,W99-0905,0,\N,Missing
C04-1151,J93-1004,0,\N,Missing
C04-1151,1998.amta-tutorials.5,0,\N,Missing
C04-1151,J93-1007,0,\N,Missing
C04-1151,P98-1069,1,\N,Missing
C04-1151,C98-1066,1,\N,Missing
C04-1151,J03-3002,0,\N,Missing
C10-1023,W06-2810,0,0.0792909,"Missing"
C10-1023,W00-0405,0,0.255835,"fails to hold for domain-independent application. Instead, our algorithm aims to synthesize the article in the sentential level. We select sentences to fit the source content at run time, regardless to whether a pre-determined structural template exists or not. Therefore the requirement on the structures of source articles becomes very flexible, enabling our system to work for arbitrary domain. In a sense, rather than being a structure-aware approach, our algorithm performs in a content-aware manner. 198 Works on monolingual extractive text summarization also lend insights into our problem. (Goldstein et al., 2000) used sequential sentence selection based on Maximal Marginal Relevance Multi-Document (MMRMD) score to form summarizations for multiple documents, with the constraint of sentence count. Since our problem does not have this constraint, we employ a variant of MMR-MD and introduced new terms specific to this task. (Takamura and Okumura, 2009) formulated a text summarization task as a maximum coverage problem with knapsack constraint and proposed a variety of combinatorial mathematics-based algorithms for solving the optimization problem. For multi-lingual summarization, (Evans, 2005) applied the"
C10-1023,E09-1089,0,0.0176157,"flexible, enabling our system to work for arbitrary domain. In a sense, rather than being a structure-aware approach, our algorithm performs in a content-aware manner. 198 Works on monolingual extractive text summarization also lend insights into our problem. (Goldstein et al., 2000) used sequential sentence selection based on Maximal Marginal Relevance Multi-Document (MMRMD) score to form summarizations for multiple documents, with the constraint of sentence count. Since our problem does not have this constraint, we employ a variant of MMR-MD and introduced new terms specific to this task. (Takamura and Okumura, 2009) formulated a text summarization task as a maximum coverage problem with knapsack constraint and proposed a variety of combinatorial mathematics-based algorithms for solving the optimization problem. For multi-lingual summarization, (Evans, 2005) applied the concept of multi-lingual text similarity to summarization and improved readability of English summaries of Arabic text by replacing machine translated Arabic sentences with highly similar English sentences whenever possible. 3 Methodology Figure 1 describes the high-level algorithm of our approach. The system takes as input the English Wik"
C10-1023,W04-3252,0,\N,Missing
C10-1023,J97-1003,0,\N,Missing
C10-1023,W04-1013,0,\N,Missing
C10-1023,P09-1024,0,\N,Missing
C10-1146,J97-1003,0,\N,Missing
C10-1146,D08-1035,0,\N,Missing
C10-1146,W03-1203,1,\N,Missing
C10-1146,P08-1054,0,\N,Missing
C10-1146,P96-1038,0,\N,Missing
C10-1146,W04-1013,0,\N,Missing
C10-1146,P09-1024,0,\N,Missing
C10-1146,J02-4002,0,\N,Missing
C10-1146,P07-1069,0,\N,Missing
C12-1102,D08-1102,0,0.16449,"ts own grammar. In code-switch, the matrix language is the ‘principal’ language, where the ‘embedded’ language is the second language(C. and C., 1993; Coulmas, 1998). There are two main approaches to recognizing code-switch mixed language speech. One is to detect the boundaries at which the speaker code-switches, then identify the language in the speech segments between the boundaries, and decode the speech segments using the acoustic and language models in the corresponding language(Chan et al., 2005; Shia et al., 2004; Lyu and Lyu, 2008; Vu et al., 2012). For text only code-switch language, Solorio and Liu (2008) used Naive Bayes and Value Feature Interval to classify the hypothesis code-switch points by F-measure and the naturalness rating. However, this approach requires multiple passes of boundary detection, language identification and speech recognition. The boundaries and language identity of each speech segment are irreversibly determined by the previews pass. Moreover, the speech segment of the embedded language tends to be very short. This poses challenges to the state-of-the-art language identification approaches. A more holistic way to decode code-switch speech is by using a set of universal"
C12-1102,J97-3002,0,0.0467824,"2. Code-switch is not allowed between the first three words with syntactic inversion. Word-aligned parallel sentences in the matrix and the embedded languages are used to constrain at which point code-switch is allowed. We propose to generate bilingual training data as follows: 1) Translate words of the mixed language sentences in the embedded language into the matrix language using a statistical machine translation system; 1674 2) Translate the monolingual sentences from 1) into the embedded language by a statistical machine translation system with inversion transduction grammar constraint (Wu, 1997; Wu and Fung, 2005) to obtain monolingual sentences in the embedded language; 3) Align the pairs of monolingual sentences in the matrix and embedded languages from 1) and 2). In theory, we can generate as many bilingual sentence pairs as possible. The code-switch boundary prediction model trains the probabilities of a sequence of words segmented into a sequence of phrases from the aligned parallel sentences. A phrase is a word or a concatenation of words in which there exists one or more inversions of an aligned parallel sentence pair in the matrix language and the embedded language. P(v1n ,"
C12-1102,I05-1023,1,0.265851,"witch is not allowed between the first three words with syntactic inversion. Word-aligned parallel sentences in the matrix and the embedded languages are used to constrain at which point code-switch is allowed. We propose to generate bilingual training data as follows: 1) Translate words of the mixed language sentences in the embedded language into the matrix language using a statistical machine translation system; 1674 2) Translate the monolingual sentences from 1) into the embedded language by a statistical machine translation system with inversion transduction grammar constraint (Wu, 1997; Wu and Fung, 2005) to obtain monolingual sentences in the embedded language; 3) Align the pairs of monolingual sentences in the matrix and embedded languages from 1) and 2). In theory, we can generate as many bilingual sentence pairs as possible. The code-switch boundary prediction model trains the probabilities of a sequence of words segmented into a sequence of phrases from the aligned parallel sentences. A phrase is a word or a concatenation of words in which there exists one or more inversions of an aligned parallel sentence pair in the matrix language and the embedded language. P(v1n , n|w1m ) = n 1 Y Zn i"
C16-2058,J81-4005,0,0.718825,"Missing"
C16-2058,D14-1181,0,0.00476774,"Missing"
C16-2058,rousseau-etal-2014-enhancing,0,0.0405706,"Missing"
C16-2058,N16-3018,1,\N,Missing
C94-2178,P93-1002,0,0.319397,"es such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. 1. Motivation There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work (Church et al, 1993), w"
C94-2178,P93-1001,1,0.941913,"e Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. 1. Motivation There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work (Church et al, 1993), we have reported"
C94-2178,J90-2002,0,0.141512,"Missing"
C94-2178,P91-1022,0,0.77201,"Missing"
C94-2178,J93-1004,1,0.896188,"Missing"
C94-2178,P93-1003,0,0.0847022,"we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. 1. Motivation There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work (Church et al, 1993), we have reported some preliminary success in aligning the English and Japanese versions of the A W K manual (Aho, Kernighan, Weinberger (1980)), using 1096 charalign (Church, 1993"
C94-2178,P93-1004,0,0.309716,"Missing"
C94-2178,E93-1054,0,\N,Missing
C94-2178,J93-2003,0,\N,Missing
C94-2178,C90-3031,0,\N,Missing
C94-2178,H91-1026,1,\N,Missing
C94-2178,P94-1012,0,\N,Missing
C98-1066,P91-1022,0,0.053373,"munity regards the WWW as a vast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable te"
C98-1066,J93-2003,0,0.0284546,"orpus linguistic community regards the WWW as a vast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-d"
C98-1066,P93-1001,0,0.0101641,"ast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable texts in electronic form."
C98-1066,A94-1006,0,0.0198685,"! ; (Chee-hwa) ~ (Leung) ~ i ~ (Zhuhai) I~ (Lei) ~1~~J ~l~ (Yeltsin) ~-1[~l~ (Chee-hwa) ~ (Lain) ~)~ (Lava) ~ ] ~ (Poultry) ~ ) ~ (Teng-hui) ~ } ~ (Teng-hui) J~ (Tang) ~ (Leung) ~:~ (Leung) t[~N (SAR) ~ (Lunar) ~ (Tung) poultry PrimeMinister President China Lien poultry China flu PrivaeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we"
C98-1066,J94-4003,0,0.0174166,"e Chinese name for bird flu is poultry flu. In fact, ahnost all unambiguous Chinese new words find their translations in the first 100 of the ranked list. Six of the Chinese words have correct translation as their first candidate. 9 Table 5: Some Chinese unknown word translation output Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korflmge, 1995; Jones, 1979). This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 418 0.008421 0.007895 0.007669 0.007588 0.007283 0.006812 0.006430 0.006218 0.005921 0.005527 0.005335 0.005335 0.005221 0.004731 0.004470 0.004275 0.003878 0.003859 0.003859 0.003784 0.003686 0.003550 0.003519 0.003481 0.003407 0.003407 0.OO3338 0.003324 0.003250 0.003206 0.003202 0.003040 0.003033 0.002888 0.002886 English Teng-hui SAR ~ } ~ (Teng-hui) ~[~, (SAR) flu N ~ (m,) Lei ~ (Lei) Chinese poultry ~j~ (Pou"
C98-1066,C94-2178,1,0.204037,"~ ] ~ (Poultry) ~ ) ~ (Teng-hui) ~ } ~ (Teng-hui) J~ (Tang) ~ (Leung) ~:~ (Leung) t[~N (SAR) ~ (Lunar) ~ (Tung) poultry PrimeMinister President China Lien poultry China flu PrivaeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have d e m o n s t r a t e d that the associations between a word and its context seed words are well-pres"
C98-1066,W97-0119,1,0.735369,"Missing"
C98-1066,W95-0114,1,0.626467,"poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have d e m o n s t r a t e d that the associations between a word and its context seed words are well-preserved in nonparallel, comparable texts of different languages. 10 Discussions Our algorithm is the first to have generated a collocation bilingual lexicon, albeit small, from a nonparallel"
C98-1066,P95-1032,1,0.477026,"poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have d e m o n s t r a t e d that the associations between a word and its context seed words are well-preserved in nonparallel, comparable texts of different languages. 10 Discussions Our algorithm is the first to have generated a collocation bilingual lexicon, albeit small, from a nonparallel"
C98-1066,J93-1004,0,0.136072,"WW as a vast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable texts in electronic form."
C98-1066,P92-1032,0,0.00552519,"d flu is poultry flu. In fact, ahnost all unambiguous Chinese new words find their translations in the first 100 of the ranked list. Six of the Chinese words have correct translation as their first candidate. 9 Table 5: Some Chinese unknown word translation output Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korflmge, 1995; Jones, 1979). This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 418 0.008421 0.007895 0.007669 0.007588 0.007283 0.006812 0.006430 0.006218 0.005921 0.005527 0.005335 0.005335 0.005221 0.004731 0.004470 0.004275 0.003878 0.003859 0.003859 0.003784 0.003686 0.003550 0.003519 0.003481 0.003407 0.003407 0.OO3338 0.003324 0.003250 0.003206 0.003202 0.003040 0.003033 0.002888 0.002886 English Teng-hui SAR ~ } ~ (Teng-hui) ~[~, (SAR) flu N ~ (m,) Lei ~ (Lei) Chinese poultry ~j~ (Poultry) SAR hijack ~&apos;"
C98-1066,P93-1003,0,0.0493704,"i) ~1~~J ~l~ (Yeltsin) ~-1[~l~ (Chee-hwa) ~ (Lain) ~)~ (Lava) ~ ] ~ (Poultry) ~ ) ~ (Teng-hui) ~ } ~ (Teng-hui) J~ (Tang) ~ (Leung) ~:~ (Leung) t[~N (SAR) ~ (Lunar) ~ (Tung) poultry PrimeMinister President China Lien poultry China flu PrivaeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have d e m o n s t r a t e d that th"
C98-1066,P95-1050,0,0.299887,"zen) ~ i : ~ (I{Ollg t~011~) r~ (infection) ~ (confirmed) Freq. 147 90 84 69 62 ~,~, (show) 62 ~IJ~ ~H ~,. ~ (discover) (yesterday) (patient) (suspected) 56 54 53 50 N~ (doctor) 49 ~ J 2 (infected) N~,,~ (hospital) 47 44 NN (no) 4~ ~ $~ (governme.t) (event) 41 40 Table 2: ~,~,~, :~.~. and Aft&apos;lea have different contexts English South Aft&apos;lean China ties diplomatic Taiwan relations Test Mandela Taipei Africans January visit tense survived Beijing w i t h flu Unlike in parallel texts, the position of a word in a text does not give us infbrmation a b o u t its translation in the other language. (Rapp, 1995; Fung and McKcown, 1997) suggest that a content word is closely associated with some words in its context. As a tutorial example, we postulate that the words which appear in the context of i ~ / l i o u g a n should be similar to the words appearing in the context of its English translation, flu. We can tbrm a vector space model of a word in terms of its context word indices, similar to the vector space model of a text in terms of its constituent word indices (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; rihlrtle and Croft, 1992; Bookstein, 1983; Korfllage, 1995; Jones, 1979)"
C98-1066,1992.tmi-1.7,0,0.0372328,"urces. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable texts in electronic form. The type of nonparallel texts obtai"
C98-1066,J96-1001,0,0.129048,"Yeltsin) ~-1[~l~ (Chee-hwa) ~ (Lain) ~)~ (Lava) ~ ] ~ (Poultry) ~ ) ~ (Teng-hui) ~ } ~ (Teng-hui) J~ (Tang) ~ (Leung) ~:~ (Leung) t[~N (SAR) ~ (Lunar) ~ (Tung) poultry PrimeMinister President China Lien poultry China flu PrivaeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have d e m o n s t r a t e d that the associations betwee"
C98-1066,1994.amta-1.26,0,0.017645,"-I~ (Chee-hwa) ~ 1 ! ; (Chee-hwa) ~ (Leung) ~ i ~ (Zhuhai) I~ (Lei) ~1~~J ~l~ (Yeltsin) ~-1[~l~ (Chee-hwa) ~ (Lain) ~)~ (Lava) ~ ] ~ (Poultry) ~ ) ~ (Teng-hui) ~ } ~ (Teng-hui) J~ (Tang) ~ (Leung) ~:~ (Leung) t[~N (SAR) ~ (Lunar) ~ (Tung) poultry PrimeMinister President China Lien poultry China flu PrivaeMinister President poultry Kalkanov poultry SAR Zhuhai PrimeMinister President flu apologise Dee Tang ~iSlng Leung China Zhuhai Ttulg 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fhmg, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 19953; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another lan"
C98-1066,P95-1026,0,0.0447765,"Chinese new words find their translations in the first 100 of the ranked list. Six of the Chinese words have correct translation as their first candidate. 9 Table 5: Some Chinese unknown word translation output Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korflmge, 1995; Jones, 1979). This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 418 0.008421 0.007895 0.007669 0.007588 0.007283 0.006812 0.006430 0.006218 0.005921 0.005527 0.005335 0.005335 0.005221 0.004731 0.004470 0.004275 0.003878 0.003859 0.003859 0.003784 0.003686 0.003550 0.003519 0.003481 0.003407 0.003407 0.OO3338 0.003324 0.003250 0.003206 0.003202 0.003040 0.003033 0.002888 0.002886 English Teng-hui SAR ~ } ~ (Teng-hui) ~[~, (SAR) flu N ~ (m,) Lei ~ (Lei) Chinese poultry ~j~ (Poultry) SAR hijack ~&apos;~ ~)~ ~ (Chee-hwa) (Teng-hui) (SAR) ~,ng ~ (Chee-h"
C98-1066,J93-1006,0,\N,Missing
C98-1066,H91-1026,0,\N,Missing
D12-1070,J04-2004,0,0.0211292,"uage model. ~ :w ~ / P(r ) w r1 1 1 ~ :w ~ / P(r |r ) w r2 2 2 1 ~ :w ~ / P(r |r ) w r3 3 3 2 Figure 2: An example of reordering WFST Ωr implementing the phrasal reordering model under the first order Markov assumption. 2.3 Phrase-to-Phrase Transduction Model Once the phrase sequence of the Lw transcript is reordered into the Lv transcript order, we use the phrase-to-phrase transduction model specified in Eq. (6) to perform the cross-language transduction. Given sufficient parallel training data, the contextdependent phrase-to-phrase transduction model can be estimated using the GIATI method (Casacuberta and Vidal, 2004). However, for the translation task with scarce training data, the contextdependent transduction probabilities may not be reliably estimated. Therefore, we assume that a phrase v˜k is generated independently by each phrase w ˜ rk . C(˜ vk , w ˜rk ) is the number of times that phrase v˜k is aligned to w ˜rk in the parallel corpus. This model can be implemented by a WFST Tvw which transduces v˜k to w ˜rk . Figure 3(a3) shows an example of Tvw transducing v2 v3 to w2 w3 . P (˜ v1K |r1K , w ˜1K , w1J ) = P (˜ v1K |r1K , w ˜1K ) = = K Y k=1 K Y k=1 2.4 3 Speech Recognition with Cross-Lingual Langua"
D12-1070,W07-0414,0,0.0787055,"does not have an official written form, there are very few written texts available for training language models. In this paper, we treat Cantonese as a typical resourcepoor language and Mandarin as a typical resource-rich language. This language pair will be used for illustration purposes throughout this paper. 2 767 ing. A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al., 1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. It has been shown in (Zens and Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007) that ITG constraints perform better than other constraints when tackling the reordering between many language pairs. Previous work on weighted finitestate transducer (WFST) based speech translation such as (Casacuberta et al., 2004; Zhou et al., 2005; Zhou et al., 2006; Mathias and Byrne, 2006; Matusov et al., 2006; Saon and Picheny, 2007) only train the reordering model using IBM constraints, local constraints or ad hoc rules. We will use ITG constraints, which have only been applied to text translation tasks before, to model the syntactic differences in cross-lingual language modeling for s"
D12-1070,W05-0831,0,0.0221685,"002). Since Cantonese does not have an official written form, there are very few written texts available for training language models. In this paper, we treat Cantonese as a typical resourcepoor language and Mandarin as a typical resource-rich language. This language pair will be used for illustration purposes throughout this paper. 2 767 ing. A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al., 1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. It has been shown in (Zens and Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007) that ITG constraints perform better than other constraints when tackling the reordering between many language pairs. Previous work on weighted finitestate transducer (WFST) based speech translation such as (Casacuberta et al., 2004; Zhou et al., 2005; Zhou et al., 2006; Mathias and Byrne, 2006; Matusov et al., 2006; Saon and Picheny, 2007) only train the reordering model using IBM constraints, local constraints or ad hoc rules. We will use ITG constraints, which have only been applied to text translation tasks before, to model the syntactic differences in cross-lingual l"
D12-1070,W03-1003,0,0.0838504,"Missing"
D12-1070,J99-4005,0,0.198223,"Missing"
D12-1070,H05-1021,0,0.0230325,"use crosslingual language modeling with syntactic reorder1 For example, Hindi and Malayalam (Geethakumary, 2002). Since Cantonese does not have an official written form, there are very few written texts available for training language models. In this paper, we treat Cantonese as a typical resourcepoor language and Mandarin as a typical resource-rich language. This language pair will be used for illustration purposes throughout this paper. 2 767 ing. A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al., 1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. It has been shown in (Zens and Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007) that ITG constraints perform better than other constraints when tackling the reordering between many language pairs. Previous work on weighted finitestate transducer (WFST) based speech translation such as (Casacuberta et al., 2004; Zhou et al., 2005; Zhou et al., 2006; Mathias and Byrne, 2006; Matusov et al., 2006; Saon and Picheny, 2007) only train the reordering model using IBM constraints, local constraints or ad hoc rules. We will use ITG constraints, which have"
D12-1070,I11-1174,0,0.111648,"Missing"
D12-1070,D09-1141,0,0.0454333,"Missing"
D12-1070,J03-1002,0,0.00574962,"˜1K , w1J ) and reconstrucduction model P (˜ v1K |r1K , w K K I ˜1K , w1J ). Before presentv1 , r1 , w tion model P (v1 |˜ ing each component model, we need to extract two phrase tables for the Lv transcript and the Lw transcript, respectively. P (v1I |w1J ) ≈ max v˜1K ,r1K ,w ˜1K P (w ˜1K |w1J ) · P (r1K |w ˜1K , w1J ) · P (˜ v1K |r1K , w ˜1K , w1J ) · P (v1I |˜ v1K , r1K , w ˜1K , w1J ) (2) The phrase extraction is based on word-to-word alignments of the parallel corpus. We train word alignments in both directions with GIZA++, and then symmetrize the two alignments using the refined method (Och and Ney, 2003). Figure 1 shows an example of word-to-word alignment results between an Lv transcript (Cantonese) and an Lw transcript (Mandarin), from which phrase-to-phrase alignments are derived by identifying deletion, substitution, insertion and inversion. Prior to phrasal reordering, the segmentation model P (w ˜1K |w1J ) implemented by a segmentation WFST Sw is applied to segment a word sequence w1J in the Lw language model into a phrase sequence {w ˜1 , w ˜2 , ..., w˜K }. The maximum number of words that can be segmented into one phrase is controlled by a segmentation order s. An example of Sw is sho"
D12-1070,W99-0604,0,0.245684,"Missing"
D12-1070,P02-1040,0,0.083706,"antonese outputs. In our experiments, we use the following evaluation criteria: WER (word error rate). The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the reference sentence (Zens et al., 2004). The WER relates the speech recognition accuracy. The lower WER, the better. BLEU (bilingual evaluation understudy) score. The BLEU score measures the precision of n-grams (unigrams, bigrams, trigrams and fourgrams) with respect to a reference translation with a penalty for too short sentences (Papineni et al., 2002). The BLEU score reflects the translation accuracy. The larger BLEU score, the better. We perform WER evaluation of decoding outputs of Eq. (10) and BLEU score evaluation of decoding outputs of Eq. (9) using the evaluation set. The WER evaluation is on the Cantonese output against the Cantonese reference transcription (manual transcription). The BLEU score evaluation is on the Mandarin output against the Mandarin reference transcription (Hansard transcription). 4.3 Parameter Settings The performance of our proposed cross-lingual language models is sensitive to many parameters. Firstly, segment"
D12-1070,J97-3002,0,0.620695,"t a better approach than interpolation and word-level transduction is to use crosslingual language modeling with syntactic reorder1 For example, Hindi and Malayalam (Geethakumary, 2002). Since Cantonese does not have an official written form, there are very few written texts available for training language models. In this paper, we treat Cantonese as a typical resourcepoor language and Mandarin as a typical resource-rich language. This language pair will be used for illustration purposes throughout this paper. 2 767 ing. A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al., 1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. It has been shown in (Zens and Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007) that ITG constraints perform better than other constraints when tackling the reordering between many language pairs. Previous work on weighted finitestate transducer (WFST) based speech translation such as (Casacuberta et al., 2004; Zhou et al., 2005; Zhou et al., 2006; Mathias and Byrne, 2006; Matusov et al., 2006; Saon and Picheny, 2007) only train the reordering model using IBM c"
D12-1070,P03-1019,0,0.176628,"lam (Geethakumary, 2002). Since Cantonese does not have an official written form, there are very few written texts available for training language models. In this paper, we treat Cantonese as a typical resourcepoor language and Mandarin as a typical resource-rich language. This language pair will be used for illustration purposes throughout this paper. 2 767 ing. A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al., 1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. It has been shown in (Zens and Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007) that ITG constraints perform better than other constraints when tackling the reordering between many language pairs. Previous work on weighted finitestate transducer (WFST) based speech translation such as (Casacuberta et al., 2004; Zhou et al., 2005; Zhou et al., 2006; Mathias and Byrne, 2006; Matusov et al., 2006; Saon and Picheny, 2007) only train the reordering model using IBM constraints, local constraints or ad hoc rules. We will use ITG constraints, which have only been applied to text translation tasks before, to model the syntactic differen"
D12-1070,C04-1030,0,0.0273846,"ned by composition of T and G. 4.2 Decoding and Evaluation Method Decoding of the speech recognition search space ASR is performed by T 3 Decoder (Dixon et al., 2009), which is a state-of-the-art WFST-based LVCSR speech decoder. Decoding of ASR in Eq. (9) gives Mandarin outputs. Decoding of ASR in Eq. (10) gives Cantonese outputs. In our experiments, we use the following evaluation criteria: WER (word error rate). The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the reference sentence (Zens et al., 2004). The WER relates the speech recognition accuracy. The lower WER, the better. BLEU (bilingual evaluation understudy) score. The BLEU score measures the precision of n-grams (unigrams, bigrams, trigrams and fourgrams) with respect to a reference translation with a penalty for too short sentences (Papineni et al., 2002). The BLEU score reflects the translation accuracy. The larger BLEU score, the better. We perform WER evaluation of decoding outputs of Eq. (10) and BLEU score evaluation of decoding outputs of Eq. (9) using the evaluation set. The WER evaluation is on the Cantonese output against"
D14-1098,P10-2041,0,0.0673936,"Missing"
D16-1110,P14-1062,0,0.0140171,".0 56.3 72.8 61.1 50.9 71.1 61.2 CNN 61.2 62.0 72.9 66.6 60.1 71.4 65.7 Table 1: Accuracy obtained, percentage, in the Convolutional Neural Network model for emotion classification from raw audio samples. Figure 1: Convolutional Neural Network model for emotion classification from raw audio samples. Figure 2: Convolutional neural network model for sentiment classification 4 Sentiment Inference from Speech and Text Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance also on the practically important task of sentence classification (Johnson and Zhang, 2014; Kalchbrenner et al., 2014; Kim, 2014). In our approach, we use a CNN-based classifier with Word2Vec to analyze the sentiment of recognized speech. We train a CNN with one layer of convolution and max pooling (Collobert et al., 2011) on top of word embedding vectors trained on the Google News corpus (Mikolov et al., 2013) of size 300. We apply on top of the word vectors a convolutional sliding window of size 3, 4 and 5 to represent multiple features. We then apply a max-pooling operation over the output vectors of the convolutional layer, that allows the model to pick up the most valuable information wherever it happen"
D16-1110,D14-1181,0,0.00363117,"61.2 CNN 61.2 62.0 72.9 66.6 60.1 71.4 65.7 Table 1: Accuracy obtained, percentage, in the Convolutional Neural Network model for emotion classification from raw audio samples. Figure 1: Convolutional Neural Network model for emotion classification from raw audio samples. Figure 2: Convolutional neural network model for sentiment classification 4 Sentiment Inference from Speech and Text Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance also on the practically important task of sentence classification (Johnson and Zhang, 2014; Kalchbrenner et al., 2014; Kim, 2014). In our approach, we use a CNN-based classifier with Word2Vec to analyze the sentiment of recognized speech. We train a CNN with one layer of convolution and max pooling (Collobert et al., 2011) on top of word embedding vectors trained on the Google News corpus (Mikolov et al., 2013) of size 300. We apply on top of the word vectors a convolutional sliding window of size 3, 4 and 5 to represent multiple features. We then apply a max-pooling operation over the output vectors of the convolutional layer, that allows the model to pick up the most valuable information wherever it happens in the inp"
D16-1110,rousseau-etal-2014-enhancing,0,0.00694315,"Missing"
D18-1143,D15-1075,0,0.0167994,"the baselines from Thorne et al. (2018). Madotto et al., 2018) also have similar aspects to these works, although aiming at a different goal. Other fields that are related to the particular individual modules of our system are the following: Document and evidence retrieval for identifying text segments and documents to support a given claim (Salton and Buckley, 1987; Le and Mikolov, 2014; Cartright et al., 2011; Bellot et al., 2013; Rinott et al., 2015). Recognizing textual entailment that aims to determine whether a hypothesis h can justifiably be inferred from a premise (Dang et al., 2007; Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2017b; Glockner et al., 2018). In some of these work (Rinott et al., 2015; Rashkin et al., 2017), the lexical and linguistic features are leveraged to further improve the performance. 6 Conclusion In this paper, we extend the pipeline framework for fact-checking and propose a neural ranker for evidence selection. Our experiments show that the usage of lexical tagging is helpful in simplifying the task and improving the generalization ability. Moreover, reducing noise in the input of RTE module, by de-noising the DR and SR modules, appears to be crucial for i"
D18-1143,P17-1171,0,0.121314,"et al., 2014) library. Using this information is helpful in the following ways: 1) it helps keyword extraction for each claim. 2) it reduces the out-of-vocabulary (OOV) problems related to name or organization entities, for better generalization. For example, a claim like “Michael Jackson and Justin Timberlake are friends,” is replaced as “PERSON-1 and PERSON-2 are friends”. In this way, we encourage our model to learn verification without dealing with the real entity values but the delexicalized indexed tokens. 2.2 Document Retrieval (DR) For document retrieval, we extend the method of DrQA (Chen et al., 2017a), which calculates cosine similarity between query and document, using binned unigram and bigram TF-IDF features. We refer to this method as DRtf idf . Instead of directly selecting top k document using TF-IDF as in DRtf idf , our document retriever 1 https://github.com/HLTCHKUST/fact-checking POSmatch POSmatch , rtitle = , POSclaim POStitle = rclaim × rtitle × tf -idf rclaim = frank To capture the relevance from the title, all the POS tags with high discriminating power (NN, NNS, NNP, NNPS, JJ, CD) of a claim are chosen as keywords. POSclaim and POStitle are the counts of such POS tags insi"
D18-1143,P17-1152,0,0.172318,"et al., 2014) library. Using this information is helpful in the following ways: 1) it helps keyword extraction for each claim. 2) it reduces the out-of-vocabulary (OOV) problems related to name or organization entities, for better generalization. For example, a claim like “Michael Jackson and Justin Timberlake are friends,” is replaced as “PERSON-1 and PERSON-2 are friends”. In this way, we encourage our model to learn verification without dealing with the real entity values but the delexicalized indexed tokens. 2.2 Document Retrieval (DR) For document retrieval, we extend the method of DrQA (Chen et al., 2017a), which calculates cosine similarity between query and document, using binned unigram and bigram TF-IDF features. We refer to this method as DRtf idf . Instead of directly selecting top k document using TF-IDF as in DRtf idf , our document retriever 1 https://github.com/HLTCHKUST/fact-checking POSmatch POSmatch , rtitle = , POSclaim POStitle = rclaim × rtitle × tf -idf rclaim = frank To capture the relevance from the title, all the POS tags with high discriminating power (NN, NNS, NNP, NNPS, JJ, CD) of a claim are chosen as keywords. POSclaim and POStitle are the counts of such POS tags insi"
D18-1143,P17-1045,0,0.059328,"Missing"
D18-1143,P18-2103,0,0.0488454,"or noise, from early module could harm the subsequent RTE module. 2.4 Recognizing Textual Entailment (RTE) Given a claim and l possible evidence, a DArte classifier is trained to recognize the textual entailment to be support, refute or not enough information to verify (NEI). Same as Thorne et al. (2018), we use the decomposable attention (DA) between the claim and the evidence for RTE. DA model decomposes the RTE problem into subproblems, which can be considered as bi-direction wordlevel attention features. Note that the DA model is utilized over other models such as as Chen et al. (2017b); Glockner et al. (2018), because it is a simple but effective model. Our DArte model must correctly decide whether a claim is NEI, when the evidence retrieved is irrelevant and insufficient. However, NEI claims have no annotated evidence, thus cannot be used to train RTE. To overcome this issue, same as (Thorne et al., 2018), the most probable NEI evidence are simulated by sampling sentences from the nearest page to the claim using the document retrieval module. MLP 63.2 DArte 78.4 DArte +NER 79.9 Table 4: Oracle RTE classification accuracy in the test set using gold evidence. 3 Experimental setup Dataset: FEVER dat"
D18-1143,P18-1136,1,0.879521,"Missing"
D18-1143,P17-2067,0,0.184527,"Missing"
D18-1143,P14-5010,0,0.00351113,"the state-of-the-art performance on the dataset. 2 Methodology Our pipeline framework1 has three main modules: document retrieval (DR), evidence selection (ES), and textual entailment recognition (TER). The goal is to verify a given claim with a set of evidence from Wikipedia (Table 1). The verification labels are support, refute and not enough information (NEI) to verify. 2.1 Lexical Tagging In our framework, two lexical tags (i.e. partof-speech (POS) and named entity recognition (NER)) are used to enhance the performance. We compute the tags for claims in advance using the Stanford CoreNLP (Manning et al., 2014) library. Using this information is helpful in the following ways: 1) it helps keyword extraction for each claim. 2) it reduces the out-of-vocabulary (OOV) problems related to name or organization entities, for better generalization. For example, a claim like “Michael Jackson and Justin Timberlake are friends,” is replaced as “PERSON-1 and PERSON-2 are friends”. In this way, we encourage our model to learn verification without dealing with the real entity values but the delexicalized indexed tokens. 2.2 Document Retrieval (DR) For document retrieval, we extend the method of DrQA (Chen et al.,"
D18-1143,D16-1244,0,0.109162,"Missing"
D18-1143,D14-1162,0,0.0826059,"us fact extraction and verification works, with around 5.4M Wikipedia documents and 185k samples. The claims are generated by altering sentences extracted from Wikipedia, with humanannotated evidence sentences and verification labels (e.g. Table 1). The training/validation/test sets of these three datasets are split in advance by the providers. Note that the test-set was equally split into 3 classes: Supported (3333), Refuted (3333), NEI (3333). Training: We trained our models end-to-end using Adagrad optimizer (Duchi et al., 2011). The embedding size is set to 200 and initialized with GloVe (Pennington et al., 2014). The dropout rate is set to 0.2. In all the datasets, we tuned the hyper-parameters with grid-search over the validation set. Evaluation: For each module, we independently measure oracle performance, where we assume gold standard documents and set of evidence are provided (oracle evaluation). For the final fullpipeline, we compare to and follow the metric defined in Thorne et al. (2018). NoScoreEv is a simple classification accuracy that only considers the correctness of the verification label. On the other hand, ScoreEv is a stricter measure that also considers the correctness of the retriev"
D18-1143,D17-1317,0,0.0232617,"tion dataset with speedup. 1 Label Table 1: Example of verified claim with evidence from multiple Wikipedia pages Introduction With the rapid growth of available textual information, automatic extraction and verification, also known as fact-checking, has become important in order to identify relevant and factual information from the ever-growing information pool. The FakeNews Challenge (Pomerleau and Rao) addresses fact-checking as a simple stance detection problem, where the article is verified by checking the stance agreement between an article’s title and content. Similar to the FakeNews, (Rashkin et al., 2017; Vlachos and Riedel, 2014) focused on political statements from Politifact.com to verify the degree of truthfulness. However, they assume that the gold standard documents containing the evidence are already known, which overly simplifies the task. Question Answering (QA) is similar to factchecking in the sense that a question and its answers can be considered as a claim and evidence respectively, but the answers may come from a large-scale database. Several approaches (Chen ∗ These two authors contributed equally. Finding Dory was written by anyone but an American. Finding Dory: Directed by A"
D18-1143,D15-1050,0,0.031008,"F1 0.310 0.517 0.523 Evidence F1 0.175 0.563 Table 7: Full-pipeline evaluation on the test set using k = 2 and th = 0.6. The first and the second one (with *) are the baselines from Thorne et al. (2018). Madotto et al., 2018) also have similar aspects to these works, although aiming at a different goal. Other fields that are related to the particular individual modules of our system are the following: Document and evidence retrieval for identifying text segments and documents to support a given claim (Salton and Buckley, 1987; Le and Mikolov, 2014; Cartright et al., 2011; Bellot et al., 2013; Rinott et al., 2015). Recognizing textual entailment that aims to determine whether a hypothesis h can justifiably be inferred from a premise (Dang et al., 2007; Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2017b; Glockner et al., 2018). In some of these work (Rinott et al., 2015; Rashkin et al., 2017), the lexical and linguistic features are leveraged to further improve the performance. 6 Conclusion In this paper, we extend the pipeline framework for fact-checking and propose a neural ranker for evidence selection. Our experiments show that the usage of lexical tagging is helpful in simplifying the tas"
D18-1143,N18-1074,0,0.107313,"We incorporate the TFIDF score (tf -idf ) to ensure that the content information is not neglected. Our experiments show that our re-rank strategy increases the document recall compared to the single-step approach (Table 2). To decide on the optimal value for hyperparameter k, full-pipeline performance was compared to evaluate the effect of k on final verification accuracy. 2.3 Evidence Selection (ES) In this module, l sentences are extracted as possible evidence for the claim. Instead of selecting the sentences by recomputing sentence-level TFIDF features between claim and document text as in Thorne et al. (2018), we propose a neural ranker using decomposable attention (DA) model (Parikh et al., 2016) to perform evidence selection. DA model does not require the input text to be parsed syntactically, nor is an ensemble, and it is faster without any recurrent structure. In general, using neural methods is better for the following reasons: 1) The TF-IDF may have limited ability to capture semantics compared to word representation learning 2) Faster inference time compared to TF-IDF methods that need real-time reconstruction. The neural ranker DArank is trained using a fake task, which is to classify whet"
D18-1143,W14-2508,0,0.0250301,"dup. 1 Label Table 1: Example of verified claim with evidence from multiple Wikipedia pages Introduction With the rapid growth of available textual information, automatic extraction and verification, also known as fact-checking, has become important in order to identify relevant and factual information from the ever-growing information pool. The FakeNews Challenge (Pomerleau and Rao) addresses fact-checking as a simple stance detection problem, where the article is verified by checking the stance agreement between an article’s title and content. Similar to the FakeNews, (Rashkin et al., 2017; Vlachos and Riedel, 2014) focused on political statements from Politifact.com to verify the degree of truthfulness. However, they assume that the gold standard documents containing the evidence are already known, which overly simplifies the task. Question Answering (QA) is similar to factchecking in the sense that a question and its answers can be considered as a claim and evidence respectively, but the answers may come from a large-scale database. Several approaches (Chen ∗ These two authors contributed equally. Finding Dory was written by anyone but an American. Finding Dory: Directed by Andrew Stanton with co-direc"
D18-1302,Q17-1010,0,0.0362818,"by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters: 1. CNN: Convolution layers with 3 filters with the size of [3,4,5], feature map size=100, Embedding Size=300, Maxpooling, Dropout=0.5 2. GRU: hidden dimension=512, Maximum Sequence Length=100, Embedding Size=300, Dropout=0.3 3. α-GRU: hidden dimension=256 (bidirectional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embedding Size=300, Dropout=0.3 We also compare different pre-trained embeddings, word2vec (Mikolov et al., 2013) trained on Google News corpus, FastText (Bojanowski et al., 2017)) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged. 4.3 Results & Discussions Tables 4 and 5 show the bias measurement experiment results for st and abt, respectively. As expected, pre-trained embeddings improved task performance. The score on the unbiased generated test set (Gen. ROC) also improved since word embeddings can provide prior knowledge of words. However, the equality difference scores tended to be larger when pre-trained embeddings were 2801 Model CNN GRU α-GRU Embed. random"
D18-1302,D17-1169,0,0.0343355,"test set. F N ED = random fasttext word2vec random fasttext word2vec random fasttext word2vec Orig. AUC .881 .906 .906 .854 .887 .887 .868 .891 .890 Table 4: Table 3: Example of offensive and non-offensive X Embed. Experimental Setup We first measure gender biases in st and abt datasets. We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) (Park and Fung, 2017), Gated Recurrent Unit (GRU) (Cho et al., 2014), and Bidirectional GRU with self-attention (α-GRU) (Pavlopoulos et al., 2017), but with a simpler mechanism used in Felbo et al. (2017). Hyperparameters are found using the validation set by finding the best performing ones in terms of original AUC scores. These are the used hyperparameters: 1. CNN: Convolution layers with 3 filters with the size of [3,4,5], feature map size=100, Embedding Size=300, Maxpooling, Dropout=0.5 2. GRU: hidden dimension=512, Maximum Sequence Length=100, Embedding Size=300, Dropout=0.3 3. α-GRU: hidden dimension=256 (bidirectional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embedding Size=300, Dropout=0.3 We also compare different pre-trained embeddings, word2vec (Mikolov et"
D18-1302,S18-2005,0,0.277363,"also because bias correction can improve the robustness of the models. Bolukbasi et al. (2016) is one of the first works to point out the gender stereotypes inside word2vec (Mikolov et al., 2013) and propose an algorithm to correct them. Caliskan et al. (2017) also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. Dixon et al. (2017) is one of the first works that point out existing “unintended” bias in abusive language detection models. Kiritchenko and Mohammad (2018) compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems. Zhao et al. (2018) shows the effectiveness of measuring and correcting gender biases in coreference resolution tasks. We later show how we extend a few of these works into ours. 3 3.1 Datasets Sexist Tweets (st) This dataset consists of tweets with sexist tweets collected from Twitter by searching for tweets that contain common terms pertaining to sexism such as “feminazi.” The tweets were then annotated by experts b"
D18-1302,D17-1117,0,0.147224,"ucting abusive language datasets from different sources and labeling them through crowd2799 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sourcing or user moderation (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018; Wulczyn et al., 2017). Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system (Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). However, these works do not explicitly address any model bias in their models. Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. Bolukbasi et al. (2016) is one of the first works to point out the gender stereotypes inside word2vec (Mikolov et al., 2013) and propose an algorithm to correct them. Caliskan et al. (2017) also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside w"
D18-1302,W16-5618,0,0.141031,"gender swap data augmentation, (3) fine-tuning with a larger corpus. Moreover, we compare the effects of different pre-trained word embeddings and model architectures on gender bias. 2 Related Work So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd2799 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sourcing or user moderation (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018; Wulczyn et al., 2017). Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system (Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). However, these works do not explicitly address any model bias in their models. Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. Bolukbasi et al. (2016"
D18-1302,N16-2013,0,0.0907294,"sed word embedding, (2) gender swap data augmentation, (3) fine-tuning with a larger corpus. Moreover, we compare the effects of different pre-trained word embeddings and model architectures on gender bias. 2 Related Work So far, many efforts were put into defining and constructing abusive language datasets from different sources and labeling them through crowd2799 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799–2804 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sourcing or user moderation (Waseem and Hovy, 2016; Waseem, 2016; Founta et al., 2018; Wulczyn et al., 2017). Many deep learning approaches have been explored to train a classifier with those datasets to develop an automatic abusive language detection system (Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). However, these works do not explicitly address any model bias in their models. Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. Bolukbas"
D18-1302,N18-2003,0,0.162066,"algorithm to correct them. Caliskan et al. (2017) also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. Dixon et al. (2017) is one of the first works that point out existing “unintended” bias in abusive language detection models. Kiritchenko and Mohammad (2018) compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems. Zhao et al. (2018) shows the effectiveness of measuring and correcting gender biases in coreference resolution tasks. We later show how we extend a few of these works into ours. 3 3.1 Datasets Sexist Tweets (st) This dataset consists of tweets with sexist tweets collected from Twitter by searching for tweets that contain common terms pertaining to sexism such as “feminazi.” The tweets were then annotated by experts based on criteria founded in critical race theory. The original dataset also contained a relatively small number of “racist” label tweets, but we only retain “sexist” samples to focus on gender biase"
D19-1012,S19-2005,0,0.0266457,"l., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empathetic dialogues and train multi-task models on it. 3 Mixture of Empathetic Listeners The dialogue context is an alternating set of utter"
D19-1012,P18-5002,0,0.0254446,"Missing"
D19-1012,P19-1358,0,0.0329288,"erban et al., 2016; Vinyals and Le, 2015; Wolf et al., 2019). A recent trend is to produce personalized responses by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et"
D19-1012,D16-1230,0,0.213158,"Missing"
D19-1012,P19-1542,1,0.807543,"es by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017)"
D19-1012,P18-1136,1,0.896748,"Missing"
D19-1012,D18-1298,0,0.414457,"rsation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rashkin et al., 2018) about how an empathetic person would respond to the stressful situation the Speaker has been through. However, despite the importance of empathy and emotional understanding in human conversations, it is still very challenging to train a dialogue agent able to recognize and respond with the correct emotion. So far, to solve the problem of empathetic dialogue response generation, which is to understand the user emot"
D19-1012,S19-2184,1,0.802043,"follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empathetic dialogues and train multi-task mode"
D19-1012,D16-1147,0,0.0241154,"for each token. i where T RSDec refers to the i-th listener, including the shared one. Conceptually, we expect that 0 , to be the output from the shared listener, T RSDec a general representation which can help the model to capture the dialogue context. On the other hand, we expect that each empathetic listener learns how to respond to a particular emotion. To model this behavior, we assign different weights to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. To elaborate, we construct a Key-Value Memory Network (Miller et al., 2016) and represent each memory slot as a vector pair (ki , Vi ), where ki ∈ Rdmodel denotes the key vector and Vi is from Equation 4. Then, the encoder informed query q is used to address the key vectors k by performing a dot product followed by a Softmax function. Thus, we have: tracker. We first flatten all dialogue turns in C, and map each token into its vectorial representation using the context embedding E C . Then the encoder encodes the context sequence into a context representation. We add a query token QRY at the beginning of each input sequence as in BERT (Devlin et al., 2018), to comput"
D19-1012,N16-1014,0,0.269283,"y less focus is emotional understanding and empathy (Rashkin et al., 2018; Dinan et al., 2019; Wolf et al., 2019). Intuitively, ordinary social conversations between two humans are often about their daily lives that revolve around happy or sad experiences. In such scenarios, people generally tend to respond in a way that acknowledges the feelings of their conversational partners. Introduction Neural network approaches for conversation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rash"
D19-1012,P02-1040,0,0.104181,"Missing"
D19-1012,P16-1094,0,0.266273,"y less focus is emotional understanding and empathy (Rashkin et al., 2018; Dinan et al., 2019; Wolf et al., 2019). Intuitively, ordinary social conversations between two humans are often about their daily lives that revolve around happy or sad experiences. In such scenarios, people generally tend to respond in a way that acknowledges the feelings of their conversational partners. Introduction Neural network approaches for conversation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rash"
D19-1012,D14-1162,0,0.0820006,"l MoEL vs TRS MoEL vs Multi-TRS 4 4.1 p(r1:t |C, r0:t−1 ) = softmax(O&gt; W ) (9) where O ∈ Rdmodel ×t is the output of meta listener and p(r1:t |C, r0:t−1 ) is a distribution over the vocabulary for the next tokens. We then use a standard maximum likelihood estimator (MLE) to optimize the response prediction: L2 = − log p (St |C) 4.2 Experiment Dataset Training We train our model using Adam optimizer (Kingma and Ba, 2014) and varied the learning rate during training following (Vaswani et al., 2017). The weight of both losses α and β are set to 1 for simplicity. We use pre-trained Glove vectors (Pennington et al., 2014) to initialize the word embedding and we share it across the encoder and the decoder. The rest of the parameters are randomly initialized. In the early training stage, emotion tracker randomly assign weights to the listeners, and may send noisy gradient flow back to the wrong listeners, which can make the model convergence harder. To stabilize the learning process, we replace the distribution p of the listeners with the or(10) Lastly, all the parameters are jointly trained endto-end to optimize the listener selection and response generation by minimizing the weightedsum of two losses: L = αL1"
D19-1012,N19-1126,0,0.045197,"Missing"
D19-1012,W18-5713,0,0.0125101,"019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative mo"
D19-1012,S19-2021,1,0.875815,"Missing"
D19-1012,W18-6243,1,0.763174,"istent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empa"
D19-1012,K18-1053,0,0.0147919,"roduce personalized responses by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dial"
D19-1012,P18-1205,0,0.0948744,"Missing"
D19-1012,P18-1104,0,0.0734934,"lly, our analysis demonstrates that not only MoEL effectively attends to the right listener, but also each listener learns how to properly react to its corresponding emotion, hence allowing a more interpretable generative process. of work. The first is a multi-task approach that jointly trains a model to predict the current emotional state of the user and generate an appropriate response based on the state (Lubis et al., 2018; Rashkin et al., 2018). Instead, the second line of work focuses on conditioning the response generation to a certain fixed emotion (Hu et al., 2017; Wang and Wan, 2018; Zhou and Wang, 2018; Zhou et al., 2018). Both cases have succeeded in generating empathetic and emotional responses, but have neglected some crucial points in empathetic dialogue response generation. 1) The first assumes that by understanding the emotion, the model implicitly learns how to respond appropriately. However, without any additional inductive bias, a single decoder learning to respond for all emotions will not only lose interpretability in the generation process, but will also promote more generic responses. 2) The second assumes that the emotion to condition the generation on is given as input, but w"
D19-1012,D16-1127,0,\N,Missing
D19-1012,D16-1110,1,\N,Missing
D19-1012,W19-5917,0,\N,Missing
D19-1129,D16-1250,0,0.0328322,"mains (weather, alarm, and reminder) and translate them using bilingual lexicons. We refine the embeddings by leveraging the framework proposed in Artetxe et al. (2017). Let X and Z be the aligned cross-lingual word embeddings between two languages. Xi∗ and Zj∗ are the embeddings for the ith source word and j th target word. We denote a binary dictionary matrix D: Dij = 1 if the ith source language word is aligned with the j th target language word and Dij = 0 otherwise. The goal is to find the optimal mapping matrix W∗ by minimizing: W∗ = arg min W X Dij ||Xi∗ W − Zj∗ ||2 . (1) i,j Following Artetxe et al. (2016), with orthogonal constraints, mean centering, and length normaliza1298 1 The embeddings are available in https://fasttext.cc tion, we can maximize the following instead: W∗ = arg max Tr(XWZT DT ). (2) W We iteratively optimize Equation 2 until distances between domain-related seed words are closer than a certain threshold after refinement. Figure 1 illustrates better alignment for domainrelated words after refinement. 3.2 Gaussian Noise Injection To cope with the noise in alignments, we inject Gaussian noise to English embeddings, so the trained model will be more robust to variance. This is"
D19-1129,P17-1042,0,0.119995,"the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-switching name entity recognition by combining multiple monolingual word embeddings. Chen et al. (2018) proposed a teacher-student frame"
D19-1129,Q17-1010,0,0.0433367,"r, forecast, temperature, rain, hot, cold, remind, forget, alarm, cancel, tomorrow, which are related to the three dialogue domains (weather, alarm, and reminder). We translate them by leveraging bilingual dictionaries2 . The corresponding translations in Spanish and Thai are clima, pron´ostico, temperatura, lluvia, caliente, fr´ıo, recordar, olvidar, alarma, cancelar, ma˜nana and อากาศ, พยากรณ, อุณหภูมิ, ฝน, รอน, หนาว, 2 https://github.com/facebookresearch/MUSE 4.4 Evaluation We implement and evaluate the following models: Zero-shot SLU Upadhyay et al. (2018) used cross-lingual embeddings (Bojanowski et al., 2017) to do zero-shot transfer learning. Conditional Random Fields (CRF) We reproduce the baseline model in Schuster et al. (2019), and also add embedding noise, cross-lingual refinement, and delexicalization. Latent Variable Model (LVM) - Ours We replace the CRF module with latent variables and also apply it to intent prediction. Besides, we directly compare with the baseline models illustrated in Schuster et al. (2019): Multi. CoVe w/ auto They combined Multilingual CoVe (Yu et al., 2018) with an auto-encoder objective and then used the trained encoder with the CRF model. 1300 noche เ น What will"
D19-1129,D18-1038,0,0.197164,"Cortana) as a virtual agent to tend to the needs of the users. However, these agents have mostly been trained with the monolingual dataset that is often expensive to build or acquire. In order to cope with the scarcity of low-resource language dialogue data, we are motivated to look into cross-lingual dialogue systems which can adapt with very little or no training data in the target language. This task of zero-shot adaptation of dialogue systems to different languages is relatively new and has not been explored thoroughly enough yet. The main approach of previous work (Upadhyay et al., 2018; Chen et al., 2018; Schuster et al., 2019) in this task is using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskorient"
D19-1129,D17-1169,0,0.0396427,"but not negligible differences across languages. Instead, using latent variables will allow us to model the distribution that captures the variance of semantically similar sentences across different languages. The whole training process is defined as follows: [h1 ...ht ...hT ] = BiLSTM(e∗ ), (3) T X exp(mt ) mt = ht wa , at = PT , v= at ht , exp(m ) j j=1 t=1 (4)     S I µt µ = WrS ht , = WrI v, log σtS )2 log σ I )2 (5) ztS ∼ qtS (z|ht ), z I ∼ q I (z|v), (6) pSt (st |ztS ) = Softmax(WgS ztS ), (7) I I p (I|z ) = Softmax(WgI z I ), (8) where attention vector (v) is obtained by following Felbo et al. (2017) and wa is the weight {S,I} matrix for the attention layer, W{r,g} are trainable parameters, superscripts S and I refer to slot prediction and intent detection respectively, subscript “r” refers to “recognition” for obtaining the LI = Ez I [log pI (I|z I )], LSt = EztS [log pSt (st |ztS )], LS = T X LSt , (9) (10) (11) t=1 hence, the final objective function to minimize is, L = LS + LI . (12) The model prediction is not deterministic since the latent variables ztS and z I are sampled from the Gaussian distributions. Therefore, in the inference time, we use the true mean µSt and µI to replace z"
D19-1129,N18-1032,0,0.0164652,"d achieve state-of-the-art results in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to"
D19-1129,D18-1330,0,0.0233275,"Missing"
D19-1129,S19-2184,1,0.762048,"f-the-art results in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lin"
D19-1129,W19-5327,1,0.850579,"in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-represen"
D19-1129,N19-1380,0,0.291983,"al agent to tend to the needs of the users. However, these agents have mostly been trained with the monolingual dataset that is often expensive to build or acquire. In order to cope with the scarcity of low-resource language dialogue data, we are motivated to look into cross-lingual dialogue systems which can adapt with very little or no training data in the target language. This task of zero-shot adaptation of dialogue systems to different languages is relatively new and has not been explored thoroughly enough yet. The main approach of previous work (Upadhyay et al., 2018; Chen et al., 2018; Schuster et al., 2019) in this task is using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen"
D19-1129,W19-4320,1,0.821524,"of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-switching name entity recognition by combining multiple monolingual word embeddings. Chen et al. (2018) proposed a teacher-student framework leveraging bilingual data for crosslingual transfer learning in dialogue state trackOur model consists of a refined cross-lingual embedding layer followed by a BiLSTM (Hochreiter and Schmidhuber, 1997) which parameterizes the Latent Variable Model, as illustrated in Figure 2. We jointly train our model to predict both slots and user intents. We denote w = [w1 , . . . , wT ] as th"
D19-1129,W18-6243,1,0.833712,"ation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-s"
D19-1129,W18-3023,0,0.0445921,"nd evaluate the following models: Zero-shot SLU Upadhyay et al. (2018) used cross-lingual embeddings (Bojanowski et al., 2017) to do zero-shot transfer learning. Conditional Random Fields (CRF) We reproduce the baseline model in Schuster et al. (2019), and also add embedding noise, cross-lingual refinement, and delexicalization. Latent Variable Model (LVM) - Ours We replace the CRF module with latent variables and also apply it to intent prediction. Besides, we directly compare with the baseline models illustrated in Schuster et al. (2019): Multi. CoVe w/ auto They combined Multilingual CoVe (Yu et al., 2018) with an auto-encoder objective and then used the trained encoder with the CRF model. 1300 noche เ น What will the weather be like this evening weather อากาศ clima Cancel tuesday alarm clock evening English Spanish Thai English Spanish Thai Figure 3: Visualization of latent variables on words (left) and sentences (right). Left: We choose “weather-climaอากาศ” and “evening-noche- เยน” from parallel sentences. English: “What will the weather be like this evening”, Spanish: “C´omo ser´a el clima esta noche”, Thai: “ตอน เยน นี อากาศ จะ เปน อยางไร”. Right: We choose two English sentences and sh"
D19-1129,W18-5001,0,0.039756,"gual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen et al., 2017; Zhao et al., 2017, 2018; Zhao and Eskenazi, 2018; Le et al., 2018). However, we notice that naively using latent variables does not help the model improve much in slot filling and intent prediction. We hypothesize that the variance of the cross-lingual word embeddings is too large for the model to learn any meaningful latent variables. Hence, we propose to first refine the cross-lingual embeddings with ∼10 seed word-pairs related to the dialogue domains. We then add Gaussian noise (Zheng et al., 2016) to further compensate the imperfect alignment of cross-lingual embeddings. As a result, a combination of these methods allows us to build a t"
D19-1129,P18-1101,0,0.0456593,"Missing"
D19-1129,P17-1061,0,0.0291931,"s using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen et al., 2017; Zhao et al., 2017, 2018; Zhao and Eskenazi, 2018; Le et al., 2018). However, we notice that naively using latent variables does not help the model improve much in slot filling and intent prediction. We hypothesize that the variance of the cross-lingual word embeddings is too large for the model to learn any meaningful latent variables. Hence, we propose to first refine the cross-lingual embeddings with ∼10 seed word-pairs related to the dialogue domains. We then add Gaussian noise (Zheng et al., 2016) to further compensate the imperfect alignment of cross-lingual embeddings. As a result, a combination of these"
D19-1303,N18-1150,0,0.0156643,"t applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summarie"
D19-1303,D17-1169,0,0.0359551,"ansfer the style by directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word level(Tang et al., 2016; Xu et al., 2018b) and sentence level(Felbo et al., 2017; Winata et al., 2019, 2018; Park et al., 2018; Lee et al., 2019). It has also been considered an important factor in engaging interactive systems(Lin et al., 2019b; Winata et al., 2017; Zhou et al., 2018a). Although we observe that sensational headlines contain emotion, it is still not clear which emotion and how emotions will influence the sensationalism. 6 Conclusion and Future Work In this paper, we propose a model that generates sensational headlines without labeled data using Reinforcement Learning. Firstly, we propose a distant supervision strategy to train the sensationalism scorer. As"
D19-1303,P16-1154,0,0.023977,"that our 6 https://www.figure-eight.com/ Table 2: Generated Chinese headlines from different models. Our model (Pointer-Gen+ARL-SEN) sensationalized the headline with the phrase “In Serious Trouble!”. model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summarization baselines, to validate that our implementation achieves comparable performance to existing work. In our implementation, Pointer-Gen achieves a 34.51 RG-1 score, 22.21 RG-2 score, and 31.68 RG-L score, which is similar to the results of Gu et al. (2016). PointerGen+ARL-SEN, although optimized for the sensationalism reward, achieves similar performance to our Pointer-Gen baseline, which means that Pointer-Gen+ARL-SEN still keeps its summarization ability. An example of headlines generated from different models in Table 2 shows that Pointer-Gen and Pointer-Gen+RL-ROUGE learns to summarize the main point of the article: “The Nikon D600 camera is reported to have black spots when taking photos”. Pointer-Gen+RL-SEN 3070 Model Pointer-Gen Pointer-Gen-Pos Pointer-Gen+Same-FT Pointer-Gen+Pos-FT Pointer-Gen+RL-ROUGE Pointer-Gen+RL-SEN Pointer-Gen+ARL"
D19-1303,S18-1039,1,0.900112,"Missing"
D19-1303,D15-1229,0,0.198555,"ARL loss function becomes: LARL-SEN = (1 − αsen (y ∗ )) LRL + αsen (y ∗ ) LMLE (15) If αsen (y ∗ ) is high, meaning the training headline is sensational, our loss function encourages our model to imitate the sample more using the MLE training. If αsen (y ∗ ) is low, our loss function replies on RL training to improve the sensationalism. Note that the weight αsen (y ∗ ) is different from our sensationalism reward αsen (y s ) and we call the loss function Auto-tuned Reinforcement Learning, because the ratio between MLE and RL are well “tuned” towards different samples. 3.3 Dataset We use LCSTS (Hu et al., 2015) as our dataset to train the summarization model. The dataset is collected from the Chinese microblogging website Sina Weibo. It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text. The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing. We tokenize each sentence with Jieba 4 and a vocabulary size of 50000 is saved. 3068 4 https://github.com/fxsjy/jieba 3.5 Figure 2: The probability density function (pdf) of predicted sensationalism score in log scale. Low sensationalism score has"
D19-1303,N18-2102,0,0.0208419,"attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu"
D19-1303,D18-1207,0,0.0344404,"Missing"
D19-1303,S19-2184,1,0.872022,"Missing"
D19-1303,D15-1166,0,0.0385727,"Pointer-Gen Headline Generator We choose Pointer Generator (Pointer-Gen) (See et al., 2017), a widely used summarization model, as our headline generator for its ability to copy words from the input article. It takes a news article as input and generates a headline. Firstly, the tokens of each article, {x1 , x2 , x3 , · · · , xM }, are fed into the encoder one-by-one and the encoder generates a sequence of hidden states hi . For each decoding step t, the decoder receives the embedding for each token of a headline yt as input and updates its hidden states st . An attention mechanism following Luong et al. (2015) is used: Figure 1: The loss function of Auto-tuned Reinforcement Learning is a weighted sum of LRL and LMLE , where the weight is decided by our sensationalism scorer. eti = v T tanh(Wh hi + Ws st + battn ) t t a = softmax(e ) X h∗t = ati hi (2) (3) i 0.50 averaged F1 score. This confirms that the predicted sensationalism score can partially capture the sensationalism of headlines. On the other hand, a more natural choice is to take headlines with few comments as negative examples. Thus, we train another baseline classifier on a crawled balanced sensationalism corpus of 84k headlines where th"
D19-1303,K16-1028,0,0.0832857,"Missing"
D19-1303,Q18-1027,0,0.0194518,"se extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) transfer the styles by separating sentence representations into content and style, for example using backtranslation(Prabhumoye et al., 2018). However, these methods cannot guarantee the content consistency between the original sentence and transferred output (Xu et al., 2018a). Explicit methods (Zhang et al., 2018b; Xu et"
D19-1303,P18-1080,0,0.170636,"eadline, we collect a sensationalism dataset and then train a sensationalism scorer. For the sensationalism dataset collection, we choose headlines with many comments from popular online websites as positive samples. For the negative samples, we propose to use the generated headlines from a sentence summarization model. Intuitively, the summarization model, which is trained to preserve the semantic meaning, will lose the sensationalization ability and thus the generated negative samples will be less sensational than the original one, similar to the obfuscation of style after back-translation (Prabhumoye et al., 2018). For example, an original headline like “一 趟 挣10万？铁总增开申通、顺丰专列” (One trip to earn 100 thousand? China Railway opens new 3 https://github.com/HLTCHKUST/ sensational_headline Shentong and Shunfeng special lines) will become “中铁总将增开京广两列快递专列” (China Railway opens two special lines for express) from the baseline model, which loses the sensational phrases of “一 趟 挣10万 ？” (One trip to earn 100 thousand?) . We then train the sensationalism scorer by classifying sensational and nonsensational headlines using a one-layer CNN with a binary cross entropy loss Lsen . Firstly, 1-D convolution is used to ext"
D19-1303,P18-2025,0,0.0678051,"Missing"
D19-1303,D15-1044,0,0.0549155,"will hurt the performance, both in sensationalism and fluency. After manually checking the outputs, we observe that our model is able to generate sensational headlines using diverse sensationalization strategies. These strategies include, but are not limited to, creating a curiosity gap, asking questions, highlighting numbers, being emotional and emphasizing the user. Examples can be found in Table 4. 5 Related Work Our work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summar"
D19-1303,P17-1099,0,0.0703767,"ur work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al."
D19-1303,P17-1108,0,0.0603074,"Missing"
D19-1303,P18-1090,0,0.108045,"Missing"
D19-1303,W18-6243,1,0.839564,"ion task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) transfer the styles by separating sentence representations into content and style, for example using backtranslation(Prabhumoye et al., 2018). However, these methods cannot guarantee the content consistency between the original sentence and transferred output (Xu et al., 2018a). Explicit methods (Zhang et al., 2018b; Xu et al., 2018a) transfer the style by directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word"
D19-1303,D18-1088,0,0.0168206,"arse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performan"
D19-1303,D18-1138,0,0.025251,"arse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performan"
D19-1303,P18-1061,0,0.0231592,"et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achi"
D19-1303,P17-1101,0,0.018251,"ally checking the outputs, we observe that our model is able to generate sensational headlines using diverse sensationalization strategies. These strategies include, but are not limited to, creating a curiosity gap, asking questions, highlighting numbers, being emotional and emphasizing the user. Examples can be found in Table 4. 5 Related Work Our work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2"
D19-1303,S19-2021,1,0.844097,"directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word level(Tang et al., 2016; Xu et al., 2018b) and sentence level(Felbo et al., 2017; Winata et al., 2019, 2018; Park et al., 2018; Lee et al., 2019). It has also been considered an important factor in engaging interactive systems(Lin et al., 2019b; Winata et al., 2017; Zhou et al., 2018a). Although we observe that sensational headlines contain emotion, it is still not clear which emotion and how emotions will influence the sensationalism. 6 Conclusion and Future Work In this paper, we propose a model that generates sensational headlines without labeled data using Reinforcement Learning. Firstly, we propose a distant supervision strategy to train the sensationalism scorer. As a result, we achieve"
D19-5827,buck-etal-2014-n,0,0.0699868,"Missing"
D19-5827,N18-1143,0,0.0462149,"Missing"
D19-5827,P17-1055,0,0.116098,"ore of 68.98, which significantly improves the BERT-Large baseline by 8.39 and 7.22, respectively. 1 Introduction Reading comprehension (RC) is a fundamental human skills needed to answer questions that require knowledge of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on r"
D19-5827,P19-1285,0,0.0283993,"n’s Book Test, and SQuAD datasets. Baseline MRQA organizers have released the BERT-base and BERT-large models as baselines implemented using the AllenNLP (Gardner et al., 2018) platform. 1 The BERT transformer receives a passage and a question that is separated by an [SEP] token. On top of this, the baseline models deploys a linear layer to find the corresponding span which answers the question from the passage. 3.2 XLNet 4 Model XLNet (Yang et al., 2019) is a recently proposed generalized autoregressive pre-training model for language understanding which naively follows the Transformer(-XL) (Dai et al., 2019) architecture. Instead of the bidirectional encoding structure used in BERT (Devlin et al., 2019), XLNet leverages a permutation language modeling objective and target-aware representations with a two-stream attention mechanism to enable the model to capture the context on both sides. Besides the datasets which are also used in the pre-training procedure of BERT (Devlin et al., 2019), XLNet involves Giga5 (Parker et al., 2011), ClueWeb 2012-B (an extension version of Callan et al. (2009)) and Common Crawl (Buck et al., 1 Attention-over-Attention 4.1 Experiments Preprocessing The original setti"
D19-5827,N19-1423,0,0.485599,"ention (Chung et al., 2017; Sun et al., 2018) has been paid towards generalization, i.e., building QA systems that can generalize well on different datasets and transfer to new domains quickly. One major factor that could contribute to generalization, is effective contextual representation (Talmor and Berant, 2019). Recently, models pretrained on a large unlabeled corpus, by adding an extra final layer and fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet (Yang et al., 2019), the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how to adapt to new QA tasks using few or even no prior training examples. McCann et al. (2018); Liu et al. (2019); Talmor and Berant (2019) show that promising results can be obtained in transferring to new domains by training models on multiple tasks simultaneously using multi-task learnWith a large numbe"
D19-5827,P17-1168,0,0.0262056,"7.22, respectively. 1 Introduction Reading comprehension (RC) is a fundamental human skills needed to answer questions that require knowledge of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models"
D19-5827,D18-2012,0,0.0285063,"XLNet involves Giga5 (Parker et al., 2011), ClueWeb 2012-B (an extension version of Callan et al. (2009)) and Common Crawl (Buck et al., 1 Attention-over-Attention 4.1 Experiments Preprocessing The original setting of the sequence length is 512 in the XLNet-large model, but because of the constraint on the computational ability of a single GPU, a trade-off is made between the size of the context and the performance of the model. The sequence length is set as 340 when fine-tuning on the GPU but kept at 512 on the tensor processing unit (TPU). All the datasets are tokenized with SentencePiece (Kudo and Richardson, 2018) and uniformed in lower cases. 4.2 Data Analysis Datasets Under the scenario of this task, the model should be trained on six training datasets. https://github.com/mrqa/MRQA-Shared-Task-2019 205 datasets separately, and then evaluate the model on all the in-domain and out-of-domain development sets. More details about fine-tuning the XLNet model on the GPU are mentioned in §4.4. The evaluation results can be found in Table 3. When evaluating the in-domain datasets, the similarity can be computed as Similarity = GPU 12 - 24 (13) 16 4 340 512, 384, 1 (1) where Pij refers to the F1 score when fin"
D19-5827,N19-1246,0,0.0373277,"to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poorly in unseen domains due"
D19-5827,Q19-1026,0,0.0167794,"on language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poo"
D19-5827,D17-1082,0,0.155324,"tanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models to learn useful representations more generally by unifying tasks under a single perspective. Thus, a model, which is trained on multiple source QA datasets, can"
D19-5827,K17-1034,0,0.0169416,"e than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language representations, QA models might still perform poorly in unseen domains due to the data scarcity. MLP ... /2 XLNet /2−1 ... 1 Batch Generation ... ... ... SQuAD NewsQA ... ... 2.3 2.1 Multi-task Learning Liu et al. (2019) proposed a multi-tas"
D19-5827,P19-1441,0,0.124321,"d fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet (Yang et al., 2019), the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how to adapt to new QA tasks using few or even no prior training examples. McCann et al. (2018); Liu et al. (2019); Talmor and Berant (2019) show that promising results can be obtained in transferring to new domains by training models on multiple tasks simultaneously using multi-task learnWith a large number of datasets being released and new techniques being proposed, Question answering (QA) systems have witnessed great breakthroughs in reading comprehension (RC) tasks. However, most existing methods focus on improving in-domain performance, leaving open the research question of how these models and techniques can generalize to out-ofdomain and unseen RC tasks. To enhance the generalization ability, we p"
D19-5827,P17-1147,0,0.0437326,"ge model and a next-sentence prediction objective. Recently, XLNet (Yang et al., 2019), a permutation language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leve"
D19-5827,D16-1264,0,0.457475,"of the world and understanding of natural language. This task is essential for intelligent dialogue systems to quickly respond in a search engine or a product recommendation system. Recently, we have witnessed several breakthroughs in question answering (QA) systems, such as bidirectional attention flow (BiDAF) (Seo et al., 2017), the attention over attention mechanism (AoA) (Cui et al., 2017), and a multi-hop architecture using gated-attention readers (Dhingra et al., 2017). A large number of QA datasets have been proposed in recent years for single-hop and multi-hop reasoning applications (Rajpurkar et al., 2016; Lai et al., 2017; Saha et al., 2018; Trischler et al., ∗ These two authors contributed equally. 203 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 203–211 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics ing. Multi-task learning explores the relationships between different tasks by capitalizing on relatedness while mitigating interference from dissimilarities, thus forcing models to learn useful representations more generally by unifying tasks under a single perspective. Thus, a model, which is trained on multiple sourc"
D19-5827,P19-1485,0,0.123317,"xam writers, etc.), and the relationship of the question to the passage are different among datasets (e.g., collected as independent vs. dependent on evidence, multi-hop, etc). The availability of such datasets promotes the development of models that work well for only a specific domain. However, little attention (Chung et al., 2017; Sun et al., 2018) has been paid towards generalization, i.e., building QA systems that can generalize well on different datasets and transfer to new domains quickly. One major factor that could contribute to generalization, is effective contextual representation (Talmor and Berant, 2019). Recently, models pretrained on a large unlabeled corpus, by adding an extra final layer and fine-tuning on task-specific supervised data, obtained breakthrough performances on many language understanding tasks such as the GLUE benchmark and the SQuAD QA task (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019). This indicates the power of pre-trained language models in representing contextual information. Thus, we adopt XLNet (Yang et al., 2019), the state-of-the-art pre-trained language model as our language representation. Another critical issue related to generalization is how t"
D19-5827,W17-2623,0,0.190951,"e Model Fine-tuning Dan Su∗, Yan Xu∗ , Genta Indra Winata, Peng Xu, Hyeondey Kim, Zihan Liu, Pascale Fung Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong {dsu, yxucb, giwinata, pxuab}@connect.ust.hk, {hdkimaa, zliucr}@connect.ust.hk, pascale@ece.ust.hk Abstract 2017; Joshi et al., 2017). However, each QA dataset is built for a particular domain and focus (Talmor and Berant, 2019). Dataset passages cover different topics, such as movies (Saha et al., 2018), news (Trischler et al., 2017), and biomedicine (Tsatsaronis et al., 2012). Also, the styles of questions (e.g., entity-centric, relational, other tasks reformulated as QA, etc.), the sources (e.g., crowd-workers, domain experts, exam writers, etc.), and the relationship of the question to the passage are different among datasets (e.g., collected as independent vs. dependent on evidence, multi-hop, etc). The availability of such datasets promotes the development of models that work well for only a specific domain. However, little attention (Chung et al., 2017; Sun et al., 2018) has been paid towards generalization, i.e., b"
D19-5827,D18-1259,0,0.0269959,"XLNet (Yang et al., 2019), a permutation language model, was introduced to leverage the bidirectional context and overcome the drawbacks of BERT due to its autoregressive nature. XLNet-based models have already achieved better performance than BERTbased models on many NLP tasks. 2.2 Unlike traditional knowledge-based QA (Kalyanpur et al., 2012), nowadays, many QA systems involve natural language understanding and knowledge of the world. Many datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), DROP (Dua et al., 2019), RACE (Lai et al., 2017), DueRC (Saha et al., 2018), BioASQ (Tsatsaronis et al., 2012), TextbookQA (Kembhavi et al., 2017), and RelationExtraction (Levy et al., 2017), have been published for specific QA tasks. Among all these tasks, one of the most widely studied one is extractive QA, which is to find a directly mentioned span in the article which answers the particular question. Although many studies on extractive QA have achieved significant improvements by leveraging attention-based models and pre-trained language repre"
H92-1014,H91-1042,1,0.887996,"s optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI. The basic interface 76 Condition N WE Text FaUback on &quot; &quot; Fallback off &quot; &quot; (1) 1 5 20 1 5 20 47.9 64.6 58.0 60.1 64.2 56.9 59.0 5 56.6 Two Pass Finally, we"
H92-1014,H92-1061,1,0.872639,"Missing"
H92-1014,H92-1062,1,0.888034,"e October '91 dry-run corpus as development test in Table 4. The results of our experiments indicated that an N of 5 was optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI. The basic interface 76 Condition N W"
H92-1014,H89-2027,1,0.880499,"10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and trigram language models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would n"
H92-1014,H91-1012,0,0.032865,"uage models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would not be represented in the new test (dialects were predominantly southern in the ATIS0 subcorpus). The overall system architecture for this evaluation is similar to that used in the February '91 tests [6]. Specifically, we use a 4-pass approach to produce the N-best lists for natural language processing. We filtered the training data for quality in several ways. All utterances that were marked as truncated in the SRO (speech recognition output) transcription were ignored. Similarly, we omitted from the training all utterances that contained a word fragment, We also ignored any utterances 1. A forward pass with a bigram grammar and discrete 72 that contained rare nonspeech events. Finally, our forwardbackward training program rejected any input that failed to align properly. These steps removed"
H92-1014,H92-1003,0,0.105162,"the N-best interface. Results are presented for speech recognition alone and for the overall spoken language system. A detailed discussion of DELPHI is presented in [2,3] elsewhere in these proceedings. 2. B Y B L O S - S P E E C H 2.1 Training and D e v e l o p m e n t Test Data We used speech data from the ATIS2 subcorpus exclusively to train the parameters of the acoustic model. This subcorpus consists of 10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and"
H92-1014,H90-1003,1,0.903119,"10411 spontaneous utterances from 286 subjects. The data originated from 5 collection sites using a variety of strategies for eliciting and capturing spontaneous queries from the subjects [7]. The training data was not balanced across the five sites, however. MIT was represented by 3-4 times as much data as any other site. Overall, MIT data accounted for nearly half of the ATIS2 subcorpus (4600 utterances). RECOGNITION The BYBLOS speech recognition system produces an ordered list of the N top-scoring hypotheses which is then reordered by several detailed knowledge sources. The Nbest strategy [4,8] permits the use of computationally prohibitive models by greatly reducing the search space to a few dozen word sequences. It has enabled us to use crossword-boundary triphone models and trigram language models with ease. The N-best list is also a robust interface between speech and natural language that provides a way to recover from speech errors in the top choice word sequence. The evaluation test data was drawn from this same pool of data so we decided to ignore the earlier batches of ATIS data that were collected under still different circumstances (most of it was read speech) and would n"
H92-1014,H89-2006,1,0.902556,"s of one such experiment, utilizing the October '91 dry-run corpus as development test in Table 4. The results of our experiments indicated that an N of 5 was optimal, and that the two-pass processing strategy was slightly better than either of the others. This was the configuration we used on the February '92 evaluation data. SPOKEN LANGUAGE UNDERSTANDING - HARC, BBN's spoken language system, utilizes BYBLOS as its speech recognition component, and DELPHI as its natural language understanding component. DELPHI uses a definite clause grammar formalism, augmented by the use of constraint nodes [9] and a labelled argument formalism [3]. The parsing algorithm uses a statistically trained agenda to produce the single best parse for an input utterance [1]. In Table 5 we show our Weighted Error on the February '92 evaluation data for Combined Class A+D, and Classes A and D separately, as calculated by NIST. During the test run, we had neglected to include the date information provided for individual scenarios. We include the results of a re-run with the same system as ran the February '92 test set, with We experimented with several conditions to optimize the connection of BYBLOS with DELPHI"
I05-1023,P95-1033,1,0.878394,"e available. But while loose translations by themselves already have numerous applications, truly parallel sentence translations provide invaluable types of information for the aforementioned types of mining and induction, which cannot easily be obtained from merely loose translations or comparable sentence pairs. In particular, truly parallel bi-sentences are especially useful for extracting more precise syntactic and semantic relations within word sequences. We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy signiﬁcantly higher than previous known methods. We focus here on very non-parallel quasi-comparable monolingual corpora, which are available in far larger quantities but are signiﬁcantly more diﬃcult to mine than either noisy parallel corpora or comparable corpora. The majority of previous work has concerned noisy parallel corpora (sometimes imprecisely also called “comparable corpora”), which contain non-aligned sentences that are nevertheless mostly bilingual translations of the same doc"
I05-1023,J97-3002,1,0.819269,"But while loose translations by themselves already have numerous applications, truly parallel sentence translations provide invaluable types of information for the aforementioned types of mining and induction, which cannot easily be obtained from merely loose translations or comparable sentence pairs. In particular, truly parallel bi-sentences are especially useful for extracting more precise syntactic and semantic relations within word sequences. We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy signiﬁcantly higher than previous known methods. We focus here on very non-parallel quasi-comparable monolingual corpora, which are available in far larger quantities but are signiﬁcantly more diﬃcult to mine than either noisy parallel corpora or comparable corpora. The majority of previous work has concerned noisy parallel corpora (sometimes imprecisely also called “comparable corpora”), which contain non-aligned sentences that are nevertheless mostly bilingual translations of the same document. More r"
I05-1023,P03-1019,0,0.0446484,"es. Conversely, ITGs should do well at rejecting candidates where (1) too many words in one sentence ﬁnd no correspondence in the other, (2) frames do not nest in similar ways in the candidate sentence pair, or (3) too many arguments must be transposed to achieve an alignment—all of which would suggest that the sentences probably express diﬀerent ideas. Various forms of empirical conﬁrmation for the ITG Hypothesis have emerged recently, which quantitatively support the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens"
I05-1023,C04-1060,0,0.0325904,"eve an alignment—all of which would suggest that the sentences probably express diﬀerent ideas. Various forms of empirical conﬁrmation for the ITG Hypothesis have emerged recently, which quantitatively support the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal brac"
I05-1023,P01-1067,0,0.029135,", which quantitatively support the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we des"
I05-1023,P05-1059,0,0.0266738,"t the qualitative cross-linguistic characteristics just described across a variety of language pairs and tasks. Zens and Ney (2003) [3] show that ITG constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our"
I05-1023,C04-1030,0,0.0190595,"constraints yield signiﬁcantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quas"
I05-1023,P05-1033,0,0.00610294,"-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using Bracketing ITGs produces signiﬁcantly lower Chinese-English alignment error rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. With regard to translation rather than alignment accuracy, Zens et al. (2004) [7] show that decoding under ITG constraints yields signiﬁcantly lower word error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] obtains signiﬁcant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quasi-comparable corpus of Chinese and English from the topic detection task. 2 Recent Approaches to Mining Non-parallel Corpora Recent work (Fung a"
I05-1023,W04-3208,1,0.93986,"cant BLEU score improvements via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quasi-comparable corpus of Chinese and English from the topic detection task. 2 Recent Approaches to Mining Non-parallel Corpora Recent work (Fung and Cheung 2004 [9]; Munteanu et al. 2004 [10]; Zhao and Vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely based on ﬁnding on-topic documents ﬁrst through similarity matching and time alignment. However, Zhao and Vogel used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with “roughly similar sentence order 260 D. Wu and P. Fung of content”. This corpus can be more accurately described as a noisy parallel corpus. Munteanu et al. used comparable corpora of news articles published within the same 5-day window. In both cases, the corpora contain doc"
I05-1023,N04-1034,0,0.0267443,"ts via unsupervised induction of hierarchical phrasal bracketing ITGs. Such results partly motivate the work we discuss here. We will begin by surveying recent related work and reviewing the formal properties of ITGs. Subsequently we describe the architecture of our new method, which relies on multiple stages so as to balance eﬃciency and accuracy considerations. Finally we discuss experimental results on a quasi-comparable corpus of Chinese and English from the topic detection task. 2 Recent Approaches to Mining Non-parallel Corpora Recent work (Fung and Cheung 2004 [9]; Munteanu et al. 2004 [10]; Zhao and Vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely based on ﬁnding on-topic documents ﬁrst through similarity matching and time alignment. However, Zhao and Vogel used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with “roughly similar sentence order 260 D. Wu and P. Fung of content”. This corpus can be more accurately described as a noisy parallel corpus. Munteanu et al. used comparable corpora of news articles published within the same 5-day window. In both cases, the corpora contain documents on the same matching"
I05-1023,P99-1043,1,0.841703,"Missing"
I05-1023,2003.mtsummit-papers.32,0,0.0300776,"o reﬁne document matching and parallel sentence extraction. Convergence. The IBM model parameters, including sentence alignment score and word alignment scores, are computed in each iteration. The parameter values eventually stay unchanged and the set of extracted bi-sentence candidates also converges to a ﬁxed size. The iteration then terminates and returns the last set of bilingual sentence pairs as the generated candidate sentences. 5 ITG Scoring The ITG model computes scores upon the set of candidates generated in the preceding stage. A variant of the approach used by Leusch et al. (2003) [16] allows us to forego training to estimate true probabilities; instead, rules are simply given unit weights. This allows the scores computed by ITG biparsing to be interpreted as a generalization of classical Levenshtein string edit distance, where inverted block transpositions are also allowed. Even without probability estimation, Leusch et al. found excellent correlation with human judgment of similarity between translated paraphrases. As mentioned earlier, biparsing for ITGs can be accomplished eﬃciently in polynomial time, rather than the exponential time required for classical SDTGs. The b"
I05-1023,J93-2003,0,\N,Missing
I05-1023,P00-1056,0,\N,Missing
I11-1047,1998.amta-tutorials.5,0,0.198481,"Missing"
I11-1047,C10-1054,0,0.0292649,"n the target language, the system simply returns &lt;not-found&gt;. 2.1 Representing Source Document We cannot enter documents with thousands of words directly into an online search engine. We need to convert full text into keywords to perform automated queries. A keyword may exist in multiple articles. However, several keywords cam uniquely identify a document if they are grouped together as a keyword set (Jiang et al., 2009). We then translate each keyword to target language to form the initial query. There are several reasons why using the translated keyword set as query directly, as proposed by Hong et al. (2010), does not always yields the desired target document: 1) Keyword translation might not correspond to the actual words in the target document; 2) Certain keywords in the target document might have been removed by content editors; 3) There are errors in keyword translation or selection. It is essential to select appropriate keywords to find the desired target document in a search engine. Two conditions that an appropriate keyword set should satisfy are: (1) they should represent the document exclusively (Jiang et al., 2009) (2) they should have unique or common translation in both languages. We"
I11-1047,W06-1710,0,0.0758253,"Missing"
I11-1047,ma-2006-champollion,0,0.0653889,"Missing"
I11-1047,J05-4003,0,0.0870527,"Missing"
I11-1047,J03-3002,0,0.0532487,"cuments from the web. We suggest that parallel documents can be mined with high precision from web sites that are not necessarily parallel to each other. Parallel resources reside on a diverse range of websites which can be classified into the following categories: Parallel websites: single website with structurally aligned bilingual pages. Typically they are websites of institutions, governments and commercial companies. (e.g. Financial Times Chinese/English, Wall Street Journal Chinese/English). Structure based methods were previously proposed to mine parallel documents from these websites: Resnik and Smith (2003) used (1) parent pages containing links to versions of one document in different languages and (2) sibling pages contains link to translation of the current documents. They also rely on the URL and anchor text to spot language specific version of documents. A structural alignment using DOM tree representation was proposed by Shi et al. (2006) to align parallel documents by using HTML structure. They identify the translational equivalent texts and hyperlinks between two parallel DOM trees to find parallel documents. However, the web is a heterogeneous collection of documents that extend far bey"
I11-1047,P06-1062,0,0.0236433,"ly they are websites of institutions, governments and commercial companies. (e.g. Financial Times Chinese/English, Wall Street Journal Chinese/English). Structure based methods were previously proposed to mine parallel documents from these websites: Resnik and Smith (2003) used (1) parent pages containing links to versions of one document in different languages and (2) sibling pages contains link to translation of the current documents. They also rely on the URL and anchor text to spot language specific version of documents. A structural alignment using DOM tree representation was proposed by Shi et al. (2006) to align parallel documents by using HTML structure. They identify the translational equivalent texts and hyperlinks between two parallel DOM trees to find parallel documents. However, the web is a heterogeneous collection of documents that extend far beyond bilingual and comparable pages with obvious structural features, such as similar URLs or common titles. Structural features only work for bilingual websites or document pairs that are already linked by editors. Comparable websites: websites that contain parallel content in different languages without any structural relation between docume"
I11-1047,C10-1124,0,0.0345992,"Missing"
I11-1047,P11-1133,1,0.874666,"Missing"
I11-1047,P06-1011,0,0.0608623,"Missing"
I11-1047,E09-1003,0,\N,Missing
I11-1047,P02-1040,0,\N,Missing
K19-1026,N13-1073,0,0.139357,"constraint. We propose to generate synthetic code-switching data by the following steps: chinese Permissible switching 这个 其实 是 belonged to 简体 中⽂ 这个 其实 是 belonged to simpliﬁed chinese Impermissible switching this belonged to simpliﬁed chinese 是 其实 Figure 2: Example of equivalence constraint (Li and Fung, 2012). Solid lines show the alignment between the matrix language (top) and the embedded language (bottom). The dotted lines denote impermissible switching. We use a beam search to select the N -best codeswitching sentences. 2.2 1. Align the L1 sentences Q and L2 sentences E using fast_align2 (Dyer et al., 2013). We use the mapping from the L1 sentences to the L2 sentences. Equivalence Constraint Studies on the EC (Poplack, 1980, 2013) show that code-switching only occurs where it does not violate the syntactic rules of either language. An example of a English-Mandarin mixed-language sentence generation is shown in Figure 2, where EC theory does not allow the word “其实&quot; to come after “是&quot; in Chinese, or the word “is&quot; to come after “actually&quot;. Pratapa et al. (2018) apply the EC in English-Spanish language modeling with a strong assumption. We are working with English and Mandarin, which have distinctive"
K19-1026,D18-1346,0,0.404003,"The main reason lies in the unpredictability of code-switching points in an utterance and data scarcity. Creating a large-scale code-switching dataset is also very expensive. Therefore, code-switching data generation methods to augment existing datasets are a useful workaround. Existing methods that apply equivalence constraint theory to generate code-switching sentences (Li and Fung, 2012; Pratapa et al., 2018) may suffer performance issues as they receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data generator model using pointergenerator networks (Pointer-Gen) (See et al., 2017) to learn code-switching constraints from a limited source of code-switching data and leverage their translations in both languages. IntuTraining"
K19-1026,C12-1102,1,0.941147,"they have word alignments with an inverted order. Building a language model (LM) and an automatic speech recognition (ASR) system that can handle intra-sentential code-switching is known to be a difficult research challenge. The main reason lies in the unpredictability of code-switching points in an utterance and data scarcity. Creating a large-scale code-switching dataset is also very expensive. Therefore, code-switching data generation methods to augment existing datasets are a useful workaround. Existing methods that apply equivalence constraint theory to generate code-switching sentences (Li and Fung, 2012; Pratapa et al., 2018) may suffer performance issues as they receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data g"
K19-1026,D15-1166,0,0.0548825,"oder. We use a bidirectional long short-term memory (LSTM), which, produces hidden state ht in each step t. The decoder is a unidirectional LSTM receiving the word embedding of the previous word. For each decoding step, a generation probability pgen ∈ [0,1] is calculated, which weights the probability of generating words from the vocabulary, and copying words from the source text. pgen is a soft gating probability to decide whether to generate the next token from the decoder or to copy the word from the input instead. The attention distribution at is a standard attention with general scoring (Luong et al., 2015). It considers all encoder hidden states to derive the context vector. The vocabulary distribution Pvoc (w) is calculated by concatenating the decoder state st and the context vector h∗t : • We propose a language-agnostic method to generate code-switching sentences using a pointer-generator network (See et al., 2017) that learns when to switch and copy words from parallel sentences, without using external word alignments or constituency parsers. By using the generated data in the language model training, we achieve the state-of-theart performance in perplexity and also improve the end-to-end A"
K19-1026,P14-5010,0,0.00251229,"tural code-switching sequences from generation candidates. A word count is added to avoid generating very short sentences. P (Y ) is calculated as follows: p P (Y ) = αPtrans (Y |X) + βplm (Y ) + γ wc(Y ) (3) where α is the parameter to control the decoding probability from the probability of characters from the decoder Ptrans (Y |X), β is the parameter to control the language model probability plm (Y ), and γ is the parameter to control the effect of the word count wc(Y ). 4 4.1 from Winata et al. (2018a). The details are depicted in Table 1. We tokenize words using the Stanford NLP toolkit (Manning et al., 2014). For monolingual speech datasets, we use HKUST (Liu et al., 2006), comprising spontaneous Mandarin Chinese telephone speech recordings, and Common Voice, an open-accented English dataset collected by Mozilla.3 We split Chinese words into characters to avoid word boundary issues, similarly to Garg et al. (2018). We generate L1 sentences and L2 sentences by translating the training set of SEAME Phase II into English and Chinese using the Google NMT system (To enable reproduction of the results, we release the translated data).4 Then, we use them to generate 270,531 new pieces of code-switching"
K19-1026,P13-2037,0,0.402545,"gnate words and to avoid confusion. Also, English adverbs such as “then&quot; and “so&quot; are phrase or sentence connectors between two language phrases for intra-sentential and intersentential code-switching. On the other hand, Chinese transitional words such as the measure word “个&quot; or associative word “的&quot; are frequently used as inter-lingual word associations. 6 extended recurrent neural networks (RNNs) by adding POS information to the input layer and a factorized output layer with a language identifier. The factorized RNNs were also combined with an n-gram backoff model using linear interpolation (Adel et al., 2013b), and syntactic and semantic features were added to them (Adel et al., 2015). Baheti et al. (2017) adapted an effective curriculum learning by training a network with monolingual corpora of two languages, and subsequently trained on code-switched data. A further investigation of EC and curriculum learning showed an improvement in English-Spanish language modeling (Pratapa et al., 2018), and a multitask learning approach was introduced to train the syntax representation of languages by constraining the language generator (Winata et al., 2018a). Garg et al. (2018) proposed to use SeqGAN (Yu et"
K19-1026,W17-7509,0,0.117319,"entence connectors between two language phrases for intra-sentential and intersentential code-switching. On the other hand, Chinese transitional words such as the measure word “个&quot; or associative word “的&quot; are frequently used as inter-lingual word associations. 6 extended recurrent neural networks (RNNs) by adding POS information to the input layer and a factorized output layer with a language identifier. The factorized RNNs were also combined with an n-gram backoff model using linear interpolation (Adel et al., 2013b), and syntactic and semantic features were added to them (Adel et al., 2015). Baheti et al. (2017) adapted an effective curriculum learning by training a network with monolingual corpora of two languages, and subsequently trained on code-switched data. A further investigation of EC and curriculum learning showed an improvement in English-Spanish language modeling (Pratapa et al., 2018), and a multitask learning approach was introduced to train the syntax representation of languages by constraining the language generator (Winata et al., 2018a). Garg et al. (2018) proposed to use SeqGAN (Yu et al., 2017) for generating new mixed-language sequences. Winata et al. (2018b) leveraged character r"
K19-1026,P18-1143,0,0.344093,"nments with an inverted order. Building a language model (LM) and an automatic speech recognition (ASR) system that can handle intra-sentential code-switching is known to be a difficult research challenge. The main reason lies in the unpredictability of code-switching points in an utterance and data scarcity. Creating a large-scale code-switching dataset is also very expensive. Therefore, code-switching data generation methods to augment existing datasets are a useful workaround. Existing methods that apply equivalence constraint theory to generate code-switching sentences (Li and Fung, 2012; Pratapa et al., 2018) may suffer performance issues as they receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data generator model using po"
K19-1026,E17-2025,0,0.0971241,"Missing"
K19-1026,P17-1099,0,0.314318,"receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data generator model using pointergenerator networks (Pointer-Gen) (See et al., 2017) to learn code-switching constraints from a limited source of code-switching data and leverage their translations in both languages. IntuTraining code-switched language models is difficult due to lack of data and complexity in the grammatical structure. Linguistic constraint theories have been used for decades to generate artificial code-switching sentences to cope with this issue. However, this require external word alignments or constituency parsers that create erroneous results on distant languages. We propose a sequence-to-sequence model using a copy mechanism to generate code-switching da"
K19-1026,W19-4320,1,0.822278,"and subsequently trained on code-switched data. A further investigation of EC and curriculum learning showed an improvement in English-Spanish language modeling (Pratapa et al., 2018), and a multitask learning approach was introduced to train the syntax representation of languages by constraining the language generator (Winata et al., 2018a). Garg et al. (2018) proposed to use SeqGAN (Yu et al., 2017) for generating new mixed-language sequences. Winata et al. (2018b) leveraged character representations to address out-of-vocabulary words in the code-switching named entity recognition. Finally, Winata et al. (2019) proposed a method to represent code-switching sentence using language-agnostic meta-representations. Related Work Code-switching language modeling research has been focused on building a model that handles mixed-language sentences and on generating synthetic data to solve the data scarcity issue. The first statistical approach using a linguistic theory was introduced by Li and Fung (2012), who adapted the EC on monolingual sentence pairs during the decoding step of an ASR system. Ying and Fung (2014) implemented a functional-head constraint lattice parser with a weighted finite-state transduc"
K19-1026,W18-3207,1,0.929414,"(Y ) as the probability of the sentence. We incorporate language model probability plm (Y ) to select more natural code-switching sequences from generation candidates. A word count is added to avoid generating very short sentences. P (Y ) is calculated as follows: p P (Y ) = αPtrans (Y |X) + βplm (Y ) + γ wc(Y ) (3) where α is the parameter to control the decoding probability from the probability of characters from the decoder Ptrans (Y |X), β is the parameter to control the language model probability plm (Y ), and γ is the parameter to control the effect of the word count wc(Y ). 4 4.1 from Winata et al. (2018a). The details are depicted in Table 1. We tokenize words using the Stanford NLP toolkit (Manning et al., 2014). For monolingual speech datasets, we use HKUST (Liu et al., 2006), comprising spontaneous Mandarin Chinese telephone speech recordings, and Common Voice, an open-accented English dataset collected by Mozilla.3 We split Chinese words into characters to avoid word boundary issues, similarly to Garg et al. (2018). We generate L1 sentences and L2 sentences by translating the training set of SEAME Phase II into English and Chinese using the Google NMT system (To enable reproduction of th"
K19-1026,W18-3214,1,0.896391,"(Y ) as the probability of the sentence. We incorporate language model probability plm (Y ) to select more natural code-switching sequences from generation candidates. A word count is added to avoid generating very short sentences. P (Y ) is calculated as follows: p P (Y ) = αPtrans (Y |X) + βplm (Y ) + γ wc(Y ) (3) where α is the parameter to control the decoding probability from the probability of characters from the decoder Ptrans (Y |X), β is the parameter to control the language model probability plm (Y ), and γ is the parameter to control the effect of the word count wc(Y ). 4 4.1 from Winata et al. (2018a). The details are depicted in Table 1. We tokenize words using the Stanford NLP toolkit (Manning et al., 2014). For monolingual speech datasets, we use HKUST (Liu et al., 2006), comprising spontaneous Mandarin Chinese telephone speech recordings, and Common Voice, an open-accented English dataset collected by Mozilla.3 We split Chinese words into characters to avoid word boundary issues, similarly to Garg et al. (2018). We generate L1 sentences and L2 sentences by translating the training set of SEAME Phase II into English and Chinese using the Google NMT system (To enable reproduction of th"
K19-1026,D14-1098,1,0.846911,"s to address out-of-vocabulary words in the code-switching named entity recognition. Finally, Winata et al. (2019) proposed a method to represent code-switching sentence using language-agnostic meta-representations. Related Work Code-switching language modeling research has been focused on building a model that handles mixed-language sentences and on generating synthetic data to solve the data scarcity issue. The first statistical approach using a linguistic theory was introduced by Li and Fung (2012), who adapted the EC on monolingual sentence pairs during the decoding step of an ASR system. Ying and Fung (2014) implemented a functional-head constraint lattice parser with a weighted finite-state transducer to reduce the search space on a codeswitching ASR system. Then, Adel et al. (2013a) 7 Conclusion We propose a novel method for generating synthetic code-switching sentences using Pointer-Gen by learning how to copy words from parallel cor278 pora. Our model can learn code-switching points by attending to input words and aligning the parallel words, without requiring any word alignments or constituency parsers. More importantly, it can be effectively used for languages that are syntactically differe"
L16-1079,barbieri-saggion-2014-modelling-irony,0,0.160976,"roduction The term “humor” refers to various kinds of stimuli, including acoustic, verbal, visual and situational, that are able to trigger a laughter reaction in the recipient. It is an important aspect of our everyday life, and is supposed to give benefits to physical and psychological health (Sumners, 1988; Martineau, 1972; La Fave et al., 1976; Anderson and Arnoult, 1989; Lefcourt et al., 1997; Lefcourt and Martin, 2012). There has recently been many attempts in detecting humor from canned jokes (Yang et al., 2015), customer reviews (Reyes and Rosso, 2012) and Twitter (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015). All these analyses are only on isolated textual data. Fewer work took into consideration other elements, such as the surrounding context (Bamman and Smith, 2015; Karoui et al., 2015) or acoustic and prosodic features (Rakov and Rosenberg, 2013). We propose to predict when people would laugh in a dialog with a supervised machine learning approach. While most of the past attempts concentrate on isolated examples, the response to humor in a conversation depends heavily on the surrounding context, such as the conversational topic and the previous utteran"
L16-1079,P15-2124,0,0.0260018,"s of stimuli, including acoustic, verbal, visual and situational, that are able to trigger a laughter reaction in the recipient. It is an important aspect of our everyday life, and is supposed to give benefits to physical and psychological health (Sumners, 1988; Martineau, 1972; La Fave et al., 1976; Anderson and Arnoult, 1989; Lefcourt et al., 1997; Lefcourt and Martin, 2012). There has recently been many attempts in detecting humor from canned jokes (Yang et al., 2015), customer reviews (Reyes and Rosso, 2012) and Twitter (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015). All these analyses are only on isolated textual data. Fewer work took into consideration other elements, such as the surrounding context (Bamman and Smith, 2015; Karoui et al., 2015) or acoustic and prosodic features (Rakov and Rosenberg, 2013). We propose to predict when people would laugh in a dialog with a supervised machine learning approach. While most of the past attempts concentrate on isolated examples, the response to humor in a conversation depends heavily on the surrounding context, such as the conversational topic and the previous utterances. It is quite common that the same utte"
L16-1079,P15-2106,0,0.0440705,"supposed to give benefits to physical and psychological health (Sumners, 1988; Martineau, 1972; La Fave et al., 1976; Anderson and Arnoult, 1989; Lefcourt et al., 1997; Lefcourt and Martin, 2012). There has recently been many attempts in detecting humor from canned jokes (Yang et al., 2015), customer reviews (Reyes and Rosso, 2012) and Twitter (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015). All these analyses are only on isolated textual data. Fewer work took into consideration other elements, such as the surrounding context (Bamman and Smith, 2015; Karoui et al., 2015) or acoustic and prosodic features (Rakov and Rosenberg, 2013). We propose to predict when people would laugh in a dialog with a supervised machine learning approach. While most of the past attempts concentrate on isolated examples, the response to humor in a conversation depends heavily on the surrounding context, such as the conversational topic and the previous utterances. It is quite common that the same utterance may trigger a different effect on the recipient depending on when it is used. Two distinct moments can be identified in humor and joke generation: a setup where appropriate input"
L16-1079,D13-1066,0,0.0385636,"Missing"
L16-1079,I13-1183,0,0.0703079,"Missing"
L16-1079,D15-1284,0,0.0808165,"understanding humor in general. Keywords: humor prediction, neural networks, TV-sitcoms 1. Introduction The term “humor” refers to various kinds of stimuli, including acoustic, verbal, visual and situational, that are able to trigger a laughter reaction in the recipient. It is an important aspect of our everyday life, and is supposed to give benefits to physical and psychological health (Sumners, 1988; Martineau, 1972; La Fave et al., 1976; Anderson and Arnoult, 1989; Lefcourt et al., 1997; Lefcourt and Martin, 2012). There has recently been many attempts in detecting humor from canned jokes (Yang et al., 2015), customer reviews (Reyes and Rosso, 2012) and Twitter (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015). All these analyses are only on isolated textual data. Fewer work took into consideration other elements, such as the surrounding context (Bamman and Smith, 2015; Karoui et al., 2015) or acoustic and prosodic features (Rakov and Rosenberg, 2013). We propose to predict when people would laugh in a dialog with a supervised machine learning approach. While most of the past attempts concentrate on isolated examples, the response to humor in a conversation"
L16-1312,P91-1019,0,0.439428,"Missing"
li-etal-2012-mandarin,D08-1102,0,\N,Missing
N04-4008,P98-1013,0,0.00855464,"f the statistical NLP community as their results are a valuable resource for statistical machine translation, cross-lingual question answering, and other bilingual or cross-lingual tasks. Recently, there has been an increasing trend of using semantic information for these tasks spurred by the availability of various ontology databases such as WordNet, FrameNet, PropBank, etc. Among these, the Berkeley FrameNet database is a semantic lexical resource consisting of frame-semantic descriptions of more than 7000 English lexical items, together with example sentences annotated with semantic roles (Baker et al., 1998). The current version of FrameNet has been applied successfully to English question answering systems (Gildea, 2002). However, the manual development of FrameNet in other languages has been on a small scale (e.g. German, Spanish, Japanese) or unfinished (e.g. Chinese). Since manually annotation is rather time consuming, the main objective of our work is to automatically create multilingual FrameNet to enable semantic analysis in multiple languages rather than in English. Another objective is to quantify the mapping between semantic structures across language pairs for statistical NLP systems."
N04-4008,C02-1143,0,0.0300398,"expect that our technique is applicable to other languages as well. There are two Chinese semantic resources available today-Cilin (tong2yi4ci2ci2lin2) (Mei et al., 1982) and HowNet (Dong and Dong, 2000). Much like WordNet, Cilin is a thesaurus with a hierarchical structure of word clusters, but it does not describe any semantic relationship between words and categories. HowNet, on the other hand, is an ontology with a graph structure of inter-concept relations and inter-attribute relations. In addition, HowNet has been widely used in resolving NLP problems, such as word sense disambiguation (Dang et al., 2002) and machine translation (Dorr et al., 2002). For our work, we choose to align HowNet concepts to lexical entries in FrameNet in order to construct the English-Chinese bilingual FrameNet. (Dorr et al., 2002) describes a technique for the construction of a Chinese-English verb lexicon based on HowNet and an English verb database called the LCS Verb Database (LVD). They created links between Chinese concepts in HowNet and English verb classes in LVD using both statistics and a manually constructed “seed mapping” of thematic classes between HowNet and LVD. Ngai et al. (2002) employed a word-vecto"
N04-4008,1985.tmi-1.5,0,0.426094,"Missing"
N04-4008,P00-1065,0,0.052547,"Missing"
N04-4008,O02-2003,0,0.0592216,"Missing"
N04-4008,C02-1162,1,0.853573,"Missing"
N04-4008,C98-1013,0,\N,Missing
N04-4008,J02-3001,0,\N,Missing
N04-4010,W99-0612,0,0.0357195,"Missing"
N04-4010,W03-1026,0,0.0267296,"Missing"
N04-4010,O03-5001,0,0.0577236,"Missing"
N04-4010,W02-2024,0,0.0232548,"Missing"
N04-4010,W03-0419,0,0.0212412,"Missing"
N04-4010,W03-0433,1,0.873511,"Missing"
N04-4010,C02-1012,0,\N,Missing
N07-2054,N06-2023,0,0.019575,"ic/prosodic features of the Japanese broadcast news transcriptions; while (Chen et al., 2006) proposes the use of probabilistic latent topical information for extractive summarization of Mandarin spoken documents. (Huang et al., 2005) presents Mandarin spo213 Proceedings of NAACL HLT 2007, Companion Volume, pages 213–216, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics ken document summarization scheme using acoustic, prosodic, and semantic information. Alternatively, some methods which are independent of lexical features are presented (Maskey and Hirschberg, 2003; Maskey and Hirschberg, 2006). (Maskey and Hirschberg, 2003) extracts structural information from audio documents to help summarization. (Maskey and Hirschberg, 2006) focuses on how to use acoustic information alone to help predict sentences to be included in a summary and shows a novel way of using continuous HMMs for summarizing speech documents without transcriptions. It is advantageous to build speech summarization models without using lexical features: we can summarize speech data without placing a stringent demand on the speech recognition accuracy. In this paper, we propose one such model on Mandarin broadcast news"
N09-2004,P05-1048,1,0.807624,"uracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation. The model makes use of a typical representative SMT system based on Moses, plus shallow semantic parsers for both English and Chinese. 2 Hybrid two"
N09-2004,D07-1007,1,0.0695729,"pers, pages 13–16, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tribute to improving SMT accuracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation. The model makes use o"
N09-2004,2007.tmi-papers.10,1,0.897646,"Missing"
N09-2004,P01-1030,0,0.0292028,"ng can improve translation accuracy of SMT models. We note that accuracy here was measured via BLEU, and it has been widely observed that the negative impacts of semantic predicate-argument errors on the utility of the translation are underestimated by evaluation metrics based on lexical criteria such as BLEU. We conjecture that more expensive manual evaluation techniques which directly measure translation utility could even more strongly reveal improvement in role confusion errors. The hybrid two-pass approach can be compared with the greedy re-ordering based strategy of the ReWrite decoder (Germann et al. 2001), although our search is breadth-first rather than purely greedy. Whereas ReWrite was based on wordlevel re-ordering, however, our approach is based on constituent phrase re-ordering, and the phrases to be re-ordered are more selectively chosen via the semantic parse labels. Moreover, the objective function being maximized by ReWrite is still the SMT model score; whereas in our case the new objective function is cross-lingual semantic predicate-argument match (plus an implicit search bias toward fewer re-orderings). The hybrid two-pass approach can also be compared with serial combination arch"
N09-2004,P00-1065,0,0.195005,"Missing"
N09-2004,W07-0738,0,0.207301,"of NAACL HLT 2009: Short Papers, pages 13–16, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tribute to improving SMT accuracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, successfully applies semantic parsing technology to the challenge of improving the quality of ChineseEnglish statistical machine translation"
N09-2004,W08-0332,0,0.117393,"Missing"
N09-2004,W05-1002,0,0.0327693,"Missing"
N09-2004,P02-1040,0,0.102811,"Missing"
N09-2004,N04-1030,0,0.273541,"Missing"
N09-2004,N04-1032,0,0.0625469,"Missing"
N09-2004,J97-3002,1,0.119844,"tic parsers to identify inconsistent semantic frame and role mappings between the input source sentences and their output translations. However, we take note of the difficult experience in making syntactic and semantic models con13 Proceedings of NAACL HLT 2009: Short Papers, pages 13–16, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tribute to improving SMT accuracy. On the one hand, there is reason to be optimistic. Over the past decade, we have seen an accumulation of evidence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accuracy, in the form of techniques adapted from word sense disambiguation models (Chan et al. 2007; Giménez and Màrquez 2007b; Carpuat and Wu 2007). On the other hand, both directions saw unexpected disappointments along the way (e.g., Och et al. 2003; Carpuat and Wu 2005). We are therefore forewarned that it is likely to be at least as difficult to successfully adapt the even more complex types of lexical semantics modeling from semantic parsing and role lab"
N09-2004,C08-1115,0,\N,Missing
N09-2004,W07-0719,0,\N,Missing
N09-2004,P07-1005,0,\N,Missing
N09-2004,N04-1021,0,\N,Missing
N16-1016,E14-3007,0,0.00785785,"ata obtained from meanings, and matches words that do not appear to others similar in meaning. The convolution and max-pooling operation is applied individually to each feature, and the three vectors obtained are then concatenated together and fed to the final sentence encoding layer, which combines all the contributions. 2.2 keeps track of the context in each scene, and mimics human memory to accumulate the setup that may trigger a punchline. Before the output we incorporate a set of high level features from our previous work (Bertero and Fung, 2016b) and past literature (Reyes et al., 2013; Barbieri and Saggion, 2014). They include: • Structural features: average word length, sentence length, difference in sentence length with the five previous utterances. Long/Short Term Memory for the utterance sequence The LSTM is an improvement over the Recurrent Neural Network aimed to improve its memory capabilities. In a standard RNN the hidden memory layer is updated through a function of the input and the hidden layer at the previous time instant: ht = tanh(Wx xt + Wh ht−1 + b) • Part of speech proportion: noun, verbs, adjectives and adverbs. • Antonyms: proportion of antonym words with the previous utterance (fro"
N16-1016,L16-1079,1,0.906945,"c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics but then just gave up. LAUGH The utterances before the punchline are the setup. Without them, the punchlines may not be perceived as humorous (the last utterance, out of context, may be a political complaint), only with proper setup a laughter would be triggered. The humorous intent is also strengthen by the fact the dialog takes place in a bar (evident from the previous and following utterances), where a request of 40 ml of “Ethyl Alcohol” is unusual and weird. Our previous attempts on the same corpus (Bertero and Fung, 2016b; Bertero and Fung, 2016a) showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal. For this reason we propose a method based on a Long Short-Term Memory network (Hochreiter and Schmidhuber, 1997), where we encode each sentence through a Convolutional Neural Network (Collobert et al., 2011). LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition (Graves et al., 2013), dependency parsing (Dyer et al., 2015) and conversation modelling (Shang et al., 2"
N16-1016,P15-1033,0,0.00554099,"r previous attempts on the same corpus (Bertero and Fung, 2016b; Bertero and Fung, 2016a) showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal. For this reason we propose a method based on a Long Short-Term Memory network (Hochreiter and Schmidhuber, 1997), where we encode each sentence through a Convolutional Neural Network (Collobert et al., 2011). LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition (Graves et al., 2013), dependency parsing (Dyer et al., 2015) and conversation modelling (Shang et al., 2015). This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general humor detection tasks. 2 Methodology We employ a supervised classification method to detect when punchlines occur. The bulk of our classifier is made of a concatenation of a Convolutional Neural Network (Collobert et al., 2011) to encode each utterance, followed by a Long ShortTerm Memory (Hochreiter and Schmidhuber, 1997) to model the sequential context of the dialog. Before the output softmax layer we add a vector of higher leve"
N16-1016,esuli-sebastiani-2006-sentiwordnet,0,0.00812433,"us utterances. Long/Short Term Memory for the utterance sequence The LSTM is an improvement over the Recurrent Neural Network aimed to improve its memory capabilities. In a standard RNN the hidden memory layer is updated through a function of the input and the hidden layer at the previous time instant: ht = tanh(Wx xt + Wh ht−1 + b) • Part of speech proportion: noun, verbs, adjectives and adverbs. • Antonyms: proportion of antonym words with the previous utterance (from WordNet (Miller, 1995)). (1) • Sentiment: positive, negative and average sentiment score among all words (from SentiWordNet (Esuli and Sebastiani, 2006)). where x is the network input and b the bias term. This kind of connection is not very effective to maintain the information stored for long time instants, as well as it does not allow to forget unneeded information between two time steps. The LSTM enhances the RNN with a series of three multiplicative gates. The structure is the following: it = σ(Wix xt + Wih ht−1 + bi ) (2) ft = σ(Wfx xt + Wfh ht−1 + bf ) (3) ot = σ(Wox xt + Woh ht−1 + bo ) (4) st = tanh(Wsx xt + Wsh ht−1 + bh ) (5) ct = ft ct−1 + it st (6) ht = tanh(ct ) ot (7) where is the element-wise product. Each gate factor is able t"
N16-1016,P15-2124,0,0.0222443,"Missing"
N16-1016,P15-2106,0,0.0212153,"seline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes. 1 Introduction There has been many recent attempts to detect and understand humor, irony and sarcasm from sentences, usually taken from Twitter (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015), customer reviews (Reyes and Rosso, 2012) or generic canned jokes (Yang et al., 2015). Bamman and Smith (2015) and Karoui et al. (2015) included the surrounding context. Our work has a different focus from the above. We analyze transcripts of funny dialogues, a genre somehow neglected but important for human-robot interaction. Laughter is the natural reaction of people to a verbal or textual humorous stimulus. We want to predict when the audience would laugh. Compared to a typical canned joke or a sarcastic Tweet, a dialog utterance is perceived as funny only in relation to the dialog context and the past history. In a spontaneous setting a funny dialog is usually built through a setup which prepares the audience to receive t"
N16-1016,P15-1107,0,0.00840685,"Missing"
N16-1016,D13-1066,0,0.0100935,"Missing"
N16-1016,P15-1152,0,0.0179469,"and Fung, 2016b; Bertero and Fung, 2016a) showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal. For this reason we propose a method based on a Long Short-Term Memory network (Hochreiter and Schmidhuber, 1997), where we encode each sentence through a Convolutional Neural Network (Collobert et al., 2011). LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition (Graves et al., 2013), dependency parsing (Dyer et al., 2015) and conversation modelling (Shang et al., 2015). This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general humor detection tasks. 2 Methodology We employ a supervised classification method to detect when punchlines occur. The bulk of our classifier is made of a concatenation of a Convolutional Neural Network (Collobert et al., 2011) to encode each utterance, followed by a Long ShortTerm Memory (Hochreiter and Schmidhuber, 1997) to model the sequential context of the dialog. Before the output softmax layer we add a vector of higher level syntactic, structural and sentiment features."
N16-1016,D15-1284,0,0.30817,"Missing"
P06-2031,P98-1013,0,0.00405409,"translation candidate. Whereas some translation ambiguities are preserved over languages, most are not. In particular, for languages as different as English and Chinese, there is little overlap between how lexicon is broken-down (Ploux and Ji 2003). Some cognitive scientists suggest that a bilingual speaker tends to group concepts in a single semantic map and simply attach different words in English and Chinese to the categories in this map. Based on the above, we propose the oneframe-two-languages idea for constructing a bilingual word sense dictionary from monolingual ontologies. FrameNet (Baker et al. 1998) is a collection of lexical entries grouped by frame semantics. Each lexical entry represents an individual word sense, and is associated with semantic roles and some annotated sentences. Lexical entries with the same semantic roles are grouped into a “frame” and the semantic roles are called “frame elements”. Each frame in FrameNet is a concept class and a single word sense belongs to only one frame. However, the Chinese HowNet represents a hierarchical view of lexical semantics in Chinese. HowNet (Dong and Dong 2000) is a Chinese ontology with a graph structure of word senses called “concept"
P06-2031,boas-2002-bilingual,0,0.0154271,"sambiguation task. 6 Related Work The most relevant previous works include word sense translation and translation disambiguation (Li & Li 2003; Cao & Li 2002; Koehn and Knight 2000; Kikui 1999; Fung et al., 1999), frame semantic induction (Green et al., 2004; Fung & Chen 2004), and bilingual semantic mapping (Fung & Chen 2004; Huang et al. 2004; Ploux & Ji, 2003, Ngai et al., 2002; Palmer & Wu 1995). Other than the English FrameNet (Baker et al, 1998), we also note the construction of the Spanish FrameNet (Subirats & Petruck, 2003), the Japanese FrameNet (Ikeda 1998), and the German FrameNet (Boas, 2002). In terms of learning method, Chen and Palmer (2004) also used EM learning to cluster Chinese verb senses. Word Sense Translation Previous word sense translation methods are based on using context information to improve translation. These methods look at the context words and discourse surrounding the source word and use methods ranging from boostrapping (Li & Li 2003), EM iterations (Cao and Li, 2002; Koehn and Knight 2000), and the cohesive relation between the source sentence and translation candidates (Fung et al. 1999; Kikui 1999). Our proposed translation disambiguation method compares"
P06-2031,C02-1011,0,0.0545122,"Missing"
P06-2031,P04-1038,0,0.0213943,"relevant previous works include word sense translation and translation disambiguation (Li & Li 2003; Cao & Li 2002; Koehn and Knight 2000; Kikui 1999; Fung et al., 1999), frame semantic induction (Green et al., 2004; Fung & Chen 2004), and bilingual semantic mapping (Fung & Chen 2004; Huang et al. 2004; Ploux & Ji, 2003, Ngai et al., 2002; Palmer & Wu 1995). Other than the English FrameNet (Baker et al, 1998), we also note the construction of the Spanish FrameNet (Subirats & Petruck, 2003), the Japanese FrameNet (Ikeda 1998), and the German FrameNet (Boas, 2002). In terms of learning method, Chen and Palmer (2004) also used EM learning to cluster Chinese verb senses. Word Sense Translation Previous word sense translation methods are based on using context information to improve translation. These methods look at the context words and discourse surrounding the source word and use methods ranging from boostrapping (Li & Li 2003), EM iterations (Cao and Li, 2002; Koehn and Knight 2000), and the cohesive relation between the source sentence and translation candidates (Fung et al. 1999; Kikui 1999). Our proposed translation disambiguation method compares favorably to (Li & Li 2003) in that we obtain an aver"
P06-2031,C04-1134,1,0.737428,"a dictionary, this bilingual ontology forms part of the translation “lexicon”, and can be used either by human translators or automatic translation systems. Such a bilingual frame semantics ontology also provides a simulation of the “concept lexicon&quot; of a bilingual person, as suggested by cognitive scientists. Figure 1 shows an example of a bilingual frame that possibly corresponds to the semantic structure in a bilingual person. Figure 1. An example bilingual frame We previously proposed using the Chinese HowNet and a bilingual lexicon to map the English FrameNet into a bilingual BiFrameNet (Fung and Chen 2004). We used a combination of frame size thresholding and taxonomy distance to obtain the final alignment between FrameNet frames and HowNet categories, to generate the BiFrameNet. Our previous algorithm had the disadvantage of requiring the ad hoc tuning of thresholds. This results in poor performance on lexical entries from small frames (i.e. frames with very few lexical entries). The tuning process also means that a development set of annotated data is needed. In this paper, we propose a fully automatic estimation-maximization algorithm instead, to generate a similar FrameNet to HowNet 240 bil"
P06-2031,P99-1043,1,0.692588,"Missing"
P06-2031,P04-1048,0,0.0303081,"us works include word sense translation and translation disambiguation (Li & Li 2003; Cao & Li 2002; Koehn and Knight 2000; Kikui 1999; Fung et al., 1999), frame semantic induction (Green et al., 2004; Fung & Chen 2004), and bilingual semantic mapping (Fung & Chen 2004; Huang et al. 2004; Ploux & Ji, 2003, Ngai et al., 2002; Palmer & Wu 1995). Other than the English FrameNet (Baker et al, 1998), we also note the construction of the Spanish FrameNet (Subirats & Petruck, 2003), the Japanese FrameNet (Ikeda 1998), and the German FrameNet (Boas, 2002). In terms of learning method, Chen and Palmer (2004) also used EM learning to cluster Chinese verb senses. Word Sense Translation Previous word sense translation methods are based on using context information to improve translation. These methods look at the context words and discourse surrounding the source word and use methods ranging from boostrapping (Li & Li 2003), EM iterations (Cao and Li, 2002; Koehn and Knight 2000), and the cohesive relation between the source sentence and translation candidates (Fung et al. 1999; Kikui 1999). Our proposed translation disambiguation method compares favorably to (Li & Li 2003) in that we obtain an aver"
P06-2031,huang-etal-2004-sinica,0,0.021198,"Missing"
P06-2031,O02-2003,0,0.0674753,"Missing"
P06-2031,C02-1162,1,0.898892,"Missing"
P06-2031,J03-2001,0,0.0207752,"ation for Computational Linguistics (tang)” carries the “cause_harm|damage” sense, whereas “burn |烧 (shao)” carries the “heat|cooking” sense. The source sentence “My hands are burned” has the “cause_harm|damage” sense, therefore the correct translation of “burn” is “ 烫 (tang)” not “烧(shao)”. The frame semantics information of the source word can thus lead to the best translation candidate. Whereas some translation ambiguities are preserved over languages, most are not. In particular, for languages as different as English and Chinese, there is little overlap between how lexicon is broken-down (Ploux and Ji 2003). Some cognitive scientists suggest that a bilingual speaker tends to group concepts in a single semantic map and simply attach different words in English and Chinese to the categories in this map. Based on the above, we propose the oneframe-two-languages idea for constructing a bilingual word sense dictionary from monolingual ontologies. FrameNet (Baker et al. 1998) is a collection of lexical entries grouped by frame semantics. Each lexical entry represents an individual word sense, and is associated with semantic roles and some annotated sentences. Lexical entries with the same semantic role"
P06-2031,C98-1013,0,\N,Missing
P06-2031,J02-3001,0,\N,Missing
P11-1133,P08-2064,0,0.0258513,"to achieve different accuracy depending on the pair of languages involved, the co-occurrence model is totally language independent. In the case of binary classification of translations, the two models are complementary to each other: word pairs with null co-occurrence are not considered by the context model while the context vector model gives more semantic information than the co-occurrence model. For these reasons, we suggest that it is possible to use a decision tree trained on one pair of languages to extract translations from another pair of languages. A similar approach is proposed in (Alfonseca et al., 2008): they present a word decomposition model designed for German language that they successfully applied to other compounding languages. Our approach consists in training a decision tree on a pair of languages and applying this model to the classification of unknown pairs of words in another pair of languages. Such an approach is especially useful for prospecting new translations from less known languages, using a well known language as training. We used the same algorithms and same features as in the previous sections, but used the data computed from one pair of languages as the training set, an"
P11-1133,C02-2020,0,0.274666,"Missing"
P11-1133,J93-1003,0,0.0806821,"a perfect association (words always appear together, never one without the other), the more one word appears without the other, the lower the score. 4.2 Context-vector similarity We implemented the context-vector similarity in a way similar to (Morin et al., 2007). In all experiments, we used the same set of parameters, as they yielded the best results on our corpora. We built the context-vectors using nouns only as seed lexicon, with a window size of 20. Source context-vectors are translated in the target language using the resources presented in the next section. We used the log-likelihood (Dunning, 1993, eq. 2) for contextvector normalization (O is the observed number of co-occurrence in the corpus, E is the expected number of co-occurrences under the null hypothesis). We used the Cosine similarity (eq. 3) for contextvector comparisons. ll(wi , wj ) = 2 X ij Cosine(A, B) = kAk2 Oij log Oij Eij A·B + kBk2 − A · B (2) (3) 4.3 Binary classification of rare translations 4.4 Extension to another pair of languages We suggest to incorporate both the context-vector similarity and the co-occurrence features in a machine learning approach. This approach consists of training a classifier on positive ex"
P11-1133,C10-1070,0,0.229165,"ranslate automatically using statistical methods due to their low occurrences. However, the Zipf’s Law claims that, for any corpus of natural language text, the frequency of a word wn (n being its rank in the frequency table) will be roughly twice as high as the frequency of word wn+1 . The logical consequence is that in any corpus, there are very few frequent words and many rare words. We propose a novel approach to extract rare word translations from comparable corpora, relying on two main features. The first feature is the context-vector similarity (Fung, 2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais, 2010): each word is characterized by its context in both source and target corpora, words in translation should have similar context in both languages. The second feature follows the assumption that specific terms and their translations should appear together often in documents on the same topic, and rarely in non-related documents. This is the general assumption behind early work on bilingual lexicon extraction from parallel documents using sentence boundary as the context window size for cooccurrence computation, we suggest to extend it to aligned comparable documents using document as the contex"
P11-1133,P07-1084,0,0.454778,"that words are randomly distributed. If they appear together more often than expected, they are considered as associated (Evert, 2008). We focus in this work on rare words, more precisely on specialized terminology. We define them as the set of terms that appear from 1 (hapaxes) 1329 |Ai ∩ Aj | ; Ai = {d : wi ∈ d} |Ai ∪ Aj | (1) A score of 1 indicates a perfect association (words always appear together, never one without the other), the more one word appears without the other, the lower the score. 4.2 Context-vector similarity We implemented the context-vector similarity in a way similar to (Morin et al., 2007). In all experiments, we used the same set of parameters, as they yielded the best results on our corpora. We built the context-vectors using nouns only as seed lexicon, with a window size of 20. Source context-vectors are translated in the target language using the resources presented in the next section. We used the log-likelihood (Dunning, 1993, eq. 2) for contextvector normalization (O is the observed number of co-occurrence in the corpus, E is the expected number of co-occurrences under the null hypothesis). We used the Cosine similarity (eq. 3) for contextvector comparisons. ll(wi , wj )"
P11-1133,J05-4003,0,0.0971353,"each sample) are very similar across the subsets. Less frequent words, such as abnormality (between 70 and 16 occurrences in each sample) have very unstable context-vectors, hence a lower similarity across the subsets. This observation actually indicates that it will be difficult to align abnormality with itself. 3 Aligned comparable documents A pair of aligned comparable documents is a particular case of comparable corpus: two comparable documents share the same topic and domain; they both relate the same information but are not mutual translations; although they might share parallel chunks (Munteanu and Marcu, 2005) – paragraphs, sentences or phrases – in the general case they were written independently. These comparable documents, when concatenated together in order, form an aligned comparable corpus. Examples of such aligned documents can be found, for example in (Munteanu and Marcu, 2005): they aligned comparable documents with close publication dates. (Tao and Zhai, 2005) used an iterative, bootstrapping approach to align comparable documents using examples of already aligned corpora. (Smith et al., 2010) aligned documents from Wikipedia following the interlingual links provided on articles. We take"
P11-1133,N10-1063,0,0.0256641,"me information but are not mutual translations; although they might share parallel chunks (Munteanu and Marcu, 2005) – paragraphs, sentences or phrases – in the general case they were written independently. These comparable documents, when concatenated together in order, form an aligned comparable corpus. Examples of such aligned documents can be found, for example in (Munteanu and Marcu, 2005): they aligned comparable documents with close publication dates. (Tao and Zhai, 2005) used an iterative, bootstrapping approach to align comparable documents using examples of already aligned corpora. (Smith et al., 2010) aligned documents from Wikipedia following the interlingual links provided on articles. We take advantage of this alignment between documents: by looking at what is common between two aligned documents and what is different in other documents, we obtain more precise information about terms than when using a larger comparable corpus without alignment. This is especially interesting in the case of rare lexicon as the classic context-vector similarity is not discriminatory enough and fails at raising interesting translation for rare words. 4 Rare word translations from aligned comparable documen"
P11-1133,D07-1057,0,0.0234514,"kBk2 − A · B (2) (3) 4.3 Binary classification of rare translations 4.4 Extension to another pair of languages We suggest to incorporate both the context-vector similarity and the co-occurrence features in a machine learning approach. This approach consists of training a classifier on positive examples of translation pairs, and negative examples of non-translations pairs. The trained model (in our case, a decision tree) is then used to tag an unknown pair of words as either ”Translation” or ”Non-Translation”. One potential problem for building the training set, as pointed out for example by (Zhao and Ng, 2007) is this: we have a limited number of positive examples, but a very large amount of nontranslation examples as obviously is the case for rare word translations in any training corpus. Including two many negative examples in the training set would lead the classifier to label every pairs as ”Non-Translation”. To tackle this problem, (Zhao and Ng, 2007) tuned the imbalance of positive/negative ratio by resampling the positive examples in the training set. We chose to reduce the set of negative examples, and found that a ratio of five negative examples to one positive is optimal in our case. A lo"
P11-1133,H91-1026,0,\N,Missing
P17-4021,W93-0231,0,0.359468,"Fung et al., 2016). A separate module handles user-initiated queries, which chats with the user until the user desires to go back and finish the test. Abusive language - such as swearing, explicit sexist, or racist remarks - are also handled by a separate module. A simplified system architecture diagram is given in Figure 2. Zara is a web application, run on a server, and can be rendered on a browser. 3 Big Five Personality Induction Personality is the complex pattern of individual differences, behavior, and thinking style. It is predominantly described with the Big Five model of personality (Goldberg, 1993), which defines five personality traits and rates a person along them. The traits are (abbreviations used in this paper are bolded): Extraversion vs. introversion, Agreeableness vs. detachment, Conscientiousness vs. carelessness, Neuroticism vs. emotional stability, and Openness to Experience vs. cautiousness. Big Five personalities can be determined by self-reported assessments, such as the NEO-PI-R (Costa and McCrae, 2008). An automatic system of personality recognition is useful for situations where this is not practical. Early work on automatic personality recognition was mostly concerned"
P17-4021,P14-1062,0,0.00867363,"in (Mohammadi and Vinciarelli, 2012). This speeds up the computation, which is essential for dialog systems. Raw audio is inserted straight into a CNN. These architectures have been applied very successfully in speech recognition tasks recently (Palaz et al., 2015). Our CNN architecture is shown in Figure 3. The audio input has sampling rate 8 kHz. The first convolutional layer is applied directly on a raw audio sample x: xCi = ReLU(WC x[i,i+v] + bC ) (1) where v is the convolution window size. We apply a window size of 25ms and move the convolution window with a step of 2.5ms. The layer 122 (Kalchbrenner et al., 2014; Kim, 2014). In particular, using pre-trained word embeddings like word2vec (Mikolov et al., 2013) to represent the text has proved to be useful in classifying text from different domains. We have trained a model that takes as input text represented with the pretrained word embeddings, followed by a single CNN layer, and then a max pooling, and a fully connected layer with softmax at the end to map the features to the binary classification task. The convolution window sizes of 3, 4 and 5 were used in order to represent the n-gram features, and a total of 128 filters were trained in parallel."
P17-4021,D14-1181,0,0.0064924,"Missing"
P17-4021,N16-3018,1,\N,Missing
P17-4021,D16-1110,1,\N,Missing
P18-1136,P16-1154,0,0.0453427,"itectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models (Luong et al., 2015; Bahdanau et al., 2015), Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models Eric and Manning (2017), were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work, are the memory based decoders and the nonrecurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN) (Graves et al., 2014, 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016), and meta-learning application (Kaiser et al., 2017). However, Mem2Seq differs from these models as such: it uses multi1475"
P18-1136,P16-1014,0,0.463055,"n Ptr-Unk Distance 5 miles 4 miles 5 miles 4 miles 6 miles 6 miles 2 miles techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al., 2016; Wen et al., 2017; Zhao et al., 2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism (Gulcehre et al., 2016; Eric and Manning, 2017) have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities. However, although the above mentioned approaches were successful, they still suffer from two main problems: 1) They struggle to effectively incorporate external KB information into the RNN hidden states (Sukhbaatar et al., 2015), 1468 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long"
P18-1136,P17-1019,0,0.0132652,"used in task-oriented dialog systems (Zhao et al., 2017). These architectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models (Luong et al., 2015; Bahdanau et al., 2015), Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models Eric and Manning (2017), were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work, are the memory based decoders and the nonrecurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN) (Graves et al., 2014, 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016), and meta-learning application (Kaiser et al., 2017). H"
P18-1136,W14-4337,0,0.106322,"r turns Avg. Sys turns Avg. KB results Avg. Sys words Max. Sys words Pointer Ratio Vocabulary Train dialogs Val dialogs Test dialogs 1 4 6 0 6.3 9 .23 2 6.5 9.5 0 6.2 9 .53 3 4 5 6.4 3.5 12.9 9.9 3.5 18.4 24 7 23.7 7.2 5.7 6.5 9 8 9 .46 .19 .60 3747 1000 1000 1000 + 1000 OOV DSTC2 6.7 9.3 39.5 10.2 29 .46 1229 1618 500 1117 In-Car 2.6 2.6 66.1 8.6 87 .42 1601 2425 302 304 3.2 Table 2: Dataset statistics for 3 different datasets. 3 3.1 Experimental Setup Dataset We use three public multi-turn task-oriented dialog datasets to evaluate our model: the bAbI dialog (Bordes and Weston, 2017), DSTC2 (Henderson et al., 2014) and In-Car Assistant (Eric et al., 2017). The train/validation/test sets of these three datasets are split in advance by the providers. The dataset statistics are reported in Table 2. The bAbI dialog includes five end-to-end dialog learning tasks in the restaurant domain, which are simulated dialog data. Task 1 to 4 are about API calls, refining API calls, recommending options, and providing additional information, respectively. Task 5 is the union of tasks 1-4. There are two test sets for each task: one follows the same distribution as the training set and the other has out-of-vocabulary (OO"
P18-1136,N16-1014,0,0.0364496,"g are correct, which can be considered as the task-completion rate. Note that Bordes and Weston (2017) tests their model by selecting the system response from predefined response candidates, that is, their system solves a multi-class classification task. Since Mem2Seq generates each token individually, evaluating with this metric is much more challenging for our model. BLEU: It is a measure commonly used for machine translation systems (Papineni et al., 2002), but it has also been used in evaluating dialog systems (Eric and Manning, 2017; Zhao et al., 2017) and chat-bots (Ritter et al., 2011; Li et al., 2016). Moreover, BLEU score is a relevant measure in task-oriented dialog as there is not a large variance between the generated answers, unlike open domain generation (Liu et al., 2016). Hence, we include BLEU score in our evaluation (i.e. using Moses multi-bleu.perl script). Entity F1: We micro-average over the entire set of system responses and compare the entities in plain text. The entities in each gold system response are selected by a predefined entity list. This metric evaluates the ability to generate relevant entities from the provided KBs and to capture the semantics of the dialog (Eric"
P18-1136,D16-1230,0,0.0609271,"Missing"
P18-1136,E17-1001,0,0.239676,"t F1 scores reported in Eric et al. (2017) uses the entities in their canonicalized forms, which are not calculated based on real entity value. Since the datasets are not designed for slot-tracking, we report entity F1 rather than the slot-tracking accuracy as in (Wen et al., 2017; Zhao et al., 2017). 4 BLEU 13.5 6.6 13.2 8.4 9.3 8.3 11.6 12.6 9.9 Experimental Results We mainly compare Mem2Seq with hop 1,3,6 with several existing models: query-reduction networks (QRN, Seo et al. (2017)), end-toend memory networks (MemNN, Sukhbaatar et al. (2015)), and gated end-to-end memory networks (GMemNN, Liu and Perez (2017)). We also implemented the following baseline models: standard sequence-to-sequence (Seq2Seq) models with and without attention (Luong et al., 2015), and pointer to unknown (Ptr-Unk, Gulcehre et al. (2016)). Note that the results we listed in Table 3 and Table 4 for QRN are different from the original paper, because based on their released code, 3 we discovered that the per-response accuracy was not correctly computed. bAbI Dialog: In Table 3, we follow Bordes 3 We simply modified the evaluation part and reported the results. (https://github.com/uwnlp/qrn) and Weston (2017) to compare the perf"
P18-1136,D15-1166,0,0.117279,"ficial state labels. End-to-End Memory Networks (Bordes and Weston, 2017; Sukhbaatar et al., 2015), and its variants (Liu and Perez, 2017; Wu et al., 2017, 2018) have also shown good results in such tasks. In each of these architectures, the output is produced by generating a sequence of tokens, or by selecting a set of predefined utterances. Sequence-to-sequence (Seq2Seq) models have also been used in task-oriented dialog systems (Zhao et al., 2017). These architectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models (Luong et al., 2015; Bahdanau et al., 2015), Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models Eric and Manning (2017), were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work"
P18-1136,D16-1147,0,0.0263522,"o control a soft gate in a language modeling task. With this method, the model does not need to learn a gating function separately as in Gulcehre et al. (2016), and is not constrained by a soft gate function as in See et al. (2017). 2.3 Memory Content We store word-level content X in the memory module. Similar to Bordes and Weston (2017), we add temporal information and speaker information in each token of X to capture the sequential dependencies. For example, “hello t1 $u” means “hello” at time step 1 spoken by a user. On the other hand, to store B, the KB information, we follow the works of Miller et al. (2016); Eric et al. (2017) that use a (subject, relation, object) representation. For example, we represent the information of The Westin in Table 1: (The Westin, Distance, 5 miles). Thus, we sum word embeddings of the subject, relation, and object to obtain each KB memory representation. During decoding stage, the object part is used as the generated word for Pptr . For instance, when the KB tuple (The Westin, Distance, 5 miles) is pointed, our model copies “5 miles” as an output word. Notice that only a specific section of the KB, relevant to a specific dialog, is loaded into the memory. 1470 Task"
P18-1136,P02-1040,0,0.10299,"uracy: A generative response is correct only if it is exactly the same as the gold response. A dialog is correct only if every generated responses of the dialog are correct, which can be considered as the task-completion rate. Note that Bordes and Weston (2017) tests their model by selecting the system response from predefined response candidates, that is, their system solves a multi-class classification task. Since Mem2Seq generates each token individually, evaluating with this metric is much more challenging for our model. BLEU: It is a measure commonly used for machine translation systems (Papineni et al., 2002), but it has also been used in evaluating dialog systems (Eric and Manning, 2017; Zhao et al., 2017) and chat-bots (Ritter et al., 2011; Li et al., 2016). Moreover, BLEU score is a relevant measure in task-oriented dialog as there is not a large variance between the generated answers, unlike open domain generation (Liu et al., 2016). Hence, we include BLEU score in our evaluation (i.e. using Moses multi-bleu.perl script). Entity F1: We micro-average over the entire set of system responses and compare the entities in plain text. The entities in each gold system response are selected by a predef"
P18-1136,D11-1054,0,0.0993531,"Missing"
P18-1136,P17-1099,0,0.163623,"the Pptr is trained to produce the sentinel token $, as shown in Equation 1. Once the sentinel is chosen, our model generates the token from Pvocab , otherwise, it takes the memory content using the Pptr distribution. Basically, the sentinel token is used as a hard gate to control which distribution to use at each time step. A similar approach has been used in (Merity et al., 2017) to control a soft gate in a language modeling task. With this method, the model does not need to learn a gating function separately as in Gulcehre et al. (2016), and is not constrained by a soft gate function as in See et al. (2017). 2.3 Memory Content We store word-level content X in the memory module. Similar to Bordes and Weston (2017), we add temporal information and speaker information in each token of X to capture the sequential dependencies. For example, “hello t1 $u” means “hello” at time step 1 spoken by a user. On the other hand, to store B, the KB information, we follow the works of Miller et al. (2016); Eric et al. (2017) that use a (subject, relation, object) representation. For example, we represent the information of The Westin in Table 1: (The Westin, Distance, 5 miles). Thus, we sum word embeddings of th"
P18-1136,D16-1027,0,0.0207541,"used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work, are the memory based decoders and the nonrecurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN) (Graves et al., 2014, 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016), and meta-learning application (Kaiser et al., 2017). However, Mem2Seq differs from these models as such: it uses multi1475 hop attention in combination with copy mechanism, whereas other models use a single matrix representation. 2) non-recurrent generative models (Vaswani et al., 2017), which only rely on selfattention mechanism, are related to the multi-hop attention mechanism used in MemNN. 7 Conclusion In this work, we present an end-to-end trainable Memory-to-Sequence model for task-oriented dialog systems. Mem2Seq combines the multi-hop attention mechanism in end-to-end memory networks"
P18-1136,E17-1042,0,0.412796,"away and PAD is no traffic Palo Alto Cafe is 4 miles away at 436 Alger Drive Palo Alto is located at 436 Alger Dr. Table 1: Example of generated responses for the In-Car Assistant on the navigation domain. Introduction ∗ DRIVER Seq2Seq +Attn Ptr-Unk Distance 5 miles 4 miles 5 miles 4 miles 6 miles 6 miles 2 miles techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al., 2016; Wen et al., 2017; Zhao et al., 2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism (Gulcehre et al., 2016; Eric and Manning, 2017) have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities. However, although the above mentioned approaches were successful, they still suffe"
P18-1136,P17-1062,0,0.0548383,"ntities and recurrent patterns. Thus, we believe BLEU score still can be considered as a relevant measure. In future works, several methods could be applied (e.g. Reinforcement Learning (Ranzato et al., 2016), Beam Search (Wiseman and Rush, 2016)) to improve both responses relevance and entity F1 score. However, we preferred to keep our model as simple as possible in order to show that it works well even without advanced training methods. 6 Related Works End-to-end task-oriented dialog systems train a single model directly on text transcripts of dialogs (Wen et al., 2017; Serban et al., 2016; Williams et al., 2017; Zhao et al., 2017; Seo et al., 2017; Serban et al., 2017). Here, RNNs play an important role due to their ability to create a latent representation, avoiding the need for artificial state labels. End-to-End Memory Networks (Bordes and Weston, 2017; Sukhbaatar et al., 2015), and its variants (Liu and Perez, 2017; Wu et al., 2017, 2018) have also shown good results in such tasks. In each of these architectures, the output is produced by generating a sequence of tokens, or by selecting a set of predefined utterances. Sequence-to-sequence (Seq2Seq) models have also been used in task-oriented dia"
P18-1136,D16-1137,0,0.0296642,"Missing"
P18-1136,W17-5505,0,0.460644,"o traffic Palo Alto Cafe is 4 miles away at 436 Alger Drive Palo Alto is located at 436 Alger Dr. Table 1: Example of generated responses for the In-Car Assistant on the navigation domain. Introduction ∗ DRIVER Seq2Seq +Attn Ptr-Unk Distance 5 miles 4 miles 5 miles 4 miles 6 miles 6 miles 2 miles techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al., 2016; Wen et al., 2017; Zhao et al., 2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism (Gulcehre et al., 2016; Eric and Manning, 2017) have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities. However, although the above mentioned approaches were successful, they still suffer from two main prob"
P18-2096,D14-1181,0,0.00264283,"ximum quality for the ground truth annotations. For this channel we extract word2vec word embeddings from transcriptions and feed those into a CNN. The embeddings have a dimensionality of k = 300 and were pre-trained on Google News data (around 300 billion words). This enables us to take into account much more contextual information than available in just the corpus at hand. Transcriptions per sample were split up into different sentences. We normalized the text in order to align our corpus with the embedding dictionary. We fed the computed matrix into a CNN, whose architecture is inspired by Kim (2014). Three convolutional windows of size three, four, and five words are slid over the sentence, taking steps of one word each time. These windows are expected to extract compact n-grams from sentences. After this layer, a max-pooling is taken for the outcome of each of the kernels separately to get a final sentence encoding. The representation is then mapped through a fully connected layer with sigmoid activation, to the final Big Five personality traits. 2.3 Video channel In the video channel, we first take a random frame from each of the videos, which leads to personality recognition from only"
P19-1078,D18-1398,0,0.0366563,"y. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limit"
P19-1078,P16-1014,0,0.0299372,"ded dialogue history is represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal"
P19-1078,K16-1002,0,0.113926,"Missing"
P19-1078,W14-4337,0,0.761555,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,D18-1547,0,0.333872,"For example, as shown in Fig. 1, (slot, value) pairs such as (price, cheap) and (area, centre) are extracted from the conversation. Accurate DST performance is crucial for ∗ Work partially done while the first author was an intern at Salesforce Research. 808 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808–819 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of possible values. Therefore, many of the previous works that are based on neural classification models may not be applicable in real scenario. Budzianowski et al. (2018) recently introduced a multi-domain dialogue dataset (MultiWOZ), which adds new challenges in DST due to its mixed-domain conversations. As shown in Fig. 1, a user can start a conversation by asking to reserve a restaurant, then requests information regarding an attraction nearby, and finally asks to book a taxi. In this case, the DST model has to determine the corresponding domain, slot and value at each turn of dialogue, which contains a large number of combinations in the ontology, i.e., 30 (domain, slot) pairs and over 4,500 possible slot values in total. Another challenge in the multidoma"
P19-1078,W14-4340,0,0.678325,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,N18-2115,0,0.0286875,"th only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications"
P19-1078,P18-5002,0,0.0582567,"Missing"
P19-1078,D18-1299,0,0.504676,"Missing"
P19-1078,P18-1133,0,0.202571,"eatures and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters between slots, and Zhong et al. (2018) uses slot-specific local modules to learn slot features, which has proved to successfully improve tracking of rare slot values. Lei et al. (2018) use a Seq2Seq model to generate belief spans and the delexicalized response at the same time. Ren et al. (2018) propose StateNet that generates a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approac"
P19-1078,P18-1136,1,0.873258,"represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y l"
P19-1078,P17-1099,0,0.030768,"he 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y label is used. We tween Pjk j define Lv as sum of two dis"
P19-1078,D16-1022,0,0.0865702,"Missing"
P19-1078,P17-1163,0,0.574186,"Missing"
P19-1078,D17-1314,0,0.0174056,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,P17-2023,0,0.0220702,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,D14-1162,1,0.106752,"five domains. The numbers in the last three rows indicate the number of dialogues for train, validation and test sets. Fi (Θi − ΘS,i )2 , (8) MinimizeΘ L(Θ) Hotel price, type, parking, stay, day, people, area, stars, internet, name 3381 416 394 4.2 Training Details Multi-domain Joint Training The model is trained end-to-end using the Adam optimizer (Kingma and Ba, 2015) with a batch size of 32. The learning rate annealing is in the range of [0.001, 0.0001] with a dropout ratio of 0.2. Both α and β in Eq (7) are set to one. All the embeddings are initialized by concatenating Glove embeddings (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2016), where the dimension is 400 for each vocabulary word. A greedy search decoding strategy is used for our state generator since the generated slot values are usually short in length. In addition, to in(9) where L(Θ, K) is the loss value of the K stored samples. Lopez-Paz et al. (2017) show how to solve the optimization problem in Eq (9) with quadratic programming if the loss of the stored samples increases. 812 crease model generalization and simulate an outof-vocabulary setting, a word dropout is utilized with the utterance encoder by randomly"
P19-1078,P18-2069,0,0.230289,"tes a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approach using two-layer bi-GRU. Although it does not need an ad-hoc state update mechanism, it relies on delexicalization to extract the features. Ramadan et al. (2018) propose a model to jointly track domain and the dialogue states using multiple bi-LSTM. They utilize semantic similarity between utterances and the ontology terms and allow the information to be shared across domains. For a more general overview, readers may refer to the neural dialogue review paper from Gao et al. (2018). 7 Conclusion We introduce a transferable dialogue state generator for multi-domain dialogue state tracking, which learns to track states without any predefined domain ontology. TRADE shares all of its parameters across multiple domains and achieves stateof-the-art joint goa"
P19-1078,W13-4067,0,0.279858,"al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share"
P19-1078,E17-1042,0,0.323621,"Missing"
P19-1078,W14-4339,0,0.101165,", 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters betwe"
P19-1078,P17-1062,0,0.0575854,"ple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representatio"
P19-1078,P18-1134,0,0.500512,"for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogue systems, such as restaurant reservation or ticket booking. The goal of DST is to extract user goals/intentions expressed d"
P19-1078,N18-1109,0,0.0241526,"type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opin"
P19-1078,W18-5001,0,0.0369032,"nd, number-related slots such as arrive by, people, and stay usually have the lowest error rates. We also find that the type slot of hotel domain has a high error rate, even if it is an easy task with only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al."
P19-1078,P18-1135,1,0.917453,"ng, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogu"
P19-1542,P17-1152,0,0.0492102,"Missing"
P19-1542,P18-5002,0,0.0782059,"Missing"
P19-1542,D18-1398,0,0.0579393,"lf. Recently, several meta-learning models has been proposed for solving few-shot image classification (Ravi −0.05 0 1 3 5 10 K-shot Figure 2: k-shot results for different settings. Consistency of PAML grows linearly with respect to k. and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017). Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al."
P19-1542,P16-1094,0,0.200529,"ows linearly with respect to k. and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017). Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018). However, all of these previous works conditioned their response on the persona description, instead of using the dialogues produced by the persona. 5 Conclusion In this paper, we present a novel"
P19-1542,D16-1230,0,0.112712,"Missing"
P19-1542,P18-1136,1,0.862632,"tead of using the dialogues produced by the persona. 5 Conclusion In this paper, we present a novel meta-learning setting for personalizing dialogue agents without conditioning the model response to the persona description. This is especially useful since obtaining such persona description requires human effort. Moreover, we show that a dialogue agent trained with meta-learning achieves a more consistent dialogue by both of automatic measures and human evaluation. In future works, we plan to apply meta-learning to comment genera5457 tion (Lin et al., 2019) and task-oriented dialogues systems (Madotto et al., 2018; Wu et al., 2019, 2017, 2018; Reddy et al., 2018). Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. 2019. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415. 6 Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, and Xiaodong He. 2018. Natural language to structured query generation via metalearning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 732–738. Association for Computational Ling"
P19-1542,D18-1298,0,0.149968,"of them. For example, “I am an old man” and “I like to play football” are one of the possible persona descriptions provided to the speaker. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona consistent dialogue (Zhang et al., 2018). However, it is difficult to capture a persona just by using few sentences, and collecting a nonsynthetic set of persona descriptions from a real human-human conversation, e.g., Reddit, is challenging as well since it requires hand-crafted fea† These two authors contributed equally. ture designs (Mazare et al., 2018). In light of this, we propose to leverage a set of dialogues done by the same persona directly, instead of using its persona descriptions, to generate a more consistent response. We consider learning different personas as different tasks via meta-learning algorithms, which is fundamentally different from optimizing the model to represent all the personas. A high-level intuition of the difference between these two approaches is shown in Figure 1. We aim to learn a persona-independent model that is able to quickly adapt to a new persona given the dialogues. We formulate this task as a few-shot"
P19-1542,P02-1040,0,0.103242,"Missing"
P19-1542,D14-1162,0,0.0812706,"Missing"
P19-1542,N18-1109,0,0.0238206,"been proposed for solving few-shot image classification (Ravi −0.05 0 1 3 5 10 K-shot Figure 2: k-shot results for different settings. Consistency of PAML grows linearly with respect to k. and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017). Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018). However, all of these previous works c"
P19-1542,K18-1053,0,0.167015,"w resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018). However, all of these previous works conditioned their response on the persona description, instead of using the dialogues produced by the persona. 5 Conclusion In this paper, we present a novel meta-learning setting for personalizing dialogue agents without conditioning the model response to the persona description. This is especially useful since obtaining such persona description requires human effort. Moreover, we show that a dialogue agent trained with meta-learning achieves a more consistent dialogue by both of automatic measures and human evaluation. In future works"
P19-1542,P18-1205,0,0.638362,"man designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-metalearning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency. 1 ⋯ ⋯ Figure 1: The difference between finetuning from a) joint training on all personas and b) meta-learning persona. The solid line represents the optimization path of the initial parameters and dashed line the fine-tuning path. Meta-learned initial parameters can faster adapt to a new persona. Introduction There is a growing interest in learning personalized chit-chat dialogue agents for making chatbots more consistent. Recently, a m"
P95-1032,P91-1022,0,0.231888,"Missing"
P95-1032,P93-1002,0,0.0588472,"Missing"
P95-1032,P93-1001,0,0.204834,"Missing"
P95-1032,W93-0301,0,0.47111,"Missing"
P95-1032,C94-2178,1,0.875843,"Missing"
P95-1032,1994.amta-1.11,1,0.684668,"Missing"
P95-1032,J93-1004,0,0.343319,"Missing"
P95-1032,C94-1009,0,0.608205,"Missing"
P95-1032,P93-1003,0,0.302745,"Missing"
P95-1032,H94-1027,0,0.113459,"Missing"
P95-1032,P94-1012,0,0.136483,"re nouns, proper nouns or noun phrases, compiling a bilingual lexicon of these word groups is an important first step. We have been studying robust lexicon compilation methods which do not rely on sentence alignment. Existing lexicon compilation methods (Kupiec 1993; Smadja & McKeown 1994; Kumano & Hirakawa 1994; Dagan et al. 1993; Wu & Xia 1994) all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts. However, sentence alignment (Brown et al. 1991; Kay & RSscheisen 1993; Gale & Church 1993; Church 1993; Chen 1993; Wu 1994) is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language. Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier (Fung & Church 1994; Fung & McKeown 1994). Those procedures attempted to align texts by finding matching word pairs and have demonstrated their effectiveness for Chinese/English and Japanese/English. The main focus then was accurate alignment, but the procedure produced a small number of word translations as a by-product. In contrast, our new al"
P95-1032,1994.amta-1.26,0,0.702656,"Missing"
P95-1032,J93-1006,0,\N,Missing
P98-1069,P91-1022,0,0.048507,"nity regards the W W W as a vast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable te"
P98-1069,J93-2003,0,0.0297706,"pus linguistic community regards the W W W as a vast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-d"
P98-1069,P93-1001,0,0.00972848,"ast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable texts in electronic form."
P98-1069,A94-1006,0,0.0313631,"g Chinese ~ } ~ (Weng-hui) (~u) ~ (Lei) ~ j ~ (Poultry) ~ (Chee-hwa) ~}~ (Teng-hui) ~#~ ~'~ :~ ~}~ (SAR) (Chee-hwa) (Teng-hui) (Weng-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we"
P98-1069,J94-4003,0,0.0156287,"oultry in Chinese is closely related to flu because the Chinese name for bird flu is poultry flu. In fact, almost all unambiguous Chinese new words find their translations in the first 100 of the ranked list. Six of the Chinese words have correct translation as their first candidate. 9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979). This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 418 score 0.008421 0.007895 0.007669 0.007588 0.007283 0.006812 0.006430 0.006218 0.005921 0.005527 0.005335 0.005335 0.005221 0.004731 0.004470 0.004275 0.003878 0.003859 0.003859 0.003784 0.003686 0.003550 0.003519 0.003481 0.003407 0.003407 0.003338 0.003324 0.003250 0.003206 0.003202 0.003040 0.003033 0.002888 0.002886 English Teng-hui SAR flu Lei poultry SAR hijack poultry Tung Diaoyu PrimeMinister President"
P98-1069,C94-2178,1,0.205722,"ee-hwa) (Teng-hui) (Weng-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have demonstrated that the associations between a word and its context seed words are well-preserved in no"
P98-1069,W97-0119,1,0.810303,"3 Mb of text whereas the Chinese text contains 8.8 Mb of 2 byte character texts. So both texts are comparable in size. Since they are both local mainstream newspapers, it is reasonable to assume that their contents are comparable as well. 3 Table 2: ~ English South African China ties diplomatic Taiwan relations Test Mandela Taipei Africans January visit tense survived Beijing YL~,/liougan is a s s o c i a t e d w i t h flu b u t n o t w i t h Africa Unlike in parallel texts, the position of a word in a text does not give us information about its translation in the other language. (Rapp, 1995; Fung and McKeown, 1997) suggest that a content word is closely associated with some words in its context. As a tutorial example, we postulate that the words which appear in the context of ~ / l i o u g a n should be similar to the words appearing in the context of its English translation, flu. We can form a vector space model of a word in terms of its context word indices, similar to the vector space model of a text in terms of its constituent word indices (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979). The value of the i-th dimens"
P98-1069,W95-0114,1,0.618783,"g-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have demonstrated that the associations between a word and its context seed words are well-preserved in nonparallel, c"
P98-1069,P95-1032,1,0.477687,"g-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have demonstrated that the associations between a word and its context seed words are well-preserved in nonparallel, c"
P98-1069,J93-1004,0,0.117341,"W as a vast potential of corpus resources. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable texts in electronic form."
P98-1069,P92-1032,0,0.00532404,"losely related to flu because the Chinese name for bird flu is poultry flu. In fact, almost all unambiguous Chinese new words find their translations in the first 100 of the ranked list. Six of the Chinese words have correct translation as their first candidate. 9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979). This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 418 score 0.008421 0.007895 0.007669 0.007588 0.007283 0.006812 0.006430 0.006218 0.005921 0.005527 0.005335 0.005335 0.005221 0.004731 0.004470 0.004275 0.003878 0.003859 0.003859 0.003784 0.003686 0.003550 0.003519 0.003481 0.003407 0.003407 0.003338 0.003324 0.003250 0.003206 0.003202 0.003040 0.003033 0.002888 0.002886 English Teng-hui SAR flu Lei poultry SAR hijack poultry Tung Diaoyu PrimeMinister President China Lien poultry"
P98-1069,P93-1003,0,0.0593727,"Poultry) ~ (Chee-hwa) ~}~ (Teng-hui) ~#~ ~'~ :~ ~}~ (SAR) (Chee-hwa) (Teng-hui) (Weng-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have demonstrated that the associati"
P98-1069,P95-1050,0,0.335541,"tains about 3 Mb of text whereas the Chinese text contains 8.8 Mb of 2 byte character texts. So both texts are comparable in size. Since they are both local mainstream newspapers, it is reasonable to assume that their contents are comparable as well. 3 Table 2: ~ English South African China ties diplomatic Taiwan relations Test Mandela Taipei Africans January visit tense survived Beijing YL~,/liougan is a s s o c i a t e d w i t h flu b u t n o t w i t h Africa Unlike in parallel texts, the position of a word in a text does not give us information about its translation in the other language. (Rapp, 1995; Fung and McKeown, 1997) suggest that a content word is closely associated with some words in its context. As a tutorial example, we postulate that the words which appear in the context of ~ / l i o u g a n should be similar to the words appearing in the context of its English translation, flu. We can form a vector space model of a word in terms of its context word indices, similar to the vector space model of a text in terms of its constituent word indices (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979). The"
P98-1069,1992.tmi-1.7,0,0.034691,"urces. It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms. It remains to be seen how we can also make use of the multilingual texts as NLP resources. In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts. Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts. This type of texts are known as nonparallel corpora. Such nonparallel, monolingual texts should be much more prevalent than parallel texts. However, previous attempts at using nonparallel corpora for terminology translation 414 were constrained by the inadequate availability of same-domain, comparable texts in electronic form. The type of nonparallel texts obtai"
P98-1069,J96-1001,0,0.153082,"ee-hwa) ~}~ (Teng-hui) ~#~ ~'~ :~ ~}~ (SAR) (Chee-hwa) (Teng-hui) (Weng-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper, we have demonstrated that the associations between a word an"
P98-1069,1994.amta-1.26,0,0.0169787,"g China Zhuhai Tung Chinese ~ } ~ (Weng-hui) (~u) ~ (Lei) ~ j ~ (Poultry) ~ (Chee-hwa) ~}~ (Teng-hui) ~#~ ~'~ :~ ~}~ (SAR) (Chee-hwa) (Teng-hui) (Weng-hui) W}~ (Weng-hui) CLam) ~ } ~ (Teng-hui) ~-~ (Chee-hwa) ~_}~ (Teng-hui) (Lei) ~ ' ~ (Chee-hwa) ~ ' ~ (Chee-hwa) . ~ (Leung) ~ (Zhuhai) I~ (Lei) ~J~ (Yeltsin) ~ - ~ (Chee-hwa) )~ (Lam) (Lam) ~ j ~ (Poultry) W~ (Teng-hui) ~ } ~ (Teng-hui) (Tang) (Leung) (Leung) ~#~ (SAR) ~ (Lunar) (Tung) 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b). These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997). Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another lan"
P98-1069,P95-1026,0,0.0378523,"rd flu is poultry flu. In fact, almost all unambiguous Chinese new words find their translations in the first 100 of the ranked list. Six of the Chinese words have correct translation as their first candidate. 9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979). This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 418 score 0.008421 0.007895 0.007669 0.007588 0.007283 0.006812 0.006430 0.006218 0.005921 0.005527 0.005335 0.005335 0.005221 0.004731 0.004470 0.004275 0.003878 0.003859 0.003859 0.003784 0.003686 0.003550 0.003519 0.003481 0.003407 0.003407 0.003338 0.003324 0.003250 0.003206 0.003202 0.003040 0.003033 0.002888 0.002886 English Teng-hui SAR flu Lei poultry SAR hijack poultry Tung Diaoyu PrimeMinister President China Lien poultry China flu PrimeMinister President poultry Kalkanov"
P98-1069,J93-1006,0,\N,Missing
P98-1069,H91-1026,0,\N,Missing
P99-1043,P91-1022,0,0.0162147,"problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), para"
P99-1043,P98-1069,1,0.858053,"of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995"
P99-1043,W97-0119,1,0.824276,", there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shfitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use all the contextual words in the query sentence. Each contextual word ""votes"" by its mutual information with all translation candidates. Suppose there are n primary language words in S = E 1 , E 2 , . . . , C , . . . , E n , as shown in Figure 2, we compute mutual information scores 336 Table 1: Mutual information between all translation candidates and words in the sentence For each row j in Table 1, the largest scoring MIjci receives a vote. The r"
P99-1043,J94-4003,0,0.304657,"n are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We"
P99-1043,P91-1017,0,0.501087,"nguage disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rar"
P99-1043,J93-1004,0,0.0180758,"ct translations of C should co-occur frequently with the contextual words Ei and incorrect translation of C should co-occur rarely with the contextual words. Obviously, other information such as syntactical relationship between words or the part-of-speech tags could be used as weights too. However, it is difficult to parse and tag a mixed language sentence. The only information we can use to disambiguate C is the co-occurrence information between its translation candidates { Ec, } and El, E2, . . . , En. Mutual information is a good measure of the co-occurrence relationship between two words (Gale and Church, 1993). We first compute the mutual information between any word pair from a monolingual corpus in the primary language 2 Baseline: single neighboring w o r d as d i s a m b i g u a t i n g feature The first disambiguating feature we present here is similar to the statistical feature in (Dagan et al., 1991; Smadja et al., 1996; Dagan and Itai, 1994; Ballesteros and Croft, 1998), namely the co-occurrence with neighboring words. We do not use any syntactic relationship as in (Dagan and Itai, 1994) because such relationship is not available for mixed-language sentences. The assumption here is that the"
P99-1043,P92-1032,0,0.0299506,"ranslation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a me"
P99-1043,1992.tmi-1.9,0,0.0198032,"ranslation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a me"
P99-1043,H93-1016,0,0.0234767,"es only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shfitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multiple context words are useful (Fung and Lo, 1998; Fung and McKeown, 1997). Based on the above arguments, we enlarge the disambiguation window to be the entire sentence instead of only one word to the left or right. We use all the contextual w"
P99-1043,P93-1003,0,0.0142471,"ndary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tan"
P99-1043,J96-1001,0,0.188732,"IR system in the secondary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993;"
P99-1043,J93-1007,0,0.042204,"secondary language and the query is passed to another IR system in the secondary language. Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual paralle"
P99-1043,C96-2098,0,0.0211505,"uage query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu"
P99-1043,1995.tmi-1.28,0,0.0123225,". Our methods allows for both general and crosslanguage IR from a mixed language query. To draw a parallel to the three problems of query translation, we suggest that the three main problems of mixed language disambiguation are: 1. generating translation candidates in the primary language, 2. weighting translation candidates, and 3. pruning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iw"
P99-1043,H93-1052,0,0.0358899,"ne. Ey is the neighboring word for C. Since MI1 is greater than MI2, Ecl is selected as the translation of C. E2 Ec~ °o. 2.3.2 Voting: multiple contextual w o r d s as d i s a m b i g u a t i n g f e a t u r e The baseline m e t h o d uses only the neighboring word to disambiguate C. Is one or two neighboring word really sufficient for disambiguation? The intuition for choosing the nearest neighboring word Ey as the disambiguating feature for C is based on the assumption that they are part of a phrase or collocation term, and that there is only one sense per collocation (Dagan and Itai, 1994; Yarowsky, 1993). However, in most cases where C is a single word, there might be some other words which are more useful for disambiguating C. In fact, such long-distance dependency occurs frequently in natural language (Rosenfeld, 1995; Huang et al., 1993). Another reason against using single neighboring word comes from (Gale and Church, 1994) where it is argued that as many as 100,000 context words might be needed to have high disambiguation accuracy. (Shfitze, 1992; Yarowsky, 1995) all use multiple context words as discriminating features. We have also demonstrated in our domain translation task that multi"
P99-1043,P95-1026,0,0.273129,"ning translation alternatives for query translation. Co-occurrence information between neighboring words and words in the same sentence has been used in phrase extraction (Smadja, 1993; Fung and Wu, 1994), phrasal translation (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Dagan and Church, 1994), target word selection (Liu and Li, 1997; Tanaka and Iwasaki, 1996), domain word translation (Fung and Lo, 1998; Fung, 1998), sense disambiguation (Brown et al., 1991; Dagan et al., 1991; Dagan and Itai, 1994; Gale et al., 1992a; Gale et al., 1992b; Gale et al., 1992c; Shiitze, 1992; Gale et al., 1993; Yarowsky, 1995), and even recently for query translation in cross-language IR as well (Ballesteros and Croft, 1998). Co-occurrence statistics is collected from either bilingual parallel and non-parallel corpora (Smadja et al., 1996; Kupiec, 1993; Wu, 1995; Tanaka and Iwasaki, 1996; Fung and Lo, 1998), or monolingual corpora (Smadja, 1993; Fung and Wu, 1994; Liu and Li, 1997; Shiitze, 1992; Yarowsky, 1995). As we noted in (Fung and Lo, 1998; Fung, 1998), parallel corpora are rare in most domains. We want to devise a method that uses only monolingual data in the primary language to train co-occurrence informat"
P99-1043,1998.amta-tutorials.5,0,\N,Missing
P99-1043,C98-1066,1,\N,Missing
P99-1043,A94-1006,0,\N,Missing
S15-2004,P08-1007,0,0.0304881,"(Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words are similar in each meaning, BADGER (Parker, 2008) the distance between the compression of each sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking one of the sentences as the reference and the other as the sample and then vice-versa. Both scores are included as distinct features"
S15-2004,P09-1053,0,0.0856355,"labeled by five users via Amazon Mechanical Turk, hence providing a six-level classification label (from (5, 0) when all the five user classify the pair as a paraphrase, to (0, 5) when none of them identifies the pair to be a paraphrase). 3.2 Experimental Setup The neural network was setup with a hidden layer dimension of three times the input. The development set was used to tune the L2 regularization coefficient, set at γ = 0.01, as well as the learning rate and the other hyperparameters, and to have a measure of improvement against the official thresholding baseline provided for the task (Das and Smith, 2009). To implement the neural network we used THEANO Python toolkit (Bergstra et al., 2010). We train the network with all the sentences provided in the training set. The objective label of the cross-entropy objective function was set to 1.0 for pairs labeled (5, 0) and (4, 1), 0.75 for pairs labeled (3, 2), 0.5 for pairs labeled (2, 3) and 0.0 for pairs labeled (0, 5). This choice allowed a more fine training for task 2, where a continuous similarity value must be estimated, without altering too much the behavior in the binary estimation task 1. The training procedure was repeated several times,"
S15-2004,P12-1091,0,0.0524097,"rved in previous experiments where we employed it, which fails to analyze sentences containing certain 26 patterns and predicates. The second reason, more specific to Twitter domain, is that some sentences lack a valid predicate or a proper grammatical structure. This prevents the semantic parser from giving an accurate output. The inclusion on latent semantic features in run 2 proved to be ineffective, as it improved subtask 1 F-score by less than 0.001, and gave a worse performance in subtask 2. During the evaluation phase other experiments were tried as using the latent semantic vectors of Guo and Diab (2012), or using the vectors as described in Ji and Eisenstein (2013) instead of the extra layer, and other modifications, all without obtaining any perceptible improvement when the system was tested on the development set. The non-perfect implementation and usage of these features, together with the fact they might not be suitable to be applied to Twitter domain, may explain this lack of improvement. 4 Conclusions We have used a neural network classifier, with a combination of multiple views of lexical, syntactic and semantic information, as the system which participated in SemEval 2015 task 1, who"
S15-2004,P11-1038,0,0.0456045,"Missing"
S15-2004,I05-5003,0,0.0553018,"Missing"
S15-2004,W13-2254,0,0.0173989,"atures is obtained by looking at the semantic roles themselves and their alignment without looking at the content: these include the percentage of semantic roles of one sentence that are also present in the other, the percentage of correct pairs of semantic roles after the alignment operated for MEANT, and a binary feature equal to 1 in case the semantic parser fails to give any output for at least one of the sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is set to the value of the Jaccard coefficient between the whole sentences (Lo and Wu, 2013). 2.3 Translation Metrics Previous work (Finch et al., 2005; Madnani et al., 2012) have shown that machine translation evaluation metrics are useful for the paraphrase recognition task, due to their ability to capture useful similarity information to correctly classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the m"
S15-2004,2007.tmi-papers.10,1,0.737528,"a algorithm (Zhang and Shasha, 1989; Wan et al., 2006). 2.2 Semantic Similarity Features The way we evaluate the semantic similarity of each pair of sentences is through the analysis of the semantic roles. The first feature we choose in this 24 sense is the semantic role based MEANT machine translation score (Lo et al., 2012), effective to provide, as shown by various experiments, a translation evaluation closer to human judges. This metric first annotates each sentence with semantic roles (Pradhan et al., 2004), then aligns them and computes a similarity score only within the aligned frames (Fung et al., 2007) using the Jaccard coefficient (Tumuluru et al., 2012). Another set of features is obtained by looking at the semantic roles themselves and their alignment without looking at the content: these include the percentage of semantic roles of one sentence that are also present in the other, the percentage of correct pairs of semantic roles after the alignment operated for MEANT, and a binary feature equal to 1 in case the semantic parser fails to give any output for at least one of the sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is"
S15-2004,W12-3129,0,0.036214,"Missing"
S15-2004,N12-1019,0,0.195133,"Water Bay, Hong Kong dbertero@ust.hk, pascale@ece.ust.hk Abstract latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. This paper describes the system developed by our team (HLTC-HKUST) for task 1 of SemEval 2015 workshop about paraphrase classification and semantic similarity in Twitter. We trained a neural network classifier over a range of features that includes translation metrics, lexical and syntactic similarity score and semantic features based on semantic roles. The neural network was trained taking into consideratio"
S15-2004,P02-1040,0,0.0927033,"he sentences. In this last case all the other features based on semantic roles are 0 except the MEANT score which is set to the value of the Jaccard coefficient between the whole sentences (Lo and Wu, 2013). 2.3 Translation Metrics Previous work (Finch et al., 2005; Madnani et al., 2012) have shown that machine translation evaluation metrics are useful for the paraphrase recognition task, due to their ability to capture useful similarity information to correctly classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evalu"
S15-2004,2006.amta-papers.25,0,0.0515021,"y classify the sentence pairs. The various translation metrics all take into account different aspects of sentence similarities. BLEU (Papineni et al., 2002) and the subsequent evaluation metrics such as NIST (Goutte, 2006) and SEPIA (Habash and Elkholy, 2008) look at n-gram overlaps between the source and the target sentences. While the most basic BLEU takes into consideration only n-gram overlap, the other metrics also consider synonyms, stemming, simple paraphrase patterns and the syntactic structure of the n-grams. Yet another set of metrics are based instead on different principles: TER (Snover et al., 2006) and TERp (Snover et al., 2009) count the number of edits needed to transform a sentence into the other, MAXSIM (Chan and Ng, 2008) evaluates lexical similarity performing a word-by-word matching and finding out how much the aligned words are similar in each meaning, BADGER (Parker, 2008) the distance between the compression of each sentence obtained from the Burrows-Wheeler transform algorithm (Burrows and Wheeler, 1994), and MEANT which, as discussed in the previous section, scores the similarity of aligned semantic frames. For each pair of sentences the scores are calculated first taking on"
S15-2004,Y12-1062,0,0.060443,"Water Bay, Hong Kong dbertero@ust.hk, pascale@ece.ust.hk Abstract latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. This paper describes the system developed by our team (HLTC-HKUST) for task 1 of SemEval 2015 workshop about paraphrase classification and semantic similarity in Twitter. We trained a neural network classifier over a range of features that includes translation metrics, lexical and syntactic similarity score and semantic features based on semantic roles. The neural network was trained taking into consideratio"
S15-2004,U06-1019,0,0.561368,"Hong Kong dbertero@ust.hk, pascale@ece.ust.hk Abstract latent semantic vectors, lexical and syntactic similarity features. Although their main objective was to show the effectiveness of a method based on latent semantic analysis, it is also evident that other features pertinent to different aspects of sentence similarity are able to boost the results. Previously Socher et al. (2011) used a recursive autoencoder to similarly obtain a vector representation of each sentence, again combining other lexical similarity features to improve the results. Other methods, such as Madnani et al. (2012) or Wan et al. (2006) used instead a more traditional supervised classification approach over different sets of features and different classifiers, most of which improved previous results. This paper describes the system developed by our team (HLTC-HKUST) for task 1 of SemEval 2015 workshop about paraphrase classification and semantic similarity in Twitter. We trained a neural network classifier over a range of features that includes translation metrics, lexical and syntactic similarity score and semantic features based on semantic roles. The neural network was trained taking into consideration in the objective fu"
S15-2004,S15-2001,0,0.0635914,"Missing"
S15-2004,I05-5002,0,\N,Missing
S15-2004,Q14-1034,0,\N,Missing
S15-2004,D13-1090,0,\N,Missing
S15-2004,N04-1030,0,\N,Missing
S18-1039,D14-1162,0,0.0853705,"nsfer the emotional knowledge by exploiting neural network models as feature extractors and use these representations for traditional machine learning models such as support vector regression (SVR) and logistic regression to solve the competition tasks. Our system is placed among the Top3 for all subtasks we participated. 1 Introduction Finding a good representation of texts is very challenging since texts are sequences of words which are represented in a discrete space of the vocabulary. For this reason, many past works have investigated in finding the mapping of words (Mikolov et al., 2013; Pennington et al., 2014) or sentences (Kiros et al., 2015) to continuous spaces, so that each text can be represented by a fixed-size, realvalued N-dimensional vector. This vector representation then can be applied to machine learning models to solve problems like classification and regression. A good representation should contain essential information inside each text and be a useful input for statistical models. Emotions in texts further deepen the complexity of modeling natural language since they not only depend on the semantics of a language but also are inherently subjective and ambiguous. Despite the difficult"
S18-1039,S16-1010,0,0.049169,"Missing"
S18-1039,D17-1169,0,0.0439512,"we group the 34 emojis into 11 clusters according to the distance on the correlation matrix of the hierarchical clustering from Felbo et al. (2017) and use them as categorical labels tags and emoticons (Suttles and Ide, 2013; Wang et al., 2012), and found them useful to distantly label an emotion of each text. Furthermore, the recent popular culture of using emojis (Wood and Ruder, 2016) inside social media posts and messages provides us even richer evidence of different emotions, and they have been proven to be very effective in learning rich representations for various affect-related tasks (Felbo et al., 2017). 2.1 Methodology & Emoji Dataset One thing i dislike is laggers man I hate inconsistency The paper is irritating me As of right now i hate dre im sick of crying im tired of trying why body pain why uuugh i really have nothing to do right now i dont wanna go back to mex looking forward to holiday well today am on lake garda enjoying the life perfect time to read book im feeling great enjoying my holiday In this paper, we compare two models using two different emoji dataset to transform the competition data into robust sentence representations. First model is the pre-trained DeepMoji model (Fel"
S18-1039,D14-1181,0,0.00882238,"Missing"
S18-1039,P14-1146,0,0.109383,"n labels, we focus on four emotion categories: joy, sadness, anger, and fear, since the competition tasks are only limited to those categories. In total, our hashtag dataset consists of 1.9 million tweets (Table 2). 3.2 Hashtag Dataset Emotional word vectors (EVEC) We also explore word-level representations, along with emoji sentence representations. Although sentence-level representations already build up from word representations (in particular we use pretrained Glove vectors (Pennington et al., 2014)), they may not be enough to attend to the valence that each word contains. Previous works (Tang et al., 2014) examine the significance of using sentiment-specific word embedding for related tasks. For this reason, we train emotional word vectors that not only cluster together direct emotion words such as anger and joy, but also capture emotions inside indirect emotion words, such as anger inside headache and joy inside beach. We learn these vectors by training a Convolutional Neural Network (CNN) from another separate Twitter corpus distantly labeled with hashtags. 3.1 Methodology Our intuition to learn effective emotion word vectors is that given a document labeled with emotion there exists one or m"
S18-1039,S18-1001,0,0.131685,"Missing"
S18-1039,L18-1030,0,0.0438488,"Missing"
S18-1039,S16-1040,0,0.0447858,"Missing"
S19-2021,D16-1110,1,0.844814,"Missing"
S19-2021,S19-2005,0,0.0485243,"Missing"
S19-2021,P15-1162,0,0.0559911,"Missing"
S19-2021,S18-1019,0,0.0419734,"Missing"
S19-2021,D17-1054,0,0.0171696,"ji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et al., 2017; Fung et al., 2018). 5 Conclusion In this paper, we compare different pre-trained word embedding features by using Logistic Regression and XGBoost along with flat and hierarchical architectures trained in end-to-end models. We further explore a GP for faster hyperparameter search. Our experiments show that hierarchical architectures give significant impro"
S19-2021,W16-6208,0,0.0375742,"Missing"
S19-2021,S18-1039,1,0.819812,"Missing"
S19-2021,D17-1169,0,0.268995,"ecifically by contextual emotion detection in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion with a hierarchical"
S19-2021,N18-1202,0,0.275659,"tep is to be able to detect emotion with a hierarchical structure. To the best of our knowledge, this task of extracting emotional knowledge in a hierarchical setting has not yet been extensively explored in the literature. Therefore, in this paper, we investigate this problem in depth with several strong hierarchical baselines and by using a large variety of pre-trained word embeddings. 2 Methodology In this task, we focus on two main approaches: 1) feature-based and 2) end-to-end. The former compares several well-known pre-trained embeddings, including GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018), as well as emotional embeddings. We combine these pre-trained features with a simple Logistic Regression (LR) and XGBoost (Chen and Guestrin, 2016) model as the classifier to compare their effectiveness. The latter approach is to train Emotion a) b) Softmax T2 c) Emotion Voting Softmax Encoder (E) T1 Emotion T3 E E E M1 T1 T2 T3 T1 T2 T3 ... Mn−1 Mn T1 T2 T3 T1 T2 T3 Figure 1: a) Flat model; b) Hierarchical model; c) Voting scheme a model fully end-to-end with back-propagation. We mainly compare the performances of flat models and hierarchical models, which al"
S19-2021,P17-4021,1,0.890635,"Missing"
S19-2021,D13-1170,0,0.00394712,"to be empathetic, specifically by contextual emotion detection in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion"
S19-2021,J11-2001,0,0.00541946,"t ways. Word-level emotional representations, inspired from word embeddings, learn a vector for each word, and have shown effectiveness in different emotion related tasks, such as sentiment classification (Tang et al., 2016), emotion classification (Xu et al., 2018), and emotion intensity prediction (Park et al., 2018). Sentence-level emotional representations, such as DeepMoji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et"
S19-2021,D17-1052,0,0.0309174,"l., 2016), emotion classification (Xu et al., 2018), and emotion intensity prediction (Park et al., 2018). Sentence-level emotional representations, such as DeepMoji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et al., 2017; Fung et al., 2018). 5 Conclusion In this paper, we compare different pre-trained word embedding features by using Logistic Regression and XGBoost along with flat and hierarchical architectures trained i"
S19-2021,W18-6243,1,0.925306,"in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion with a hierarchical structure. To the best of our knowledge"
S19-2021,D14-1162,0,\N,Missing
S19-2021,W18-3505,0,\N,Missing
S19-2184,S19-2145,0,0.0304441,"Fake news is a kind of news that is typically inflammatory, extremely one-sided (hyper-partisan) or untruthful to mislead the public into having distorted belief. Previous works attempted to solve fake news problem from various aspects, ranging from knowledge-based (Wu et al., 2014; Shi and Weninger, 2016; Lee et al., 2018) to stylebased (Wang, 2017; Potthast et al., 2018). There are some publicly available fake news datasets, however, often too small in size to be suitable for neural approaches (Horne and Adali, 2017; P´erezRosas et al., 2017). Recently, the organizers of SemEval2019 Task 4 (Kiesel et al., 2019) have released large-scale dataset to address fake news detection as a hyper-partisan news detection problem. The task is to determine whether a given article is hyper-partisan (extremely right-wing or leftwing) or not (mainstream). Such task will allow for pre-screening of semi-automatic fake news detection, and more importantly, bring us one step closer to solving fully automated fake news detection. Initially, we focused on learning and utilizing useful features such as topic and sentiment infor∗ These two authors contributed equally. Figure 1: Illustration of filtering. Sample a and c are"
S19-2184,P17-2067,0,0.0351143,"Missing"
S19-2184,D18-1143,1,0.888948,"Missing"
S19-2184,N18-1202,0,0.0282287,"s empirically showed interesting relationships between topics and hyperpartisanship. Sensitive topics such as war and political parties tend to have more hyperpartisan news than neutral-topics such as school and sports games. We believe that leveraging such information would be helpful in future works. 5 Related Works In this part, we briefly review the prior work in language representation as well as the semisupervised learning method we used. 5.1 Language Representation (Kiros et al., 2015) tried to learn sentence embedding by reconstructing the surrounding sentences of an encoded passage. (Peters et al., 2018) proposed to extract context-sensitive features from a language model. (Devlin et al., 2018) jointly conditioned on both left and right context and obtained state-of-the-art results on eleven natural language processing tasks. 5.2 Semi-supervised Learning (Triguero et al., 2015) provided a survey of selflabeled methods for semi-supervised classification. (Zhu and Goldberg, 2009) showed selflabeled techniques are typically divided into selftraining and co-training. (Lin et al., 2018) proposed semi-supervised learning to leverage a small amount of user-comment data to train a model and then expa"
S19-2184,P18-1022,0,0.0463968,"Missing"
W03-1203,P00-1041,0,0.0623125,"Missing"
W03-1203,H01-1065,0,0.104475,"o (2001,2002)). In these works, vector space models are used and document or sentence vectors are clustered together according to some similarity measure (Deerwester (1991), Dagan et al. (1997)). The disadvantage of clustering methods lies in their ad hoc nature. Since sentence vectors are considered to be independent sample points, the sentence order information is lost. Various heuristics and revision strategies have been applied to the general sentence selection schema to take into consideration text cohesion (White & Cardie (2002), Mani and Bloedorn (1999), Aone et. al (1999), Zha (2002), Barzilay et al., (2001)). We would like to preserve the natural linear cohesion of sentences in a text as a baseline prior to the application of any revision strategies. To compensate for the ad hoc nature of vector space models, probabilistic approaches have regained some interests in information retrieval in recent years (Knight & Marcu (2000), Berger & Lafferty (1999), Miller et al., (1999)). These recent probabilistic methods in information retrieval are largely inspired by the success of probabilistic models in machine translation in the early 90s (Brown et. al), and regard information retrieval as a noisy chan"
W03-1203,A88-1019,0,0.363865,"Missing"
W03-1203,P97-1008,0,0.124335,"tence and classification can often be accomplished by unsupervised, clustering methods, with little or no requirement of human labeled data (Deerwester (1991), White & Cardie (2002), Jing et. al (2000)). Unsupervised methods or hybrids of supervised and unsupervised methods for extractive summarization have been found to yield promising results that are either comparable or superior to supervised methods (Nomoto & Matsumoto (2001,2002)). In these works, vector space models are used and document or sentence vectors are clustered together according to some similarity measure (Deerwester (1991), Dagan et al. (1997)). The disadvantage of clustering methods lies in their ad hoc nature. Since sentence vectors are considered to be independent sample points, the sentence order information is lost. Various heuristics and revision strategies have been applied to the general sentence selection schema to take into consideration text cohesion (White & Cardie (2002), Mani and Bloedorn (1999), Aone et. al (1999), Zha (2002), Barzilay et al., (2001)). We would like to preserve the natural linear cohesion of sentences in a text as a baseline prior to the application of any revision strategies. To compensate for the a"
W03-1203,P94-1002,0,0.160417,"Missing"
W03-1203,W00-0403,0,0.0230266,"Missing"
W03-1203,J81-4005,0,0.590756,"Missing"
W03-1203,P97-1013,0,0.0212222,"s(2)), , c( s(t )), , c( s(T ))) . Following Bayes Rule gives us P(C |D) = P( D |C ) P(C ) / P( D) . We assume P(D) is equally likely for all documents, so that finding the best class sequence becomes: arg max P (C |D ) ≅ arg max P ( D |C ) P (C ) C C = arg max P ( s (1), c( s (1)), s ( 2), c( s ( 2)), , s (t ), c( s (t )), C P (c( s (1)), c( s (2)), , c( s (t )), c ( s (T ))) s (T ), c( s (T ))) ⋅ Note that the total number of theme classes is far fewer than the total number of sentences in a document and the mapping is not one-to-one. Our task is similar to the concept of discourse parsing (Marcu (1997)), where discourse structures are extracted from the text. In our case, we are carrying out discourse tagging, whereby we assign the class labels or tags to each sentence in the document. We use Hidden Markov Model for this stochastic process, where the classes are assumed to be hidden states. We make the following assumptions: • The probability of the sentence given its past only depends on its theme class (emission probabilities); The probability of the theme class only depends on the theme classes of the previous N sentences (transition probabilities). • The above assumptions lead to a Hidd"
W03-1203,J94-2001,0,0.145782,"Missing"
W03-1203,P02-1059,0,0.0411751,"Missing"
W03-1203,W02-0401,0,0.0379287,"story. Another application is the tracking of news stories from the single source over different time frame. In this case, documents are related by topic over time. Multi-document summarization is also an extension of single document summarization. One of the most robust and domain-independent summarization approaches is extraction-based or shallow summarization (Mani (1999)). In extraction-based summarization, salient sentences are automatically extracted to form a summary directly (Kupiec et. al, (1995), Myaeng & Jang (1999), Jing et. al, (2000), Nomoto & Matsumoto (2001,2002), Zha (2002), Osborne (2002)), or followed by a synthesis stage to generate a more natural summary (McKeown & Radev (1999), Hovy & Lin (1999)). Summarization therefore involves some theme or topic identification and then extraction of salient segments in a document. Story segmentation, document and sentence and classification can often be accomplished by unsupervised, clustering methods, with little or no requirement of human labeled data (Deerwester (1991), White & Cardie (2002), Jing et. al (2000)). Unsupervised methods or hybrids of supervised and unsupervised methods for extractive summarization have been found to yi"
W03-1203,W02-0402,0,0.0183874,"ly extracted to form a summary directly (Kupiec et. al, (1995), Myaeng & Jang (1999), Jing et. al, (2000), Nomoto & Matsumoto (2001,2002), Zha (2002), Osborne (2002)), or followed by a synthesis stage to generate a more natural summary (McKeown & Radev (1999), Hovy & Lin (1999)). Summarization therefore involves some theme or topic identification and then extraction of salient segments in a document. Story segmentation, document and sentence and classification can often be accomplished by unsupervised, clustering methods, with little or no requirement of human labeled data (Deerwester (1991), White & Cardie (2002), Jing et. al (2000)). Unsupervised methods or hybrids of supervised and unsupervised methods for extractive summarization have been found to yield promising results that are either comparable or superior to supervised methods (Nomoto & Matsumoto (2001,2002)). In these works, vector space models are used and document or sentence vectors are clustered together according to some similarity measure (Deerwester (1991), Dagan et al. (1997)). The disadvantage of clustering methods lies in their ad hoc nature. Since sentence vectors are considered to be independent sample points, the sentence order i"
W03-1203,C96-2166,0,0.0810864,"class-sentence pairs. We can then extract salient sentences from each class to be included in a summary, or for question-answering. To evaluate the effectiveness of our method as a foundation for extractive summarization, we extract sentences from each theme class in each document using four features, namely: (1) the position of the sentence p = 1 -- the further it is n from the title, the less important it is supposed to be; (2) the cosine similarity of the sentence with the centroid of its class ψ1; (3) its similarity with the first sentence in the article ψ2; and (4) the so-called Z model (Zechner (1996), Nomoto & Matsumoto (2000)), where the mass of a sentence is computed as the sum of tf, idf values of index terms in that sentence and the center of mass is chosen as the salient sentence to be included in a summary. z = arg max s At each estimation step of the training process, the λ for the Poisson distribution is estimated from the centroid of each theme cluster. 1 1 Strictly speaking, we ought to re-estimate the IDF in the k-mixture during each iteration by using the re-estimated clusters from the kmeans step as the documents. However, we simplify the process by using the pre-computed IDF"
W03-1203,W02-0404,0,\N,Missing
W03-1203,W97-0704,0,\N,Missing
W03-1203,A88-1000,0,\N,Missing
W04-3208,W03-1004,0,0.158024,"el sentences from multilingual corpora. Some of them are described in detail in (Manning and Schűtze, 1999, Wu, 2001, Veronis 2001). The challenge of these tasks varies by the degree of parallel-ness of the input multilingual documents. Figure1. Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data. Recent work (Munteanu et al., 2004, Zhao and Vogel, 2002) on extracting parallel sentences from comparable data, and others on extracting paraphrasing sentences from monolingual corpora (Barzilay and Elhadad 2003) are based on the “find-topic-extract-sentence” principle which claims that parallel sentences only exist in document pairs with high similarity. They all use lexical information (e.g. word overlap, cosine similarity) to match documents first, before extracting sentences from these documents. However, the non-parallel corpora used so far in the previous work tend to be quite comparable. Zhao and Vogel (2002) used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with “roughly similar sentence order of content”. This corpus can be more accurately described as"
W04-3208,J93-2003,0,0.00707638,"of words in the two sentences that are translations of each other. The better our bilingual lexicon is, the more accurate the sentence similarity will be. In the following section, we discuss how to find new word translations. 5.4. EM lexical learning from matched sentence pairs This step updates the bilingual lexicon according to the intermediate results of parallel sentence extraction. New bilingual word pairs are learned from the extracted sentence pairs based on an EM learning method. We use the GIZA++ (Och and Ney, 2000) implementation of the IBM statistical translation lexicon Model 4 (Brown et al., 1993) for this purpose. Initial document matching This initial step is based on the same “find-topic-extract-sentence” principle as in earlier works. The aim of this step is to roughly match the Chinese-English documents pairs that have the same topic, in order to extract parallel sentences from 1 them. Similar to previous work, comparability is defined by cosine similarity between document vectors. http://www.cs.unt.edu/~rada/wpt/ This model is based on the conditional probability of a source word being generated by the target word in the other language, based on EM estimation from aligned sentenc"
W04-3208,W97-0119,1,0.710909,"Missing"
W04-3208,P98-1069,1,0.840551,"Missing"
W04-3208,N03-1015,0,0.0837481,"sentence, they must contain others as well. Based on this principle, we propose an effective Bootstrapping method to accomplish our task (Figure 1). We also apply the IBM Model 4 EM lexical learning to find unknown word translations from the extracted parallel sentences from our system. The IBM models are commonly used for word alignment in statistical MT systems. This EM method differs from some previous work, which used a seed-word lexicon to extract new word translations or word senses from comparable corpora (Rapp 1995, Fung & McKeown 1997, Grefenstette 1998, Fung and Lo 1998, Kikui 1999, Kaji 2003). 2. Bilingual Sentence Alignment There have been conflicting definitions of the term “comparable corpora” in the research community. In this paper, we contrast and analyze different bilingual corpora, ranging from the parallel, noisy parallel, comparable, to very-non-parallel corpora. A parallel corpus is a sentence-aligned corpus containing bilingual translations of the same document. The Hong Kong Laws Corpus is a parallel corpus with manually aligned sentences, and is used as a parallel sentence resource for statistical machine translation systems. There are 313,659 sentence pairs in Chine"
W04-3208,N04-1034,0,0.264679,"es for training and improving statistical machine translation and cross-lingual information retrieval systems. Various methods have been previously proposed to extract parallel sentences from multilingual corpora. Some of them are described in detail in (Manning and Schűtze, 1999, Wu, 2001, Veronis 2001). The challenge of these tasks varies by the degree of parallel-ness of the input multilingual documents. Figure1. Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data. Recent work (Munteanu et al., 2004, Zhao and Vogel, 2002) on extracting parallel sentences from comparable data, and others on extracting paraphrasing sentences from monolingual corpora (Barzilay and Elhadad 2003) are based on the “find-topic-extract-sentence” principle which claims that parallel sentences only exist in document pairs with high similarity. They all use lexical information (e.g. word overlap, cosine similarity) to match documents first, before extracting sentences from these documents. However, the non-parallel corpora used so far in the previous work tend to be quite comparable. Zhao and Vogel (2002) used a co"
W04-3208,P00-1056,0,0.0105025,"Missing"
W04-3208,P95-1050,0,0.0823963,"h claims that as long as two documents are found to contain one pair of parallel sentence, they must contain others as well. Based on this principle, we propose an effective Bootstrapping method to accomplish our task (Figure 1). We also apply the IBM Model 4 EM lexical learning to find unknown word translations from the extracted parallel sentences from our system. The IBM models are commonly used for word alignment in statistical MT systems. This EM method differs from some previous work, which used a seed-word lexicon to extract new word translations or word senses from comparable corpora (Rapp 1995, Fung & McKeown 1997, Grefenstette 1998, Fung and Lo 1998, Kikui 1999, Kaji 2003). 2. Bilingual Sentence Alignment There have been conflicting definitions of the term “comparable corpora” in the research community. In this paper, we contrast and analyze different bilingual corpora, ranging from the parallel, noisy parallel, comparable, to very-non-parallel corpora. A parallel corpus is a sentence-aligned corpus containing bilingual translations of the same document. The Hong Kong Laws Corpus is a parallel corpus with manually aligned sentences, and is used as a parallel sentence resource for"
W09-3105,W04-1013,0,0.00606991,"Missing"
W09-3105,P04-1075,0,0.0276601,"aries but cannot agree on borderline cases. Consequently, annotator agreement is low. Reference summary generation is a tedious and low efficiency task. On the other hand, supervised learning of extractive summarization requires a large amount of training data of reference summaries. To reduce the amount of human annotation effort and improve annotator agreement on the reference summaries, we propose that active learning (selective sampling) is one possible solution. Active learning has been applied to NLP tasks such as spoken language understanding (Tur et al., 2005), information extraction (Shen et al., 2004), and text classification (Lewis and Catlett, 1994; McCallum and Nigam, 1998; Tong and Koller, 2002). Different from supervised learning which needs the entire corpus with manual labeling result, active learning selects the most useful examples for labeling and requires manual labeling of training dataset to re-train model. In this paper, we suggest a framework of reference summary annotation with relatively high inter labeler agreement based on the rhetorical structure in presentation slides. Based on this framework, we further propose a certainty-based active learning method to alleviate the"
W14-3907,li-etal-2012-mandarin,1,0.84418,"Missing"
W14-3907,W14-3917,0,0.103569,"Missing"
W14-3907,W14-3909,0,0.0388692,"Missing"
W14-3907,W14-3915,0,0.0710818,"Missing"
W14-3907,D13-1084,0,0.207232,"Missing"
W14-3907,W14-3911,1,0.921109,"t exploiting. For instance, the NE lexicons might account for the best results in the NE class in both the Twitter data and the Surprise genre (see Table 4 last row for SPA-EN and second to last for SPAEN Surprise). Most systems showed considerable 67 F-measure 1 0.9 Baseline 0.894 0.892 0.888 0.838 0.8 0.7 0.6 (Jain and Bhat, 2014) (Lin et al., 2014) (Chittaranjan et al., 2014) (King et al., 2014) F-measure (a) MAN-EN Baseline Test1 0.4 Baseline Test2 0.3 0.196 0.2 0.152 0.118 0.1 (Chittaranjan et al., 2014) 0.417 0.360 0.338 0.260 (King et al., 2014) 0.095 0.048 0.044 (Jain and Bhat, 2014) (Elfardy et al., 2014) (Lin et al., 2014) F-measure (b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2 Baseline 1 0.952 0.962 (King et al., 2014) (Lin et al., 2014) 0.975 0.974 0.972 0.977 0.9 0.8 (Jain and Bhat, 2014) (Shrestha, 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (c) NEP-EN F-measure 1 0.9 Baseline 0.8 0.7 0.6 0.703 0.754 0.753 0.783 0.793 0.822 0.634 (Shrestha, 2014) (King et al., 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (Jain and Bhat, 2014) (Lin et al., 2014) (Bar and Dershowitz, 2014) (d) SPA-EN Figure 1: Prediction results on"
W14-3907,W14-3916,0,0.102327,"Missing"
W14-3907,W14-3910,0,0.0496265,"Missing"
W14-3907,C82-1023,0,0.500666,"olumbia.edu pascale@ece.ust.hk ayc2135@columbia.edu Abstract this direction. We define CS broadly as a communication act, whether spoken or written, where two or more languages are being used interchangeably. In its spoken form, CS has probably been around ever since different languages first came in contact. Linguists have studied this phenomenon since the mid 1900s. In contrast, the Natural Language Processing (NLP) community has only recently started to pay attention to CS, with the earliest work in this area dating back to Joshi’s theoretical work proposing an approach to parsing CS data (Joshi, 1982) based on the Matrix and Embedded language framework. With the wide-spread use of social media, CS is now being used more and more in written language and thus we are seeing an increase in published papers dealing with CS. We are specifically interested in intrasentential code switched phenomena. As a result of this task, we have successfully created the first set of annotated data for several language pairs with a coherent set of labels across the languages. As the shared task results show, CS poses new research questions that warrant new NLP approaches, and thus we expect to see a significan"
W14-3907,P11-2007,0,0.0996179,"Missing"
W14-3907,N13-1131,0,0.144834,"Missing"
W14-3907,W14-3912,0,0.116613,"Missing"
W14-3907,W15-3116,0,\N,Missing
W14-3907,E14-1001,0,\N,Missing
W14-3907,W15-2902,0,\N,Missing
W14-3907,N15-1109,0,\N,Missing
W14-3907,W15-5936,0,\N,Missing
W17-3006,W17-1101,0,0.0899404,"er Ji Ho Park and Pascale Fung Human Language Technology Center Department of Electronic and Computer Engineering Hong Kong University of Science and Technology jhpark@connect.ust.hk, pascale@ece.ust.hk Abstract deep learning-based models using ensemble gradient boost classifiers to perform multi-class classification on sexist and racist language. All approaches have been on one step. Many have addressed the difficulty of the definition of abusive language while annotating the data, because they are often subjective to individuals (Ross et al. 2016) and lack of context (Waseem and Hovy, 2016; Schmidt & Wiegand, 2017). This makes it harder for non-experts to annotate without having a certain amount of domain knowledge (Waseem, 2016). In this research, we aim to experiment a twostep approach of detecting abusive language first and then classifying into specific types and compare with a one-step approach of doing one multiclass classification on sexist and racist language. Moreover, we explore applying a convolutional neural network (CNN) to tackle the task of abusive language detection. We use three kinds of CNN models that use both character-level and word-level inputs to perform classification on differen"
W17-3006,N16-2013,0,0.548006,"uage Detection on Twitter Ji Ho Park and Pascale Fung Human Language Technology Center Department of Electronic and Computer Engineering Hong Kong University of Science and Technology jhpark@connect.ust.hk, pascale@ece.ust.hk Abstract deep learning-based models using ensemble gradient boost classifiers to perform multi-class classification on sexist and racist language. All approaches have been on one step. Many have addressed the difficulty of the definition of abusive language while annotating the data, because they are often subjective to individuals (Ross et al. 2016) and lack of context (Waseem and Hovy, 2016; Schmidt & Wiegand, 2017). This makes it harder for non-experts to annotate without having a certain amount of domain knowledge (Waseem, 2016). In this research, we aim to experiment a twostep approach of detecting abusive language first and then classifying into specific types and compare with a one-step approach of doing one multiclass classification on sexist and racist language. Moreover, we explore applying a convolutional neural network (CNN) to tackle the task of abusive language detection. We use three kinds of CNN models that use both character-level and word-level inputs to perform"
W18-3207,P13-2037,0,0.238927,". We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity (Press and Wolf, 2017). Learning multiple NL"
W18-3207,W17-4419,0,0.0427497,"去 check. (I want to go) check. (2) 我 不 懂 要 怎么 讲 一 个 小时 seriously I didn’t have so much things to say (I don’t understand how to speak for an hour) seriously I didn’t have so much things to say 62 Proceedings of The Third Workshop on Computational Approaches to Code-Switching, pages 62–67 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics mains (Collobert et al., 2011; Luong et al., 2016; Hashimoto et al., 2016). They presented a joint many-task model to handle multiple NLP tasks and share parameters with growing depth in a single end-to-end model. A work by Aguilar et al. (2017) showed the potential of combining POS tagging with Named-Entity Recognition task. Language modeling using only word lexicons is not adequate to learn the complexity of codeswitching patterns, especially in a low resource setting. Learning at the same time syntactic features such as POS tag and language identifier allows to have a shared grammatical information that constraint the next word prediction. Due to this reason, we propose a multi-task learning framework for code-switching language modeling task which is able to leverage syntactic features such as language and POS tag. The main contr"
W18-3207,N16-1024,0,0.0293915,"012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity (Press and Wolf, 2017). Learning multiple NLP tasks using multi-task learning have been recently used in many do3.2 Model Description Figure 1 illustrates our multi-task learning extension to recurrent language model. In this multitask learning setting, the tasks are language modeling and POS tagging. The POS tagging task shares the POS tag vector and the hidden states to LM task, but it doe"
W18-3207,E17-2025,0,0.0424242,"icial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity (Press and Wolf, 2017). Learning multiple NLP tasks using multi-task learning have been recently used in many do3.2 Model Description Figure 1 illustrates our multi-task learning extension to recurrent language model. In this multitask learning setting, the tasks are language modeling and POS tagging. The POS tagging task shares the POS tag vector and the hidden states to LM task, but it does not receive any information from the other loss. Let wt be the word lexicon in the document and pt be the POS tag of the corresponding wt at index t. They are mapped into embedding matrices to get their d-dimensional vector p"
W18-3207,N03-1033,0,0.0452277,"sity, 2015). 2 3 Methodology This section shows how to build the features and how to train our multi-task learning language model. Multi-task learning consists of two NLP tasks: Language modeling and POS sequence tagging. 3.1 Feature Representation In the model, word lexicons and syntactic features are used as input. Word Lexicons: Sentences are encoded as 1hot vectors and our vocabulary is built from training data. Syntactic Features: For each language island, phrase within the same language, we extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolat"
W18-3207,C12-1102,1,0.875453,"extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dye"
W18-3207,I05-3005,0,0.0144798,"Technological University, 2015). 2 3 Methodology This section shows how to build the features and how to train our multi-task learning language model. Multi-task learning consists of two NLP tasks: Language modeling and POS sequence tagging. 3.1 Feature Representation In the model, word lexicons and syntactic features are used as input. Word Lexicons: Sentences are encoded as 1hot vectors and our vocabulary is built from training data. Syntactic Features: For each language island, phrase within the same language, we extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which"
W18-3207,P14-5010,0,0.00643434,"INEN 4,801 CDZH 20,158 VBEN 4,703 Ltotal = pLlm + (1 − p)Lpt where p is the weight of the loss in the training. 3.3 Experimental Setup In this section, we present the experimental setting for this task Corpus: SEAME (South East Asia MandarinEnglish), a conversational Mandarin-English code-switching speech corpus consists of spontaneously spoken interviews and conversations (Nanyang Technological University, 2015). Our dataset (LDC2015S04) is the most updated version of the Linguistic Data Consortium (LDC) Preprocessing: First, we tokenized English and Chinese word using Stanford NLP toolkit (Manning et al., 2014). Second, all hesitations and punctuations were removed except apostrophe, for examples: “let’s” and “it’s”. Table 1 and Table 2 show the statistics of SEAME Phase I and II corpora. Table 3 shows the most common trigger POS tag for Phase II corpus. 64 Training: The baseline model was trained using RNNLM (Mikolov et al., 2011)1 . Then, we trained our LSTM models with different hidden sizes [200, 500]. All LSTMs have 2 layers and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size. A dropout regularization (Srivastava et al., 2014) was applied to the word embedding vector"
W18-3207,D14-1098,1,0.862172,"eratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) propos"
W18-3214,W17-4419,0,0.0775262,"al Network (CNN) was used in NER task as word decoder by Collobert et al. (2011) and a few years later, Huang et al. (2015) introduced Bidirectional Long-Short Term Memory (BiLSTM) (Sundermeyer et al., 2012). Character-level features were explored by using neural architecture and replaced hand-crafted features (Dyer et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Limsopatham and Collier, 2016). Lample et al. (2016) also showed Conditional Random Field (CRF) (Lafferty et al., 2001) decoders to improve the results and used Stack memory-based LSTMs for their work in sequence chunking. Aguilar et al. (2017) proposed multi-task learning by combining Part-of-Speech tagging task with NER and using gazetteers to provide language-specific knowledge. Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER (Lample et al., 2016), POS tagging, and language modeling (Ling et al., 2015). Introduction Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence. Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen; Chiu and Nichols, 20"
W18-3214,W17-2630,0,0.0131081,"ning by combining Part-of-Speech tagging task with NER and using gazetteers to provide language-specific knowledge. Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER (Lample et al., 2016), POS tagging, and language modeling (Ling et al., 2015). Introduction Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence. Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen; Chiu and Nichols, 2016; Lample et al., 2016; Shen et al., 2017) using monolingual corpus. However, learning from code-switching tweets data is very challenging due to several reasons: (1) words may have different semantics in different context and language, for instance, the word “cola” can be associated with product or “queue” in Spanish (2) data from social media are noisy, with many inconsistencies such as spelling mistakes, repetitions, and informalities which eventually points to Out-ofVocabulary (OOV) words issue (3) entities may appear in different language other than the matrix language. For example “todos los Domingos en Westland Mall” where “Wes"
W18-3214,Q16-1026,0,0.0689048,"Missing"
W18-3214,P15-1033,0,0.0535152,"Missing"
W18-3214,L18-1550,0,0.0684472,"Missing"
W18-3214,N16-1030,0,0.0318593,"level features used in our model. Word Representation: Words are encoded into continuous representation. The vocabulary is built from training data. The Twitter data are very noisy, there are many spelling mistakes, irregular ways to use a word and repeating characters. 111 3.3 Model Description output In this section, we describe our model architecture and hyper-parameters setting. Bilingual Char-RNN: This is one of the approaches to learn character-level embeddings without needing of any lexical hand-crafted features. We use an RNN for representing the word with character-level information (Lample et al., 2016). Figure 1 shows the model architecture. The inputs are characters extracted from a word and every character is embedded with d dimension vector. Then, we use it as the input for a Bidirectional LSTM as character encoder, wherein every time step, a character is input to the network. Consider at as the hidden states for word t. BiLSTM decoder a7 a6 a5 a4 a3 a2 a1 a1 a2 a3 a4 a5 a6 a7 a8 b a I-LOC c1 c2 c3 c4 c5 c6 h6 h5 h4 h3 h2 h1 h1 h2 h3 h4 h5 h6 Char lang2 Bilingual Char-RNN Figure 2: Main architecture → − ← − ct = ht ⊕ ht where ⊕ denotes the concatenation operator. Dropout is applied to th"
W18-3214,W16-3920,0,0.0567317,"Missing"
W18-3214,D15-1176,0,0.0345107,"yer et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Limsopatham and Collier, 2016). Lample et al. (2016) also showed Conditional Random Field (CRF) (Lafferty et al., 2001) decoders to improve the results and used Stack memory-based LSTMs for their work in sequence chunking. Aguilar et al. (2017) proposed multi-task learning by combining Part-of-Speech tagging task with NER and using gazetteers to provide language-specific knowledge. Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER (Lample et al., 2016), POS tagging, and language modeling (Ling et al., 2015). Introduction Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence. Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen; Chiu and Nichols, 2016; Lample et al., 2016; Shen et al., 2017) using monolingual corpus. However, learning from code-switching tweets data is very challenging due to several reasons: (1) words may have different semantics in different context and language, for instance, the word “cola” can be associated with product or “queue” i"
W18-6243,P15-1166,0,0.036932,"rious methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we propose Emo2Vec to represent emotion with vectors using a multi-task training framework. Six affect-related tasks are utilized, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. We empirically show how Emo2Vec leverages multi-task training to learn a generalized emotion representation. In addition, Emo2Vec outperforms existing affect-related embeddings on more than te"
W18-6243,D17-1268,0,0.0190674,"eveloped for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we propose Emo2Vec to represent emotion with vectors using a multi-task training framework. Six affect-related tasks are utilized, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. We empirically show how Emo2Vec leverages multi-task training to learn a generalized emotion representation. In addition, Emo2Vec outperforms existing affect-related embeddings on more than ten different datasets. By"
W18-6243,D17-1054,0,0.0408305,", more attention needs to be paid to words. ing (Peng et al., 2017). Hashimoto et al. (2017) jointly learns POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment by considering linguistic hierarchy and achieves state-of-the-results on five datasets. For sentiment analysis, Balikas et al. (2017) jointly trains ternary and fine-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et"
W18-6243,P17-1186,0,0.0314113,"Missing"
W18-6243,uryupina-etal-2014-sentube,0,0.0275266,"ed is illustrated in Figure 2. Firstly, 1-D convolution is used to extract n293 personality recognition are included. The reason why we include many datasets is to 1) leverage different aspects of words emotion knowledge, which may not be present in single domain dataset; 2) create a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Suppleme"
W18-6243,D17-1052,0,0.0637837,"-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we prop"
W18-6243,D14-1162,0,0.0828235,"sive language classification, insult detection, and personality recognition. Our evaluation of Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with GloVe, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using a simple logistic regression classifier. 1 Introduction Recent work on word representation has been focusing on embedding syntactic and semantic information into fixed-sized vectors (Mikolov et al., 2013; Pennington et al., 2014) based on the distributional hypothesis, and have proven to be useful in many natural language tasks (Collobert et al., 2011). However, despite the rising popularity regarding the use of word embeddings, they often fail to capture the emotional semantics the words convey. For example, the GloVe vector captures the semantic meaning of “headache”, as it is closer to words of ill symptoms like “fever” and “toothache”, but misses the emotional association that the word carries. The word “headache” in the sentence “You are giving me a headache” does not really mean that the speaker will get a heada"
W18-6243,D16-1058,0,0.0571363,"Missing"
W18-6243,W16-5618,0,0.0119463,"ts of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone. Parameters of T and CNN are randomly initialized and Adam is used for optimization. Best parameter settings are tuned on the validation set. For the best model,"
W18-6243,W13-1603,0,0.02274,"nt in single domain dataset; 2) create a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model a"
W18-6243,N16-2013,0,0.0172717,"stic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone. Parameters of T and CNN are randomly initialized and Adam is used for optimization. Best parameter settings are tuned on the validation set. For the best model, we use the batch size o"
W18-6243,D13-1170,0,0.00939142,"ation Baselines: We use 50-dimension Sentimentspecific Word Embedding (SSWE) (Tang et al., 2016) as our baseline, which is an embedding model trained with 10 millions of tweets by encoding both semantic and sentiment information into vectors. Also, lots of work about the detection/classification in sentiment analysis implicitly encodes emotion inside the word vectors. For example, Felbo et al. (2017) trains a two-layer bidirectional Long Short-Term Memory (bi-LSTM) model, named DeepMoji, to predict emoji of the Small datasets For sentiment, we include 8 datasets. (1,2) SSTfine and SST-binary (Socher et al., 2013) (3) OpeNER (Agerri et al., 2013) (4,5) tube auto 2 http://hci.epfl.ch/sharing-emotion-lexicons-anddata#emo-hash-data 294 model SSWE DeepMoji embedding CNN embedding Emo2Vec SS-T 0.815 0.788 0.803 0.801 SS-Y 0.835 0.841 0.862 0.859 SS-binary 0.698 0.751 0.734 0.812 SS-fine 0.365 0.369 0.369 0.416 OpeNER 0.701 0.754 0.713 0.744 tube auto 0.620 0.628 0.605 0.629 tube tablet 0.654 0.675 0.667 0.688 SemEval 0.629 0.676 0.622 0.638 average 0.665 0.685 0.672 0.698 Table 1: Comparison between different emotion representations on sentiment datasets, all results are reported with accuracy. The best res"
W18-6243,P14-2070,0,0.0448146,"ate a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alo"
W18-6243,D17-1056,0,0.0363301,"by Multi-task Training Peng Xu, Andrea Madotto, Chien-Sheng Wu, Ji Ho Park and Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology, Clear Water Bay [pxuab,eeandreamad,cwuak,jhpark,pascale]@ust.hk Abstract and syntactic contextual information in a vector space. This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment-related tasks compared to other word embeddings. However, they only focus on binary labels, which weakens their generalization ability on other affect tasks. Yu et al. (2017) instead uses emotion lexicons to tune the vector space, which gives them better results. Nevertheless, this method requires human-labeled lexicons and cannot scale to large amounts of data. Felbo et al. (2017) achieves good results on affect tasks by training a two-layer bidirectional Long Short-Term Memory (bi-LSTM) model, named DeepMoji, to predict emoji of the input document using a huge dataset of 1.2 billions of tweets. However, collecting billions of tweets is expensive and time consuming for researchers. Furthermore, most works in sentiment and emotion analysis have focused solely on a"
W18-6243,D17-1115,0,0.0312324,"017). Hashimoto et al. (2017) jointly learns POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment by considering linguistic hierarchy and achieves state-of-the-results on five datasets. For sentiment analysis, Balikas et al. (2017) jointly trains ternary and fine-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which"
W19-4320,D18-1306,0,0.0129436,"e weights equally. Related Work Early studies on named entity recognition heavily relied on language-specific knowledge resources, such as hand-crafted features or gazetteers (Lafferty et al., 2001; Ratinov and Roth, 2009; Tsai et al., 2016). However, this approach was costly for new languages and domains. Thus, end-toend approaches that do not rely on any external knowledge were proposed. Sobhana et al. (2010) proposed to use a CRF without any external resources, to leverage the label dependencies. Then, neural-based approaches, such as LSTM with a CRF (Lample et al., 2016; Lin et al., 2017; Greenberg et al., 2018) and LSTM with a CNN (Chiu and Nichols, 2016) showed a significant improvement in performance. Liu et al. (2018); Trivedi et al. (2018) proposed a character-level LSTM to capture the underlying style and structure, such as word boundaries and spellings. Finally, wordembedding ensemble techniques and preprocessing techniques, such as tokenization and normalIn the cross-lingual setting, our model does not perform well when we only use one auxiliary language, as seen in Table 2. A significant improvement is shown after we combine both languages, and MME shows a similar performance to the previous"
W19-4320,D18-1176,0,0.109267,"oduction Learning a representation through embedding is a fundamental technique to capture latent word semantics (Clark, 2015). Practically, word-level representation has been extensively explored to improve many downstream natural language processing (NLP) tasks (Mikolov et al., 2013; Pennington et al., 2014; Grave et al., 2018). A new wave of ""meta-embeddings"" research aims to learn how to effectively combine pre-trained word embeddings in supervised training into a single dense representation (Yin and Schütze, 2016; Muromägi et al., 2017; Bollegala et al., 2018; Coates and Bollegala, 2018; Kiela et al., 2018). This method is known to be effective to overcome domain and modality limitations. However, the generalization ability of previous works has been limited to monolingual tasks, so we aim to extend the method to multilingual contexts which benefits the processing of code-switching text. In multilingual societies, speakers tend to move back and forth from one language to another during the same conversation, which is commonly 181 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 181–186 c Florence, Italy, August 2, 2019. 2019 Association for Computational"
W19-4320,W18-3219,0,0.122111,"Missing"
W19-4320,N16-1030,0,0.0209524,"ear ensemble approaches always score the weights equally. Related Work Early studies on named entity recognition heavily relied on language-specific knowledge resources, such as hand-crafted features or gazetteers (Lafferty et al., 2001; Ratinov and Roth, 2009; Tsai et al., 2016). However, this approach was costly for new languages and domains. Thus, end-toend approaches that do not rely on any external knowledge were proposed. Sobhana et al. (2010) proposed to use a CRF without any external resources, to leverage the label dependencies. Then, neural-based approaches, such as LSTM with a CRF (Lample et al., 2016; Lin et al., 2017; Greenberg et al., 2018) and LSTM with a CNN (Chiu and Nichols, 2016) showed a significant improvement in performance. Liu et al. (2018); Trivedi et al. (2018) proposed a character-level LSTM to capture the underlying style and structure, such as word boundaries and spellings. Finally, wordembedding ensemble techniques and preprocessing techniques, such as tokenization and normalIn the cross-lingual setting, our model does not perform well when we only use one auxiliary language, as seen in Table 2. A significant improvement is shown after we combine both languages, and MME"
W19-4320,W14-3902,0,0.0585118,"primary languages ′ w i,2 E2 ′ w i,3 E3 ... ... Input ′ w i,n En auxiliary languages Figure 1: Multilingual Meta-Embeddings. The inputs are word embeddings and the output is a single word representation. called “code-switching"". Code-Switching is produced in both written text and speech in a discourse. Recent studies in code-switching has been mainly focused on natural language tasks, such as language modeling (Winata et al., 2018a; Pratapa et al., 2018; Garg et al., 2018), named entity recognition (Aguilar et al., 2018), and language identification (Solorio et al., 2014; Molina et al., 2016; Barman et al., 2014). Code-Switching is considered as a challenging task because words from different languages may co-exist within a sequence, and models are required to recognize the context of mixed-language sentences. Meanwhile, some words with the same spelling may have entirely different meanings (e.g., cola in English and Spanish) (Winata et al., 2018b). Language identifiers were commonly used to solve the word ambiguity issue in mixed-language sentences. However, it may not reliably cover all code-switching cases, and it creates a bottleneck that would require large-scale crowdsourcing to annotate languag"
W19-4320,W17-4421,0,0.0192818,"es always score the weights equally. Related Work Early studies on named entity recognition heavily relied on language-specific knowledge resources, such as hand-crafted features or gazetteers (Lafferty et al., 2001; Ratinov and Roth, 2009; Tsai et al., 2016). However, this approach was costly for new languages and domains. Thus, end-toend approaches that do not rely on any external knowledge were proposed. Sobhana et al. (2010) proposed to use a CRF without any external resources, to leverage the label dependencies. Then, neural-based approaches, such as LSTM with a CRF (Lample et al., 2016; Lin et al., 2017; Greenberg et al., 2018) and LSTM with a CNN (Chiu and Nichols, 2016) showed a significant improvement in performance. Liu et al. (2018); Trivedi et al. (2018) proposed a character-level LSTM to capture the underlying style and structure, such as word boundaries and spellings. Finally, wordembedding ensemble techniques and preprocessing techniques, such as tokenization and normalIn the cross-lingual setting, our model does not perform well when we only use one auxiliary language, as seen in Table 2. A significant improvement is shown after we combine both languages, and MME shows a similar pe"
W19-4320,W16-5805,0,0.0720133,"e task. 1 ′ w i,1 E1 primary languages ′ w i,2 E2 ′ w i,3 E3 ... ... Input ′ w i,n En auxiliary languages Figure 1: Multilingual Meta-Embeddings. The inputs are word embeddings and the output is a single word representation. called “code-switching"". Code-Switching is produced in both written text and speech in a discourse. Recent studies in code-switching has been mainly focused on natural language tasks, such as language modeling (Winata et al., 2018a; Pratapa et al., 2018; Garg et al., 2018), named entity recognition (Aguilar et al., 2018), and language identification (Solorio et al., 2014; Molina et al., 2016; Barman et al., 2014). Code-Switching is considered as a challenging task because words from different languages may co-exist within a sequence, and models are required to recognize the context of mixed-language sentences. Meanwhile, some words with the same spelling may have entirely different meanings (e.g., cola in English and Spanish) (Winata et al., 2018b). Language identifiers were commonly used to solve the word ambiguity issue in mixed-language sentences. However, it may not reliably cover all code-switching cases, and it creates a bottleneck that would require large-scale crowdsourci"
W19-4320,W18-3221,0,0.461006,". , wm )Wt + Wp , (6) hl = Transformer_blocks(h0 ), Baselines o = hl Wo + bo , We compare our method to two baselines: (1) concatenation and (2) linear ensembles. (7) (8) where Wt is the projection matrix, Wp is the positional encoding matrix, Wo is the output layer, h0 is the first layer hidden states, and hl is the output representation from the final transformer layer. The output of the final layer is logits o. Concatenation We concatenate word embeddings by merging the dimensions of word representations. This is the simplest way to utilize all 182 Approaches Trivedi et al. (2018) (Single) Wang et al. (2018) (Single) Wang et al. (2018) (Ensemble) Winata et al. (2018b) (Single) Trivedi et al. (2018) (Ensemble) MONOLINGUAL EN ES CONCAT EN + ES EN + ES + CA EN + ES + PT EN + ES + CA + PT LINEAR EN + ES + CA + PT (Single) EN + ES + CA + PT (Ensemble) MME EN + ES EN + ES + CA EN + ES + PT EN + ES + CA + PT (Single) EN + ES + CA + PT (Ensemble) Conditional Random Field This model calculates the dependencies across tag labels. NER requires a stronger constraint where I-PERSON should follow only after B-PERSON. We use CRF to learn the correlations between the current label and its neighbors (Lafferty et"
W19-4320,W18-3207,1,0.378537,"l results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task. 1 ′ w i,1 E1 primary languages ′ w i,2 E2 ′ w i,3 E3 ... ... Input ′ w i,n En auxiliary languages Figure 1: Multilingual Meta-Embeddings. The inputs are word embeddings and the output is a single word representation. called “code-switching"". Code-Switching is produced in both written text and speech in a discourse. Recent studies in code-switching has been mainly focused on natural language tasks, such as language modeling (Winata et al., 2018a; Pratapa et al., 2018; Garg et al., 2018), named entity recognition (Aguilar et al., 2018), and language identification (Solorio et al., 2014; Molina et al., 2016; Barman et al., 2014). Code-Switching is considered as a challenging task because words from different languages may co-exist within a sequence, and models are required to recognize the context of mixed-language sentences. Meanwhile, some words with the same spelling may have entirely different meanings (e.g., cola in English and Spanish) (Winata et al., 2018b). Language identifiers were commonly used to solve the word ambiguity is"
W19-4320,W17-0212,0,0.0524542,"nguages. Our approach can be seen as a method to create a universal mulIntroduction Learning a representation through embedding is a fundamental technique to capture latent word semantics (Clark, 2015). Practically, word-level representation has been extensively explored to improve many downstream natural language processing (NLP) tasks (Mikolov et al., 2013; Pennington et al., 2014; Grave et al., 2018). A new wave of ""meta-embeddings"" research aims to learn how to effectively combine pre-trained word embeddings in supervised training into a single dense representation (Yin and Schütze, 2016; Muromägi et al., 2017; Bollegala et al., 2018; Coates and Bollegala, 2018; Kiela et al., 2018). This method is known to be effective to overcome domain and modality limitations. However, the generalization ability of previous works has been limited to monolingual tasks, so we aim to extend the method to multilingual contexts which benefits the processing of code-switching text. In multilingual societies, speakers tend to move back and forth from one language to another during the same conversation, which is commonly 181 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 181–1"
W19-4320,W18-3214,1,0.259499,"l results show that our proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task. 1 ′ w i,1 E1 primary languages ′ w i,2 E2 ′ w i,3 E3 ... ... Input ′ w i,n En auxiliary languages Figure 1: Multilingual Meta-Embeddings. The inputs are word embeddings and the output is a single word representation. called “code-switching"". Code-Switching is produced in both written text and speech in a discourse. Recent studies in code-switching has been mainly focused on natural language tasks, such as language modeling (Winata et al., 2018a; Pratapa et al., 2018; Garg et al., 2018), named entity recognition (Aguilar et al., 2018), and language identification (Solorio et al., 2014; Molina et al., 2016; Barman et al., 2014). Code-Switching is considered as a challenging task because words from different languages may co-exist within a sequence, and models are required to recognize the context of mixed-language sentences. Meanwhile, some words with the same spelling may have entirely different meanings (e.g., cola in English and Spanish) (Winata et al., 2018b). Language identifiers were commonly used to solve the word ambiguity is"
W19-4320,D14-1162,0,0.0870442,"bottleneck that would require large-scale crowdsourcing to annotate language identifiers in code-switching data correctly. To overcome the code-switching problem, we introduce a multilingual meta-embedding model learned from different languages. Our approach can be seen as a method to create a universal mulIntroduction Learning a representation through embedding is a fundamental technique to capture latent word semantics (Clark, 2015). Practically, word-level representation has been extensively explored to improve many downstream natural language processing (NLP) tasks (Mikolov et al., 2013; Pennington et al., 2014; Grave et al., 2018). A new wave of ""meta-embeddings"" research aims to learn how to effectively combine pre-trained word embeddings in supervised training into a single dense representation (Yin and Schütze, 2016; Muromägi et al., 2017; Bollegala et al., 2018; Coates and Bollegala, 2018; Kiela et al., 2018). This method is known to be effective to overcome domain and modality limitations. However, the generalization ability of previous works has been limited to monolingual tasks, so we aim to extend the method to multilingual contexts which benefits the processing of code-switching text. In m"
W19-4320,P18-1143,0,0.112781,"r proposed method achieves state-of-the-art performance on the multilingual setting, and it has the ability to generalize to an unseen language task. 1 ′ w i,1 E1 primary languages ′ w i,2 E2 ′ w i,3 E3 ... ... Input ′ w i,n En auxiliary languages Figure 1: Multilingual Meta-Embeddings. The inputs are word embeddings and the output is a single word representation. called “code-switching"". Code-Switching is produced in both written text and speech in a discourse. Recent studies in code-switching has been mainly focused on natural language tasks, such as language modeling (Winata et al., 2018a; Pratapa et al., 2018; Garg et al., 2018), named entity recognition (Aguilar et al., 2018), and language identification (Solorio et al., 2014; Molina et al., 2016; Barman et al., 2014). Code-Switching is considered as a challenging task because words from different languages may co-exist within a sequence, and models are required to recognize the context of mixed-language sentences. Meanwhile, some words with the same spelling may have entirely different meanings (e.g., cola in English and Spanish) (Winata et al., 2018b). Language identifiers were commonly used to solve the word ambiguity issue in mixed-language s"
W19-4320,P16-1128,0,0.0300368,"arned from different languages. Our approach can be seen as a method to create a universal mulIntroduction Learning a representation through embedding is a fundamental technique to capture latent word semantics (Clark, 2015). Practically, word-level representation has been extensively explored to improve many downstream natural language processing (NLP) tasks (Mikolov et al., 2013; Pennington et al., 2014; Grave et al., 2018). A new wave of ""meta-embeddings"" research aims to learn how to effectively combine pre-trained word embeddings in supervised training into a single dense representation (Yin and Schütze, 2016; Muromägi et al., 2017; Bollegala et al., 2018; Coates and Bollegala, 2018; Kiela et al., 2018). This method is known to be effective to overcome domain and modality limitations. However, the generalization ability of previous works has been limited to monolingual tasks, so we aim to extend the method to multilingual contexts which benefits the processing of code-switching text. In multilingual societies, speakers tend to move back and forth from one language to another during the same conversation, which is commonly 181 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL"
W19-4320,W09-1119,0,0.0166031,"elines on different language combinations, which shows its effectiveness. The results also show that the two baselines cannot effectively exploit the information from auxiliary languages. Here we note that the main advantage of MME is that it dynamically weights the different language pre-trained embeddings for each input token, while the concatenation and linear ensemble approaches always score the weights equally. Related Work Early studies on named entity recognition heavily relied on language-specific knowledge resources, such as hand-crafted features or gazetteers (Lafferty et al., 2001; Ratinov and Roth, 2009; Tsai et al., 2016). However, this approach was costly for new languages and domains. Thus, end-toend approaches that do not rely on any external knowledge were proposed. Sobhana et al. (2010) proposed to use a CRF without any external resources, to leverage the label dependencies. Then, neural-based approaches, such as LSTM with a CRF (Lample et al., 2016; Lin et al., 2017; Greenberg et al., 2018) and LSTM with a CNN (Chiu and Nichols, 2016) showed a significant improvement in performance. Liu et al. (2018); Trivedi et al. (2018) proposed a character-level LSTM to capture the underlying styl"
W19-4320,W18-3220,0,0.355851,"der: h0 = Concat(w0 , w1 , . . . , wm )Wt + Wp , (6) hl = Transformer_blocks(h0 ), Baselines o = hl Wo + bo , We compare our method to two baselines: (1) concatenation and (2) linear ensembles. (7) (8) where Wt is the projection matrix, Wp is the positional encoding matrix, Wo is the output layer, h0 is the first layer hidden states, and hl is the output representation from the final transformer layer. The output of the final layer is logits o. Concatenation We concatenate word embeddings by merging the dimensions of word representations. This is the simplest way to utilize all 182 Approaches Trivedi et al. (2018) (Single) Wang et al. (2018) (Single) Wang et al. (2018) (Ensemble) Winata et al. (2018b) (Single) Trivedi et al. (2018) (Ensemble) MONOLINGUAL EN ES CONCAT EN + ES EN + ES + CA EN + ES + PT EN + ES + CA + PT LINEAR EN + ES + CA + PT (Single) EN + ES + CA + PT (Ensemble) MME EN + ES EN + ES + CA EN + ES + PT EN + ES + CA + PT (Single) EN + ES + CA + PT (Ensemble) Conditional Random Field This model calculates the dependencies across tag labels. NER requires a stronger constraint where I-PERSON should follow only after B-PERSON. We use CRF to learn the correlations between the current label and"
W19-4320,K16-1022,0,0.025639,"uage combinations, which shows its effectiveness. The results also show that the two baselines cannot effectively exploit the information from auxiliary languages. Here we note that the main advantage of MME is that it dynamically weights the different language pre-trained embeddings for each input token, while the concatenation and linear ensemble approaches always score the weights equally. Related Work Early studies on named entity recognition heavily relied on language-specific knowledge resources, such as hand-crafted features or gazetteers (Lafferty et al., 2001; Ratinov and Roth, 2009; Tsai et al., 2016). However, this approach was costly for new languages and domains. Thus, end-toend approaches that do not rely on any external knowledge were proposed. Sobhana et al. (2010) proposed to use a CRF without any external resources, to leverage the label dependencies. Then, neural-based approaches, such as LSTM with a CRF (Lample et al., 2016; Lin et al., 2017; Greenberg et al., 2018) and LSTM with a CNN (Chiu and Nichols, 2016) showed a significant improvement in performance. Liu et al. (2018); Trivedi et al. (2018) proposed a character-level LSTM to capture the underlying style and structure, suc"
W19-4331,P18-2096,1,0.924963,"y machine learning tasks where information is available from multiple source modalities, typically achieving better predictions through integration of information from different modalities. Multimodal integration can handle missing data from one or more modalities. Since some modalities can include noise, it can also lead to more robust prediction. Moreover, since some information may not be visible in some modalities or a single modality may not be powerful enough for a specific task, considering multiple modalities often improves performance (Potamianos et al., 2003; Soleymani et al., 2012; Kampman et al., 2018). For example, humans assign personality traits to each other, as well as to virtual characters by inferring personality from diverse cues, both behavioral and verbal, suggesting that a model to predict personality should take into account multiple modalities such as language, speech, and visual cues. Our method, Modality-based Redundancy Reduction multimodal Fusion (MRRF), builds on recent work in mutimodal fusion utilizing first an outer product tensor of input modalities to better capture inter-modality dependencies (Zadeh et al., 2017) and a recent approach to reduce the number of elements"
W19-4331,P18-1209,0,0.222955,"ther, as well as to virtual characters by inferring personality from diverse cues, both behavioral and verbal, suggesting that a model to predict personality should take into account multiple modalities such as language, speech, and visual cues. Our method, Modality-based Redundancy Reduction multimodal Fusion (MRRF), builds on recent work in mutimodal fusion utilizing first an outer product tensor of input modalities to better capture inter-modality dependencies (Zadeh et al., 2017) and a recent approach to reduce the number of elements in the resulting tensor through low rank factorization (Liu et al., 2018). Whereas the factorization used in (Liu et al., 2018) utilizes a single compression rate across all modalities, we instead use Tuckers tensor decomposition (see the Methodology section), which allows different compression rates for each modality. This allows the model to adapt to variations in the amount of useful information between modalities. Modality-specific factors are chosen by maximizing performance on a validation set. Applying a modality-based factorization method results in removing redundant information duplicated across modalities and leading to fewer parameters with minimal info"
W19-4331,P17-1081,0,0.0343976,"how that their proposed method improves the accuracy by considering both inter-modality and intra-modality relations. However, the generated representation has a very large dimension which leads to a very large hidden layer and therefore a huge number of parameters. Notation The operator ⊗ is the outer product operator where z1 ⊗ . . . ⊗ zM for zi ∈ Rdi leads to a M-way tensor in Rd1 ×...×dM . The operator ×k , for a given k, is k-mode product of a tensor R ∈ Rr1 ×r2 ×...×rM and a matrix W ∈ Rdk ×rk as W ×k R, which results in a tensor ¯ ∈ Rr1 ×...×rk−1 ×dk ×rk+1 ×...×rM . R 2 The authors of (Poria et al., 2017a,b; Zadeh et al., 2018a,b) introduce attention mechanisms utilizing the contextual information available from the utterances for each speaker. They require additional information like the identity of the speaker, the sequence of the utterance-sentiments while integrating the multimodal data. Since these methods, despite our proposed method, need additional information might not be available in the general scenario, we do not include them in our experiments. Related Work Multimodal Fusion: Multimodal fusion (Ngiam et al., 2011) has a very broad range of applications, including audio-visual spe"
W19-4331,N16-1020,0,0.0309054,"ated work. We elaborate on the details of our proposed method in Methodology section. In the following section we go on to describe our experimental setup. In the Results section, we compare the performance of MRRF and state-of-the-art baselines on three different datasets and discuss the effect of compression rate on each modality. Finally, we provide a brief conclusion of the approach and the results. Supplementary materials describe the methodology in greater detail. combines the decision for each modality (either classification, or regression), by voting (Morvant et al., 2014), averaging (Shutova et al., 2016) or weighted sum of the outputs of the learned models (Glodek et al., 2011; Shutova et al., 2016). The hybrid approach combines the prediction by early fusion and unimodal predictions. It has been observed that early fusion (feature level fusion) concentrates on the inter-modality information rather than intra-modality information (Zadeh et al., 2017) due to the fact that intermodality information can be more complicated at the feature level and dominates the learning process. On the other hand, these fusion approaches are not powerful enough to extract the inter-modality integration model and"
W19-5327,P07-2045,0,0.00743541,"s have been shown very effective in many natural language processing tasks (Park et al., 2018; Winata et al., 2019). We apply an ensemble method by taking the top five translations from word-level and subword-level NMT, and rescore all translations using our pre-trained Czech language model mentioned in §2.3. Then, we select the best translation with the lowest perplexity. 3 3.1 Data Post-processing Quotes Fixing The quotes are fixed to keep them the same as the source sentences. Recaser For all the models mentioned above that work under a lower-case setting, a recaser implemented with Moses (Koehn et al., 2007) is applied to convert the translations to the real cases. Experiments Data Pre-processing Patch-up From our observation, the ensemble NMT model lacks the ability to translate name entities correctly. We find that words with capital We note that in the corpus, there are tokens representing quantity or date. Therefore, we delex278 Recaser We use the recaser model provided in Moses and train the model with the two million latest sentences in the Czech monolingual dataset. After the training procedure, the recaser can restore words to the form in which the maximum probability occurs. characters a"
W19-5327,P17-1042,0,0.336426,", 2018). However, the need for a large amount of manual parallel data obstructs its performance under low-resource conditions. Building an effective model on low resource data or even in an unsupervised way is always an interesting and challenging research topic (Gu et al., 2018; Radford et al., 2016; Lee et al., 2019). Recently, unsupervised MT (Artetxe et al., 2018b,a; Conneau et al., 2018; Lample et al., 2018b; Wu et al., 2019), which can immensely reduce the reliance on parallel corpora, has been gaining more and more interest. Training cross-lingual word embeddings (Conneau et al., 2018; Artetxe et al., 2017) is always the • We propose a method to combine word and subword (BPE) pre-trained input representations aligned using MUSE (Conneau et al., 2018) as an NMT training initialization on a morphologically-rich language pair such as German and Czech. • We study the effectiveness of language model *These two authors contributed equally. 275 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 275–282 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics S, u∗ (y) similarly denotes sentences in the source language"
W19-5327,D18-1399,0,0.0503739,"Missing"
W19-5327,J82-2005,0,0.783709,"Missing"
W19-5327,Q17-1010,0,0.0687881,"). To overcome OOV issues, we leverage subword information, which can lead to better performance. We employ subword units (Sennrich et al., 2016a) to tackle the morphological richness problem. There are two advantages of using the subword-level. First, we can alleviate the OOV issue by zeroing out the number of unknown words. Second, we can leverage the semantics of subword units from these languages. However, German and Czech are distant languages that originate from different roots, so they only share a small fraction of subword units. To tackle this problem, we train FastText word vectors (Bojanowski et al., 2017) separately for German and Czech, and apply MUSE (Conneau et al., 2018) to align these embeddings. Methodology In this section, we describe how we built our main unsupervised machine translation system, which is illustrated in Figure 1. 2.1 Unsupervised Machine Translation 2.1.1 Word-level Unsupervised NMT We follow the unsupervised NMT in Lample et al. (2018b) by leveraging initialization, language modeling and back-translation. However, instead of using BPE, we use MUSE (Conneau et al., 2018) to align word-level embeddings of German and Czech, which are trained by FastText (Bojanowski et al."
W19-5327,S19-2184,1,0.543217,"gh beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations. 1 Introduction Machine translation (MT) has achieved huge advances in the past few years (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017, 2018). However, the need for a large amount of manual parallel data obstructs its performance under low-resource conditions. Building an effective model on low resource data or even in an unsupervised way is always an interesting and challenging research topic (Gu et al., 2018; Radford et al., 2016; Lee et al., 2019). Recently, unsupervised MT (Artetxe et al., 2018b,a; Conneau et al., 2018; Lample et al., 2018b; Wu et al., 2019), which can immensely reduce the reliance on parallel corpora, has been gaining more and more interest. Training cross-lingual word embeddings (Conneau et al., 2018; Artetxe et al., 2017) is always the • We propose a method to combine word and subword (BPE) pre-trained input representations aligned using MUSE (Conneau et al., 2018) as an NMT training initialization on a morphologically-rich language pair such as German and Czech. • We study the effectiveness of language model *Thes"
W19-5327,N18-2085,0,0.0141531,"till boost its performance. However, this pseudo-parallel data from the PBSMT model can not improve the word-level NMT model since the large percentage of OOV words limits its performance. After applying unknown words replacement to the wordlevel NMT model, the performance improves by a BLEU score of around 2. Using the Czech language model to re-score helps the model improve by around a 0.3 BLEU score each time. We also use this language model to create an ensemble of the best word-level and subword-level NMT model and achieve the best performance. Language Model According to the findings in Cotterell et al. (2018), the morphological richness of a language is closely related to the performance of the model, which indicates that the language models will be extremely hard to train for Czech, as it is one of the most complex languages. We train the QRNN model with 12 million sentences randomly sampled from the original WMT Czech monolingual dataset, 2 which is also pre-processed in the way mentioned in §3.1. To maintain the quality of the language model, we enlarge the vocabulary size to three million by including all the words that appear more than 15 times. Finally, the PPL of the language model on the t"
W19-5327,P03-1021,0,0.0387154,"Cased TER BEER 2.0 CharacterTER 4 8.3 3.8 7.7 0.887 0.384 0.429 0.773 0.743 9.4 9.8 10.3 7.9 7.9 10.1 10.4 9.1 9.5 10 7.6 7.7 9.6 9.9 0.832 0.833 0.829 0.829 0.419 0.424 0.426 0.412 0.413 0.432 0.432 0.756 0.756 0.749 0.823 0.819 0.766 0.764 10.6 10.6 10.2 10.2 0.829 0.833 0.429 0.430 0.755 0.757 Table 2: Unsupervised translation results. We report the scores of several evaluation methods for every step of our approach. Except the result that is listed on the last line, all results are under the condition that the translations are post-processed without patch-up. 4 4.1 Related Work and MERT (Och, 2003) to iteratively refine the SMT model. Wu et al. (2019) proposed to discard back-translation. Instead, they extracted and edited the nearest sentences in the target language to construct pseudo-parallel data, which was used as a supervision signal. Unsupervised Cross-lingual Embeddings Cross-lingual word embeddings can provide a good initialization for both the NMT and SMT models. In the unsupervised senario, Artetxe et al. (2017) independently trained embeddings in different languages using monolingual corpora, and then learned a linear mapping to align them in a shared space based on a biling"
W19-5327,S18-1039,1,0.887797,"Missing"
W19-5327,N18-1032,0,0.0366251,"translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations. 1 Introduction Machine translation (MT) has achieved huge advances in the past few years (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017, 2018). However, the need for a large amount of manual parallel data obstructs its performance under low-resource conditions. Building an effective model on low resource data or even in an unsupervised way is always an interesting and challenging research topic (Gu et al., 2018; Radford et al., 2016; Lee et al., 2019). Recently, unsupervised MT (Artetxe et al., 2018b,a; Conneau et al., 2018; Lample et al., 2018b; Wu et al., 2019), which can immensely reduce the reliance on parallel corpora, has been gaining more and more interest. Training cross-lingual word embeddings (Conneau et al., 2018; Artetxe et al., 2017) is always the • We propose a method to combine word and subword (BPE) pre-trained input representations aligned using MUSE (Conneau et al., 2018) as an NMT training initialization on a morphologically-rich language pair such as German and Czech. • We study"
W19-5327,W11-2123,0,0.121217,"Missing"
W19-5327,P16-1009,0,0.344427,"ing Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring Zihan Liu∗ , Yan Xu∗ , Genta Indra Winata, Pascale Fung Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong {zliurc,yxucb,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract first step of the unsupervised MT models which produce a word-level shared embedding space for both the source and target, but the lexical coverage can be an intractable problem. To tackle this issue, Sennrich et al. (2016b) provided a subwordlevel solution to overcome the out-of-vocabulary (OOV) problem. In this work, the systems we implement for the German-Czech language pair are built based on the previously proposed unsupervised MT systems, with some adaptations made to accommodate the morphologically rich characteristics of German and Czech (Tsarfaty et al., 2010). Both word-level and subword-level neural machine translation (NMT) models are applied in this task and further tuned by pseudo-parallel data generated from a phrase-based statistical machine translation (PBSMT) model, which is trained following"
W19-5327,P16-1162,0,0.42512,"ing Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring Zihan Liu∗ , Yan Xu∗ , Genta Indra Winata, Pascale Fung Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong {zliurc,yxucb,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract first step of the unsupervised MT models which produce a word-level shared embedding space for both the source and target, but the lexical coverage can be an intractable problem. To tackle this issue, Sennrich et al. (2016b) provided a subwordlevel solution to overcome the out-of-vocabulary (OOV) problem. In this work, the systems we implement for the German-Czech language pair are built based on the previously proposed unsupervised MT systems, with some adaptations made to accommodate the morphologically rich characteristics of German and Czech (Tsarfaty et al., 2010). Both word-level and subword-level neural machine translation (NMT) models are applied in this task and further tuned by pseudo-parallel data generated from a phrase-based statistical machine translation (PBSMT) model, which is trained following"
W19-5327,W10-1401,0,0.083653,"Missing"
W19-5327,W18-1819,0,0.0574586,"Missing"
W19-5327,S19-2021,1,0.814845,"nal source sentences. Instead of direct translation with NMT models, we generate several translation candidates using beam search with a beam size of five. We build the language model proposed by Merity et al. (2018b,a) trained using a monolingual Czech dataset to rescore the generated translations. The scores are determined by the perplexity (PPL) of the generated sentences and the translation candidate with the lowest PPL will be selected as the final translation. 2.4 Model Ensemble Ensemble methods have been shown very effective in many natural language processing tasks (Park et al., 2018; Winata et al., 2019). We apply an ensemble method by taking the top five translations from word-level and subword-level NMT, and rescore all translations using our pre-trained Czech language model mentioned in §2.3. Then, we select the best translation with the lowest perplexity. 3 3.1 Data Post-processing Quotes Fixing The quotes are fixed to keep them the same as the source sentences. Recaser For all the models mentioned above that work under a lower-case setting, a recaser implemented with Moses (Koehn et al., 2007) is applied to convert the translations to the real cases. Experiments Data Pre-processing Patch"
W19-5327,N19-1120,0,0.0319571,"Missing"
W19-5327,D18-1549,0,\N,Missing
W19-5508,P19-1542,1,0.776258,"Missing"
W19-5508,S19-2021,1,0.86315,"Missing"
W95-0114,W93-0301,0,0.122189,"Missing"
W95-0114,P95-1032,1,0.500623,"Missing"
W95-0114,C94-2178,1,0.545099,"Missing"
W95-0114,1994.amta-1.11,1,0.862744,"Missing"
W95-0114,C94-1009,0,0.02308,"Missing"
W95-0114,P93-1003,0,0.0213569,"Missing"
W95-0114,H94-1027,0,0.0393394,"Missing"
W95-0114,P94-1012,0,0.0211877,"its translation are relatively similar in a parallel corpus, they have little correlation in non-parallel texts. Our task is, therefore, to identify a word feature correlating a pair of words even if they appear in texts which are not translations of each other. This feature should also be language and character set independent, i.e. it should be applicable to pairs of languages very different from each other. We propose that context heterogeneity is such a feature. 2 A Non-parallel Corpus of Chinese and English We use parts of the HKUST English-Chinese Bilingual Corpora for our experiments (Wu 1994), consisting of transcriptions of the Hong Kong Legislative Council debates in both English and Chinese. We use the data from 1988-1992, taking the first 73618 sentences from the English text, and the next 73618 sentences from the Chinese text. There are no overlapping sentences between the texts. The topic of these debates varies though is to some extent confined to the same domain, namely the political and social issues of Hong Kong. Although we select the same number of sentences from each language, there are 22147 unique words from English, and only 7942 unique words from Chinese. 3 Some L"
W95-0114,A94-1030,1,0.328521,"Missing"
W95-0114,1994.amta-1.26,0,0.1489,"Missing"
W97-0119,P91-1022,0,0.0455584,"Missing"
W97-0119,J93-2003,0,0.005838,"Missing"
W97-0119,P93-1002,0,0.0761217,"Missing"
W97-0119,A94-1006,0,0.150209,"Missing"
W97-0119,J94-4003,0,0.0180615,"Missing"
W97-0119,J93-1004,0,0.0357784,"Missing"
W97-0119,W95-0115,0,0.012085,"Missing"
W97-0119,P95-1050,0,0.0825282,"Missing"
W97-0119,1992.tmi-1.7,0,0.145406,"Missing"
W97-0119,J96-1001,0,0.0612321,"Missing"
W97-0119,C96-2098,0,0.0694768,"Missing"
W97-0119,1994.amta-1.26,0,0.101418,"Missing"
W97-0119,H93-1052,0,0.0960345,"Missing"
W97-0119,P93-1001,0,\N,Missing
W97-0119,P94-1012,0,\N,Missing
W97-0406,1995.tmi-1.13,0,0.0583723,"Missing"
W97-0406,1995.tmi-1.15,0,0.127456,"ght forward implementation of a mixed language model-based speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set, parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules. 1 Introduction In the past few decades, automatic speech recognition (ASR) and machine translation (MT) have both undergone rapid technical progress. Spoken language translation has emerged as a new field combining the advances in ASR and MT(Levin et al., 1995; Mayfield et al., 1995; Lavie et al., 1995; Vilar et al., 1996). Robustness is a critical issue which must be addressed for this technology to be useful in real applications. There are several robustness issues arising from the multilingual characteristics of many spoken language translation systems which have not studied by the speech recognition community since the latter tends to focus on monolingual recognition systems. One problem in a multilingual system is accent variability. It is frequently assumed that the speakers using a system are native speakers belonging to the same accent group. However, this is not"
