D19-6301,The Second Multilingual Surface Realisation Shared Task ({SR}{'}19): Overview and Evaluation Results,2019,0,2,2,0.465617,5966,simon mille,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"We report results from the SR{'}19 Shared Task, the second edition of a multilingual surface realisation task organised as part of the EMNLP{'}19 Workshop on Multilingual Surface Realisation. As in SR{'}18, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in eleven, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR{'}19 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
D19-5526,Conceptualisation and Annotation of Drug Nonadherence Information for Knowledge Extraction from Patient-Generated Texts,2019,0,0,1,1,26421,anja belz,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Approaches to knowledge extraction (KE) in the health domain often start by annotating text to indicate the knowledge to be extracted, and then use the annotated text to train systems to perform the KE. This may work for annotat- ing named entities or other contiguous noun phrases (drugs, some drug effects), but be- comes increasingly difficult when items tend to be expressed across multiple, possibly non- contiguous, syntactic constituents (e.g. most descriptions of drug effects in user-generated text). Other issues include that it is not al- ways clear how annotations map to actionable insights, or how they scale up to, or can form part of, more complex KE tasks. This paper reports our efforts in developing an approach to extracting knowledge about drug nonadher- ence from health forums which led us to con- clude that development cannot proceed in sep- arate steps but that all aspects{---}from concep- tualisation to annotation scheme development, annotation, KE system training and knowl- edge graph instantiation{---}are interdependent and need to be co-developed. Our aim in this paper is two-fold: we describe a generally ap- plicable framework for developing a KE ap- proach, and present a specific KE approach, developed with the framework, for the task of gathering information about antidepressant drug nonadherence. We report the conceptual- isation, the annotation scheme, the annotated corpus, and an analysis of annotated texts."
W18-6516,{S}patial{VOC}2{K}: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects,2018,0,0,1,1,26421,anja belz,Proceedings of the 11th International Conference on Natural Language Generation,0,"We present SpatialVOC2K, the first multilingual image dataset with spatial relation annotations and object features for image-to-text generation, built using 2,026 images from the PASCAL VOC2008 dataset. The dataset incorporates (i) the labelled object bounding boxes from VOC2008, (ii) geometrical, language and depth features for each object, and (iii) for each pair of objects in both orders, (a) the single best preposition and (b) the set of possible prepositions in the given language that describe the spatial relationship between the two objects. Compared to previous versions of the dataset, we have roughly doubled the size for French, and completely reannotated as well as increased the size of the English portion, providing single best prepositions for English for the first time. Furthermore, we have added explicit 3D depth features for objects. We are releasing our dataset for free reuse, along with evaluation tools to enable comparative evaluation."
W18-6517,Adding the Third Dimension to Spatial Relation Detection in 2{D} Images,2018,0,1,3,0,27665,brandon birmingham,Proceedings of the 11th International Conference on Natural Language Generation,0,"Detection of spatial relations between objects in images is currently a popular subject in image description research. A range of different language and geometric object features have been used in this context, but methods have not so far used explicit information about the third dimension (depth), except when manually added to annotations. The lack of such information hampers detection of spatial relations that are inherently 3D. In this paper, we use a fully automatic method for creating a depth map of an image and derive several different object-level depth features from it which we add to an existing feature set to test the effect on spatial relation detection. We show that performance increases are obtained from adding depth features in all scenarios tested."
W18-6527,Underspecified {U}niversal {D}ependency Structures as Inputs for Multilingual Surface Realisation,2018,0,1,2,0.493927,5966,simon mille,Proceedings of the 11th International Conference on Natural Language Generation,0,"In this paper, we present the datasets used in the Shallow and Deep Tracks of the First Multilingual Surface Realisation Shared Task (SR{'}18). For the Shallow Track, data in ten languages has been released: Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish. For the Deep Track, data in three languages is made available: English, French and Spanish. We describe in detail how the datasets were derived from the Universal Dependencies V2.0, and report on an evaluation of the Deep Track input quality. In addition, we examine the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), and assess whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process."
W18-3601,The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results,2018,0,11,2,0.493927,5966,simon mille,Proceedings of the First Workshop on Multilingual Surface Realisation,0,"We report results from the SR{'}18 Shared Task, a new multilingual surface realisation task organised as part of the ACL{'}18 Workshop on Multilingual Surface Realisation. As in its English-only predecessor task SR{'}11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in ten, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR{'}18 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume."
W17-3517,Shared Task Proposal: Multilingual Surface Realization Using {U}niversal {D}ependency Trees,2017,0,1,4,0.493927,5966,simon mille,Proceedings of the 10th International Conference on Natural Language Generation,0,"We propose a shared task on multilingual Surface Realization, i.e., on mapping unordered and uninflected universal dependency trees to correctly ordered and inflected sentences in a number of languages. A second deeper input will be available in which, in addition, functional words, fine-grained PoS and morphological information will be removed from the input trees. The first shared task on Surface Realization was carried out in 2011 with a similar setup, with a focus on English. We think that it is time for relaunching such a shared task effort in view of the arrival of Universal Dependencies annotated treebanks for a large number of languages on the one hand, and the increasing dominance of Deep Learning, which proved to be a game changer for NLP, on the other hand."
W16-6639,"Effect of Data Annotation, Feature Selection and Model Choice on Spatial Description Generation in {F}rench",2016,6,1,1,1,26421,anja belz,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-3914,Analysis of {T}witter Data for Postmarketing Surveillance in Pharmacovigilance,2016,0,2,4,0,33339,julie pain,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Postmarketing surveillance (PMS) has the vital aim to monitor effects of drugs after release for use by the general population, but suffers from under-reporting and limited coverage. Automatic methods for detecting drug effect reports, especially for social media, could vastly increase the scope of PMS. Very few automatic PMS methods are currently available, in particular for the messy text types encountered on Twitter. In this paper we describe first results for developing PMS methods specifically for tweets. We describe the corpus of 125,669 tweets we have created and annotated to train and test the tools. We find that generic tools perform well for tweet-level language identification and tweet-level sentiment analysis (both 0.94 F1-Score). For detection of effect mentions we are able to achieve 0.87 F1-Score, while effect-level adverse-vs.-beneficial analysis proves harder with an F1-Score of 0.64. Among other things, our results indicate that MetaMap semantic types provide a very promising basis for identifying drug effect mentions in tweets."
W16-3209,"Exploring Different Preposition Sets, Models and Feature Sets in Automatic Generation of Spatial Image Descriptions",2016,0,1,1,1,26421,anja belz,Proceedings of the 5th Workshop on Vision and Language,0,None
W15-4717,Generating Descriptions of Spatial Relations between Objects in Images,2015,6,3,2,0.576923,27660,adrian muscat,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"We investigate the task of predicting prepositions that can be used to describe the spatial relationships between pairs of objects depicted in images. We explore the extent to which such spatial prepositions can be predicted from (a) language information, (b) visual information, and (c) combinations of the two. In this paper we describe the dataset of object pairs and prepositions we have created, and report first results for predicting prepositions for object pairs, using a Naive Bayes framework. The features we use include object class labels and geometrical features computed from object bounding boxes. We evaluate the results in terms of accuracy against human-selected prepositions."
W15-2816,Describing Spatial Relationships between Objects in Images in {E}nglish and {F}rench,2015,9,4,1,1,26421,anja belz,Proceedings of the Fourth Workshop on Vision and Language,0,"The context for the work we report here is the automatic description of spatial relationships between pairs of objects in images. We investigate the task of selecting prepositions for such spatial relationships. We describe the two datasets of object pairs and prepositions we have created for English and French, and report results for predicting prepositions for object pairs in both of these languages, using two methods: (a) an existing approach which manually fixes the mapping from geometrical features to prepositions, and (b) a Naive Bayes classifier trained on the English and French datasets. For the latter we use features based on object class labels and geometrical measurements of object bounding boxes. We evaluate the automatically generated prepositions on unseen data in terms of accuracy against the human-selected prepositions."
W14-5420,The Last 10 Metres: Using Visual Analysis and Verbal Communication in Guiding Visually Impaired Smartphone Users to Entrances,2014,7,0,1,1,26421,anja belz,Proceedings of the Third Workshop on Vision and Language,0,"Blindness and partial sight are increasing, due to changing demographics and greater incidence of diseases such as diabetes, at vast financial and human cost (WHO, 2013). Organisations for the visually impaired stress the importance of independent living, of which safe and independent travel is an integral part. While existing smartphone facilities such as Applexe2x80x99s Siri are encouraging, the supporting localisation services are not sufficiently accurate or precise to enable navigation between e.g. a bus stop or taxi rank and the entrance to a public space such as a hospital, supermarket or train station. In this paper, we report plans and progress to date of research addressing xe2x80x98the problem of the Last 10 Metres.xe2x80x99 We are developing methods for safely guiding users not just to the general vicinity of a target destination (as done by GPS-based services), but right up to the main entrance of the target destination, by a combination of semantically and visually enriched maps, visual analysis, and language generation."
hastie-belz-2014-comparative,A Comparative Evaluation Methodology for {NLG} in Interactive Systems,2014,40,4,2,0,1049,helen hastie,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Interactive systems have become an increasingly important type of application for deployment of NLG technology over recent years. At present, we do not yet have commonly agreed terminology or methodology for evaluating NLG within interactive systems. In this paper, we take steps towards addressing this gap by presenting a set of principles for designing new evaluations in our comparative evaluation methodology. We start with presenting a categorisation framework, giving an overview of different categories of evaluation measures, in order to provide standard terminology for categorising existing and new evaluation techniques. Background on existing evaluation methodologies for NLG and interactive systems is presented. The comparative evaluation methodology is presented. Finally, a methodology for comparative evaluation of NLG components embedded within interactive systems is presented in terms of the comparative evaluation methodology, using a specific task for illustrative purposes."
W12-1525,The Surface Realisation Task: Recent Developments and Future Plans,2012,13,8,1,1,26421,anja belz,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"The Surface Realisation Shared Task was first run in 2011. Two common-ground input representations were developed and for the first time several independently developed surface realisers produced realisations from the same shared inputs. However, the input representations had several shortcomings which we have been aiming to address in the time since. This paper reports on our work to date on improving the input representations and on our plans for the next edition of the SR Task. We also briefly summarise other related developments in NLG shared tasks and outline how the different ideas may be usefully brought together in the future."
belz-gatt-2012-repository,A Repository of Data and Evaluation Resources for Natural Language Generation,2012,10,1,1,1,26421,anja belz,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Starting in 2007, the field of natural language generation (NLG) has organised shared-task evaluation events every year, under the Generation Challenges umbrella. In the course of these shared tasks, a wealth of data has been created, along with associated task definitions and evaluation regimes. In other contexts too, sharable NLG data is now being created. In this paper, we describe the online repository that we have created as a one-stop resource for obtaining NLG task materials, both from Generation Challenges tasks and from other sources, where the set of materials provided for each task consists of (i) task definition, (ii) input and output data, (iii) evaluation software, (iv) documentation, and (v) publications reporting previous results."
kow-belz-2012-lg,{LG}-Eval: A Toolkit for Creating Online Language Evaluation Experiments,2012,6,10,2,0,37803,eric kow,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper we describe the LG-Eval toolkit for creating online language evaluation experiments. LG-Eval is the direct result of our work setting up and carrying out the human evaluation experiments in several of the Generation Challenges shared tasks. It provides tools for creating experiments with different kinds of rating tools, allocating items to evaluators, and collecting the evaluation scores."
W11-2829,Generation Challenges 2011 Preface,2011,0,0,1,1,26421,anja belz,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Generation Challenges 2011 (GenChal'11) was the fifth round of shared-task evaluation competitions (STECs) involving the generation of natural language. It followed four previous events: the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge in 2007 which had its results meeting at UCNLGMT in Copenhagen, Denmark; Referring Expression Generation (REG) Challenges in 2008, with a results meeting at INLG'08 in Ohio, US; Generation Challenges 2009 with a results meeting at ENLG'09 in Athens, Greece; and most recently Generation Challenges 2010 with a results meeting at INLG'10 in Trim, Ireland. More information about all these NLG STEC events can be found via the links on the Generation Challenges homepage (http://www.nltg.brighton.ac.uk/research/genchal11)."
W11-2832,The First Surface Realisation Shared Task: Overview and Evaluation Results,2011,17,53,1,1,26421,anja belz,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"The Surface Realisation (SR) Task was a new task at Generation Challenges 2011, and had two tracks: (1) Shallow: mapping from shallow input representations to realisations; and (2) Deep: mapping from deep input representations to realisations. Five teams submitted six systems in total, and we additionally evaluated human toplines. Systems were evaluated automatically using a range of intrinsic metrics. In addition, systems were assessed by human judges in terms of Clarity, Readability and Meaning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report."
W11-1214,Unsupervised Alignment of Comparable Data and Text Resources,2011,14,0,1,1,26421,anja belz,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"In this paper we investigate automatic data-text alignment, i.e. the task of automatically aligning data records with textual descriptions, such that data tokens are aligned with the word strings that describe them. Our methods make use of log likelihood ratios to estimate the strength of association between data tokens and text tokens. We investigate data-text alignment at the document level and at the sentence level, reporting results for several methodological variants as well as baselines. We find that log likelihood ratios provide a strong basis for predicting data-text alignment."
P11-2040,Discrete vs. Continuous Rating Scales for Language Evaluation in {NLP},2011,11,12,1,1,26421,anja belz,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Studies assessing rating scales are very common in psychology and related fields, but are rare in NLP. In this paper we assess discrete and continuous scales used for measuring quality assessments of computer-generated language. We conducted six separate experiments designed to investigate the validity, reliability, stability, interchangeability and sensitivity of discrete vs. continuous scales. We show that continuous scales are viable for use in language evaluation, and offer distinct advantages over discrete scales."
W10-4201,Comparing Rating Scales and Preference Judgements in Language Evaluation,2010,10,10,1,1,26421,anja belz,Proceedings of the 6th International Natural Language Generation Conference,0,"Rating-scale evaluations are common in NLP, but are problematic for a range of reasons, e.g. they can be unintuitive for evaluators, inter-evaluator agreement and self-consistency tend to be low, and the parametric statistics commonly applied to the results are not generally considered appropriate for ordinal data. In this paper, we compare rating scales with an alternative evaluation paradigm, preference-strength judgement experiments (PJEs), where evaluators have the simpler task of deciding which of two texts is better in terms of a given quality criterion. We present three pairs of evaluation experiments assessing text fluency and clarity for different data sets, where one of each pair of experiments is a rating-scale experiment, and the other is a PJE. We find the PJE versions of the experiments have better evaluator self-consistency and inter-evaluator agreement, and a larger proportion of variation accounted for by system differences, resulting in a larger number of significant differences being found."
W10-4217,Extracting Parallel Fragments from Comparable Corpora for Data-to-text Generation,2010,19,13,1,1,26421,anja belz,Proceedings of the 6th International Natural Language Generation Conference,0,"Building NLG systems, in particular statistical ones, requires parallel data (paired inputs and outputs) which do not generally occur naturally. In this paper, we investigate the idea of automatically extracting parallel resources for data-to-text generation from comparable corpora obtained from the Web. We describe our comparable corpus of data and texts relating to British hills and the techniques for extracting paired input/output fragments we have developed so far."
W10-4225,Generation Challenges 2010 Preface,2010,0,0,1,1,26421,anja belz,Proceedings of the 6th International Natural Language Generation Conference,0,None
W10-4226,The {GREC} Challenges 2010: Overview and Evaluation Results,2010,-1,-1,1,1,26421,anja belz,Proceedings of the 6th International Natural Language Generation Conference,0,None
W10-4237,Finding Common Ground: Towards a Surface Realisation Shared Task,2010,24,9,1,1,26421,anja belz,Proceedings of the 6th International Natural Language Generation Conference,0,"In many areas of NLP reuse of utility tools such as parsers and POS taggers is now common, but this is still rare in NLG. The subfield of surface realisation has perhaps come closest, but at present we still lack a basis on which different surface realisers could be compared, chiefly because of the wide variety of different input representations used by different realisers. This paper outlines an idea for a shared task in surface realisation, where inputs are provided in a common-ground representation formalism which participants map to the types of input required by their system. These inputs are derived from existing annotated corpora developed for language analysis (parsing etc.). Outputs (realisations) are evaluated by automatic comparison against the human-authored text in the corpora as well as by human assessors."
W10-3206,Construction of bilingual multimodal corpora of referring expressions in collaborative problem solving,2010,20,2,6,0,301,takenobu tokunaga,Proceedings of the Eighth Workshop on {A}sian Language Resouces,0,"This paper presents on-going work on constructing bilingual multimodal corpora of referring expressions in collaborative problem solving for English and Japanese. The corpora were collected from dialogues in which two participants collaboratively solved Tangram puzzles with a puzzle simulator. Extra-linguistic information such as operations on puzzle pieces, mouse cursor position and piece positions were recorded in synchronisation with utterances. The speech data was transcribed and time-aligned with the extra-linguistic information. Referring expressions in utterances that refer to puzzle pieces were annotated in terms of their spans, their referents and their other attributes. The Japanese corpus has already been completed, but the English counterpart is still undergoing annotation. We have conducted a preliminary comparative analysis of both corpora, mainly with respect to task completion time, task success rates and attributes of referring expressions. These corpora showed significant differences in task completion time and success rate."
dahab-belz-2010-game,A Game-based Approach to Transcribing Images of Text,2010,4,2,2,0,46126,khalil dahab,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Creating language resources is expensive and time-consuming, and this forms a bottleneck in the development of language technology, for less-studied non-European languages in particular. The recent internet phenomenon of crowd-sourcing offers a cost-effective and potentially fast way of overcoming such language resource acquisition bottlenecks. We present a methodology that takes as its input scanned documents of typed or hand-written text, and produces transcriptions of the text as its output. Instead of using Optical Character Recognition (OCR) technology, the methodology is game-based and produces such transcriptions as a by-product. The approach is intended particularly for languages for which language technology and resources are scarce and reliable OCR technology may not exist. It can be used in place of OCR for transcribing individual documents, or to create corpora of paired images and transcriptions required to train OCR tools. We present Minefield, a prototype implementation of the approach which is currently collecting Arabic transcriptions."
W09-2816,The {GREC} Main Subject Reference Generation Challenge 2009: Overview and Evaluation Results,2009,9,9,1,1,26421,anja belz,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"The GREC-MSR Task at Generation Challenges 2009 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted one system each, and we additionally created four baseline systems. Systems were tested automatically using existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in an intrinsic evaluation involving human judges. This report describes the GREC-MSR Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results."
W09-2817,The {GREC} Named Entity Generation Challenge 2009: Overview and Evaluation Results,2009,5,5,1,1,26421,anja belz,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"The GREC-NEG Task at Generation Challenges 2009 required participating systems to select coreference chains for all people entities mentioned in short encyclopaedic texts about people collected from Wikipedia. Three teams submitted six systems in total, and we additionally created four baseline systems. Systems were tested automatically using a range of existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in an intrinsic evaluation involving human judges. This report describes the GREC-NEG Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results."
W09-0603,System Building Cost vs. Output Quality in Data-to-Text Generation,2009,16,22,1,1,26421,anja belz,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"Data-to-text generation systems tend to be knowledge-based and manually built, which limits their reusability and makes them time and cost-intensive to create and maintain. Methods for automating (part of) the system building process exist, but do such methods risk a loss in output quality? In this paper, we investigate the cost/quality trade-off in generation system building. We compare four new data-to-text systems which were created by predominantly automatic techniques against six existing systems for the same domain which were created by predominantly manual techniques. We evaluate the ten systems using intrinsic automatic metrics and human quality ratings. We find that increasing the degree to which system building is automated does not necessarily result in a reduction in output quality. We find furthermore that standard automatic evaluation metrics underestimate the quality of handcrafted systems and over-estimate the quality of automatically created systems."
W09-0627,Generation {C}hallenges 2009: Preface,2009,0,0,1,1,26421,anja belz,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"Generation Challenges 2009 was the third round of shared-task evaluation competitions (STECs) that involve the generation of natural language, and followed the Pilot Attribute Selection for Generating Referring Expressions Challenge in 2007 (ASGRE'07) and Referring Expression Generation Challenges in 2008 (REG'08). More information about all these NLG STEC activities can be found via the links on the Generation Challenges homepage: http://www.nltg.brighton.ac.uk/research/genchal09"
W09-0629,The {TUNA}-{REG} {C}hallenge 2009: Overview and Evaluation Results,2009,17,87,2,0.666667,6764,albert gatt,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"The GREC Task at REG '08 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted a total of 6 systems, and we additionally created four baseline systems. Systems were tested automatically using a range of existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in a reading/comprehension experiment involving human subjects. This report describes the GREC Task and the evaluation methods, gives brief descriptions of the participating systems, and presents the evaluation results."
J09-4008,An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems,2009,57,76,2,0,5931,ehud reiter,Computational Linguistics,0,"There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies."
J09-1008,Last Words: That{'}s Nice ... What Can You Do With It?,2009,27,24,1,1,26421,anja belz,Computational Linguistics,0,"A regular fixture on the mid 1990s international research seminar circuit was the billion-neuron artificial brain talk. The idea behind this project was simple: in order to create artificial intelligence, what was needed first of all was a very large artificial brain; if a big enough set of interconnected modules of neurons could be implemented, then it would be possible to evolve mammalian-level behavior with current computational- neuron technology. The talk included progress reports on the current size of the artificial brain, its structure, update rate, and power consumption, and explained how intelli- gent behavior was going to develop by mechanisms simulating biological evolution. What the talk didnt mention was what kind of functionality the team had so far managed to evolve, and so the first comment at the end of the talk was inevitably nice work, but have you actually done anything with the brain yet?1 In human language technology (HLT) research, we currently report a range of evaluation scores that measure and assess various aspects of systems, in particular the similarity of their outputs to samples of human language or to human-produced gold- standard annotations, but are we leaving ourselves open to the same question as the billion-neuron artificial brain researchers?"
W08-1108,Attribute Selection for Referring Expression Generation: New Algorithms and Evaluation Methods,2008,27,22,2,0.666667,6764,albert gatt,Proceedings of the Fifth International Natural Language Generation Conference,0,"Referring expression generation has recently been the subject of the first Shared Task Challenge in NLG. In this paper, we analyse the systems that participated in the Challenge in terms of their algorithmic properties, comparing new techniques to classic ones, based on results from a new human task-performance experiment and from the intrinsic measures that were used in the Challenge. We also consider the relationship between different evaluation methods, showing that extrinsic task-performance experiments and intrinsic evaluation methods yield results that are not significantly correlated. We argue that this highlights the importance of including extrinsic evaluation methods in comparative NLG evaluations."
W08-1126,{REG} Challenge Preface,2008,0,0,1,1,26421,anja belz,Proceedings of the Fifth International Natural Language Generation Conference,0,None
W08-1127,The {GREC} Challenge 2008: Overview and Evaluation Results,2008,-1,-1,1,1,26421,anja belz,Proceedings of the Fifth International Natural Language Generation Conference,0,None
W08-1131,The {TUNA} Challenge 2008: Overview and Evaluation Results,2008,-1,-1,2,0.666667,6764,albert gatt,Proceedings of the Fifth International Natural Language Generation Conference,0,None
P08-2050,Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation,2008,11,39,1,1,26421,anja belz,"Proceedings of ACL-08: HLT, Short Papers",0,"In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task."
W07-2302,Generation of repeated references to discourse entities,2007,17,16,1,1,26421,anja belz,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"Generation of Referring Expressions is a thriving subfield of Natural Language Generation which has traditionally focused on the task of selecting a set of attributes that unambiguously identify a given referent. In this paper, we address the complementary problem of generating repeated, potentially different referential expressions that refer to the same entity in the context of a piece of discourse longer than a sentence. We describe a corpus of short encyclopaedic texts we have compiled and annotated for reference to the main subject of the text, and report results for our experiments in which we set human subjects and automatic methods the task of selecting a referential expression from a wide range of choices in a full-text context. We find that our human subjects agree on choice of expression to a considerable degree, with three identical expressions selected in 50% of cases. We tested automatic selection strategies based on most frequent choice heuristics, involving different combinations of information about syntactic MSR type and domain type. We find that more information generally produces better results, achieving a best overall test set accuracy of 53.9% when both syntactic MSR type and domain type are known."
W07-2304,Modelling control in generation,2007,10,3,5,0,32465,roger evans,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"In this paper we present a view of natural language generation in which the control structure of the generator is clearly separated from the content decisions made during generation, allowing us to explore and compare different control strategies in a systematic way. Our approach factors control into two components, a 'generation tree' which maps out the relationships between different decisions, and an algorithm for traversing such a tree which determines which choices are actually made. We illustrate the approach with examples of stylistic control and automatic text revision using both generative and empirical techniques. We argue that this approach provides a useful basis for the theoretical study of control in generation, and a framework for implementing generators with a range of control strategies. We also suggest that this approach can be developed into tool for analysing and adapting control aspects of other advanced wide-coverage generation systems."
N07-1021,Probabilistic Generation of Weather Forecast Texts,2007,15,18,1,1,26421,anja belz,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"This paper reports experiments in which pC RU xe2x80x94 a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space xe2x80x94 is used to semi-automatically create several versions of a weather forecast text generator. The generators are evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system, and (iii) a HALOGEN-style statistical generator. The most striking result is that despite acquiring all decision-making abilities automatically, the best pC RU generators receive higher scores from human judges than forecasts written by experts."
2007.mtsummit-ucnlg.13,The attribute selection for generation of referring expressions challenge. [Introduction to Shared Task Evaluation Challenge.],2007,-1,-1,1,1,26421,anja belz,Proceedings of the Workshop on Using corpora for natural language generation,0,None
2007.mtsummit-ucnlg.14,The attribute selection for {GRE} challenge: overview and evaluation results,2007,4,40,1,1,26421,anja belz,Proceedings of the Workshop on Using corpora for natural language generation,0,") Challenge wasthe xefxacx81rst shared-task evaluation challenge inthe xefxacx81eld of Natural Language Generation.Six teams submitted a total of 22 systems.All submitted systems were tested automat-ically for minimality, uniqueness and xe2x80x98hu-manlikenessxe2x80x99. In addition, the output of 15systems was tested in a task-based exper-iment where subjects were asked to iden-tify referents, and the speed and accuracy ofidentixefxacx81cation was measured. This report de-scribes the"
W06-1418,Introduction to the {INLG}{'}06 Special Session on Sharing Data and Comparative Evaluation,2006,0,0,1,1,26421,anja belz,Proceedings of the Fourth International Natural Language Generation Conference,0,"The idea for this special session had its origins in discussions with many different members of the NLG community at the 2005 Workshop on Using Corpora for Natural Language Generation (UCNLG'05, held in conjunction with the Corpus Linguistics 2005 conference at the University of Birmingham in July 2005), and subsequently at the 10th European Natural Language Generation Workshop (ENLG'05, held at the University of Aberdeen in August 2005). At the latter event, the excitement about introducing shared tasks was infectious: the topic hijacked several of the organised discussion groups, it was the focus of conversation at many tables during lunch-breaks, and even the end of the conference didn't put a stop to it, with discussions carrying right on until the taxis to the airport arrived."
W06-1421,Shared-Task Evaluations in {HLT}: Lessons for {NLG},2006,4,15,1,1,26421,anja belz,Proceedings of the Fourth International Natural Language Generation Conference,0,"While natural language generation (NLG) has a strong evaluation tradition, in particular in userbased and task-oriented evaluation, it has never evaluated different approaches and techniques by comparing their performance on the same tasks (shared-task evaluation, STE). NLG is characterised by a lack of consolidation of results, and by isolation from the rest of NLP where STE is now standard. It is, moreover, a shrinking field (state-of-the-art MT and summarisation no longer perform generation as a subtask) which lacks the kind of funding and participation that natural language understanding (NLU) has attracted."
W06-1422,{GENEVAL}: A Proposal for Shared-task Evaluation in {NLG},2006,10,7,2,0,5931,ehud reiter,Proceedings of the Fourth International Natural Language Generation Conference,0,"We propose to organise a series of sharedtask NLG events, where participants are asked to build systems with similar input/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events."
E06-1040,Comparing Automatic and Human Evaluation of {NLG} Systems,2006,21,97,1,1,26421,anja belz,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NI ST, B LEU, and ROUGE. We find that NI ST scores correlate best (>0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain."
W05-1601,Statistical Generation: Three Methods Compared and Evaluated,2005,25,31,1,1,26421,anja belz,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"Statistical NL G has largely meant n-gram modelling which has the considerable advantages of lending robustness to NL G systems, and of making automatic adaptation to new domains from raw corpora possible. On the downside, n-gram models are expensive to use as selection mechanisms and have a built-in bias towards shorter realisations. This paper looks at treebank-training of generators, an alternative method for building statistical models for NL G from raw corpora, and two different ways of using treebank-trained models during generation. Results show that the treebank-trained generators achieve improvements similar to a 2-gram generator over a baseline of random selection. However, the treebank-trained generators achieve this at a much lower cost than the 2-gram generator, and without its strong preference for shorter reasations."
bouayad-agha-etal-2002-pills,{PILLS}: Multilingual generation of medical information documents with overlapping content,2002,5,25,4,0,39891,nadjet bouayadagha,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In the pharmaceutical industry, products have to be described by a range of document types with overlapping content. Moreover, much of this documentation has to be produced in many languages. This situation is commonplace in many commercial domains, and leads to well-known problems in maintaining a set of related documents and their translations. We describe a potential solution explored in the PILLS project. All relevant knowledge about a product is entered only once, through a natural-language interface to a knowledge base. From this xe2x80x98master modelxe2x80x99, specialised models for a range of document types are derived automatically; from each specialised model, documents are generated automatically in all supported languages. As an illustration of this approach, the PILLS demonstrator generates three medical document types in English, German and French."
C02-1068,Learning Grammars for Different Parsing Tasks by Partition Search,2002,15,2,1,1,26421,anja belz,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes a comparative application of Grammar Learning by Partition Search to four different learning tasks: deep parsing, NP identification, flat phrase chunking and NP chunking. In the experiments, base grammars were extracted from a treebank corpus. From this starting point, new grammars optimised for the different parsing tasks were learnt by Partition Search. No lexical information was used. In half of the experiments, local structural context in the form of parent phrase category information was incorporated into the grammars. Results show that grammars which contain this information outperform grammars which do not by large margins in all tests for all parsing tasks. It makes the biggest difference for deep parsing, typically corresponding to an improvement of around 5%. Overall, Partition Search with parent phrase category information is shown to be a successful method for learning grammars optimised for a given parsing task, and for minimising grammar size. The biggest margin of improvement over a base grammar was a 5.4% increase in the F-Score for deep parsing. The biggest size reductions were 93.5% fewer nonterminals (for NP identification), and 31.3% fewer rules (for XP chunking)"
W01-0712,Learning Computational Grammars,2001,30,6,2,0,38842,john nerbonne,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG) project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use. We were interested in a more systematic survey to understand the relevance of many factors to the success of learning, esp. the availability of annotated data, the kind of dependencies in the data, and the availability of knowledge bases (grammars). We focused on syntax, esp. noun phrase (NP) syntax."
W00-1805,Multi-Syllable Phonotactic Modelling,2000,0,3,1,1,26421,anja belz,Proceedings of the Fifth Workshop of the {ACL} Special Interest Group in Computational Phonology,0,None
W98-0905,An Approach to the Automatic Acquisition of Phonotactic Constraints,1998,20,3,1,1,26421,anja belz,{SIGPHON}{'}98 The Computation of Phonological Constraints,0,None
P98-2240,Discovering Phonotactic Finite-State Automata by Genetic Search,1998,5,5,1,1,26421,anja belz,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper presents a genetic algorithm based approach to the automatic discovery of finite-state automata (FSAs) from positive data. FSAs are commonly used in computational phonology, but-given the limited learnability of FSAs from arbitrary language subsets-are usually constructed manually. The approach presented here offers a practical automatic method that helps reduce the cost of manual FSA construction."
C98-2235,Discovering Phonotactic Finite-State Automata by Genetic Search,1998,5,5,1,1,26421,anja belz,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper presents a genetic algorithm based approach to the automatic discovery of finite-state automata (FSAs) from positive data. FSAs are commonly used in computational phonology, but-given the limited learnability of FSAs from arbitrary language subsets-are usually constructed manually. The approach presented here offers a practical automatic method that helps reduce the cost of manual FSA construction."
