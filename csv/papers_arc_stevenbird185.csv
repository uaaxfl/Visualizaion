2021.emnlp-main.157,Local Word Discovery for Interactive Transcription,2021,-1,-1,2,1,1366,william lane,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for Kunwinjku, a morphologically-complex Australian language. We combine a finite state implementation of a published grammar with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17{\%}. Further, we find that 75{\%} of breath groups in the test set receive at least one correct partial or full-word suggestion."
2021.dash-1.16,A Computational Model for Interactive Transcription,2021,-1,-1,3,1,1366,william lane,Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances,0,"Transcribing low resource languages can be challenging in the absence of a good lexicon and trained transcribers. Accordingly, we seek a way to enable interactive transcription whereby the machine amplifies human efforts. This paper presents a data model and a system architecture for interactive transcription, supporting multiple modes of interactivity, increasing the likelihood of finding tasks that engage local participation in language work. The approach also supports other applications which are useful in our context, including spoken document retrieval and language learning."
2020.coling-main.303,Enabling Interactive Transcription in an Indigenous Community,2020,-1,-1,2,0,18204,eric ferrand,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a novel transcription workflow which combines spoken term detection and human-in-the-loop, together with a pilot experiment. This work is grounded in an almost zero-resource scenario where only a few terms have so far been identified, involving two endangered languages. We show that in the early stages of transcription, when the available data is insufficient to train a robust ASR system, it is possible to take advantage of the transcription of a small number of isolated words in order to bootstrap the transcription of a speech collection."
2020.coling-main.313,Decolonising Speech and Language Technology,2020,-1,-1,1,1,8953,steven bird,Proceedings of the 28th International Conference on Computational Linguistics,0,"After generations of exploitation, Indigenous people often respond negatively to the idea that their languages are data ready for the taking. By treating Indigenous knowledge as a commodity, speech and language technologists risk disenfranchising local knowledge authorities, reenacting the causes of language endangerment. Scholars in related fields have responded to calls for decolonisation, and we in the speech and language technology community need to follow suit, and explore what this means for our practices that involve Indigenous languages and the communities who own them. This paper reviews colonising discourses in speech and language technology, and suggests new ways of working with Indigenous communities, and seeks to open a discussion of a postcolonial approach to computational methods for supporting language vitality."
2020.coling-main.405,Interactive Word Completion for Morphologically Complex Languages,2020,-1,-1,2,1,1366,william lane,Proceedings of the 28th International Conference on Computational Linguistics,0,"Text input technologies for low-resource languages support literacy, content authoring, and language learning. However, tasks such as word completion pose a challenge for morphologically complex languages thanks to the combinatorial explosion of possible words. We have developed a method for morphologically-aware text input in Kunwinjku, a polysynthetic language of northern Australia. We modify an existing finite state recognizer to map input morph prefixes to morph completions, respecting the morphosyntax and morphophonology of the language. We demonstrate the portability of the method by applying it to Turkish. We show that the space of proximal morph completions is many orders of magnitude smaller than the space of full word completions for Kunwinjku. We provide a visualization of the morph completion space to enable the text completion parameters to be fine-tuned. Finally, we report on a web services deployment, along with a web interface which helps users enter morphologically complex words and which retrieves corresponding entries from the lexicon."
2020.cl-4.1,Sparse Transcription,2020,-1,-1,1,1,8953,steven bird,Computational Linguistics,0,"The transcription bottleneck is often cited as a major obstacle for efforts to document the world{'}s endangered languages and supply them with language technologies. One solution is to extend methods from automatic speech recognition and machine translation, and recruit linguists to provide narrow phonetic transcriptions and sentence-aligned translations. However, I believe that these approaches are not a good fit with the available data and skills, or with long-established practices that are essentially word-based. In seeking a more effective approach, I consider a century of transcription practice and a wide range of computational approaches, before proposing a computational model based on spoken term detection that I call {``}sparse transcription.{''} This represents a shift away from current assumptions that we transcribe phones, transcribe fully, and transcribe first. Instead, sparse transcription combines the older practice of word-level transcription with interpretive, iterative, and interactive processes that are amenable to wider participation and that open the way to new methods for processing oral languages."
2020.acl-main.594,Bootstrapping Techniques for Polysynthetic Morphological Analysis,2020,18,0,2,1,1366,william lane,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word. This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies. To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language. We generate data from a finite state transducer to train an encoder-decoder model. We improve the model by {``}hallucinating{''} missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes. The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7{\%} overall, a 10 percentage point improvement over the FST baseline. This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources."
U19-1001,Towards A Robust Morphological Analyzer for Kunwinjku,2019,-1,-1,2,1,1366,william lane,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,"Kunwinjku is an indigenous Australian language spoken in northern Australia which exhibits agglutinative and polysynthetic properties. Members of the community have expressed interest in co-developing language applications that promote their values and priorities. Modeling the morphology of the Kunwinjku language is an important step towards accomplishing the community{'}s goals. Finite State Transducers have long been the go-to method for modeling morphologically rich languages, and in this paper we discuss some of the distinct modeling challenges present in the morphosyntax of verbs in Kunwinjku. We show that a fairly straightforward implementation using standard features of the foma toolkit can account for much of the verb structure. Continuing challenges include robustness in the face of variation and unseen vocabulary, as well as how to handle complex reduplicative processes. Our future work will build off the baseline and challenges presented here."
L18-1530,Evaluation Phonemic Transcription of Low-Resource Tonal Languages for Language Documentation,2018,0,6,5,1,11431,oliver adams,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Transcribing speech is an important part of language documentation, yet speech recognition technology has not been widely harnessed to aid linguists. We explore the use of a neural network architecture with the connectionist temporal classification loss function for phonemic and tonal transcription in a language documentation setting. In this framework, we explore jointly modelling phonemes and tones versus modelling them separately, and assess the importance of pitch information versus phonemic context for tonal prediction. Experiments on two tonal languages, Yongning Na and Eastern Chatino, show the changes in recognition performance as training data is scaled from 10 minutes up to 50 minutes for Chatino, and up to 224 minutes for Na. We discuss the findings from incorporating this technology into the linguistic workflow for documenting Yongning Na, which show the method's promise in improving efficiency, minimizing typographical errors, and maintaining the transcription's faithfulness to the acoustic signal, while highlighting phonetic and phonemic facts for linguistic consideration."
W17-0121,Developing a Suite of Mobile Applications for Collaborative Language Documentation,2017,9,0,2,0,11303,mat bettinson,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,None
E17-1084,Multilingual Training of Crosslingual Word Embeddings,2017,32,20,4,1,25446,long duong,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Crosslingual word embeddings represent lexical items from different languages using the same vector space, enabling crosslingual transfer. Most prior work constructs embeddings for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space.In this way, we can exploit and combine strength of many languages. We obtained high performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks."
E17-1088,Cross-Lingual Word Embeddings for Low-Resource Language Modeling,2017,42,25,4,1,11431,oliver adams,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Most languages have no established writing system and minimal written records. However, textual data is essential for natural language processing, and particularly important for training language models to support speech recognition. Even in cases where text data is missing, there are some languages for which bilingual lexicons are available, since creating lexicons is a fundamental task of documentary linguistics. We investigate the use of such lexicons to improve language models when textual training data is limited to as few as a thousand sentences. The method involves learning cross-lingual word embeddings as a preliminary step in training monolingual language models. Results across a number of languages show that language models are improved by this pre-training. Application to Yongning Na, a threatened language, highlights challenges in deploying the approach in real low-resource environments."
N16-1109,An Attentional Model for Speech Translation Without Transcription,2016,27,54,4,1,25446,long duong,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
D16-1136,Learning Crosslingual Word Embeddings without Bilingual Corpora,2016,23,25,4,1,25446,long duong,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task."
D16-1263,Learning a Lexicon and Translation Model from Phoneme Lattices,2016,23,6,4,1,11431,oliver adams,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
S15-1012,Collective Document Classification with Implicit Inter-document Semantic Relationships,2015,35,1,2,0,37312,clint burford,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"This paper addresses the question of how document classifiers can exploit implicit information about document similarity to improve document classifier accuracy. We infer document similarity using simple n-gram overlap, and demonstrate that this improves overall document classification performance over two datasets. As part of this, we find that collective classification based on simple iterative classifiers outperforms the more complex and computationally-intensive dual classifier approach."
P15-2139,Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural Network Parser,2015,20,81,3,1,25446,long duong,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Training a high-accuracy dependency parser requires a large treebank. However, these are costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method."
K15-1012,Cross-lingual Transfer for Unsupervised Dependency Parsing Without Parallel Data,2015,33,14,3,1,25446,long duong,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Cross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement."
D15-1040,A Neural Network Model for Low-Resource {U}niversal {D}ependency Parsing,2015,28,18,3,1,25446,long duong,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Accurate dependency parsing requires large treebanks, which are only available for a few languages. We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data. We propose a model for learning a shared xe2x80x9cuniversalxe2x80x9d parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations."
2015.iwslt-papers.18,Inducing bilingual lexicons from small quantities of sentence-aligned phonemic transcriptions,2015,-1,-1,4,1,11431,oliver adams,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
W14-2201,{A}ikuma: A Mobile App for Collaborative Language Documentation,2014,7,23,1,1,8953,steven bird,Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,"Proliferating smartphones and mobile software offer linguists a scalable, networked recording device. This paper describes Aikuma, a mobile app that is designed to put the key language documentation tasks of recording, respeaking, and translating in the hands of a speech community. After motivating the approach we describe the system and briefly report on its use in field tests."
D14-1096,What Can We Get From 1000 Tokens? A Case Study of Multilingual {POS} Tagging For Resource-Poor Languages,2014,30,19,4,1,25446,long duong,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages. We use parallel data to transfer part-of-speech information from resource-rich to resourcepoor languages. Additionally, we use a small amount of annotated data to learn to xe2x80x9ccorrectxe2x80x9d errors from projected approach such as tagset mismatch between languages, achieving state-of-the-art performance (91.3%) across 8 languages. Our approach is based on modest data requirements, and uses minimum divergence classification. For situations where no universal tagset mapping is available, we propose an alternate method, resulting in state-of-the-art 85.6% accuracy on the resource-poor language Malagasy."
C14-1096,Collecting Bilingual Audio in Remote Indigenous Communities,2014,24,16,1,1,8953,steven bird,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Most of the worldxe2x80x99s languages are under-resourced, and most under-resourced languages lack a writing system and literary tradition. As these languages fall out of use, we lose important sources of data that contribute to our understanding of human language. The first, urgent step is to collect and orally translate a large quantity of spoken language. This can be digitally archived and later transcribed, annotated, and subjected to the full range of speech and language processing tasks, at any time in future. We have been investigating a mobile application for recording and translating unwritten languages. We visited indigenous communities in Brazil and Nepal and taught people to use smartphones for recording spoken language and for orally interpreting it into the national language, and collected bilingual phrase-aligned speech recordings. In spite of several technical and social issues, we found that the technology enabled an effective workflow for speech data collection. Based on this experience, we argue that the use of special-purpose software on smartphones is an effective and scalable method for large-scale collection of bilingual audio, and ultimately bilingual text, for languages spoken in remote indigenous communities."
P13-2112,Simpler unsupervised {POS} tagging with bilingual projections,2013,10,16,3,1,25446,long duong,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present an unsupervised approach to part-of-speech tagging based on projections of tags in a word-aligned bilingual parallel corpus. In contrast to the existing state-of-the-art approach of Das and Petrov, we have developed a substantially simpler method by automatically identifying xe2x80x9cgoodxe2x80x9d training sentences from the parallel corpus and applying self-training. In experimental results on eight languages, our method achieves state-of-the-art results."
I13-1161,Large-Scale Text Collection for Unwritten Languages,2013,10,11,2,0,38686,florian hanke,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Existing methods for collecting texts from endangered languages are not creating the quantity of data that is needed for corpus studies and natural language processing tasks. This is because the process of transcribing and translating from audio recordings is too onerous. A more effective method, we argue, is to involve local speakers in the field location, using an audio-only translation interface that is portable and easy to use. We present encouraging early results of an experimental investigation of the efficiency of creating translations using this method, and report on the quality of the resulting content."
I13-1177,Increasing the Quality and Quantity of Source Language Data for Unsupervised Cross-Lingual {POS} Tagging,2013,11,3,3,1,25446,long duong,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Bilingual corpora offer a promising bridge between resource-rich and resource-poor languages, enabling the development of natural language processing systems for the latter. English is often selected as the resource-rich language, but another choice might give better performance. In this paper, we consider the task of unsupervised cross-lingual POS tagging, and construct a model that predicts the best source language for a given target language. In experiments on 9 languages, this model improves on using a single fixed source language. We then show that further improvements can be made by combining information from multiple source languages."
C12-3022,{F}angorn: A System for Querying very large Treebanks,2012,13,6,2,1,43657,sumukh ghodke,Proceedings of {COLING} 2012: Demonstration Papers,0,"The efficiency and robustness of statistical parsers has made it possible to create very large treebanks. These serve as the starting point for further work including enrichment, extraction, and curation: semantic annotations are added, syntactic features are mined, erroneous analyses are corrected. In many such cases manual processing is required, and this must operate efficiently on the largest scale. We report on an efficient web-based system for querying very large treebanks called Fangorn. It implements an XPath-like query language which is extended with a linguistic operator to capture proximity in the terminal sequence. Query results are displayed using scalable vector graphics and decorated with the original query, making it easy for queries to be modified and resubmitted. Fangorn is built on the Apache Lucene text search engine and is available under the Apache License."
C12-2013,Machine Translation for Language Preservation,2012,12,6,1,1,8953,steven bird,Proceedings of {COLING} 2012: Posters,0,"Statistical machine translation has been remarkably successful for the worldxe2x80x99s well-resourced languages, and much effort is focussed on creating and exploiting rich resources such as treebanks and wordnets. Machine translation can also support the urgent task of documenting the worldxe2x80x99s endangered languages. The primary object of statistical translation models, bilingual aligned text, closely coincides with interlinear text, the primary artefact collected in documentary linguistics. It ought to be possible to exploit this similarity in order to improve the quantity and quality of documentation for a language. Yet there are many technical and logistical problems to be addressed, starting with the problem that xe2x80x90 for most of the languages in question xe2x80x90 no texts or lexicons exist. In this position paper, we examine these challenges, and report on a data collection effort involving 15 endangered languages spoken in the highlands of Papua New Guinea."
W11-1216,Towards a Data Model for the Universal Corpus,2011,4,5,2,0,14766,steven abney,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"We describe the design of a comparable corpus that spans all of the world's languages and facilitates large-scale cross-linguistic processing. This Universal Corpus consists of text collections aligned at the document and sentence level, multilingual wordlists, and a small set of morphological, lexical, and syntactic annotations. The design encompasses submission, storage, and access. Submission preserves the integrity of the work, allows asynchronous updates, and facilitates scholarly citation. Storage employs a cloud-hosted filestore containing normalized source data together with a database of texts and annotations. Access is permitted to the filestore, the database, and an application programming interface. All aspects of the Universal Corpus are open, and we invite community participation in its design and implementation, and in supplying and using its data."
P11-1151,Collective Classification of Congressional Floor-Debate Transcripts,2011,25,38,2,0,44692,clinton burfoot,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"This paper explores approaches to sentiment classification of U. S. Congressional floor-debate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique."
I11-1059,Normalising Audio Transcriptions for Unwritten Languages,2011,10,1,2,0,44768,adel foda,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"The task of documenting the worldxe2x80x99s languages is a mainstream activity in linguistics which is yet to spill over into computational linguistics. We propose a new task of transcription normalisation as an algorithmic method for speeding up the process of transcribing audio sources, leading to text collections of usable quality. We report on the application of sentence and word alignment algorithms to this task, before describing a new algorithm. All of the algorithms are evaluated over synthetic datasets. Although the results are nuanced, the transcription normalisation task is suggested as an NLP contribution to the grand challenge of documenting the worldxe2x80x99s languages."
I11-1088,A Breadth-First Representation for Tree Matching in Large Scale Forest-Based Translation,2011,11,1,2,1,43657,sumukh ghodke,Proceedings of 5th International Joint Conference on Natural Language Processing,0,Efficient data structures are necessary for searching large translation rule dictionaries in forest-based machine translation. We propose a breadth-first representation of tree structures that allows trees to be stored and accessed efficiently. We describe an algorithm that allows incremental search for trees in a forest and show that its performance is orders of magnitude faster than iterative search. A B-tree index is used to store the rule dictionaries. Prefix-compressed indexes with a large page size are found to provide a balance of fast search and disk space utilisation.
P10-1010,The Human Language Project: Building a Universal Corpus of the World{'}s Languages,2010,23,45,2,0,14766,steven abney,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a grand challenge to build a corpus that will include all of the world's languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world's linguistic heritage before more languages fall silent."
N10-1034,Fast Query for Large Treebanks,2010,35,13,2,1,43657,sumukh ghodke,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"A variety of query systems have been developed for interrogating parsed corpora, or treebanks. With the arrival of efficient, wide-coverage parsers, it is feasible to create very large databases of trees. However, existing approaches that use in-memory search, or relational or XML database technologies, do not scale up. We describe a method for storage, indexing, and query of treebanks that uses an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks."
J09-3007,Last Words: Natural Language Processing and Linguistic Fieldwork,2009,11,17,1,1,8953,steven bird,Computational Linguistics,0,"March 2009 marked an important milestone: the First International Conference on Language Documentation and Conservation, held at the University of Hawaixe2x80x98i.1 The scale of the event was striking, with five parallel tracks running over three days. The organizers coped magnificently with three times the expected participation (over 300). The buzz among the participants was that we were at the start of something big, that we were already part of a significant and growing community dedicated to supporting small languages together, the conference subtitle. The event was full of computation and linguistics, yet devoid of computational linguistics. The language documentation community uses technology to process language, but is largely ignorant of the field of natural language processing. I pondered what we have to offer this community: xe2x80x9cSend us your 10 million words of Nahuatl-English bitext and wexe2x80x99ll do you a machine translation system!xe2x80x9d xe2x80x9cShow us your Bambara WordNet and wexe2x80x99ll use it to train a word sense disambiguation tool!xe2x80x9d xe2x80x9cWrite up the word-formation rules of Inuktitut in this arcane format and wexe2x80x99ll give you a morphological analyzer!xe2x80x9d Is there not some more immediate contribution we could offer?"
Y08-1008,Toward a Global Infrastructure for the Sustainability of Language Resources,2008,-1,-1,2,0,47619,gary simons,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,None
W08-0204,Defining a Core Body of Knowledge for the Introductory Computational Linguistics Curriculum,2008,11,4,1,1,8953,steven bird,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"Discourse in and about computational linguistics depends on a shared body of knowledge. However, little content is shared across the introductory courses in this field. Instead, they typically cover a diverse assortment of topics tailored to the capabilities of the students and the interests of the instructor. If the core body of knowledge could be agreed and incorporated into introductory courses several benefits would ensue, such as the proliferation of instructional materials, software support, and extension modules building on a common foundation. This paper argues that it is worthwhile to articulate a core body of knowledge, and proposes a starting point based on the ACM Computer Science Curriculum. A variety of issues specific to the multidisciplinary nature of computational linguistics are explored."
W08-0208,Multidisciplinary Instruction with the Natural Language Toolkit,2008,18,36,1,1,8953,steven bird,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"The Natural Language Toolkit (NLTK) is widely used for teaching natural language processing to students majoring in linguistics or computer science. This paper describes the design of NLTK, and reports on how it has been used effectively in classes that involve different mixes of linguistics and computer science students. We focus on three key issues: getting started with a course, delivering interactive demonstrations in the classroom, and organizing assignments and projects. In each case, we report on practical experience and make recommendations on how to use NLTK to maximum effect."
bird-etal-2008-acl,The {ACL} {A}nthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics,2008,15,134,1,1,8953,steven bird,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research."
W07-0907,Dynamic Path Prediction and Recommendation in a Museum Environment,2007,13,15,3,1,41273,karl grieser,Proceedings of the Workshop on Language Technology for Cultural Heritage Data ({L}a{T}e{CH} 2007).,0,"This research is concerned with making recommendations to museum visitors based on their history within the physical environment, and textual information associated with each item in their history. We investigate a method of providing such recommendations to users through a combination of language modelling techniques, geospatial modelling of the physical space, and observation of sequences of locations visited by other users in the past. This study compares and analyses different methods of path prediction including an adapted naive Bayes method, document similarity, visitor feedback and measures of lexical similarity."
U06-1022,Analysis and Prediction of User Behaviour in a Museum Environment,2006,-1,-1,3,1,41273,karl grieser,Proceedings of the Australasian Language Technology Workshop 2006,0,None
P06-4018,{NLTK}: The {N}atural {L}anguage {T}oolkit,2006,13,1392,1,1,8953,steven bird,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simplified toolkit and explains how it is used in teaching NLP."
hughes-etal-2006-reconsidering,Reconsidering Language Identification for Written Language Resources,2006,17,61,3,0.980392,49832,baden hughes,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The task of identifying the language in which a given document (ranging from a sentence to thousands of pages) is written has been relatively well studied over several decades. Automated approachesto written language identification are used widely throughout research and industrial contexts, over both oral and written source materials. Despite this widespread acceptance, a review of previous research in written language identification reveals a number of questions which remain openand ripe for further investigation."
Y05-1001,{LP}ath+: A First-Order Complete Language for Linguistic Tree Query,2005,21,5,2,1,6040,catherine lai,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,"Annotated linguistic databases are widely used in linguistic research and in language technology development. These annotations are typically hierarchical, and represent the nested structure of syntactic and prosodic constituents. Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees. We establish the formal expressiveness of LPath relative to the XPath family of languages. We also extend LPath to permit simple closures, resulting in a first-order complete language which we believe is sufficiently expressive for the majority of linguistic tree query needs."
U05-1018,Structuring Documents Efficiently,2005,8,0,2,0,50864,robert marshall,Proceedings of the Australasian Language Technology Workshop 2005,0,"Documents are typically marked up to enable rendering and to facilitate reuse. However, retargetting a document often requires pervasive changes to the markup. Power et al. have proposed a new level of representation called document structure which captures just those aspects of graphical organisation that are signicant for conveying meaning. These document structures can be generated automatically from rhetorical structures, abstract representations of the meaning of a text. The mapping is highly indeterminate, being governed by a large number of interacting constraints. We present a constraint programming approach to the problem, and report on early experiments with an implementation in Prolog."
U04-1017,Representing and Rendering Linguistic Paradigms,2004,7,0,2,0,51725,david penton,Proceedings of the Australasian Language Technology Workshop 2004,0,"Linguistic forms are inherently multi-dimensional. They exhibit a variety of phonological, orthographic, morphosyntactic, semantic and pragmatic properties. Accordingly, linguistic analysis involves multidimensional exploration, a process in which the same collection of forms is laid out in many ways until clear patterns emerge. Equally, language documentation usually contains tabulations of linguistic forms to illustrate systematic patterns and variations. In all such cases, multi-dimensional data is projected onto a two-dimensional table known as a linguistic paradigm, the most widespread format for linguistic data presentation. In this paper we develop an XML data model for linguistic paradigms, and show how XSL transforms can render them. We describe a high-level interface which gives linguists flexible, high-level control of paradigm layout. The work provides a simple, general, and extensible model for the preservation and access of linguistic data."
U04-1019,Querying and Updating Treebanks: A Critical Survey and Requirements Analysis,2004,9,48,2,1,6040,catherine lai,Proceedings of the Australasian Language Technology Workshop 2004,0,"Language technology makes extensive use of hierarchically annotated text and speech data. These databases are stored in flat files and manipulated using corpus-specific query tools or special-purpose scripts. While the size of these databases and the range of applications has grown rapidly in recent years, neither method for managing the data has led to reusable, scalable software. The formal properties of the query languages are not well understood. Hence established methods for indexing tree data and optimizing tree queries cannot be employed. We analyze a range of existing linguistic query languages, and adduce a set of requirements for a reusable, scalable linguistic query language."
P04-3031,{NLTK}: The Natural Language Toolkit,2004,-1,-1,1,1,8953,steven bird,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,None
gibbon-etal-2004-securing,Securing Interpretability: The Case of {E}ga Language Documentation,2004,6,7,3,0,35231,dafydd gibbon,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The prime consideration in designing sustainable language resources is to ensure that they remain interpretable for coming generations of users. In this paper we adopt a new perspective on resource creation securing the interpretability of data, using a case study of Ega, an endangered African language for which a small amount of legacy data is available. Basic steps to securing interpretability are to transfer files to durable media, and where possible, to convert all legacy data into XML files with Unicode character encodings. In the absence of agreed xe2x80x98best practicexe2x80x99 standards, we propose a methodology of xe2x80x98better practicexe2x80x99 to assist in the transition process towards this goal. We discuss a number of issues involved in securing interpretability of the lexicon, character encodings, interlinear glossed text, annotated recordings and nomenclature in linguistic descriptions, and describe our solutions."
hughes-etal-2004-functional,Functional Requirements for an Interlinear Text Editor,2004,5,5,3,1,49832,baden hughes,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Interlinear text has long been considered a valuable format in the presentation of multilingual data, and a variety of software tools have facilitated the creation and processing of such texts by researchers. Despite the diversity of tools, a common core of editorial functionality is provided. Identifying these core functions has important implications for software engineers who seek to efficiently build tools that support interlinear text editing. While few applications are specifically designed for the creation or manipulation of interlinear text, a number of tools offer varying degrees of incidental support for this modality. In this paper we provide a comprehensive set of critieria upon which the derivation of functional criteria can be based. We describe the basis on which a group of tools was selected for investigation, along with the evaluation criteria. Finally we consolidate our findings into a functional specification for the development of software applications for the editing of interlinear text."
hughes-etal-2004-management,Management of Metadata in Linguistic Fieldwork: Experience from the {ACLA} Project,2004,4,0,3,1,49832,baden hughes,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Many linguistic research projects collect large amounts of multimodal data in digital formats. Despite the plethora of data collection applications available, it is often difficult for researchers to identify and integrate applications which enable the management of collections of multimodal data in addition to facilitating the actual collection process itself. In research projects that involve substantial data analysis, data management becomes a critical issue. Whilst best practice recommendations in regard to data formats themselves are propagated through projects such as EMELD, HRELP and DOBES, there is little corresponding information available regarding best practice for field metadata management beyond the provision of standards by entities such as OLAC and IMDI. These general problems are further exacerbated in the context of multiple researchers in geographically-disparate or connectivity-challenged locations. We describe the design of a solution for a group of researchers collecting data on child language acquisition in Australian indigenous communities. We describe the context, identify pertinent issues, outline the mechanics of a solution, and finally report the implementation. In doing so, we provide an alternative model and an open source software application suite which aims to be sufficiently general that other research groups may consider adopting some or all of the infrastructure."
macwhinney-etal-2004-talkbank,{T}alkbank: Building an Open Unified Multimodal Database of Communicative Interaction,2004,3,11,2,0,34440,brian macwhinney,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The goal of the TalkBank project (http://talkbank.org) is to support data-sharing and direct, community-wide access to naturalistic recordings and transcripts of human and animal communication. Toward this end, we have constructed a web accessible database of transcripts linked to audio and video media within fields such as conversation analysis, classroom discourse, animal communication, gesture, meetings, second language acquisition, first language acquisition, bilingualism, tutoring, and legal oral argumentation. We discuss how we have taken discrepant databases from dozens of individual projects and merged them together into a well-structured uniform database in which transcripts can be opened online through browsers, allowing direct multimedia playback. To achieve translation across corpora, we have defined a general XML schema. The validity of this schema is checked by bidirectional conversion from alternative input formats to XML and back. The resultant transcripts are then linked to hinted media and XSLT is used to format web readable browsable multimedia transcripts playable through SMIL. A parallel pathway is used to support collaborative commentary and publication of PDF linked to media through special issues of journals in the relevant fields."
W03-0805,Grid-Enabling Natural Language Engineering By Stealth,2003,9,4,2,1,49832,baden hughes,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Software Engineering and Architecture of Language Technology Systems ({SEALTS}),0,"We describe a proposal for an extensible, componentbased software architecture for natural language engineering applications. Our model leverages existing linguistic resource description and discovery mechanisms based on extended Dublin Core metadata. In addition, application design is flexible, allowing disparate components to be combined to suit the overall application functionality. An application specification language provides abstraction from the programming environment and allows ease of interface with computational grids via a broker."
U03-1008,Encoding and presenting interlinear text using {XML} technologies,2003,6,12,2,1,49832,baden hughes,Proceedings of the Australasian Language Technology Workshop 2003,0,"Interlinear text is a common presentational format for linguistic information, and its creation and management have been greatly facilitated by the development of specialised software. In earlier work we developed a four-level model and corresponding formal specification for interlinear text. Here we describe a suitable XML representation for the model and show how it can be rendered into a variety of convenient presentational formats. We conclude by discussing architectural extensions, an application programming interface for interlinear text, and prospects for embedding the interlinear model into existing applications."
W02-0109,{NLTK}: The Natural Language Toolkit,2002,6,590,2,0,45494,edward loper,Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,0,"NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset."
bird-etal-2002-open,The Open Language Archives Community,2002,0,5,1,1,8953,steven bird,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
ma-etal-2002-models,Models and Tools for Collaborative Annotation,2002,9,25,3,0,37104,xiaoyi ma,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The Annotation Graph Toolkit (AGTK) is a collection of software which facilitates development of linguistic annotatio n tools. AGTK provides a database interface which allows applications to use a database server for persistent storage. This paper dis cusses various modes of collaborative annotation and how they can be supported with tools built using AGTK and its database interface. We describe the relational database schema and API, and describe a version of the TableTrans tool which supports collaborative annotation. The remainder of the paper discusses a high-level query language for annotation graphs, along with optimizations, in support of expressive and efficient access to the annotations held on a large centra l server. The paper demonstrates that it is straightforward to support a variety of different levels of collaborative annotation with exist ing AGTK-based tools, with a minimum of additional programming effort."
bird-etal-2002-tabletrans,"{T}able{T}rans, {M}ulti{T}rans, {I}nter{T}rans and {T}ree{T}rans: Diverse Tools Built on the Annotation Graph Toolkit",2002,5,4,1,1,8953,steven bird,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Four diverse tools built on the Annotation Graph Toolkit are described. Each tool associates linguistic codes and structures with time-series data. All are based on the same software library and tool architecture. TableTrans is for observational coding, using a spreadsheet whose rows are aligned to a signal. MultiTrans is for transcribing multi-party communicative interactions recorded using multi-channel signals. InterTrans is for creating interlinear text aligned to audio. TreeTrans is for creating and manipulating syntactic trees. This work demonstrates that the development of diverse tools and re-use of software components is greatly facilitated by a common high-level application programming interface for representing the data and managing input/output, together with a common architecture for managing the interaction of multiple components."
maeda-etal-2002-creating,Creating Annotation Tools with the Annotation Graph Toolkit,2002,6,0,2,0,53535,kazauki maeda,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The Annotation Graph Toolkit is a collection of software supporting the development of annotation tools based on the annotation graph model. The toolkit includes application programming interfaces for manipulating annotation graph data and for importing data from other formats. There are interfaces for the scripting languages Tcl and Python, a database interface, specialized graphical user interfaces for a variety of annotation tasks, and several sample applications. This paper describes all the toolkit components for the benefit of would-be application developers."
cotton-bird-2002-integrated,An integrated framework for treebanks and multilayer annotations,2002,13,8,2,0,53542,scott cotton,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Treebank formats and associated software tools are proliferating rapidly, with little consideration for interoperability. We survey a wide variety of treebank structures and operations, and show how they can be mapped onto the annotation graph model, and leading to an integrated framework encompassing tree and non-tree annotations alike. This development opens up new possibilities for managing and exploiting multilayer annotations."
W01-1506,The {OLAC} Metadata Set and Controlled Vocabularies,2001,4,30,1,1,8953,steven bird,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"As language data and associated technologies proliferate and as the language resources community rapidly expands, it has become difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool can work with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper describes a new digital infrastructure for language resource discovery, based on the Open Archives Initiative, and called OLAC -- the Open Language Archives Community. The OLAC Metadata Set and the associated controlled vocabularies facilitate consistent description and focussed searching. We report progress on the metadata set and controlled vocabularies, describing current issues and soliciting input from the language resources community."
W01-1514,"Annotation Graphs and Servers and Multi-Modal Resources: Infrastructure for Interdisciplinary Education, Research and Development",2001,8,7,2,0,17560,christopher cieri,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"Annotation graphs and annotation servers offer infrastructure to support the analysis of human language resources in the form of time-series data such as text, audio and video. This paper outlines areas of common need among empirical linguists and computational linguists. After reviewing examples of data and tools used or under development for each of several areas, it proposes a common framework for future tool development, data annotation and resource sharing based upon annotation graphs and servers."
W01-1515,Annotation Tools Based on the Annotation Graph {API},2001,7,14,1,1,8953,steven bird,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data. This paper reports progress on a complete open-source software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data. This generalpurpose infrastructure uses annotation graphs as the underlying model, and allows developers to quickly create special-purpose annotation tools using common components. An application programming interface, an I/O library, and graphical user interfaces are described. Our experience has shown us that it is a straightforward task to create new special-purpose annotation tools based on this general-purpose infrastructure."
H01-1005,The Annotation Graph Toolkit: Software Components for Building Linguistic Annotation Tools,2001,7,9,2,0,46240,kazuaki maeda,Proceedings of the First International Conference on Human Language Technology Research,0,"Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data. This paper reports progress on a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data. This general-purpose infrastructure uses annotation graphs as the underlying model, and allows developers to quickly create special-purpose annotation tools using common components. An application programming interface, an I/O library, and graphical user interfaces are described. Our experience has shown us that it is a straightforward task to create new special-purpose annotation tools based on this general-purpose infrastructure."
bird-etal-2000-atlas,{ATLAS}: A Flexible and Extensible Architecture for Linguistic Annotation,2000,8,94,1,1,8953,steven bird,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We describe a formal model for annotating linguistic artifacts, from which we derive an application programming interface (API) to a suite of tools for manipulating these annotations. The abstract logical model provides for a range of storage formats and promotes the reuse of tools that interact through this API. We focus first on xe2x80x9cAnnotation Graphs,xe2x80x9d a graph model for annotations on linear signals (such as text and speech) indexed by intervals, for which efficient database storage and querying techniques are applicable. We note how a wide range of existing annotated corpora can be mapped to this annotation graph model. This model is then generalized to encompass a wider variety of linguistic xe2x80x9csignals,xe2x80x9d including both naturally occuring phenomena (as recorded in images, video, multi-modal interactions, etc.), as well as the derived resources that are increasingly important to the engineering of natural language processing systems (such as word lists, dictionaries, aligned bilingual corpora, etc.). We conclude with a review of the current efforts towards implementing key pieces of this architecture."
geoffrois-etal-2000-transcribing,Transcribing with Annotation Graphs,2000,6,15,3,0,34765,edouard geoffrois,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Transcriber is a tool for manual annotation of large speech files. It was originally designed for the broadcast news transcription task. The annotation file format was derived from previous formats used for this task, and many related features were hard-coded. In this paper we present a generalization of the tool based on the annotation graph formalism, and on a more modular design. This will allow us to address new tasks, while retaining Transcriberxe2x80x99s simple, crisp user-interface which is critical for user acceptance."
bird-etal-2000-towards,Towards a Query Language for Annotation Graphs,2000,16,39,1,1,8953,steven bird,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The multidimensional, heterogeneous, and temporal nature of speech databases raises interesting challenges for representation and query. Recently, annotation graphs have been proposed as a general-purpose representational framework for speech databases. Typical queries on annotation graphs require path expressions similar to those used in semistructured query languages. However, the underlying model is rather different from the customary graph models for semistructured data: the graph is acyclic and unrooted, and both temporal and inclusion relationships are important. We develop a query language and describe optimization techniques for an underlying relational representation."
graff-bird-2000-many,"Many Uses, Many Annotations for Large Speech Corpora: Switchboard and {TDT} as Case Studies",2000,5,19,2,0,18193,david graff,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper discusses the challenges that arise when large speech corpora receive an ever-broadening range of diverse and distinct annotations. Two case studies of this process are presented: the Switchboard Corpus of telephone conversations and the TDT2 corpus of broadcast news. Switchboard has undergone two independent transcriptions and various types of additional annotation, all carried out as separate projects that were dispersed both geographically and chronologically. The TDT2 corpus has also received a variety of annotations, but all directly created or managed by a core group. In both cases, issues arise involving the propagation of repairs, consistency of references, and the ability to integrate annotations having different formats and levels of detail. We describe a general framework whereby these issues can be addressed successfully."
W99-0301,Annotation Graphs as a Framework for Multidimensional Linguistic Data Analysis,1999,4,48,1,1,8953,steven bird,Towards Standards and Tools for Discourse Tagging,0,None
W97-1105,A Lexical Database Tool for Quantitative Phonological Research,1997,6,6,1,1,8953,steven bird,Computational Phonology: Third Meeting of the {ACL} Special Interest Group in Computational Phonology,0,"SIL Cameroon B.P. 1299 Yaound6, Cameroon Steven. Birdied. ac. uk I N T R O D U C T I O N A lexical database tool tailored for phonological research is described. Database fields include transcriptions, glosses and hyperlinks to speech files. Database queries are expressed using HTML forms, and these permit regular expression search on any combination of fields. Regular expressions are passed directly to a Perl CGI program, enabling the full flexibility of Perl extended regular iexpressions. The regular expression notation is extended to better support phonological searches, such as search for minimal pairs. Search results are presented fin the form of HTML or I~TEX tables, where each call is either a number (representing frequency) or a designated subset of the fields. Tables have up to four dimensions, with an elegant system for specifying iwhich fragments of which fields should be used for tile row/column labelsxe2x80xa2 The tool [ xe2x80xa2 xe2x80xa2 offers several advantages over traditional methods of xe2x80xa2 xe2x80xa2 I xe2x80xa2 analysts: (i) it suppo~s a quantitative method of doing phonological researcfi; (ii) it gives universal access to the same set of informants; (iii) it enables other r researchers to hear the original speech data without having to rely on published transcriptions; (iv) it makes the full power of regular expression search available, and search results are full multimedia documents; and (v) it enables the earl), refutation of false hypotheses, shortening the analysis-hypothesis-test loop. A lifesize application to an African tone language (Dschang) is used for exemplificgtion throughout the paper. The database contains 2200 records, each with approximately 15 fields. Running on a PC laptop with a standalone web server, the 'Dschang HyperLexicon' has already been used ex!ensively in phonological fieldwork and analysis in Cameroon. Initial stages of phonological analysis typically focus on words in isolation, as the phonemic inventory and syllable canon are established. Data is stored as a lexicon, where each word is entered as a transcription accompanied by at least a gloss (so the word can be elicited again) and the major syntactic category. In managing a lexicon, the working phonologist has a variety of computational needs: storage and retrieval; searching and sorting; tabular reports on distributions and contrasts; updates to database and to reports as distinctions are discovered or discarded. In the past the analyst had to do all this computation by hand using index cards kept in shoeboxes. But now many of these particular tasks are automated by software such as the SIL programs Shoebox (Buseman et al., 1996) and Findphone (Bevan, 1995), 1 or using commercial database packages. Of course, many tasks other than those listed above have already benefitted from (partial) automation. 2 Additionally, it has been shown how a computational inheritance model can be used for structuring lexical information relevant for phonology (Reinhard & Gibbon, 1991). And there is a body of work on the use of finite state devices closely related to regular expressions for modelling phonological phenomena (Kaplan & Kay, 1994) and for speech processing (cf. Kornai's 1Unlike regular database management systems, these include international and phonetic character sets and user-defined keystrokes for entering them, and a utility to dump a database into an RTF file in a user-defined lexicon format for use in desktop publishing. 2For example, see (Ellison, 1992; Lowe & Mazaudon, 1994; Coleman, Dirksen, Hussain & Waals, 1996)."
W94-0201,Automated Tone Transcription,1994,8,2,1,1,8953,steven bird,Computational Phonology,0,"In this paper I report on an investigation into the problem of assigning tones to pitch contours. The proposed model is intended to serve as a tool for phonologists working on instrumentally obtained pitch data from tone languages. Motivation and exemplification for the model is provided by data taken from my fieldwork on Bamileke Dschang (Cameroon). Following recent work by Liberman and others, I provide a parametrised F_0 prediction function P which generates F_0 values from a tone sequence, and I explore the asymptotic behaviour of downstep. Next, I observe that transcribing a sequence X of pitch (i.e. F_0) values amounts to finding a tone sequence T such that P(T) {}~= X. This is a combinatorial optimisation problem, for which two non-deterministic search techniques are provided: a genetic algorithm and a simulated annealing algorithm. Finally, two implementations---one for each technique---are described and then compared using both artificial and real data for sequences of up to 20 tones. These programs can be adapted to other tone languages by adjusting the F_0 prediction function."
J94-3010,Phonological Analysis in Typed Feature Systems,1994,60,48,1,1,8953,steven bird,Computational Linguistics,0,"Research on constraint-based grammar frameworks has focused on syntax and semantics largely to the exclusion of phonology. Likewise, current developments in phonology have generally ignored the technical and linguistic innovations available in these frameworks. In this paper we suggest some strategies for reuniting phonology and the rest of grammar in the context of a uniform constraint formalism. We explain why this is a desirable goal, and we present some conservative extensions to current practice in computational linguistics and in nonlinear phonology that we believe are necessary and sufficient for achieving this goal.We begin by exploring the application of typed feature logic to phonology and propose a system of prosodic types. Next, taking HPSG as an exemplar of the grammar frameworks we have in mind, we show how the phonology attribute can be enriched so that it can encode multi-tiered, hierarchical phonological representations. Finally, we exemplify the approach in some detail for the nonconcatenative morphology of Sierra Miwok and for schwa alternation in French. The approach taken in this paper lends itself particularly well to capturing phonological generalizations in terms of high-level prosodic constraints."
J94-1003,One-Level Phonology: Autosegmental Representations and Rules as Finite Automata,1994,37,99,1,1,8953,steven bird,Computational Linguistics,0,"When phonological rules are regarded as declarative descriptions, it is possible to construct a model of phonology in which rules and representations are no longer distinguished and such procedural devices as rule-ordering are absent. In this paper we present a finite-state model of phonology in which automata are the descriptions and tapes (or strings) are the objects being described. This provides the formal semantics for an autosegmental phonology without structure-changing rules. Logical operations on the phonological domain---such as conjunction, disjunction, and negation---make sense since the phonological domain consists of descriptions rather than objects. These operations as applied to automata are the straightforward operations of intersection, union, and complement. If the arrow in a rewrite rule is viewed as logical implication, then a phonological rule can also be represented as an automaton, albeit a less restrictive automaton than would be required for a lexical representaton. The model is then compared with the transducer models for autosegmental phonology of Kay (1987), Kornai (1991), and Wiebe (1992). We conclude that the declarative approach to phonology presents an attractive way of extending finite-state techniques to autosegmental phonology while remaining within the confines of regular grammar."
C92-1015,Finite-State Phonology in {HPSG},1992,25,10,1,1,8953,steven bird,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Attention on constraint-based grammar formalisms such as Head-driven Phrase Structure Grammar (HPSG) has focussed on syntax and semantics to the exclusion of phonology. This paper investigates the incorporation of a non-procedural theory of phonology into HPSG, based on the 'one-level' model of Bird & Ellison (1992). The standard rule-representation distinction is replaced by the description-object distinction which is more germane in the context of constraint-based grammar. Prosodic domains, which limit the applicability of phonological constraints, are expressed in a prosodic type hierarchy modelled on HPSG's lexical type hierarchy. Interactions between phonology and morphology and between phonology and syntax are discussed and exemplified."
E91-1016,A Logical Approach to {A}rabic Phonology,1991,19,15,1,1,8953,steven bird,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Logical approaches to linguistic description, particularly those which employ feature structures, have generally treated phonology as though it was the same as orthography. This approach breaks down for languages where the phonological shape of a morpheme can be heavily dependent on the phonological shape of another, as is the case in Arabic. In this paper we show how the tense logical approach investigated by Blackburn (1989) can be used to encode hierarchical and temporal phonological information of the kind explored by Bird (1990). Then we show how some Arabic morphemes may be represented and combined."
