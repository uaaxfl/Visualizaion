O14-3003,Resolving the Representational Problems of Polarity and Interaction between Process and State Verbs,2014,10,1,4,1,34552,shuling huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 2, June 2014",0,"Event classification is one of the crucial tasks in lexical semantic representation. Traditionally, researchers have regarded process and state as two top-level events and discriminated between them by semantic and syntactic characteristics. In this paper, we add cause-result relativity as an auxiliary criterion to discriminate between process and state by structuring about 40,000 Chinese verbs to the two correspondent event hierarchies in E-HowNet. All verbs are classified according to their semantic similarity with the corresponding conceptual types of ontology. As a result, we discover deficiencies of the dichotomy approach and point out that any discrete event classification system is insufficient to make a clear-cut classification for synonyms with slightly different semantic focuses. We then propose a solution to remedy the deficiencies of the dichotomy approach. For the process or state type mismatched verbs, their inherited semantic properties will be adjusted according to their PoS and semantic expressions to preserve their true semantic and syntactic information. Furthermore, cause-result relations will be linked between corresponding processes and states to bridge the gaps of the dichotomy approach."
D14-1100,Ambiguity Resolution for Vt-N Structures in {C}hinese,2014,14,2,3,1,34554,yuming hsieh,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"The syntactic ambiguity of a transitive verb (Vt) followed by a noun (N) has long been a problem in Chinese parsing. In this paper, we propose a classifier to resolve the ambiguity of Vt-N structures. The design of the classifier is based on three important guidelines, namely, adopting linguistically motivated features, using all available resources, and easy integration into a parsing model. The linguistically motivated features include semantic relations, context, and morphological structures; and the available resources are treebank, thesaurus, affix database, and large corpora. We also propose two learning approaches that resolve the problem of data sparseness by autoparsing and extracting relative knowledge from large-scale unlabeled data. Our experiment results show that the Vt-N classifier outperforms the current PCFG parser. Furthermore, it can be easily and effectively integrated into the PCFG parser and general statistical parsing models. Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction."
W13-4401,Keynote Speech: Lexical Semantics of {C}hinese Language,2013,0,0,1,1,39226,kehjiann chen,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"In this talk, we are going to give a systematic view of lexical semantics of Chinese language. From macro perspective point of view, lexical conceptual meanings are classified into hierarchical semantic types and each type plays some particular semantic functions of Host, Attribute, and Value to form a semantic compositional system. Lexical senses and their compositional functions will be exemplified by the semantic expressions of E-HowNet. Entities and relations are two major semantic types of the compositional system. Lexical senses and phrasal senses are compositions of these two types. From micro perspective point of view, each lexical word has individual idiosyncratic semantic contents, focuses and features. Hence words of same semantic type may have various different syntactic properties which make automatic language processing very difficult. On the other hand lexical syntactic properties are strongly influenced by lexical semantic structures. Morpho-semantic structures may systematically lead the way to derive lexical senses and syntactic behaviors of lexemes. It was observed that allowable alternations of sentence-patterns for verbs are mainly determined by their lexical semantic structures. It follows that senses and syntactic properties of out-ofvocabulary words become predictable and lexical compositional properties do shed light on automatic Chinese language understanding. Supporting evidences and logical interpretations of semantic and syntactic interactions will be presented in this talk."
W13-4404,Lexical Representation and Classification of Eventive Verbs - Polarity and Interaction between Process and State,2013,5,0,4,1,34552,shuling huang,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"Event classification is one of the most crucial tasks in lexical semantic representation. Traditionally, researchers regarded process and state as two top level events and discriminated them by semantic and syntactic characteristics. In this paper, we add cause-result relativity as an auxiliary criterion to discriminate between process and state by structuring about 40,000 Chinese verbs to the two correspondent event hierarchies in E-HowNet. All verbs are classified according to their semantic similarity with the corresponding conceptual types of the ontology. As a consequence, we discover deficiencies of the dichotomy approach and point out that any discrete event classification system is insufficient to make a clear cut classification for synonyms with slightly different semantic focuses. We then propose a solution to remedy the deficiencies of the dichotomy approach. For the process or state type mismatched verbs, their inherited semantic properties will be adjusted according to their POS and semantic expressions to preserve their true semantic and syntactic information. Furthermore, cause-result relations will be linked between corresponding processes and states to bridge the gaps of the dichotomy approach."
W13-4410,Introduction to {CKIP} {C}hinese Spelling Check System for {SIGHAN} Bakeoff 2013 Evaluation,2013,6,9,3,1,34554,yuming hsieh,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"In order to accomplish the tasks of identifying incorrect characters and error correction, we developed two error detection systems with different dictionaries. First system, called CKIP-WS, adopted the CKIP word segmentation system which based on CKIP dictionary as its core detection procedure; another system, called G1-WS, used Google 1T uni-gram data to extract pairs of potential error word and correction candidates as dictionary. Both detection systems use the confusion character set provided by the bakeoff organizer to reduce the suggested correction candidates. A simple maximizing tri-gram frequency model based on Google 1T tri-gram was designed to validate and select the correct answers. The CKIP group of Academia Sinica participated in both Sub-Task1 (Error Detection) and Sub-Task2 (Error Correction) in 2013 SIGHAN bakeoff. The evaluation results show that the performances of our systems are pretty good on both tasks."
O13-5004,A Semantic-Based Approach to Noun-Noun Compound Interpretation,2013,3,1,2,1,41522,youshan chung,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 18, Number 4, {D}ecember 2013-Special Issue on Selected Papers from {ROCLING} {XXV}",0,"In this project, we have studied Chinese noun-noun compounds (NNCs) and have found that N1 and N2 are linked either by semantic roles assigned by events (complex relations) or by static relations (simple relations), including meronymy, conjunction, and the host-attribute-value relation. Using data from the Frame Net and E-How Net, we have found that, for NNCs of either type, the major semantic relations between the two components are limited enough to allow computational implementation. Regarding simple relations, most conjunction pairs have been listed in E-How Net, and so are host-attribute-value sets. The E-How Net Taxonomy also makes identification of meronymy possible. As for NNCs involving complex relations, each component's semantic role, along with the events that assign these roles, can be restored through mappings to corresponding frame elements (FEs) in entity and to event frames and lexical units (LUs) in Frame Net's frames, respectively, that represent the concept the NNC conveys."
O13-1013,A Semantic-Based Approach to Noun-Noun Compound Interpretation,2013,3,1,2,1,41522,youshan chung,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,"In this project, we have studied Chinese noun-noun compounds (NNCs) and have found that N1 and N2 are linked either by semantic roles assigned by events (complex relations) or by static relations (simple relations), including meronymy, conjunction, and the host-attribute-value relation. Using data from the Frame Net and E-How Net, we have found that, for NNCs of either type, the major semantic relations between the two components are limited enough to allow computational implementation. Regarding simple relations, most conjunction pairs have been listed in E-How Net, and so are host-attribute-value sets. The E-How Net Taxonomy also makes identification of meronymy possible. As for NNCs involving complex relations, each component's semantic role, along with the events that assign these roles, can be restored through mappings to corresponding frame elements (FEs) in entity and to event frames and lexical units (LUs) in Frame Net's frames, respectively, that represent the concept the NNC conveys."
I13-1103,Translating {C}hinese Unknown Words by Automatically Acquired Templates,2013,14,0,3,1,34549,minghong bai,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we present a translation template model to translate Chinese unknown words. The model exploits translation templates, which are extracted automatically from a word-aligned parallel corpus, to translate unknown words. The translation templates are designed in accordance with the structure of unknown words. When an unknown word is detected during translation, the model applies translation templates to the word to get a set of matched templates, and then translates the word into a set of suggested translations. Our experiment results demonstrate that the translations suggested by the unknown word translation template model significantly improve the performance of the Moses machine translation system."
W12-6338,Improving {PCFG} {C}hinese Parsing with Context-Dependent Probability Re-estimation,2012,16,3,4,1,34554,yuming hsieh,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Selecting the best structure from several ambiguous structures produced by a syntactic parser is a challenging issue. The quality of the solution depends on the precision of the structure evaluation methods. In this paper, we propose a general model (context-dependent probability re-estimation model, CDM) to enhance the structure probabilities estimation. Compared with using rule probabilities only, the CDM has the advantage of an effective, flexible, and broader range of contexturefeature selection. We conduct experiments on the CDM parsing model by using Sinica Chinese Treebank. The results show that our proposed model significantly outperforms the baseline parser and the open source Berkeley statistical parser. More importantly, we demonstrate that the basic framework of the parsing model does not need to be changed, and the proposed re-estimation functions will adjust the probability estimation for every particular structure, and obtaining the better parsing results."
P12-3010,{DOMCAT}: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation,2012,20,0,3,1,34549,minghong bai,Proceedings of the {ACL} 2012 System Demonstrations,0,"In this paper, we propose a web-based bilingual concordancer, DOMCAT, for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively."
O12-3001,Transitivity of a {C}hinese Verb-Result Compound and Affected Argument of the Result Verb,2012,9,0,2,1,41522,youshan chung,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 17, Number 2, June 2012-Specia Issue on Selected Papers from {ROCLING} {XXIII}",0,"The Chinese verb-result compound is productive, but its meaning and syntactic behaviors have posed challenges to theoretical and automatic analyses. Theory-wise, the current study proposes that VRs have inherent affecting direction, which argument mapping principles and selectional restrictions, event structures, or the kinds of semantic/pragmatic principles or real-world knowledge proposed by previous researchers do not seem to account for. Application-wise, we predict the VR's transitivity and whether the result component is predicated of the logical subject or the logical object, based on the transitivity of individual component verbs and the selectional restrictions between component verbs and arguments. Since the transitivity property and selectional restrictions of individual verbs can be annotated in our lexicon, the rules should fare well in automatic processing. Meanwhile, as the rules have been motivated by linguistic theories and have been observed to make correct predictions in most cases, they are worthy of further large-scale testing."
N12-1036,{T}rans{A}head: A Computer-Assisted Translation and Writing Tool,2012,13,1,3,0,33416,chungchi huang,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a method for learning to predict text completion given a source text and partial translation. In our approach, predictions are offered aimed at alleviating users' burden on lexical and grammar choices, and improving productivity. The method involves learning syntax-based phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. We present a prototype writing assistant, TransAhead, that applies the method to computer-assisted translation and language learning. The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users."
O11-1009,åè£çµæ§çåç©æ§åä¿®é£¾å°è±¡ (Transitivity of a {C}hinese Verb-result Compound and Affected Argument of the Result Verb) [In {C}hinese],2011,5,0,2,1,41522,youshan chung,Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2011),0,None
C10-3012,{E}-{H}ow{N}et and Automatic Construction of a Lexical Ontology,2010,3,9,5,0,33035,weite chen,Coling 2010: Demonstrations,0,"In this paper, we propose a lexical senses representation system called E-HowNet, in which the lexical senses are defined by basic concepts. As a result, the meanings of expressions are more specific than those derived by using primitives. We also design an ontology to express the taxo-nomic relations between concepts and the attributes of concepts. To establish the taxonomic relations between word senses, we introduce a strategy that constructs the E-HowNet ontology automatically. We then implement the lexical ontology as a Web application to demonstrate the taxonomy and the search functions for querying key-terms and E-HowNet expressions in the lexicon, which contains more than 88,000 lexical senses."
Y09-1001,A Step toward Compositional Semantics: {E}-{H}ow{N}et a Lexical Semantic Representation System,2009,6,0,1,1,39226,kehjiann chen,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"The purpose of designing the lexical semantic representation model E-HowNet is for natural language understanding. E-HowNet is a frame-based entity-relation model extended from HowNet to define lexical senses and achieving compositional semantics. The followings are major extension features of E-HowNet to achieve the goal. a) Word senses (concepts) are defined by either primitives or any well-defined concepts and conceptual relations; b) A uniform sense representation model for content words, function words and phrases; c) Semantic relations are explicitly expressed; and d) Near-canonical representations for lexical senses and phrasal senses. We demonstrate the above features and show how coarse-grained semantic composition can be carried out under the framework of E-HowNet. Possible applications of E-HowNet are also suggested. We hope that the ultimate goal of natural language understanding will be accomplished after future improvement and evolution of the current E-HowNet."
O09-5001,Modeling {T}aiwanese {POS} Tagging Using Statistical Methods and {M}andarin Training Data,2009,-1,-1,5,1,47241,ungian iunn,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 14, Number 3, September 2009",0,None
O09-3002,Automatic Sense Derivation for Determinative-Measure Compounds under the Framework of {E}-{H}ow{N}et,2009,-1,-1,4,1,47255,chiahung tai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 14, Number 1, March 2009",0,None
D09-1050,Acquiring Translation Equivalences of Multiword Expressions by Normalized Correlation Frequencies,2009,18,14,3,1,34549,minghong bai,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we present an algorithm for extracting translations of any given multiword expression from parallel corpora. Given a multiword expression to be translated, the method involves extracting a short list of target candidate words from parallel corpora based on scores of normalized frequency, generating possible translations and filtering out common subsequences, and selecting the top-n possible translations using the Dice coefficient. Experiments show that our approach outperforms the word alignment-based and other naive association-based methods. We also demonstrate that adopting the extracted translations can significantly improve the performance of the Moses machine translation system."
O08-5001,Knowledge Representation and Sense Disambiguation for Interrogatives in {E}-{H}ow{N}et,2008,3,0,2,1,34552,shuling huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 3, September 2008: Special Issue on Selected Papers from {ROCLING} {XIX}",0,"In order to train machines to xe2x80x98understandxe2x80x99 natural language, we propose a meaning representation mechanism called E-HowNet to encode lexical senses. In this paper, we take interrogatives as examples to demonstrate the mechanisms of semantic representation and composition of interrogative constructions under the framework of E-HowNet. We classify the interrogative words into five classes according to their query types, and represent each type of interrogatives with fine-grained features and operators. The process of semantic composition and the difficulties of representation, such as word sense disambiguation, are addressed. Finally, machine understanding is tested by showing how machines derive the same deep semantic structure for synonymous sentences with different surface structures."
O08-2008,å¤é åæä»¶éä¹è©å½æ¦å¿µæ´å±èç¥è­æ¶æ§ä¹å»ºç« (Conceptual Expansion and Ontological Mapping of Multi-domain Documents) [In {C}hinese],2008,0,0,3,0,47959,yongxiang chen,{ROCLING} 2008 Poster Papers,0,None
O08-1002,A Semantic Composition Method for Deriving Sense Representations of Determinative-Measure Compounds in {E}-{H}ow{N}et,2008,6,0,3,1,47255,chiahung tai,Proceedings of the 20th Conference on Computational Linguistics and Speech Processing,0,"In this paper, we take Determinative-Measure Compounds as an example to demonstrate how the E-HowNet semantic composition mechanism works in deriving the sense representations for all determinative-measure (DM) compounds which is an open set. We define the sense of a closed set of each individual determinative and measure word in E-HowNet representation exhaustively. We then make semantic composition rules to produce candidate sense representations for any newly coined DM. Then we review development set to design sense disambiguation rules. We use these heuristic disambiguation rules to determine the correct context-dependent sense of a DM and its E-HowNet representation. The experiment shows that the current model reaches 88% accuracy in DM identification and sense derivation. xe9x97x9cxe9x8dxb5xe8xa9x9e:xe8xaax9exe6x84x8fxe5x90x88xe6x88x90,xe5xaex9axe9x87x8fxe8xa4x87xe5x90x88xe8xa9x9e,xe8xaax9exe6x84x8fxe8xa1xa8xe9x81x94,xe5xbbxa3xe7xbexa9xe7x9fxa5xe7xb6xb2,xe7x9fxa5xe7xb6xb2"
O08-1012,å©ç¨çµ±è¨æ¹æ³åä¸­æè¨ç·´è³æèçå°èªæè©æ§æ¨è¨ (Modeling {T}aiwanese {POS} tagging with statistical methods and {M}andarin training data) [In {C}hinese],2008,0,0,4,1,47241,ungian iunn,Proceedings of the 20th Conference on Computational Linguistics and Speech Processing,0,None
I08-2098,Resolving Ambiguities of {C}hinese Conjunctive Structures by Divide-and-conquer Approaches,2008,8,3,3,1,48621,duenchi yang,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"This paper presents a method to enhance a Chinese parser in parsing conjunctive structures. Long conjunctive structures cause long-distance dependencies and tremendous syntactic ambiguities. Pure syntactic approaches hardly can determine boundaries of conjunctive phrases properly. In this paper, we propose a divide-andconquer approach which overcomes the difficulty of data-sparseness of the training data and uses both syntactic symmetry and semantic reasonableness to evaluate ambiguous conjunctive structures. In comparing with the performances of the PCFG parser without using the divide-andconquer approach, the precision of the conjunctive boundary detection is improved from 53.47% to 83.17%, and the bracketing f-score of sentences with conjunctive structures is raised up about 11 %."
I08-1033,Improving Word Alignment by Adjusting {C}hinese Word Segmentation,2008,22,18,2,1,34549,minghong bai,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Most of the current Chinese word alignment tasks often adopt word segmentation systems firstly to identify words. However, word-mismatching problems exist between languages and will degrade the performance of word alignment. In this paper, we propose two unsupervised methods to adjust word segmentation to make the tokens 1-to-1 mapping as many as possible between the corresponding sentences. The first method is learning affix rules from a bilingual terminology bank. The second method is using the concept of impurity measure motivated by the decision tree. Our experiments showed that both of the adjusting methods improve the performance of word alignment significantly."
Y07-1013,Modality and Modal Sense Representation in {E}-{H}ow{N}et,2007,-1,-1,3,1,41522,youshan chung,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,None
O07-4005,Improve Parsing Performance by Self-Learning,2007,10,6,3,1,34554,yuming hsieh,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 12, Number 2, June 2007",0,"There are many methods to improve performance of statistical parsers. Resolving structural ambiguities is a major task of these methods. In the proposed approach, the parser produces a set of n-best trees based on a feature-extended PCFG grammar and then selects the best tree structure based on association strengths of dependency word-pairs. However, there is no sufficiently large Treebank producing reliable statistical distributions of all word-pairs. This paper aims to provide a self-learning method to resolve the problems. The word association strengths were automatically extracted and learned by parsing a giga-word corpus. Although the automatically learned word associations were not perfect, the constructed structure evaluation model improved the bracketed f-score from 83.09% to 86.59%. We believe that the above iterative learning processes can improve parsing performances automatically by learning word-dependence information continuously from web."
O07-1012,Knowledge Representation for Interrogatives in {E}-{H}ow{N}et,2007,1,0,4,1,34552,shuling huang,Proceedings of the 19th Conference on Computational Linguistics and Speech Processing,0,"In order to train machines to xe2x80x98understandxe2x80x99 natural language, we proposed a universal concept representational mechanism called E-HowNet to encode lexical semantics. In this paper, we take interrogative constructions as examples, i.e. concepts or sentences for asking questions or making inquiries, to demonstrate the mechanisms of semantic representation and composition under the framework of E-HowNet. We classify the interrogative words into five types according to their semantic distinctions, and represent each type with fine-grained features and operators. The process of semantic composition and the difficulties of the representation, such as word sense disambiguation, will be addressed. Finally, wexe2x80x99ll show how machine discriminates two synonymous sentences with different syntactic structures and surface strings to prove that machine understanding is achievable."
Y06-1052,Semantic Representation and Composition for Unknown Compounds in {E}-{H}ow{N}et,2006,5,1,3,1,30324,yuehyin shih,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"This paper describes a universal concept representational mechanism called E-HowNet, to handle difficulties caused by unknown words in natural language processing. Semantic structures and sense disambiguation of unknown words are discovered by analogy. We intend to achieve that any concept can be defined by E-HowNet and the representation is near-canonical. The design for easy semantic composition and decomposition makes the automation of semantic processing for unknown words, phrases and even sentences possible."
W06-0101,Improving Context Vector Models by Feature Clustering for Automatic Thesaurus Construction,2006,11,7,2,1,47410,jiaming you,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"Thesauruses are useful resources for NLP; however, manual construction of thesaurus is time consuming and suffers low coverage. Automatic thesaurus construction is developed to solve the problem. Conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure. But the context vector methods suffer from the problems of vast feature dimensions and data sparseness. Latent Semantic Index (LSI) was commonly used to overcome the problems. In this paper, we propose a feature clustering method to overcome the same problems. The experimental results show that it performs better than the LSI models and do enhance contextual information for infrequent words."
O06-4003,Sense Extraction and Disambiguation for {C}hinese Words from Bilingual Terminology Bank,2006,11,2,2,1,34549,minghong bai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 3, September 2006: Special Issue on Selected Papers from {ROCLING} {XVII}",0,"Using lexical semantic knowledge to solve natural language processing problems has been getting popular in recent years. Because semantic processing relies heavily on lexical semantic knowledge, the construction of lexical semantic databases has become urgent. WordNet is the most famous English semantic knowledge database at present; many researches of word sense disambiguation adopt it as a standard. Because of the success of WordNet, there is a trend to construct WordNet in different languages. In this paper, we propose a methodology for constructing Chinese WordNet by extracting information from a bilingual terminology bank. We developed an algorithm of word-to-word alignment to extract the English-Chinese translation-equivalent word pairs first. Then, the algorithm disambiguates word senses and maps Chinese word senses to WordNet synsets to achieve the goal. In the word-to-word alignment experiment, this alignment algorithm achieves the f-score of 98.4%. In the word sense disambiguation experiment, the extracted senses cover 36.89% of WordNet synsets and the accuracy of the three proposed disambiguation rules achieve the accuracies of 80%, 83% and 87%, respectively."
O06-4004,A Probe into Ambiguities of Determinative-Measure Compounds,2006,-1,-1,4,1,34603,shihmin li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 3, September 2006: Special Issue on Selected Papers from {ROCLING} {XVII}",0,None
O06-1005,Improve Parsing Performance by Self-Learning,2006,0,0,3,1,34554,yuming hsieh,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
O05-5001,The Sinica Sense Management System: Design and Implementation,2005,48,11,6,0,1504,churen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,"A sense-based lexical knowledgebase is a core foundation for language engineering. Two important criteria must be satisfied when constructing a knowledgebase: linguistic felicity and data cohesion. In this paper, we discuss how data cohesion of the sense information collected using the Sinica Sense Management System (SSMS) can be achieved. SSMS manages both lexical entries and word senses, and has been designed and implemented by the Chinese Wordnet Team at Academia Sinica. SSMS contains all the basic information that can be merged with the future Chinese Wordnet. In addition to senses and meaning facets, SSMS also includes the following information: POS, example sentences, corresponding English synset(s) from Princeton WordNet, and lexical semantic relations, such as synonym/antonym and hypernym/hyponym. Moreover, the overarching structure of the system is managed by using a sense serial number, and an inter-entry structure is established by means of cross-references among synsets and homographs. SSMS is not only a versatile development tool and management system for a sense-based lexical knowledgebase. It can also serve as the database backend for both Chinese Wordnet and any sense-based applications for Chinese language processing."
O05-5003,Feature Representations and Logical Compatibility between Temporal Adverbs and Aspects,2005,7,0,3,1,34603,shihmin li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,"In this paper, we propose clear-cut definitions of distinct temporal adverbs and provide descriptive features for each class of temporal adverbs. By measuring time points in temporal axis, we revise and reclassify the temporal adverbs listed in [Lu and Ma 1999] into four classes of semantic roles, namely, time, frequency, duration, and time manner. The descriptive features enable us to distinguish temporal relations and predict logical compatibility between temporal adverbs and aspects."
O05-1016,A Probe into Ambiguities of Determinative-Measure Compounds,2005,10,0,3,1,34603,shihmin li,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,"This paper aims to further probe into the problems of ambiguities for automatic identification of determinative-measure compounds (DMs) in Chinese and to develop sets of rules to identify DMs and their parts of speech. It is known that Chinese DMs are identifiable by regular expressions. DM rule matching helps one solve word segmentation ambiguities, and parts of speech help one improve sense recognition and part-of-speech tagging. In this paper, a deep analysis based on corpus data was studied. With analyses of error identification and disambiguation of DM compounds, the authors classified three types of ambiguities, i.e. word segmentation, sense, and pos ambiguities. DM rules are necessary complements to dictionaries and helpful to resolve word segmentation ambiguities by applying resolution principles and segmentation models. Sense and pos ambiguities are also expected to be resolved by different approaches during postprocessing."
O05-1020,å©ç¨éèªå­¸è¡åè©åº«æ½åä¸­è±å­è©äºè­¯åè©ç¾©è§£æ­§ (Sense Extraction and Disambiguation for {C}hinese Words from Bilingual Terminology Bank) [In {C}hinese],2005,0,0,2,1,34549,minghong bai,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
I05-7001,Extended-{H}ow{N}et: A Representational Framework for Concepts,2005,10,28,1,1,39226,kehjiann chen,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"Natural languages are means to denote concepts. However word sense ambiguities make natural language processing and conceptual processing almost impossible. To bridge the gaps between natural language representations and conceptual representations, we propose a universal concept representational mechanism, called Extended-HowNet, which was evolved from HowNet. It extends the word sense definition mechanism of HowNet and uses WordNet synsets as vocabulary to describe concepts. Each word sense (or concept) is defined by some simpler concepts. The simple concepts used in the definitions can be further decomposed into even simpler concepts, until primitive or basic concepts are reached. Therefore the definition of a concept can be dynamically decomposed and unified into Extended-HowNet at different levels of representations. Extended-HowNet are language independent. Any word sense of any language can be defined and achieved near-canonical representation. For any two concepts, not only their semantic distances but also their sense similarity and difference are known by checking their definitions. In addition to taxonomy links, concepts are also associated by their shared conceptual features. Fine-grain differences among near-synonyms can be differentiated by adding new features."
I05-3007,{C}hinese {S}ketch {E}ngine and the Extraction of Grammatical Collocations,2005,9,21,8,0,1504,churen huang,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper introduces a new technology for collocation extraction in Chinese. Sketch Engine (Kilgarriff et al., 2004) has proven to be a very effective tool for automatic description of lexical information, including collocation extraction, based on large-scale corpus. The original work of Sketch Engine was based on BNC. We extend Sketch Engine to Chinese based on Gigaword corpus from LDC. We discuss the available functions of the prototype Chinese Sketch Engine (CSE) as well as the robustness of language-independent adaptation of Sketch Engine. We conclude by discussing how Chinese-specific linguistic information can be incorporated to improve the"
I05-1016,"Linguistically-Motivated Grammar Extraction, Generalization and Adaptation",2005,11,8,3,1,34554,yuming hsieh,Second International Joint Conference on Natural Language Processing: Full Papers,0,"In order to obtain a high precision and high coverage grammar, we proposed a model to measure grammar coverage and designed a PCFG parser to measure efficiency of the grammar. To generalize grammars, a grammar binarization method was proposed to increase the coverage of a probabilistic context-free grammar. In the mean time linguistically-motivated feature constraints were added into grammar rules to maintain precision of the grammar. The generalized grammar increases grammar coverage from 93% to 99% and bracketing F-score from 87% to 91% in parsing Chinese sentences. To cope with error propagations due to word segmentation and part-of-speech tagging errors, we also proposed a grammar blending method to adapt to such errors. The blended grammar can reduce about 20~30% of parsing errors due to error assignment of pos made by a word segmentation system."
W04-1116,Automatic Semantic Role Assignment for a Tree Structure,2004,5,24,2,1,47410,jiaming you,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"We present an automatic semantic roles labeling system for structured trees of Chinese sentences. It adopts dependency decision making and example-based approaches. The training data and extracted examples are from the Sinica Treebank, which is a Chinese Treebank with semantic role assigned for each constituent. It used 74 abstract semantic roles including thematic roles, such as xe2x80x98agentxe2x80x99; xe2x80x98themexe2x80x99, xe2x80x98instrumentxe2x80x99, and secondary roles of xe2x80x98locationxe2x80x99, xe2x80x98timexe2x80x99, xe2x80x98mannerxe2x80x99 and roles for nominal modifiers. The design of role assignment algorithm is based on the different decision features, such as head-argument/modifier, case makers, sentence structures etc. It labels semantic roles of parsed sentences. Therefore the practical performance of the system depends on a good parser which labels the right structures of sentences. The system achieves 92.71% accuracy in labeling the semantic roles for pre-structurebracketed texts which is considerably higher than the simple method using probabilistic model of head-modifier relations."
O04-2005,Reliable and Cost-Effective Pos-Tagging,2004,6,15,2,1,51782,yufang tsai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 9, Number 1, {F}ebruary 2004: Special Issue on Selected Papers from {ROCLING} {XV}",0,"In order to achieve fast, high quality Part-of-speech (pos) tagging, algorithms should achieve high accuracy and require less manually proofreading. This study aimed to achieve these goals by defining a new criterion of tagging reliability, the estimated final accuracy of the tagging under a fixed amount of proofreading, to be used to judge how cost-effective a tagging algorithm is. In this paper, we also propose a new tagging algorithm, called the context-rule model, to achieve cost-effective tagging. The context rule model utilizes broad context information to improve tagging accuracy In experiments, we compared the tagging accuracy and reliability of the context-rule model, Markov bi-gram model and word-dependent Markov bi-gram model. The result showed that the context-rule model outperformed both Markov models. Comparing the models based on tagging accuracy, the context-rule model reduced the number of errors 20% more than the other two Markov models did. For the best cost-effective tagging algorithm to achieve 99% tagging accuracy, it was estimated that, on average, 20% of the samples of ambiguous words needed to be rechecked. We also compared tradeoff between the amount of proofreading needed and final accuracy for the different algorithms. It turns out that an algorithm with the highest accuracy may not always be the most reliable algorithm."
O04-1014,ç¾ä»£æ¼¢èªè¤ååè©ä¹è©é¦è©å°¾ç ç©¶ (Compositional Semantics of {M}andarin Affix Verbs) [In {C}hinese],2004,0,1,3,0,49512,chihming chiu,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
O04-1015,"èªæ³è¦å¾çæ½ååæ®éåèç²¾ç¢ºåçç ç©¶ (Grammar Extraction, Generalization and Specialization) [In {C}hinese]",2004,0,1,3,1,34554,yuming hsieh,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
Y03-1016,Context-rule Model for Pos Tagging,2003,4,6,2,1,51782,yufang tsai,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,"Part-of-speech tagging for a large corpus is a labour intensive and time-consuming task. In order to achieve fast and high quality tagging, algorithms should be high precision and in particular, its tagging results should require less manual proofreading. In this paper, we proposed a context-rule model to achieve both the above goals for pos tagging. We compared the tagging precisions between Markov bi-gram model and context-rule classifier. According to the experiments, context-rule classifier performs better than those two other algorithms. Also, it covers the data sparseness problem by utilizing more context features, and reduces the amount of corpus that is need to be manual proofread by introducing the confidence measure."
W03-1705,A Bottom-up Merging Algorithm for {C}hinese Unknown Word Extraction,2003,22,62,2,1,8066,weiyun ma,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Statistical methods for extracting Chinese unknown words usually suffer a problem that superfluous character strings with strong statistical associations are extracted as well. To solve this problem, this paper proposes to use a set of general morphological rules to broaden the coverage and on the other hand, the rules are appended with different linguistic and statistical constraints to increase the precision of the representation. To disambiguate rule applications and reduce the complexity of the rule matching, a bottom-up merging algorithm for extraction is proposed, which merges possible morphemes recursively by consulting above the general rules and dynamically decides which rule should be applied first according to the priorities of the rules. Effects of different priority strategies are compared in our experiment, and experimental results show that the performance of proposed method is very promising."
W03-1726,Introduction to {CKIP} {C}hinese Word Segmentation System for the First International {C}hinese Word Segmentation Bakeoff,2003,6,171,2,1,8066,weiyun ma,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future."
O03-1010,Reliable and Cost-Effective {P}o{S}-Tagging,2003,-1,-1,2,1,51782,yufang tsai,Proceedings of Research on Computational Linguistics Conference {XV},0,None
W02-1811,Design of {C}hinese Morphological Analyzer,2002,4,25,2,1,46961,huihsin tseng,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,"This is a pilot study which aims at the design of a Chinese morphological analyzer which is in state to predict the syntactic and semantic properties of nominal, verbal and adjectival compounds. Morphological structures of compound words contain the essential information of knowing their syntactic and semantic characteristics. In particular, morphological analysis is a primary step for predicting the syntactic and semantic categories of out-of-vocabulary (unknown) words. The designed Chinese morphological analyzer contains three major functions, 1) to segment a word into a sequence of morphemes, 2) to tag the part-of-speech of those morphemes, and 3) to identify the morpho-syntactic relation between morphemes. We propose a method of using associative strength among morphemes, morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech. In our evaluation report, it is found that the accuracy of our analyzer is 81%. 5% errors are caused by the segmentation and 14% errors are due to part-of-speech. Once the internal information of a compound is known, it would be beneficial for the further researches of the prediction of a word meaning and its function."
O02-2002,A Study on Word Similarity using Context Vector Models,2002,8,12,1,1,39226,kehjiann chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 7, Number 2, August 2002: Special Issue on Computational {C}hinese Lexical Semantics",0,"There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example-based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy. The taxonomy approaches are more or less semantic-based that do not consider syntactic similarities. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarities. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word, and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together."
O02-1001,ä»¥æ§è©å¾èç¸ä¼¼æ³çºæ¬çä¸­æåè©èªååé¡ç ç©¶ (A Hybrid Approach for Automatic Classification of {C}hinese Unknown Verbs) [In {C}hinese],2002,7,2,4,1,46961,huihsin tseng,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 7, Number 1, {F}ebruary 2002: Special Issue on {H}ow{N}et and Its Applications",0,None
C02-1049,Unknown Word Extraction for {C}hinese Documents,2002,17,102,1,1,39226,kehjiann chen,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"There is no blank to mark word boundaries in Chinese text. As a result, identifying words is difficult, because of segmentation ambiguities and occurrences of unknown words. Conventionally unknown words were extracted by statistical methods because statistical methods are simple and efficient. However the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall, since character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistical methods. In addition to statistical information, we try to use as much information as possible, such as morphology, syntax, semantics, and world knowledge. The identification system fully utilizes the context and content information of unknown words in the steps of detection process, extraction process, and verification process. A practical unknown word extraction system was implemented which online identifies new words, including low frequency new words, with high precision and high recall rates."
O01-1009,ä¸­æèªæåº«æ§å»ºåç®¡çç³»çµ±è¨­è¨ (Design of Management System for {C}hinese Corpus Construction) [In {C}hinese],2001,0,0,4,1,8066,weiyun ma,Proceedings of Research on Computational Linguistics Conference {XIV},0,None
O01-1012,ä¸­æåè©èªååé¡ç ç©¶ (Automatic Classification of {C}hinese Unknown Verbs) [In {C}hinese],2001,1,0,4,1,46961,huihsin tseng,Proceedings of Research on Computational Linguistics Conference {XIV},0,None
W00-1203,Knowledge Extraction for Identification of {C}hinese Organization Names,2000,10,8,1,1,39226,kehjiann chen,Second {C}hinese Language Processing Workshop,0,"In this paper, a knowledge extraction process was proposed to extract the knowledge for identifying Chinese organization names. The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts. The knowledge extraction processes were experimented on large amount of texts retrieved from WWW. With high standard of threshold values, new organization names can be identified with very high precision. Therefore the knowledge extraction processes can be carried out automatically to self improve the performance in the future."
W00-1205,"{S}inica {T}reebank: Design Criteria, Annotation Guidelines, and On-line Interface",2000,10,41,3,0.704401,1504,churen huang,Second {C}hinese Language Processing Workshop,0,"This paper describes the design criteria and annotation guidelines of Sinica Treebank. The three design criteria are: Maximal Resource Sharing, Minimal Structural Complexity, and Optimal Semantic Information. One of the important design decisions following these criteria is the encoding of thematic role information. An on-line interface facilitating empirical studies of Chinese phrase structure is also described."
O00-2001,æ¼¢èªåè©è¾­å½èªæåæï¼è¡¨éæ¨¡å¼èç ç©¶æ¹æ³ (A Lexical-Semantic Analysis of {M}andarin {C}hinese Verbs: Representation and Methodology) [In {C}hinese],2000,0,0,2,1,54364,lili chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,None
O00-2002,The Module-Attribute Representation of Verbal Semantics: From Semantic to Argument Structure,2000,0,35,4,0.704401,1504,churen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,"In this paper, we set forth a theory of lexical knowledge. We propose two types of modules: event structure modules and role modules, as well as two sets of attributes: event-internal attributes and role-internal attributes, which are linked to the event structure module and role module, respectively. These module-attribute semantic representations have associated grammatical consequences. Our data is drawn from a comprehensive corpus-based study of Mandarin Chinese verbal semantics, and four particular case studies are presented."
O00-2003,What Can Near Synonyms Tell Us,2000,-1,-1,3,0,54365,liancheng chief,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,None
O00-2004,Alternation Across Semantic Fields: A Study on {M}andarin Verbs of Emotion,2000,4,11,2,1,54364,lili chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,"This paper explores possible co-relations between lexical semantics and morpho-syntactic structures. We first examine a consistent dichotomy among verbs of emotion, which was first observed for verbs of happiness by Tsai et al. [1998]. It is shown that the dichotomy can be determined based on the criterion of whether a verb is a VV compound or not.2 The linguistic contrasts observed include: the grammatical functions of a verb as well as their distribution, the selectional restrictions the verbs impose as an adjunct, a verb's occurrences in imperative and evaluative constructions, its aktionsart, and its transitivity. We will show that the overt morpho-syntactic contrasts are due to lexical event structure properties. The description of a state (of emotion) can focus on how the state comes to be (i.e., the inchoative state) or on the being of the state (i.e., the homogeneous state). Since VV compounding has the semantic function of referring to the generic properties of the set of event tokens, it is natural for VV compounding to be chosen as the morpho-syntactic representation of homogeneity."
C00-1026,Automatic Semantic Classification for {C}hinese Unknown Compound Nouns,2000,7,16,1,1,39226,kehjiann chen,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"The paper describes a similarity-based model to present the morphological rules for Chinese compound nouns. This representation model serves functions of 1) as the morphological rules of the compounds, 2) as a mean to evaluate the properness of a compound construction, and 3) as a mean to disambiguate the semantic ambiguity of the morphological head of a compound noun. An automatic semantic classification system for Chinese unknown compounds is thus implemented based on the model. Experiments and error analyses are also presented."
Y99-1005,Alternation Across Semantic Fields : A Study of {M}andarin Verbs of Emotion,1999,-1,-1,2,1,54364,lili chang,"Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation",0,None
O99-4004,ä¸­æå¥çµæ§æ¨¹è³æåº«çæ§å»º ({S}inica {T}reebank) [In {C}hinese],1999,0,7,3,1,54218,fengyi chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 4, Number 2, August 1999",0,None
O99-1004,åè©è©æ§èèªæ³åè½äºååæ¢ (An Explorative Study on the Interaction Between Verb Compound Constructions and Syntactic Functions) [In {C}hinese],1999,0,0,2,1,54364,lili chang,Proceedings of Research on Computational Linguistics Conference {XII},0,None
P98-1038,{PAT}-Trees with the Deletion Function as the Learning Device for Linguistic Patterns,1998,6,4,1,1,39226,kehjiann chen,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"In this study, a learning device based on the PAT-tree data structures was developed. The original PAT-trees were enhanced with the deletion function to emulate human learning competence. The learning process worked as follows. The linguistic patterns from the text corpus are inserted into the PAT-tree one by one. Since the memory was limited, hopefully, the important and new patterns would be retained in the PAT-tree and the old and unimportant patterns would be released from the tree automatically. The proposed PAT-trees with the deletion function have the following advantages. 1) They are easy to construct and maintain. 2) Any prefix substring and its frequency count through PAT-tree can be searched very quickly. 3) The space requirement for a PAT-tree is linear with respect to the size of the input text. 4) The insertion of a new element can be carried out at any time without being blocked by the memory constraints because the free space is released through the deletion of unimportant elements.Experiments on learning high frequency bigrams were carried out under different memory size constraints. High recall rates were achieved. The results show that the proposed PAT-trees can be used as on-line learning devices."
O98-3002,Unknown Word Detection for {C}hinese by a Corpus-based Learning Method,1998,8,94,1,1,39226,kehjiann chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 1, {F}ebruary 1998: Special Issue on the 10th Research on Computational Linguistics International Conference",0,"One of the most prominent problems in computer processing of the Chinese language is identification of the words in a sentence. Since there are no blanks to mark word boundaries, identifying words is difficult because of segmentation ambiguities and occurrences of out-of-vocabulary words (i.e., unknown words). In this paper, a corpus-based learning method is proposed which derives sets of syntactic rules that are applied to distinguish monosyllabic words from monosyllabic morphemes which may be parts of unknown words or typographical errors. The corpus-based learning approach has the advantages of: 1. automatic rule learning, 2. automatic evaluation of the performance of each rule, and 3. balancing of recall and precision rates through dynamic rule set selection. The experimental results show that the rule set derived using the proposed method outperformed hand-crafted rules produced by human experts in detecting unknown words."
O98-3004,Towards a Representation of Verbal Semantics {--} An Approach Based on Near-Synonyms,1998,7,22,3,1,50914,meichih tsai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 1, {F}ebruary 1998: Special Issue on the 10th Research on Computational Linguistics International Conference",0,"In this paper we propose using the distributional differences in the syntactic patterns of near-synonyms to deduce the relevant components of verb meaning. Our method involves determining the distributional differences in syntactic patterns, deducing the semantic features from the syntactic phenomena, and testing the semantic features in new syntactic frames. We determine the distributional differences in syntactic patterns through the following five steps: First, we search for all instances of the verb in the corpus. Second, we classify each of these instances into its type of syntactic function. Third, we classify each of these instances into its argument structure type. Fourth, we determine the aspectual type that is associated with each verb. Lastly, we determine each verb's sentential type. Once the distributional differences have been determined, then the relevant semantic features are postulated. Our goal is to tease out the lexical semantic features as the explanation, and as the motivation of the syntactic contrasts."
O98-1002,ä»¥èªå¢å¤å®ä¸­ææªç¥è©è©é¡çæ¹æ³ (Guessing Parts-Of-Speech For {C}hinese Unknown Words Using Context Information) [In {C}hinese],1998,0,0,3,1,34549,minghong bai,Proceedings of Research on Computational Linguistics Conference {XI},0,None
O98-1004,Quantitative Criteria for Computational {C}hinese Lexicography,1998,-1,-1,4,1,1504,churen huang,Proceedings of Research on Computational Linguistics Conference {XI},0,None
C98-1038,{PAT}-Trees with the Deletion Function as the Learning Device for Linguistic Patterns,1998,6,4,1,1,39226,kehjiann chen,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"In this study, a learning device based on the PAT-tree data structures was developed. The original PAT-trees were enhanced with the deletion function to emulate human learning competence. The learning process worked as follows. The linguistic patterns from the text corpus are inserted into the PAT-tree one by one. Since the memory was limited, hopefully, the important and new patterns would be retained in the PAT-tree and the old and unimportant patterns would be released from the tree automatically. The proposed PAT-trees with the deletion function have the following advantages. 1) They are easy to construct and maintain. 2) Any prefix substring and its frequency count through PAT-tree can be searched very quickly. 3) The space requirement for a PAT-tree is linear with respect to the size of the input text. 4) The insertion of a new element can be carried out at any time without being blocked by the memory constraints because the free space is released through the deletion of unimportant elements.Experiments on learning high frequency bigrams were carried out under different memory size constraints. High recall rates were achieved. The results show that the proposed PAT-trees can be used as on-line learning devices."
O97-4003,Segmentation Standard for {C}hinese Natural Language Processing,1997,0,36,2,1,1504,churen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 2, August 1997",0,None
O97-1001,Meaning Representation and Meaning Instantiation for {C}hinese Nominals,1997,-1,-1,3,0.625,15811,kathleen ahrens,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
O97-1003,Towards a Representation of Verbal Semantics {--} An Approach Based on Near Synonyms,1997,-1,-1,3,1,50914,meichih tsai,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
O97-1011,Unknown Word Detection for {C}hinese by a Corpus-based Learning Method,1997,-1,-1,1,1,39226,kehjiann chen,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
Y96-1018,{S}INICA {C}ORPUS : Design Methodology for Balanced Corpora,1996,9,131,1,1,39226,kehjiann chen,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"The Academia Sinica Balanced Corpus (Sinica Corpus) is the first balanced Chinese corpus with part-of-speech tagging. The corpus (Sinica 2.0) is open to the research community through the WWW (http://www.sinica.edu.twiftmsbinikiwi.sh). Current size of the corpus is 3.5 million words, and the immediate expansion target is five million words. Each text in the corpus is classified and marked according to five criteria: genre, style, mode, topic, and source. The feature values of these classifications are assigned in a hierarchy. Subcorpora can be defined with a specific set of attributes to serve different research purposes. Texts in the corpus are segmented according to the word segmentation standard proposed by the ROC Computational Linguistic Society. Each segmented word is tagged with its part-of-speech. Linguistic patterns and language structures can be extracted from the tagged corpus via a corpus inspection program which has the functions of KWIC searching, filtering, statistics, printing, and collocation."
O96-2006,A Model for Robust {C}hinese Parser,1996,18,11,1,1,39226,kehjiann chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 1, Number 1, August 1996",0,"The Chinese language has many special characteristics which are substantially different from western languages, causing conventional methods of language processing to fail on Chinese. For example, Chinese sentences are composed of strings of characters without word boundaries that are marked by spaces. Therefore, word segmentation and unknown word identification techniques must be used in order to identify words in Chinese. In addition, Chinese has very few inflectional or grammatical markers, making purely syntactic approaches to parsing almost impossible. Hence, a unified approach which involves both syntactic and semantic information must be used. Therefore, a lexical feature-based grammar formalism, called Information-based Case Grammar, is adopted for the parsing model proposed here. This grammar formalism stipulates that a lexical entry for a word contains both semantic and syntactic feature structures. By relaxing the constraints on lexical feature structures, even ill-formed input can be accepted, broadening the coverage of the grammar. A model of a priority controlled chart parser is proposed which, in conjunction with a mechanism of dynamic grammar extension, addresses the problems of: (1) syntactic ambiguities, (2) under-specification and limited coverage of grammars, and (3) ill-formed sentences. The model does this without causing inefficient parsing of sentences that do not require relaxation of constraints or dynamic extension of the grammar."
O96-1006,"åèªèªé³è¾¨èªä¸­å¤é åèªè¨æ¨¡åä¹è¨ç·´ãåµæ¸¬èèª¿é© (Training, Detection and Adaptation of Multi-Domain Language Models for {M}andarin Speech Recognition) [In {C}hinese]",1996,0,0,4,1,51783,sungchen lin,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,None
O96-1009,"èªæåº«å¨è¾­å\
¸ç·¨è¼¯ä¸çéç¨ (The Application of Language Corpus on Dictionary Editing) [In {C}hinese]",1996,0,1,2,1,54364,lili chang,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,None
O96-1010,èªæåº«çºæ¬çèªç¾©è¨æ¯æ½åèè¾¨æä»¥è¿ç¾©è©ç ç©¶çºä¾ (Synonym Discrimination Based on Corpus) [In {C}hinese],1996,-1,-1,3,1,50914,meichih tsai,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,None
C96-2184,Segmentation Standard for {C}hinese Natural Language Processing,1996,13,20,2,1,1504,churen huang,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper proposes a segmentation standard for Chinese natural language processing. The standard is proposed to achieve linguistic felicity, computational feasibility, and data uniformity. Linguistic felicity is maintained by defining a segmentation unit to be equivalent to the theoretical definition of word, and by providing a set of segmentation principles that are equivalent to a functional definition of a word. Computational feasibility is ensured by the fact that the above functional definitions are procedural in nature and can be converted to segmentation algorithms, as well as by the implementable heuristic guidelines which deal with specific linguistic categories. Data uniformity is achieved by stratification of the standard itself and by defining a standard lexicon as part of the segmentation standard."
O94-1001,åèªèªé³è¾¨èªä¸­è©ç¾¤èªè¨æ¨¡åä¹åç¾¤æ¹æ³èæç¨ (Methodology Implementation and Application of Word-Class Based Language Model in {M}andarin Speech Recognition) [In {C}hinese],1994,0,0,4,0,55929,yuancheng chang,Proceedings of Rocling {VII} Computational Linguistics Conference {VII},0,None
O94-1005,A Practical Tagger for {C}hinese Corpora,1994,0,5,1,1,39226,kehjiann chen,Proceedings of Rocling {VII} Computational Linguistics Conference {VII},0,None
C94-1088,Character-based Collocation for {M}andarin {C}hinese,1994,9,6,2,1,1504,churen huang,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
O93-1005,åèªèªé³è¾¨èªä¸­è©ç¾¤éé£èªè¨æ¨¡åçè§£ç¢¼æ¹æ³ (A Word-Class Bigram Approach to Linguistic Decoding in {M}andarin Speech Recognition) [In {C}hinese],1993,0,0,3,1,51783,sungchen lin,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O93-1010,ä¸­ææä»¶èªååé¡ä¹ç ç©¶ (A Study of Document Auto-Classification in {M}andarin {C}hinese) [In {C}hinese],1993,0,0,4,1,47241,ungian iunn,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O92-1005,"æ¼¢èªçåè©åç©ååæ¢{--}æ¼¢èªä¸­å¸¶è«å\
çåç©åæ´¾çåè© (A preliminary study on Nominalization in {M}andarin {C}hinese {--} argument-taking deverbal nouns) [In {C}hinese]",1992,-1,-1,4,0,56943,marie yeh,Proceedings of Rocling V Computational Linguistics Conference V,0,None
O92-1007,"Reduplication In {M}andarin {C}hinese: Their Formation Rules, Syntactic Behavior And {ICG} Representation",1992,0,2,4,1,54218,fengyi chen,Proceedings of Rocling V Computational Linguistics Conference V,0,None
C92-4194,A {C}hinese Corpus for Linguistic Research,1992,4,16,2,1,1504,churen huang,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,This is a project note on the first stage of the construction of a comprehensive corpus of both Modern and Classical Chinese. The corpus is built with the dual aim of serving as the central database for Chinese language processing and for supporting in-depth linguistic research in Mandarin Chinese.
C92-1019,Word Identification for {M}andarin {C}hinese Sentences,1992,5,162,1,1,39226,kehjiann chen,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Chinese sentences are composed with string of characters without blanks to mark words. However the basic unit for sentence parsing and understanding is word. Therefore the first step of processing Chinese sentences is to identify the words. The difficulties of identifying words include (1) the identification of complex words, such as Determinative-Measure, reduplications, derived words etc., (2) the identification of proper names, (3) resolving the ambiguous segmentations. In this paper, we propose the possible solutions for the above difficulties. We adopt a matching algorithm with 6 different heuristic rules to resolve the ambiguities and achieve an 99.77% of the success rate. The statistical data supports that the maximal matching algorithm is the most effective heuristics."
O91-1001,é£æ¥è©çèªæ³è¡¨éæ¨¡å¼ï¼ä»¥ä¸­æè¨æ¯æ ¼ä½èªæ³({ICG})çºæ¬çè¡¨éå½¢å¼ (The Grammar Representation of Conjunctions {--} a Representation Based on {ICG}) [In {C}hinese],1991,0,0,2,0,57285,wenjen wei,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
O91-1003,Determinative-Measure Compounds in {M}andarin {C}hinese Formation Rules and Parser Implementation,1991,0,10,3,0,56945,ruoping mo,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
O90-1006,è©å½è¨æ¯çå±¤æ¬¡è¡¨éèç®¡ç (Hierarchical Representation of Word Information and Management) [In {C}hinese],1990,-1,-1,2,0.705128,50944,leefeng chien,Proceedings of Rocling {III} Computational Linguistics Conference {III},0,None
C90-2010,Information-based Case Grammar,1990,16,15,1,1,39226,kehjiann chen,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper we propose a framework of Information-based Case Grammar (ICG). This grammatical formalism entails that the lexical entry for each word contain both semantic and syntactic feature structures. In the feature structure of a phrasal head, we encode syntactic and semantic constraints on grammatical phrasal patterns in terms of thematic structures, and encode the precedence relations in terms of adjunct structures. Such feature structures denote partial information which defines the set of legal phrases. They also provide sufficient information to identify thematic roles. With this formalism, parsing and thematic analysis can be achieved simultaneously. Due to the simplicity and flexibility of Information-based Case Grammar, context dependent and discontinuous relations such as agreements, coordinations, long-distance dependencies, and control and binding, can be easily expressed. ICG is a kind of unification-based formalism. Therefore it inherits the advantages of unification-based formalisms and more."
O89-1001,The Identification Of Thematic Roles In Parsing {M}andarin {C}hinese,1989,0,0,1,1,39226,kehjiann chen,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
