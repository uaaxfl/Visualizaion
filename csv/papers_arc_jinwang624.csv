2021.semeval-1.58,{YNU}-{HPCC} at {S}em{E}val-2021 Task 11: Using a {BERT} Model to Extract Contributions from {NLP} Scholarly Articles,2021,-1,-1,2,0,1793,xinge ma,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper describes the system we built as the YNU-HPCC team in the SemEval-2021 Task 11: NLPContributionGraph. This task involves first identifying sentences in the given natural language processing (NLP) scholarly articles that reflect research contributions through binary classification; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling; and finally, these scientific terms and relation phrases are categorized, identified, and organized into subject-predicate-object triples to form a knowledge graph with the help of multiclass classification and multi-label classification. We developed a system for this task using a pre-trained language representation model called BERT that stands for Bidirectional Encoder Representations from Transformers, and achieved good results. The average F1-score for Evaluation Phase 2, Part 1 was 0.4562 and ranked 7th, and the average F1-score for Evaluation Phase 2, Part 2 was 0.6541, and also ranked 7th."
2021.semeval-1.112,{YNU}-{HPCC} at {S}em{E}val-2021 Task 5: Using a Transformer-based Model with Auxiliary Information for Toxic Span Detection,2021,-1,-1,2,0,1932,ruijun chen,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Toxic span detection requires the detection of spans that make a text toxic instead of simply classifying the text. In this paper, a transformer-based model with auxiliary information is proposed for SemEval-2021 Task 5. The proposed model was implemented based on the BERT-CRF architecture. It consists of three parts: a transformer-based model that can obtain the token representation, an auxiliary information module that combines features from different layers, and an output layer used for the classification. Various BERT-based models, such as BERT, ALBERT, RoBERTa, and XLNET, were used to learn contextual representations. The predictions of these models were assembled to improve the sequence labeling tasks by using a voting strategy. Experimental results showed that the introduced auxiliary information can improve the performance of toxic spans detection. The proposed model ranked 5th of 91 in the competition. The code of this study is available at https://github.com/Chenrj233/semeval2021{\_}task5"
2021.semeval-1.144,{YNU}-{HPCC} at {S}em{E}val-2021 Task 6: Combining {ALBERT} and Text-{CNN} for Persuasion Detection in Texts and Images,2021,-1,-1,2,0,2021,xingyu zhu,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In recent years, memes combining image and text have been widely used in social media, and memes are one of the most popular types of content used in online disinformation campaigns. In this paper, our study on the detection of persuasion techniques in texts and images in SemEval-2021 Task 6 is summarized. For propaganda technology detection in text, we propose a combination model of both ALBERT and Text CNN for text classification, as well as a BERT-based multi-task sequence labeling model for propaganda technology coverage span detection. For the meme classification task involved in text understanding and visual feature extraction, we designed a parallel channel model divided into text and image channels. Our method achieved a good performance on subtasks 1 and 3. The micro F1-scores of 0.492, 0.091, and 0.446 achieved on the test sets of the three subtasks ranked 12th, 7th, and 11th, respectively, and all are higher than the baseline model."
2021.semeval-1.184,{YNU}-{HPCC} at {S}em{E}val-2021 Task 10: Using a Transformer-based Source-Free Domain Adaptation Model for Semantic Processing,2021,-1,-1,2,0,2096,zhewen yu,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Data sharing restrictions are common in NLP datasets. The purpose of this task is to develop a model trained in a source domain to make predictions for a target domain with related domain data. To address the issue, the organizers provided the models that fine-tuned a large number of source domain data on pre-trained models and the dev data for participants. But the source domain data was not distributed. This paper describes the provided model to the NER (Name entity recognition) task and the ways to develop the model. As a little data provided, pre-trained models are suitable to solve the cross-domain tasks. The models fine-tuned by large number of another domain could be effective in new domain because the task had no change."
2021.rocling-1.51,{ROCLING}-2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts,2021,-1,-1,2,0,2441,liangchih yu,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper presents the ROCLING 2021 shared task on dimensional sentiment analysis for educational texts which seeks to identify a real-value sentiment score of self-evaluation comments written by Chinese students in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 7 teams registered for this shared task for two-dimensional sentiment analysis, 6 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques for the educational domain. All data sets with gold standards and scoring script are made publicly available to researchers."
2021.findings-acl.206,{MA}-{BERT}: Learning Representation by Incorporating Multi-Attribute Knowledge in Transformers,2021,-1,-1,2,1,8009,you zhang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.semeval-1.110,{YNU}-{HPCC} at {S}em{E}val-2020 Task 7: Using an Ensemble {B}i{GRU} Model to Evaluate the Humor of Edited News Titles,2020,-1,-1,2,0,15148,joseph tomasulo,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes an ensemble model designed for Semeval-2020 Task 7. The task is based on the Humicroedit dataset that is comprised of news titles and one-word substitutions designed to make them humorous. We use BERT, FastText, Elmo, and Word2Vec to encode these titles then pass them to a bidirectional gated recurrent unit (BiGRU) with attention. Finally, we used XGBoost on the concatenation of the results of the different models to make predictions."
2020.semeval-1.116,{YNU}-{HPCC} at {S}em{E}val-2020 Task 8: Using a Parallel-Channel Model for Memotion Analysis,2020,-1,-1,2,0,15153,li yuan,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"this paper proposed a parallel-channel model to process the textual and visual information in memes and then analyze the sentiment polarity of memes. In the shared task of identifying and categorizing memes, we preprocess the dataset according to the language behaviors on social media. Then, we adapt and fine-tune the Bidirectional Encoder Representations from Transformers (BERT), and two types of convolutional neural network models (CNNs) were used to extract the features from the pictures. We applied an ensemble model that combined the BiLSTM, BIGRU, and Attention models to perform cross domain suggestion mining. The officially released results show that our system performs better than the baseline algorithm"
2020.semeval-1.120,{HPCC}-{YNU} at {S}em{E}val-2020 Task 9: A Bilingual Vector Gating Mechanism for Sentiment Analysis of Code-Mixed Text,2020,-1,-1,2,0,15158,jun kong,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"It is fairly common to use code-mixing on a social media platform to express opinions and emotions in multilingual societies. The purpose of this task is to detect the sentiment of code-mixed social media text. Code-mixed text poses a great challenge for the traditional NLP system, which currently uses monolingual resources to deal with the problem of multilingual mixing. This task has been solved in the past using lexicon lookup in respective sentiment dictionaries and using a long short-term memory (LSTM) neural network for monolingual resources. In this paper, we present a system that uses a bilingual vector gating mechanism for bilingual resources to complete the task. The model consists of two main parts: the vector gating mechanism, which combines the character and word levels, and the attention mechanism, which extracts the important emotional parts of the text. The results show that the proposed system outperforms the baseline algorithm. We achieved fifth place in Spanglish and 19th place in Hinglish."
2020.semeval-1.197,{YNU}-{HPCC} at {S}em{E}val-2020 Task 11: {LSTM} Network for Detection of Propaganda Techniques in News Articles,2020,-1,-1,2,0,15283,jiaxu dao,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper summarizes our studies on propaganda detection techniques for news articles in the SemEval-2020 task 11. This task is divided into the SI and TC subtasks. We implemented the GloVe word representation, the BERT pretraining model, and the LSTM model architecture to accomplish this task. Our approach achieved good results for both the SI and TC subtasks. The macro- F 1 - score for the SI subtask is 0.406, and the micro- F 1 - score for the TC subtask is 0.505. Our method significantly outperforms the officially released baseline method, and the SI and TC subtasks rank 17th and 22nd, respectively, for the test set. This paper also compares the performances of different deep learning model architectures, such as the Bi-LSTM, LSTM, BERT, and XGBoost models, on the detection of news promotion techniques."
2020.semeval-1.224,{YNU}-{HPCC} at {S}em{E}val-2020 Task 10: Using a Multi-granularity Ordinal Classification of the {B}i{LSTM} Model for Emphasis Selection,2020,-1,-1,2,0,15316,dawei liao,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In this study, we propose a multi-granularity ordinal classification method to address the problem of emphasis selection. In detail, the word embedding is learned from Embeddings from Language Model (ELMO) to extract feature vector representation. Then, the ordinal classifica-tions are implemented on four different multi-granularities to approximate the continuous em-phasize values. Comparative experiments were conducted to compare the model with baseline in which the problem is transformed to label distribution problem."
2020.aacl-main.4,Graph Attention Network with Memory Fusion for Aspect-level Sentiment Analysis,2020,-1,-1,2,0,15153,li yuan,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Aspect-level sentiment analysis(ASC) predicts each specific aspect term{'}s sentiment polarity in a given text or review. Recent studies used attention-based methods that can effectively improve the performance of aspect-level sentiment analysis. These methods ignored the syntactic relationship between the aspect and its corresponding context words, leading the model to focus on syntactically unrelated words mistakenly. One proposed solution, the graph convolutional network (GCN), cannot completely avoid the problem. While it does incorporate useful information about syntax, it assigns equal weight to all the edges between connected words. It may still incorrectly associate unrelated words to the target aspect through the iterations of graph convolutional propagation. In this study, a graph attention network with memory fusion is proposed to extend GCN{'}s idea by assigning different weights to edges. Syntactic constraints can be imposed to block the graph convolutional propagation of unrelated words. A convolutional layer and a memory fusion were applied to learn and exploit multiword relations and draw different weights of words to improve performance further. Experimental results on five datasets show that the proposed method yields better performance than existing methods."
S19-2063,{YUN}-{HPCC} at {S}em{E}val-2019 Task 3: Multi-Step Ensemble Neural Network for Sentiment Analysis in Textual Conversation,2019,0,0,2,0,25008,dawei li,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This paper describes our approach to the sentiment analysis of Twitter textual conversations based on deep learning. We analyze the syntax, abbreviations, and informal-writing of Twitter; and perform perfect data preprocessing on the data to convert them to normative text. We apply a multi-step ensemble strategy to solve the problem of extremely unbalanced data in the training set. This is achieved by taking the GloVe and Elmo word vectors as input into a combination model with four different deep neural networks. The experimental results from the development dataset demonstrate that the proposed model exhibits a strong generalization ability. For evaluation on the best dataset, we integrated the results using the stacking ensemble learning approach and achieved competitive results. According to the final official review, the results of our model ranked 10th out of 165 teams."
S19-2142,{YNU}-{HPCC} at {S}em{E}val-2019 Task 6: Identifying and Categorising Offensive Language on {T}witter,2019,0,2,2,0,25107,chengjin zhou,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"This document describes the submission of team YNU-HPCC to SemEval-2019 for three Sub-tasks of Task 6: Sub-task A, Sub-task B, and Sub-task C. We have submitted four systems to identify and categorise offensive language. The first subsystem is an attention-based 2-layer bidirectional long short-term memory (BiLSTM). The second subsystem is a voting ensemble of four different deep learning architectures. The third subsystem is a stacking ensemble of four different deep learning architectures. Finally, the fourth subsystem is a bidirectional encoder representations from transformers (BERT) model. Among our models, in Sub-task A, our first subsystem performed the best, ranking 16th among 103 teams; in Sub-task B, the second subsystem performed the best, ranking 12th among 75 teams; in Sub-task C, the fourth subsystem performed best, ranking 4th among 65 teams."
S19-2207,{YNU}-{HPCC} at {S}em{E}val-2019 Task 8: Using A {LSTM}-Attention Model for Fact-Checking in Community Forums,2019,0,0,2,0,14523,peng liu,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We propose a system that uses a long short-term memory with attention mechanism (LSTM-Attention) model to complete the task. The LSTM-Attention model uses two LSTM to extract the features of the question and answer pair. Then, each of the features is sequentially composed using the attention mechanism, concatenating the two vectors into one. Finally, the concatenated vector is used as input for the MLP and the MLP{'}s output layer uses the softmax function to classify the provided answers into three categories. This model is capable of extracting the features of the question and answer pair well. The results show that the proposed system outperforms the baseline algorithm."
S19-2224,{YNU}-{HPCC} at {S}em{E}val-2019 Task 9: Using a {BERT} and {CNN}-{B}i{LSTM}-{GRU} Model for Suggestion Mining,2019,0,0,2,0,25209,ping yue,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"Consumer opinions towards commercial entities are generally expressed through online reviews, blogs, and discussion forums. These opinions largely express positive and negative sentiments towards a given entity,but also tend to contain suggestions for improving the entity. In this task, we extract suggestions from given the unstructured text, compared to the traditional opinion mining systems. Such suggestion mining is more applicability and extends capabilities."
D19-1343,Investigating Dynamic Routing in Tree-Structured {LSTM} for Sentiment Analysis,2019,0,3,1,1,1794,jin wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Deep neural network models such as long short-term memory (LSTM) and tree-LSTM have been proven to be effective for sentiment analysis. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the parse tree will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement."
S18-1040,{YNU}-{HPCC} at {S}em{E}val-2018 Task 1: {B}i{LSTM} with Attention based Sentiment Analysis for Affect in Tweets,2018,0,1,2,1,8009,you zhang,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"We implemented the sentiment system in all five subtasks for English and Spanish. All subtasks involve emotion or sentiment intensity prediction (regression and ordinal classification) and emotions determining (multi-labels classification). The useful BiLSTM (Bidirectional Long-Short Term Memory) model with attention mechanism was mainly applied for our system. We use BiLSTM in order to get word information extracted from both directions. The attention mechanism was used to find the contribution of each word for improving the scores. Furthermore, based on BiLSTMATT (BiLSTM with attention mechanism) a few deep-learning algorithms were employed for different subtasks. For regression and ordinal classification tasks we used domain adaptation and ensemble learning methods to leverage base model. While a single base model was used for multi-labels task."
S18-1073,{YNU}-{HPCC} at {S}em{E}val-2018 Task 2: Multi-ensemble {B}i-{GRU} Model with Attention Mechanism for Multilingual Emoji Prediction,2018,0,0,2,0.666667,16066,nan wang,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our approach to SemEval-2018 Task 2, which aims to predict the most likely associated emoji, given a tweet in English or Spanish. We normalized text-based tweets during pre-processing, following which we utilized a bi-directional gated recurrent unit with an attention mechanism to build our base model. Multi-models with or without class weights were trained for the ensemble methods. We boosted models without class weights, and only strong boost classifiers were identified. In our system, not only was a boosting method used, but we also took advantage of the voting ensemble method to enhance our final system result. Our method demonstrated an obvious improvement of approximately 3{\%} of the macro F1 score in English and 2{\%} in Spanish."
S18-1101,{YNU}-{HPCC} at {S}em{E}val-2018 Task 3: Ensemble Neural Network Models for Irony Detection on {T}witter,2018,0,1,2,0.75,2442,bo peng,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describe the system we proposed to participate the first year of Irony detection in English tweets competition. Previous works demonstrate that LSTMs models have achieved remarkable performance in natural language processing; besides, combining multiple classification from various individual classifiers in general is more powerful than a single classification. In order to obtain more precision classification of irony detection, our system trained several individual neural network classifiers and combined their results according to the ensemble-learning algorithm."
S18-1177,{YNU}-{HPCC} at {S}emeval-2018 Task 11: Using an Attention-based {CNN}-{LSTM} for Machine Comprehension using Commonsense Knowledge,2018,0,0,2,1,28917,hang yuan,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This shared task is a typical question answering task. Compared with the normal question and answer system, it needs to give the answer to the question based on the text provided. The essence of the problem is actually reading comprehension. Typically, there are several questions for each text that correspond to it. And for each question, there are two candidate answers (and only one of them is correct). To solve this problem, the usual approach is to use convolutional neural networks (CNN) and recurrent neural network (RNN) or their improved models (such as long short-term memory (LSTM)). In this paper, an attention-based CNN-LSTM model is proposed for this task. By adding an attention mechanism and combining the two models, this experimental result has been significantly improved."
S18-1187,{YNU}-{HPCC} at {S}em{E}val-2018 Task 12: The Argument Reasoning Comprehension Task Using a Bi-directional {LSTM} with Attention Model,2018,0,0,3,0,28923,quanlei liao,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"An argument is divided into two parts, the claim and the reason. To obtain a clearer conclusion, some additional explanation is required. In this task, the explanations are called warrants. This paper introduces a bi-directional long short term memory (Bi-LSTM) with an attention model to select a correct warrant from two to explain an argument. We address this question as a question-answering system. For each warrant, the model produces a probability that it is correct. Finally, the system chooses the highest correct probability as the answer. Ensemble learning is used to enhance the performance of the model. Among all of the participants, we ranked 15th on the test results."
W17-5227,{YNU}-{HPCC} at {E}mo{I}nt-2017: Using a {CNN}-{LSTM} Model for Sentiment Intensity Prediction,2017,8,5,3,1,8009,you zhang,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"In this paper, we present a system that uses a convolutional neural network with long short-term memory (CNN-LSTM) model to complete the task. The CNN-LSTM model has two combined parts: CNN extracts local n-gram features within tweets and LSTM composes the features to capture long-distance dependency across tweets. Additionally, we used other three models (CNN, LSTM, BiLSTM) as baseline algorithms. Our introduced model showed good performance in the experimental results."
S17-2134,{YNU}-{HPCC} at {S}em{E}val 2017 Task 4: Using A Multi-Channel {CNN}-{LSTM} Model for Sentiment Classification,2017,0,4,2,0,32360,haowei zhang,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper, we propose a multi-channel convolutional neural network-long short-term memory (CNN-LSTM) model that consists of two parts: multi-channel CNN and LSTM to analyze the sentiments of short English messages from Twitter. Un-like a conventional CNN, the proposed model applies a multi-channel strategy that uses several filters of different length to extract active local n-gram features in different scales. This information is then sequentially composed using LSTM. By combining both CNN and LSTM, we can consider both local information within tweets and long-distance dependency across tweets in the classification process. Officially released results show that our system outperforms the baseline algo-rithm."
I17-4002,{IJCNLP}-2017 Task 2: Dimensional Sentiment Analysis for {C}hinese Phrases,2017,0,0,3,0,2441,liangchih yu,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 19 teams registered for this shared task for two-dimensional sentiment analysis, 13 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing. All data sets with gold standards and scoring script are made publicly available to researchers."
I17-4011,{YNU}-{HPCC} at {IJCNLP}-2017 Task 1: {C}hinese Grammatical Error Diagnosis Using a Bi-directional {LSTM}-{CRF} Model,2017,0,0,2,0,28923,quanlei liao,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"Building a system to detect Chinese grammatical errors is a challenge for natural-language processing researchers. As Chinese learners are increasing, developing such a system can help them study Chinese more easily. This paper introduces a bi-directional long short-term memory (BiLSTM) - conditional random field (CRF) model to produce the sequences that indicate an error type for every position of a sentence, since we regard Chinese grammatical error diagnosis (CGED) as a sequence-labeling problem."
I17-4029,{YNU}-{HPCC} at {IJCNLP}-2017 Task 4: Attention-based Bi-directional {GRU} Model for Customer Feedback Analysis Task of {E}nglish,2017,0,2,2,0.666667,16066,nan wang,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"This paper describes our submission to IJCNLP 2017 shared task 4, for predicting the tags of unseen customer feedback sentences, such as comments, complaints, bugs, requests, and meaningless and undetermined statements. With the use of a neural network, a large number of deep learning methods have been developed, which perform very well on text classification. Our ensemble classification model is based on a bi-directional gated recurrent unit and an attention mechanism which shows a 3.8{\%} improvement in classification accuracy. To enhance the model performance, we also compared it with several word-embedding models. The comparative results show that a combination of both word2vec and GloVe achieves the best performance."
I17-4035,{YNU}-{HPCC} at {IJCNLP}-2017 Task 5: Multi-choice Question Answering in Exams Using an Attention-based {LSTM} Model,2017,0,0,3,1,28917,hang yuan,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"A shared task is a typical question answering task that aims to test how accurately the participants can answer the questions in exams. Typically, for each question, there are four candidate answers, and only one of the answers is correct. The existing methods for such a task usually implement a recurrent neural network (RNN) or long short-term memory (LSTM). However, both RNN and LSTM are biased models in which the words in the tail of a sentence are more dominant than the words in the header. In this paper, we propose the use of an attention-based LSTM (AT-LSTM) model for these tasks. By adding an attention mechanism to the standard LSTM, this model can more easily capture long contextual information."
D17-1056,Refining Word Embeddings for Sentiment Analysis,2017,3,41,2,0,2441,liangchih yu,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST)."
W16-4920,{C}hinese Grammatical Error Diagnosis Using Single Word Embedding,2016,0,1,3,0,32802,jinnan yang,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"Abstract Automatic grammatical error detection for Chinese has been a big challenge for NLP researchers. Due to the formal and strict grammar rules in Chinese, it is hard for foreign students to master Chinese. A computer-assisted learning tool which can automatically detect and correct Chinese grammatical errors is necessary for those foreign students. Some of the previous works have sought to identify Chinese grammatical errors using template- and learning-based methods. In contrast, this study introduced convolutional neural network (CNN) and long-short term memory (LSTM) for the shared task of Chinese Grammatical Error Diagnosis (CGED). Different from traditional word-based embedding, single word embedding was used as input of CNN and LSTM. The proposed single word embedding can capture both semantic and syntactic information to detect those four type grammatical error. In experimental evaluation, the recall and f1-score of our submitted results Run1 of the TOCFL testing data ranked the fourth place in all submissions in detection-level."
P16-2037,Dimensional Sentiment Analysis Using a Regional {CNN}-{LSTM} Model,2016,26,96,1,1,1794,jin wang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valencearousal (VA) space. Compared to the categorical approach that focuses on sentiment classification such as binary classification (i.e., positive and negative), the dimensional approach can provide more fine-grained sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts: regional CNN and LSTM to predict the VA ratings of texts. Unlike a conventional CNN which considers a whole text as input, the proposed regional CNN uses an individual sentence as a region, dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction. Such regional information is sequentially integrated across regions using LSTM for VA prediction. By combining the regional CNN and LSTM, both local (regional) information within sentences and long-distance dependency across sentences can be considered in the prediction process. Experimental results show that the proposed method outperforms lexicon-based, regression-based, and NN-based methods proposed in previous studies."
N16-1066,Building {C}hinese Affective Resources in Valence-Arousal Dimensions,2016,17,48,4,0,2441,liangchih yu,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
P15-2129,Predicting Valence-Arousal Ratings of Words Using a Weighted Graph Method,2015,26,18,2,0,2441,liangchih yu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Compared to the categorical approach that represents affective states as several discrete classes (e.g., positive and negative), the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as the valence-arousal (VA) space, thus allowing for more fine-grained sentiment analysis. In building dimensional sentiment applications, affective lexicons with valence-arousal ratings are useful resources but are still very rare. Therefore, this study proposes a weighted graph model that considers both the relations of multiple nodes and their similarities as weights to automatically determine the VA ratings of affective words. Experiments on both English and Chinese affective lexicons show that the proposed method yielded a smaller error rate on VA prediction than the linear regression, kernel method, and pagerank algorithm used in previous studies."
P98-2205,Summarization-based Query Expansion in Information Retrieval,1998,14,18,2,0.269679,14511,tomek strzalkowski,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"We discuss a semi-interactive approach to information retrieval which consists of two tasks performed in a sequence. First, the system assists the searcher in building a comprehensive statement of information need, using automatically generated topical summaries of sample documents. Second, the detailed statement of information need is automatically processed by a series of natural language processing routines in order to derive an optimal search query for a statistical information retrieval system. In this paper, we investigate the role of automated document summarization in building effective search statements. We also discuss the results of latest evaluation of our system at the annual Text Retrieval Conference (TREC)."
C98-2200,Summarization-based Query Expansion in Information Retrieval,1998,14,18,2,0.269679,14511,tomek strzalkowski,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"We discuss a semi-interactive approach to information retrieval which consists of two tasks performed in a sequence. First, the system assists the searcher in building a comprehensive statement of information need, using automatically generated topical summaries of sample documents. Second, the detailed statement of information need is automatically processed by a series of natural language processing routines in order to derive an optimal search query for a statistical information retrieval system. In this paper, we investigate the role of automated document summarization in building effective search statements. We also discuss the results of latest evaluation of our system at the annual Text Retrieval Conference (TREC)."
A97-1044,Building Effective Queries In Natural Language Information Retrieval,1997,12,37,4,0.348181,14511,tomek strzalkowski,Fifth Conference on Applied Natural Language Processing,0,"In this paper we report on our natural language information retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5). The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval. One of our goals was to demonstrate that robust if relatively shallow NLP can help to derive a better representation of text documents for statistical search. Recently, we have turned our attention away from text representation issues and more towards query development problems. While our NLIR system still performs extensive natural language processing in order to extract phrasal and other indexing terms, our focus has shifted to the problems of building effective search queries. Specifically, we are interested in query construction that uses words, sentences, and entire passages to expand initial topic specifications in an attempt to cover their various angles, aspects and contexts. Based on our earlier results indicating that NLP is more effective with long, descriptive queries, we allowed for long passages from related documents to be liberally imported into the queries. This method appears to have produced a dramatic improvement in the performance of two different statistical search engines that we tested (Cornell's SMART and NIST's Prise) boosting the average precision by at least 40%. In this paper we discuss both manual and automatic procedures for query expansion within a new stream-based information retrieval model."
X96-1036,Integration of Document Detection and Information Extraction,1996,4,2,3,0.254681,37309,louise guthrie,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"We have conducted a number of experiments to evaluate various modes of building an integrated detection/extraction system. The experiments were performed using SMART system as baseline. The goal was to determine if advanced information extraction methods can improve recall and precision of document detection. We identified the following two modes of integration:I. Extraction to Detection: broad-coverage extraction1. Extraction step: identify concepts for indexing2. Detection step 1: low recall, high initial precision3. Detection step 2: automatic relevance feedback using top N retrieved documents to regain recall.II. Detection to Extraction: query-specific extraction1. Detection step 1: high recall, low precision run2. Extraction step: learn concept(s) from query and retrieved subcollection3. Detection step 2: re-rank the subcollection to increase precisionOur integration effort concentrated on mode I, and the following issues:1. use of shallow but fast NLP for phrase extractions and disambiguation in place of a full syntactic parser2. use existing MUC-6 extraction capabilities to index a retrieval collection3. mixed Boolean/soft match retrieval model4. create a Universal Spotter algorithm for learning arbitrary concepts"
C96-2157,A Self-Learning Universal Concept Spotter,1996,7,28,2,0.348181,14511,tomek strzalkowski,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"We describe the Universal Spotter, a system for identifying in-text references to entities of an arbitrary, user-specified type, such as people, organizations, equipment, products, materials, etc. Starting with some initial seed examples, and a training text corpus, the system generates rules that will find further concepts of the same type. The initial seed information is provided by the user in the form of a typical lexical context in which the entities to be spotted occur, e.g., the name ends with Co., or to the right of produced or made, and so forth, or by simply supplying examples of the concept itself, e.g., Ford Taurus, gas turbine, Big Mac. In addition, negative examples can be supplied, if known. Given a sufficiently large training corpus, an unsupervised learning process is initiated in which the system will: (1) find instances of the sought-after concept using the seed-context information while maximizing recall and precision; (2) find additional contexts in which these entities occur; and (3) expand the initial seed-context with selected new contexts to find even more entities. Preliminary results of creating spotters for organizations and products are discussed."
X93-1021,{CRL}/{B}randeis: The {D}iderot System,1993,8,7,3,0,53660,jim cowie,"TIPSTER TEXT PROGRAM: PHASE {I}: Proceedings of a Workshop held at Fredricksburg, Virginia, September 19-23, 1993",0,Diderot is an information extraction system built at CRL and Brandeis University over the past two years. It was produced as part of our efforts in the Tipster project. The same overall system architecture has been used for English and Japanese and for the micro-electronics and joint venture domains.
M93-1015,{CRL/B}randeis: Description of the \\textit{ {D}iderot} System as Used for {MUC}-5,1993,8,4,3,0,53660,jim cowie,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,This report describes the major developments over the last six months in completing the Diderot information extraction system for the MUC-5 evaluation.Diderot is an information extraction system built at CRL and Brandeis University over the past two years. It was produced as part of our efforts in the Tipster project. The same overall system architecture has been used for English and Japanese and for the micro-electronics and joint venture domains.The past history of the system is discussed and the operation of its major components described. A summary of scores at the 24 month workshop is given and the performance of the system on the texts selected for the system walkthrough is discussed.
C92-1039,Syntactic Preferences for Robust Parsing With Semantic Preferences,1992,16,3,1,1,1794,jin wang,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Using constraints in robust parsing seems to have what we call robust parsing paradox. Preference Semantics and Connectionism both offered a promising approach to this problem. However, Preference Semantics has not addressed the problem of how to make full use of syntactic constraints, and Connectionism has some inherent difficulties of its own which prevent it producing a practical system. In this paper we are proposing a method to add syntactic preferences to the Preference Semantics paradigm while maintaining its fundamental philosophy. It will be shown that syntactic preferences can be coded as a set of weights associated with the set of symbolically manipulatable rules of a new grammar formalism. The syntactic preferences such coded can be easily used to compute with semantic preferences. With the help of some techniques borrowed from Connectionism, these weights can be adjusted through training."
