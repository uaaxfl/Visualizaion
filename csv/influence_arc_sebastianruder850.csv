2020.acl-main.421,P19-1494,1,0.842535,"e observe larger gaps in the syntactic evaluation dataset. This suggests that transferring syntactic abstractions is more challenging than semantic abstractions. We leave a more thorough investigation of whether joint multilingual pre-training reduces to learning a lexical-level alignment for future work. CLWE. models—although similar in spirit to M ONOT RANS—are only competitive on the easiest and smallest task (MLDoc), and perform poorly on the more challenging ones (XNLI and XQuAD). While previous work has questioned evaluation methods in this research area (Glavaˇs et al., 2019; 4629 CLWE Artetxe et al., 2019), our results provide evidence that existing methods are not competitive in challenging downstream tasks and that mapping between two fixed embedding spaces may be overly restrictive. For that reason, we think that designing better integration techniques of CLWE to downstream models is an important future direction. Lifelong learning. Humans learn continuously and accumulate knowledge throughout their lifetime. In contrast, existing multilingual models focus on the scenario where all training data for all languages is available in advance. The setting to transfer a monolingual model to other l"
2020.acl-main.421,D15-1075,0,0.0502676,"g test set for each downstream task. For M ONOT RANS, we observe stability issues when learning language-specific position embeddings for Greek, Thai and Swahili. The second step would occasionally fail to converge to a good solution. For these three languages, we run Step 2 of our proposed method (§2) three times and pick the best model on the XNLI development set. 3.3 XNLI: Natural Language Inference In natural language inference (NLI), given two sentences (a premise and a hypothesis), the goal is to decide whether there is an entailment, contradiction, or neutral relationship between them (Bowman et al., 2015). We train all models on the MultiNLI dataset (Williams et al., 2018) in English and evaluate on XNLI (Conneau et al., 2018b)—a cross-lingual NLI dataset consisting of 2,500 development and 5,000 test instances translated from English into 14 languages. We report our results on XNLI in Table 1 together with the previous results from mBERT and XLM.7 We summarize our main findings below. J OINT M ULTI is comparable with the literature. Our best J OINT M ULTI model is substantially better than mBERT, and only one point worse (on average) than the unsupervised XLM model, which is larger in size. A"
2020.acl-main.421,D18-1269,0,0.469127,"oss-lingual word embeddings that are aligned to a monolingual transformer body. In contrast to this approach, standard cross-lingual word embedding mappings first align monolingual lexical spaces and then learn a multilingual deep model on top of this space. We also include a method based on this alternative approach where we train skip-gram embeddings for each language, and map them to a shared space using VecMap (Artetxe et al., 2018).5 We then train an English BERT model using MLM and NSP on top of the frozen mapped embeddings. The model is 4 We use all languages that are included in XNLI (Conneau et al., 2018b). 5 We use the orthogonal mode in VecMap and map all languages into English. 4625 then fine-tuned using English labeled data while keeping the embeddings frozen. We zero-shot transfer to a new language by plugging in its respective mapped embeddings. Cross-lingual transfer of monolingual models (M ONOT RANS). Our method described in §2. We use English as L1 and try multiple variants with different extensions. 3.2 Setting Vocabulary. We perform subword tokenization using the unigram model in SentencePiece (Kudo and Richardson, 2018). In order to understand the effect of sharing subwords acros"
2020.acl-main.421,N19-1423,0,0.36187,"from SQuAD v1.1 translated into ten languages by professional translators. 1 . . . having word pieces used in all languages (numbers, URLs, etc), which have to be mapped to a shared space forces the co-occurring pieces to also be mapped to a shared space, thus spreading the effect to other word pieces, until different languages are close to a shared space. . . . mBERT’s ability to generalize cannot be attributed solely to vocabulary memorization, and that it must be learning a deeper multilingual representation. Introduction Multilingual pre-training methods such as multilingual BERT (mBERT, Devlin et al., 2019) have been successfully used for zero-shot cross-lingual transfer (Pires et al., 2019; Conneau and Lample, 2019). These methods work by jointly training a ∗ Work done as an intern at DeepMind. Cao et al. (2020) echoed this sentiment, and Wu and Dredze (2019) further observed that mBERT performs better in languages that share many subwords. As such, the current consensus of the crosslingual generalization ability of mBERT is based on a combination of three factors: (i) shared vocabulary items that act as anchor points; (ii) joint training across multiple languages that spreads this effect; whic"
2020.acl-main.421,D19-1572,1,0.89098,"Missing"
2020.acl-main.421,N18-2017,0,0.0346255,"Missing"
2020.acl-main.421,P12-1092,0,0.0386839,"oint multilingual pre-training is not essential for cross-lingual generalization, suggesting that monolingual models learn linguistic abstractions that generalize across languages. To get a better understanding of this phenomenon, we probe the representations of M ONO T RANS. As existing probing datasets are only available in English, we train monolingual representations in non-English languages and transfer them to English. We probe representations from the resulting English models with the Word in Context (WiC; Pilehvar and Camacho-Collados, 2019), Stanford Contextual Word Similarity (SCWS; Huang et al., 2012), and the syntactic evaluation (Marvin and Linzen, 2018) datasets. We provide details of our experimental setup in Appendix D and show a summary of our results in Table 4. The results indicate that monolingual semantic representations learned from non-English languages transfer to English to a degree. On WiC, models transferred from non-English languages are comparable with models trained on English. On SCWS, while there are more variations, models trained on other languages still perform surprisingly well. In contrast, we observe larger gaps in the syntactic evaluation dataset. This suggests"
2020.acl-main.421,C12-1089,0,0.0215614,"oised fine-tuning reduce the gap to only 1.1 points. Adapters mostly improve performance, except for low-resource languages such as Urdu, Swahili, Thai, and Greek. In subsequent experiments, we include results for all variants of M ONO T RANS and J OINT PAIR, the best CLWE variant (768d ident), and J OINT M ULTI with 32k and 200k voc. 3.4 MLDoc: Document Classification In MLDoc (Schwenk and Li, 2018), the task is to classify documents into one of four different genres: corporate/industrial, economics, government/social, and markets. The dataset is an improved version of the Reuters benchmark (Klementiev et al., 2012), and consists of 1,000 training and 4,000 test documents in 7 languages. We show the results of our MLDoc experiments in Table 2. In this task, we observe that simpler models tend to perform better, and the best overall results are from CLWE. We believe that this can be attributed to: (i) the superficial nature of the task itself, as a model can rely on a few keywords to identify the genre of an input document without requiring any high-level understanding and (ii) the small size of the training set. Nonetheless, all of the four model families obtain generally similar results, corroborating o"
2020.acl-main.421,D16-1264,0,0.0576974,"second language using the same training objective over its monolingual corpus; (iii) fine-tune the model on English while keeping the embeddings frozen; and (iv) zero-shot transfer it to the new language by swapping the token embeddings. vocabulary items nor joint learning. However, we show that it is competitive with joint multilingual pre-training across standard zero-shot cross-lingual transfer benchmarks (XNLI, MLDoc, and PAWSX). We also experiment with a new Cross-lingual Question Answering Dataset (XQuAD), which consists of 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 (Rajpurkar et al., 2016) translated into ten languages by professional translators. Question answering as a task is a classic probe for language understanding. It has also been found to be less susceptible to annotation artifacts commonly found in other benchmarks (Kaushik and Lipton, 2018; Gururangan et al., 2018). We believe that XQuAD can serve as a more comprehensive cross-lingual benchmark and make it publicly available at https://github. com/deepmind/xquad. Our results on XQuAD show that the monolingual transfer approach can be made competitive with mBERT by learning second language-specific transformations via"
2020.acl-main.421,N19-1162,0,0.0415642,"ngual representations. A common approach to learn multilingual representations is based on cross-lingual word embedding mappings. These methods learn a set of monolingual word embeddings for each language and map them to a shared space through a linear transformation. Recent approaches perform this mapping with an unsupervised initialization based on heuristics (Artetxe et al., 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a), which is further improved through self-learning (Artetxe et al., 2017). The same approach has also been adapted for contextual representations (Schuster et al., 2019). Unsupervised deep multilingual representations. In contrast to the previous approach, which learns a shared multilingual space at the lexical level, state-of-the-art methods learn deep representations with a transformer. Most of these methods are based on mBERT. Extensions to mBERT include scaling it up and incorporating parallel data (Conneau and Lample, 2019), adding auxiliary pretraining tasks (Huang et al., 2019), and encouraging representations of translations to be similar (Cao et al., 2020). Concurrent to this work, Tran (2020) propose a more complex approach to transfer a monolingual"
2020.acl-main.421,N18-1101,0,0.0292202,"ability issues when learning language-specific position embeddings for Greek, Thai and Swahili. The second step would occasionally fail to converge to a good solution. For these three languages, we run Step 2 of our proposed method (§2) three times and pick the best model on the XNLI development set. 3.3 XNLI: Natural Language Inference In natural language inference (NLI), given two sentences (a premise and a hypothesis), the goal is to decide whether there is an entailment, contradiction, or neutral relationship between them (Bowman et al., 2015). We train all models on the MultiNLI dataset (Williams et al., 2018) in English and evaluate on XNLI (Conneau et al., 2018b)—a cross-lingual NLI dataset consisting of 2,500 development and 5,000 test instances translated from English into 14 languages. We report our results on XNLI in Table 1 together with the previous results from mBERT and XLM.7 We summarize our main findings below. J OINT M ULTI is comparable with the literature. Our best J OINT M ULTI model is substantially better than mBERT, and only one point worse (on average) than the unsupervised XLM model, which is larger in size. A larger vocabulary is beneficial. J OINT M ULTI variants with a large"
2020.acl-main.421,D19-1077,0,0.0729269,"hus spreading the effect to other word pieces, until different languages are close to a shared space. . . . mBERT’s ability to generalize cannot be attributed solely to vocabulary memorization, and that it must be learning a deeper multilingual representation. Introduction Multilingual pre-training methods such as multilingual BERT (mBERT, Devlin et al., 2019) have been successfully used for zero-shot cross-lingual transfer (Pires et al., 2019; Conneau and Lample, 2019). These methods work by jointly training a ∗ Work done as an intern at DeepMind. Cao et al. (2020) echoed this sentiment, and Wu and Dredze (2019) further observed that mBERT performs better in languages that share many subwords. As such, the current consensus of the crosslingual generalization ability of mBERT is based on a combination of three factors: (i) shared vocabulary items that act as anchor points; (ii) joint training across multiple languages that spreads this effect; which ultimately yields (iii) deep cross-lingual representations that generalize across languages and tasks. In this paper, we empirically test this hypothesis by designing an alternative approach that violates all of these assumptions. As illustrated in Figure"
2020.acl-main.421,D19-1382,0,0.216595,"he genre of an input document without requiring any high-level understanding and (ii) the small size of the training set. Nonetheless, all of the four model families obtain generally similar results, corroborating our previous findings that joint multilingual pre-training and a shared vocabulary are not needed to achieve good performance. 3.5 PAWS-X: Paraphrase Identification PAWS is a dataset that contains pairs of sentences with a high lexical overlap (Zhang et al., 2019). The task is to predict whether each pair is a paraphrase or not. While the original dataset is only in English, PAWS-X (Yang et al., 2019) provides human translations into six languages. We evaluate our models on this dataset and show our results in Table 2. Similar to experiments on other datasets, M ONOT RANS is competitive with the best joint variant, with a difference of only 0.6 points when we learn language-specific position embeddings. 4 XQuAD: Cross-lingual Question Answering Dataset Our classification experiments demonstrate that M ONOT RANS is competitive with J OINT M ULTI and J OINT PAIR, despite being multilingual at the embedding layer only (i.e. the transformer body is trained 4627 MLDoc en Prev work mBERT fr es d"
2020.acl-main.421,P17-1179,0,0.030876,"models are often released without the data they are trained on. In that regard, our work provides a baseline for multilingual lifelong learning. 6 Related Work Unsupervised lexical multilingual representations. A common approach to learn multilingual representations is based on cross-lingual word embedding mappings. These methods learn a set of monolingual word embeddings for each language and map them to a shared space through a linear transformation. Recent approaches perform this mapping with an unsupervised initialization based on heuristics (Artetxe et al., 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a), which is further improved through self-learning (Artetxe et al., 2017). The same approach has also been adapted for contextual representations (Schuster et al., 2019). Unsupervised deep multilingual representations. In contrast to the previous approach, which learns a shared multilingual space at the lexical level, state-of-the-art methods learn deep representations with a transformer. Most of these methods are based on mBERT. Extensions to mBERT include scaling it up and incorporating parallel data (Conneau and Lample, 2019), adding auxiliary pretraining tasks (Huang"
2020.acl-main.421,N19-1131,0,0.0724588,"Missing"
2020.acl-main.658,P19-1310,0,0.0576863,"Missing"
2020.acl-main.658,D18-1214,0,0.0511819,"Missing"
2020.acl-main.658,D18-1549,0,0.113759,"Missing"
2020.acl-main.658,2020.acl-main.421,1,0.822517,"RT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-lingual word embeddings to initialize a sha"
2020.acl-main.658,P18-1231,0,0.0275146,"named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of"
2020.acl-main.658,Q17-1010,0,0.0770184,"they assume document-level sequences (Conneau and Lample, 2019) or sentence-level sequences (Artetxe et al., 2018c; Lample et al., 2018a). Nature of atomic symbols. A more important consideration is the nature of the atomic symbols in such sequences. To the best of our knowledge, previous work assumes some form of word segmentation or tokenization (e.g., splitting by whitespaces or punctuation marks). Early work on cross-lingual word embeddings considered such tokens as atomic units. However, more recent work (Hoshen and Wolf, 2018; Glavaš et al., 2019) has primarily used fastText embeddings (Bojanowski et al., 2017) which incorporate subword information into the embedding learning, although the vocabulary is still defined at the token level. In addition, there have also been approaches that incorporate character-level information into the alignment learning itself (Heyman et al., 2017; Riley and Gildea, 2018). In contrast, most work on contextual word embeddings and unsupervised machine translation operates with a subword vocabulary (Devlin et al., 2019; Conneau and Lample, 2019). While the above distinction might seem irrelevant from a practical perspective, we think that it is important from a more fun"
2020.acl-main.658,buck-etal-2014-n,0,0.0197389,"Missing"
2020.acl-main.658,D18-1024,0,0.0275962,"s is acceptable and ultimately necessary for UCL. However, we believe that any connection stemming from a (partly) shared writing system belongs to a different category, and should be considered a separate cross-lingual signal. Our rationale is that a given writing system pertains to a specific form to encode a language, but cannot be considered to be part of the language itself.6 4.3 Multilinguality While most work in unsupervised cross-lingual learning considers two languages at a time, there have recently been some attempts to extend these methods to multiple languages (Duong et al., 2017; Chen and Cardie, 2018; Heyman et al., 2019), and most work on unsupervised cross-lingual pretraining is multilingual (Pires et al., 2019; Conneau 6 As a matter of fact, languages existed well before writing was invented, and a given language can have different writing systems or new ones can be designed. 7379 Monolingual signal Cross-lingual signal Sequence of symbols Sets of sentences/documents Tokens/subwords Linguistic analysis Shared writing system Identical words String similarity Table 1: Different types of monolingual and crosslingual signals that have been used for unsupervised cross-lingual learning, orde"
2020.acl-main.658,P17-1176,0,0.0594735,"s String similarity Table 1: Different types of monolingual and crosslingual signals that have been used for unsupervised cross-lingual learning, ordered roughly from least to most linguistic knowledge (top to bottom). and Lample, 2019). When considering parallel data across a subset of the language pairs, multilinguality gives rise to additional scenarios. For instance, the scenario where two languages have no parallel data between each other but are well connected through a third (pivot) language has been explored by several authors in the context of machine translation (Cheng et al., 2016; Chen et al., 2017). However, given that the languages in question are still indirectly connected through parallel data, this scenario does not fall within the unsupervised category, and is instead commonly known as zero-resource machine translation. An alternative scenario explored in the contemporaneous work of Liu et al. (2020) is where a set of languages are connected through parallel data, and there is a separate language with monolingual data only. We argue that, when it comes to the isolated language, such a scenario should still be considered as UCL, as it does not rely on any parallel data for that part"
2020.acl-main.658,D18-1269,0,0.171286,"unt of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder tra"
2020.acl-main.658,D19-1169,0,0.021601,"recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD 7381 Methodological issues Examples Validation and hyperparameter tuning Systematic tuning with parallel data or on test data Evaluation on favorable conditions Typologically similar languages; always including English; training on the same domain Over-reliance on translation tasks Overfitting to bilingual lexicon induction; known issues with existing datasets Lack of an established benchmark Evaluation on many different tasks; problems with common tasks (MLDoc and XNLI) Table 2: Methodolog"
2020.acl-main.658,D13-1173,0,0.0177374,"of tokenization. Such a tabula rasa approach is potentially applicable to any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the advent of unsupervised NMT, statistical deci7378 pherment was already shown to benefit from incorporating syntactic dependency relations (Dou and Knight, 2013). For other tasks such as unsupervised POS tagging (Snyder et al., 2008), monolingual tag dictionaries have been used. While such approaches could still be considered unsupervised from a cross-lingual perspective, we argue that the interest of this research direction is greatly limited by two factors: (i) from a theoretical perspective, it assumes some fundamental knowledge that is not directly inferred from the raw monolingual corpora; and (ii) from a more practical perspective, it is not reasonable to assume that such resources are available in the less resourced settings where this research"
2020.acl-main.658,E17-1084,0,0.0251037,"inguistics universals is acceptable and ultimately necessary for UCL. However, we believe that any connection stemming from a (partly) shared writing system belongs to a different category, and should be considered a separate cross-lingual signal. Our rationale is that a given writing system pertains to a specific form to encode a language, but cannot be considered to be part of the language itself.6 4.3 Multilinguality While most work in unsupervised cross-lingual learning considers two languages at a time, there have recently been some attempts to extend these methods to multiple languages (Duong et al., 2017; Chen and Cardie, 2018; Heyman et al., 2019), and most work on unsupervised cross-lingual pretraining is multilingual (Pires et al., 2019; Conneau 6 As a matter of fact, languages existed well before writing was invented, and a given language can have different writing systems or new ones can be designed. 7379 Monolingual signal Cross-lingual signal Sequence of symbols Sets of sentences/documents Tokens/subwords Linguistic analysis Shared writing system Identical words String similarity Table 1: Different types of monolingual and crosslingual signals that have been used for unsupervised cross"
2020.acl-main.658,E14-1049,0,0.0301901,"deep multilingual pre-training (§2.2), and unsupervised machine translation (§2.3). 2.1 2.3 Cross-lingual word embeddings Cross-lingual word embedding methods traditionally relied on parallel corpora (Gouws et al., 2015; Luong et al., 2015). Nonetheless, the amount of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), the"
2020.acl-main.658,P19-1070,1,0.884346,"Missing"
2020.acl-main.658,L18-1550,0,0.0236778,"s3 of which nearly half have less than 10,000 articles. While one could hope to overcome this by taking the entire web as a corpus, as facilitated by Common Crawl4 and similar initiatives, this is not 3 https://en.wikipedia.org/wiki/List_ of_Wikipedias 4 https://commoncrawl.org/ always feasible for low-resource languages. First, the presence of less resourced languages on the web is very limited, with only a few hundred languages recognized as being used in websites.5 This situation is further complicated by the limited coverage of existing tools such as language detectors (Buck et al., 2014; Grave et al., 2018), which only cover a few hundred languages. Alternatively, speech could also serve as a source of monolingual data (e.g., by recording public radio stations). However, this is an unexplored direction within UCL, and collecting, processing and effectively capitalizing on speech data is far from trivial, particularly for low-resource languages. All in all, we conclude that the alleged scenario involving no parallel data and sufficient monolingual data is not met in the real world in the terms explored by recent UCL research. Needless to say, effectively exploiting unlabeled data is important in"
2020.acl-main.658,N18-2017,0,0.0393367,"Missing"
2020.acl-main.658,D19-1632,0,0.14985,"ols in different languages to be disjoint, without prior knowledge of the relationship between them. Needless to say, any form of learning requires making assumptions, as one needs some criterion to prefer one mapping over another. In the case of UCL, such assumptions stem from the structural similarity across languages (e.g. semantically equivalent words in different languages are assumed to occur in similar contexts). In practice, these assumptions weaken as the distribution of the datasets diverges, and some UCL models have been reported to break under a domain shift (Søgaard et al., 2018; Guzmán et al., 2019; Marchisio et al., 2020). Similarly, approaches that leverage linguistic features such as syntactic dependencies may assume that these are similar across languages. In addition, one can also assume that the sets of symbols that are used to represent different languages have some commonalities. This departs from the strict definition of UCL above, establishing some prior connections between the sets of symbols in different languages. Such an assumption is reasonable from a practical perspective, as there are a few scripts (e.g. Latin, Arabic or Cyrillic) that cover a large fraction of language"
2020.acl-main.658,Q19-1033,0,0.0144797,"derlying assumptions might not generalize to different writing systems (e.g. logographic instead of alphabetic). For instance, subword tokenization has been shown to perform poorly on reduplicated words (Vania and Lopez, 2017). In relation to that, one could also consider the text in each language as a stream of discrete character-like symbols without any notion of tokenization. Such a tabula rasa approach is potentially applicable to any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the advent of unsupervised NMT, statistical deci7378 pherment was already shown to benefit from incorporating syntactic dependency relations (Dou and Knight, 2013). For other tasks such as unsupervised POS tagging (Snyder et al., 2008), monolingual tag dictionaries have been used. While such approaches could still be considered unsupervised from a cross-lingual perspective, we argue that the inte"
2020.acl-main.658,N19-1188,0,0.0821336,"Missing"
2020.acl-main.658,E17-1102,0,0.0631211,"Missing"
2020.acl-main.658,D18-1043,0,0.121563,"corpora (Gouws et al., 2015; Luong et al., 2015). Nonetheless, the amount of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devl"
2020.acl-main.658,P18-1031,1,0.811822,"initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder trained on masked language modeling and next sentence prediction, which led to impressive gains on various downstream tasks. While the above approaches are limited to a single language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined"
2020.acl-main.658,D19-1607,0,0.0194325,"i et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD 7381 Methodological issues Examples Validation and hyperparameter tuning Systematic tuning with parallel data or on test data Evaluation on favorable conditions Typologically similar languages; always including English; training on the same domain Over-reliance on translation tasks Overfitting to bilingual lexicon induction; known issues with existing datasets Lack of an established benchmark Evaluation on many different tasks; problems with common tasks (MLDoc and XNLI) Table 2: Methodological issues pertain"
2020.acl-main.658,kamholz-etal-2014-panlex,0,0.0171766,"irs in the real world” (Xu et al., 2018) is largely inaccurate. For instance, the JW300 parallel corpus covers 343 languages with around 100,000 parallel sentences per language pair on average (Agi´c and Vuli´c, 2019), and the multilingual Bible corpus collected by Mayer and Cysouw (2014) covers 837 language varieties (each with a unique ISO 639-3 code). Moreover, the PanLex project aims to collect multilingual lexica for all human languages in the world, and already covers 6,854 language varieties with at least 20 lexemes, 2,364 with at least 200 lexemes, and 369 with at least 2,000 lexemes (Kamholz et al., 2014). While 20 or 200 lexemes might seem insufficient, weakly supervised cross-lingual word embedding methods already proved effective with as little as 25 word pairs (Artetxe et al., 2017). More recent methods have focused on completely removing this weak supervision (Conneau et al., 2018a; Artetxe et al., 2018a), which can hardly be justified from a practical perspective given the existence of such resources and additional training signals stemming from a (partially) shared script (§4.2). Finally, given the availability of sufficient monolingual data, noisy parallel data can often be obtained by"
2020.acl-main.658,D19-1328,0,0.0166423,"eam tasks. In particular, they observe that some mapping methods that are specifically designed for bilingual lexicon induction perform poorly on other tasks, showing the risk of relying excessively on translation benchmarks for evaluating cross-lingual models. Moreover, existing translation benchmarks have been shown to have several issues on their own. In particular, bilingual lexicon induction datasets have been reported to misrepresent morphological variations, overly focus on named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al"
2020.acl-main.658,J82-2005,0,0.68359,"Missing"
2020.acl-main.658,2020.tacl-1.47,0,0.107835,"ase-based Statistical Machine Translation (SMT), obtaining large improvements over the original NMT-based systems (Lample et al., 2018b; Artetxe et al., 2018b). This alternative approach uses cross-lingual n-gram embeddings to build an initial phrase table, which is combined with an n-gram language model and a distortion model, and further refined through iterative backtranslation. There have been several follow-up attempts to combine NMT and SMT based approaches (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019b). More recently, Conneau and Lample (2019), Song et al. (2019) and Liu et al. (2020) obtain strong results using deep multilingual pretraining rather than cross-lingual word embeddings to initialize unsupervised NMT systems. 3 Motivating fully unsupervised learning In this section, we challenge the narrative of motivating UCL based on a lack of parallel resources. We argue that the strict unsupervised scenario cannot be motivated from an immediate practical perspective, and elucidate what we believe should be the true goals of this research direction. 2 The full version of their model (XLM) requires parallel corpora for their translation language modeling objective, but the a"
2020.acl-main.658,W15-1521,0,0.0312387,"translation) in UCL (§6), and conclude with a summary of our recommendations (§7). 7375 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7375–7388 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2 Background In this section, we briefly review existing work on UCL, covering cross-lingual word embeddings (§2.1), deep multilingual pre-training (§2.2), and unsupervised machine translation (§2.3). 2.1 2.3 Cross-lingual word embeddings Cross-lingual word embedding methods traditionally relied on parallel corpora (Gouws et al., 2015; Luong et al., 2015). Nonetheless, the amount of supervision required was greatly reduced via crosslingual word embedding mappings, which work by separately learning monolingual word embeddings in each language and mapping them into a shared space through a linear transformation. Early work required a bilingual dictionary to learn such a transformation (Mikolov et al., 2013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning"
2020.acl-main.658,2020.wmt-1.68,0,0.0586124,"Missing"
2020.acl-main.658,D18-1399,1,0.927958,"ng autoencoding, backtranslation, and optionally adversarial learning. Subsequent work adapted these principles to unsupervised phrase-based Statistical Machine Translation (SMT), obtaining large improvements over the original NMT-based systems (Lample et al., 2018b; Artetxe et al., 2018b). This alternative approach uses cross-lingual n-gram embeddings to build an initial phrase table, which is combined with an n-gram language model and a distortion model, and further refined through iterative backtranslation. There have been several follow-up attempts to combine NMT and SMT based approaches (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019b). More recently, Conneau and Lample (2019), Song et al. (2019) and Liu et al. (2020) obtain strong results using deep multilingual pretraining rather than cross-lingual word embeddings to initialize unsupervised NMT systems. 3 Motivating fully unsupervised learning In this section, we challenge the narrative of motivating UCL based on a lack of parallel resources. We argue that the strict unsupervised scenario cannot be motivated from an immediate practical perspective, and elucidate what we believe should be the true goals of this research direction."
2020.acl-main.658,W19-5330,0,0.0262475,"optimal hyperparameter choice might not necessarily transfer well across languages. In contrast, Conneau et al. (2018a) and Lample et al. (2018a) propose an unsupervised validation criterion that is defined over monolingual data and shown to correlate well with test performance. This enables systematic tuning on the language pair of interest, but still requires parallel data to guide the development of the unsupervised validation criterion itself. A parallel validation set has also been used for systematic tuning in 7380 the context of unsupervised machine translation (Marie and Fujita, 2018; Marie et al., 2019; Stojanovski et al., 2019). While this is motivated as a way to abstract away the issue of unsupervised tuning—which the authors consider to be an open problem—we argue that any systematic use of parallel data should not be considered UCL. Finally, previous work often does not report the validation scheme used. In particular, unsupervised crosslingual word embedding methods have almost exclusively been evaluated on bilingual lexicons that do not have a validation set, and presumably use the test set to guide development to some extent. Our position is that a completely blind development model"
2020.acl-main.658,mayer-cysouw-2014-creating,0,0.0141436,"ual corpus. From this argument, it follows that monolingual data is cheaper to obtain than parallel data, so unsupervised crosslingual learning should in principle be more generally applicable than supervised learning. However, we argue that the common claim that the requirement for parallel data “may not be met for many language pairs in the real world” (Xu et al., 2018) is largely inaccurate. For instance, the JW300 parallel corpus covers 343 languages with around 100,000 parallel sentences per language pair on average (Agi´c and Vuli´c, 2019), and the multilingual Bible corpus collected by Mayer and Cysouw (2014) covers 837 language varieties (each with a unique ISO 639-3 code). Moreover, the PanLex project aims to collect multilingual lexica for all human languages in the world, and already covers 6,854 language varieties with at least 20 lexemes, 2,364 with at least 200 lexemes, and 369 with at least 2,000 lexemes (Kamholz et al., 2014). While 20 or 200 lexemes might seem insufficient, weakly supervised cross-lingual word embedding methods already proved effective with as little as 25 word pairs (Artetxe et al., 2017). More recent methods have focused on completely removing this weak supervision (Co"
2020.acl-main.658,D14-1162,0,0.0914393,"013a; Faruqui and Dyer, 2014). This requirement was later reduced with self-learning (Artetxe et al., 2017), and ultimately removed via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder trained on masked language modeling and next sentence prediction, which led to impressive gains on various downstream tasks. While the above approaches are limited to a single language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main"
2020.acl-main.658,N18-1202,0,0.0160418,"oved via unsupervised initialization heuristics (Artetxe et al., 2018a; Hoshen and Wolf, 2018) and adversarial learning (Zhang et al., 2017a; Conneau et al., 2018a). Finally, several recent methods have formulated cross-lingual embedding alignment as an optimal transport problem (Zhang et al., 2017b; Grave et al., 2019; Alvarez-Melis and Jaakkola, 2018). 2.2 Deep multilingual pretraining Following the success in learning shallow word embeddings (Mikolov et al., 2013b; Pennington et al., 2014), there has been an increasing interest in learning contextual word representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018). Recent research has been dominated by BERT (Devlin et al., 2019), which uses a bidirectional transformer encoder trained on masked language modeling and next sentence prediction, which led to impressive gains on various downstream tasks. While the above approaches are limited to a single language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BE"
2020.acl-main.658,P19-1493,0,0.0930796,"gual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-lingual word embeddin"
2020.acl-main.658,P19-1015,0,0.0363163,"morphological variations, overly focus on named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 201"
2020.acl-main.658,P11-1002,0,0.0760115,"the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-lingual word embeddings to initialize a shared encoder, and train it in conjunction with the decoder using a combination of denoising autoencoding, backtranslation, and optionally adversarial learning. Subsequent work adapted these principles to unsupervised phrase-based Statistical Machine Translat"
2020.acl-main.658,P18-2062,0,0.110805,"es some form of word segmentation or tokenization (e.g., splitting by whitespaces or punctuation marks). Early work on cross-lingual word embeddings considered such tokens as atomic units. However, more recent work (Hoshen and Wolf, 2018; Glavaš et al., 2019) has primarily used fastText embeddings (Bojanowski et al., 2017) which incorporate subword information into the embedding learning, although the vocabulary is still defined at the token level. In addition, there have also been approaches that incorporate character-level information into the alignment learning itself (Heyman et al., 2017; Riley and Gildea, 2018). In contrast, most work on contextual word embeddings and unsupervised machine translation operates with a subword vocabulary (Devlin et al., 2019; Conneau and Lample, 2019). While the above distinction might seem irrelevant from a practical perspective, we think that it is important from a more fundamental point of view (e.g. in relation to the distributional hypothesis as discussed in §3.2). Moreover, some of the underlying assumptions might not generalize to different writing systems (e.g. logographic instead of alphabetic). For instance, subword tokenization has been shown to perform poor"
2020.acl-main.658,D18-1042,1,0.810075,"h et al., 2017; Søgaard et al., 2018) or string-level similarity across languages (Riley and Gildea, 2018; Artetxe et al., 2019b) as training signals. Other methods use a joint subword vocabulary for all languages, indirectly exploiting the commonalities in their writing system (Lample et al., 2018b; Conneau and Lample, 2019). However, past work greatly differs on the nature and relevance that is attributed to such a training signal. The reliance on identically spelled words has been considered as a weak form of supervision in the cross-lingual word embedding literature (Søgaard et al., 2018; Ruder et al., 2018), and significant effort has been put into developing strictly unsupervised methods that do not rely on such signal (Conneau et al., 2018a). In contrast, the unsupervised machine translation literature has not payed much attention to this factor, and has often relied on identical words (Artetxe et al., 2018c), string-level similarity (Artetxe et al., 2019b), or a joint subword vocabulary (Lample et al., 2018b; Conneau and Lample, 2019) under the unsupervised umbrella. The same is true for unsupervised deep multilingual pretraining, where a shared subword vocabulary has been a common component"
2020.acl-main.658,N19-1162,0,0.0215779,"uction datasets have been reported to misrepresent morphological variations, overly focus on named entities and frequent words, and have pervasive gaps in the gold-standard targets (Czarnowska et al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as quest"
2020.acl-main.658,L18-1560,0,0.0284769,"al., 2019; Kementchedjhieva et al., 2019). More generally, most of these datasets are limited to relatively close languages and comparable corpora. Lack of an established cross-lingual benchmark. At the same time, there is no de facto standard benchmark to evaluate cross-lingual models beyond translation. Existing approaches have been evaluated in a wide variety of tasks including dependency parsing (Schuster et al., 2019), named entity recognition (Rahimi et al., 2019), sentiment analysis (Barnes et al., 2018), natural language inference (Conneau et al., 2018b), and document classification (Schwenk and Li, 2018). XNLI (Conneau et al., 2018b) and MLDoc (Schwenk and Li, 2018) are common choices, but they have their own problems: MultiNLI, the dataset from which XNLI was derived, has been shown to contain superficial cues that can be exploited (Gururangan et al., 2018), while MLDoc can be solved by keyword matching (Artetxe et al., 2020b). There are nonEnglish counterparts for more challenging tasks such as question answering (Cui et al., 2019; Hsu et al., 2019), but these only exist for a handful of languages. More recent datasets such as XQuAD 7381 Methodological issues Examples Validation and hyperpa"
2020.acl-main.658,D08-1109,0,0.0583697,"any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the advent of unsupervised NMT, statistical deci7378 pherment was already shown to benefit from incorporating syntactic dependency relations (Dou and Knight, 2013). For other tasks such as unsupervised POS tagging (Snyder et al., 2008), monolingual tag dictionaries have been used. While such approaches could still be considered unsupervised from a cross-lingual perspective, we argue that the interest of this research direction is greatly limited by two factors: (i) from a theoretical perspective, it assumes some fundamental knowledge that is not directly inferred from the raw monolingual corpora; and (ii) from a more practical perspective, it is not reasonable to assume that such resources are available in the less resourced settings where this research direction has more potential for impact. 4.2 Cross-lingual training sig"
2020.acl-main.658,P18-1072,1,0.918484,"Missing"
2020.acl-main.658,W19-5344,0,0.0253622,"er choice might not necessarily transfer well across languages. In contrast, Conneau et al. (2018a) and Lample et al. (2018a) propose an unsupervised validation criterion that is defined over monolingual data and shown to correlate well with test performance. This enables systematic tuning on the language pair of interest, but still requires parallel data to guide the development of the unsupervised validation criterion itself. A parallel validation set has also been used for systematic tuning in 7380 the context of unsupervised machine translation (Marie and Fujita, 2018; Marie et al., 2019; Stojanovski et al., 2019). While this is motivated as a way to abstract away the issue of unsupervised tuning—which the authors consider to be an open problem—we argue that any systematic use of parallel data should not be considered UCL. Finally, previous work often does not report the validation scheme used. In particular, unsupervised crosslingual word embedding methods have almost exclusively been evaluated on bilingual lexicons that do not have a validation set, and presumably use the test set to guide development to some extent. Our position is that a completely blind development model without any parallel data"
2020.acl-main.658,P17-1184,0,0.022059,"n contextual word embeddings and unsupervised machine translation operates with a subword vocabulary (Devlin et al., 2019; Conneau and Lample, 2019). While the above distinction might seem irrelevant from a practical perspective, we think that it is important from a more fundamental point of view (e.g. in relation to the distributional hypothesis as discussed in §3.2). Moreover, some of the underlying assumptions might not generalize to different writing systems (e.g. logographic instead of alphabetic). For instance, subword tokenization has been shown to perform poorly on reduplicated words (Vania and Lopez, 2017). In relation to that, one could also consider the text in each language as a stream of discrete character-like symbols without any notion of tokenization. Such a tabula rasa approach is potentially applicable to any arbitrary language, even when its writing system is not known, but has so far only been explored for a limited number of languages in a monolingual setting (Hahn and Baroni, 2019). Linguistic information. Finally, one can exploit additional linguistic knowledge through linguistic analysis such as lemmatization, part-of-speech tagging, or syntactic parsing. For instance, before the"
2020.acl-main.658,D19-1449,0,0.0899592,"Missing"
2020.acl-main.658,P16-1024,0,0.106921,"Missing"
2020.acl-main.658,W18-5446,0,0.0677908,"Missing"
2020.acl-main.658,D19-1077,0,0.0171822,"language, a multilingual extension of BERT (mBERT) has been shown to also be effective at learning cross-lingual representations in an unsupervised way.1 The main idea is to combine monolingual corpora in different languages, upsampling those with less data, and training a regular BERT model on the combined data. Conneau and Lample (2019) follow a similar approach but perform a more thorough evaluation and report substantially 1 https://github.com/google-research/ bert/blob/master/multilingual.md stronger results,2 which was further scaled up by Conneau et al. (2019). Several recent studies (Wu and Dredze, 2019; Pires et al., 2019; Artetxe et al., 2020b; Wu et al., 2019) analyze mBERT to get a better understanding of its capabilities. Unsupervised machine translation Early attempts to build machine translation systems using monolingual data alone go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012, 2013). However, this approach was only shown to work in limited settings, and the first convincing results on standard benchmarks were achieved by Artetxe et al. (2018c) and Lample et al. (2018a) on unsupervised Neural Machine Translation (NMT). Both approaches rely on cross-l"
2020.acl-main.658,D18-1268,0,0.0181008,"pervised variant using masked language modeling alone. 7376 3.1 How practical is the strict unsupervised scenario? Monolingual resources subsume parallel resources. For instance, each side of a parallel corpus effectively serves as a monolingual corpus. From this argument, it follows that monolingual data is cheaper to obtain than parallel data, so unsupervised crosslingual learning should in principle be more generally applicable than supervised learning. However, we argue that the common claim that the requirement for parallel data “may not be met for many language pairs in the real world” (Xu et al., 2018) is largely inaccurate. For instance, the JW300 parallel corpus covers 343 languages with around 100,000 parallel sentences per language pair on average (Agi´c and Vuli´c, 2019), and the multilingual Bible corpus collected by Mayer and Cysouw (2014) covers 837 language varieties (each with a unique ISO 639-3 code). Moreover, the PanLex project aims to collect multilingual lexica for all human languages in the world, and already covers 6,854 language varieties with at least 20 lexemes, 2,364 with at least 200 lexemes, and 369 with at least 2,000 lexemes (Kamholz et al., 2014). While 20 or 200 l"
2020.acl-main.658,P17-1179,0,0.199662,"include translation as well as pretraining multilingual representations. We will use the term interchangeably with “cross-lingual learning”. ∗ Equal contribution. Recent work in this direction has increasingly focused on purely unsupervised cross-lingual learning (UCL)—i.e., cross-lingual learning without any parallel signal across the languages. We provide an overview in §2. Such work has been motivated by the apparent dearth of parallel data for most of the world’s languages. In particular, previous work has noted that “data encoding cross-lingual equivalence is often expensive to obtain” (Zhang et al., 2017a) whereas “monolingual data is much easier to find” (Lample et al., 2018a). Overall, it has been argued that unsupervised cross-lingual learning “opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely” (Zhang et al., 2017a). We challenge this narrative and argue that the scenario of no parallel data and sufficient monolingual data is unrealistic and not reflected in the real world (§3.1). Nevertheless, UCL is an important research direction and we advocate for its study based on an inherent scientific interest (to better un"
2020.acl-main.658,D17-1207,0,0.119875,"include translation as well as pretraining multilingual representations. We will use the term interchangeably with “cross-lingual learning”. ∗ Equal contribution. Recent work in this direction has increasingly focused on purely unsupervised cross-lingual learning (UCL)—i.e., cross-lingual learning without any parallel signal across the languages. We provide an overview in §2. Such work has been motivated by the apparent dearth of parallel data for most of the world’s languages. In particular, previous work has noted that “data encoding cross-lingual equivalence is often expensive to obtain” (Zhang et al., 2017a) whereas “monolingual data is much easier to find” (Lample et al., 2018a). Overall, it has been argued that unsupervised cross-lingual learning “opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely” (Zhang et al., 2017a). We challenge this narrative and argue that the scenario of no parallel data and sufficient monolingual data is unrealistic and not reflected in the real world (§3.1). Nevertheless, UCL is an important research direction and we advocate for its study based on an inherent scientific interest (to better un"
2020.acl-main.658,D12-1025,0,\N,Missing
2020.acl-main.658,P17-1042,1,\N,Missing
2020.acl-main.658,P19-1494,1,\N,Missing
2020.acl-main.658,N19-1423,0,\N,Missing
2020.acl-main.658,D19-1090,1,\N,Missing
2020.acl-main.658,2020.emnlp-main.484,0,\N,Missing
2020.acl-main.658,2020.emnlp-main.618,1,\N,Missing
2020.coling-main.256,D16-1250,0,0.0680169,"night, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translat"
2020.coling-main.256,P17-1042,0,0.0271259,"ng, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translation of a source form is among the k-best candidates returned by the model. In this work we exclusively consider the precision"
2020.coling-main.256,P18-1073,0,0.0227704,"ion between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translation of a source form is among the k-best candidates returned by the model. In this work we exclusively consider the precision@1 metric, which is the least forgiving. 2.3 Morphological Inflection: A Challenge for BLI Most datasets for BLI operate at the level of inflected forms and impose no restriction on the morphosyntactic category of translated words. From a"
2020.coling-main.256,P13-1133,0,0.0642117,"ska et al. (2019). As discussed in §2.3, given that inflectional morphology is present in the induced lexicon, BLI models should be trained and evaluated on datasets which list a range of compatible inflected form pairs for every source-target lexeme pair. At this time, the dictionaries of Czarnowska et al. (2019) are the only publicly available resource that meets this criterion, and, for this reason, they are the most important evaluation benchmark used in this work. The dictionaries were generated based on Open Multilingual WordNet (Bond and Paik, 2012), Extended Open Multilingual WordNet (Bond and Foster, 2013) and UniMorph8 (Kirov et al., 2016; McCarthy et al., 2020), a resource comprised of inflectional word paradigms for 107 languages. The dictionaries only list parts of speech that undergo inflection in either the source or the target language; these are nouns, adjectives and verbs in the Romance languages. Conneau et al. (2018). MUSE (Conneau et al., 2018) was generated using an “internal translation tool” and is one of the few other resources which covers pairs of Romance languages. However, it is skewed towards most frequent forms: The vast majority of forms in MUSE are ranked in the top 10k"
2020.coling-main.256,J90-2002,0,0.81584,"ected form that lexicographers have chosen to be representative of the lexeme. For example, the lexeme RUN’s lemma is run. In many languages, the infinitive is the verbal lemma and the nominative singular is the nominal lemma. We consider a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et"
2020.coling-main.256,K17-2001,1,0.898886,"Missing"
2020.coling-main.256,D19-1090,1,0.853852,"odules have no means of handling irregular morphology, beyond the irregular forms they have been exposed to during training. Guided by this insight, we propose an alternative version of our model, which employs a special treatment for forms likely to have irregular morphology—the most frequent forms (Bybee, 1995a; Baayen and Lieber, 1996; Wu et al., 2019a). We term this extension the hybrid model. It employs a frequency-based heuristic and translates the source form through its lemma only if the lemma is more frequent.6 Otherwise, it translates the inflected form 4 Indeed, the dictionaries of Czarnowska et al. (2019) on which we experiment also make this assumption. Note that the translator’s distribution, as defined in eq. (3) and eq. (4), is over all inflected forms in the target lexicon. An alternative would be to define a distribution over lemmata only. However, this would require filtering out all non-lemma forms from the target embedding matrix, which is not trivial. In our preliminary experiments, we observed that this can lead to a further performance increase. 6 We rely on the order of FAST T EXT embeddings for the relative ranking of inflected forms. 5 2850 directly, using only the translator co"
2020.coling-main.256,P19-1070,1,0.883225,"Missing"
2020.coling-main.256,L18-1550,0,0.0423771,"to supervised and semi-supervised approaches. 5.3 Skyline We also consider a version of our model which uses an oracle analyzer—the source lemma λs and tag τs are known a priori. The skyline provides an upper-bound of performance—to wit, what performance would be achievable if the model had had access to more information about the translated source form. 5.4 Experimental Details We implemented all models in PyTorch (Paszke et al., 2019), adapting the code of Wu et al. (2019b) for the transducers (analyzer and inflector). Throughout our experiments we used the Wikipedia FAST T EXT embeddings (Grave et al., 2018), which we length-normalized and mean-centered before training the models. As is standard, we trained all translators on the top 200k most frequent word forms in the vocabularies of both languages. To evaluate on very rare forms present in the dictionaries of Czarnowska et al. (2019) which are out-of-vocabulary (OOV) for FAST T EXT, we created an OOV FAST T EXT embedding for every OOV form that appears in a union of WordNet and UniMorph and appended those representations to the original embedding matrices.10 We evaluated all models using precision@1 as a metric, which is equivalent to accuracy"
2020.coling-main.256,D19-1328,0,0.0227454,"the source or the target language; these are nouns, adjectives and verbs in the Romance languages. Conneau et al. (2018). MUSE (Conneau et al., 2018) was generated using an “internal translation tool” and is one of the few other resources which covers pairs of Romance languages. However, it is skewed towards most frequent forms: The vast majority of forms in MUSE are ranked in the top 10k of the vocabularies in their respective languages, causing it to omit many morphological variants of words. The dataset also suffers from other issues, such as a high level of noise coming from proper nouns (Kementchedjhieva et al., 2019). Thus, we do not view this resource as a reasonable benchmark for BLI. 5.2 Baselines Artetxe et al. (2016). They learn an orthogonal linear transformation matrix between the source language space and the target language space, after length-normalizing and mean-centering the monolingual embedding matrices. Their method is fully supervised and works best with large amounts of training data (several thousand translation pairs). Ruder et al. (2018). They introduce a weakly supervised, self-learning model, which can induce a dictionary given only a handful of initial, seed translations. This is ac"
2020.coling-main.256,L16-1498,0,0.0605189,"3, given that inflectional morphology is present in the induced lexicon, BLI models should be trained and evaluated on datasets which list a range of compatible inflected form pairs for every source-target lexeme pair. At this time, the dictionaries of Czarnowska et al. (2019) are the only publicly available resource that meets this criterion, and, for this reason, they are the most important evaluation benchmark used in this work. The dictionaries were generated based on Open Multilingual WordNet (Bond and Paik, 2012), Extended Open Multilingual WordNet (Bond and Foster, 2013) and UniMorph8 (Kirov et al., 2016; McCarthy et al., 2020), a resource comprised of inflectional word paradigms for 107 languages. The dictionaries only list parts of speech that undergo inflection in either the source or the target language; these are nouns, adjectives and verbs in the Romance languages. Conneau et al. (2018). MUSE (Conneau et al., 2018) was generated using an “internal translation tool” and is one of the few other resources which covers pairs of Romance languages. However, it is skewed towards most frequent forms: The vast majority of forms in MUSE are ranked in the top 10k of the vocabularies in their respe"
2020.coling-main.256,W02-0902,0,0.391248,"lected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et a"
2020.coling-main.256,P93-1003,0,0.087534,"cographers have chosen to be representative of the lexeme. For example, the lexeme RUN’s lemma is run. In many languages, the infinitive is the verbal lemma and the nominative singular is the nominal lemma. We consider a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) le"
2020.coling-main.256,P15-1027,0,0.0406645,"Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many tim"
2020.coling-main.256,W19-4226,1,0.850478,"ctored into two parts. The first part, the inflector, produces an inflected form ιt given a lemma λt and a morphological tag τt . This problem has been well studied in the NLP literature (Cotterell et al., 2016; Cotterell et al., 2017). The second part, tag translator, determines the possible target-side morphological tags that are compatible with the features present in the source tag. In principle, our model is compatible with any probabilistic inflector. In this paper, we employ the model of Wu et al. (2019b), which obtained the single-model state of the art at the time of experimentation (McCarthy et al., 2019). The model has a latent character-level monotonic alignment between the source and target inflections that is jointly learned with the transducer and is, in effect, a neuralized version of a hidden Markov model for translation (Vogel et al., 1996). 2849 In this work we focus on closely related languages and make a simplifying assumption that there exists a single most-plausible translation for each inflected form.4 We formalize the tag translator using an indicator function: ( 1 if τt = τs p(τt |τs ) = 0 if τt 6= τs For experiments with more distant language pairs one can define p(τt |τs ) to"
2020.coling-main.256,W16-1614,0,0.0589174,"ms belonging to different morpho-syntactic categories for French–Spanish. was a success, but, on the other, our more nuanced conclusion is that the task of BLI, as currently researched in NLP, is ill-defined with respect to inflectional morphology. Indeed, the authors suggest that BLI needs redirection going forward. The recent trend in BLI research is to remain data-driven and to avoid specialist linguistic annotation. Current projection-based approaches to BLI depend heavily on the assumption that the lexicons of different languages are approximately isomorphic (Mikolov et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none o"
2020.coling-main.256,P18-2036,0,0.0223414,"t the lexicons of different languages are approximately isomorphic (Mikolov et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none of those studies directly target inflectional morphology. In this work we highlight that inflectional morphology complicates BLI and NLP researchers should strive to develop a cleaner way to integrate it into their models. We contend the models we present make progress in this direction but there is still a long way to go. We now make three concrete suggestions for BLI going forward. The first two involve engaging with morphology more seriously and are extensions to the ideas in this paper. The third focuses on b"
2020.coling-main.256,P19-1492,0,0.0209653,"anguages are approximately isomorphic (Mikolov et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none of those studies directly target inflectional morphology. In this work we highlight that inflectional morphology complicates BLI and NLP researchers should strive to develop a cleaner way to integrate it into their models. We contend the models we present make progress in this direction but there is still a long way to go. We now make three concrete suggestions for BLI going forward. The first two involve engaging with morphology more seriously and are extensions to the ideas in this paper. The third focuses on backing away from morphol"
2020.coling-main.256,P19-1018,0,0.0207411,"v et al., 2013; Miceli Barone, 2016). However, given the immense variation in morphological systems of worlds’ languages, this assumption is prima fascie false. Consider the simple contradiction of Spanish and English, where the first exhibits much more morphological inflection than the latter; there can be no one-to-one alignment between the words in those two lexicons. The failure of the isomorphism assumption has been discussed and addressed in many recent works on cross-lingual word embeddings (Søgaard et al., 2018; Nakashole and Flauger, 2018; Ormazabal et al., 2019; Vuli´c et al., 2019; Patra et al., 2019). However, none of those studies directly target inflectional morphology. In this work we highlight that inflectional morphology complicates BLI and NLP researchers should strive to develop a cleaner way to integrate it into their models. We contend the models we present make progress in this direction but there is still a long way to go. We now make three concrete suggestions for BLI going forward. The first two involve engaging with morphology more seriously and are extensions to the ideas in this paper. The third focuses on backing away from morphology. More Fine-Grained Lexicons. Our first"
2020.coling-main.256,P95-1050,0,0.688785,"a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013;"
2020.coling-main.256,P18-2062,0,0.0349516,"edictable items (e.g. Prasada and Pinker (1993) Pinker and Prince (1994)). 3 For more references we refer the reader to the survey of Ruder et al. (2019). 2848 evaluated on datasets that contain a more representative range of morphological inflections. We use the term morphologically enriched dictionary for such bilingual lexicons (see §5.1). To our knowledge, we are the first to explicitly model inflectional morphology in BLI. Closest to our endeavor, Yang et al. (2019) address morphology in BLI by incorporating grammatical information learned by a pre-trained denoising language model, while Riley and Gildea (2018) enhance the projection-based approach of Artetxe et al. (2017) with orthographic features to improve performance on BLI for related languages. 3 A Joint Model for Morphologically Aware Word-level Translation The primary contribution of this work is a morphologically aware probabilistic model for word-level translation. Our model exploits a simple intuition: Because the core unit of meaning is the lexeme, one should translate through the lexeme and then inflect the word according to the target language’s morphology. Notation. In the task of BLI, we consider a source language s and a target lan"
2020.coling-main.256,D18-1042,1,0.886316,"n for both the source and the target language, although in practice these look-up functions are distinct. The model has a single matrix of parameters: Ω ∈ RNt ×Ns where Ns is the source embedding dimensionality and Nt the target embedding dimensionality. Our translator is defined as the following conditional model p(λt |λs ) =  1 exp e(λt )> Ω e(λs ) Z(λs ) (3) X (4) where the normalizer is defined as Z(λs ) = exp e(λ0t )> Ω e(λs )  λ0t ∈Lt Note that this log-bilinear model differs from most embedding-based bilingual lexicon inducers which predict embeddings, rather than words. For example, Ruder et al. (2018)’s approach contains a multivariate Gaussian over the target-language’s embedding space.5 Orthogonal Regularization. During training we employ a special regularization term on the parameter matrix Ω. Specifically, we use R(Ω) = α Ω> Ω − I (5) F with a tunable “strength” hyperparameter α ∈ R≥0 . This term encourages the translation matrix to be orthogonal, which has led to consistent gains in past work (Xing et al., 2015; Artetxe et al., 2016; Ruder et al., 2018). 3.3 The Analyzer: p(λs , τs |ιs ) For our probabilistic analyzer we use the same hard attention model as in the inflector. The model"
2020.coling-main.256,H94-1027,0,0.381259,"e chosen to be representative of the lexeme. For example, the lexeme RUN’s lemma is run. In many languages, the infinitive is the verbal lemma and the nominative singular is the nominal lemma. We consider a lexicon of a language to be a set of inflected forms.2 2.2 Bilingual Lexicon Induction In the NLP literature, the BLI task is to translate a given list of source-side word forms into the most appropriate corresponding target-side word forms. It dates back to 1990s. The first data-driven experiments on parallel corpora made use of word-alignment techniques (Brown et al., 1990; Kupiec, 1993; Smadja and McKeown, 1994). Such approaches were later extended to operate on non-parallel or even unrelated texts by leveraging the correlation between word co-occurrence patterns in different languages (Rapp, 1995; Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation"
2020.coling-main.256,P18-1072,1,0.891487,"Missing"
2020.coling-main.256,C96-2141,0,0.765278,"t, tag translator, determines the possible target-side morphological tags that are compatible with the features present in the source tag. In principle, our model is compatible with any probabilistic inflector. In this paper, we employ the model of Wu et al. (2019b), which obtained the single-model state of the art at the time of experimentation (McCarthy et al., 2019). The model has a latent character-level monotonic alignment between the source and target inflections that is jointly learned with the transducer and is, in effect, a neuralized version of a hidden Markov model for translation (Vogel et al., 1996). 2849 In this work we focus on closely related languages and make a simplifying assumption that there exists a single most-plausible translation for each inflected form.4 We formalize the tag translator using an indicator function: ( 1 if τt = τs p(τt |τs ) = 0 if τt 6= τs For experiments with more distant language pairs one can define p(τt |τs ) to be a multi-label classifier. 3.2 The Translator: p(λt |λt ) As our translator, we construct a log-bilinear model that yields a distribution over all elements in the target lexicon. We assume the existence of both source- and target-side embeddings"
2020.coling-main.256,D19-1449,0,0.0245645,"Missing"
2020.coling-main.256,P19-1505,1,0.888004,"ns of handling irregular morphology, beyond the irregular forms they have been exposed to during training. Guided by this insight, we propose an alternative version of our model, which employs a special treatment for forms likely to have irregular morphology—the most frequent forms (Bybee, 1995a; Baayen and Lieber, 1996; Wu et al., 2019a). We term this extension the hybrid model. It employs a frequency-based heuristic and translates the source form through its lemma only if the lemma is more frequent.6 Otherwise, it translates the inflected form 4 Indeed, the dictionaries of Czarnowska et al. (2019) on which we experiment also make this assumption. Note that the translator’s distribution, as defined in eq. (3) and eq. (4), is over all inflected forms in the target lexicon. An alternative would be to define a distribution over lemmata only. However, this would require filtering out all non-lemma forms from the target embedding matrix, which is not trivial. In our preliminary experiments, we observed that this can lead to a further performance increase. 6 We rely on the order of FAST T EXT embeddings for the relative ranking of inflected forms. 5 2850 directly, using only the translator co"
2020.coling-main.256,P19-1148,1,0.891112,"ιt ,τt i∈π t hιt ,τt i∈π t inflector tag translator The joint distribution over forms and tags is factored into two parts. The first part, the inflector, produces an inflected form ιt given a lemma λt and a morphological tag τt . This problem has been well studied in the NLP literature (Cotterell et al., 2016; Cotterell et al., 2017). The second part, tag translator, determines the possible target-side morphological tags that are compatible with the features present in the source tag. In principle, our model is compatible with any probabilistic inflector. In this paper, we employ the model of Wu et al. (2019b), which obtained the single-model state of the art at the time of experimentation (McCarthy et al., 2019). The model has a latent character-level monotonic alignment between the source and target inflections that is jointly learned with the transducer and is, in effect, a neuralized version of a hidden Markov model for translation (Vogel et al., 1996). 2849 In this work we focus on closely related languages and make a simplifying assumption that there exists a single most-plausible translation for each inflected form.4 We formalize the tag translator using an indicator function: ( 1 if τt ="
2020.coling-main.256,N15-1104,0,0.0421115,"Fung and Lo, 1998; Fung, 1998; Koehn and Knight, 2002). Apart from the distributional signal, the early approaches make use of other monolingual clues, e.g. word spelling, cognates or word frequency.3 More recent approaches leverage the distributional signal in word embeddings without any explicit linguistic clues. Many current models (Mikolov et al., 2013; Ruder et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, whi"
2020.coling-main.256,P19-1308,0,0.0385215,"975), McClelland et al. (1987) and Bybee (1995b), and stands in opposition to alternative views of the lexicon being comprised of only the unpredictable items (e.g. Prasada and Pinker (1993) Pinker and Prince (1994)). 3 For more references we refer the reader to the survey of Ruder et al. (2019). 2848 evaluated on datasets that contain a more representative range of morphological inflections. We use the term morphologically enriched dictionary for such bilingual lexicons (see §5.1). To our knowledge, we are the first to explicitly model inflectional morphology in BLI. Closest to our endeavor, Yang et al. (2019) address morphology in BLI by incorporating grammatical information learned by a pre-trained denoising language model, while Riley and Gildea (2018) enhance the projection-based approach of Artetxe et al. (2017) with orthographic features to improve performance on BLI for related languages. 3 A Joint Model for Morphologically Aware Word-level Translation The primary contribution of this work is a morphologically aware probabilistic model for word-level translation. Our model exploits a simple intuition: Because the core unit of meaning is the lexeme, one should translate through the lexeme and"
2020.coling-main.256,P17-1179,0,0.0330424,"r et al., 2019) learn a linear transformation between two monolingual word embedding spaces, often guided by an initial set of seed translations. This seed dictionary frequently spans several thousand word pairs (Mikolov et al., 2013; Xing et al., 2015; Lazaridou et al., 2015; Artetxe et al., 2016) but one can also provide weaker supervision, through listing only identical strings or shared numerals (Artetxe et al., 2017; Søgaard et al., 2018). For unsupervised BLI, the initial translations may also be induced automatically through exploiting the structure of the monolingual embedding spaces (Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b). We focus on supervised and weakly supervised BLI which outperform unsupervised approaches (Glavaˇs et al., 2019). The BLI models are typically evaluated using the precision@k metric, which tells us how many times the correct translation of a source form is among the k-best candidates returned by the model. In this work we exclusively consider the precision@1 metric, which is the least forgiving. 2.3 Morphological Inflection: A Challenge for BLI Most datasets for BLI operate at the level of inflected forms and impose no restriction on the morphosy"
2020.emnlp-demos.7,S17-2001,0,0.0526606,"Missing"
2020.emnlp-demos.7,2020.acl-main.747,0,0.143648,"Missing"
2020.emnlp-demos.7,N19-1423,0,0.0455398,"state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in lowresource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml. 1 Introduction Recent advances in NLP leverage transformerbased language models (Vaswani et al., 2017), pretrained on large amounts of text data (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020). These models are fine-tuned on a target task and achieve state-of-the-art (SotA) performance for most natural language understanding tasks. Their performance has been shown to scale with their size (Kaplan et al., 2020) and recent models have reached ∗ 1 *Equal contribution. https://github.com/huggingface/transformers 46 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 46–54 c November 16-20, 2020. 2020 Association for Computational Linguistics hance transformers with adapter modules that can be combined with existing SotA models with min"
2020.emnlp-demos.7,N16-1181,0,0.0279735,"l´ 2 Aishwarya Kamath , Ivan Vuli´c4 , Sebastian Ruder5 , Kyunghyun Cho2,3 , Iryna Gurevych1 1 Technical University of Darmstadt 2 New York University 3 CIFAR Associate Fellow 4 University of Cambridge 5 DeepMind AdapterHub.ml Abstract billions of parameters (Raffel et al., 2019; Brown et al., 2020). While fine-tuning large pre-trained models on target task data can be done fairly efficiently (Howard and Ruder, 2018), training them for multiple tasks and sharing trained models is often prohibitive. This precludes research on more modular architectures (Shazeer et al., 2017), task composition (Andreas et al., 2016), and injecting biases and external information (e.g., world or linguistic knowledge) into large models (Lauscher et al., 2019; Wang et al., 2020). Adapters (Houlsby et al., 2019) have been introduced as an alternative lightweight fine-tuning strategy that achieves on-par performance to full fine-tuning (Peters et al., 2019) on most tasks. They consist of a small set of additional newly initialized weights at every layer of the transformer. These weights are then trained during fine-tuning, while the pre-trained parameters of the large model are kept frozen/fixed. This enables efficient parame"
2020.emnlp-demos.7,I05-5002,0,0.0149582,"Missing"
2020.emnlp-demos.7,D19-1165,0,0.313909,"e been trained for particular tasks, domains, and languages. This opens up the possibility of building on and combining information from many more sources than was previously possible, and makes research such as intermediate task training (Pruksachatkun et al., 2020), composing information from many tasks (Pfeiffer et al., 2020a), and training models for very low-resource languages (Pfeiffer et al., 2020b) much more accessible. representations in intermediate layers of the pretrained model. Current work predominantly focuses on training adapters for each task separately (Houlsby et al., 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a,b), which enables parallel training and subsequent combination of the weights. In NLP, adapters have been mainly used within deep transformer-based architectures (Vaswani et al., 2017). At each transformer layer l, a set of adapter parameters Φl is introduced. The placement and architecture of adapter parameters Φ within a pre-trained model is non-trivial and may impact their efficacy: Houlsby et al. (2019) experiment with different adapter architectures, empirically validating that a two-layer feed-forward neural network with a bottleneck works well. While this down-"
2020.emnlp-demos.7,W07-1401,0,0.115017,"Missing"
2020.emnlp-demos.7,2020.acl-main.740,0,0.0195176,"no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracted and open-sourced (®) and visualized (¯). Pre-trained adapters are downlo"
2020.emnlp-demos.7,2020.acl-main.467,0,0.0786619,"Missing"
2020.emnlp-demos.7,P18-1031,1,0.821346,"tely, the necessity of sampling heuristics due to skewed data set sizes no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracte"
2020.emnlp-demos.7,D16-1264,0,0.0151158,"Missing"
2020.emnlp-demos.7,N19-5004,1,0.857724,"Missing"
2020.emnlp-demos.7,W19-4302,1,0.887479,"Missing"
2020.emnlp-demos.7,2020.emnlp-main.617,1,0.706042,"Missing"
2020.emnlp-demos.7,D13-1170,0,0.0449199,"020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of additional newly introduced parameters Φ within a large pre-trained model with parameters Θ. The parameters Φ are learnt on a target task while keeping Θ fixed; Φ thus learn to encode task-specific 2 Layer normalization learns to normalize the inputs across the features. This is usually done by introducing a new set of features for mean and variance. 47 Full RTE (Wang et al., 2018) MRPC (Dolan and Brockett, 2005) STS-B (Cer et al., 2017) CoLA (Warstadt et al., 2019) SST-2 (Socher et al., 2013) QNLI (Rajpurkar et al., 2016) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) 66.2 90.5 88.8 59.5 92.6 91.3 84.1 91.4 Pfeif. Houl. 70.8 89.7 89.0 58.9 92.2 91.3 84.1 90.5 CRate 69.8 91.5 89.2 59.1 92.8 91.2 84.1 90.8 64 16 2 Base #Params Size Large #Params Size 0.2M 0.9M 7.1M 0.9Mb 3.5Mb 28Mb 0.8M 3.1M 25.2M 3.2Mb 13Mb 97Mb Table 2: Number of additional parameters and compressed storage space of the adapter of Pfeiffer et al. (2020a) in (Ro)BERT(a)-Base and Large transformer architectures. The adapter of Houlsby et al. (2019) requires roughly twice as much space. CRate refers to the adap"
2020.emnlp-demos.7,P19-1355,0,0.0848206,"Missing"
2020.emnlp-demos.7,W18-5446,0,0.202668,"scalability, modularity, and composition. We now provide a few use-cases for adapters to illustrate their usefulness in practice. Task-specific Layer-wise Representation Learning. Prior to the introduction of adapters, in order to achieve SotA performance on downstream tasks, the entire pre-trained transformer model needs to be fine-tuned (Peters et al., 2019). Adapters have been shown to work on-par with full fine-tuning, by adapting the representations at every layer. We present the results of fully fine-tuning the model compared to two different adapter architectures on the GLUE benchmark (Wang et al., 2018) in Table 1. The adapters of Houlsby et al. (2019, Figure 3c) and Pfeiffer et al. (2020a, Figure 3b) comprise two and one down- and up-projection Adapters While the predominant methodology for transfer learning is to fine-tune all weights of the pre-trained model, adapters have recently been introduced as an alternative approach, with applications in computer vision (Rebuffi et al., 2017) as well as the NLP domain (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of"
2020.emnlp-demos.7,Q19-1040,0,0.0324564,"Missing"
2020.emnlp-demos.7,N18-1101,0,0.0345727,"Missing"
2020.emnlp-main.257,W13-3520,0,0.254793,"which has so far been largely attributed only to inherent typological differences. In fact, the amount of data used to induce the monolingual embeddings is predictive of the quality of the aligned cross-lingual word embeddings, as evaluated on bilingual lexicon induction (BLI). Consider, for motivation, Figure 1; it shows the performance of a state-of-the-art alignment method— RCSLS with iterative normalisation (Zhang et al., 2019)—on mapping English embeddings onto embeddings in other languages, and its correlation (ρ = 0.72) with the size of the tokenised target language Polyglot Wikipedia (Al-Rfou et al., 2013). We investigate to what extent the amount of data available for some languages and corresponding training conditions provide a sufficient explanation for the variance in reported results; that is, whether it is the full story or not: The answer is ’almost’, 3178 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3178–3192, c November 16–20, 2020. 2020 Association for Computational Linguistics that is, its interplay with inherent typological differences does have a crucial impact on the ‘alignability’ of monolingual vector spaces. We first discuss cur"
2020.emnlp-main.257,D19-1328,1,0.761399,"IM; higher is better) computed across different AR and JA snapshots. 2019) whose neighbourhoods may be more isomorphic (Nakashole, 2018). We thus create new evaluation dictionaries for English–Spanish that consist of words in different frequency bins: we sample EN words for 300 translation pairs respectively from (i) the top 5k words of the full English Wikipedia (HFREQ); (ii) the interval [10k, 20k] (MFREQ); (iii) the interval [20k, 50k] (LFREQ). The entire dataset (ALL -FREQ; 900 pairs) consists of (i) + (ii) + (iii). We exclude named entities as they are over-represented in many test sets (Kementchedjhieva et al., 2019) and include nouns, verbs, adjectives, and adverbs in all three sets. All 900 words have been carefully manually translated and double-checked by a native Spanish speaker. There are no duplicates. We also report BLI results on the PanLex test lexicons (Vuli´c et al., 2019). For English–Spanish, we create training dictionaries of sizes 1k and 5k based on PanLex (Kamholz et al., 2014) following Vuli´c et al. (2019). We exclude all words from ALL -FREQ from the training set. For EN–JA / AR BLI experiments, we rely on the standard training and test dictionaries from the MUSE benchmark (Conneau et"
2020.emnlp-main.257,N13-1090,0,0.737461,"sv hu he vi de pl ja ru zh uk 20M lt 50M 100M 200M # of tokens in Wikipedia 500M Figure 1: Performance of a state-of-the-art BLI model mapping from English to a target language and the size of the target language Wikipedia are correlated. Linear fit shown as a blue line (log scale). Word embeddings have been argued to reflect how language users organise concepts (Mandera et al., 2017; Torabi Asr et al., 2018). The extent to which they really do so has been evaluated, e.g., using semantic word similarity and association norms (Hill et al., 2015; Gerz et al., 2016), and word analogy benchmarks (Mikolov et al., 2013c). If word embeddings reflect more or less languageindependent conceptual organisations, word embeddings in different languages can be expected to be near-isomorphic. Researchers have exploited this to learn linear transformations between such spaces (Mikolov et al., 2013a; Glavaˇs et al., 2019), which have been used to induce bilingual dictionaries, as well as to facilitate multilingual modeling and cross-lingual transfer (Ruder et al., 2019). In this paper, we show that near-isomorphism arises only with sufficient amounts of training. This is of practical interest for applications of linear"
2020.emnlp-main.257,D18-1047,0,0.147371,"For clarity, the corresponding isomorphism scores (and impact of training duration on isomorphism of vector spaces) over the same training snapshots for Spanish are shown in Figure 5. (c) We again report scores without and with self-learning on EN– AR / JA BLI evaluation sets from the MUSE benchmark with 1k seed translation pairs. The results with 5k seed pairs for EN–AR / JA are available in the appendix. Dashed lines without any marks show isomorphism scores (computed by RSIM; higher is better) computed across different AR and JA snapshots. 2019) whose neighbourhoods may be more isomorphic (Nakashole, 2018). We thus create new evaluation dictionaries for English–Spanish that consist of words in different frequency bins: we sample EN words for 300 translation pairs respectively from (i) the top 5k words of the full English Wikipedia (HFREQ); (ii) the interval [10k, 20k] (MFREQ); (iii) the interval [20k, 50k] (LFREQ). The entire dataset (ALL -FREQ; 900 pairs) consists of (i) + (ii) + (iii). We exclude named entities as they are over-represented in many test sets (Kementchedjhieva et al., 2019) and include nouns, verbs, adjectives, and adverbs in all three sets. All 900 words have been carefully ma"
2020.emnlp-main.257,D18-1268,0,0.0306659,"een a source and a target embedding space (Mikolov et al., 2013a). Such mapping-based approaches assume that the monolingual embedding spaces are isomorphic, i.e., that one can be transformed into the other via a linear transformation (Xing et al., 2015; Artetxe et al., 2018a). Recent unsupervised approaches rely even more strongly on this assumption: They assume that the structures of the embedding spaces are so similar that they can be aligned by minimising the distance between the transformed source language and the target language embedding space (Zhang et al., 2017; Conneau et al., 2018; Xu et al., 2018; Alvarez-Melis and Jaakkola, 2018; Hartmann et al., 2019). 2.1 Quantifying Isomorphism We employ measures that quantify isomorphism in three distinct ways—based on graphs, metric spaces, and vector similarity. Eigenvector similarity (Søgaard et al., 2018) Eigenvector similarity (EVS) estimates the degree of isomorphism based on properties of the nearest neighbour graphs of the two embedding spaces. We first length-normalise embeddings in both embedding spaces and compute the nearest neighbour graphs on a subset of the top most frequent N words. We then calculate the Laplacian matrices L1 and"
2020.emnlp-main.257,P16-1023,0,0.146189,"Missing"
2020.emnlp-main.257,J15-4004,0,\N,Missing
2020.emnlp-main.257,D14-1162,0,\N,Missing
2020.emnlp-main.257,kamholz-etal-2014-panlex,0,\N,Missing
2020.emnlp-main.257,N15-1104,0,\N,Missing
2020.emnlp-main.257,Q17-1010,0,\N,Missing
2020.emnlp-main.257,D16-1099,0,\N,Missing
2020.emnlp-main.257,P17-1042,0,\N,Missing
2020.emnlp-main.257,D17-1207,0,\N,Missing
2020.emnlp-main.257,D18-1056,1,\N,Missing
2020.emnlp-main.257,P19-1492,0,\N,Missing
2020.emnlp-main.257,P19-1018,0,\N,Missing
2020.emnlp-main.257,D19-1449,1,\N,Missing
2020.emnlp-main.257,D15-1243,0,\N,Missing
2020.emnlp-main.617,D16-1264,0,0.11385,"Missing"
2020.emnlp-main.617,D19-1454,0,0.0261635,"anguages, it achieves competitive performance even for high-resource languages and on more challenging tasks. These evaluations also hint at the modularity of the adapter-based MAD-X approach, which holds promise of quick adaptation to more tasks: we use exactly the same languagespecific adapters in NER, CCR, and QA for languages such as English and Mandarin Chinese that appear in all three evaluation language samples. 7 Further Analysis Impact of Invertible Adapters We also analyse the relative performance difference of MAD-X 80 60 F1 ting from Ponti et al. (2020a)—fine-tuning first on SIQA (Sap et al., 2019) and on the English COPA training set—and report other possible settings in the appendix. Target language adaptation outperforms XLM-RBase while MAD-XBase achieves the best scores. It shows gains in particular for the two unseen languages, Haitian Creole (ht) and Quechua (qu). Performance on the other languages is also generally competitive or better. 40 Language qu cdo ilo 20 0 0 20k 40k xmf mi mhr 60k Number of iterations Epochs 25 50 100 tk gn 80k 100k Figure 4: Cross-lingual NER performance of MAD-X transferring from English to the target languages with invertible and language adapters tra"
2020.emnlp-main.617,N19-1423,0,\N,Missing
2020.emnlp-main.617,D19-1572,1,\N,Missing
2020.emnlp-main.617,W19-4330,0,\N,Missing
2020.emnlp-main.617,D19-1165,0,\N,Missing
2020.emnlp-main.692,D13-1170,0,\N,Missing
2020.emnlp-main.692,W19-5034,0,\N,Missing
2020.emnlp-main.692,P19-1513,0,\N,Missing
2020.emnlp-main.692,N19-1423,0,\N,Missing
2020.emnlp-main.692,2020.acl-main.398,0,\N,Missing
2021.acl-long.243,2020.emnlp-main.367,0,0.649627,"e model capacity (Artetxe et al., 2020; Pfeiffer et al., 2020b; Chau et al., 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020b; Ponti et al., 2020). Another observation concerns substantially reduced crosslingual and monolingual abilities of the models for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020). Those languages remain underrepresented in the subword vocabulary and the model’s shared representation space despite oversampling. Despite recent efforts to mitigate this issue (e.g., Chung et al. (2020) propose to cluster and merge the vocabularies of similar languages, before defining a joint vocabulary across all languages), the multilingual LMs still struggle with balancing their parameters across many languages. Monolingual versus Multilingual LMs. New monolingual language-specific models also emerged for many languages, following BERT’s architecture and pretraining procedure. There are monolingual BERT variants for Arabic (Antoun et al., 2020), French (Martin et al., 2020), Finnish (Virtanen et al., 2019), Dutch (de Vries et al., 2019), to name only a few. Pyysalo et al. (2020) released"
2021.acl-long.243,2020.tacl-1.30,0,0.117373,"ng) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks. Regarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties. Regarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (R¨onnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vuli´c et al., 2020), we ensure that our experimental framework takes both into account, thus also satisfying C3. We achieve task diversity and generalizability by selecting a combination of tasks driven by lowe"
2021.acl-long.243,2020.acl-main.747,0,0.0639935,"Missing"
2021.acl-long.243,D18-1269,0,0.0249882,"9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4 3 Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1. 4 Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4. 3120 Language ISO Language Family Pretrained BERT Model Arabic English Finnish Indonesian Japanese Korean Russian Turkish Chinese AR Afroasiatic Indo-European Uralic Austronesian Japonic Koreanic Indo-European Turkic Sino-Tibetan AraBERT (Antoun et al., 2020) BERT (D"
2021.acl-long.243,D18-1029,1,0.883569,"Missing"
2021.acl-long.243,2021.eacl-main.270,1,0.685166,"Missing"
2021.acl-long.243,2020.acl-main.560,0,0.0242538,"ir comparisons. 3.1 Language and Task Selection Our selection of languages has been guided by several (sometimes competing) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks. Regarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties. Regarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (R¨onnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vuli´c et al., 2020), we ensure that our experimental framework takes both into account, thu"
2021.acl-long.243,2020.emnlp-main.363,1,0.857757,"Missing"
2021.acl-long.243,2020.acl-main.653,0,0.0385977,". Finally, we select a set of 9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4 3 Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1. 4 Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4. 3120 Language ISO Language Family Pretrained BERT Model Arabic English Finnish Indonesian Japanese Korean Russian Turkish Chinese AR Afroasiatic Indo-European Uralic Austronesian Japonic Koreanic Indo-European Turkic Sino-Tibetan AraBE"
2021.acl-long.243,N19-1392,0,0.0619148,"verified the scores, nor have they performed a controlled impartial comparison. Vuli´c et al. (2020) probed mBERT and monolingual BERT models across six typologically diverse languages for lexical semantics. They show that pretrained monolingual BERT models encode significantly more lexical information than mBERT. Zhang et al. (2020) investigated the role of pretraining data size with RoBERTa, finding that the model learns most syntactic and semantic features on corpora spanning 10M–100M word tokens, but still requires massive datasets to learn higher-level semantic and commonsense knowledge. Mulcaire et al. (2019) compared monolingual and bilingual ELMo (Peters et al., 2018) LMs across three downstream tasks, finding that contextualized representations from the bilingual models can improve monolingual task performance relative to their monolingual counterparts.2 However, it is unclear how their findings extend to massively multilingual LMs potentially suffering from the curse of multilinguality. R¨onnqvist et al. (2019) compared mBERT to monolingual BERT models for six languages (German, English, Swedish, Danish, Norwegian, Finnish) on three different tasks. They find that mBERT lags behind its monolin"
2021.acl-long.47,N19-1131,0,0.0249616,"80.74±7.6 86.67±5.0 60.74±16.66 76.29±4.45 81.48±6.2 87.41±2.96 89.63±4.32 Question Classification TREC 4 16 32 100 500 1000 2000 28.11±5.9 40.08±12.6 62.49±6.2 87.79±0.7 93.57±1.3 95.5±0.9 96.87±1.3 23.61±7.7 43.45±14.0 59.6±7.0 78.07±3.8 93.65±1.7 96.06±0.4 97.03±0.7 28.85±6.9 49.40±9.5 68.94±7.5 88.42±1.7 94.78±1.4 96.72±1.3 96.92±0.9 Question Answering BoolQ 3.3 Low-resource Fine-tuning Average scores on GLUE Sa m pl es Dataset # et al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002). For CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the ada"
2021.acl-long.47,2020.coling-main.448,0,0.0333384,"ltiple challenges such as catastrophic forgetting, and handling disproportionate task sizes resulting in a model overfitting in lowresource tasks while underfitting in high-resource ones (Arivazhagan et al., 2019). Liu et al. (2019a) proposed Multi-Task Deep Neural Network (MTDNN) for learning from multiple NLU tasks. Although MTDNN obtains impressive results on GLUE, it applies multi-task learning as a form of pretraining followed by task-specific fine-tuning. Concurrently 572 Prior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020). Meta-learning approaches are notoriously slow to train. In addition, generating softmax parameters requires a substantially higher number of parameters, leaves the method unable to adapt the lower layers of the model, and restricts their application to classification tasks. ¨ un et al. (2020) In contemporaneous work, Ust¨ proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks (Platanios et al., 2018) where they generate adapter parameters conditioned on trained input language embeddings. Their study is limited to m"
2021.acl-long.47,D19-1165,0,0.0354995,"generating separate adapter layers for each task. For each new task, our model only requires learning an additional task embedding, reducing the number of trained parameters. We use the encoder-decoder T5 model (Raffel et al., 2020) as the underlying model for our experiments and evaluate on the standard GLUE benchmark (Wang et al., 2019b). We achieve strong gains over both the T5BASE model as well as adapters (Houlsby et al., 2019). To our knowledge, this is the first time that adapters have been successfully integrated into a stateof-the-art encoder-decoder model beyond machine translation (Bapna and Firat, 2019), demonstrating that our method effectively balances sharing information across tasks while minimizing negative transfer. In summary, we make the following contributions: (1) We propose a parameter-efficient method for multitask fine-tuning based on hypernetworks and adapter layers. (2) We demonstrate that our method scales more efficiently than prior work. (3) We provide empirical results on GLUE demonstrating the effectiveness of the proposed method on multi-task learning. (4) We perform extensive few-shot domain transfer experiments, which reveal that the captured shared knowledge can posit"
2021.acl-long.47,2020.acl-main.625,0,0.0335057,"pter parameters, which is more parameter-inefficient than our method. Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights of the target model. Our method is substantially more efficient as we do not generate all the weights of the target model but a very small number of parameters for adapter modules to allow the model to adapt to each individual task efficiently. Similarly, Jin et al. (2020) generate the full model from task-specific descriptions in different domains whereas we efficiently generate only small adapter modules for each task. 5 Related Work Multi-task learning: Multi-task learning, i.e., learning a unified model to perform well on multiple different tasks, is a challenging problem in NLP. It requires addressing multiple challenges such as catastrophic forgetting, and handling disproportionate task sizes resulting in a model overfitting in lowresource tasks while underfitting in high-resource ones (Arivazhagan et al., 2019). Liu et al. (2019a) proposed Multi-Task Dee"
2021.acl-long.47,C02-1150,0,0.0401945,"4.32 Question Classification TREC 4 16 32 100 500 1000 2000 28.11±5.9 40.08±12.6 62.49±6.2 87.79±0.7 93.57±1.3 95.5±0.9 96.87±1.3 23.61±7.7 43.45±14.0 59.6±7.0 78.07±3.8 93.65±1.7 96.06±0.4 97.03±0.7 28.85±6.9 49.40±9.5 68.94±7.5 88.42±1.7 94.78±1.4 96.72±1.3 96.92±0.9 Question Answering BoolQ 3.3 Low-resource Fine-tuning Average scores on GLUE Sa m pl es Dataset # et al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002). For CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the adapter and the task embedding respectively trained on the most simi"
2021.acl-long.47,P19-1441,0,0.0584278,"s (βτ and γτ ). During training, we only update layer normalizations in T5, hypernetworks, and task embeddings. The compact HYPERFORMER++ shares the same hypernetworks across all layers and tasks and computes the task embedding based on task, layer id, and position of the adapter module (§2.4). model. 2) Fine-tuning the model across multiple tasks allows sharing information between the different tasks and positive transfer to other related tasks. Specifically, when target datasets have limited training data, multi-task learning improves the performance compared to individually trained models (Liu et al., 2019a; Ratner et al., 2018). However, multi-task fine-tuning can result in models underperforming on high-resource tasks due to constrained capacity (Arivazhagan et al., 2019; McCann et al., 2018). An additional issue with multi-task fine-tuning is the potential for task interference or negative transfer, 565 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 565–576 August 1–6, 2021. ©2021 Association for Computational Linguistics where achieving good performance on one task can"
2021.acl-long.47,N19-1300,0,0.0187174,"ifferent from other tasks and is not grouped with any of the observed task embeddings. In addition, the task embeddings for 1) all the sentiment analysis datasets namely SST-2, Yelp polarity, and IMDB; 2) the two large-scale NLI datasets namely MNLI and SciTail; 3) question answering datasets, i.e. BoolQ and QNLI; and 4) paraphrase datasets namely QQP and PAWS are each grouped together. with us, Tay et al. (2021) propose a multi-task learning method by training task-conditioned hyper networks; however, their method is 43x less parameter efficient compared to ours. In another line of research, Clark et al. (2019b) proposed to learn multi-task models with knowledge distillation. Houlsby et al. (2019) trained adapters for each task separately, keeping the model fixed. Stickland and Murray (2019) share the model parameters across tasks and introduce task-specific adapter parameters, which is more parameter-inefficient than our method. Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights"
2021.acl-long.47,P11-1015,0,0.0361431,"3.40±0.2 4 16 32 100 250 57.78±10.9 77.04±7.2 80.0±7.6 85.93±5.4 85.19±4.7 51.11±9.2 74.81±5.4 74.81±5.9 80.74±7.6 86.67±5.0 60.74±16.66 76.29±4.45 81.48±6.2 87.41±2.96 89.63±4.32 Question Classification TREC 4 16 32 100 500 1000 2000 28.11±5.9 40.08±12.6 62.49±6.2 87.79±0.7 93.57±1.3 95.5±0.9 96.87±1.3 23.61±7.7 43.45±14.0 59.6±7.0 78.07±3.8 93.65±1.7 96.06±0.4 97.03±0.7 28.85±6.9 49.40±9.5 68.94±7.5 88.42±1.7 94.78±1.4 96.72±1.3 96.92±0.9 Question Answering BoolQ 3.3 Low-resource Fine-tuning Average scores on GLUE Sa m pl es Dataset # et al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002). For CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test"
2021.acl-long.47,P19-1595,0,0.0171301,"ifferent from other tasks and is not grouped with any of the observed task embeddings. In addition, the task embeddings for 1) all the sentiment analysis datasets namely SST-2, Yelp polarity, and IMDB; 2) the two large-scale NLI datasets namely MNLI and SciTail; 3) question answering datasets, i.e. BoolQ and QNLI; and 4) paraphrase datasets namely QQP and PAWS are each grouped together. with us, Tay et al. (2021) propose a multi-task learning method by training task-conditioned hyper networks; however, their method is 43x less parameter efficient compared to ours. In another line of research, Clark et al. (2019b) proposed to learn multi-task models with knowledge distillation. Houlsby et al. (2019) trained adapters for each task separately, keeping the model fixed. Stickland and Murray (2019) share the model parameters across tasks and introduce task-specific adapter parameters, which is more parameter-inefficient than our method. Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights"
2021.acl-long.47,L18-1269,0,0.0206226,"n sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the adapter and the task embedding respectively trained on the most similar GLUE task for initialization, i.e. MNLI for NLI, QNLI for QA, SST-2 for sentiment analysis, and QQP for paraphrase detection. Following prior evidence of positive transfer from NLI to other tasks (Conneau and Kiela, 2018; Yin et al., 2020; Phang et al., 2018), we initialize the out-of-domain TREC from MNLI. We show the results of full fine-tuning of all model’s parameters, Adapters†, and HYPERFORMER++4 in Table 2. Our method significantly surpasses the baselines on the majority of settings. 4 16 32 100 500 1000 2000 85 50.49±11.1 56.50±7.1 58.43±4.9 60.10±2.4 66.49±1.2 69.01±1.1 71.58±0.8 53.48±2.8 51.37±6.5 54.52±5.1 58.60±1.6 66.72±0.7 70.21±1.3 73.60±0.8 48.03±4.8 50.21±7.9 58.37±3.7 62.03±2.0 70.04±1.4 72.35±1.7 74.94±0.6 Sentiment Analysis 80 75 IMDB 70 65 T5BASE HyperFormer++BASE 60 0 1000 2000 3000 # S"
2021.acl-long.47,W19-4302,1,0.820575,"aneously on multiple tasks, obtaining state-of-the-art performance across a diverse set of tasks. We use the T5 framework as it enables training a universal model that interfaces with many language tasks. Our model has three main components: 1) task conditional adapter layers; 2) task conditional layer normalizations; and 3) hypernetworks that generate task-specific parameters. We next describe these components. 2.1 Task Conditional Adapter Layers Prior work has shown that fine-tuning all parameters of the model can result in a sub-optimal solution, particularly for resource-limited datasets (Peters et al., 2019). As an alternative to fine-tuning all the model’s parameters, prior work (Houlsby et al., 2019; Rebuffi et al., 2018; Stickland and Murray, 2019) inserted small modules called adapter layers within layers of a pretrained model, as shown in Figure 1. Adapters introduce no change to the structure or parameters of the original model. In this work, we propose conditional adapter modules, in which we generate the adapters weights based on input task embeddings using shared hypernetworks (Ha et al., 2017), which capture information across tasks that can be used to positively transfer to other relev"
2021.acl-long.47,D18-1039,0,0.0204,"572 Prior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020). Meta-learning approaches are notoriously slow to train. In addition, generating softmax parameters requires a substantially higher number of parameters, leaves the method unable to adapt the lower layers of the model, and restricts their application to classification tasks. ¨ un et al. (2020) In contemporaneous work, Ust¨ proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks (Platanios et al., 2018) where they generate adapter parameters conditioned on trained input language embeddings. Their study is limited to multilingual dependency parsing, while our work studies multi-task learning and applies to several tasks thanks to the general sequence-to-sequence nature of our model. Moreover, their number of trainable parameters is 2.88× larger than their base model since they employ a contextual parameter generator in each layer. In contrast, we use a single compact hypernetwork allowing us to efficiently condition on multiple tasks and layers of a transformer model. Marie-Catherine De Marne"
2021.acl-long.47,Q19-1040,0,0.019891,"sk projector network hI (.), which is a multi-layer perceptron consisting of two feed-forward layers and a ReLU non-linearity: Iτ =hI (zτ ), (5) t0 where zτ ∈ R can be a learnable parameter or any pretrained task features (Vu et al., 2020), and the task projector network hI (.) learns a suitable compressed task embedding from input task features. In this work, we consider a parametric zτ to allow end-to-end training which is convenient in practice.1 Removing task prefixes: The T5 model prepends task-specific prefixes to the input sequence for conditioning. For instance, when training on CoLA (Warstadt et al., 2019), cola sentence: is prepended to each sample. Instead, we remove task prefixes and use task embeddings for conditioning. Task conditioned hypernetworks: We consider simple linear layers as hypernetworks that are functions of input task embeddings Iτ . We introduce these hypernetworks in each layer of the transformer. We define hypernetwork hlA(.) that generates task conditional adapter weights (Uτl , Dτl ): Conventional layer normalization (Ba et al., 2016) is defined as: 1 We ran some pilot experiments with pretrained task embeddings (Vu et al., 2020), but did not observe extra benefits. 567"
2021.acl-long.47,2020.emnlp-main.180,0,0.128417,"Missing"
2021.emnlp-main.699,P18-1060,0,0.0391068,"Missing"
2021.emnlp-main.699,2020.lrec-1.293,0,0.0675783,"Missing"
2021.emnlp-main.699,2020.tacl-1.30,0,0.0550978,"Missing"
2021.emnlp-main.699,N19-1423,0,0.0296779,"corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBARTLARGE (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, local languages to achieve more efficient learning and faster inference for very low-resource languages like Javanese and Sundanese.1 developed for high-resource languages such as English, French, and Chinese (Devlin et al., 2019; Martin et al., 2020; Chen et al., 2020). Although the number of datasets, models, and benchmarks has been increasing for low-resource languages such as Indonesian (Wilie et al., 2020; Koto et al., 2020b), Bangla (Bhattacharjee et al., 2021), and Filipino (Cruz and Cheng, 2020), these datasets primarily focus on natural language understanding (NLU) tasks, which only cover a subset of practical NLP systems today. In contrast, much fewer natural language generation (NLG) benchmarks have been developed for low-resource languages; most multilingual NLG resources thus far have primarily focused on"
2021.emnlp-main.699,2020.lrec-1.325,0,0.0256818,"ore efficient learning of low-resource languages. 2 Related Work NLP Benchmarks. Numerous benchmarks have recently emerged, which have catalyzed advances in monolingual and cross-lingual transfer learning. These include NLU benchmarks for low-resource languages including IndoNLU (Wilie et al., 2020), IndoLEM (Koto et al., 2020b), and those focusing on Filipino (Cruz and Cheng, 2020), Bangla (Bhattacharjee et al., 2021), and Thai (Lowphansirikul et al., 2021); neural machine translation (MT) datasets for low-resource scenarios including for Indonesian (Guntara et al., 2020), African languages (Duh et al., 2020; Lakew et al., 2020), and Nepali and Sinhala (Guzmán et al., 2019); and large-scale multilingual benchmarks such as XTREME (Hu et al., 2020), MTOP (Li et al., 2020), and XGLUE (Liang et al., 2020). Winata et al. (2021); Aguilar et al. (2020); Khanuja et al. (2020) further developed multilingual benchmarks to evaluate the effectiveness of pretrained multilingual language models. More recently, GEM (Gehrmann et al., 2021) covers NLG tasks in various languages, together Our contributions are as follows: 1) we curate with automated and human evaluation metrics. Our a multilingual pretraining data"
2021.emnlp-main.699,2020.emnlp-main.393,0,0.0593647,"Missing"
2021.emnlp-main.699,D19-1632,0,0.0215437,"k NLP Benchmarks. Numerous benchmarks have recently emerged, which have catalyzed advances in monolingual and cross-lingual transfer learning. These include NLU benchmarks for low-resource languages including IndoNLU (Wilie et al., 2020), IndoLEM (Koto et al., 2020b), and those focusing on Filipino (Cruz and Cheng, 2020), Bangla (Bhattacharjee et al., 2021), and Thai (Lowphansirikul et al., 2021); neural machine translation (MT) datasets for low-resource scenarios including for Indonesian (Guntara et al., 2020), African languages (Duh et al., 2020; Lakew et al., 2020), and Nepali and Sinhala (Guzmán et al., 2019); and large-scale multilingual benchmarks such as XTREME (Hu et al., 2020), MTOP (Li et al., 2020), and XGLUE (Liang et al., 2020). Winata et al. (2021); Aguilar et al. (2020); Khanuja et al. (2020) further developed multilingual benchmarks to evaluate the effectiveness of pretrained multilingual language models. More recently, GEM (Gehrmann et al., 2021) covers NLG tasks in various languages, together Our contributions are as follows: 1) we curate with automated and human evaluation metrics. Our a multilingual pretraining dataset for Indonesian, benchmark compiles languages and tasks that are"
2021.emnlp-main.699,2020.acl-main.703,0,0.0278531,"84 10,972 3,862 2,810 855 484 Table 2: Task statistics and descriptions. † We create new splits for the train and test. English) MT tasks, Indonesian summarization, and Indonesian chit-chat dialogue. Pretrained NLG Models. Recently, the paradigm of pretraining-then-fine-tuning has achieved remarkable success in NLG, as evidenced by the success of monolingual pretrained NLG models. GPT-2 (Radford et al., 2019), and later GPT-3 (Brown et al., 2020), demonstrated that language models can perform zero-shot transfer to downstream tasks via generation. Other recent state-of-the-art models are BART (Lewis et al., 2020), which maps corrupted documents to their original, and the encoder-decoder T5 (Raffel et al., 2020), which resulted from a thorough investigation of architectures, objectives, datasets, and pretraining strategies. These monolingual models have been generalised to the multilingual case by pretraining the architectures on multiple languages; examples include mBART (Liu et al., 2020) and mT5 (Xue et al., 2020). In this paper, we focus on local, near-monolingual models for the languages of Indonesia, and systematically compare them on our benchmark with such larger multilingual models. 3 3.1 Indo"
2021.emnlp-main.699,W04-1013,0,0.102429,"-5, 1e-5, 5e-6] and report the best results. We report the best hyperparameter settings for each model in Appendix C. 5 Evaluation Procedure For evaluation, we use beam search with a beam width of 5, a length penalty α of 1.0, and limit the maximum sequence length to 512 for all models and all tasks. We conduct both automatic and human evaluations to assess the models. We use a different evaluation metric for each task following the standard evaluation metric on the corresponding task. For machine translation, we report the SacreBLEU (Post, 2018) score. For summarization, we report the ROUGE (Lin, 2004) score. For IndoGPT. We pretrain our IndoGPT model us- QA, the F1 and exact match scores are reported foling an autoregressive language modeling objec- lowing the original SQUAD V2 (Rajpurkar et al., 2018) evaluation metrics. For chit-chat, we report tive (Radford et al., 2019) for 640k iterations on 8 NVIDIA V100 GPUs, with a batch size of 512, both the BLEU and SacreBLEU scores (Papineni et al., 2002). an initial learning rate of 5e-5, and a maximum sequence length of 1024. We apply distributed data We further conduct human evaluation on eight parallelism (DDP) with ZeRO-DP (Rajbhandari task"
2021.emnlp-main.699,2020.tacl-1.47,0,0.228056,"languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBARTLARGE (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, local languages to achieve more efficient learning and faster inference for very low-resource languages like Javanese and Sundanese.1 developed for high-resource languages such as English, French, and Chinese (Devlin et al., 2019; Martin et al., 2020; Chen et al., 2020). Although the number of datasets, models, and benchmarks has been increasing for low-resource languages such as Indonesian (Wilie et al., 2020; Koto et al., 2020b), Bangla (Bhattacharjee et al., 2021), and Filipino (Cruz and Cheng, 2020), these datasets"
2021.emnlp-main.699,D17-1238,0,0.0332864,"Missing"
2021.emnlp-main.699,2020.sltu-1.18,0,0.0370521,"tive since there are only few unlabelled data available for pretraining. In this paper, we explore two approaches. The first approach is to leverage existing pretrained multilingual models, such as mBART (Liu et al., 2020). While this approach is quite effective, we explore a second approach that leverages positive transfer from related languages (Hu et al., 2020; Khanuja et al., 2021), such as pretraining with a corpus of mostly Indonesian text. We justify this approach through the fact that Sundanese, Javanese, and Indonesian all belong to the same Austronesian language family (Blust, 2013; Novitasari et al., 2020), and share various morphological and semantic features as well as common lexical items through the presence of Sundanese and Javanese loanwords in the Indonesian language (Devianty, 2016). We show that pretraining on mostly Indonesian text achieves competitive performance to the larger multilingual models—despite using 5× fewer parameters and smaller pretraining data—and achieves particularly strong performance on tasks involving the very lowresource Javanese and Sundanese languages. languages in Indonesia, IndoBART and IndoGPT; 3) to the best of our knowledge, we develop the first diverse be"
2021.emnlp-main.699,P02-1040,0,0.113803,"luation metric for each task following the standard evaluation metric on the corresponding task. For machine translation, we report the SacreBLEU (Post, 2018) score. For summarization, we report the ROUGE (Lin, 2004) score. For IndoGPT. We pretrain our IndoGPT model us- QA, the F1 and exact match scores are reported foling an autoregressive language modeling objec- lowing the original SQUAD V2 (Rajpurkar et al., 2018) evaluation metrics. For chit-chat, we report tive (Radford et al., 2019) for 640k iterations on 8 NVIDIA V100 GPUs, with a batch size of 512, both the BLEU and SacreBLEU scores (Papineni et al., 2002). an initial learning rate of 5e-5, and a maximum sequence length of 1024. We apply distributed data We further conduct human evaluation on eight parallelism (DDP) with ZeRO-DP (Rajbhandari tasks, i.e., En ↔ Id (News), Su ↔ Id (Bible), Jv ↔ 8880 Model ID→EN (News) ID→SU (Bible) ID→JV (Bible) EN→ID (News) SU→ID (Bible) JV→ID (Bible) Fluency Adequacy Fluency Adequacy Fluency Adequacy Fluency Adequacy Fluency Adequacy Fluency Adequacy Baseline Ground-truth Scratch 4.4±0.8 3.8±0.9 4.2±0.9 2.8±1.0 4.2±0.8 3.1±0.9 3.7±1.2 2.1±1.1 4.5±0.7 3.3±1.0 4.0±0.9 2.2±1.0 4.7±0.5 3.9±0.9 4.4±0.6 2.7±0.9 4.4±0."
2021.emnlp-main.699,W18-6319,0,0.0137132,"search for the learning rate over the range [1e-3, 1e-4, 5e-5, 1e-5, 5e-6] and report the best results. We report the best hyperparameter settings for each model in Appendix C. 5 Evaluation Procedure For evaluation, we use beam search with a beam width of 5, a length penalty α of 1.0, and limit the maximum sequence length to 512 for all models and all tasks. We conduct both automatic and human evaluations to assess the models. We use a different evaluation metric for each task following the standard evaluation metric on the corresponding task. For machine translation, we report the SacreBLEU (Post, 2018) score. For summarization, we report the ROUGE (Lin, 2004) score. For IndoGPT. We pretrain our IndoGPT model us- QA, the F1 and exact match scores are reported foling an autoregressive language modeling objec- lowing the original SQUAD V2 (Rajpurkar et al., 2018) evaluation metrics. For chit-chat, we report tive (Radford et al., 2019) for 640k iterations on 8 NVIDIA V100 GPUs, with a batch size of 512, both the BLEU and SacreBLEU scores (Papineni et al., 2002). an initial learning rate of 5e-5, and a maximum sequence length of 1024. We apply distributed data We further conduct human evaluation"
2021.emnlp-main.699,P17-1099,0,0.106428,"Missing"
2021.emnlp-main.699,2020.acl-main.704,0,0.0348461,"Missing"
2021.emnlp-main.699,P17-1061,0,0.04508,"Missing"
2021.emnlp-main.800,2021.findings-emnlp.410,1,0.87244,"Missing"
2021.emnlp-main.800,2020.acl-main.421,1,0.820989,"The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a new embedding matrix for the target language (Artetxe et al., 2020) or adds new tokens to the pretrained vocabulary. While the former has only been applied to high-resource languages, the latter approaches have been limited to languages with seen scripts (Chau et al., 2020; Müller et al., 2021) and large pretraining corpora (Wang et al., 2020). Another line of work adapts the embedding layer as well as other layers of the model via adapters (Pfeiffer et al., 2020b; Üstün et al., 2020). Such methods, however, cannot be directly applied to languages with unseen scripts. In this work, we first empirically verify that the original tokenizer and the original embed"
2021.emnlp-main.800,D19-1165,0,0.0261306,"ained multilingual model such as mBERT or XLM-R is 1) fine-tuning it on labelled data of a downstream task in a source language and then 2) applying it directly to perform inference in a target language (Hu et al., 2020). However, as the model must balance between many languages in its representation space, it is not suited to excel at a specific language at inference time without further adaptation (Pfeiffer et al., 2020b). Adapters for Cross-lingual Transfer. Adapterbased approaches have been proposed as a remedy (Rebuffi et al., 2017, 2018; Houlsby et al., 2019; Stickland and Murray, 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a, 2021). In the crosslingual setups, the idea is to increase the multilingual model capacity by storing language-specific knowledge of each language in dedicated parameters (Pfeiffer et al., 2020b; Vidoni et al., 2020). We start from MAD-X (Pfeiffer et al., 2020b), a state-of-the-art adapter-based framework for crosslingual transfer. For completeness, we provide a brief overview of the framework in what follows. MAD-X comprises three adapter types: language, task, and invertible adapters; this enables learning language and task-specific transformations in a modular and"
2021.emnlp-main.800,2020.findings-emnlp.118,0,0.510962,"Figure 2. Pearson’s ρ correlation scores between the lexical overlap and 2 An alternative approach based on transliteration (Müller proportion of UNKs (see Table 1) and NER perforet al., 2021) side-steps script adaptation but relies on languagespecific heuristics, which are not available for most languages. mance are 0.443 and −0.798, respectively. 10188 Method EL- RAND EL- LEX MFC ∗ - RAND MFC ∗ - LEX Special tokens X X X X Lexical overlap Latent semantic concepts Language clusters New params # of new params Reference X X X0 X0 F0 , I0 F0 , I0 7.68M 7.68M 1M + C·10k 1M + C·10k Artetxe et al. (2020) Chau et al. (2020); Wang et al. (2020) Ours Ours X X X X Table 2: Overview of our methods and related approaches together with the pretrained knowledge they utilize. We calculate the number of new parameters per language with V 0 = 10k, D = 768, and D0 = 100. We do not include up-projection matrices G as these are learned only once and make up a comparatively small number of parameters. Recent approaches such as invertible adapters (Pfeiffer et al., 2020b) that adapt embeddings in the pretrained multilingual vocabulary may be able to deal with lesser degrees of lexical overlap. Still, they ca"
2021.emnlp-main.800,2020.emnlp-main.367,0,0.255907,"embedding matrix X whereas F stores token-specific information. G only needs to be pretrained once and can be used and fine-tuned for every new language. To this end, we simply learn new low-dimensional embeddings 0 0 F0 ∈ R|V |×D with the pretraining task, which are up-projected with G and fed to the model. MFC KM EANS -∗. When C > 1, each token is associated with one of C up-projection matrices. Grouping tokens and using a separate up-projection matrix per group may help balance sharing information across typologically similar languages with learning a robust representation for each token (Chung et al., 2020). We propose two approaches to automatically learn such a clustering. In our first, pipeline-based approach, we first cluster X into C clusters using KMeans. For each cluster, we then factorize the subset of embeddings Xc associated with the c-th cluster separately using Semi-NMF equivalently as for MF1 -∗. For a new language, we learn new low-dim em0 0 beddings F0 ∈ R|V |×D and a randomly initialized 0 matrix Z ∈ R|V |×C , which allows us to compute 0 the cluster assignment matrix I0 ∈ R|V |×C . Specifically, for token v, we obtain its cluster assignment as arg max of z0v,· . As arg max is no"
2021.emnlp-main.800,2020.emnlp-main.358,0,0.0272702,"ddings X0 ∈ R|V |×D for all V 0 vocabulary items where D is the dimensionality of the existing embeddings X ∈ R|V |×D , and only initialize special tokens (e.g. [CLS], [SEP]) with their pretrained representations. We train the new embeddings of the X0 with the pretraining task. This approach, termed EL- RAND, was proposed by Artetxe et al. (2020): they show that it allows learning aligned representations for a new language but only evaluate on high-resource languages. The shared special tokens allow the model to access a minimum amount of lexical information, which can be useful for transfer (Dufter and Schütze, 2020). Beyond this, this approach leverages knowledge from the existing embedding matrix only implicitly to the extent that the higher-level hidden representations are aligned to the lexical representations. 3.2 Initialization with Lexical Overlap 0 , and us denote this vocabulary subset with Vlex 0 0 0 Vrand = V  Vlex . In particular, we initialize the embeddings of all lexically overlapping tokens 0 with their pretrained representaX0lex from Vlex tions from the original matrix X, while the tokens 0 from Vrand receive randomly initialized embeddings X0rand . We then fine-tune all target language"
2021.emnlp-main.800,P19-1070,1,0.848463,"able parameters and yield more efficient model adaptation. Our approach, based on matrix factorization and language clusters, extracts relevant information from the pretrained embedding matrix. 4) We show that our methods outperform previous approaches with both resourcerich and resource-poor languages. They substantially reduce the gap between random and lexicallyoverlapping initialization, enabling better model adaption to unseen scripts. The code for this work is released at github.com/ Adapter-Hub/UNKs_everywhere. surpassed (static) cross-lingual word embedding spaces (Ruder et al., 2019; Glavas et al., 2019) as the state-of-the-art paradigm for cross-lingual transfer in NLP (Pires et al., 2019; Wu and Dredze, 2019; Wu et al., 2020; Hu et al., 2020; K et al., 2020). However, recent studies have also indicated that even current state-of-the-art models such as XLM-R (Large) still do not yield reasonable transfer performance across a large number of target languages (Hu et al., 2020). The largest drops are reported for resource-poor target languages (Lauscher et al., 2020), and (even more dramatically) for languages not covered at all during pretraining (Pfeiffer et al., 2020b). Standard Cross-Lingua"
2021.emnlp-main.800,2021.eacl-main.270,1,0.788219,"default implementation provided by Bauckhage et al. (2011).9 We train for 3,000 update steps and leverage the corresponding matrices F and G as initialization for the new vocabulary. We choose the reduced embedding dimensionality D0 = 100. F is only used when initializing the (lower-dimensional) embedding matrix with lexically overlapping representations. transfer. We train all the models with a batch size of 16 on high resource languages. For NER we use learning rates 2e − 5 and 1e − 4 for full fine-tuning and adapter-based training, respectively. For DP, we use a transformer-based variant (Glavas and Vulic, 2021) of the standard deep biaffine attention dependency parser (Dozat and Manning, 2017) and train with learning rates 2e − 5 and 5e − 4 for full fine-tuning and adapter-based training respectively. Masked Language Modeling. For MLM pretraining we leverage the entire Wikipedia corpus of the respective language. We train for 200 epochs or ∼100k update steps, depending on the corpus size. The batch size is 64; the learning rate is 1e − 4. 5 Results and Discussion The main results are summarised in Table 3a for NER, and in Table 3b for DP. First, our novel MAD-X 2.0 considerably outperforms the MADX"
2021.emnlp-main.800,2020.emnlp-main.363,1,0.903259,"Missing"
2021.emnlp-main.800,2021.naacl-main.38,0,0.0340559,"nguage vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model. Amharic የእግዚአብሔርን የሚሰጡዋቸውን የእግዚአብሔር ቤተክርስቲያን ኢትዮጵያውያን Tibetan Divehi ཉྩཡོནིརོདྷཨེཝ ެގާޔްއިރޫހްމުޖްލުސީއަރ བཱདཱིམཧཱཤྲམཎ ެވެއަވަންނަގިއަޑަވެނެދ བཱདཱིམཧཱཤྲམཎ އޓްސޮއ ި ޯރތާއ ު ަްސިޓިއ ཧཱུཕཊསྭཱཧཱ ލސީއަރ ު ްާޔްއިރޫހްމުޖ ཧཱུཧཱུཕཊ ާވިއަފްނެގިއަ ަޑވި ބިލ Figure 1: Example tokens of unseen scripts. training corpora (Pfeiffer et al., 2020b; Müller et al., 2021; Ansell et al., 2021). The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a new embedding matrix fo"
2021.emnlp-main.800,L16-1262,0,0.0926263,"Missing"
2021.emnlp-main.800,2020.lrec-1.497,0,0.0513677,"Missing"
2021.emnlp-main.800,P17-1178,0,0.0893718,"Missing"
2021.emnlp-main.800,2020.emnlp-demos.7,1,0.921743,"Figure 2. Pearson’s ρ correlation scores between the lexical overlap and 2 An alternative approach based on transliteration (Müller proportion of UNKs (see Table 1) and NER perforet al., 2021) side-steps script adaptation but relies on languagespecific heuristics, which are not available for most languages. mance are 0.443 and −0.798, respectively. 10188 Method EL- RAND EL- LEX MFC ∗ - RAND MFC ∗ - LEX Special tokens X X X X Lexical overlap Latent semantic concepts Language clusters New params # of new params Reference X X X0 X0 F0 , I0 F0 , I0 7.68M 7.68M 1M + C·10k 1M + C·10k Artetxe et al. (2020) Chau et al. (2020); Wang et al. (2020) Ours Ours X X X X Table 2: Overview of our methods and related approaches together with the pretrained knowledge they utilize. We calculate the number of new parameters per language with V 0 = 10k, D = 768, and D0 = 100. We do not include up-projection matrices G as these are learned only once and make up a comparatively small number of parameters. Recent approaches such as invertible adapters (Pfeiffer et al., 2020b) that adapt embeddings in the pretrained multilingual vocabulary may be able to deal with lesser degrees of lexical overlap. Still, they ca"
2021.emnlp-main.800,2020.emnlp-main.617,1,0.343661,"en mBERT’s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model. Amharic የእግዚአብሔርን የሚሰጡዋቸውን የእግዚአብሔር ቤተክርስቲያን ኢትዮጵያውያን Tibetan Divehi ཉྩཡོནིརོདྷཨེཝ ެގާޔްއިރޫހްމުޖްލުސީއަރ བཱདཱིམཧཱཤྲམཎ ެވެއަވަންނަގިއަޑަވެނެދ བཱདཱིམཧཱཤྲམཎ އޓްސޮއ ި ޯރތާއ ު ަްސިޓިއ ཧཱུཕཊསྭཱཧཱ ލސީއަރ ު ްާޔްއިރޫހްމުޖ ཧཱུཧཱུཕཊ ާވިއަފްނެގިއަ ަޑވި ބިލ Figure 1: Example tokens of unseen scripts. training corpora (Pfeiffer et al., 2020b; Müller et al., 2021; Ansell et al., 2021). The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a n"
2021.emnlp-main.800,P19-1493,0,0.0619527,"Missing"
2021.emnlp-main.800,P19-1015,0,0.109665,"Missing"
2021.emnlp-main.800,2021.acl-long.243,1,0.644693,"Massachusetts Encyclopedia Pennsylvania Jacksonville Turkmenistan Melastomataceae munisipalidad Internasional internasional International establecimiento vicepresidente Internacional internacional Independencia University Therefore suffering existence practice languages language formula disease control government Chinese govern system ation International Bangladesh wikipedia Australia Zimbabwe Table 5: Longest lexically overlapping (sub)words. model (e.g. Georgian (ka), Urdu (ur), and Hindi (hi)), the proposed methods outperform MAD-X 2.0 for all tasks. This is in line with contemporary work (Rust et al., 2021), which emphasizes the importance of tokenizer quality for the downstream task. Consequently, for unseen languages with under-represented scripts, the performance gains are even larger, e.g., we see large improvements for Min Dong (cdo), Mingrelian (xmf), and Sindhi (sd). For unseen languages with the Latin script, our methods perform competitively (e.g. Maori (mi), Ilokano (ilo), Guarani (gn), and Wolof (wo)): this empirically confirms that the Latin script is adequately represented in the original vocabulary. The largest gains are achieved for languages with unseen scripts (e.g. Amharic (am)"
2021.emnlp-main.800,P18-1072,1,0.88992,"Missing"
2021.emnlp-main.800,2020.emnlp-main.180,0,0.115408,"Missing"
2021.emnlp-main.802,2020.findings-emnlp.195,0,0.0856569,"Missing"
2021.emnlp-main.802,2020.emnlp-main.630,1,0.933863,"l., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks mance as mean average precision at 20 (mAP@20). 3.2.1 Multilingual Causal Reasoning LAReQA Language Agnostic Retrieval Question XCOPA The Cross-lingual Choice of Plausible Answering (Roy et al., 2020) is a sentence retrieval Alternatives (Ponti et al., 2020) dataset asks models task. Each query has target answers in multiple lanto decide which of two sentences causally follows guages, and models are expected to rank all correct 10218 Table 3: C HECK L IST templates and generated tests for different capabilities in English, Hebrew, Arabic, and Bengali."
2021.emnlp-main.802,2021.naacl-main.280,0,0.0801691,"Missing"
2021.emnlp-main.802,2020.tacl-1.30,1,0.79452,"i.e. RemBERT for retrieval and mT5 for retrieval and tagging. but remain well below performance on English. On POS tagging (Figure 1d), scores remain largely the same; performance is lower for some languages with non-Latin scripts and low-resource languages. We show the scores for the remaining tasks in Appendix B. The remaining gap to English performance on these tasks is partially an artefact of the evaluation setup: zero-shot cross-lingual transfer from English favors English representations whereas models fine-tuned on in-language monolingual data perform more similarly across languages (Clark et al., 2020; Hu et al., 2020). ilar to the downstream setting but does not significantly improve performance on other tasks. Finetuning on automatically translated task-specific data yields strong gains and is used by most recent models to achieve the best performance (Hu et al., 2020; Ouyang et al., 2020; Luo et al., 2020). Nevertheless, key challenges such as how to learn robust cross-lingual syntactic and semantic processing capabilities during pre-training remain. 3 XTREME-R In order to encourage the NLP community to tackle challenging research directions in pursuit of betOverall, representations fro"
2021.emnlp-main.802,2020.acl-main.747,0,0.251756,"aset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models. XTREME XTREME - R Analysis tools 40 9 Classification, structured prediction, QA, retrieval — Leaderboard Static 50 − 2 + 3 = 10 +language-agnostic retrieval M ULTI C HECK L IST, Explainaboard Interactive, +metadata # of languages # of tasks Task categories Table 1: Overview of XTREME and XTREME - R. have been introduced, consolidating existing multilingual tasks and covering tens of languages. When XTREME was released, the gap between the best-performing baseline, XLM-R Large (Conneau et al., 2020), and human-level performance was roughly 25. This has since shrunk to less than 12 points, a much smaller but still substantial gap compared to the difference from human-level performance observed in English transfer learning (Wang et al., 2019a), which has recently been closed entirely on some evaluation suites (He et al., 2021). In order to examine the nature of this progress, we first perform an analysis of state-of-the-art mul1 Introduction tilingual models on XTREME. We observe that progress has not been uniform, but concentrated on Most research in natural language processing cross-ling"
2021.emnlp-main.802,D18-1269,0,0.022892,"ll be used to search over only English candidates. However, practical settings often violate this assumption, e.g. the answer to a question may be available in any number of languages, possibly different from the query language. Models that cannot compare the appropriateness of retrieval results across languages are thus ineffective in such real-world scenarios. XTREME - R includes two new related crosslingual retrieval tasks. The first seeks to measure the extent to which cross-lingual representations 3.1 Retained Tasks are “strongly aligned” (Roy et al., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks m"
2021.emnlp-main.802,N19-1423,0,0.0292115,"rk (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 languages using MLM. XLM-R XLM-R Large (Conneau et al., 2020) uses the same MLM objective with a larger model, and was trained on a magnitude more web data from 100 languages. mT5 Multilingual T5 (Xue et al., 2021) is an encoder-decoder transformer that frames NLP tasks in a “text-to-text” format. It was pre-trained with MLM on a large multilingual web corpus covering 101 languages. We employ the largest mT5-XXL variant with 13B parameters. Translate-train To evaluate the impact of MT, we fine-tune mBERT on translations of English training data fro"
2021.emnlp-main.802,2020.emnlp-main.489,1,0.872791,"zing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leaderboard. To make it easier to choose the best model for a use case, each submission is required to provide metadata such as the number of parameters and amount of pre-training data, which we make available via an interactive leaderboard. We also introduce task and language-specific s"
2021.emnlp-main.802,N18-1108,0,0.0260624,"tr, uk, ur, vi, wo, yo, zh.2 XTREME - R is similarly typologically and genealogically diverse as XTREME while covering a larger number of languages (see Appendix D). 3.4 Diagnostic and evaluation suite To increase the language coverage of low-resource languages in XTREME - R and to enable us to systematically evaluate a model’s cross-lingual generalization ability, we augment XTREME - R with a massively multilingual diagnostic and evaluation suite. Challenge sets and diagnostic suites in NLP (Wang et al., 2019a,b; Belinkov and Glass, 2019) are mostly limited to English, with a few exceptions (Gulordava et al., 2018). As challenge sets are generally created with a human in the loop, the main challenge for creating a large multilingual diagnostic suite is to scale the annotation or translation effort to many languages and to deal with each language’s idiosyncrasies. M ULTI C HECK L IST To address this, we build on the C HECK L IST (Ribeiro et al., 2020) framework, which facilitates creating parameterized tests for models. C HECK L IST enables the creation of test cases using templates, which test for specific behavioral capabilities of a model with regard to a downstream task. Importantly, by relying on te"
2021.emnlp-main.802,2020.emnlp-main.204,0,0.0263038,"erent attribute values. We define new taskspecific attributes for the four task types as well as task-independent attributes (see Appendix K). Metadata We additionally would like to enable practitioners to rank submissions based on other information. To this end, we ask each submission to XTREME - R for relevant metadata such as the number of parameters, the amount of pre-training data, etc. We will show this information in an interactive leaderboard (see Appendix H for the metadata of current XTREME submissions). 4 Experiments glish. While recent work (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 l"
2021.emnlp-main.802,2020.emnlp-main.479,1,0.848683,"Missing"
2021.emnlp-main.802,2020.acl-main.560,0,0.0412131,"been closed entirely on some evaluation suites (He et al., 2021). In order to examine the nature of this progress, we first perform an analysis of state-of-the-art mul1 Introduction tilingual models on XTREME. We observe that progress has not been uniform, but concentrated on Most research in natural language processing cross-lingual retrieval tasks where fine-tuning on (NLP) to date has focused on developing methods other tasks and pre-training with parallel data lead that work well for English and a small set of other to large gains. On other task categories improvehigh-resource languages (Joshi et al., 2020). In ments are more modest. Models still generally contrast, methods for other languages can be vastly perform poorly on languages with limited data and more beneficial as they enable access to language non-Latin scripts. Fine-tuning on additional transtechnology for more than three billion speakers of low-resource languages and prevent the NLP com- lated data generally leads to the best performance. munity from overfitting to English. Motivated by Based on this analysis, we propose XTREME - R these benefits, the area of multilingual NLP has (XTREME Revisited), a new benchmark with the attract"
2021.emnlp-main.802,2020.findings-emnlp.445,0,0.0706199,"Missing"
2021.emnlp-main.802,2020.emnlp-main.40,0,0.031752,"n to XTREME - R for relevant metadata such as the number of parameters, the amount of pre-training data, etc. We will show this information in an interactive leaderboard (see Appendix H for the metadata of current XTREME submissions). 4 Experiments glish. While recent work (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 languages using MLM. XLM-R XLM-R Large (Conneau et al., 2020) uses the same MLM objective with a larger model, and was trained on a magnitude more web data from 100 languages. mT5 Multilingual T5 (Xue et al., 2021) is an encoder-decoder transformer that frames NLP tasks in a “text-"
2021.emnlp-main.802,2020.acl-main.329,0,0.0428374,"Missing"
2021.emnlp-main.802,2020.emnlp-main.363,0,0.0253632,"Missing"
2021.emnlp-main.802,2020.acl-main.653,0,0.0342518,"of retrieval results across languages are thus ineffective in such real-world scenarios. XTREME - R includes two new related crosslingual retrieval tasks. The first seeks to measure the extent to which cross-lingual representations 3.1 Retained Tasks are “strongly aligned” (Roy et al., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks mance as mean average precision at 20 (mAP@20). 3.2.1 Multilingual Causal Reasoning LAReQA Language Agnostic Retrieval Question XCOPA The Cross-lingual Choice of Plausible Answering (Roy et al., 2020) is a sentence retrieval Alternatives (Ponti et al., 2020) dataset asks mo"
2021.emnlp-main.802,2020.acl-main.465,0,0.0160657,"s by covering 50 typologically diverse languages and 10 challenging, diverse tasks. To make retrieval more difficult, we introduce two new tasks that focus on “language-agnostic” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on lan"
2021.emnlp-main.802,E17-2002,0,0.063781,"Missing"
2021.emnlp-main.802,2021.acl-demo.34,1,0.889858,"rformance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leaderboard. To make it easier to choose the best model for a use case, each submission is required to provide metadata such as the number of parameters and amount of pre-training data, which we make available via an interactive leaderboard. We also introduce task and language-specific sub-leaderboards to"
2021.emnlp-main.802,2021.ccl-1.108,0,0.0472179,"Missing"
2021.emnlp-main.802,2021.emnlp-main.3,0,0.0855395,"Missing"
2021.emnlp-main.802,P17-1178,0,0.0484182,"Missing"
2021.emnlp-main.802,P02-1040,0,0.111792,"Missing"
2021.emnlp-main.802,2020.emnlp-main.617,1,0.881246,"Missing"
2021.emnlp-main.802,2020.aacl-main.56,0,0.0322166,"tic processing capabilities during pre-training remain. 3 XTREME-R In order to encourage the NLP community to tackle challenging research directions in pursuit of betOverall, representations from token-level MLM pre-training are of limited use for cross-lingual sen- ter cross-lingual model generalization, we propose XTREME - R (XTREME Revisited). XTREME - R tence retrieval, as evidenced by the comparatively poor performance of the mBERT and XLM-R mod- shares its predecessor’s core design principles for creating an accessible benchmark to evaluate crossels. Fine-tuning on sentence-level tasks (Phang et al., 2020; Fang et al., 2021) can mitigate this. lingual transfer but makes some key changes. The strong performance of recent models such as First, XTREME - R focuses on the tasks that have VECO and ERNIE-M on the retrieval tasks can proven to be hardest for current multilingual modbe attributed to a combination of parallel data and els. To this end, it drops XTREME’s PAWS-X and new pre-training objectives that make use of it. Pre- BUCC tasks since recent advances have left less training on parallel data improves performance on room for further improvement, and they cover only retrieval by making the"
2021.emnlp-main.802,2020.emnlp-main.185,0,0.0332192,"Missing"
2021.emnlp-main.802,P19-1015,0,0.0348481,"Missing"
2021.emnlp-main.802,D16-1264,0,0.111238,"Missing"
2021.emnlp-main.802,2020.emnlp-main.477,1,0.864987,"Missing"
2021.emnlp-main.802,D19-1454,0,0.0537933,"Missing"
2021.emnlp-main.802,P18-1072,1,0.884249,"Missing"
2021.emnlp-main.802,P19-1355,0,0.0118816,"forts to include training data in multiple languages. Biases in multilingual models 7.4 Environmental concerns XTREME - R aims to enable efficient evaluation of multilingual models. To this end, we created a new dataset, Mewsli-X, that captures the essence of multilingual entity linking against a diverse knowledge base but is computationally cheaper to evaluate than the large-scale Mewsli-9 (Botha et al., 2020). Nevertheless, the models that perform best on benchmarks like XTREME - R are generally large-scale Transformer models pre-trained on large amounts of data, which comes at a high cost (Strubell et al., 2019). We thus particularly encourage the development of efficient methods to adapt existing models to new languages (Pfeiffer et al., 2020) rather than training multilingual models entirely from scratch. Acknowledgements We thank Marco Tulio Ribeiro for advice on C HECK L IST. We are grateful to Laura Rimell and Jon Clark for valuable feedback on drafts of this paper, and to Dan Gillick for feedback on the Mewsli-X dataset design. We thank Hila Gonen, Bidisha Samantha, and Partha Talukdar for advice on Arabic, Bengali, and Hebrew C HECK L IST examples. References Antonios Anastasopoulos and Graham"
2021.emnlp-main.802,N18-1101,0,0.0932773,"Missing"
2021.emnlp-main.802,2020.acl-main.442,0,0.188983,"c” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leade"
2021.emnlp-main.802,2021.naacl-main.41,1,0.926532,"and XGLUE (Liang et al., 2020) multilingual, diverse, and accessible. It expands on 10215 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215–10245 c November 7–11, 2021. 2021 Association for Computational Linguistics by covering 50 typologically diverse languages and 10 challenging, diverse tasks. To make retrieval more difficult, we introduce two new tasks that focus on “language-agnostic” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabil"
2021.emnlp-tutorials.4,N19-1423,0,0.447678,"ks viable for the first time. We generally aim to highlight methods and techniques that can be applied to adapt to many domains and languages in order to be helpful to the majority of the audience. While multi-domain and multilingual data differ in many ways both can be formulated as transfer learning problems and approached using a similar set of fundamental tools and principles, which we aim to convey to our audience. As one example of such a tool, we will cover training procedures for large pre-trained language models (LMs). For multi-domain QA, we will discuss adaptation of LMs e.g. BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019). For multilingual QA, we will teach the methods for training LMs from large multilingual supervised and unsupervised data e.g. XLM-RoBERTa (Conneau et al., 2019) and M4 (Arivazhagan et al., 2019). Notably, our tutorial will highlight the challenges of applying such methods to specific domains and languages. Overall, we will aim to provide a set of best practices that will enable researchers and practitioners to train methods for their domain and Question answering (QA) is one of the most challenging and impactful tasks in natural language processing. Most researc"
2021.emnlp-tutorials.4,2020.acl-main.421,1,0.861317,"of several 18 3.2 non-English multilingual question answering datasets and systems such as DuReader (He et al., 2018) and DRCD (Shao et al., 2018) in Chinese, ARCD (Mozannar et al., 2019) in Arabic, multi-domain QA (Gupta et al., 2018) in Hindi-English, and visual QA (Gao et al., 2016) in Chinese-English. We distinguish between datasets that have been created by obtaining naturally occurring data in a language or via translations from SQuAD into Korean (Lee et al., 2018; Li et al., 2018), French and Japanese (Asai et al., 2018) and Italian (Croce et al., 2019). Recent datasets such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) cover more languages while the recently introduced TyDiQA (Clark et al., 2020) and MKQA (Longpre et al., 2020) can be seen as multilingual counterparts to Natural Questions. Three of these datasets are part of XTREME (Hu et al., 2020), a massively multilingual benchmark for testing the cross-lingual generalization ability of state-ofthe-art methods. While state-of-the-art models have matched or surpassed human performance in general-purpose monolingual benchmarks such as GLUE (Wang et al., 2019), current methods still fall short of human performance on multilingu"
2021.emnlp-tutorials.4,L18-1440,0,0.0202548,"ill first survey some of the large multilingual language models e.g. mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2019), M4 (Arivazhagan et al., 2019). We will show how they have helped close the gap on cross-lingual tasks by introducing zero-shot cross-lingual learning. 2. Multilingual QA [40 mins]: Then we will give a comprehensive overview of several 18 3.2 non-English multilingual question answering datasets and systems such as DuReader (He et al., 2018) and DRCD (Shao et al., 2018) in Chinese, ARCD (Mozannar et al., 2019) in Arabic, multi-domain QA (Gupta et al., 2018) in Hindi-English, and visual QA (Gao et al., 2016) in Chinese-English. We distinguish between datasets that have been created by obtaining naturally occurring data in a language or via translations from SQuAD into Korean (Lee et al., 2018; Li et al., 2018), French and Japanese (Asai et al., 2018) and Italian (Croce et al., 2019). Recent datasets such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) cover more languages while the recently introduced TyDiQA (Clark et al., 2020) and MKQA (Longpre et al., 2020) can be seen as multilingual counterparts to Natural Questions. Three of t"
2021.emnlp-tutorials.4,2020.acl-main.740,0,0.053328,"Missing"
2021.emnlp-tutorials.4,W18-2605,0,0.0204768,"ngual QA and open research problems 1. From Mono to large Multilingual Language Models [15 mins]: In this half we will first survey some of the large multilingual language models e.g. mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2019), M4 (Arivazhagan et al., 2019). We will show how they have helped close the gap on cross-lingual tasks by introducing zero-shot cross-lingual learning. 2. Multilingual QA [40 mins]: Then we will give a comprehensive overview of several 18 3.2 non-English multilingual question answering datasets and systems such as DuReader (He et al., 2018) and DRCD (Shao et al., 2018) in Chinese, ARCD (Mozannar et al., 2019) in Arabic, multi-domain QA (Gupta et al., 2018) in Hindi-English, and visual QA (Gao et al., 2016) in Chinese-English. We distinguish between datasets that have been created by obtaining naturally occurring data in a language or via translations from SQuAD into Korean (Lee et al., 2018; Li et al., 2018), French and Japanese (Asai et al., 2018) and Italian (Croce et al., 2019). Recent datasets such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) cover more languages while the recently introduced TyDiQA (Clark e"
2021.emnlp-tutorials.4,D16-1264,0,0.342045,"we discuss stateof-the-art approaches that achieve impressive performance, ranging from zero-shot transfer learning to out-of-the-box training with open-domain QA systems. Finally, we will present open research problems that this new research agenda poses such as multi-task learning, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models.1 1 Overall Question answering (QA) has emerged as one of the most popular areas in natural language processing (NLP). Established benchmarks such as the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016) are used as a standard testing ground for new models while opendomain QA benchmarks such as Natural Questions (Kwiatkowski et al., 2019) represent the frontier of what is possible with current NLP technology (Zaheer et al., 2020). In this tutorial, we will review recent advances in open-domain QA but focus on an area that has received less attention both in research and in past tutorials—multi-domain and multilingual QA. Open-domain QA is of interest for building general-purpose assistants that can answer questions about any topic (Adiwardana et al., 2019). 1 The tutorial materials are availa"
2021.emnlp-tutorials.4,L18-1431,0,0.026959,"problems 1. From Mono to large Multilingual Language Models [15 mins]: In this half we will first survey some of the large multilingual language models e.g. mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2019), M4 (Arivazhagan et al., 2019). We will show how they have helped close the gap on cross-lingual tasks by introducing zero-shot cross-lingual learning. 2. Multilingual QA [40 mins]: Then we will give a comprehensive overview of several 18 3.2 non-English multilingual question answering datasets and systems such as DuReader (He et al., 2018) and DRCD (Shao et al., 2018) in Chinese, ARCD (Mozannar et al., 2019) in Arabic, multi-domain QA (Gupta et al., 2018) in Hindi-English, and visual QA (Gao et al., 2016) in Chinese-English. We distinguish between datasets that have been created by obtaining naturally occurring data in a language or via translations from SQuAD into Korean (Lee et al., 2018; Li et al., 2018), French and Japanese (Asai et al., 2018) and Italian (Croce et al., 2019). Recent datasets such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) cover more languages while the recently introduced TyDiQA (Clark et al., 2020) and MKQA (Longpr"
2021.emnlp-tutorials.4,Q19-1026,0,0.293149,"raining with open-domain QA systems. Finally, we will present open research problems that this new research agenda poses such as multi-task learning, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models.1 1 Overall Question answering (QA) has emerged as one of the most popular areas in natural language processing (NLP). Established benchmarks such as the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016) are used as a standard testing ground for new models while opendomain QA benchmarks such as Natural Questions (Kwiatkowski et al., 2019) represent the frontier of what is possible with current NLP technology (Zaheer et al., 2020). In this tutorial, we will review recent advances in open-domain QA but focus on an area that has received less attention both in research and in past tutorials—multi-domain and multilingual QA. Open-domain QA is of interest for building general-purpose assistants that can answer questions about any topic (Adiwardana et al., 2019). 1 The tutorial materials are available https://github.com/sebastianruder/ emnlp2021-multiqa-tutorial. at 17 Proceedings of the 2021 Conference on Empirical Methods in Natur"
2021.emnlp-tutorials.4,L18-1437,0,0.018285,"cross-lingual tasks by introducing zero-shot cross-lingual learning. 2. Multilingual QA [40 mins]: Then we will give a comprehensive overview of several 18 3.2 non-English multilingual question answering datasets and systems such as DuReader (He et al., 2018) and DRCD (Shao et al., 2018) in Chinese, ARCD (Mozannar et al., 2019) in Arabic, multi-domain QA (Gupta et al., 2018) in Hindi-English, and visual QA (Gao et al., 2016) in Chinese-English. We distinguish between datasets that have been created by obtaining naturally occurring data in a language or via translations from SQuAD into Korean (Lee et al., 2018; Li et al., 2018), French and Japanese (Asai et al., 2018) and Italian (Croce et al., 2019). Recent datasets such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) cover more languages while the recently introduced TyDiQA (Clark et al., 2020) and MKQA (Longpre et al., 2020) can be seen as multilingual counterparts to Natural Questions. Three of these datasets are part of XTREME (Hu et al., 2020), a massively multilingual benchmark for testing the cross-lingual generalization ability of state-ofthe-art methods. While state-of-the-art models have matched or surpassed human performan"
2021.emnlp-tutorials.4,2020.acl-main.653,0,0.0792514,"Missing"
2021.emnlp-tutorials.4,D18-1317,0,0.0253371,"s by introducing zero-shot cross-lingual learning. 2. Multilingual QA [40 mins]: Then we will give a comprehensive overview of several 18 3.2 non-English multilingual question answering datasets and systems such as DuReader (He et al., 2018) and DRCD (Shao et al., 2018) in Chinese, ARCD (Mozannar et al., 2019) in Arabic, multi-domain QA (Gupta et al., 2018) in Hindi-English, and visual QA (Gao et al., 2016) in Chinese-English. We distinguish between datasets that have been created by obtaining naturally occurring data in a language or via translations from SQuAD into Korean (Lee et al., 2018; Li et al., 2018), French and Japanese (Asai et al., 2018) and Italian (Croce et al., 2019). Recent datasets such as XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020) cover more languages while the recently introduced TyDiQA (Clark et al., 2020) and MKQA (Longpre et al., 2020) can be seen as multilingual counterparts to Natural Questions. Three of these datasets are part of XTREME (Hu et al., 2020), a massively multilingual benchmark for testing the cross-lingual generalization ability of state-ofthe-art methods. While state-of-the-art models have matched or surpassed human performance in general-purp"
2021.emnlp-tutorials.4,2021.ccl-1.108,0,0.0583382,"Missing"
2021.emnlp-tutorials.4,W19-4612,0,0.0374206,"Missing"
2021.emnlp-tutorials.4,P18-2124,0,0.0720249,"Missing"
2021.findings-emnlp.410,P18-1073,0,0.0529912,"Missing"
2021.findings-emnlp.410,2020.acl-main.421,1,0.808072,"ponent inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans- this work, we adopt the competitive and lightweight fer methods of prior work. (so-called bottleneck) adapter variant of Pfeiffer In experiments on zero-shot cross-lingual trans- et al. (2021a). There, only one adapter module, 4763 consisting of a successive down-projection and up-projection, is injected per Transformer layer"
2021.findings-emnlp.410,N19-1191,0,0.133549,"ross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to train a language adapter on the unlabeled data for each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of"
2021.findings-emnlp.410,N16-1101,0,0.0319479,"he nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimensional language embeddings λ(l) ∈ Rdl . These are used by the generator g, a hyper-network (Ha et al., 2017) component4 with its own parameterization φ, to produce the language-specific parameterization of the main model: θ (l) = gφ (λ(l) ). While g can in principle be any differentiable function (i.e., arbitrarily deep neural model), in practice it is typically set to a simple linear projection (i.e., φ = W ): gW (λ(l) ) , W λ(l) , (2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameter"
2021.findings-emnlp.410,P19-1070,1,0.898419,"Missing"
2021.findings-emnlp.410,2021.eacl-main.270,1,0.690338,"Missing"
2021.findings-emnlp.410,2020.acl-main.560,0,0.0116496,"e generation of task-agnostic LAs that can support downstream cross-lingual transfer for arbitrary NLP tasks. 4 A hyper-network is a neural model that generates the parameters of another (main) neural model. 5 Training MAD-G on 95 languages with dl = 32 (this work) achieves roughly a threefold saving in parameter size. 4764 3 MAD-G: Methodology MAD-G aims to enable resource-efficient adaptation of MMTs to a wide range of previously unseen, radically resource-poor languages,6 and contribute in this manner to more sustainable (Strubell et al., 2019; Moosavi et al., 2020) and more inclusive NLP (Joshi et al., 2020). We couple (i) the computational efficiency of the light-weight adapters (cf. Section 2.1) and (ii) knowledge sharing and zero-shot language transfer capabilities of CPG (cf. Section 2.2), with (iii) external linguistic (i.e., typological) knowledge (Ponti et al., 2019a) towards supporting arbitrary NLP tasks for (even radically) resource-poor languages. MAD-G mitigates important limitations of prior work. Unlike Üstün et al. (2020), we generate taskagnostic LAs, (re)usable across NLP tasks. Unlike the MAD-X framework (Pfeiffer et al., 2020b), which trains LAs independently for each language"
2021.findings-emnlp.410,2020.emnlp-main.363,1,0.824906,"Missing"
2021.findings-emnlp.410,E17-2002,0,0.408844,"rs can be leveraged in typically achieved through the use of adapter layers arbitrary downstream tasks (Pfeiffer et al., 2020b). (Houlsby et al., 2019; Pfeiffer et al., 2020b). MAD-G shares information across languages (i) In particular, a language adapter is a light-weight at the level of hidden representations by sharing component inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans-"
2021.findings-emnlp.410,2021.eacl-main.39,1,0.794583,"amed entity recognition (NER) on the MasakhaNER dataset for African languages (Adelani et al., 2021). For POS and DP, we evaluate on a substantial subset of all UD languages with available treebanks.8 We discern between three language groups in evaluation, with some examples in Table 1: (i) mBERT-seen languages are those included in mBERT’s pretraining; (ii) MAD-G-seen languages were not part of mBERT’s pretraining but are included in MAD8 For POS and DP, we omit only (i) languages with scripts unseen in mBERT’s pretraining, where mBERT’s tokenizer predominantly produces unknown (UNK) tokens (Pfeiffer et al., 2021b), (ii) languages lacking any information in URIEL, and (iii) languages whose treebanks have missing fields. For MasakhaNER, we evaluate on all dataset languages except Amharic, as Amharic also uses a script unseen by mBERT. G training; and (iii) unseen languages are those not included in mBERT pretraining nor in MAD-G training. 4.1 Baselines and MAD-G Variants mBERT is an MMT pretrained on the Wikipedias of 104 languages. We use mBERT as the base MMT for MAD-G. XLM-R is a state-of-the-art MMT pretrained on the CommonCrawl data of 100 languages (Conneau et al., 2020).9 We evaluate them in the"
2021.findings-emnlp.410,2020.emnlp-demos.7,1,0.70459,"Missing"
2021.findings-emnlp.410,2020.emnlp-main.617,1,0.622952,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.800,1,0.87244,"Missing"
2021.findings-emnlp.410,P19-1493,0,0.0388325,"te that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The gen"
2021.findings-emnlp.410,D18-1039,0,0.123067,"each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of learning separate adapters for each language, MAD-G leverages contextual parameter generation (CPG; Platanios et al., 2018a; Ponti et al., 2019b), that is, it learns a single model that can generate a language adapter for an arbitrary target language. At the core of MAD-G is a contextual parameter generator which 1 mBERT and XLM-R have been trained on corpora from Multilingual NLP has witnessed large ad104 and 100 languages, respectively. According to Glottolog vances, with cross-lingual word embedding spaces (Hammarström et al., 2017), however, there are over 7,000 (Mikolov et al., 2013; Artetxe et al., 2018; Glavaš languages spoken around the world. 4762 Findings of the Association for Computational Linguistics"
2021.findings-emnlp.410,2020.emnlp-main.185,1,0.891273,"Missing"
2021.findings-emnlp.410,J19-3005,1,0.858703,"Missing"
2021.findings-emnlp.410,D19-1288,1,0.883413,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.626,1,0.768969,"genealogical ties to high(er)-resource languages. CPG is a technique introduced by Platanios et al. (2018a) to address these drawbacks. While originally conceived for neural machine translation (NMT), CPG can be applied to any neural model f parameterized by θ, for which we aim to learn parameterizations for a number of different contexts; in multilingual NLP, these “contexts” are languages. In the instance-per-language approach, an independent parameterization θ (l) , l ∈ {1, . . . , nl }, is learned for each of the nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimens"
2021.findings-emnlp.410,2021.acl-long.243,1,0.823183,"Missing"
2021.findings-emnlp.410,2021.findings-acl.106,1,0.880987,"2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameters of f . The total number of parameters learned when training nl independent models is nl np , whereas the number of parameters in the W matrix is dl np . Therefore, neglecting the small number of parameters dedicated to language embeddings, the CPG approach uses fewer parameters when dl < nl .5 More importantly, in multilingual training the generator matrix W is shared across all languages, which enables knowledge sharing across languages and leads to improved transfer performance. Platanios et al. (2018b) and Ponti et al. (2021a) opt for randomly initializing language embeddings λ(l) and learning them end-to-end. Specified like this, however, CPG cannot generalize to languages unseen in training, as it would lack embeddings for those languages at inference. To support generalization to arbitrary new languages, one must ground language embeddings in some external language representation, available for many languages. To this end, Ponti et al. (2019b) exploit typological language vectors from the URIEL database (Littell et al., 2017) directly as language embeddings to generate a full set of model parameters. In a simi"
2021.findings-emnlp.410,2020.emnlp-main.180,0,0.119503,"Missing"
2021.findings-emnlp.410,D19-1077,0,0.021338,"sfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to tr"
2021.findings-emnlp.410,2021.naacl-main.41,0,0.0352696,"offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed a"
2021.findings-emnlp.63,D19-1165,0,0.027544,"t, there are limited datasets and benchmarks that evaluate NLP models’ ability to generalize to unseen dialect variations. Therefore, we only test our method on NER and POS tagging tasks because they have the best language coverage. It is an important future direction to construct high-quality datasets that consider language and dialect variations. Second, our method has slower inference speed due to test time computation. Future work can aim to reduce the cost by algorithmic or hardware innovations. Our work is related to parameter efficient fine- Acknowledgement tuning of pretrained models (Bapna et al., 2019; Pfeiffer et al., 2020b; Li and Liang, 2021; Guo This material is based on work supported by the et al., 2021). Specifically, (Üstün et al., 2020; National Science Foundation under grants 2040926 Karimi Mahabadi et al., 2021) make adapters more and 2125201. XW is supported by the Apple PhD generalizable by learning a parameter generator, fellowship. The authors would like to thank Laura while our work aims to utilize existing pretrained Rimell, Sachin Kumar and Hieu Pham for helpful adapters without further training. Pfeiffer et al. discussions on the drafts of the paper. (2021) propose to le"
2021.findings-emnlp.63,2020.coling-main.579,0,0.0997332,"Missing"
2021.findings-emnlp.63,2020.acl-main.747,0,0.0305789,"nguages and then a task adapter on annotated data in the source language. One drawback of this framework is that a separate language adapter is required for each target language, which is problematic in cases where the data to train these adapters cannot be easily obtained, such as for languages with diverse regional 1 Introduction or demographic variations. In fact, certain language Massively multilingual pretrained models (Devlin varieties are not included in the standard language et al., 2019; Huang et al., 2019; Conneau and identification tools, which makes it challenging to Lample, 2019; Conneau et al., 2020) combined reliably obtain even unlabeled data (Salameh et al., with cross-lingual transfer now define the state 2018; Caswell et al., 2020; Demszky et al., 2021). of the art on a variety of NLP tasks (Hu et al., To give just one example, the Nordic languages 2020). Within this paradigm, multilingual pre- and dialects form a dialect continuum where the trained models are fine-tuned on annotated data total number of language varieties is difficult to esof a task in a high-resource language, and trans- timate, and language varieties constantly emerge in ferred to other languages. Several recent w"
2021.findings-emnlp.63,2021.naacl-main.184,0,0.084255,"Missing"
2021.findings-emnlp.63,N19-1423,0,0.19552,"e embedding space for task Ti . MAD-X trains the adapters Ti (·) and Lj (·) in two steps. First, for each language Lj , its adapter Adapter Ensembling As a first solution to this Lj is inserted into M to replace the output of each problem, we propose an extremely simple strategy layer h with Lj (h). The resulting model, which we of averaging the transformed outputs of multiple denote as Lj ◦ M, is trained on unlabeled data in language adapters. Specifically, we use both the Lj using an unsupervised objective such as masked source language adapter Lsrc and adapters from language modeling (MLM; Devlin et al., 2019). related languages with similar linguistic properties Second, for each task Ti , its adapter Ti is inserted to the new language. Let R be the set of the source on top of a src language adapter Lsrc . The resulting and related language adapters. To do inference on model Ti ◦ Lsrc ◦ M is trained on the downstream a task T for the new language Lnew , we transform task Ti in language Lsrc . After these two steps, 1 Ti ◦ Lj ◦ M can be used to perform zero-shot crosshttps://adapterhub.ml/ 731 the output h of each layerP in M with the language adapters as Lavg (h) = R1 R i=1 Li (h). Entropy Minimize"
2021.findings-emnlp.63,2021.acl-long.353,0,0.0524787,"Missing"
2021.findings-emnlp.63,P17-1178,0,0.0159875,"..., T-1 do . Calculate entropy H(x, α) ← Entropy(T ◦ Lwavg (h, αt ) ◦ M) . Calculate gradient g t = ∇α H(x; αt ) . Update weighting αt+1 ← Update(αt , g t ) end . Calculate final prediction yˆ ← Predict(T ◦ Lwavg (h, αT ) ◦ M) Related hi is ru Additional Test en,ar en,de en mr,bn,ta,bho fo,no,da be,uk,bg Table 1: Test language groups and their corresponding language adapters. Adapters from languages in the first two columns are applied to the test languages in the third column. conduct experiments on named entity recognition (NER) and part-of-speech tagging (POS). We use the WikiAnn dataset (Pan et al., 2017) for NER and Universial Treebank 2.0 for POS tagging (Nivre et al., 2018). Model We use the mBERT (Devlin et al., 2019) model, which shows good performance for lowresource languages on the structured prediction tasks (Pfeiffer et al., 2020b; Hu et al., 2020). We use the English annotated data to train the task adapter. Each experiment is run with 3 different random seeds and we report the average performance. More details can be found in Appendix A. Languages Due to the scarcity of datasets for dialects, we focus on three groups of closely related languages to simulate the setup of language va"
2021.findings-emnlp.63,2021.eacl-main.39,0,0.0208731,"ining and storing extra parameters. Its performance is also not consistent across languages and tasks, likely because it is only trained on English labeled data. 50 70.0 67.5 Baselines We compare with several baselines: 1) En: the English adapter; 2) Related: the best performing related language adapter; 3) Continual learning (CL): we use the English language adapter and update its parameters using the entropy loss for each test input; 4) Fusion: learn another set of key, value and query parameters in each layer that uses the layer output as a query to mix together the output of each adapter (Pfeiffer et al., 2021). Since we do not use labeled data in the new language, we train the fusion parameters on English labeled data. 4.1 mr no F1 3.0 F1 gain by adding English F1 gain over ensemble Table 2: F1 of the baselines and our methods for each language group. EMEA-s1 updates the adapter weights with a single gradient step while EMEA-s10 updates for 10 steps. New adapter EMEA 1k 10k 50k 100k Monolingual Data 40 1k 10k 50k 100k Monolingual Data Figure 4: Comparison to training adapter on different amount of monolingual data. provements or is comparable to the best baseline on other languages. EMEA delivers f"
2021.findings-emnlp.63,2020.emnlp-demos.7,1,0.859962,"Missing"
2021.findings-emnlp.63,2021.acl-long.378,0,0.0665002,"Missing"
2021.findings-emnlp.63,2020.emnlp-main.617,1,0.857427,"Missing"
2021.findings-emnlp.63,2020.acl-main.244,0,0.0241551,"dapters to support a new language Lnew , which is not in {L1 , L2 , ..., Ln } on a given task T without training a new adapter for Lnew . Related Language Adapters One potential solution is to find the most related language Lrel ∈ {L1 , L2 , ..., Ln } and then use T ◦ Lrel ◦ M to do inference in Lnew . However, this has two disadvantages. First, the task adapter T is only trained in the setting of T ◦ Lsrc ◦ M, so it might not generalize well to the test time setting of T ◦ Lrel ◦ M (as shown in § 4.1). Second, while the pretrained model M may be relatively robust against distribution shifts (Hendrycks et al., 2020), the specialized language adapters might make the model brittle to language variations because they are trained for specific languages. Our experiments in § 4.1 show that this solution indeed leads to poor performance. To facilitate our discussion, we briefly summarize the MAD-X framework (Pfeiffer et al., 2020b) for zero-shot cross-lingual transfer and identify its shortcomings. The goal of MAD-X is to fine-tune a multilingual pretrained model M to m downstream tasks T1 , T2 , ..., Tm , each of which could be in n languages L1 , L2 , ..., Ln . To this end, MAD-X relies on language and task a"
2021.findings-emnlp.63,D19-1252,0,0.025998,"m zero-shot transfer by first training language-level adapters on monolingual data in different languages and then a task adapter on annotated data in the source language. One drawback of this framework is that a separate language adapter is required for each target language, which is problematic in cases where the data to train these adapters cannot be easily obtained, such as for languages with diverse regional 1 Introduction or demographic variations. In fact, certain language Massively multilingual pretrained models (Devlin varieties are not included in the standard language et al., 2019; Huang et al., 2019; Conneau and identification tools, which makes it challenging to Lample, 2019; Conneau et al., 2020) combined reliably obtain even unlabeled data (Salameh et al., with cross-lingual transfer now define the state 2018; Caswell et al., 2020; Demszky et al., 2021). of the art on a variety of NLP tasks (Hu et al., To give just one example, the Nordic languages 2020). Within this paradigm, multilingual pre- and dialects form a dialect continuum where the trained models are fine-tuned on annotated data total number of language varieties is difficult to esof a task in a high-resource language, and t"
2021.findings-emnlp.63,2021.acl-long.47,1,0.82652,"Missing"
2021.findings-emnlp.63,2021.eacl-main.6,0,0.0305732,"lts in Tab. 2 showing en as the best individual adapter. For the hi language group, the ar adapter tends to have the least benefit, probably because it has a different script from the languages we test on. 5 Related Work beled data to combine pretrained multitask adapters whereas our method does not require any training or labeled data. While we focus on language adapters in this work, our method is also applicable to ensembling domain or task adapters. Finally, our method is inspired by the test time adaptation framework proposed for image classification (Sun et al., 2020; Wang et al., 2021; Kedia and Chinthakindi, 2021). Instead of adapting a single model, we focus on efficient utilization of many pre-trained language adapters to improve the model’s robustness to language variations. 6 Discussion and Conclusion Language and dialect cannot be simply categorized into monolithic entities. Thus a truly intelligent NLP system should be able to recognize and adapt to personalized language varieties after it is trained and deployed. However, the standard system evaluation is built on the assumption that an NLP model is fixed once it is trained. In this paper, we focus on a specific case of this general problem—we f"
2021.findings-emnlp.63,C18-1113,0,0.0405423,"Missing"
2021.findings-emnlp.63,2020.emnlp-main.180,0,0.0424408,"Missing"
2021.naacl-main.40,D18-1269,0,0.0411757,"n the input tokenized by deterministic segmentation only. Therefore, our method does not add additional decoding latency. MVR needs about twice the fine-tuning cost compared to the baseline. However, compared to pretraining and inference usage of a model, fine-tuning is generally the least expensive component. 5 5.1 Experiments Training and evaluation We evaluate the multilingual representations using tasks from the XTREME benchmark (Hu et al., 2020), focusing on the zero-shot cross-lingual transfer with English as the source language. We consider sentence classification tasks including XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019), a structured prediction task of multilingual NER (Pan et al., 2017), and questionanswering tasks including XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). 5.2 Experiment setup We evaluate on both the mBERT model which utilizes BPE to tokenize the inputs, and the XLM-R models which uses ULM segmentation. To replicate the baseline, we follow the hyperparameters provided in the XTREME codebase4 . Models are fine-tuned on English training data and zero-shot transferred to other languages. We run each experiment with 5 random seeds and record the average"
2021.naacl-main.40,2020.acl-main.275,0,0.0119164,"that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unlabeled data. Several self-training methods utilize unlabeled examples to minimize the distance between the model predictions based on the unlabeled example and a noised version of the same input (Miyato et al., 2017b,a; Xu and Yang, 2017; Clark et al., 2018; Xie et al., 2018). Xu and Yang (2017) use knowledge distillation on unlabeled data to adapt models to a new language. Clark et al. (2018) propose to mask out differen"
2021.naacl-main.40,2020.acl-main.421,1,0.811039,"pared to the baseline. However, compared to pretraining and inference usage of a model, fine-tuning is generally the least expensive component. 5 5.1 Experiments Training and evaluation We evaluate the multilingual representations using tasks from the XTREME benchmark (Hu et al., 2020), focusing on the zero-shot cross-lingual transfer with English as the source language. We consider sentence classification tasks including XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019), a structured prediction task of multilingual NER (Pan et al., 2017), and questionanswering tasks including XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). 5.2 Experiment setup We evaluate on both the mBERT model which utilizes BPE to tokenize the inputs, and the XLM-R models which uses ULM segmentation. To replicate the baseline, we follow the hyperparameters provided in the XTREME codebase4 . Models are fine-tuned on English training data and zero-shot transferred to other languages. We run each experiment with 5 random seeds and record the average results and the standard deviation. SR We use BPE-dropout (Provilkov et al., 2020) for mBERT and ULM-sample (Kudo, 2018) for XLM-R models to do probabilistic segmentat"
2021.naacl-main.40,P18-2049,0,0.0209554,"and they require modification to the model pretraining stage. Chung et al. (2020) propose to cluster related languages together and run subword vocabulary construction on each language cluster when constructing vocabularies for mBERT. Their method is also applied at the pretraining stage and could be combined with our method for potential additional improvements. Our method is also related to prior work that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unlabeled data. Several self-tr"
2021.naacl-main.40,D19-1252,0,0.0811424,"tation that is inconsistent across languages, harming cross-lingual transfer performance, particularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy—which generally does Multilingual pre-trained representations (Devlin et al., 2019; Huang et al., 2019; Conneau and Lam- not agree with a language’s morphology—could ple, 2019; Conneau et al., 2020) are now an es- map words from different languages to very distant representations, hurting cross-lingual transfer. In sential component of state-of-the-art methods for fact, previous work (Conneau et al., 2020; Artetxe cross-lingual transfer (Wu and Dredze, 2019; Pires et al., 2020) has shown that heuristic fixes such as et al., 2019). These methods pretrain an encoder by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource language"
2021.naacl-main.40,2020.findings-emnlp.414,0,0.11505,"er by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation can lead to significant be fine-tuned on annotated data of a downstream performance improvements. task in a high-resource language, often English, and Despite this, there is not much work studying transferred to another language. In order to encode or improving subword segmentation methods for hundreds of languages with diverse vocabulary, it cross-lingual transfer. Bostrom and Durrett (2020) is standard for such multilingual models to employ empirically compare several popular word segmena shared subword vocabulary jointly learned on the tation algorithms for pretrained language models 1 Code for the method is released here: of a single language. Several works propose to https://github.com/cindyxinyiwang/ multiview-subword-regularization use different representation granularities, such as 473 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473–482 June 6–11, 2021. ©2021 Associati"
2021.naacl-main.40,2020.findings-emnlp.118,0,0.125653,"subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation can lead to significant be fine-tuned on annotated data of a downstream performance improvements. task in a high-resource language, often English, and Despite this, there is not much work studying transferred to another language. In order to encode or improving subword segmentation methods for hundreds of languages with diverse vocabulary, it cross-lingual transfer. Bostrom and Durrett (2020) is standard for such multilingual models to employ empirically compare several popular word segmena shared subword vocabulary jointly learned on the tation algorithms for pretrained language models 1 Code for the method is released here: of a single language. Several works propose to https://github.com/cindyxinyiwang/ multiview-subword-regularization use different representation granularities, such as 473 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473–482 June 6–11, 2021. ©2021 Associati"
2021.naacl-main.40,D14-1181,0,0.00330276,"l. (2018) propose to mask out different parts of the unlabeled input and encourage the model to make consistent prediction given these different inputs. These methods all focus on semi-supervised learning, while our method regulates model consistency to mitigate the subword segmentation discrepancy between different languages. 8 Conclusion Several works propose to optimize subwordsensitive word encoding methods for pretrained We believe that the results in this paper convinclanguage models. Ma et al. (2020) uses convolu- ingly demonstrate that standard deterministic subtional neural networks (Kim, 2014) on characters word segmentation is sub-optimal for multilingual to calculate word representations. Zhang and Li pretrained representations. Even incorporating sim(2020) propose to add phrases into the vocabulary ple methods for subword regularization such as 480 BPE-dropout at fine-tuning can improve the crosslingual transfer of pretrained models, and our proposed Multi-view Subword Regularization method further shows consistent and strong improvements over a variety of tasks for models built upon different subword segmentation algorithms. Going forward, we suggest that some variety of subwor"
2021.naacl-main.40,D18-1461,0,0.01798,"d representations of a single language, and they require modification to the model pretraining stage. Chung et al. (2020) propose to cluster related languages together and run subword vocabulary construction on each language cluster when constructing vocabularies for mBERT. Their method is also applied at the pretraining stage and could be combined with our method for potential additional improvements. Our method is also related to prior work that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model"
2021.naacl-main.40,2020.emnlp-main.367,0,0.61274,"e sample different segmentations x0 for each input sentence x⇤ . Previous works (Kudo, 2018; Provilkov et al., 2020) have demonstrated that subword regularization us2 ing both BPE-dropout and ULM-sampling are efWe use Pan et al. (2017)’s named entity recognition test fective at improving machine translation accuracy, data with mBERT’s tokenizer. 475 propose to adapt a pretrained multilingual model to a new language by augmenting the vocabulary with a new subword vocabulary learned on the target language, but this method might not help for languages other than the target language it adapts to. Chung et al. (2020) propose to separately construct a subword segmentation model for each cluster of related languages for pretraining the multilingual representations. However, directly modifying the word segmentation requires retraining large pretrained models, which is computationally prohibitive in most cases. In this paper, we instead propose a more efficient approach of using probabilistic segmentation during fine-tuning on labeled data of a downstream task. As mismatch in segmentation is one of the factors harming cross-lingual transfer, we expect a model that becomes more robust to different varieties of"
2021.naacl-main.40,D18-1217,0,0.334623,"sequence of characters. be sub-optimal as it creates a discrepancy between It then counts the most frequent character token the segmentations during the pretraining and fine- bigrams in the data, merges them into a new token, tuning stages. To address this problem, we pro- and adds the new token to the vocabulary. This pose Multi-view Subword Regularization (MVR; process is done iteratively until a predefined vocabFig. 1), a novel method—inspired by the usage ulary size is reached. of consistency regularization in semi-supervised To segment a word, BPE simply splits the word learning methods (Clark et al., 2018; Xie et al., into character tokens, and iteratively merges adja2018)—which utilizes both the standard and proba- cent tokens with the highest priority until no merge bilistically segmented inputs, enforcing the model’s operation is possible. That is, for an input x⇤ , it predictions to be consistent across the two views. assigns segmentation probability P (b x) = 1 for the Such consistency regularization further improves sequence x b obtained from the greedy merge operaaccuracy, with MVR finally demonstrating consis- tions, and assigns other possible segmentations a tent gains of up to 2.5 po"
2021.naacl-main.40,2020.acl-main.747,0,0.596672,"ticularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy—which generally does Multilingual pre-trained representations (Devlin et al., 2019; Huang et al., 2019; Conneau and Lam- not agree with a language’s morphology—could ple, 2019; Conneau et al., 2020) are now an es- map words from different languages to very distant representations, hurting cross-lingual transfer. In sential component of state-of-the-art methods for fact, previous work (Conneau et al., 2020; Artetxe cross-lingual transfer (Wu and Dredze, 2019; Pires et al., 2020) has shown that heuristic fixes such as et al., 2019). These methods pretrain an encoder by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation"
2021.naacl-main.40,P18-1007,0,0.317836,"g1 Sebastian Ruder2 Graham Neubig1 1 Language Technology Institute, Carnegie Mellon University 2 DeepMind xinyiw1@cs.cmu.edu,ruder@google.com,gneubig@cs.cmu.edu Abstract Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf"
2021.naacl-main.40,D18-2012,0,0.165334,"brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf/re/gung en/j/ousi/asmÏc fr pt ru excita/tion excita/ção волн/ение Table 1: XLM-R segmentation of “excitement” in different languages. The English word is not segmented while the same word in other languages is over-segmented. A better segmentation would allow the model to match the verb stem and derivational affix across languages. multilingual data using heuristic word segmentation methods based on byte-pair-encoding (BPE; Sennrich et al., 2016) or unigram language models (Kudo and Richardson, 2018) (details in §2). However, subword-based preprocessing can lead to sub-optimal segmentation that is inconsistent across languages, harming cross-lingual transfer performance, particularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy"
2021.naacl-main.40,Q17-1026,0,0.0193419,"a single language, and they require modification to the model pretraining stage. Chung et al. (2020) propose to cluster related languages together and run subword vocabulary construction on each language cluster when constructing vocabularies for mBERT. Their method is also applied at the pretraining stage and could be combined with our method for potential additional improvements. Our method is also related to prior work that optimize word representations for NMT and language modeling. Character level embeddings have been utilized instead of subword segmentation for NMT (Cherry et al., 2018; Lee et al., 2017; Ataman and Federico, 2018) and language modeling (Kim et al., 2016; Józefowicz et al., 2016). Wang et al. (2019) propose a multilingual word embedding method for NMT that relies on character n-gram embedding and a latent semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unl"
2021.naacl-main.40,2020.acl-main.653,0,0.0907825,"Missing"
2021.naacl-main.40,2020.coling-main.4,0,0.611782,"vocabulary jointly learned on the tation algorithms for pretrained language models 1 Code for the method is released here: of a single language. Several works propose to https://github.com/cindyxinyiwang/ multiview-subword-regularization use different representation granularities, such as 473 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 473–482 June 6–11, 2021. ©2021 Association for Computational Linguistics phrase-level segmentation (Zhang and Li, 2020) or character-aware representations (Ma et al., 2020) for pretrained language models of a single highresource language, such as English or Chinese only. However, it is not a foregone conclusion that methods designed and tested on monolingual models will be immediately applicable to multilingual representations. Furthermore, they add significant computation cost to the pretraining stage, which is especially problematic for multilingual pretraining on hundreds of languages. The problem of suboptimal subword segmentation has drawn more attention in the context of neural machine translation (NMT). Specifically, subword regularization methods have be"
2021.naacl-main.40,P17-1178,0,0.143433,"ual alignment. Despite these issues, few methods have tried to address this subword segmentation problem for multilingual pretrained models. Chau et al. (2020) Subword regularization (Kudo, 2018) is a method that incorporates probabilistic segmentation at training time to improve the robustness of models to different segmentations. The idea is conceptually simple: at training time sample different segmentations x0 for each input sentence x⇤ . Previous works (Kudo, 2018; Provilkov et al., 2020) have demonstrated that subword regularization us2 ing both BPE-dropout and ULM-sampling are efWe use Pan et al. (2017)’s named entity recognition test fective at improving machine translation accuracy, data with mBERT’s tokenizer. 475 propose to adapt a pretrained multilingual model to a new language by augmenting the vocabulary with a new subword vocabulary learned on the target language, but this method might not help for languages other than the target language it adapts to. Chung et al. (2020) propose to separately construct a subword segmentation model for each cluster of related languages for pretraining the multilingual representations. However, directly modifying the word segmentation requires retrain"
2021.naacl-main.40,2020.emnlp-main.617,1,0.871264,"Missing"
2021.naacl-main.40,P19-1493,0,0.169022,"Missing"
2021.naacl-main.40,2020.acl-main.170,0,0.564093,"Ruder2 Graham Neubig1 1 Language Technology Institute, Carnegie Mellon University 2 DeepMind xinyiw1@cs.cmu.edu,ruder@google.com,gneubig@cs.cmu.edu Abstract Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency between predictions of using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf/re/gung en/j/ousi/asmÏc"
2021.naacl-main.40,P16-1162,0,0.334748,"ilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.1 en de el excitement Auf/re/gung en/j/ousi/asmÏc fr pt ru excita/tion excita/ção волн/ение Table 1: XLM-R segmentation of “excitement” in different languages. The English word is not segmented while the same word in other languages is over-segmented. A better segmentation would allow the model to match the verb stem and derivational affix across languages. multilingual data using heuristic word segmentation methods based on byte-pair-encoding (BPE; Sennrich et al., 2016) or unigram language models (Kudo and Richardson, 2018) (details in §2). However, subword-based preprocessing can lead to sub-optimal segmentation that is inconsistent across languages, harming cross-lingual transfer performance, particularly on under-represented languages. As one example, consider the segmentation of the word “excitement” in different languages in Tab. 1. The English word is not segmented, but its translations in the other languages, including the relatively high-resourced French and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to"
2021.naacl-main.40,D19-1077,0,0.0632026,"ench and German, are segmented into multiple subwords. Since each sub1 Introduction word is mapped to a unique embedding vector, the segmentation discrepancy—which generally does Multilingual pre-trained representations (Devlin et al., 2019; Huang et al., 2019; Conneau and Lam- not agree with a language’s morphology—could ple, 2019; Conneau et al., 2020) are now an es- map words from different languages to very distant representations, hurting cross-lingual transfer. In sential component of state-of-the-art methods for fact, previous work (Conneau et al., 2020; Artetxe cross-lingual transfer (Wu and Dredze, 2019; Pires et al., 2020) has shown that heuristic fixes such as et al., 2019). These methods pretrain an encoder by increasing the subword vocabulary capacity and uplearning in an unsupervised way from raw textual sampling low-resource languages during learning data in up to hundreds of languages which can then of the subword segmentation can lead to significant be fine-tuned on annotated data of a downstream performance improvements. task in a high-resource language, often English, and Despite this, there is not much work studying transferred to another language. In order to encode or improving"
2021.naacl-main.40,P17-1130,0,0.0191392,"nt semantic embedding shared between different languages. Ataman and Federico (2018) show that character n-gram based embedding performs better than BPE for morphologically rich languages. He et al. (2020) propose to learn the optimal segmentation given a subword vocabulary for NMT. Our method is inspired by semi-supervised learning methods that enforce model consistency on unlabeled data. Several self-training methods utilize unlabeled examples to minimize the distance between the model predictions based on the unlabeled example and a noised version of the same input (Miyato et al., 2017b,a; Xu and Yang, 2017; Clark et al., 2018; Xie et al., 2018). Xu and Yang (2017) use knowledge distillation on unlabeled data to adapt models to a new language. Clark et al. (2018) propose to mask out different parts of the unlabeled input and encourage the model to make consistent prediction given these different inputs. These methods all focus on semi-supervised learning, while our method regulates model consistency to mitigate the subword segmentation discrepancy between different languages. 8 Conclusion Several works propose to optimize subwordsensitive word encoding methods for pretrained We believe that the"
2021.naacl-main.40,D19-1382,0,0.0414781,"stic segmentation only. Therefore, our method does not add additional decoding latency. MVR needs about twice the fine-tuning cost compared to the baseline. However, compared to pretraining and inference usage of a model, fine-tuning is generally the least expensive component. 5 5.1 Experiments Training and evaluation We evaluate the multilingual representations using tasks from the XTREME benchmark (Hu et al., 2020), focusing on the zero-shot cross-lingual transfer with English as the source language. We consider sentence classification tasks including XNLI (Conneau et al., 2018) and PAWS-X (Yang et al., 2019), a structured prediction task of multilingual NER (Pan et al., 2017), and questionanswering tasks including XQuAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020). 5.2 Experiment setup We evaluate on both the mBERT model which utilizes BPE to tokenize the inputs, and the XLM-R models which uses ULM segmentation. To replicate the baseline, we follow the hyperparameters provided in the XTREME codebase4 . Models are fine-tuned on English training data and zero-shot transferred to other languages. We run each experiment with 5 random seeds and record the average results and the standard devia"
D16-1103,W13-3520,0,0.0272963,"Missing"
D16-1103,S16-1044,0,0.107748,"Missing"
D16-1103,D16-1057,0,0.0151486,"language information is not enough to perform onpar with approaches that make use of large external resources (Kumar et al., 2016) and meticulously hand-crafted features (Brun et al., 2016). Sentiment lexicons are a popular way to inject additional information into models for sentiment analysis. We experimented with using sentiment lexicons by Kumar et al. (2016) but were not able to significantly improve upon our results with pre-trained embeddings11 . In light of the diversity of domains in the context of aspect-based sentiment analysis and many other applications, domain-specific lexicons (Hamilton et al., 2016) are often preferred. Finding better ways to incorporate such domain-specific resources into models as well as methods to inject other forms of domain information, e.g. by constraining them with rules (Hu et al., 2016) is thus an important research avenue, which we leave for future work. 6 Conclusion In this paper, we have presented a hierarchical model of reviews for aspect-based sentiment analysis. We demonstrate that by allowing the model to take into account the structure of the review and the sentential context for its predictions, it is able to outperform models that only rely on sentenc"
D16-1103,P16-1228,0,0.00479928,"way to inject additional information into models for sentiment analysis. We experimented with using sentiment lexicons by Kumar et al. (2016) but were not able to significantly improve upon our results with pre-trained embeddings11 . In light of the diversity of domains in the context of aspect-based sentiment analysis and many other applications, domain-specific lexicons (Hamilton et al., 2016) are often preferred. Finding better ways to incorporate such domain-specific resources into models as well as methods to inject other forms of domain information, e.g. by constraining them with rules (Hu et al., 2016) is thus an important research avenue, which we leave for future work. 6 Conclusion In this paper, we have presented a hierarchical model of reviews for aspect-based sentiment analysis. We demonstrate that by allowing the model to take into account the structure of the review and the sentential context for its predictions, it is able to outperform models that only rely on sentence information and achieves performance competitive with models that leverage large external resources and handengineered features. Our model achieves state-ofthe-art results on 5 out of 11 datasets for aspectbased sent"
D16-1103,S16-1174,0,0.104948,"rning (Sutskever et al., 2014). We segment Chinese data before tokenization. We train our model to minimize the cross-entropy loss, using stochastic gradient descent, the Adam update rule (Kingma and Ba, 2015), mini-batches of size 10, and early stopping with a patience of 10. 4.3 Comparison models We compare our model using random (H-LSTM) and pre-trained word embeddings (HP-LSTM) against the best model of the SemEval-2016 Aspectbased Sentiment Analysis task (Pontiki et al., 2016) for each domain-language pair (Best) as well as against the two best single models of the competition: IIT-TUDA (Kumar et al., 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al., 2016), which uses a parser aug8 Labeling them with a NONE aspect and predicting neutral slightly decreased performance. 1002 mented with hand-crafted, domain-specific rules. In order to ascertain that the hierarchical nature of our model is the deciding factor, we additionally compare against the sentence-level convolutional neural network of Ruder et al. (2016) (CNN) and against a sentence-level Bi-LSTM (LSTM), which is identical to the first layer of our model.9 5 Results and Discussion We present our results in"
D16-1103,N16-1062,0,0.0204096,"ls. Hierarchical models have been used predominantly for representation learning and generation of paragraphs and documents: Li et al. (2015) use a hierarchical LSTM-based autoencoder to reconstruct reviews and paragraphs of Wikipedia articles. Serban et al. (2016) use a hierarchical recurrent encoder-decoder with latent variables for dialogue generation. Denil et al. (2014) use a hierarchical ConvNet to extract salient sentences from reviews, while Kotzias et al. (2015) use the same architecture to learn sentence-level labels from review-level labels using a novel cost function. The model of Lee and Dernoncourt (2016) is perhaps the most similar to ours. While they also use a sentencelevel LSTM, their class-level feed-forward neural network is only able to consider a limited number of preceding texts, while our review-level bidirectional LSTM is (theoretically) able to consider an unlimited number of preceding and successive sentences. 1000 3 Model In the following, we will introduce the different components of our hierarchical bidirectional LSTM architecture displayed in Figure 2. 3.1 Sentence and Aspect Representation Each review consists of sentences, which are padded to length l by inserting padding to"
D16-1103,P15-1107,0,0.0245273,"Missing"
D16-1103,D15-1298,0,0.0336535,"Missing"
D16-1103,D14-1162,0,0.100383,"Missing"
D16-1103,S14-2004,0,0.148914,"r 1-5, 2016. 2016 Association for Computational Linguistics within a review renders our model fully languageindependent. We show that the hierarchical model outperforms strong sentence-level baselines for aspect-based sentiment analysis, while achieving results competitive with the state-of-the-art and outperforming it on several datasets without relying on any hand-engineered features or sentiment lexica. 2 Related Work Aspect-based sentiment analysis. Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence. However, it is less expressive than ours, as it only extracts features from the preceding and subsequent sentence without any notion of structure. Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015). In contrast, our model requires no feature engineering, no positional information, and no"
D16-1103,S15-2082,0,0.184003,"iew renders our model fully languageindependent. We show that the hierarchical model outperforms strong sentence-level baselines for aspect-based sentiment analysis, while achieving results competitive with the state-of-the-art and outperforming it on several datasets without relying on any hand-engineered features or sentiment lexica. 2 Related Work Aspect-based sentiment analysis. Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence. However, it is less expressive than ours, as it only extracts features from the preceding and subsequent sentence without any notion of structure. Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015). In contrast, our model requires no feature engineering, no positional information, and no parser outputs, which are often unavailable for low-resou"
D16-1103,S16-1053,1,0.840454,"ment Analysis task (Pontiki et al., 2016) for each domain-language pair (Best) as well as against the two best single models of the competition: IIT-TUDA (Kumar et al., 2016), which uses large sentiment lexicons for every language, and XRCE (Brun et al., 2016), which uses a parser aug8 Labeling them with a NONE aspect and predicting neutral slightly decreased performance. 1002 mented with hand-crafted, domain-specific rules. In order to ascertain that the hierarchical nature of our model is the deciding factor, we additionally compare against the sentence-level convolutional neural network of Ruder et al. (2016) (CNN) and against a sentence-level Bi-LSTM (LSTM), which is identical to the first layer of our model.9 5 Results and Discussion We present our results in Table 1. Our hierarchical model achieves results superior to the sentencelevel CNN and the sentence-level Bi-LSTM baselines for almost all domain-language pairs by taking the structure of the review into account. We highlight examples where this improves predictions in Table 2. In addition, our model shows results competitive with the best single models of the competition, while requiring no expensive hand-crafted features or external resou"
D16-1103,S15-2079,0,0.110161,"iew renders our model fully languageindependent. We show that the hierarchical model outperforms strong sentence-level baselines for aspect-based sentiment analysis, while achieving results competitive with the state-of-the-art and outperforming it on several datasets without relying on any hand-engineered features or sentiment lexica. 2 Related Work Aspect-based sentiment analysis. Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence. However, it is less expressive than ours, as it only extracts features from the preceding and subsequent sentence without any notion of structure. Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015). In contrast, our model requires no feature engineering, no positional information, and no parser outputs, which are often unavailable for low-resou"
D16-1103,S15-2125,0,0.0511759,"s within a review renders our model fully languageindependent. We show that the hierarchical model outperforms strong sentence-level baselines for aspect-based sentiment analysis, while achieving results competitive with the state-of-the-art and outperforming it on several datasets without relying on any hand-engineered features or sentiment lexica. 2 Related Work Aspect-based sentiment analysis. Past approaches use classifiers with expensive hand-crafted features based on n-grams, parts-of-speech, negation words, and sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). The model by Zhang and Lan (2015) is the only approach we are aware of that considers more than one sentence. However, it is less expressive than ours, as it only extracts features from the preceding and subsequent sentence without any notion of structure. Neural network-based approaches include an LSTM that determines sentiment towards a target word based on its position (Tang et al., 2015) as well as a recursive neural network that requires parse trees (Nguyen and Shirai, 2015). In contrast, our model requires no feature engineering, no positional information, and no parser outputs, which are often unavailable for low-resou"
D17-1038,D11-1033,0,0.218578,"e J and the value of J is returned. Gaussian Processes (GP) are a popular choice for p(f ) due to their descriptive power (Rasmussen, 2006). We use GP with Monte Carlo acquistion and Expected Improvement (EI) Data selection model In order to select training data for adaptation for a task T , existing approaches rank the available n training examples X = {x1 , x2 , · · · , xn } of k source domains D = {D1 , D2 , · · · , Dk } according to a domain similarity measure S and choose the top m samples for training their algorithm. While this has been shown to work empirically (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Van Asch and Daelemans, 2010; Remus, 2012), using a pre-existing metric leaves us unable to adapt to the characteristics of our task T and target domain DT and foregoes additional knowledge that may be gleaned from the interaction of different metrics. For this reason, we propose to learn the following linear domain similarity measure S as a linear combina373 (Moˇckus, 1974) as acquisition function as this combination has been shown to outperform comparable approaches (Snoek et al., 2012).1 3.2 • Bhattacharyya (Bhattacharya, P √ distance 1943): ln( i Pi Qi ) • Cosi"
D17-1038,P07-1034,0,0.116529,"We show the importance of complementing similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain"
D17-1038,Q16-1023,0,0.0539593,"most similar domain baseline. We train an LDA model (Blei et al., 2003) with 50 topics and 10 iterations for topic distribution-based representations and use GloVe embeddings (Pennington et al., 2014) trained on 42B tokens of Common Crawl data6 for word embedding-based representations. For sentiment analysis, we conduct 10 runs of each feature set for every domain and report mean and variance. For POS tagging and parsing, we observe that variance is low and perform one run while retaining random seeds for reproducibility. Parsing For parsing, we evaluate the state-ofthe-art Bi-LSTM parser by Kiperwasser and Goldberg (2016) with default hyperparameters.4 We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 shared task.5 2 All code is available at https://github.com/ sebastianruder/learn-to-select-data. 3 https://github.com/bplank/bilstm-aux 4 https://github.com/elikip/bist-parser 5 We leave investigating the effect of the adapted taggers on parsing for future work. 6 https://nlp.stanford.edu/projects/ glove/ 375 Base Learned measures Feature set Random Jensen-Shannon divergence – examples Jensen-Shannon divergence – domain Similarity"
D17-1038,P07-1056,0,0.743989,"s, partof-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and"
D17-1038,W06-1615,0,0.497077,"1 We also experimented with FABOLAS (Klein et al., 2017), but found its ability to adjust the training set size during optimization to be inconclusive for our relatively small training sets. 374 4.1 Tasks, datasets, and models We evaluate our approach on three tasks: sentiment analysis, part-of speech (POS) tagging, and dependency parsing. We use the n examples with the highest score as determined by the learned data selection measure for training our models.2 We show statistics for all datasets in Table 1. Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset (Blitzer et al., 2006). We use tf-idf-weighted unigram and bigram features and a linear SVM classifier (Blitzer et al., 2007). We set the vocabulary size to 10,000 and the number of training examples n = 1600 to conform with existing approaches (Bollegala et al., 2011) and stratify the training set. # labeled # unlabeled Sentiment Experiments Book DVD Electronics Kitchen 2000 2000 2000 2000 4465 3586 5681 5945 POS/Parsing 4 T Domain Answers Emails Newsgroups Reviews Weblogs WSJ 3489 4900 2391 3813 2031 2976 27274 1194173 1000000 1965350 524834 30060 Table 1: Number of labeled and unlabeled sentences for each domain"
D17-1038,P14-1014,0,0.0365427,"of complementing similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a f"
D17-1038,P11-1014,0,0.0383682,"pproach on three tasks: sentiment analysis, part-of speech (POS) tagging, and dependency parsing. We use the n examples with the highest score as determined by the learned data selection measure for training our models.2 We show statistics for all datasets in Table 1. Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset (Blitzer et al., 2006). We use tf-idf-weighted unigram and bigram features and a linear SVM classifier (Blitzer et al., 2007). We set the vocabulary size to 10,000 and the number of training examples n = 1600 to conform with existing approaches (Bollegala et al., 2011) and stratify the training set. # labeled # unlabeled Sentiment Experiments Book DVD Electronics Kitchen 2000 2000 2000 2000 4465 3586 5681 5945 POS/Parsing 4 T Domain Answers Emails Newsgroups Reviews Weblogs WSJ 3489 4900 2391 3813 2031 2976 27274 1194173 1000000 1965350 524834 30060 Table 1: Number of labeled and unlabeled sentences for each domain in the Amazon Reviews dataset (Blitzer et al., 2006) (above) and the SANCL 2012 dataset (Petrov and McDonald, 2012) for POS tagging and parsing (below). POS tagging For POS tagging and parsing, we evaluate on the coarse-grained POS data (12 unive"
D17-1038,P13-2119,0,0.201258,"nce these are very different tasks. Between related tasks, the combination of similarity and diversity features achieves the most robust transfer and outperforms the baselines in both cases. This suggests that even in the absence of target task data, we only require data of a related task to learn a successful data selection measure. 6 Related work Most prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (Søgaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011; Søgaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be se"
D17-1038,2014.amta-researchers.23,0,0.238541,"s sentiment in more similar ways, while for POS tagging having more varied training instances is intuitively more beneficial. In fact, when inspecting the domain distribution of our approach, we find that the best SA model chooses more instances from the closest domain, while for POS tagging instances are more balanced across domains. This suggests that the Web treebank domains are less clear-cut. In fact, training a model on all sources, which is considerably more and varied data (in this setup, 14-17.5k training instances) is beneficial. This is in line with findings in machine translation (Mirkin and Besacier, 2014), which show that similarity-based selection works best if domains are very different. Results are thus less pronounced for POS tagging, and we leave experimenting with larger n for future work. To gain some insight into the optimization procedure, Figure 1 shows the development accuracy for the Structured Perceptron for an example domain. The top-right and bottom graphs show the hypothesis space exploration of Bayesian Optimization for different single feature sets, while the Feature set ↓ MS → Term similarity Diversity Term similarity+diversity Answers B Pproxy 93.43 93.67 92.58 93.19 93.46"
D17-1038,Q14-1002,0,0.0176484,"similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a feature space X and a marginal"
D17-1038,D14-1162,0,0.120323,"set. We optimize each similarity measure using Bayesian Optimization with 300 iterations according to the objective measure J of each task (accuracy for sentiment analysis and POS tagging; LAS for parsing) with respect to the validation set of the corresponding target domain. Unlabeled data is used in addition to calculate the representation of the target domain and to calculate the source domain representation for the most similar domain baseline. We train an LDA model (Blei et al., 2003) with 50 topics and 10 iterations for topic distribution-based representations and use GloVe embeddings (Pennington et al., 2014) trained on 42B tokens of Common Crawl data6 for word embedding-based representations. For sentiment analysis, we conduct 10 runs of each feature set for every domain and report mean and variance. For POS tagging and parsing, we observe that variance is low and perform one run while retaining random seeds for reproducibility. Parsing For parsing, we evaluate the state-ofthe-art Bi-LSTM parser by Kiperwasser and Goldberg (2016) with default hyperparameters.4 We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 share"
D17-1038,D10-1069,0,0.0376712,"target domain used for learning metric S. B: Book. D: DVD. E: Electronics. K: Kitchen. Sim: term distributionbased similarity. Div: diversity. Best per feature set: bold. In-domain results: gray. SDAMS (Wu and Huang, 2016) listed as comparison. Transfer across models In addition, we are interested how well the metric learned for one target domain transfers to other settings. We first investigate its ability to transfer to another model. In practice, a metric can be learned using a model that is cheap to evaluate and serves as proxy for a state-of-the-art model, in a way similar to uptraining (Petrov et al., 2010). For this, we employ the data selection features learned using the Structured Perceptron model for POS tagging and use them to select data for the Bi-LSTM tagger. The results in Table 4 indicate that cross-model transfer is indeed possible, with most transferred feature sets achieving similar results or even outperforming features learned with the Bi-LSTM. In particular, transferred diversity significantly outperforms its in-model equivalent. This is encouraging, as it allows to learn a data selection metric using less complex models. 5.2 DS B D E K B D E K B D E K - with the highest performa"
D17-1038,P11-2120,0,0.0712149,"absence of target task data, we only require data of a related task to learn a successful data selection measure. 6 Related work Most prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (Søgaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011; Søgaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008; Van Asch and Daelemans, 2010). Many domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between t"
D17-1038,P11-1157,1,0.848495,"Missing"
D17-1038,P16-1013,0,0.303901,"training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a feature space X and a marginal probability distribution P (X) over X , 372 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 372–382 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tion of feature values: where X = {x1 , · · · , xn } ∈ X . For document"
D17-1038,P16-2067,1,0.889797,"00-5000 labeled sentences and more than 100,000 unlabeled sentences. In the case of WSJ, we use its dev and test data as labeled samples and treat the remaining sections as unlabeled. We set n = 2000 for POS tagging and parsing to retain enough examples for the most-similar-domain baseline. To evaluate the impact of model choice, we compare two models: a Structured Perceptron (inhouse implementation with commonly used features pertaining to tags, words, case, prefixes, as well as prefixes and suffixes) trained for 5 iterations with a learning rate of 0.2; and a state-of-theart Bi-LSTM tagger (Plank et al., 2016) with word and character embeddings as input. We perform early stopping on the validation set with patience of 2 and use otherwise default hyperparameters3 as provided by the authors. 4.2 Training details In practice, as feature values occupy different ranges, we have found it helpful to apply znormalisation similar to Tsvetkov et al. (2016). We moreover constrain the weights w to [−1, 1]. For each dataset, we treat each domain as target domain and all other domains as source domains. Similar to Bousmalis et al. (2016), we chose to use a small number (100) target domain examples as validation"
D17-1038,W10-2605,0,0.472993,"Missing"
D17-1038,D17-1147,0,0.141376,"Missing"
D17-1038,D08-1093,0,0.151303,", 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (Søgaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011; Søgaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008; Van Asch and Daelemans, 2010). Many domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between two domains in order to determine examples for annotation. Van Asch and Daelemans (2010) find that Rényi divergence outperforms other metrics in predicting POS tagging accuracy, while Plank and van Noord (2011) observe that topic distribution-based representations with Jensen-Shannon divergence perform best for data selection for parsing. Remus (2012) apply JensenShannon divergence to select training examples for se"
D17-1038,P16-1029,0,0.512777,"istribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a feature space X and a marginal probability distribution P (X) over X , 372 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 372–382 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computationa"
D17-1038,P10-2041,0,\N,Missing
D17-1038,N10-1004,0,\N,Missing
D18-1042,D16-1250,0,0.30206,"om a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. 6 Experiments We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English–Italian, English–German, and English–Finnish. 462 Mikolov et al. (2013c) Xing et al. (2015) Zhang et al. (2016) Artetxe et al. (2016) Artetxe et al. (2017) Ours (1:1) Ours (1:1, rank constr.) 5,000 English–Italian 25 num iden 5,000 34.93 36.87 36.73 39.27 39.67 41.00 42.47 00.00 0.00 0.07 0.07 37.27 39.63 41.13 1.87 27.13 28.07 31.07 39.97 41.07 41.80 35.00 41.27 40.80 41.87 40.87 42.60 41.93 0.00 0.13 0.27 0.40 39.40 40.47 41.40 English–German 25 num iden 0.00 0.07 0.13 0.13 39.60 42.40 42.40 0.07 0.53 0.87 0.73 40.27 42.60 41.93 19.20 38.13 38.27 41.53 40.67 43.20 41.47 5,000 25.91 28.23 28.16 30.62 28.72 29.78 28.23 English–Finnish 25 num iden 0.00 0.07 0.14 0.21 28.16 0.07 27.04 0.00 0.56 0.42 0.77 26.47 3.02 27.60 7.02"
D18-1042,P17-1042,0,0.0577102,"g a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008). However, like more recent approaches (Artetxe et al., 2017), our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over sever"
D18-1042,J93-2003,0,0.197772,"and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as analyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit modeling assumptions explicit. To this end, we provide a reinterpretation of Artetxe et al. (2017) as a latent-variable model with an IBM Model 1–style (Brown et al., 1993) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and Artetxe et al. (2017), the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages’ respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over Artetxe et al. (2017). 2 Background: Bilingual Lexicon Induction and Word Embeddings Bilingual lexicon induction2 is the task of finding word-level translations betwee"
D18-1042,P15-2001,0,0.0227503,"h Søgaard et al. (2018), we additionally use a dictionary of identically spelled strings in both vocabularies. Table 2: Spearman correlations on English–Italian and English–German cross-lingual word similarity datasets. 6.1 Experimental Details Datasets For bilingual dictionary induction, we use the English–Italian dataset by Dinu et al. (2015) and the English–German and English–Finnish datasets by Artetxe et al. (2017). For cross-lingual word similarity, we use the RG-65 and WordSim353 cross-lingual datasets for English–German and the WordSim-353 cross-lingual dataset for English– Italian by Camacho-Collados et al. (2015). Monolingual Embeddings We follow Artetxe et al. (2017) and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 billion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Implementation details Similar to Artetxe et al. (2017), we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 × 10−6 between succeeding iterations. Unless stated other"
D18-1042,W95-0114,0,0.626272,"ilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language. 458 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 458–468 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence"
D18-1042,P08-1088,0,0.333996,"ova KementchedjhievaZ Anders SøgaardZ @Insight Research Centre, National University of Ireland, Galway, Ireland HAylien Ltd., Dublin, Ireland SThe Computer Laboratory, University of Cambridge, Cambridge, UK PDepartment of Computer Science, Johns Hopkins University, Baltimore, USA ZDepartment of Computer Science, University of Copenhagen, Copenhagen, Denmark sebastian@ruder.io,ryan.cotterell@jhu.com,{yova|soegaard}@di.ku.dk Abstract We introduce a novel discriminative latentvariable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embeddingbased approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.1 1 Introduction Is there a more fundamental bilingual linguistic resource than a dictionary? The task of bilingual lexicon induction seeks to create a dictionary in a datadriven manner di"
D18-1042,N13-1056,0,0.0178519,"ntiev et al., 2012) to cross-lingual named entity recognition (Mayhew et al., 2017). In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison-Burch (2013), infused with the bipartite matching dictionary prior of Haghighi et al. (2008). However, like more recent approaches (Artetxe et al., 2017), our model operates directly over pretrained word embeddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectationmaximization algorithm (EM; Neal and Hinton, 1998) and employ an efficient matching algorithm. Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bil"
D18-1042,K18-1021,1,0.770121,"nking neighbor lists. Lazaridou et al. (2015) proposed a max-marging objective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the softmax (Smi, 2017) or scaling the similarity values (Conneau et al., 2018). 9 Table 5: Example translations for German-English. (2017) in German and seek their nearest neighbours in the English embedding space. P@1 over the German-English test set is 36.38 and 39.18 for Artetxe et al. (2017) and our method respectively. We show examples where nearest neighbours of the methods differ in Table 5. Similar to Kementchedjhieva et al. (2018), we find that morphologically related words are often the source of mistakes. Other common sources of mistakes in this dataset are names that are translated to different names and nearly synonymous words being predicted. Both of these sources indicate that while the learned alignment is generally good, it is often not sufficiently precise. 8 Conclusion We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of Artetxe et al. (2017). Our model combines the prior over bipartite matchings inspired by Haghighi et al. (2008) and the discriminative, rat"
D18-1042,E12-1014,0,0.0237578,"ally helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.1 1 Introduction Is there a more fundamental bilingual linguistic resource than a dictionary? The task of bilingual lexicon induction seeks to create a dictionary in a datadriven manner directly from monolingual corpora in the respective languages and, perhaps, a small seed set of translations. From a practical point of view, bilingual dictionaries have found uses in a myriad of NLP tasks ranging from machine translation (Klementiev et al., 2012) to cross-lingual named entity recognition (Mayhew et al., 2017). In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work. Our proposed model is a bridge between current state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by Mikolov et al. (2013b)’s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probability model, inspired by Irvine and Callison"
D18-1042,2005.mtsummit-papers.11,0,0.0965587,"1 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence of linguistic resources. m richtig bird Vogel mother trinken drink Mutter right Wurzel eye werfen ash usrc rot utrg Figure 1: Partial lexicons of German and English shown as a 2.1 bipartite graph. German is the target language and English is the source language. The ntrg = 7 German"
D18-1042,P15-1027,0,0.605353,"results for our approach in comparison to the baselines in Figure 2 for English–Italian using a 5,000 word seed lexicon across vocabularies consisting of different 12 Other recent improvements such as symmetric reweighting (Artetxe et al., 2018) are orthogonal to our method, which is why we do not explicitly compare to them here. 13 Note that results are not directly comparable to (Conneau et al., 2018) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia). Hubness problem We analyze empirically whether the prior helps with the hubness problem. Following Lazaridou et al. (2015), we define the hubness Nk (y) at k of a target word y as follows: Nk (y) = |{x ∈ Q |y ∈ NNk (x, G)}| (14) where Q is a set of query source language words and NNk (x, G) denotes the k nearest neighbors 464 14 We only use the words in the 5,000 word seed lexicon that are contained in the n most frequent words. We do not show results for the 25 word seed lexicon and numerals as they are not contained in the smallest n of most frequent words. (a) English–Italian (b) English–German (c) English–Finnish Figure 3: Bilingual dictionary induction results of our method with different priors using a 5,00"
D18-1042,D17-1269,0,0.108922,"Missing"
D18-1042,P95-1050,0,0.637006,"paper, we use bilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language. 458 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 458–468 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics equivalent, so the pair Hund–dog should be an entry in a German–English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of Rapp (1995) and Fung (1995). In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Importantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., EuroParl (Koehn, 2005). The bitext assumption is quite common in the literature; see Ruder et al. (2018, Table 2) for a survey. Additionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely"
D18-1042,N12-1087,0,0.0176416,"tep. Viterbi EM estimates the parameters by alternating between the two steps until convergence. We give the complete pseudocode in Algorithm 1. 4.1 Viterbi E-Step The E-step asks us to compute the posterior of latent bipartite matchings p(m |S, T ). Computation of this distribution, however, is intractable as it would require a sum over all bipartite matchings, which is #P-hard (Valiant, 1979). Tricks from combinatorial optimization make it possible to maximize over all bipartite matchings in polynomial time. Thus, we fall back on the Viterbi approximation for the E-step (Brown et al., 1993; Samdani et al., 2012). The derivation will follow Haghighi et al. (2008). In order to compute m? = argmax log pθ (m |S, T ) Finding a Maximal Bipartite Matching We frame finding an optimal one-to-one alignment between nsrc source and ntrg words as a combinatorial optimization problem, specifically, a linear assignment problem (LAP; Bertsimas and Tsitsiklis, 1997). In its original formulation, the LAP requires assigning a number of agents (source words) to a number of tasks (target words) at a cost that varies based on each assignment. An optimal solution assigns each source word to exactly one target word and vice"
D18-1042,P18-1072,1,0.88512,"Missing"
D18-1042,P16-1024,0,0.136192,"Missing"
D18-1042,N15-1104,0,0.514311,"source cannot be used more than once.4 (ii) There exists an orthogonal transformation, after which the embedding spaces are more or less equivalent. Assumption (i) may be true for related languages, but is likely false for morphologically rich languages that have a many-to-many relationship between the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in §6. In addition, we experiment with priors that express different matchings in §7. As for assumption (ii), previous work (Xing et al., 2015; Artetxe et al., 2017) has achieved some success using an orthogonal transformation; recently, however, Søgaard et al. (2018) demonstrated that monolingual embedding spaces are not approximately isomorphic and that there is a complex relationship between word form and meaning, which is only inadequately modeled by current approaches, which for example cannot model polysemy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in §6, giving them practical utility. Why it Works: The Hubness Problem Why should we expect the bipartite matching prior to help,"
D18-1042,N16-1156,0,0.309433,"Sa and Ta formed from a one-to-many alignment a, rather than a matching m. Why a Reinterpretation? The reinterpretation of Artetxe et al. (2017) as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce. 6 Experiments We first conduct experiments on bilingual dictionary induction and cross-lingual word similarity on three standard language pairs, English–Italian, English–German, and English–Finnish. 462 Mikolov et al. (2013c) Xing et al. (2015) Zhang et al. (2016) Artetxe et al. (2016) Artetxe et al. (2017) Ours (1:1) Ours (1:1, rank constr.) 5,000 English–Italian 25 num iden 5,000 34.93 36.87 36.73 39.27 39.67 41.00 42.47 00.00 0.00 0.07 0.07 37.27 39.63 41.13 1.87 27.13 28.07 31.07 39.97 41.07 41.80 35.00 41.27 40.80 41.87 40.87 42.60 41.93 0.00 0.13 0.27 0.40 39.40 40.47 41.40 English–German 25 num iden 0.00 0.07 0.13 0.13 39.60 42.40 42.40 0.07 0.53 0.87 0.73 40.27 42.60 41.93 19.20 38.13 38.27 41.53 40.67 43.20 41.47 5,000 25.91 28.23 28.16 30.62 28.72 29.78 28.23 English–Finnish 25 num iden 0.00 0.07 0.14 0.21 28.16 0.07 27.04 0.00 0.56 0.42 0.77"
D19-1090,P17-1042,0,0.480953,"imental paradigm in which we independently control for four different variables: the word form’s frequency, morphology, the lexeme frequency and the lexeme (a total of 480 experiments). Our comprehensive analysis reveals that BLI models can generalize for frequent morphosyntactic categories, even of infrequent lexemes, but fail to generalize for the more rare categories. This yields a more nuanced picture of the known deficiency of word embeddings to underperform on infrequent words (Gong et al., 2018). Our findings also contradict the strong empirical claims made elsewhere in the literature (Artetxe et al., 2017; Conneau et al., 2018; Ruder et al., 2018; Grave et al., 2018b), as we observe that performance severely degrades when the evaluation includes rare morphological variants of a word and infrequent lexemes. We picture this general trend in Figure 1, which also highlights the skew of existing dictionaries towards more frequent words.2 As our final contribution, we demonstrate that better encoding of morphology is indeed beneficial: enforcing a simple morphological constraint yields consistent performance improvements for all Romance language pairs and many of the Slavic language pairs. 2 2.1 Mor"
D19-1090,P18-1073,0,0.206257,"ˇre moˇre moˇre N ; NOM ; PL N ; DAT; SG N ; NOM ; SG N ; INS ; PL N ; GEN ; PL N ; ESS ; SG N ; DAT; PL Table 1: An example extract from our morphologically complete Polish–Czech dictionary. tions of new and less common forms, not present in the existing resources. In spite of this, most ground truth lexica used for BLI evaluation contain mainly frequent word forms. Many available resources are restricted to the top 200k most frequent words; this applies to the English–Italian dictionary of Dinu et al. (2015), the English–German and English– Finnish dictionaries of Artetxe et al. (2017), and Artetxe et al. (2018a)’s English–Spanish resource. The dictionaries of Irvine and Callison-Burch (2017) contain only the top most frequent 10k words for each language. Zhang et al. (2017) extracted their Spanish–English and Italian–English lexica from Open Multilingual WordNet (Bond and Paik, 2012), a resource which only yields high frequency, lemma level mappings. Another example is the recent MUSE dataset (Conneau et al., 2018), which was generated using an “internal translation tool”, and in which the majority of word pairs consist of forms ranked in the top 10k of the vocabularies of their respective language"
D19-1090,Q17-1010,1,0.332483,"al., 2018) and our morphologically complete dictionary, which contains many rare morphological variants of words. The numbers above the bars correspond to the number of translated source words (a hyphen represents an empty dictionary). 1 The dictionaries are available at https://github. com/pczarnowska/morph_dictionaries. ? Sebastian is now affiliated with DeepMind. humans do. Generalization to rare and novel words is arguably the main point of BLI as a task—most frequent translation pairs are already contained in digital dictionaries. Modern word embeddings encode character-level knowledge (Bojanowski et al., 2017), which should—in principle—enable the models to learn this behaviour; but morphological generalization has never been directly tested. Most existing dictionaries used for BLI evaluation do not account for the full spectrum of linguistic properties of language. Specifically, as we demonstrate in §2, they omit most morphological inflections of even common lexemes. To enable a more thorough evaluation we introduce a new resource: 40 morphologically complete dictionar974 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conferen"
D19-1090,P13-1133,0,0.0277452,"ocus on pairs of genetically-related languages for which we can cleanly map one morphological inflection onto another.3 We selected 5 languages from the Slavic family: Polish, Czech, Russian, Slovak and Ukrainian, and 5 Romance languages: French, Spanish, Italian, Portuguese and Catalan. Table 1 presents an example extract from our resource; every source–target pair is followed by their corresponding lemmata and a shared tag. We generated our dictionaries automatically based on openly available resources: Open Multilingual WordNet (Bond and Paik, 2012) and Extended Open Multilingual WordNet4 (Bond and Foster, 2013), both of which are collections of lexical databases which group words into sets of synonyms (synsets), and UniMorph5 (Kirov et al., 2016)—a resource comprised of inflectional word paradigms for 107 languages, extracted from Wiktionary6 and annotated according to the UniMorph schema (Sylak-Glassman, 2016). For each language pair (L1, L2) we first generated lemma translation pairs by mapping all L1 lemmata to all L2 lemmata for each synset that appeared in both L1 3 One may translate talked, the past tense of talk, into many different Spanish forms, but the Portuguese falavam has, arguably, onl"
D19-1090,L18-1550,1,0.925323,"erent variables: the word form’s frequency, morphology, the lexeme frequency and the lexeme (a total of 480 experiments). Our comprehensive analysis reveals that BLI models can generalize for frequent morphosyntactic categories, even of infrequent lexemes, but fail to generalize for the more rare categories. This yields a more nuanced picture of the known deficiency of word embeddings to underperform on infrequent words (Gong et al., 2018). Our findings also contradict the strong empirical claims made elsewhere in the literature (Artetxe et al., 2017; Conneau et al., 2018; Ruder et al., 2018; Grave et al., 2018b), as we observe that performance severely degrades when the evaluation includes rare morphological variants of a word and infrequent lexemes. We picture this general trend in Figure 1, which also highlights the skew of existing dictionaries towards more frequent words.2 As our final contribution, we demonstrate that better encoding of morphology is indeed beneficial: enforcing a simple morphological constraint yields consistent performance improvements for all Romance language pairs and many of the Slavic language pairs. 2 2.1 Morphological Dictionaries Existing Dictionaries Frequent word fo"
D19-1090,J17-2001,0,0.0525854,"GEN ; PL N ; ESS ; SG N ; DAT; PL Table 1: An example extract from our morphologically complete Polish–Czech dictionary. tions of new and less common forms, not present in the existing resources. In spite of this, most ground truth lexica used for BLI evaluation contain mainly frequent word forms. Many available resources are restricted to the top 200k most frequent words; this applies to the English–Italian dictionary of Dinu et al. (2015), the English–German and English– Finnish dictionaries of Artetxe et al. (2017), and Artetxe et al. (2018a)’s English–Spanish resource. The dictionaries of Irvine and Callison-Burch (2017) contain only the top most frequent 10k words for each language. Zhang et al. (2017) extracted their Spanish–English and Italian–English lexica from Open Multilingual WordNet (Bond and Paik, 2012), a resource which only yields high frequency, lemma level mappings. Another example is the recent MUSE dataset (Conneau et al., 2018), which was generated using an “internal translation tool”, and in which the majority of word pairs consist of forms ranked in the top 10k of the vocabularies of their respective languages. Another problem associated with existing resources is ‘semantic leakage’ between"
D19-1090,L16-1498,0,0.045505,"Missing"
D19-1090,D18-1042,1,0.74662,"but they will only have seen the less frequent, first-person plural future form hablar´amos a few times. Nevertheless, they would have no problem translating the latter. In this paper we ask whether current methods for bilingual lexicon induction (BLI) generalize morphologically as k Re ma 00 50 0 - ini ng OO Vs - –6 k - 00 00 –5 k - 40 0 k 30 0 k - 00 –3 20 0 00 –2 00 - –4 - 10 0 0k 0k –5 –1 50 10 –1 vo ca +O b OV s In Introduction - k - 0.0 Figure 1: The relation between the BLI performance and the frequency of source words in the test dictionary. The graph presents results for the model of Ruder et al. (2018) evaluated on both the MUSE dictionary (Conneau et al., 2018) and our morphologically complete dictionary, which contains many rare morphological variants of words. The numbers above the bars correspond to the number of translated source words (a hyphen represents an empty dictionary). 1 The dictionaries are available at https://github. com/pczarnowska/morph_dictionaries. ? Sebastian is now affiliated with DeepMind. humans do. Generalization to rare and novel words is arguably the main point of BLI as a task—most frequent translation pairs are already contained in digital dictionaries. Modern"
D19-1090,N13-1011,0,0.0656821,"Missing"
D19-1090,P17-1179,0,0.0476665,"olish–Czech dictionary. tions of new and less common forms, not present in the existing resources. In spite of this, most ground truth lexica used for BLI evaluation contain mainly frequent word forms. Many available resources are restricted to the top 200k most frequent words; this applies to the English–Italian dictionary of Dinu et al. (2015), the English–German and English– Finnish dictionaries of Artetxe et al. (2017), and Artetxe et al. (2018a)’s English–Spanish resource. The dictionaries of Irvine and Callison-Burch (2017) contain only the top most frequent 10k words for each language. Zhang et al. (2017) extracted their Spanish–English and Italian–English lexica from Open Multilingual WordNet (Bond and Paik, 2012), a resource which only yields high frequency, lemma level mappings. Another example is the recent MUSE dataset (Conneau et al., 2018), which was generated using an “internal translation tool”, and in which the majority of word pairs consist of forms ranked in the top 10k of the vocabularies of their respective languages. Another problem associated with existing resources is ‘semantic leakage’ between train and evaluation sets. As we demonstrate in §2.3, it is common for a single lex"
D19-1572,N18-2085,0,0.0383715,"and Conneau, 2019) additionally pretrains BERT with parallel data. These models enable zero-shot transfer, but achieve lower results than monolingual models. In contrast, we focus on making the training of monolingual language models more efficient in a multi-lingual context. Concurrent work (Mulcaire et al., 2019) pretrains on English and another language, but shows that cross-lingual pretraining only helps sometimes. Multi-lingual language modeling Training language models in non-English languages has only recently received some attention. Kawakami et al. (2017) evaluate on seven languages. Cotterell et al. (2018) study 21 languages. Gerz et al. (2018) create datasets for 50 languages. All of these studies, however, only create small datasets, which are inadequate for pretraining language models. In contrast, we are among the first to report the a) Pretrain Target Language Wikipedia b) Fine-tune Pretrained LM c) Train Classifier Fine-tuned LM Zero-shot Classifier Figure 2: The steps of our cross-lingual bootstrapping method for zero-shot cross-lingual transfer. a) A monolingual language model (LM) is pretrained on target language data; b) the LM is fine-tuned on target language documents; and c) the LM"
D19-1572,N19-1392,0,0.0246697,"LSTM as our language model. Cross-lingual pretrained language models The multi-lingual BERT model is pretrained on the Wikipedias of 104 languages using a shared word piece vocabulary. LASER (Artetxe and Schwenk, 2018) is trained on parallel data of 93 languages with a shared BPE vocabulary. XLM (Lample and Conneau, 2019) additionally pretrains BERT with parallel data. These models enable zero-shot transfer, but achieve lower results than monolingual models. In contrast, we focus on making the training of monolingual language models more efficient in a multi-lingual context. Concurrent work (Mulcaire et al., 2019) pretrains on English and another language, but shows that cross-lingual pretraining only helps sometimes. Multi-lingual language modeling Training language models in non-English languages has only recently received some attention. Kawakami et al. (2017) evaluate on seven languages. Cotterell et al. (2018) study 21 languages. Gerz et al. (2018) create datasets for 50 languages. All of these studies, however, only create small datasets, which are inadequate for pretraining language models. In contrast, we are among the first to report the a) Pretrain Target Language Wikipedia b) Fine-tune Pretr"
D19-1572,N18-1202,0,0.313946,"nguages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code1 . 1 Introduction Pretrained language models (LMs) have shown striking improvements on a range of natural language processing (NLP) tasks (Peters et al., 2018a; Howard and Ruder, 2018; Devlin et al., 2018). These models only require unlabelled data for training and are thus particularly useful in scenarios where labelled data is scarce. As much of NLP research has focused on the English language, the larger promise of these models is to bridge the digital language divide2 and enable the application of NLP methods to many of the world’s other 6,000 languages where labelled data is less plentiful. Recently, cross-lingual extensions of these LMs have been proposed that train on multiple languages jointly (Artetxe and Schwenk, 2018; Lample and Conneau,"
D19-1572,Q18-1032,0,0.0271758,") additionally pretrains BERT with parallel data. These models enable zero-shot transfer, but achieve lower results than monolingual models. In contrast, we focus on making the training of monolingual language models more efficient in a multi-lingual context. Concurrent work (Mulcaire et al., 2019) pretrains on English and another language, but shows that cross-lingual pretraining only helps sometimes. Multi-lingual language modeling Training language models in non-English languages has only recently received some attention. Kawakami et al. (2017) evaluate on seven languages. Cotterell et al. (2018) study 21 languages. Gerz et al. (2018) create datasets for 50 languages. All of these studies, however, only create small datasets, which are inadequate for pretraining language models. In contrast, we are among the first to report the a) Pretrain Target Language Wikipedia b) Fine-tune Pretrained LM c) Train Classifier Fine-tuned LM Zero-shot Classifier Figure 2: The steps of our cross-lingual bootstrapping method for zero-shot cross-lingual transfer. a) A monolingual language model (LM) is pretrained on target language data; b) the LM is fine-tuned on target language documents; and c) the LM"
D19-1572,D18-1179,0,0.0840164,"nguages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code1 . 1 Introduction Pretrained language models (LMs) have shown striking improvements on a range of natural language processing (NLP) tasks (Peters et al., 2018a; Howard and Ruder, 2018; Devlin et al., 2018). These models only require unlabelled data for training and are thus particularly useful in scenarios where labelled data is scarce. As much of NLP research has focused on the English language, the larger promise of these models is to bridge the digital language divide2 and enable the application of NLP methods to many of the world’s other 6,000 languages where labelled data is less plentiful. Recently, cross-lingual extensions of these LMs have been proposed that train on multiple languages jointly (Artetxe and Schwenk, 2018; Lample and Conneau,"
D19-1572,P10-1114,0,0.271594,"ual’ as referring to training independent models in multiple languages. We use ‘cross-lingual‘ to refer to training a joint model across multiple languages. 5702 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5702–5707, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics monolingual language model to the zero-shot setting. We evaluate our models on two widely used cross-lingual classification datasets, MLDoc (Schwenk and Li, 2018) and CLS (Prettenhofer and Stein, 2010) where we outperform the stateof-the-art zero-shot model LASER (Artetxe and Schwenk, 2018) and multi-lingual BERT (Devlin et al., 2018) in the supervised setting—even without any pretraining. In the zero-shot setting, we outperform both models using pseudo labels—and report significantly higher performance with as little as 100 examples. We finally show that information from monolingual and cross-lingual language models is complementary and that pretraining makes models robust to noise. 2 SUB1 400 embs. 1550 QRNN1 1550 QRNN2 1550 QRNN3 400 QRNN4 SUB2 400 embs. 1550 QRNN1 1550 QRNN2 1550 QRNN3"
D19-1572,P18-1096,1,0.844015,".02 85.55 85.93 85.82 89.30 90.25 85.35 85.15 87.48 88.63 90.03 85.65 84.65 86.85 87.52 87.65 87.30 88.98 90.72 92.17 90.03 92.52 Zero-shot (1,000 source language examples) MultiCCA LASER, paper LASER, code MultiBERT MultiFiT, pseudo Cross-lingual Bootstrapping Prior methods have employed cross-lingual training strategies relying on parallel data and a shared BPE vocabulary. These can be combined with our language model, but increase its training complexity. For the case where an existing pretrained cross-lingual model and source language data are available, we propose a bootstrapping method (Ruder and Plank, 2018) that uses the pretrained model’s zero-shot predictions as pseudo labels to fine-tune the monolingual model on target language data. The steps of the method can be seen in Figure 2. Specifically, we first fine-tune a linear classification layer on top of pretrained cross-lingual representations on source language training data. We then apply this cross-lingual classifier to the target language data and store its predicted label for every example. We now fine-tune our pretrained LM on the target language data and these pseudo labels7 . Importantly, this method enables our monolingual LM to sign"
D19-1572,P18-1031,1,0.924399,"lti-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code1 . 1 Introduction Pretrained language models (LMs) have shown striking improvements on a range of natural language processing (NLP) tasks (Peters et al., 2018a; Howard and Ruder, 2018; Devlin et al., 2018). These models only require unlabelled data for training and are thus particularly useful in scenarios where labelled data is scarce. As much of NLP research has focused on the English language, the larger promise of these models is to bridge the digital language divide2 and enable the application of NLP methods to many of the world’s other 6,000 languages where labelled data is less plentiful. Recently, cross-lingual extensions of these LMs have been proposed that train on multiple languages jointly (Artetxe and Schwenk, 2018; Lample and Conneau, 2019). These models are"
D19-1572,L18-1560,0,0.261966,"al., 2018). 5 We use ‘multilingual’ as referring to training independent models in multiple languages. We use ‘cross-lingual‘ to refer to training a joint model across multiple languages. 5702 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5702–5707, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics monolingual language model to the zero-shot setting. We evaluate our models on two widely used cross-lingual classification datasets, MLDoc (Schwenk and Li, 2018) and CLS (Prettenhofer and Stein, 2010) where we outperform the stateof-the-art zero-shot model LASER (Artetxe and Schwenk, 2018) and multi-lingual BERT (Devlin et al., 2018) in the supervised setting—even without any pretraining. In the zero-shot setting, we outperform both models using pseudo labels—and report significantly higher performance with as little as 100 examples. We finally show that information from monolingual and cross-lingual language models is complementary and that pretraining makes models robust to noise. 2 SUB1 400 embs. 1550 QRNN1 1550 QRNN2 1550 QRNN3 400 QRNN4 SUB2 400"
D19-1572,P07-2045,0,0.00743074,"Missing"
D19-1572,P18-1007,0,0.223668,"in the embedding space.4 2) Infrequent scripts are over-segmented in the shared word piece vocabulary (Wang et al., 2019). In this work, we show that small monolingual LMs are able to outperform expensive crosslingual models both in the zero-shot and the supervised setting. We propose Multi-lingual language model Fine-tuning (MultiFit) to enable practitioners to train and fine-tune language models efficiently.5 Our model combines universal language model fine-tuning (ULMFiT; Howard and Ruder, 2018) with the quasi-recurrent neural network (QRNN; Bradbury et al., 2017) and subword tokenization (Kudo, 2018) and can be pretrained on a single Tesla V100 GPU in a few hours. In addition, we propose to use a pretrained cross-lingual model’s predictions as pseudo labels to adapt the 3 The training cost is amortized over time as pretraining only needs to be performed once and fine-tuning is much cheaper. However, if a model needs to be applied to a new language or a domain not covered by the model, a new model needs to be trained from scratch. 4 This is similar to how word embeddings are known to underperform on low-frequency tokens (Gong et al., 2018). 5 We use ‘multilingual’ as referring to training"
D19-1572,P16-1162,0,0.0192824,"hyperparameters. To enable faster training and finetuning of the model, we replace it with a QRNN (Bradbury et al., 2017). The QRNN alternates convolutional layers, which are parallel across timesteps, and a recurrent pooling function, which is parallel across channels. It has been shown to outperform LSTMs, while being up to 16× faster at train and test time. ULMFiT in addition is restricted to words as input. To make our model 5703 more robust across languages, we use subword tokenization based on a unigram language model (Kudo, 2018), which is more flexible compared to byte-pair encoding (Sennrich et al., 2016). We additionally employ label smoothing (Szegedy et al., 2016) and a novel cosine variant of the one-cycle policy (Smith, 2018)6 , which we found to outperform ULMFiT’s slanted triangular learning rate schedule and gradual unfreezing. The full model can be seen in Figure 1. 3.2 Experimental setup This section provides an overview of our experimental setup; see the appendix for full details. Data We evaluate our models on the Multilingual Document Classification Corpus (MLDoc; Schwenk and Li, 2018)8 —a new subset of Reuters Corpus Volume 2 (Lewis et al., 2004) with balanced class priors for ei"
D19-1572,P16-1133,0,0.0599365,"Missing"
K18-1021,W16-1614,0,0.151595,"Missing"
K18-1021,E14-1049,0,0.504024,"vector spaces indeed reported superior performance with linear models as compared to non-linear neural approaches (Mikolov et al., 2013). The relative success of the simple linear approach can be explained in terms of isomorphism across monolingual semantic spaces,1 an idea that receives support from cognitive science (Youn et al., 1999). Word vector spaces are not perfectly isomorphic, however, as shown by Søgaard et al. (2018), who use a Laplacian graph similarity metric to measure this property. In this work, we show that projecting both source and target vector spaces into a third space (Faruqui and Dyer, 2014), using a variant of PA known as Generalized Procrustes Analysis (Gower, 1975), makes it easier to learn the alignment between two word vector spaces, as compared to the single linear transform used in Conneau et al. (2018). Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for support"
K18-1021,N15-1028,0,0.0274487,"nality, as studied in Mimno and Thompson (2017), increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. 6 Related work Bilingual embeddings Many diverse crosslingual word embedding models have been proposed (Ruder et al., 2018). The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013). In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017, 2018; Conneau et al., 2018; Lu et al., 2015). The approach most similar to ours, Faruqui and Dyer (2014), uses canonical correlation analysis (CCA) to project both source and target language spaces into a third, joint space. In this setup, similarly to GPA, the third space is iteratively updated, such that at timestep t, it is a product of the two language spaces as transformed by the mapping learned at timestep t − 1. The objective that drives the updates of the mapping matrices is to maximize the correlation between the projected embeddings of translational equivalents (where the latter are taken from a gold-standard seed dictionary)."
K18-1021,D17-1308,0,0.0306298,"w-resource languages do not necessarily have lower Procrustes fits than high-resource ones (compare Estonian and Finnish, for example), the gap between the Procrustes fit and GPA precision is on average much higher within low-resource languages than within high-resource ones (52.4610 compared to 25.47, respectively). This finding is in line with the common understanding that the quality of distributional word vectors depends on the amount of data available—we can infer from these results that suboptimal embeddings results in suboptimal cross-lingual alignments. 5.3 rectionality, as studied in Mimno and Thompson (2017), increases with corpus size, so we would expect embedding spaces learned from corpora comparable in size, to also be more similar in shape. 6 Related work Bilingual embeddings Many diverse crosslingual word embedding models have been proposed (Ruder et al., 2018). The most popular kind learns a linear transformation from source to target language space (Mikolov et al., 2013). In most recent work, this mapping is constrained to be orthogonal and solved using Procrustes Analysis (Xing et al., 2015; Artetxe et al., 2017, 2018; Conneau et al., 2018; Lu et al., 2015). The approach most similar to"
K18-1021,P17-1042,0,0.683002,"ment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings. 1 Introduction Several papers recently demonstrated the potential of very weakly supervised or entirely unsupervised approaches to bilingual dictionary induction (BDI) (Barone, 2016; Artetxe et al., 2017; Zhang et al., 2017; Conneau et al., 2018; Søgaard et al., 2018), the task of identifying translational equivalents across two languages. These approaches cast BDI as a problem of aligning monolingual word embeddings. Pairs of monolingual word vector spaces can be aligned without any explicit crosslingual supervision, solely based on their distributional properties (for an adversarial approach, see Conneau et al. (2018)). Alternatively, weak supervision can be provided in the form of numerals (Artetxe et al., 2017) or identically spelled words (Søgaard et al., 2018). Successful unsupervised o"
K18-1021,P18-1072,1,0.852776,"Missing"
K18-1021,D17-1270,0,0.108811,"Missing"
K18-1021,N15-1104,0,0.607491,"GPA, which aligns both source and target spaces with a third, latent space, constructed by averaging over the two language spaces. (1) thus minimizing the trace between each two corresponding rows of the transformed space T E and F . We build E and F based on a seed dictionary of N entries, such that each pair of corresponding rows in E and F , (en , fn ) for n = 1, . . . , N consists of the embeddings of a translational pair of words. In order to preserve the monolingual quality of the transformed embeddings, it is beneficial to use an orthogonal matrix T for cross-lingual mapping purposes (Xing et al., 2015; Artetxe et al., 2017). Conveniently, the orthogonal Procrustes problem has an analytical solution, based on Singular Value Decomposition (SVD): F > E = U ΣV > T = V U> 3 For an analytical solution to GPA, we compute the average of the embedding matrices E1...k after transformation by T1...k : G=k {T1 ,...,Tk } k X ||Ti Ei − Tj Ej )||2 Ei Ti (4) thus obtaining a latent space, G, which captures properties of each of E1...k , and potentially additional properties emerging from the combination of the spaces. On the very first iteration, prior to having any estimates of T1...k , we set G = Ei for"
K18-1021,P17-1179,0,0.141472,"Missing"
K18-1021,J13-3004,0,\N,Missing
N18-1172,E17-2026,1,0.796307,"ks, possibly accounting for its poor performance on the inference task. We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning. 6.2 Auxilary Tasks For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4. In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017). Indeed we find that most often a combination of auxiliary tasks achieves the best performance. Indomain tasks are less used than we assumed; only Target is consistently used by all Twitter main tasks. In addition, tasks with a higher number of labels, e.g. Topic-5 are used more often. Such tasks provide a more fine-grained reward signal, which may help in learning representations that generalise better. Finally, tasks with large amounts Table 4: Best-performing auxiliary tasks for different main tasks. of training data such as FNC-1 and MultiNLI are also used more often. Even if not directly"
N18-1172,W17-0225,0,0.0664442,"o provides us with a picture of what auxilary tasks are beneficial, and to what extent we can expect synergies from multitask learning. For instance, the notion of positive sentiment appears to be very similar across the topic-based and aspect-based tasks, while the conceptions of negative and neutral sentiment differ. In addition, we can see that the model has failed to learn a relationship between MultiNLI labels and those of other tasks, possibly accounting for its poor performance on the inference task. We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning. 6.2 Auxilary Tasks For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4. In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017). Indeed we find that most often a combination of auxiliary tasks achieves the best performance. Indomain tasks are less used than we assumed; only"
N18-1172,P17-1031,1,0.833536,"ed data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis. 1 Introduction Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP. Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017). Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016). If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000). However, while sharing hidden layers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we pot"
N18-1172,S16-1044,0,0.060276,"Missing"
N18-1172,D16-1070,0,0.0255161,"can thus not be directly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers ha"
N18-1172,W17-5307,0,0.0309675,"Missing"
N18-1172,P14-2009,0,0.0469174,"o power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again” known to be about the topic “AC/DC”, which is labelled as a positive sentiment. The evaluation metrics for Topic-2 and Topic-5 are macro-averaged recall (ρP N ) and macro-averaged mean absolute error (M AE M ) respectively, which are both averaged across topics. Target-dependent sentiment analysis Targetdependent sentiment analysis (Target) seeks to classify the sentiment of a text’s author towards an entity that occurs in the text as positive, negative, or neutral. We use the data from Dong et al. (2014). An example instance is the expression “how do you like settlers of catan for the wii?” which is labelled as neutral towards the target “wii’.’ The evaluation metric is macroaveraged F1 (F1M ). Fake news detection The goal of fake news detection in the context of the Fake News Challenge2 is to estimate whether the body of a news article agrees, disagrees, discusses, or is unrelated towards a headline. We use the data from the first stage of the Fake News Challenge (FNC-1). An example for this dataset is the document “Dino Ferrari hooked the whopper wels catfish, (...), which could be the bigg"
N18-1172,D16-1084,1,0.940888,"C in the hope it’ll make the electricity come back again Topic: AC/DC Label: positive Target-dependent sentiment analysis: Text: how do you like settlers of catan for the wii? Target: wii Label: neutral Aspect-based sentiment analysis: Text: For the price, you cannot eat this well in Manhattan Aspects: restaurant prices, food quality Label: positive Stance detection Stance detection (Stance) requires a model, given a text and a target entity, which might not appear in the text, to predict whether the author of the text is in favour or against the target or whether neither inference is likely (Augenstein et al., 2016). We use the data of SemEval-2016 Task 6 Subtask B (Mohammad et al., 2016). An example from this dataset would be to predict the stance of the tweet “Be prepared - if we continue the policies of the liberal left, we will be #Greece” towards the topic “Donald Trump”, labelled as “favor”. The evaluation metric is the macro-averaged F1 score of the “favour” and “against” classes (F1F A ). Stance detection: Tweet: Be prepared - if we continue the policies of the liberal left, we will be #Greece Target: Donald Trump Label: favor Fake news detection: Document: Dino Ferrari hooked the whopper wels ca"
N18-1172,W16-6208,1,0.846241,"we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabelled and auxiliary task"
N18-1172,S16-1010,0,0.0663037,"Missing"
N18-1172,D17-1169,1,0.779568,"and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabel"
N18-1172,D17-1206,0,0.0278696,"ers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels. This paper sets out to build an architecture in which such synergies are exploited, ? The first two authors contributed equally. with an application to pairwise sequence classification tasks. Doing so, we achieve a new state of the art on topic-based sentiment analysis. For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc. We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning. In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-la"
N18-1172,S16-1023,0,0.0416916,"Missing"
N18-1172,P17-1186,0,0.0246557,"rectly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using au"
N18-1172,P16-2067,1,0.828447,"f our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate la"
N18-1172,P15-1046,0,0.0715286,"with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces Problem definition In our multi-task learning scenario, we have access to labelled datasets for T tasks T1 , . . . , TT at training time with a target task TT that we particularly care about. The training dataset for task Ti consists of Nk examples XTi = {xT1 i , . . . , xTNik } and their Ti labels YTi = {y1Ti , . . . , yN }. Our base model is k a deep neural network that performs classic hard parameter"
N18-1172,S16-1174,0,0.0165112,"e often highly specialised, taskdependent architectures. Our architectures, in contrast, have not been optimised to compare favourably against the state of the art, as our main objective is to develop a novel approach to multi-task learning leveraging synergies between label sets and knowledge of marginal distributions from unlabeled data. For example, we do not use pre-trained word embeddings (Augenstein et al., 2016; Palogiannidi et al., 2016; Vo and Zhang, 2015), class weighting to deal with label imbalance (Balikas and Amini, 2016), or domainspecific sentiment lexicons (Brun et al., 2016; Kumar et al., 2016). Nevertheless, our approach outperforms the state-of-the-art on two-way topic-based sentiment analysis (Topic-2). The poor performance compared to the stateof-the-art on FNC and MultiNLI is expected; as we alternate among the tasks during training, our model only sees a comparatively small number of examples of both corpora, which are one and two orders of magnitude larger than the other datasets. For this reason, we do not achieve good performance on these tasks as main tasks, but they are still useful as auxiliary tasks as seen in Table 4. 6 6.1 Analysis Label Embeddings Our results above s"
N18-1172,P17-1001,0,0.0348797,"009), induce a shared prior (Yu et al., 2005; Xue et al., 2007; Daumé III, 2009), or learn a grouping (Kang et al., 2011; Kumar and Daumé III, 2012). These approaches focus on homogeneous tasks and employ linear or Bayesian models. They can thus not be directly applied to our setting with tasks using disparate label sets. Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017). These approaches, however, are not able to take into account relationships between labels that may aid in learning. Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017). In contrast, the different nature of our tasks requires a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closel"
N18-1172,S16-1003,0,0.11624,"Missing"
N18-1172,P17-1194,0,0.0446297,"a modelling of their label spaces. Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces P"
N18-1172,N13-1008,0,0.0450068,"t of task Ti . In practice, we apply the same weight to all tasks. We show the full set-up in Figure 1a. 3.2 Label Embedding Layer In order to learn the relationships between labels, we propose a Label Embedding Layer (LEL) that embeds the labels of all tasks in a joint space. Instead of training separate softmax output layers as above, we introduce a label compatibility function c(·, ·) that measures how similar a label with embedding l is to the hidden representation h: c(l, h) = l · h (3) where · is the dot product. This is similar to the Universal Schema Latent Feature Model introduced by Riedel et al. (2013). In contrast to 1897 12/6/2017 multi-task_learning.html 12/6/2017 1  1 = (p ,y 1 ) 2  2 = (p ,y 2 ) 3  3 = (p label_embedding_layer.html ,y 3 i  i = (p ) 1 ∈ ℝ 2 L1 p L2 ∈ ℝ 3 p i ) l i p ,y 12/6/2017 p L3 ∈ ℝ 1 1 l Li ∈ ℝ 2 3 l label_transfer_network.html i  i = (p 2 1 l l ∈ ℝ l 1 2 ,y i ) 3 l 1 1 l 2 i  pseudo = p Li ∈ ℝ 2 3 l 2 1 l l ∈ ℝ l 3 2 1 2 l T MSE(p ,y T oi−1 ∈ ℝ ) ∗ l Label Embedding Layer Label Embedding Layer oi ∈ ℝ l oi+1 ∈ ℝ [⋅, ⋅] Label Transfer Network h h ∈ ℝ h h h ∈ ℝ h ∈ ℝ z i y ∈ Y i x i ∈ X i i (a) Multi-task learning y ∈ Y i"
N18-1172,D16-1103,1,0.852079,"able 1, and summarise examples in Table 2: Topic-based sentiment analysis Topic-based sentiment analysis aims to estimate the sentiment of a tweet known to be about a given topic. We use the data from SemEval-2016 Task 4 Subtask B and C (Nakov et al., 2016) for predicting on a twopoint scale of positive and negative (Topic-2) and five-point scale ranging from highly negative to highly positive (Topic-5) respectively. An example from this dataset would be to classify the 1899 whether an aspect, i.e. a particular property of an item is associated with a positive, negative, or neutral sentiment (Ruder et al., 2016). We use the data of SemEval-2016 Task 5 Subtask 1 Slot 3 (Pontiki et al., 2016) for the laptops (ABSA-L) and restaurants (ABSA-R) domains. An example is the sentence “For the price, you cannot eat this well in Manhattan”, labelled as positive towards both the aspects “restaurant prices” and “food quality”. The evaluation metric for both domains is accuracy (Acc). Topic-based sentiment analysis: Tweet: No power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again Topic: AC/DC Label: positive Target-dependent sentiment analysis: Text: how do you lik"
N18-1172,D17-1038,1,0.926539,"onding label of the target task based on auxiliary predictions. To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks. The LTN can be used to label unlabelled and auxiliary task data by utilising the ‘dark knowledge’ (Hinton et al., 2015) contained in auxiliary model predictions. This pseudo-labelled data is then incorporated into the model via semisupervised learning, leading to a natural combination of multi-task learning and semi-supervised learning. We additionally augment the LTN with data-specific diversity features (Ruder and Plank, 2017) that aid in learning. Contributions Our contributions are: a) We model the relationships between labels by inducing a joint label space for multi-task learning. b) We propose a Label Transfer Network that learns to transfer labels between tasks and propose to use semi-supervised learning to leverage them for training. c) We evaluate MTL approaches on a variety of classification tasks and shed new light on settings where multi-task learning works. d) We perform an extensive ablation study of our model. 1896 Proceedings of NAACL-HLT 2018, pages 1896–1906 c New Orleans, Louisiana, June 1 - 6, 20"
N18-1172,P16-2038,1,0.857175,"eddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis. 1 Introduction Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP. Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017). Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016). If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000). However, while sharing hidden layers of neural networks is an effective regul"
N18-1172,D12-1125,0,0.0236666,"ereof, several of which have also been used in NLP. Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007). In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners. Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models. Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new. Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer. More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces. There has also been work on label transformations 3.1 Multi-task learning with disparate label spaces Problem definition In our multi-task learning scenario, we have access to labelled datasets for T tasks T1 , . . . , TT at training time with a target task TT that we particularly care about. The training dat"
N18-1172,W17-5301,0,\N,Missing
N18-5007,N12-1072,0,0.029534,"pinion. Given a topic, the tool aggregates relevant news articles from different sources and leverages recent advances in stance detection to lay them out on a spectrum ranging from support to opposition to the topic. Stance detection is the task of estimating whether the attitude expressed in a text towards a given topic is ‘in favour’, ‘against’, or ‘neutral’. We collected and annotated a novel dataset, which associates news articles with a stance towards a specified topic. We then trained a state-of-the-art 2 Related work Until recently, stance detection had been mostly studied in debates (Walker et al., 2012; Hasan and Ng, 2013) and student essays (Faulkner, 2014). Lately, research in stance detection focused on Twitter (Rajadesingan and Liu, 2014; Mohammad et al., 2016; Augenstein et al., 2016), particularly with regard to identifying rumors (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015). More recently, claims and headlines in news have been considered for stance detection (Ferreira and Vlachos, 2016), which require recognizing entailment relations between claim and article. 1 The demo can be accessed here: http://bit.do/ aylien-stance-detection-demo. A screencast of the demo"
N18-5007,D16-1084,0,0.0436373,"Missing"
N18-5007,P17-2067,0,0.0116917,"an unbiased, balanced opinion towards a topic. To ameliorate this, we propose 360° Stance Detection, a tool that aggregates news with multiple perspectives on a topic. It presents them on a spectrum ranging from support to opposition, enabling the user to base their opinion on multiple pieces of diverse evidence. 1 Introduction The growing epidemic of fake news in the wake of the election cycle for the 45th President of the United States has revealed the danger of staying within our filter bubbles. In light of this development, research in detecting false claims has received renewed interest (Wang, 2017). However, identifying and flagging false claims may not be the best solution, as putting a strong image, such as a red flag, next to an article may actually entrench deeply held beliefs (Lyons, 2017). A better alternative would be to provide additional evidence that will allow a user to evaluate multiple viewpoints and decide with which they agree. To this end, we propose 360° Stance Detection, a tool that provides a wide view of a topic from different perspectives to aid with forming a balanced opinion. Given a topic, the tool aggregates relevant news articles from different sources and leve"
N18-5007,N16-1138,0,0.0131994,"h associates news articles with a stance towards a specified topic. We then trained a state-of-the-art 2 Related work Until recently, stance detection had been mostly studied in debates (Walker et al., 2012; Hasan and Ng, 2013) and student essays (Faulkner, 2014). Lately, research in stance detection focused on Twitter (Rajadesingan and Liu, 2014; Mohammad et al., 2016; Augenstein et al., 2016), particularly with regard to identifying rumors (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015). More recently, claims and headlines in news have been considered for stance detection (Ferreira and Vlachos, 2016), which require recognizing entailment relations between claim and article. 1 The demo can be accessed here: http://bit.do/ aylien-stance-detection-demo. A screencast of the demo is available here: https://www.youtube. com/watch?v=WYckOr2NhFM. 31 Proceedings of NAACL-HLT 2018: Demonstrations, pages 31–35 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Figure 1: Interface provided to annotators. Annotation instructions are not shown. 3 Dataset 3.1 Topic type Task definition The objective of stance detection in our case is to classify the stance of an a"
N18-5007,I13-1191,0,0.0113532,", the tool aggregates relevant news articles from different sources and leverages recent advances in stance detection to lay them out on a spectrum ranging from support to opposition to the topic. Stance detection is the task of estimating whether the attitude expressed in a text towards a given topic is ‘in favour’, ‘against’, or ‘neutral’. We collected and annotated a novel dataset, which associates news articles with a stance towards a specified topic. We then trained a state-of-the-art 2 Related work Until recently, stance detection had been mostly studied in debates (Walker et al., 2012; Hasan and Ng, 2013) and student essays (Faulkner, 2014). Lately, research in stance detection focused on Twitter (Rajadesingan and Liu, 2014; Mohammad et al., 2016; Augenstein et al., 2016), particularly with regard to identifying rumors (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015). More recently, claims and headlines in news have been considered for stance detection (Ferreira and Vlachos, 2016), which require recognizing entailment relations between claim and article. 1 The demo can be accessed here: http://bit.do/ aylien-stance-detection-demo. A screencast of the demo is available here: ht"
N18-5007,D15-1311,0,0.012739,"pressed in a text towards a given topic is ‘in favour’, ‘against’, or ‘neutral’. We collected and annotated a novel dataset, which associates news articles with a stance towards a specified topic. We then trained a state-of-the-art 2 Related work Until recently, stance detection had been mostly studied in debates (Walker et al., 2012; Hasan and Ng, 2013) and student essays (Faulkner, 2014). Lately, research in stance detection focused on Twitter (Rajadesingan and Liu, 2014; Mohammad et al., 2016; Augenstein et al., 2016), particularly with regard to identifying rumors (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015). More recently, claims and headlines in news have been considered for stance detection (Ferreira and Vlachos, 2016), which require recognizing entailment relations between claim and article. 1 The demo can be accessed here: http://bit.do/ aylien-stance-detection-demo. A screencast of the demo is available here: https://www.youtube. com/watch?v=WYckOr2NhFM. 31 Proceedings of NAACL-HLT 2018: Demonstrations, pages 31–35 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Figure 1: Interface provided to annotators. Annotation instructions"
N18-5007,S16-1003,0,0.0796992,"Missing"
N18-5007,D14-1162,0,0.0906782,"Missing"
N18-5007,D11-1147,0,0.0375562,"whether the attitude expressed in a text towards a given topic is ‘in favour’, ‘against’, or ‘neutral’. We collected and annotated a novel dataset, which associates news articles with a stance towards a specified topic. We then trained a state-of-the-art 2 Related work Until recently, stance detection had been mostly studied in debates (Walker et al., 2012; Hasan and Ng, 2013) and student essays (Faulkner, 2014). Lately, research in stance detection focused on Twitter (Rajadesingan and Liu, 2014; Mohammad et al., 2016; Augenstein et al., 2016), particularly with regard to identifying rumors (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015). More recently, claims and headlines in news have been considered for stance detection (Ferreira and Vlachos, 2016), which require recognizing entailment relations between claim and article. 1 The demo can be accessed here: http://bit.do/ aylien-stance-detection-demo. A screencast of the demo is available here: https://www.youtube. com/watch?v=WYckOr2NhFM. 31 Proceedings of NAACL-HLT 2018: Demonstrations, pages 31–35 c New Orleans, Louisiana, June 2 - 4, 2018. 2018 Association for Computational Linguistics Figure 1: Interface provided to annotators. A"
N19-5004,J93-2003,0,0.0708895,"general representations are learned on a source task or domain followed by an adaptation phase during which the learned knowledge is applied to a target task or domain. Our discussion of the pretraining stage will review the main forms of pretraining methods commonly used today. We will try to provide attendants with an overview of what type of information these pretraining schemes are capturing and how pretraining schemes are devised. In particular, we will review unsupervised approaches which aim to model the dataset itself, briefly presenting non-neural approaches (Deerwester et al., 1990; Brown et al., 1993; Blei et al., 2003) before detailing deep neural network approaches like auto-encoding/skip-thoughts models (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Logeswaran and Lee, 2018) and the current trend of language model-based approaches (Dai and Le, 2015; Peters et al., 2018a; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). We will then describe supervised approaches which make use of large annotated datasets (Zoph et al., 2016; Yang et al., 2017; Wieting et al., 2016; Conneau et al., 2017; McCann et al., 2017) before turning to distant supervision approaches w"
N19-5004,K17-1029,0,0.0288122,"on top of the pre-trained to the insertion of intervening layers or modules inside the pre-trained model. 4. Break (20 minutes) 5. Adaptation (30 minutes): In this section, we will present several ways to adapt these representations, feature extraction and fine-tuning. We will discuss practical considerations such as learning rate schedules, architecture modifications, etc. Optimization schedules for the adaptation phase can involve fine-tuning a varying portion of the pre-trained model (Long et al., 2015; Felbo et al., 2017; Howard and Ruder, 2018) with specifically designed regularization (Wiese et al., 2017; Kirkpatrick et al., 2017) or even fine-tuning in sequence a model on a series of datasets using several training objectives. We will summarize current trends in adapting pre-trained model to target tasks while highlighting best practices when they can be identified. We will then focus on a selection of downstream applications such as classification (Howard and Ruder, 2018), natural language generation, structured prediction (Swayamdipta et al., 2018) or other classification tasks (Peters et al., 2018a; Devlin et al., 2018). This part will comprise handson examples designed around representat"
N19-5004,P17-1078,0,0.0263313,"pproaches which aim to model the dataset itself, briefly presenting non-neural approaches (Deerwester et al., 1990; Brown et al., 1993; Blei et al., 2003) before detailing deep neural network approaches like auto-encoding/skip-thoughts models (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Logeswaran and Lee, 2018) and the current trend of language model-based approaches (Dai and Le, 2015; Peters et al., 2018a; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). We will then describe supervised approaches which make use of large annotated datasets (Zoph et al., 2016; Yang et al., 2017; Wieting et al., 2016; Conneau et al., 2017; McCann et al., 2017) before turning to distant supervision approaches which use heuristics to automatically label datasets (Mintz et al., 2009; Severyn and Moschitti, 2015; Felbo et al., 2017; Yang et al., 2017). Our review of distant supervision approaches will aim to provide attendants with a sense of how they can design heuristics that can automatically provide supervision in their own applications. Last The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset"
N19-5004,W18-5448,0,0.0400894,"Missing"
N19-5004,D16-1163,0,0.0286616,"view unsupervised approaches which aim to model the dataset itself, briefly presenting non-neural approaches (Deerwester et al., 1990; Brown et al., 1993; Blei et al., 2003) before detailing deep neural network approaches like auto-encoding/skip-thoughts models (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Logeswaran and Lee, 2018) and the current trend of language model-based approaches (Dai and Le, 2015; Peters et al., 2018a; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). We will then describe supervised approaches which make use of large annotated datasets (Zoph et al., 2016; Yang et al., 2017; Wieting et al., 2016; Conneau et al., 2017; McCann et al., 2017) before turning to distant supervision approaches which use heuristics to automatically label datasets (Mintz et al., 2009; Severyn and Moschitti, 2015; Felbo et al., 2017; Yang et al., 2017). Our review of distant supervision approaches will aim to provide attendants with a sense of how they can design heuristics that can automatically provide supervision in their own applications. Last The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task usi"
N19-5004,P09-1113,0,0.077669,"work approaches like auto-encoding/skip-thoughts models (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Logeswaran and Lee, 2018) and the current trend of language model-based approaches (Dai and Le, 2015; Peters et al., 2018a; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). We will then describe supervised approaches which make use of large annotated datasets (Zoph et al., 2016; Yang et al., 2017; Wieting et al., 2016; Conneau et al., 2017; McCann et al., 2017) before turning to distant supervision approaches which use heuristics to automatically label datasets (Mintz et al., 2009; Severyn and Moschitti, 2015; Felbo et al., 2017; Yang et al., 2017). Our review of distant supervision approaches will aim to provide attendants with a sense of how they can design heuristics that can automatically provide supervision in their own applications. Last The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by"
N19-5004,N18-1202,1,0.904914,"try to provide attendants with an overview of what type of information these pretraining schemes are capturing and how pretraining schemes are devised. In particular, we will review unsupervised approaches which aim to model the dataset itself, briefly presenting non-neural approaches (Deerwester et al., 1990; Brown et al., 1993; Blei et al., 2003) before detailing deep neural network approaches like auto-encoding/skip-thoughts models (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Logeswaran and Lee, 2018) and the current trend of language model-based approaches (Dai and Le, 2015; Peters et al., 2018a; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). We will then describe supervised approaches which make use of large annotated datasets (Zoph et al., 2016; Yang et al., 2017; Wieting et al., 2016; Conneau et al., 2017; McCann et al., 2017) before turning to distant supervision approaches which use heuristics to automatically label datasets (Mintz et al., 2009; Severyn and Moschitti, 2015; Felbo et al., 2017; Yang et al., 2017). Our review of distant supervision approaches will aim to provide attendants with a sense of how they can design heuristics that can automatically"
N19-5004,D18-1179,1,0.913731,"try to provide attendants with an overview of what type of information these pretraining schemes are capturing and how pretraining schemes are devised. In particular, we will review unsupervised approaches which aim to model the dataset itself, briefly presenting non-neural approaches (Deerwester et al., 1990; Brown et al., 1993; Blei et al., 2003) before detailing deep neural network approaches like auto-encoding/skip-thoughts models (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Logeswaran and Lee, 2018) and the current trend of language model-based approaches (Dai and Le, 2015; Peters et al., 2018a; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). We will then describe supervised approaches which make use of large annotated datasets (Zoph et al., 2016; Yang et al., 2017; Wieting et al., 2016; Conneau et al., 2017; McCann et al., 2017) before turning to distant supervision approaches which use heuristics to automatically label datasets (Mintz et al., 2009; Severyn and Moschitti, 2015; Felbo et al., 2017; Yang et al., 2017). Our review of distant supervision approaches will aim to provide attendants with a sense of how they can design heuristics that can automatically"
N19-5004,S15-2079,0,0.0698163,"Missing"
N19-5004,D18-1412,1,0.829017,"a varying portion of the pre-trained model (Long et al., 2015; Felbo et al., 2017; Howard and Ruder, 2018) with specifically designed regularization (Wiese et al., 2017; Kirkpatrick et al., 2017) or even fine-tuning in sequence a model on a series of datasets using several training objectives. We will summarize current trends in adapting pre-trained model to target tasks while highlighting best practices when they can be identified. We will then focus on a selection of downstream applications such as classification (Howard and Ruder, 2018), natural language generation, structured prediction (Swayamdipta et al., 2018) or other classification tasks (Peters et al., 2018a; Devlin et al., 2018). This part will comprise handson examples designed around representative tasks and typical transfer learning schemes as detailed before. We will aim to demonstrate through practical examples how NLP researchers and practitioners can adapt these models to their own applications and provide them with a set of guidelines for practical usage. Finally, we will present open problems, challenges, and directions in transfer learning for NLP. 6. Down-stream applications (40 minutes): In this section, we will highlight how pretra"
P18-1031,D17-1169,0,0.474088,"ackward LM. We fine-tune a classifier for each LM independently using BPT3C and average the classifier predictions. Gradual unfreezing Rather than fine-tuning all layers at once, which risks catastrophic forgetting, we propose to gradually unfreeze the model starting from the last layer as this contains the least general knowledge (Yosinski et al., 2014): We first unfreeze the last layer and fine-tune all unfrozen layers for one epoch. We then unfreeze the next lower frozen layer and repeat, until we finetune all layers until convergence at the last iteration. This is similar to ‘chain-thaw’ (Felbo et al., 2017), except that we add a layer at a time to the set of ‘thawed’ layers, rather than only training a single layer at a time. While discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing all are beneficial on their own, we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets. 4 Experiments While our approach is equally applicable to sequence labeling tasks, we focus on text classification tasks in this work due to their important realworld applications. 4.1 Experimental setup Datasets and tasks We evaluate ou"
P18-1031,N18-1108,0,0.018264,"of the last layers of a pretrained model and leaving the remaining layers frozen (Long et al., 2015a). 3 Universal Language Model Fine-tuning We are interested in the most general inductive transfer learning setting for NLP (Pan and Yang, 2010): Given a static source task TS and any target task TT with TS 6= TT , we would like to improve performance on TT . Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies (Linzen et al., 2016), hierarchical relations (Gulordava et al., 2018), and sentiment (Radford et al., 2017). In contrast to tasks like MT (McCann et al., 2017) and entailment (Conneau et al., 2017), it provides data in near-unlimited quantities for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target Hypercolumns In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddin"
P18-1031,D17-1070,0,0.0470469,"Fine-tuning We are interested in the most general inductive transfer learning setting for NLP (Pan and Yang, 2010): Given a static source task TS and any target task TT with TS 6= TT , we would like to improve performance on TT . Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies (Linzen et al., 2016), hierarchical relations (Gulordava et al., 2018), and sentiment (Radford et al., 2017). In contrast to tasks like MT (McCann et al., 2017) and entailment (Conneau et al., 2017), it provides data in near-unlimited quantities for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target Hypercolumns In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns (Hariharan et al., 2015) in CV2 and is used b"
P18-1031,P17-1052,0,0.0318635,"rates, and gradual unfreezing all are beneficial on their own, we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets. 4 Experiments While our approach is equally applicable to sequence labeling tasks, we focus on text classification tasks in this work due to their important realworld applications. 4.1 Experimental setup Datasets and tasks We evaluate our method on six widely-studied datasets, with varying numbers of documents and varying document length, used by state-of-the-art text classification and transfer learning approaches (Johnson and Zhang, 2017; McCann et al., 2017) as instances of three common text classification tasks: sentiment analysis, question classification, and topic classification. We show the statistics for each dataset and task in Table 1. BPTT for Text Classification (BPT3C) Language models are trained with backpropagation through time (BPTT) to enable gradient propagation for large input sequences. In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixedlength batches of size b. At the beginning of each batch, the model is"
P18-1031,Q16-1037,0,0.234131,"her the last (Donahue et al., 2014) or several of the last layers of a pretrained model and leaving the remaining layers frozen (Long et al., 2015a). 3 Universal Language Model Fine-tuning We are interested in the most general inductive transfer learning setting for NLP (Pan and Yang, 2010): Given a static source task TS and any target task TT with TS 6= TT , we would like to improve performance on TT . Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets of language relevant for downstream tasks, such as long-term dependencies (Linzen et al., 2016), hierarchical relations (Gulordava et al., 2018), and sentiment (Radford et al., 2017). In contrast to tasks like MT (McCann et al., 2017) and entailment (Conneau et al., 2017), it provides data in near-unlimited quantities for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target Hypercolumns In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as fea"
P18-1031,D15-1279,0,0.0472913,"Missing"
P18-1031,P11-1015,0,0.0927716,"Missing"
P18-1031,P17-1161,0,0.410878,".io Abstract While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Research in NLP focused mostly on transductive transfer (Blitzer et al., 2007). For inductive transfer, fine-tuning pretrained word embeddings (Mikolov et al., 2013), a simple transfer technique that only targets a model’s first layer, has had a large impact in practice and is used in most state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness. In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability"
P18-1031,N18-1202,0,0.874481,"ve achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Research in NLP focused mostly on transductive transfer (Blitzer et al., 2007). For inductive transfer, fine-tuning pretrained word embeddings (Mikolov et al., 2013), a simple transfer technique that only targets a model’s first layer, has had a large impact in practice and is used in most state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness. In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability. We show that not the idea of LM fine-tuni"
P18-1031,P17-1194,0,0.0672569,"Missing"
P18-1031,P16-1009,0,0.0771527,"Missing"
P18-1031,P17-2081,0,0.0411984,"rained models and our code available to enable wider adoption. 2 Multi-task learning A related direction is multi-task learning (MTL) (Caruana, 1993). This is the approach taken by Rei (2017) and Liu et al. (2018) who add a language modeling objective to the model that is trained jointly with the main task model. MTL requires the tasks to be trained from scratch every time, which makes it inefficient and often requires careful weighting of the taskspecific objective functions (Chen et al., 2017). Fine-tuning Fine-tuning has been used successfully to transfer between similar tasks, e.g. in QA (Min et al., 2017), for distantly supervised sentiment analysis (Severyn and Moschitti, 2015), or MT domains (Sennrich et al., 2015) but has been shown to fail between unrelated ones (Mou et al., 2016). Dai and Le (2015) also fine-tune a language model, but overfit with 10k labeled examples and require millions of in-domain documents for good performance. In contrast, ULMFiT leverages general-domain pretraining and novel finetuning techniques to prevent overfitting even with only 100 labeled examples and achieves state-ofthe-art results also on small datasets. Related work Transfer learning in CV Features in de"
P18-1031,S15-2079,0,0.209689,"is used in most state-of-the-art models. Recent approaches that concatenate embeddings derived from other tasks with the input at different layers (Peters et al., 2017; McCann et al., 2017; Peters et al., 2018) still train the main task model from scratch and treat pretrained embeddings as fixed parameters, limiting their usefulness. In light of the benefits of pretraining (Erhan et al., 2010), we should be able to do better than randomly initializing the remaining parameters of our models. However, inductive transfer via finetuning has been unsuccessful for NLP (Mou et al., 2016). Dai and Le (2015) first proposed finetuning a language model (LM) but require millions of in-domain documents to achieve good performance, which severely limits its applicability. We show that not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow and thus require different fine-tuning methods. We propose a new method, Universal Language Model Fine-tuning (ULMFiT) that addresses these iss"
P18-1031,P17-1190,0,0.0211073,"s for most domains and languages. Additionally, a pretrained LM can be easily adapted to the idiosyncrasies of a target Hypercolumns In NLP, only recently have methods been proposed that go beyond transferring word embeddings. The prevailing approach is to pretrain embeddings that capture additional context via other tasks. Embeddings at different levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known as hypercolumns (Hariharan et al., 2015) in CV2 and is used by Peters et al. (2017), Peters et al. (2018), Wieting and Gimpel (2017), Conneau 2 A hypercolumn at a pixel in CV is the vector of activations of all CNN units above that pixel. In analogy, a hypercolumn for a word or sentence in NLP is the concatenation of embeddings at different layers in a pretrained model. 329 Softmax layer Softmax layer Softmax layer Layer 3 Layer 3 Layer 2 Layer 2 Layer 2 Layer 1 Layer 1 Layer 1 Embedding Embedding Embedding Layer 3 layer The gold layer dollar (a) LM pre-training or gold layer The best scene ever (b) LM fine-tuning The best scene ever (c) Classifier fine-tuning Figure 1: ULMFiT consists of three stages: a) The LM is trained"
P18-1031,C16-1329,0,0.0610184,"Missing"
P18-1072,P17-1042,0,0.677938,"form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify circumstances under which the unsupervised bilingual 1 Our motivation for this"
P18-1072,D18-1549,0,0.0413667,"ion for this is that dimensionality reduction will alter the geometric shape and remove characteristics of the embedding graphs that are important for alignment; but on the other hand, lower dimensionality introduces regularization, which will make the graphs more similar. Finally, in §4.6, we investigate the impact of different types of query test words on performance, including how performance varies across part-of-speech word classes and on shared vocabulary items. Learning scenarios Unsupervised neural machine translation relies on BDI using cross-lingual embeddings (Lample et al., 2018a; Artetxe et al., 2018), which in turn relies on the assumption that word embedding graphs are approximately isomorphic. The work of Conneau et al. (2018), which we focus on here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries. We"
P18-1072,W16-1614,0,0.386638,"l Dictionary Induction Anders Søgaard♥ Sebastian Ruder♠♣ Ivan Vuli´c3 ♥ University of Copenhagen, Copenhagen, Denmark ♠ Insight Research Centre, National University of Ireland, Galway, Ireland ♣ Aylien Ltd., Dublin, Ireland 3 Language Technology Lab, University of Cambridge, UK soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk Abstract of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017). Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a), and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018). In the words of Barone (2016): Unsupervised machine translation—i.e., not assuming any cross-lingual supervision signal, whether a dictionary,"
P18-1072,P07-1056,0,0.0348626,"ES when the two monolingual embedding spaces are induced by two different algorithms (see the results of the entire Spanish cbow column).9 In sum, this means that the unsupervised approach is unlikely to work on pre-trained word embeddings unless they are induced on same6 One exception here is French, which they include in their paper, but French arguably has a relatively simple morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar. 9 We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman’s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017). 783 EP 0.75 Wiki EMEA Wiki EMEA 0.60 0.55 0.65 0.60 0.55 0.50 EN:EP (a) en-es: domain similarity EMEA 0.65 0.60 0.55 0.50 EN:Wiki EN:EMEA Training Corpus (English) Wiki 0.70 Jensen-Shannon Similarity 0.65 EP"
P18-1072,Q17-1010,0,0.798117,"here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries. We focus on the work of Conneau et al. (2018), who present a fully unsupervised approach to aligning monolingual word embeddings, induced using fastText (Bojanowski et al., 2017). We describe the learning algorithm in §3.2. Conneau et al. (2018) consider a specific set of learning scenarios: 3.2 Summary of Conneau et al. (2018) We now introduce the method of Conneau et al. (2018).4 The approach builds on existing work on learning a mapping between monolingual word embeddings (Mikolov et al., 2013b; Xing et al., 2015) and consists of the following steps: 1) Monolingual word embeddings: An off-the-shelf word embedding algorithm (Bojanowski et al., 2017) is used to learn source and target language spaces X (a) The authors work with the following languages: English-{Frenc"
P18-1072,C12-1089,0,0.251885,"isomorphic between languages, and that this isomorphism can be learned from the statistics of the realizations of these processes, the monolingual corpora, in principle without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for q"
P18-1072,2005.mtsummit-papers.11,0,0.0562212,"Missing"
P18-1072,S17-2002,0,0.0503011,"morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar. 9 We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman’s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017). 783 EP 0.75 Wiki EMEA Wiki EMEA 0.60 0.55 0.65 0.60 0.55 0.50 EN:EP (a) en-es: domain similarity EMEA 0.65 0.60 0.55 0.50 EN:Wiki EN:EMEA Training Corpus (English) Wiki 0.70 Jensen-Shannon Similarity 0.65 EP 0.75 0.70 Jensen-Shannon Similarity 0.70 Jensen-Shannon Similarity EP 0.75 EN:EP 0.50 EN:Wiki EN:EMEA Training Corpus (English) (b) en-fi: domain similarity EN:EP EN:Wiki EN:EMEA Training Corpus (English) (c) en-hu: domain similarity 64.09 50 49.24 46.52 30 25.17 BLI: P@1 BLI: P@1 40 25.48 20 60 60 50 50 40 40 BLI: P@1 60 30 28.63 20 30 26.99 20 15.56 14.79 0 EN:EP 9.63 10 6.63 4.84 0 EN"
P18-1072,J82-2005,0,0.817708,"Missing"
P18-1072,E17-1072,1,0.939289,"nciple without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify circumstances under which the unsupervised bilingual 1 O"
P18-1072,D16-1235,1,0.901982,"Missing"
P18-1072,K15-1026,0,0.0281285,"models are evaluated on a held-out set of query words. Here, we analyze the performance of the unsupervised approach across different parts-ofspeech, frequency bins, and with respect to query words that have orthographically identical counterparts in the target language with the same or a different meaning. Part-of-speech We show the impact of the partof-speech of the query words in Table 4; again on a representative subset of our languages. The results indicate that performance on verbs is lowest across the board. This is consistent with research on distributional semantics and verb meaning (Schwartz et al., 2015; Gerz et al., 2016). Frequency We also investigate the impact of the frequency of query words. We calculate the word frequency of English words based on Google’s Trillion Word Corpus: query words are divided in groups based on their rank – i.e., the first group contains the top 100 most frequent words, the second one contains the 101th-1000th most frequent words, etc. – and plot performance (P@1) relative to rank in Figure 3. For EN - FI, P@1 was 0 across all frequency ranks. The plot shows sensitivity to frequency for HU, but less so for ES. Homographs Since we use identical word forms (homo"
P18-1072,P08-1088,0,0.122331,"arity is strong (ρ ∼ 0.89). 5 Related work Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018), require aligned words, sentences, or documents (Levy et al., 2017). Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015). Recent approaches try to minimize the amount of supervision needed (Vuli´c and Korhonen, 2016; Artetxe et al., 2017; Smith et al., 2017). See Upadhyay et al. (2016) and Ruder et al. (2018) for surveys. Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017), in addition, use different forms of regularization for convergence, while Conneau et al. (2018) uses additional steps to refine the induced embedding space. 6 Conclusion We investigated when u"
P18-1072,P16-1157,0,0.0607205,"value in the half-open interval [0, ∞). The correlation between BDI performance and graph similarity is strong (ρ ∼ 0.89). 5 Related work Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018), require aligned words, sentences, or documents (Levy et al., 2017). Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015). Recent approaches try to minimize the amount of supervision needed (Vuli´c and Korhonen, 2016; Artetxe et al., 2017; Smith et al., 2017). See Upadhyay et al. (2016) and Ruder et al. (2018) for surveys. Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017), in addition, use different forms of regularization for convergence, while Conneau et al. (2018)"
P18-1072,P16-1024,1,0.899531,"Missing"
P18-1072,D13-1168,1,0.909528,"Missing"
P18-1072,N15-1104,0,0.328041,"Missing"
P18-1072,P17-1179,0,0.413596,"nduction Anders Søgaard♥ Sebastian Ruder♠♣ Ivan Vuli´c3 ♥ University of Copenhagen, Copenhagen, Denmark ♠ Insight Research Centre, National University of Ireland, Galway, Ireland ♣ Aylien Ltd., Dublin, Ireland 3 Language Technology Lab, University of Cambridge, UK soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk Abstract of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017). Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a), and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018). In the words of Barone (2016): Unsupervised machine translation—i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or com"
P18-1072,W13-3520,0,\N,Missing
P18-1096,P07-1056,0,0.582094,"ritraining with disagreement is only slightly better than self-training, showing that the disagreement component might not be useful when there is a strong domain shift. Tri-training achieves the best average results on two target domains and clearly outperforms the state of the art on average. MT-Tri finally outperforms the state of the art on 3/4 domains, and even slightly traditional tritraining, resulting in the overall best method. This improvement is mainly due to the B-&gt;E and D-&gt;E scenarios, on which tri-training struggles. These domain pairs are among those with the highest Adistance (Blitzer et al., 2007), which highlights that tri-training has difficulty dealing with a strong shift in domain. Our method is able to mitigate this deficiency by training one of the three output layers only on pseudo-labeled target domain examples. In addition, MT-Tri is more efficient as it adds a smaller number of pseudo-labeled examples than tri-training at every epoch. For sentiment analysis, tri-training adds around 1800-1950/2000 unlabeled examples at every epoch, while MT-Tri only adds around 100-300 in early epochs. This shows that the orthogonality constraint is useful for inducing diversity. In addition,"
P18-1096,W06-1615,0,0.897186,"eural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such"
P18-1096,D15-1085,0,0.0373485,"Missing"
P18-1096,P13-1004,0,0.027757,"the line between “explicit” and “implicit” ensembling (Huang et al., 2017), like dropout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling. Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders. 5 Conclusions We re-evaluate a range of tr"
P18-1096,W17-3203,0,0.0455565,"Missing"
P18-1096,K16-1018,0,0.0189065,"opout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling. Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders. 5 Conclusions We re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural networ"
P18-1096,P17-2093,0,0.0213515,"redictions. In contrast, if the parameters in all output softmax layers were the same, the method would degenerate to self-training. To guarantee diversity, we introduce an orthogonality constraint (Bousmalis et al., 2016) as an additional loss term, which we define as follows: &gt; Lorth = kWm Wm2 k2F 1 (1) where |· k2F is the squared Frobenius norm and Wm1 and Wm2 are the softmax output parameters 2 Note: we use the term multi-task learning here albeit all tasks are of the same kind, similar to work on multi-lingual modeling treating each language (but same label space) as separate task e.g., (Fang and Cohn, 2017). It is interesting to point out that our model is further doing implicit multi-view learning by way of the orthogonality constraint. 1046 of the two source and pseudo-labeled output layers m1 and m2 , respectively. The orthogonality constraint encourages the models not to rely on the same features for prediction. As enforcing pairwise orthogonality between three matrices is not possible, we only enforce orthogonality between the softmax output layers of m1 and m2 ,3 while m3 is gradually trained to be more target-specific. We parameterize Lorth by γ=0.01 following (Liu et al., 2017). We do no"
P18-1096,W17-4404,1,0.881914,"Missing"
P18-1096,P17-1044,0,0.0265255,"a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline. 1 Introduction Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single be"
P18-1096,P18-1031,1,0.801555,"udo-labeled target domain examples. In addition, MT-Tri is more efficient as it adds a smaller number of pseudo-labeled examples than tri-training at every epoch. For sentiment analysis, tri-training adds around 1800-1950/2000 unlabeled examples at every epoch, while MT-Tri only adds around 100-300 in early epochs. This shows that the orthogonality constraint is useful for inducing diversity. In addition, adding fewer examples poses a smaller risk of swamping the learned representations with useless signals and is more akin to fine-tuning, the standard method for supervised domain adaptation (Howard and Ruder, 2018). We observe an asymmetry in the results between some of the domain pairs, e.g. B-&gt;D and D-&gt;B. We hypothesize that the asymmetry may be due to properties of the data and that the domains are relatively far apart e.g., in terms of A-distance. In fact, asymmetry in these domains is already reflected 1049 Model Src (+glove) Self Tri Tri-D Asym MT-Tri FLORS Target domains Newsgroups Reviews ep Answers Emails Weblogs Avg WSJ (5) (4) (7) (3) (4) 87.63 ±.37 87.64 ±.18 88.42 ±.16 88.50 ±.04 87.81 ±.19 87.92 ±.18 86.49 ±.35 86.58 ±.30 87.46 ±.20 87.63 ±.15 86.97 ±.17 87.20 ±.23 88.60 ±.22 88.42 ±.24 87"
P18-1096,P07-1034,0,0.128543,"method works well on unknown word-tag combinations. UWT tokens are very difficult to predict correctly using an unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains. 4 Related work Learning under Domain Shift There is a large body of work on domain adaptation. Studies on unsupervised domain adaptation include early work on bootstrapping (Steedman et al., 2003; McClosky et al., 2006a), shared feature representations (Blitzer et al., 2006, 2007) and instance weighting (Jiang and Zhai, 2007). Recent ap1051 proaches include adversarial learning (Ganin et al., 2016) and fine-tuning (Sennrich et al., 2016). There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift. Tri-training is less studied, and only recently re-emerged in the vision community (Saito et al., 2017), albeit is not compared to classic tri-training. Neural network ensembling Related work on self-ensembling approaches includes snapshot ensembling (Huang et al., 2017) or temporal ensembling (Laine and Aila, 2017). In general, the line between “explicit” and “implicit” e"
P18-1096,P17-1119,0,0.197404,"tic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. The"
P18-1096,Q16-1023,0,0.0145505,"g some languagespecific morphological features). We want to gauge to what extent we can adopt a nowadays fairly standard (but more lexicalized) general neural tagger. Our POS tagging model is a state-of-the-art Bi-LSTM tagger (Plank et al., 2016) with word and 100-dim character embeddings. Word embeddings are initialized with the 100-dim Glove embeddings (Pennington et al., 2014). The BiLSTM has one hidden layer with 100 dimensions. The base POS model is trained on WSJ with early stopping on the WSJ development set, using patience 2, Gaussian noise with σ = 0.2 and word dropout with p = 0.25 (Kiperwasser and Goldberg, 2016). Regarding data, the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal (WSJ) annotated for 48 fine-grained POS tags. This amounts to 30,060 labeled sentences. We use 100,000 WSJ sentences from 1988 as unlabeled data, following Schnabel and Schütze (2014).5 As target data, we use the five SANCL domains (answers, emails, newsgroups, reviews, weblogs). We restrict the amount of unlabeled data for each SANCL domain to the first 100k sentences, and do not do any pre-processing. We consider the development set of A NSWERS as our only target dev set to set hyperpara"
P18-1096,N16-1030,0,0.0574276,"are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline. 1 Introduction Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary"
P18-1096,P17-1001,0,0.136431,"e.g., (Fang and Cohn, 2017). It is interesting to point out that our model is further doing implicit multi-view learning by way of the orthogonality constraint. 1046 of the two source and pseudo-labeled output layers m1 and m2 , respectively. The orthogonality constraint encourages the models not to rely on the same features for prediction. As enforcing pairwise orthogonality between three matrices is not possible, we only enforce orthogonality between the softmax output layers of m1 and m2 ,3 while m3 is gradually trained to be more target-specific. We parameterize Lorth by γ=0.01 following (Liu et al., 2017). We do not further tune γ. More formally, let us illustrate the model by taking the sequence prediction task (Figure 1) as illustration. Given an utterance with labels y1 , .., yn , our Multi-task Tri-training loss consists of three task-specific (m1 , m2 , m3 ) tagging loss functions (where ~h is the uppermost Bi-LSTM encoding): XX L(θ) = − log Pmi (y|~h) + γLorth (2) Algorithm 3 Multi-task Tri-training 1: 2: 3: 4: 5: 6: 7: m ← train_model(L) repeat for i ∈ {1..3} do Li ← ∅ for x ∈ U do if pj (x) = pk (x)(j, k 6= i) then Li ← Li ∪ {(x, pj (x))} if i = 3 then mi = train_model(Li ) elsemi ← tr"
P18-1096,N06-1020,0,0.877312,"gorithms compared to state-of-the-art approaches on two benchmark datasets. d) We shed light on the task and data characteristics that yield the best performance for each model. 2 Neural bootstrapping methods We first introduce three classic bootstrapping methods, self-training, tri-training, and tri-training with disagreement and detail how they can be used with neural networks. For in-depth details we refer the reader to (Abney, 2007; Chapelle et al., 2006; Zhu and Goldberg, 2009). We introduce our novel multitask tri-training method in §2.3. 2.1 Self-training Self-training (Yarowsky, 1995; McClosky et al., 2006b) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model’s own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next. Self-training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration, the model provides predictions m(x) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predeter"
P18-1096,P06-1043,0,0.82215,"gorithms compared to state-of-the-art approaches on two benchmark datasets. d) We shed light on the task and data characteristics that yield the best performance for each model. 2 Neural bootstrapping methods We first introduce three classic bootstrapping methods, self-training, tri-training, and tri-training with disagreement and detail how they can be used with neural networks. For in-depth details we refer the reader to (Abney, 2007; Chapelle et al., 2006; Zhu and Goldberg, 2009). We introduce our novel multitask tri-training method in §2.3. 2.1 Self-training Self-training (Yarowsky, 1995; McClosky et al., 2006b) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model’s own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next. Self-training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration, the model provides predictions m(x) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predeter"
P18-1096,D14-1162,0,0.0860106,"Missing"
P18-1096,P16-2067,1,0.93714,"ov and McDonald, 2012) and compare to the top results in both low and high-data conditions (Schnabel and Schütze, 2014; Yin et al., 2015). Both are strong baselines, as the FLORS tagger has been developed for this challenging dataset and it is based on contextual distributional features (excluding the word’s identity), and hand-crafted suffix and shape features (including some languagespecific morphological features). We want to gauge to what extent we can adopt a nowadays fairly standard (but more lexicalized) general neural tagger. Our POS tagging model is a state-of-the-art Bi-LSTM tagger (Plank et al., 2016) with word and 100-dim character embeddings. Word embeddings are initialized with the 100-dim Glove embeddings (Pennington et al., 2014). The BiLSTM has one hidden layer with 100 dimensions. The base POS model is trained on WSJ with early stopping on the WSJ development set, using patience 2, Gaussian noise with σ = 0.2 and word dropout with p = 0.25 (Kiperwasser and Goldberg, 2016). Regarding data, the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal (WSJ) annotated for 48 fine-grained POS tags. This amounts to 30,060 labeled sentences. We use 100,000 WSJ se"
P18-1096,P07-1078,0,0.152605,"the highest confidence after every epoch and add them to the labeled data. This is one of the many variants for self-training, called throttling (Abney, 2007). We empirically confirm that this outperforms the classic selection in our experiments. Online learning In contrast to many classic algorithms, DNNs are trained online by default. We compare training setups and find that training until convergence on labeled data and then training until convergence using self-training performs best. Classic self-training has shown mixed success. In parsing it proved successful only with small datasets (Reichart and Rappoport, 2007) or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006b; Suzuki and Isozaki, 2008). Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012), while others report limited success on a variety of NLP tasks (Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017). Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift. 2.2 Tri-training Tri-training (Zhou and Li, 2005) is a classic method that red"
P18-1096,D17-1035,0,0.0192184,"utoencoder (VFAE) (Louizos et al., 2015) model and domain-adversarial neural networks (DANN) (Ganin et al., 2016). 3.3 Baselines Besides comparing to the top results published on both datasets, we include the following baselines: a) b) c) d) e) the task model trained on the source domain; self-training (Self); tri-training (Tri); tri-training with disagreement (Tri-D); and asymmetric tri-training (Saito et al., 2017). Our proposed model is multi-task tri-training (MTTri). We implement our models in DyNet (Neubig et al., 2017). Reporting single evaluation scores might result in biased results (Reimers and Gurevych, 2017). Throughout the paper, we report mean accuracy and standard deviation over five runs for POS tagging and over ten runs for 5 Note that our unlabeled data might slightly differ from theirs. We took the first 100k sentences from the 1988 WSJ dataset from the BLLIP 1987-89 WSJ Corpus Release 1. 1048 Figure 2: Average results for unsupervised domain adaptation on the Amazon dataset. Domains: B (Book), D (DVD), E (Electronics), K (Kitchen). Results for VFAE, DANN, and Asym are from Saito et al. (2017). sentiment analysis. Significance is computed using bootstrap test. The code for all experiments"
P18-1096,Q14-1002,0,0.134351,"ieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts hi"
P18-1096,P16-1009,0,0.0353828,"unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains. 4 Related work Learning under Domain Shift There is a large body of work on domain adaptation. Studies on unsupervised domain adaptation include early work on bootstrapping (Steedman et al., 2003; McClosky et al., 2006a), shared feature representations (Blitzer et al., 2006, 2007) and instance weighting (Jiang and Zhai, 2007). Recent ap1051 proaches include adversarial learning (Ganin et al., 2016) and fine-tuning (Sennrich et al., 2016). There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift. Tri-training is less studied, and only recently re-emerged in the vision community (Saito et al., 2017), albeit is not compared to classic tri-training. Neural network ensembling Related work on self-ensembling approaches includes snapshot ensembling (Huang et al., 2017) or temporal ensembling (Laine and Aila, 2017). In general, the line between “explicit” and “implicit” ensembling (Huang et al., 2017), like dropout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017)"
P18-1096,P10-2038,0,0.80088,"Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily—with a few additions—with the current generation of NLP models. Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored. In particular, we re-evaluate three traditional bootstrapping methods, self-training (Yarowsky, 1995), tri-training (Zhou and Li, 2005), and tritraining with disagreement (Søgaard, 2010) for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis). We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature. We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning (Laine and Aila, 2017) and recent neural adaptation approaches (Ganin et al., 2016; Sai"
P18-1096,P16-2038,0,0.0366655,"al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling. Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders. 5 Conclusions We re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi-supervi"
P18-1096,N03-1031,0,0.400547,"Missing"
P18-1096,P08-1076,0,0.036247,"ng (Abney, 2007). We empirically confirm that this outperforms the classic selection in our experiments. Online learning In contrast to many classic algorithms, DNNs are trained online by default. We compare training setups and find that training until convergence on labeled data and then training until convergence using self-training performs best. Classic self-training has shown mixed success. In parsing it proved successful only with small datasets (Reichart and Rappoport, 2007) or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006b; Suzuki and Isozaki, 2008). Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012), while others report limited success on a variety of NLP tasks (Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017). Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift. 2.2 Tri-training Tri-training (Zhou and Li, 2005) is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. Tri-training (cf. Algorith"
P18-1096,P16-1029,0,0.0743195,"excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make stron"
P18-1096,P95-1026,0,0.95539,"s highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily—with a few additions—with the current generation of NLP models. Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored. In particular, we re-evaluate three traditional bootstrapping methods, self-training (Yarowsky, 1995), tri-training (Zhou and Li, 2005), and tritraining with disagreement (Søgaard, 2010) for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis). We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature. We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning (L"
P18-1096,D15-1155,0,0.26012,"wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usef"
P18-1096,P16-1031,0,0.0194994,"rast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algori"
P19-1070,D18-1214,0,0.335056,"Missing"
P19-1070,P17-1042,0,0.649125,"olingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models, not demanding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of"
P19-1070,P18-1073,0,0.0782391,"irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision ("
P19-1070,J82-2005,0,0.676033,"Missing"
P19-1070,D18-1024,0,0.240154,"r alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom"
P19-1070,D18-1269,0,0.132185,"m seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models, not demanding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et"
P19-1070,D18-1062,0,0.0575484,"Missing"
P19-1070,D18-1027,0,0.560132,"ding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well o"
P19-1070,P18-1128,0,0.0627467,"Missing"
P19-1070,E14-1049,0,0.141575,"S and XT . In the general case, we learn two projection matrices WL1 and WL2 : XCL = XL1 WL1 ∪ XL2 WL2 . Many models, however, learn to directly project XL1 to XL2 , i.e., WL2 = I and XCL = XL1 WL1 ∪ XL2 . 2.2 Projection-Based CLE Models While supervised models employ external dictionaries, unsupervised models automatically induce seed translations using diverse strategies: adversarial learning (Conneau et al., 2018a), similaritybased heuristics (Artetxe et al., 2018b), PCA (Hoshen and Wolf, 2018), and optimal transport (Alvarez-Melis and Jaakkola, 2018). Canonical Correlation Analysis (CCA). Faruqui and Dyer (2014) use CCA to project XL1 and XL2 obtain monolingual vectors) and b) they do not require any multilingual corpora, they lend themselves to a wider spectrum of languages than the alternatives (Ruder et al., 2018b). XL1 , XL2 ← monolingual embeddings of L1 and L2 D ← initial word translation dictionary for each of n iterations do XS , XT ← lookups for D in XL1 , XL2 WL1 ← arg minW kXS W − XT k2 WL2 ← arg minW kXT W − XS k2 X0 L1 ← XL1 WL1 ; X0 L2 ← XL2 WL2 D1,2 ← nn(X0 L1 , XL2 ); D2,1 ← nn(X0 L2 , XL1 ) D ← D ∪ (D1,2 ∩ D2,1 ) return: WL1 (and/or WL2 ) into a shared space XCL . Projection matrices"
P19-1070,S18-2010,0,0.0429158,"of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream tasks like text classification, a large body of recent work is judged exclusively on the task of bilingual lexicon induction (BLI). This limits our understanding of CLE methodology as: 1) BLI is an intrinsic task, and agreement between BLI and downstream performance has been challenged (Ammar et al., 2016; Bakarov et al., 2018); 2) BLI is not the main motivation for inducing cross-lingual embedding spaces— rather, we seek to exploit CLEs to tackle multilinguality and downstream language transfer (Ruder et al., 2018b). In other words, previous research does not evaluate the true capacity of projectionbased CLE models to support cross-lingual NLP. It is unclear whether and to which extent BLI performance of (projection-based) CLE models correlates with various downstream tasks of different types. At the moment, it is virtually impossible to directly compare all recent projection-based CLE models on BLI due to the lack"
P19-1070,P15-1119,0,0.0573364,"em in a shared cross-lingual word vector space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from com"
P19-1070,Q17-1010,0,0.431645,"e.g., (in)appropriate evaluation metrics and lack of significance testing. Language Pairs. Our evaluation comprises eight languages: Croatian (HR), English (EN), Finnish (FI), French (FR), German (DE), Italian (IT), Russian (RU), and Turkish (TR). For diversity, we selected two languages from three different IndoEuropean branches: Germanic (EN, DE), Romance (FR, IT), and Slavic (HR, RU); as well as two nonIndo-European languages (FI, TR). From these, we create a total of 28 language pairs for evaluation. Monolingual Embeddings. Following prior work, we use 300-dimensional fastText embeddings (Bojanowski et al., 2017)6 , pretrained on complete Wikipedias of each language. We trim all vocabularies to the 200K most frequent words. Translation Dictionaries. We automatically created translation dictionaries using Google Translate, similar to prior work (Conneau et al., 2018a). We selected the 20K most frequent English words and automatically translated them to the other seven languages. We retained only tuples for which all translations were unigrams found in vocabularies of respective monolingual embedding spaces, leaving us with ≈7K tuples. We reserved 5K tuples created from the more frequent English words f"
P19-1070,P14-1006,0,0.272128,"ks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that r"
P19-1070,D15-1075,0,0.0199549,"txe and Schwenk, 2018; Lample and Conneau, 2019), but rather to provide means to analyze properties and relative performance of diverse CLE models in a downstream language understanding task. Avg 0.561 0.607 0.613 0.615 0.614 0.376 0.390 0.504 0.534 0.543 0.532 0.556 0.357 0.363 0.534 0.568 0.568 0.573 0.536 0.387 0.387 0.544 0.585 0.593 0.599 0.579 0.378 0.399 0.536 0.574 0.579 0.580 0.571 0.374 0.385 0.604 0.611 0.580 0.427* 0.613 0.536 0.510 0.383* 0.534 0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then eval"
P19-1070,P17-1152,0,0.0156855,"0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2 portion of the XNLI by feeding L2 vectors from the shared space. 13 1K 5K 1K 3K 5K 1K 5K EN – DE EN – FR EN – TR EN – RU V EC M AP M USE ICP GWA Table 3: XNLI performance (test set accuracy). Bold: highest scores, with mutually insignificant differences according to the non-parametric shuffling test (Yeh, 2000). Asterisks denote language pairs for which CLE models could not yield successful runs in the BLI task. are significant dif"
P19-1070,D18-1043,0,0.168294,"upervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While"
P19-1070,D15-1127,0,0.190112,"Missing"
P19-1070,D18-1063,0,0.0729896,"ally attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated o"
P19-1070,D18-1330,0,0.452523,"I due to the lack of a common evaluation protocol: different papers consider different language pairs and employ different training and evaluation dictionaries. Furthermore, there is a surprising lack of testing of BLI results for statistical significance. The mismatches in evaluation yield partial conclusions and inconsistencies: on the one hand, some unsupervised models (Artetxe et al., 2018b; Hoshen and Wolf, 2018) reportedly outperform competitive supervised CLE models (Artetxe et al., 2017; Smith et al., 2017). On the other hand, the most recent supervised approaches (Doval et al., 2018; Joulin et al., 2018) report performances surpassing the best unsupervised models. is easily obtainable for most language pairs.2 Therefore, despite the attractive zero-supervision setup, we see unsupervised CLE models practically justified only if such models can, unintuitively, indeed outperform their supervised competition. Contributions. We provide a comprehensive comparative evaluation of a wide range of stateof-the-art—both supervised and unsupervised— projection-based CLE models. Our benchmark encompasses BLI and three cross-lingual (CL) downstream tasks of different nature: document classification (CLDC),"
P19-1070,D18-1047,0,0.174622,"akly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream task"
P19-1070,D18-1101,0,0.0224836,"t al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 20"
P19-1070,C12-1089,0,0.765619,"embeddings (CLEs). CLE models learn vectors of words in two or more languages and represent them in a shared cross-lingual word vector space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 ,"
P19-1070,D18-1549,0,0.0266683,"anguage. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): t"
P19-1070,P15-1027,0,0.0610819,"d bipartite weighted graph G = (E, VL1 ∪ VL2 ) with edges E = VL1 × VL2 . By drawing embeddings from a Gaussian distribution and normalizing them, the weight of each edge (i, j) ∈ E is shown to correspond to the cosine similarity between vectors. In the E-step, a maximal bipartite matching is found on the sparsified graph using the Jonker-Volgenant algorithm (Jonker and Volgenant, 1987). In the M-step, a better projection WL1 is learned by solving the Procrustes problem. Ranking-Based Optimization (RCSLS). Instead of minimizing the Euclidean distance, Joulin et al. (2018) follow earlier work (Lazaridou et al., 2015) and maximize a ranking-based objective, specifically cross-domain similarity local scaling (CSLS; Conneau et al., 2018a), between the XS WL1 and XT . CSLS is an extension of cosine similarity commonly used for BLI inference. Let r(xkL1 W, XL2 ) be the average cosine similarity of the projected source vector with its N nearest neighbors from XL2 . Inversely, let r(xkL2 , XL1 W) be the average cosine similarity of a target vector with its N nearest neighbors from the projected source space XL1 W. By relaxing the orthogonality constraint on WL1 , maximization of relaxed CSLS (dubbed RCSLS) becom"
P19-1070,E17-1072,0,0.0207304,"ual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhan"
P19-1070,W15-1521,0,0.103779,"nguages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections"
P19-1070,D17-1269,0,0.0403092,"Missing"
P19-1070,D18-1042,1,0.877682,"c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models,"
P19-1070,P15-1165,0,0.135665,"Missing"
P19-1070,P18-1072,1,0.840679,"Missing"
P19-1070,P16-1157,0,0.0339789,"Procrustes model (P ROC -B, see §2.2) and show it is competitive across the board. We find that overfitting to BLI may severely hurt downstream performance, warranting the coupling of BLI experiments with downstream evaluations in order to paint a more informative picture of CLE models’ properties. 2 Projection-Based CLEs: Methodology In contrast to more recent unsupervised models, CLE models typically require bilingual signal: aligned words, sentences, or documents. CLE models based on sentence and document alignments have been extensively studied in previous work (Vuli´c and Korhonen, 2016; Upadhyay et al., 2016; Ruder et al., 2018b). Current CLE research is almost exclusively focused on projection-based CLE models; they are thus also the focus of our study.3 Supervised projection-based CLEs require merely small-sized translation dictionaries (up to a few thousand word pairs) and such bilingual signal 711 2 We argue that, if acquiring a few thousand word translation pairs is a challenge, one probably deals with a truly underresourced language for which it would be difficult to obtain reliable monolingual embeddings in the first place. Furthermore, there are initiatives in typological linguistics rese"
P19-1070,P16-1024,1,0.936437,"Missing"
P19-1070,N18-1101,0,0.0184084,"; Lample and Conneau, 2019), but rather to provide means to analyze properties and relative performance of diverse CLE models in a downstream language understanding task. Avg 0.561 0.607 0.613 0.615 0.614 0.376 0.390 0.504 0.534 0.543 0.532 0.556 0.357 0.363 0.534 0.568 0.568 0.573 0.536 0.387 0.387 0.544 0.585 0.593 0.599 0.579 0.378 0.399 0.536 0.574 0.579 0.580 0.571 0.374 0.385 0.604 0.611 0.580 0.427* 0.613 0.536 0.510 0.383* 0.534 0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2"
P19-1070,N15-1104,0,0.527117,"Missing"
P19-1070,D18-1268,0,0.0814185,"nd unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream tasks like text class"
P19-1070,C00-2137,0,0.327675,"etup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2 portion of the XNLI by feeding L2 vectors from the shared space. 13 1K 5K 1K 3K 5K 1K 5K EN – DE EN – FR EN – TR EN – RU V EC M AP M USE ICP GWA Table 3: XNLI performance (test set accuracy). Bold: highest scores, with mutually insignificant differences according to the non-parametric shuffling test (Yeh, 2000). Asterisks denote language pairs for which CLE models could not yield successful runs in the BLI task. are significant differences between BLI and XNLI performance across language pairs—while we observe much better BLI performance for EN–DE and EN– FR compared to EN – RU and especially EN – TR , XNLI performance of most models for EN–RU and EN – TR surpasses that for EN – FR and is close to that for EN–DE. While this can be an artifact of the XNLI dataset creation, we support these observations for invidivual language pairs by measuring an overall Spearman correlation of only 0.13 between BLI"
P19-1070,N16-1156,0,0.132294,"or space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a con"
P19-1070,D13-1141,0,0.0623751,"downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it"
P19-1070,P17-1179,0,\N,Missing
P19-4007,Q16-1031,0,0.0204922,"offer an elegant and language-pair independent way to represent content across different languages. They enable us to reason about word meaning in multilingual contexts and serve as an integral source of knowledge for multilingual applications such as machine translation (Artetxe et al., 2018d; Qi et al., 2018; Lample et al., 2018b) or multilingual search and question answering (Vuli´c and Moens, 2015). In addition, they are a key facilitator of cross-lingual transfer and joint multilingual training, offering support to NLP applications in a large spectrum of languages (Søgaard et al., 2015; Ammar et al., 2016a). While NLP is increasingly more embedded into a variety of products related to, e.g., translation, conversational or search tasks, resources such as annotated training data are still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has alrea"
P19-4007,D18-1024,0,0.0563881,"Missing"
P19-4007,Q17-1010,0,0.178231,"Missing"
P19-4007,D16-1136,0,0.0662203,"Missing"
P19-4007,P17-2037,1,0.876661,"Missing"
P19-4007,D12-1001,0,0.0762697,"Missing"
P19-4007,S17-2002,0,0.078474,"Missing"
P19-4007,N18-2029,1,0.901703,"Missing"
P19-4007,D18-1023,0,0.0295173,"Missing"
P19-4007,P18-1004,1,0.898441,"Missing"
P19-4007,P19-1070,1,0.876625,"Missing"
P19-4007,Q19-1007,0,0.0295517,"Missing"
P19-4007,D18-1330,0,0.069032,"Missing"
P19-4007,K18-1021,1,0.897312,"Missing"
P19-4007,N19-1188,1,0.889169,"Missing"
P19-4007,E17-1102,1,0.891004,"Missing"
P19-4007,D17-1269,0,0.060333,"Missing"
P19-4007,D18-1043,0,0.0398969,"Missing"
P19-4007,D11-1006,0,0.103244,"Missing"
P19-4007,D15-1127,0,0.0637398,"Missing"
P19-4007,N19-1386,0,0.0455551,"Missing"
P19-4007,P18-2035,0,0.0666436,"Missing"
P19-4007,Q17-1022,1,0.905704,"Missing"
P19-4007,D18-1063,0,0.0557082,"Missing"
P19-4007,P15-1165,1,0.881513,"Missing"
P19-4007,D18-1047,0,0.013888,"chosen hyper-parameters, etc. In this part, we will analyze the current problems with robustness and stability of weaklysupervised and unsupervised alignment methods in relation to all these factors, and introduce latest solutions to alleviate these problems. We will provide advice on how to approach weakly-supervised and unsupervised training based on a series of empirical observations available in recent literature (Søgaard et al., 2018; Hartmann et al., 2018). We will also discuss the (im)possibility of learning nonlinear mappings using either non-linear generators or locally linear maps (Nakashole, 2018). We will conclude by providing publicly available software packages and implementations, as well as available training datasets and evaluation protocols and systems. We will also list current state-of-the-art results on standard evaluation datasets, and sketch future research paths. • A general framework for mapping-based approaches. 3 • Importance of seed bilingual lexicons. • Learning alignment with weak supervision: small seed lexicons, shared words, numerals. Part III: Adversarial Seed Induction (30 minutes) • Fully unsupervised models using adversarial training; MUSE and related approach"
P19-4007,P18-1072,1,0.89956,"Missing"
P19-4007,P18-2036,0,0.0408295,"Missing"
P19-4007,D17-1264,0,0.0588289,"Missing"
P19-4007,N16-1072,0,0.071611,"Missing"
P19-4007,N10-1135,0,0.0513539,"Missing"
P19-4007,P16-1157,0,0.0233929,"still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has already verified the usefulness of cross-lingual word representations in a wide variety of downstream tasks, and has provided extensive model classifications in several survey papers (Upadhyay et al., 2016; Ruder et al., 2018b). They cluster supervised cross-lingual word representation models according to the bilingual supervision required to induce such shared cross-lingual semantic spaces, covering models based on word alignments and readily available bilingual dictionaries (Mikolov et al., 2013; Smith et al., 2017), sentence-aligned parallel data (Gouws et al., 2015), document-aligned data (Søgaard et al., 2015; Vuli´c 1 Learning unsupervised cross-lingual models has indeed taken the field by storm: there are 10+ papers on this very topic published in EMNLP 2018 proceedings alone, with even"
P19-4007,D18-1026,1,0.906688,"Missing"
P19-4007,N18-2084,0,0.0307978,"ource-poor settings where bilingual supervision cannot be guaranteed; 2) critical examinations of different training conditions and requirements under which unsupervised algorithms can and cannot work effectively; 3) more robust methods for distant language pairs that Cross-lingual word representations offer an elegant and language-pair independent way to represent content across different languages. They enable us to reason about word meaning in multilingual contexts and serve as an integral source of knowledge for multilingual applications such as machine translation (Artetxe et al., 2018d; Qi et al., 2018; Lample et al., 2018b) or multilingual search and question answering (Vuli´c and Moens, 2015). In addition, they are a key facilitator of cross-lingual transfer and joint multilingual training, offering support to NLP applications in a large spectrum of languages (Søgaard et al., 2015; Ammar et al., 2016a). While NLP is increasingly more embedded into a variety of products related to, e.g., translation, conversational or search tasks, resources such as annotated training data are still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no tra"
P19-4007,D18-1270,0,0.0630194,"Missing"
P19-4007,P18-1084,1,0.89476,"Missing"
P19-4007,N18-1056,0,0.0443504,"Missing"
P19-4007,D18-1042,1,0.916875,"ficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has already verified the usefulness of cross-lingual word representations in a wide variety of downstream tasks, and has provided extensive model classifications in several survey papers (Upadhyay et al., 2016; Ruder et al., 2018b). They cluster supervised cross-lingual word representation models according to the bilingual supervision required to induce such shared cross-lingual semantic spaces, covering models based on word alignments and readily available bilingual dictionaries (Mikolov et al., 2013; Smith et al., 2017), sentence-aligned parallel data (Gouws et al., 2015), document-aligned data (Søgaard et al., 2015; Vuli´c 1 Learning unsupervised cross-lingual models has indeed taken the field by storm: there are 10+ papers on this very topic published in EMNLP 2018 proceedings alone, with even more papers availabl"
P19-4007,P16-1024,1,0.90887,"Missing"
P19-4007,D13-1168,1,0.869883,"Missing"
P19-4007,N19-1162,0,0.0610116,"Missing"
P19-4007,D17-1270,1,0.903119,"Missing"
P19-4007,D18-1268,0,0.0445512,"Missing"
P19-4007,P18-1005,0,0.0613929,"Missing"
P19-4007,C16-1300,0,0.060658,"Missing"
P19-4007,P17-1179,0,0.0632695,"Missing"
P19-4007,D17-1207,0,0.0552572,"Missing"
P19-4007,P19-1307,0,0.0367463,"Missing"
P19-4007,N16-1156,0,0.0701558,"Missing"
P19-4007,D18-1022,0,0.0481647,"Missing"
S16-1026,D14-1181,0,0.218692,", 2008) allows us to gain insights about opinions towards persons, objects, and events in the public eye and is used nowadays to gauge public opinion towards companies or products, to analyze customer satisfaction, and to detect trends. Its immediacy allowed Twitter to become an important platform for expressing opinions and public discourse, while the accessibility of large quantities of data in turn made it the focal point of social media sentiment analysis research. Recently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014) and have performed well for phrase-level and messagelevel sentiment classification (Severyn and Moschitti, 2015). Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field. SemEval-2016 Task 4 (Nakov et al., 2016) is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before. We apply our deep learning-based model for sentiment analysis to all subtasks of SemEval-2016 Task 4: three-point"
S16-1026,S16-1001,0,0.0293664,"r expressing opinions and public discourse, while the accessibility of large quantities of data in turn made it the focal point of social media sentiment analysis research. Recently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014) and have performed well for phrase-level and messagelevel sentiment classification (Severyn and Moschitti, 2015). Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field. SemEval-2016 Task 4 (Nakov et al., 2016) is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before. We apply our deep learning-based model for sentiment analysis to all subtasks of SemEval-2016 Task 4: three-point scale message polarity classification (subtask A), two-point and five-point scale topic sentiment classification (subtasks B and C respectively), and two-point and five-point scale topic sentiment quantification (subtasks D and E respectively). Our model achieves excellent results for subtasks B and D, ranks"
S16-1026,D14-1162,0,0.0828565,"datasets for training of all our models. We notably do not select the model that achieves the lowest loss on the validation set, but choose the one that maximizes the F1P N score, i.e. the arithmetic mean of the F1 of positive and negative tweets, which has historically been used to evaluate the SemEval message polarity classification subtask. We observe that the lowest loss does not necessarily lead to the lowest F1P N , as it does not include F1 of neutral tweets. 4.2 Pre-processing For pre-processing, we use a script adapted from the pre-processing script1 used for training GloVe vectors (Pennington et al., 2014). Besides normalizing urls and mentions, we notably normalize happy and sad smileys, extract hashtags, and insert tags for repeated, elongated, and all caps characters. 4.3 Word embeddings Past research (Kim, 2014; Severyn and Moschitti, 2015) found a good initialization of word embed1 http://nlp.stanford.edu/projects/glove/ preprocess-twitter.rb dings to be crucial in training an accurate sentiment model. We thus evaluate the following evaluation schemes: random initialization, initialization using pre-trained GloVe vectors, fine-tuning pre-trained embeddings on a distantly supervised corpus"
S16-1026,S15-2079,0,0.446571,"ublic eye and is used nowadays to gauge public opinion towards companies or products, to analyze customer satisfaction, and to detect trends. Its immediacy allowed Twitter to become an important platform for expressing opinions and public discourse, while the accessibility of large quantities of data in turn made it the focal point of social media sentiment analysis research. Recently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014) and have performed well for phrase-level and messagelevel sentiment classification (Severyn and Moschitti, 2015). Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field. SemEval-2016 Task 4 (Nakov et al., 2016) is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before. We apply our deep learning-based model for sentiment analysis to all subtasks of SemEval-2016 Task 4: three-point scale message polarity classification (subtask A), two-point and five-point scale topic sentiment classification"
S16-1026,P14-1146,0,0.102864,"2016, pages 178–182, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2 Related work Deep-learning based approaches have recently dominated the state-of-the-art in sentiment analysis. Kim (2014) uses a one-layer convolutional neural network to achieve top performance on various sentiment analysis datasets, demonstrating the utility of pre-trained embeddings. State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al. (2014) learn sentiment-specific word embeddings from such distantly supervised data and use these as features for supervised classification, while Severyn and Moschitti (2015) use distantly supervised data to fine-tune the embeddings of a convolutional neural network. In contrast, we observe distantly supervised data not to be as important for some tasks as long as sufficient training data is available. 3 Model The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011). The model takes as input a text, which is padded to length n. We represent the text as a co"
S16-1026,S14-2009,0,\N,Missing
S16-1053,D14-1181,0,0.0607503,"entiment analysis allows us to go deeper and determine sentiment towards such aspects of an entity. Past research in aspect-based sentiment analysis has largely focused on the English language, while SemEval 2016 Task 5 (Pontiki et al., 2016) for the first time provides a forum for multilingual aspectbased sentiment analysis. Introduction With access to the Internet becoming more prevalent, an inreasing number of people express their opinions online in a plethora of lanRecently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014). A cascade of non-linearities allows them to model complex functions such as sentiment compositionality, while their ability to process raw signals renders them language and domain independent. In spite of these factors, they have largely gone untested for aspect-based sentiment analysis, particularly in the multilingual setting. In this paper, we introduce our deep-learning 330 Proceedings of SemEval-2016, pages 330–336, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics based approach to aspect-based sentiment analysis as part of our participation in S"
S16-1053,D15-1298,0,0.25294,"thora of common features, e.g. NER, POS tagging, parsing, semantic analysis, bagof-words, as well as domain-dependent ones, such as word clusters learnt from Amazon and Yelp data, while previous sentiment analysis approaches have used different classifiers with a wide range of features based on n-grams, POS, negation words, and a large array of sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). Past deep learning-based approaches have focused mostly on the sentiment analysis subtask: Tang et al. (2015) use a target-dependent LSTM to determine sentiment towards a target word, while Nguyen and Shirai (2015) use a recursive neural network that leverages both constituency as well as dependency trees. In contrast to previous approaches, our model neither relies on expensive feature engineering, availability of a parser, nor positional information, but solely on a language’s input signals. 3 Model The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011), which has been successfully used by many others (Kim, 2014). The model takes as input a text, which is 331 padded to length n. We represent the text as a concatentation of its word embeddings x1:n where xi ∈"
S16-1053,D14-1162,0,0.0828187,"tence> hyperparameters via random search over a wide range of values. For both tasks and all languages and domains, we use the following hyperparameters, which are similar to those reported by Kim (2014): mini-batch size of 10, maximum sentence length of 100 tokens, word embedding size of 300, dropout rate of 0.5, and 100 filter maps. We use filter lengths of 3, 4, and 5, and of 4, 5, and 6 for aspect extraction and aspect-based sentiment analysis respectively since these produced good results for the respective task. English word embeddings are initialized with 300-dimensional GloVe vectors (Pennington et al., 2014) trained on 840B tokens of the Common Crawl corpus for the unconstrained submission. Word embeddings for the constrained submission, for all other languages, as well as for words not present in the pre-trained set of words are initialized randomly. We train for 15 epochs using mini-batch stochastic gradient descent, the Adadelta update rule (Zeiler, 2012), and early stopping. 4.3 Aspect Category Detection To extract aspects, e.g. LAPTOP#PRICE and LAPTOP#GENERAL from sentences as in Listing 1, we cast aspect extraction as a multi-label classification problem and train a convolutional 332 neural"
S16-1053,S14-2004,0,0.0976321,"t analysis is traditionally split into an aspect extraction and a sentiment analysis subtask. Previous approaches to aspect extraction framed the task as a multiclass classification problem and relied mostly on CRS that leveraged a plethora of common features, e.g. NER, POS tagging, parsing, semantic analysis, bagof-words, as well as domain-dependent ones, such as word clusters learnt from Amazon and Yelp data, while previous sentiment analysis approaches have used different classifiers with a wide range of features based on n-grams, POS, negation words, and a large array of sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). Past deep learning-based approaches have focused mostly on the sentiment analysis subtask: Tang et al. (2015) use a target-dependent LSTM to determine sentiment towards a target word, while Nguyen and Shirai (2015) use a recursive neural network that leverages both constituency as well as dependency trees. In contrast to previous approaches, our model neither relies on expensive feature engineering, availability of a parser, nor positional information, but solely on a language’s input signals. 3 Model The model architecture we use is an extension of the CNN structure u"
S16-1053,S15-2082,0,0.323383,"nally split into an aspect extraction and a sentiment analysis subtask. Previous approaches to aspect extraction framed the task as a multiclass classification problem and relied mostly on CRS that leveraged a plethora of common features, e.g. NER, POS tagging, parsing, semantic analysis, bagof-words, as well as domain-dependent ones, such as word clusters learnt from Amazon and Yelp data, while previous sentiment analysis approaches have used different classifiers with a wide range of features based on n-grams, POS, negation words, and a large array of sentiment lexica (Pontiki et al., 2014; Pontiki et al., 2015). Past deep learning-based approaches have focused mostly on the sentiment analysis subtask: Tang et al. (2015) use a target-dependent LSTM to determine sentiment towards a target word, while Nguyen and Shirai (2015) use a recursive neural network that leverages both constituency as well as dependency trees. In contrast to previous approaches, our model neither relies on expensive feature engineering, availability of a parser, nor positional information, but solely on a language’s input signals. 3 Model The model architecture we use is an extension of the CNN structure used by Collobert et al."
S16-1053,P14-1146,0,\N,Missing
W16-6012,E14-1049,0,0.0142343,"of XT . Equivalently, we can factorize the transformation W into two transformations A and B with W = AB T that we can use to project the source and target examples into a joint subspace. We assume that XS and XT lie on lowerdimensional orthonormal subspaces, S, T ∈ RD×d , which can be represented as points on the Grassman manifold, G(d, D) as in Figure 1, where d  D. In computer vision, methods such as Subspace Alignment (Fernando et al., 2013) or the Geodesic Flow Kernel (Gong et al., 2012) have been used to find such transformations A and B. Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages. Many recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains. Similarly, in a sequence-to-sequence dialogue model (Vinyals and V. Le, 2015), we can not only train 55 the model to predict the source domain response, but also – via a reconstruction loss – its transformations to the target domain. For continuous domains, we can assume that source domain XS and target domain XT are not independent,"
W16-6012,P11-2020,0,0.0137206,"varies with domain. Dialogue modeling can be seen as a prototypical task in natural language processing akin to language modeling and should thus expose variations in the underlying language. It allows one to observe the impact of different strategies to model variation in language across domains on a downstream task, while being inherently unsupervised. In addition, dialogue has been shown to exhibit characteristics that expose how language changes as conversation partners become more linguistically similar to each other over the course of the conversation (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011). Similarly, it has been shown that the linguistic patterns of individual users in online communities adapt to match those of the community they participate in (Nguyen and Ros´e, 2011; DanescuNiculescu-Mizil et al., 2013). For this reason, we have selected reddit as a medium and compiled a dataset from large amounts of reddit data. Reddit comments live in a rich environment that is dependent on a large number of contextual factors, such as community, user, conversation, etc. Similar to Chen et al. (2016), we would like to learn representations that allow us to disentangle factors that are norm"
W16-6012,N16-1083,0,0.0144928,"ormation W into two transformations A and B with W = AB T that we can use to project the source and target examples into a joint subspace. We assume that XS and XT lie on lowerdimensional orthonormal subspaces, S, T ∈ RD×d , which can be represented as points on the Grassman manifold, G(d, D) as in Figure 1, where d  D. In computer vision, methods such as Subspace Alignment (Fernando et al., 2013) or the Geodesic Flow Kernel (Gong et al., 2012) have been used to find such transformations A and B. Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages. Many recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains. Similarly, in a sequence-to-sequence dialogue model (Vinyals and V. Le, 2015), we can not only train 55 the model to predict the source domain response, but also – via a reconstruction loss – its transformations to the target domain. For continuous domains, we can assume that source domain XS and target domain XT are not independent, but that XT has evolved from XS based on a continuous"
W16-6012,W11-0710,0,0.0959213,"Missing"
W16-6012,P11-1157,0,0.0236634,"Missing"
W16-6012,W10-2605,0,0.0481449,"Missing"
W19-4302,marelli-etal-2014-sick,0,0.0276838,"Missing"
W19-4302,I05-5002,0,0.162595,"se, the sentence representation is typically provided as input to a linear classifier ( ). LM pretraining with has also been successfully applied to sentence-level tasks. Howard and Ruder (2018, ULMFiT) propose techniques for fine-tuning a LM, including triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie reviews. NLI We use both the broad-domain MultiNLI dataset (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK-E; Marelli et al., 2014). STS We employ the Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) and SICKR (Marelli et al., 2014). Both datasets provide a similarity value from 1 to 5 for each sentence pair. 3.2 We now describe how we adapt ELMo and BERT to these tasks. For we requi"
W19-4302,D16-1046,0,0.0375898,", 2015), which employs a next-sentence prediction objective similar to BERT. Both ELMo and BERT outperform the sentence embedding method significantly, except on the semantic textual similarity tasks (STS) where Skipthoughts is similar to ELMo. The overall performance of and shows small differences except for a few notable cases. For ELMo, we find the largest differences for sentence pair tasks where consistently outperforms . For BERT, we obtain nearly the opposite result: significantly outperforms on all STS tasks, with much smaller differences for the others. 5 Discussion Past work in NLP (Mou et al., 2016) showed that similar pretraining tasks transfer better.1 In computer vision (CV), Yosinski et al. (2014) similarly found that the transferability of features decreases as the distance between the pretraining and target task increases. In this vein, Skip-thoughts—and Quick-thoughts (Logeswaran and Lee, 2018), which has similar performance— which use a next-sentence prediction objective Analyses Modelling pairwise interactions LSTMs consider each token sequentially, while Transformers can relate each token to every other in each layer (Vaswani et al., 2017). This might facilitate with Transforme"
W19-4302,P18-1031,1,0.923864,"y improved over noncontextual vectors. notations of newswire across four different entity types (PER, LOC, ORG, MISC). Sentence embedding methods Such methods learn sentence representations via different pretraining objectives such as previous/next sentence prediction (Kiros et al., 2015; Logeswaran and Lee, 2018), NLI (Conneau et al., 2017), or a combination of objectives (Subramanian et al., 2018). During the adaptation phase, the sentence representation is typically provided as input to a linear classifier ( ). LM pretraining with has also been successfully applied to sentence-level tasks. Howard and Ruder (2018, ULMFiT) propose techniques for fine-tuning a LM, including triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie"
W19-4302,D14-1181,0,0.00879252,"duce universal representations suitable for any downstream task. The first two authors contributed equally. Sebastian is now affiliated with DeepMind. 7 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Word representations Pretrained word vectors (Turian et al., 2010; Pennington et al., 2014) have been an essential component in state-of-the-art NLP systems. Word representations are often fixed and fed into a task specific model ( ), although can provide improvements (Kim, 2014). Recently, contextual word representations learned supervisedly (e.g., through MT; McCann et al., 2017) or unsupervisedly (typically through language modeling; Peters et al., 2018) have significantly improved over noncontextual vectors. notations of newswire across four different entity types (PER, LOC, ORG, MISC). Sentence embedding methods Such methods learn sentence representations via different pretraining objectives such as previous/next sentence prediction (Kiros et al., 2015; Logeswaran and Lee, 2018), NLI (Conneau et al., 2017), or a combination of objectives (Subramanian et al., 2018"
W19-4302,D14-1162,0,0.0766308,"Missing"
W19-4302,N18-1202,1,0.932679,"f adaptation guidelines for the NLP practitioner. 1 Any Any Any ELMo BERT Any Add many task parameters Any Add minimal task parameters Hyper-parameters Seq. / clas. Sent. pair Sent. pair and use use have similar performance computationally cheaper as features only need to be computed once. On the other hand, is convenient as it may allow us to adapt a general-purpose representation to many different tasks. Gaining a better understanding of the adaptation phase is key in making the most use out of pretrained representations. To this end, we compare two state-of-the-art pretrained models, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) using both and across seven diverse tasks including named entity recognition, natural language inference (NLI), and paraphrase detection. We seek to characterize the conditions under which one approach substantially outperforms the other, and whether it is dependent on the pretraining objective or target task. We find that and have comparable performance in most cases, except when the source and target tasks are either highly similar or highly dissimilar. We furthermore shed light on the practical challenges of adaptation and provide a set of guidelines to the N"
W19-4302,N03-1017,0,0.0208465,"ch the model learns a generalpurpose representation of inputs, and adaptation, in which the representation is transferred to a new task. Most previous work in NLP has focused on pretraining objectives for learning word or sentence representations (Mikolov et al., 2013; Kiros et al., 2015). Few works, however, have focused on the adaptation phase. There are two main paradigms for adaptation: feature extraction and fine-tuning. In feature extraction ( ) the model’s weights are ‘frozen’ and the pretrained representations are used in a downstream model similar to classic feature-based approaches (Koehn et al., 2003). Alternatively, a pretrained model’s parameters can be unfrozen and fine-tuned ( ) on a new task (Dai and Le, 2015). Both have benefits: enables use of task-specific model architectures and may be † Any Any Any Guidelines Table 1: This paper’s guidelines for using feature extraction ( ) and fine-tuning ( ) with ELMo and BERT. Seq.: sequence labeling. Clas.: classification. Sent. pair: sentence pair tasks. Introduction ? Conditions Adapt. Task 2 Pretraining and Adaptation In this work, we focus on pretraining tasks that seek to induce universal representations suitable for any downstream task."
W19-4302,N19-5004,1,0.763114,"ition, natural language inference (NLI), and paraphrase detection. We seek to characterize the conditions under which one approach substantially outperforms the other, and whether it is dependent on the pretraining objective or target task. We find that and have comparable performance in most cases, except when the source and target tasks are either highly similar or highly dissimilar. We furthermore shed light on the practical challenges of adaptation and provide a set of guidelines to the NLP practitioner, as summarized in Table 1. Sequential inductive transfer learning (Pan and Yang, 2010; Ruder, 2019) consists of two stages: pretraining, in which the model learns a generalpurpose representation of inputs, and adaptation, in which the representation is transferred to a new task. Most previous work in NLP has focused on pretraining objectives for learning word or sentence representations (Mikolov et al., 2013; Kiros et al., 2015). Few works, however, have focused on the adaptation phase. There are two main paradigms for adaptation: feature extraction and fine-tuning. In feature extraction ( ) the model’s weights are ‘frozen’ and the pretrained representations are used in a downstream model s"
W19-4302,D17-1038,1,0.632924,"JS div TE GO TR FI SL 84.4 -1.1 0.21 86.7 -0.2 0.18 86.1 -0.6 0.14 84.5 0.4 0.09 80.9 -0.6 0.09 Table 6: Accuracy of feature extraction ( ) and difference compared to fine-tuning ( ) with BERT-base trained on training data of different MNLI domains and evaluated on corresponding dev sets. TE: telephone. FI: fiction. TR: travel. GO: government. SL: slate. Impact of Target Domain Pretrained language model representations are intended to be universal. However, the target domain might still impact the adaptation performance. We calculate the Jensen-Shannon divergence based on term distributions (Ruder and Plank, 2017) between the domains used to train BERT (books and Wikipedia) and each MNLI domain. We show results in Table 6. We find no significant correlation. At least for this task, the distance of the source and target domains does not seem to have a major impact on the adaptation performance. Impact of additional parameters We evaluate whether adding parameters is useful for both adaptation settings on NER. We add a CRF layer (as used in ) and a BiLSTM with a CRF layer (as used in ) to both and show results in Table 5. We find that additional parameters are key for , but hurt performance with .3 In ad"
W19-4302,D13-1170,0,0.00970417,"aining with has also been successfully applied to sentence-level tasks. Howard and Ruder (2018, ULMFiT) propose techniques for fine-tuning a LM, including triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie reviews. NLI We use both the broad-domain MultiNLI dataset (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK-E; Marelli et al., 2014). STS We employ the Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) and SICKR (Marelli et al., 2014). Both datasets provide a similarity value from 1 to 5 for each sentence pair. 3.2 We now describe how we adapt ELMo and BERT to these tasks. For we require a task-specific architecture, while for we need a task-specific output layer. For fair comp"
W19-4302,W18-5446,0,0.0773103,"Missing"
W19-4302,N18-1101,0,0.0234461,"triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie reviews. NLI We use both the broad-domain MultiNLI dataset (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK-E; Marelli et al., 2014). STS We employ the Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) and SICKR (Marelli et al., 2014). Both datasets provide a similarity value from 1 to 5 for each sentence pair. 3.2 We now describe how we adapt ELMo and BERT to these tasks. For we require a task-specific architecture, while for we need a task-specific output layer. For fair comparison, we conduct an extensive hyper-parameter search for each task. Feature extraction ( ) For both ELMo and BERT, we extract contextual representations of"
W19-4302,W03-0419,0,\N,Missing
W19-4302,P10-1040,0,\N,Missing
W19-4302,N16-1030,0,\N,Missing
W19-4302,P17-1152,0,\N,Missing
W19-4302,S17-2001,0,\N,Missing
W19-4302,D17-1169,0,\N,Missing
W19-4302,W18-2501,1,\N,Missing
