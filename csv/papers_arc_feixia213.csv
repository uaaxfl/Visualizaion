2020.nlpmc-1.3,Studying Challenges in Medical Conversation with Structured Annotation,2020,-1,-1,3,1,16066,nan wang,Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,0,"Medical conversation is a central part of medical care. Yet, the current state and quality of medical conversation is far from perfect. Therefore, a substantial amount of research has been done to obtain a better understanding of medical conversation and to address its practical challenges and dilemmas. In line with this stream of research, we have developed a multi-layer structure annotation scheme to analyze medical conversation, and are using the scheme to construct a corpus of naturally occurring medical conversation in Chinese pediatric primary care setting. Some of the preliminary findings are reported regarding 1) how a medical conversation starts, 2) where communication problems tend to occur, and 3) how physicians close a conversation. Challenges and opportunities for research on medical conversation with NLP techniques will be discussed."
2020.findings-emnlp.153,Improving Constituency Parsing with Span Attention,2020,-1,-1,3,1,3939,yuanhe tian,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have been demonstrated to be useful in many tasks, and thus could also be beneficial for constituency parsing if they are appropriately modeled. In this paper, we propose span attention for neural chart-based constituency parsing to leverage n-gram information. Considering that current chart-based parsers with Transformer-based encoder represent spans by subtraction of the hidden states at the span boundaries, which may cause information loss especially for long spans, we incorporate n-grams into span representations by weighting them according to their contributions to the parsing process. Moreover, we propose categorical span attention to further enhance the model by weighting n-grams within different length categories, and thus benefit long-sentence parsing. Experimental results on three widely used benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them."
2020.emnlp-main.487,Supertagging {C}ombinatory {C}ategorial {G}rammar with Attentive Graph Convolutional Networks,2020,-1,-1,3,1,3939,yuanhe tian,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM). In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing. Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging."
2020.coling-main.63,Summarizing Medical Conversations via Identifying Important Utterances,2020,-1,-1,4,0.580342,3941,yan song,Proceedings of the 28th International Conference on Computational Linguistics,0,"Summarization is an important natural language processing (NLP) task in identifying key information from text. For conversations, the summarization systems need to extract salient contents from spontaneous utterances by multiple speakers. In a special task-oriented scenario, namely medical conversations between patients and doctors, the symptoms, diagnoses, and treatments could be highly important because the nature of such conversation is to find a medical solution to the problem proposed by the patients. Especially consider that current online medical platforms provide millions of public available conversations between real patients and doctors, where the patients propose their medical problems and the registered doctors offer diagnosis and treatment, a conversation in most cases could be too long and the key information is hard to be located. Therefore, summarizations to the patients{'} problems and the doctors{'} treatments in the conversations can be highly useful, in terms of helping other patients with similar problems have a precise reference for potential medical solutions. In this paper, we focus on medical conversation summarization, using a dataset of medical conversations and corresponding summaries which were crawled from a well-known online healthcare service provider in China. We propose a hierarchical encoder-tagger model (HET) to generate summaries by identifying important utterances (with respect to problem proposing and solving) in the conversations. For the particular dataset used in this study, we show that high-quality summaries can be generated by extracting two types of utterances, namely, problem statements and treatment recommendations. Experimental results demonstrate that HET outperforms strong baselines and models from previous studies, and adding conversation-related features can further improve system performance."
2020.coling-main.187,Joint {C}hinese Word Segmentation and Part-of-speech Tagging via Multi-channel Attention of Character N-grams,2020,-1,-1,3,1,3939,yuanhe tian,Proceedings of the 28th International Conference on Computational Linguistics,0,"Chinese word segmentation (CWS) and part-of-speech (POS) tagging are two fundamental tasks for Chinese language processing. Previous studies have demonstrated that jointly performing them can be an effective one-step solution to both tasks and this joint task can benefit from a good modeling of contextual features such as n-grams. However, their work on modeling such contextual features is limited to concatenating the features or their embeddings directly with the input embeddings without distinguishing whether the contextual features are important for the joint task in the specific context. Therefore, their models for the joint task could be misled by unimportant contextual information. In this paper, we propose a character-based neural model for the joint task enhanced by multi-channel attention of n-grams. In the attention module, n-gram features are categorized into different groups according to several criteria, and n-grams in each group are weighted and distinguished according to their importance for the joint task in the specific context. To categorize n-grams, we try two criteria in this study, i.e., n-gram frequency and length, so that n-grams having different capabilities of carrying contextual information are discriminatively learned by our proposed attention module. Experimental results on five benchmark datasets for CWS and POS tagging demonstrate that our approach outperforms strong baseline models and achieves state-of-the-art performance on all five datasets."
2020.acl-main.734,Improving {C}hinese Word Segmentation with Wordhood Memory Networks,2020,-1,-1,3,1,3939,yuanhe tian,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Contextual features always play an important role in Chinese word segmentation (CWS). Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS. Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets. Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments."
2020.acl-main.735,Joint {C}hinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge,2020,-1,-1,4,1,3939,yuanhe tian,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks. Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs. In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character. Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect. Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets."
2020.aacl-demo.7,{NLPS}tat{T}est: A Toolkit for Comparing {NLP} System Performance,2020,-1,-1,4,0,23279,haotian zhu,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations,0,"Statistical significance testing centered on p-values is commonly used to compare NLP system performance, but p-values alone are insufficient because statistical significance differs from practical significance. The latter can be measured by estimating effect size. In this pa-per, we propose a three-stage procedure for comparing NLP system performance and provide a toolkit, NLPStatTest, that automates the process. Users can upload NLP system evaluation scores and the toolkit will analyze these scores, run appropriate significance tests, estimate effect size, and conduct power analysis to estimate Type II error. The toolkit provides a convenient and systematic way to compare NLP system performance that goes beyond statistical significance testing."
W19-5027,{C}hi{M}ed: A {C}hinese Medical Corpus for Question Answering,2019,0,0,3,1,3939,yuanhe tian,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Question answering (QA) is a challenging task in natural language processing (NLP), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific QA data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale Chinese medical QA corpus called ChiMed; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks."
W19-5044,{WTMED} at {MEDIQA} 2019: A Hybrid Approach to Biomedical Natural Language Inference,2019,0,0,5,0,11323,zhaofeng wu,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Natural language inference (NLI) is challenging, especially when it is applied to technical domains such as biomedical settings. In this paper, we propose a hybrid approach to biomedical NLI where different types of information are exploited for this task. Our base model includes a pre-trained text encoder as the core component, and a syntax encoder and a feature encoder to capture syntactic and domain-specific information. Then we combine the output of different base models to form more powerful ensemble models. Finally, we design two conflict resolution strategies when the test data contain multiple (premise, hypothesis) pairs with the same premise. We train our models on the MedNLI dataset, yielding the best performance on the test set of the MEDIQA 2019 Task 1."
W18-2309,Coding Structures and Actions with the {COSTA} Scheme in Medical Conversations,2018,0,0,3,1,16066,nan wang,Proceedings of the {B}io{NLP} 2018 workshop,0,"This paper describes the COSTA scheme for coding structures and actions in conversation. Informed by Conversation Analysis, the scheme introduces an innovative method for marking multi-layer structural organization of conversation and a structure-informed taxonomy of actions. In addition, we create a corpus of naturally occurring medical conversations, containing 318 video-recorded and manually transcribed pediatric consultations. Based on the annotated corpus, we investigate 1) treatment decision-making process in medical conversations, and 2) effects of physician-caregiver communication behaviors on antibiotic over-prescribing. Although the COSTA annotation scheme is developed based on data from the task-specific domain of pediatric consultations, it can be easily extended to apply to more general domains and other languages."
L18-1116,{PDF}-to-Text Reanalysis for Linguistic Data Mining,2018,0,1,3,1,6141,michael goodman,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1464,Constructing a {C}hinese Medical Conversation Corpus Annotated with Conversational Structures and Actions,2018,0,1,3,1,16066,nan wang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-0106,{STREAMLI}n{ED} Challenges: Aligning Research Interests with Shared Tasks,2017,12,1,13,0,11446,ginaanne levow,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,This paper describes the use of Shared Task Evaluation Campaigns by designing tasks that are compelling to speech and natural language processing researchers while addressing technical challenges in language documentation and exploiting growing archives of endangered language data.
W17-0110,Inferring Case Systems from {IGT}: Enriching the Enrichment,2017,9,0,4,0,23725,kristen howell,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,None
W17-0118,Computational Support for Finding Word Classes: A Case Study of {A}bui,2017,12,0,4,0,14767,olga zamaraeva,Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,None
K17-1016,Learning Word Representations with Regularization from Prior Knowledge,2017,19,4,3,1,3941,yan song,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"Conventional word embeddings are trained with specific criteria (e.g., based on language modeling or co-occurrence) inside a single information source, disregarding the opportunity for further calibration using external knowledge. This paper presents a unified framework that leverages pre-learned or external priors, in the form of a regularizer, for enhancing conventional language model-based embedding learning. We consider two types of regularizers. The first type is derived from topic distribution by running LDA on unlabeled data. The second type is based on dictionaries that are created with human annotation efforts. To effectively learn with the regularizers, we propose a novel data structure, trajectory softmax, in this paper. The resulting embeddings are evaluated by word similarity and sentiment classification. Experimental results show that our learning framework with regularization from prior knowledge improves embedding quality across multiple datasets, compared to a diverse collection of baseline methods."
D17-1205,{CROWD}-{IN}-{THE}-{LOOP}: A Hybrid Approach for Annotating Semantic Roles,2017,16,0,5,0,8829,chenguang wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Crowdsourcing has proven to be an effective method for generating labeled data for a range of NLP tasks. However, multiple recent attempts of using crowdsourcing to generate gold-labeled training data for semantic role labeling (SRL) reported only modest results, indicating that SRL is perhaps too difficult a task to be effectively crowdsourced. In this paper, we postulate that while producing SRL annotation does require expert involvement in general, a large subset of SRL labeling tasks is in fact appropriate for the crowd. We present a novel workflow in which we employ a classifier to identify difficult annotation tasks and route each task either to experts or crowd workers according to their difficulties. Our experimental evaluation shows that the proposed approach reduces the workload for experts by over two-thirds, and thus significantly reduces the cost of producing SRL annotation at little loss in quality."
P16-4006,A Web-framework for {ODIN} Annotation,2016,6,0,3,1,29629,ryan georgi,Proceedings of {ACL}-2016 System Demonstrations,0,None
L16-1545,Annotating and Detecting Medical Events in Clinical Notes,2016,7,0,2,1,35266,prescott klassen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Early detection and treatment of diseases that onset after a patient is admitted to a hospital, such as pneumonia, is critical to improving and reducing costs in healthcare. Previous studies (Tepper et al., 2013) showed that change-of-state events in clinical notes could be important cues for phenotype detection. In this paper, we extend the annotation schema proposed in (Klassen et al., 2014) to mark change-of-state events, diagnosis events, coordination, and negation. After we have completed the annotation, we build NLP systems to automatically identify named entities and medical events, which yield an f-score of 94.7{\%} and 91.8{\%}, respectively."
W15-3709,Enriching Interlinear Text using Automatically Constructed Annotators,2015,20,3,2,1,29629,ryan georgi,"Proceedings of the 9th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"In this paper, we will demonstrate a system that shows great promise for creating Part-of-Speech taggers for languages with little to no curated resources available, and which needs no expert involvement. Interlinear Glossed Text (IGT) is a resource which is available for over 1,000 languages as part of the Online Database of INterlinear text (ODIN) (Lewis and Xia, 2010). Using nothing more than IGT from this database and a classification-based projection approach tailored for IGT, we will show that it is feasible to train reasonably performing annotators of interlinear text using projected annotations for potentially hundreds of worldxe2x80x99s languages. Doing so can facilitate automatic enrichment of interlinear resources to aid the field of linguistics."
W14-2206,Learning Grammar Specifications from {IGT}: A Case Study of Chintang,2014,31,5,4,0.237277,11448,emily bender,Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages,0,"We present a case study of the methodology of using information extracted from interlinear glossed text (IGT) to create of actual working HPSG grammar fragments using the Grammar Matrix focusing on one language: Chintang. Though the results are barely measurable in terms of coverage over running text, they nonetheless provide a proof of concept. Our experience report reflects on the ways in which this task is non-trivial and on mismatches between the assumptions of the methodology and the realities of IGT as produced in a large-scale field project."
P14-1126,Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization,2014,46,45,2,1,3731,xuezhe ma,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have translated text in a resourcerich language. We train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization. Our method can be used as a purely monolingual dependency parser, requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages. We perform experiments on three Data sets xe2x80x94 Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL shared-tasks, across ten languages. We obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems."
xia-etal-2014-enriching,Enriching {ODIN},2014,12,4,1,1,16067,fei xia,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe the expansion of the ODIN resource, a database containing many thousands of instances of Interlinear Glossed Text (IGT) for over a thousand languages harvested from scholarly linguistic papers posted to the Web. A database containing a large number of instances of IGT, which are effectively richly annotated and heuristically aligned bitexts, provides a unique resource for bootstrapping NLP tools for resource-poor languages. To make the data in ODIN more readily consumable by tool developers and NLP researchers, we propose a new XML format for IGT, called Xigt. We call the updated release ODIN-II."
song-xia-2014-modern,{M}odern {C}hinese Helps Archaic {C}hinese Processing: Finding and Exploiting the Shared Properties,2014,15,2,2,1,3941,yan song,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Languages change over time and ancient languages have been studied in linguistics and other related fields. A main challenge in this research area is the lack of empirical data; for instance, ancient spoken languages often leave little trace of their linguistic properties. From the perspective of natural language processing (NLP), while the NLP community has created dozens of annotated corpora, very few of them are on ancient languages. As an effort toward bridging the gap, we have created a word segmented and POS tagged corpus for Archaic Chinese using articles from Huainanzi, a book written during ChinaÂs Western Han Dynasty (206 BC-9 AD). We then compare this corpus with the Chinese Penn Treebank (CTB), a well-known corpus for Modern Chinese, and report several interesting differences and similarities between the two corpora. Finally, we demonstrate that the CTB can be used to improve the performance of word segmenters and POS taggers for Archaic Chinese, but only through features that have similar behaviors in the two corpora."
klassen-etal-2014-annotating,Annotating Clinical Events in Text Snippets for Phenotype Detection,2014,8,1,2,1,35266,prescott klassen,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Early detection and treatment of diseases that onset after a patient is admitted to a hospital, such as pneumonia, is critical to improving and reducing costs in healthcare. NLP systems that analyze the narrative data embedded in clinical artifacts such as x-ray reports can help support early detection. In this paper, we consider the importance of identifying the change of state for events - in particular, clinical events that measure and compare the multiple states of a patientÂs health across time. We propose a schema for event annotation comprised of five fields and create preliminary annotation guidelines for annotators to apply the schema. We then train annotators, measure their performance, and finalize our guidelines. With the complete guidelines, we then annotate a corpus of snippets extracted from chest x-ray reports in order to integrate the annotations as a new source of features for classification tasks."
W13-2710,Towards Creating Precision Grammars from Interlinear Glossed Text: Inferring Large-Scale Typological Properties,2013,23,15,4,0.237277,11448,emily bender,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,We propose to bring together two kinds of linguistic resourcesxe2x80x94interlinear glossed text (IGT) and a language-independent precision grammar resourcexe2x80x94to automatically create precision grammars in the context of language documentation. This paper takes the first steps in that direction by extracting major-constituent word order and case system properties from IGT for a diverse sample of languages.
W13-1206,Annotating Change of State for Clinical Events,2013,9,3,2,0,29127,lucy vanderwende,"Workshop on Events: Definition, Detection, Coreference, and Representation",0,"Understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text. Our task is the identification of phenotypes, specifically, pneumonia, from clinical narratives. In this paper, we consider the importance of identifying the change of state for events, in particular, events that measure and compare multiple states across time. Change of state is important to the clinical diagnosis of pneumonia; in the example xe2x80x9cthere are bibasilar opacities that are unchangedxe2x80x9d, the presence of bibasilar opacities alone may suggest pneumonia, but not when they are unchanged, which suggests the need to modify events with change of state information. Our corpus is comprised of chest Xray reports, where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas. We propose an annotation schema to capture this information as a tuple of ."
P13-2055,Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text,2013,10,6,2,1,29629,ryan georgi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"As most of the worldxe2x80x99s languages are under-resourced, projection algorithms oer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment. These algorithms, however, make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another. This assumption is useful but often leads to errors in projection. In this paper, we will address this weakness by using trees created from instances of Interlinear Glossed Text (IGT) to discover patterns of divergence between the languages. We will show that this method improves the performance of projection algorithms significantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees."
P13-2104,Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data,2013,20,2,2,1,3731,xuezhe ma,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets."
I13-1071,A Common Case of Jekyll and Hyde: The Synergistic Effect of Using Divided Source Training Data for Feature Augmentation,2013,14,1,2,1,3941,yan song,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Feature augmentation is a well-known method for domain adaptation and has been shown to be effective when tested on several NLP tasks (Daume III, 2007). However, a limitation of the method is that it requires labeled data from the target domain and very often such data is unavailable. In this paper, we propose to use training data selection to divide the source domain training data into two parts, pseudo target data (the selected part) and source data (the unselected part), and then apply feature augmentation on the two parts of the training data. This approach has two advantages: first, feature augmentation can be applied even when there is no labeled data from the target domain; second, the approach can take advantage of all the training data including the part that is not selected by training data selection. We evaluate the approach on Chinese word segmentation and part-of-speech tagging and show that it outperforms the baseline where no feature augmentation is applied."
W12-4619,Creating a {T}ree {A}djoining {G}rammar from a Multilayer Treebank,2012,12,3,3,1,40828,rajesh bhatt,Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms ({TAG}+11),0,We propose a method for the extraction of a Tree Adjoining Grammar (TAG) from a dependency treebank which has some representative examples annotated with phrase structures. We show that the resulting TAG along with corresponding dependency structure can be used to convert a dependency treebank to a TAG-based phrase structure treebank.
georgi-etal-2012-measuring,Measuring the Divergence of Dependency Structures Cross-Linguistically to Improve Syntactic Projection Algorithms,2012,11,5,2,1,29629,ryan georgi,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Syntactic parses can provide valuable information for many NLP tasks, such as machine translation, semantic analysis, etc. However, most of the world's languages do not have large amounts of syntactically annotated corpora available for building parsers. Syntactic projection techniques attempt to address this issue by using parallel corpora between resource-poor and resource-rich languages, bootstrapping the resource-poor language with the syntactic analysis of the resource-rich language. In this paper, we investigate the possibility of using small, parallel, annotated corpora to automatically detect divergent structural patterns between two languages. These patterns can then be used to improve structural projection algorithms, allowing for better performing NLP tools for resource-poor languages, in particular those that may not have large amounts of annotated data necessary for traditional, fully-supervised methods. While this detection process is not exhaustive, we demonstrate that important instances of divergence are picked up with minimal prior knowledge of a given language pair."
song-xia-2012-using,Using a Goodness Measurement for Domain Adaptation: A Case Study on {C}hinese Word Segmentation,2012,28,7,2,1,3941,yan song,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Domain adaptation is an important topic for natural language processing. There has been extensive research on the topic and various methods have been explored, including training data selection, model combination, semi-supervised learning. In this study, we propose to use a goodness measure, namely, description length gain (DLG), for domain adaptation for Chinese word segmentation. We demonstrate that DLG can help domain adaptation in two ways: as additional features for supervised segmenters to improve system performance, and also as a similarity measure for selecting training data to better match a test set. We evaluated our systems on the Chinese Penn Treebank version 7.0, which has 1.2 million words from five different genres, and the Chinese Word Segmentation Bakeoff-3 data."
tepper-etal-2012-statistical,Statistical Section Segmentation in Free-Text Clinical Records,2012,16,18,3,0,43328,michael tepper,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Automatically segmenting and classifying clinical free text into sections is an important first step to automatic information retrieval, information extraction and data mining tasks, as it helps to ground the significance of the text within. In this work we describe our approach to automatic section segmentation of clinical records such as hospital discharge summaries and radiology reports, along with section classification into pre-defined section categories. We apply machine learning to the problems of section segmentation and section classification, comparing a joint (one-step) and a pipeline (two-step) approach. We demonstrate that our systems perform well when tested on three data sets, two for hospital discharge summaries and one for radiology reports. We then show the usefulness of section information by incorporating it in the task of extracting comorbidities from discharge summaries."
wang-xia-2012-effort,Effort of Genre Variation and Prediction of System Performance,2012,22,0,2,0,12963,dong wang,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Domain adaptation is an important task in order for NLP systems to work well in real applications. There has been extensive research on this topic. In this paper, we address two issues that are related to domain adaptation. The first question is how much genre variation will affect NLP systems' performance. We investigate the effect of genre variation on the performance of three NLP tools, namely, word segmenter, POS tagger, and parser. We choose the Chinese Penn Treebank (CTB) as our corpus. The second question is how one can estimate NLP systems' performance when gold standard on the test data does not exist. To answer the question, we extend the prediction model in (Ravi et al., 2008) to provide prediction for word segmentation and POS tagging as well. Our experiments show that the predicted scores are close to the real scores when tested on the CTB data."
C12-2037,Improving Dependency Parsing with Interlinear Glossed Text and Syntactic Projection,2012,16,7,2,1,29629,ryan georgi,Proceedings of {COLING} 2012: Posters,0,"Producing annotated corpora for resource-poor languages can be prohibitively expensive, while obtaining parallel, unannotated corpora may be more easily achieved. We propose a method of augmenting a discriminative dependency parser using syntactic projection information. This modification will allow the parser to take advantage of unannotated parallel corpora where high-quality automatic annotation tools exist for one of the languages. We use corpora of interlinear glossed textxe2x80x94short bitexts commonly found in linguistic papers on resource-poor languages with an additional gloss line that supports word alignmentxe2x80x94and demonstrate this technique on eight different languages, including resource-poor languages such as Welsh, Yaqui, and Hausa. We find that incorporating syntactic projection information in a discriminative parser generally outperforms deterministic syntactic projection. While this paper uses small IGT corpora for word alignment, our method can be adapted to larger parallel corpora by using statistical word alignment instead."
C12-2116,Entropy-based Training Data Selection for Domain Adaptation,2012,14,6,3,1,3941,yan song,Proceedings of {COLING} 2012: Posters,0,"Training data selection is a common method for domain adaptation, the goal of which is to choose a subset of training data that works well for a given test set. It has been shown to be effective for tasks such as machine translation and parsing. In this paper, we propose several entropy-based measures for training data selection and test their effectiveness on two tasks: Chinese word segmentation and part-of-speech tagging. The experimental results on the Chinese Penn Treebank indicate that some of the measures provide a statistically significant improvement over random selection for both tasks."
W11-0711,Email Formality in the Workplace: A Case Study on the {E}nron Corpus,2011,-1,-1,3,0,16186,kelly peterson,Proceedings of the Workshop on Language in Social Media ({LSM} 2011),0,None
I11-1138,"Linguistic Phenomena, Analyses, and Representations: Understanding Conversion between Treebanks",2011,12,9,3,1,40828,rajesh bhatt,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Treebanks are valuable resources for natural language processing (NLP). There is much work in NLP which converts treebanks from one representation (e.g., phrase structure) to another (e.g., dependency) before applying machine learning. This paper provides a framework in which to think about the question of when such a conversion is possible."
W10-1109,Extracting Medication Information from Discharge Summaries,2010,10,12,2,0,41142,scott halgrim,Proceedings of the {NAACL} {HLT} 2010 Second Louhi Workshop on Text and Data Mining of Health Documents,0,"Extracting medication information from clinical records has many potential applications and was the focus of the i2b2 challenge in 2009. We present a hybrid system, comprised of machine learning and rule-based modules, for medication information extraction. With only a handful of template-filling rules, the system's core is a cascade of statistical classifiers for field detection. It achieved good performance that was comparable to the top systems in the i2b2 challenge, demonstrating that a heavily statistical approach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate additional resources such as medication name lists to further improve performance."
W10-0728,Preliminary Experiments with {A}mazon{'}s {M}echanical {T}urk for Annotating Medical Named Entities,2010,7,33,3,0,41013,meliha yetisgenyildiz,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"Amazon's Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research. In this paper, we report our findings in using MTurk to annotate medical text extracted from clinical trial descriptions with three entity types: medical condition, medication, and laboratory test. We compared MTurk annotations with a gold standard manually created by a domain expert. Based on the good performance results, we conclude that MTurk is a very promising tool for annotating large-scale corpora for biomedical NLP tasks."
bhatia-etal-2010-empty,Empty Categories in a {H}indi Treebank,2010,8,9,9,0,11651,archna bhatia,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We are in the process of creating a multi-representational and multi-layered treebank for Hindi/Urdu (Palmer et al., 2009), which has three main layers: dependency structure, predicate-argument structure (PropBank), and phrase structure. This paper discusses an important issue in treebank design which is often neglected: the use of empty categories (ECs). All three levels of representation make use of ECs. We make a high-level distinction between two types of ECs, trace and silent, on the basis of whether they are postulated to mark displacement or not. Each type is further refined into several subtypes based on the underlying linguistic phenomena which the ECs are introduced to handle. This paper discusses the stages at which we add ECs to the Hindi/Urdu treebank and why. We investigate methodically the different types of ECs and their role in our syntactic and semantic representations. We also examine our decisions whether or not to coindex each type of ECs with other elements in the representation."
xia-etal-2010-problems,The Problems of Language Identification within Hugely Multilingual Data Sets,2010,5,5,1,1,16067,fei xia,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"As the data for more and more languages is finding its way into digital form, with an increasing amount of this data being posted to the Web, it has become possible to collect language data from the Web and create large multilingual resources, covering hundreds or even thousands of languages. ODIN, the Online Database of INterlinear text (Lewis, 2006), is such a resource. It currently consists of nearly 200,000 data points for over 1,000 languages, the data for which was harvested from linguistic documents on the Web. We identify a number of issues with language identification for such broad-coverage resources including the lack of training data, ambiguous language names, incomplete language code sets, and incorrect uses of language names and codes. After providing a short overview of existing language code sets maintained by the linguistic community, we discuss what linguists and the linguistic community can do to make the process of language identification easier."
C10-2016,A comparison of unsupervised methods for Part-of-Speech Tagging in {C}hinese,2010,22,1,2,0,43743,alex cheng,Coling 2010: Posters,0,"We conduct a series of Part-of-Speech (POS) Tagging experiments using Expectation Maximization (EM), Variational Bayes (VB) and Gibbs Sampling (GS) against the Chinese Penn Tree-bank. We want to first establish a baseline for unsupervised POS tagging in Chinese, which will facilitate future research in this area. Secondly, by comparing and analyzing the results between Chinese and English, we highlight some of the strengths and weaknesses of each of the algorithms in POS tagging task and attempt to explain the differences based on some preliminary linguistics analysis. Comparing to English, we find that all algorithms perform rather poorly in Chinese in 1-to-1 accuracy result but are more competitive in many-to-1 accuracy. We attribute one possible explanation of this to the algorithms' inability to correctly produce tags that match the desired tag count distribution."
C10-1044,Comparing Language Similarity across Genetic and Typologically-Based Groupings,2010,12,18,2,1,29629,ryan georgi,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Recent studies have shown the potential benefits of leveraging resources for resource-rich languages to build tools for similar, but resource-poor languages. We examine what constitutes similarity by comparing traditional phylogenetic language groups, which are motivated largely by genetic relationships, with language groupings formed by clustering methods using typological features only. Using data from the World Atlas of Language Structures (WALS), our preliminary experiments show that typologically-based clusters look quite different from genetic groups, but perform as good or better when used to predict feature values of member languages."
W09-3036,A Multi-Representational and Multi-Layered Treebank for {H}indi/{U}rdu,2009,11,89,6,1,40828,rajesh bhatt,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"This paper describes the simultaneous development of dependency structure and phrase structure treebanks for Hindi and Urdu, as well as a PropBank. The dependency structure and the PropBank are manually annotated, and then the phrase structure treebank is produced automatically. To ensure successful conversion the development of the guidelines for all three representations are carefully coordinated."
W09-0307,Applying {NLP} Technologies to the Collection and Enrichment of Language Data on the Web to Aid Linguistic Research,2009,19,13,1,1,16067,fei xia,"Proceedings of the {EACL} 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education ({L}a{T}e{CH} {--} {SHELT}{\\&}R 2009)",0,"The field of linguistics has always been reliant on language data, since that is its principal object of study. One of the major obstacles that linguists encounter is finding data relevant to their research. In this paper, we propose a three-stage approach to help linguists find relevant data. First, language data embedded in existing linguistic scholarly discourse is collected and stored in a database. Second, the language data is automatically analyzed and enriched, and language profiles are created from the enriched data. Third, a search facility is provided to allow linguists to search the original data, the enriched data, and the language profiles in a variety of ways. This work demonstrates the benefits of using natural language processing technology to create resources and tools for linguistic research, allowing linguists to have easy access not only to language data embedded in existing linguistic papers, but also to automatically generated language profiles for hundreds of languages."
E09-2011,"Parsing, Projecting {\\&} Prototypes: Repurposing Linguistic Data on the Web",2009,7,3,2,0.952381,36762,william lewis,Proceedings of the Demonstrations Session at {EACL} 2009,0,"Until very recently, most NLP tasks (e.g., parsing, tagging, etc.) have been confined to a very limited number of languages, the so-called majority languages. Now, as the field moves into the era of developing tools for Resource Poor Languages (RPLs)--a vast majority of the world's 7,000 languages are resource poor--the discipline is confronted not only with the algorithmic challenges of limited data, but also the sheer difficulty of locating data in the first place. In this demo, we present a resource which taps the large body of linguistically annotated data on the Web, data which can be repurposed for NLP tasks. Because the field of linguistics has as its mandate the study of human language--in fact, the study of all human languages--and has whole-heartedly embraced the Web as a means for disseminating linguistic knowledge, the consequence is that a large quantity of analyzed language data can be found on the Web. In many cases, the data is richly annotated and exists for many languages for which there would otherwise be very limited annotated data. The resource, the Online Database of INterlinear text (ODIN), makes this data available and provides additional annotation and structure, making the resource useful to the Computational Linguistic audience."
E09-1099,Language {ID} in the Context of Harvesting Language Data off the Web,2009,19,23,1,1,16067,fei xia,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"As the arm of NLP technologies extends beyond a small core of languages, techniques for working with instances of language data across hundreds to thousands of languages may require revisiting and recalibrating the tried and true methods that are used. Of the NLP techniques that has been treated as solved is language identification (language ID) of written text. However, we argue that language ID is far from solved when one considers input spanning not dozens of languages, but rather hundreds to thousands, a number that one approaches when harvesting language data found on the Web. We formulate language ID as a coreference resolution problem and apply it to a Web harvesting task for a specific linguistic data type and achieve a much higher accuracy than long accepted language ID approaches."
W08-0202,"Building a Flexible, Collaborative, Intensive Master{'}s Program in Computational Linguistics",2008,8,1,2,0.205419,11448,emily bender,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"We present the design of a professional master's program in Computational Linguistics. This program can be completed in one-year of full-time study, or two-three years of part-time study. Originally designed for CS professionals looking for additional training, the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals."
W08-0206,The Evolution of a Statistical {NLP} Course,2008,11,2,1,1,16067,fei xia,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"This paper describes the evolution of a statistical NLP course, which I have been teaching every year for the past three years. The paper will focus on major changes made to the course (including the course design, assignments, and the use of discussion board) and highlight the lessons learned from this experience."
I08-6003,Finding parallel texts on the web using cross-language information retrieval,2008,14,1,2,0,22361,achim ruopp,Proceedings of the 2nd workshop on Cross Lingual Information Access ({CLIA}) Addressing the Information Need of Multilingual Societies,0,"Discovering parallel corpora on the web is a challenging task. In this paper, we use cross-language information retrieval techniques in combination with structural features to retrieve candidate page pairs from a commercial search engine. The candidate page pairs are then filtered using techniques described by Resnik and Smith (2003) to determine if they are translations. The results allow the comparison of efficiency of different parameter settings and provide an estimate for the percentage of pages that are parallel for a certain language pair."
I08-2093,Automatically Identifying Computationally Relevant Typological Features,2008,10,30,2,0.952381,36762,william lewis,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"In this paper we explore the potential for identifying computationally relevant typological features from a multilingual corpus of language data built from readily available language data collected off the Web. Our work builds on previous structural projection work, where we extend the work of projection to building individual CFGs for approximately 100 languages. We then use the CFGs to discover the values of typological parameters such as word order, the presence or absence of definite and indefinite determiners, etc. Our methods have the potential of being extended to many more languages and parameters, and can have significant effects on current research focused on tool and resource development for low-density languages and grammar induction from raw corpora."
I08-1003,A Hybrid Approach to the Induction of Underlying Morphology,2008,21,8,2,0,43328,michael tepper,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,We present a technique for re ning a baseline segmentation and generating a plausible underlying morpheme segmentation by integrating hand-written rewrite rules into an existing state-of-the-art unsupervised morphological induction procedure. Performance on measures which consider surface-boundary accuracy and underlying morpheme consistency indicates this technique leads to improvements over baseline segmentations for English and Turkish word lists.
I08-1069,Repurposing Theoretical Linguistic Data for Tool Development and Search,2008,30,11,1,1,16067,fei xia,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"For the majority of the worldxe2x80x99s languages, the number of linguistic resources (e.g., annotated corpora and parallel data) is very limited. Consequently, supervised methods, as well as many unsupervised methods, cannot be applied directly, leaving these languages largely untouched and unnoticed. In this paper, we describe the construction of a resource that taps the large body of linguistically analyzed language data that has made its way to the Web, and propose using this resource to bootstrap NLP tool development."
N07-1057,Multilingual Structural Projection across Interlinear Text,2007,15,37,1,1,16067,fei xia,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"This paper explores the potential for annotating and enriching data for low-density languages via the alignment and projection of syntactic structure from parsed data for resource-rich languages such as English. We seek to develop enriched resources for a large number of the worldxe2x80x99s languages, most of which have no significant digital presence. We do this by tapping the body of Web-based linguistic data, most of which exists in small, analyzed chunks embedded in scholarly papers, journal articles, Web pages, and other online documents. By harvesting and enriching these data, we can provide the means for knowledge discovery across the resulting corpus that can lead to building computational resources such as grammars and transfer rules, which, in turn, can be used as bootstraps for building additional tools and resources for the languages represented. 1"
W06-0104,"Features, Bagging, and System Combination for the {C}hinese {POS} Tagging Task",2006,12,1,1,1,16067,fei xia,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"In recent years more and more NLP packages become available to the public, and many of them are implementations of general machine learning methods. A natural question is how one can quickly build a good system using those packages. To address this issue, we built three part-of-speech taggers (i.e., trigram, TBL, and MaxEnt taggers) for Chinese using existing packages. Our experiments showed that adapting and extending a package is relative easy if the package is wellwritten and source code is available. We studied the contribution of each type of feature templates to the tagging accuracy and showed that adding some templates could help one tagger but hurt another one. Furthermore, we demonstrated that bagging (Breiman, 1996) provides a moderate gain for the TBL tagger, and combining TBL and MaxEnt taggers work better than using all three taggers."
C04-1073,Improving a Statistical {MT} System with Automatically Learned Rewrite Patterns,2004,20,242,1,1,16067,fei xia,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Current clump-based statistical MT systems have two limitations with respect to word ordering: First, they lack a mechanism for expressing and using generalization that accounts for reorderings of linguistic phrases. Second, the ordering of target words in such systems does not respect linguistic phrase boundaries. To address these limitations, we propose to use automatically learned rewrite patterns to preprocess the source sentences so that they have a word order similar to that of the target language. Our system is a hybrid one. The basic model is statistical, but we use broad-coverage rule-based parsers in two ways - during training for learning rewrite patterns, and at runtime for reordering the source sentences. Our experiments show 10% relative improvement in Bleu measure."
N03-4001,{TIPS}: A Translingual Information Processing System,2003,5,3,12,0,8332,yaser alonaizan,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"Searching online information is increasingly a daily activity for many people. The multilinguality of online content is also increasing (e.g. the proportion of English web users, which has been decreasing as a fraction the increasing population of web users, dipped below 50% in the summer of 2001). To improve the ability of an English speaker to search mutlilingual content, we built a system that supports cross-lingual search of an Arabic newswire collection and provides on demand translation of Arabic web pages into English. The cross-lingual search engine supports a fast search capability (sub-second response for typical queries) and achieves state-of-the-art performance in the high precision region of the result list. The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality. The on demand SMT uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents."
N03-2036,A Phrase-based Unigram Model for Statistical Machine Translation,2003,8,65,2,0,38270,christoph tillmann,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks - pairs of phrases. During decoding, we use a block unigram model and a word-based trigram language model. During training, the blocks are learned from source interval projections using an underlying word alignment. We show experimental results on block selection criteria based on unigram counts and phrase length."
H01-1014,Converting Dependency Structures to Phrase Structures,2001,9,97,1,1,16067,fei xia,Proceedings of the First International Conference on Human Language Technology Research,0,"Treebanks are of two types according to their annotation schemata: phrase-structure Treebanks such as the English Penn Treebank [8] and dependency Treebanks such as the Czech dependency Treebank [6]. Long before Treebanks were developed and widely used for natural language processing, there had been much discussion of comparison between dependency grammars and context-free phrase-structure grammars [5]. In this paper, we address the relationship between dependency structures and phrase structures from a practical perspective; namely, the exploration of different algorithms that convert dependency structures to phrase structures and the evaluation of their performance against an existing Treebank. This work not only provides ways to convert Treebanks from one type of representation to the other, but also clarifies the differences in representational coverage of the two approaches."
W00-2030,A Corpus-based evaluation of syntactic locality in {TAG}s,2000,0,4,1,1,16067,fei xia,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,None
W00-2041,Comparing and integrating {T}ree {A}djoining {G}rammars,2000,1,2,1,1,16067,fei xia,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,None
W00-1605,Some Experiments on Indicators of Parsing Complexity for Lexicalized Grammars,2000,9,27,2,0,8364,anoop sarkar,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,"In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms."
W00-1307,A Uniform Method of Grammar Extraction and Its Applications,2000,26,44,1,1,16067,fei xia,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"Grammars are core elements of many NLP applications. In this paper, we present a system that automatically extracts lexicalized grammars from annotated corpora. The data produced by this system have been used in several tasks, such as training NLP tools (such as Supertaggers) and estimating the coverage of hand-crafted grammars. We report experimental results on two of those tasks and compare our approaches with related work."
W00-1208,"Comparing Lexicalized Treebank Grammars Extracted from {C}hinese, {K}orean, and {E}nglish Corpora",2000,9,9,1,1,16067,fei xia,Second {C}hinese Language Processing Workshop,0,"In this paper, we present a method for comparing Lexicalized Tree Adjoining Grammars extracted from annotated corpora for three languages: English, Chinese and Korean. This method makes it possible to do a quantitative comparison between the syntactic structures of each language, thereby providing a way of testing the Universal Grammar Hypothesis, the foundation of modern linguistic theories."
xia-etal-2000-developing,Developing Guidelines and Ensuring Consistency for {C}hinese Text Annotation,2000,-1,-1,1,1,16067,fei xia,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
W98-0143,Consistent grammar development using partial-tree descriptions for {L}exicalized {T}ree-{A}djoining {G}rammars,1998,3,14,1,1,16067,fei xia,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
W97-1505,Maintaining the Forest and Burning out the Underbrush in {XTAG},1997,10,7,7,0,42791,christine doran,Computational Environments for Grammar Development and Linguistic Engineering,0,None
P97-1046,A Comparison of Head Transducers and Transfer for a Limited Domain Translation Application,1997,13,17,3,0,41856,hiyan alshawi,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We compare the effectiveness of two related machine translation models applied to the same limited-domain task. One is a transfer model with monolingual head automata for analysis and generation; the other is a direct transduction model based on bilingual head transducers. We conclude that the head transducer model is more effective according to measures of accuracy, computational requirements, model size, and development effort."
