2002.tmi-papers.20,J93-2003,0,0.170703,"by applying hierarchical phrase alignment. The hierarchical phrase alignment is a method to align bilingual sentences phrase-by-phrase employing the partial parse results. Based on the hierarchical phrase alignment, a translation model is trained on a chunked corpus by converting hierarchically aligned phrases into a sequence of chunks. The second method transforms the bilingual correspondence of the phrase alignments into that of translation model. Both of our approaches yield better quality of the translaiton model. 1 Introduction A statistical machine translation (SMT), first introduced by Brown et al. (1993), represents a translation process as a noisy channel model that consists of a source-channel model, a translation model and a prior, language model of target language texts. This transformed the problem of machine translation into a maximum posteriori solution to the source-channel paradigm. The translation model is based on word-for-word translation and limited to allow only one channel source word to be aligned from a channel target word. Although phrasal correspondence is implicitly implemented into some translation models by means of distortion, careful parameter training is required. In"
2002.tmi-papers.20,C92-2101,0,0.0830307,"nger input length. The results above demonstrate that the translation model parameters derived from the hierarchical phrase alignment were better than those acquired only by EM-training on a given corpus. One of the advantages of using hierarchical phrase alignment is that it is neutral to language pairs: They can share the same parsing system with a simple algorithm for aligning texts. The next advantage is the robustness to the input, since the phrase alignments are extracted from partial parse results. These advantages were not available in alignment methods of Yamamoto & Matsumoto (2000); Kaji et al. (1992). In addition, hierarchical phrase alignment is different from other chunking methods bacause it can preserve the correspondence in bilingual texts. Although the proposed method here did not use the higher level structures in the hierarchically aligned phrases, it will be challenging to incorporate those alignments restricted by non-terminal correspondences. The quality of translation is expected to be improved by including the restricted alignments into training steps. This idea is based on pegging (Brown et al. 1993) or from the work of Och & Ney (2000), in which a subset of all the alignmen"
2002.tmi-papers.20,J99-4005,0,0.129977,"he search problem is a critical issue for the success of statistical machine translation. The decoder, or the search system, should induce the source string from a sequence of target words by utilizing clues from a large numbers of parameters. Basically, if the vocabulary size is 10,000 and the output sentence length is 10, then 1000010 possible candidates must be enumerated. In addition, since the source sentence length is unknown to the decoder, the search system should also infer the total length of output at the same time. For details of the search problem, refer to Germann et al. (2001); Knight (1999); Och et al. (2001). 3 Hierarchical Phrase Alignment Hierarchical phrase alignment, proposed by Imamura (2001), computes the correspondence of sub-trees between source language and target language parse trees based on partial parse results. A phrase alignment is defined as an equivalent sequence of words between bilingual sentences, and it may be a sequence of words representing noun phrases and/or verb phrases etc. For instance, the sentence pairs, E: J: I have just arrived in Kyoto . kyoto ni tsui ta bakari desu . consists of three phrase alignments: in Kyoto arrived in Kyoto have just arriv"
2002.tmi-papers.20,W01-1405,0,0.0146108,"t word, B( f ), relative to the center of the previous source word, k. – d&gt;1 ( j − j0 |B( f )) : Distortion probability for non-head words. The position of a non-head word j is determined by the word class and relative to the previous target word generated from the same source word ( j0 ). • NULL Translation Model — p1 : A fixed probability of inserting a NULL word after determining each target word f (p0 = 1 − p1 ). For details, refer to Brown et al. (1993). 2.2 Problems in Statistical Machine Translation In statistical machine translation, there exists three key problems as described below (Ney 2001): Modeling Problem As this model suggests, a target word can be aligned to only a single source word. This restriction prohibits, for instance in Figure 1, “teitadake” from being mapped to both “could” and “you”, but allows only “could” to be mapped, and the other remaining source word, “you”, is treated as a zero fertility word. Och et al. (1999) introduced the concept of a translation template that could capture the phrase level correspondence, though the model relied on the HMM based translation model and could not be directly applied to fertility models such as the IBM Model 4. Training Pr"
2002.tmi-papers.20,P00-1056,0,0.826996,"el target word. Although phrasal correspondence is implicitly implemented into some translation models by means of distortion, careful parameter training is required. In addition, the training procedure relies on the EM algorithm, which can converge to an optimal solution but does not assured the global maximum parameter assignment. Furthermore, the translation models are represented by the numbers of parameters, so that easily suffered from the overfitting problem. In order to overcome these problems, simpler models, such as word-for-word translation models (Brown et al. 1993) or HMM models (Och & Ney 2000), have been introduced to determine the initial parameters and to bootstrap the training. This paper describes two methods to overcome the above problems by using hierarchical phrase alignment (Imamura 2001). Hierarchical phrase alignment (HPA) is a method to align bilingual texts phrase-by-phrase from partial parse results. One method converts the hierarchically aligned phrasal texts into a pair of sequences of chunks of words, treating the word-forword translation model as a chunk-for-chunk translation model. The second method computes the parameters for the translation model from the comput"
2002.tmi-papers.20,W99-0604,0,0.0797365,"y of inserting a NULL word after determining each target word f (p0 = 1 − p1 ). For details, refer to Brown et al. (1993). 2.2 Problems in Statistical Machine Translation In statistical machine translation, there exists three key problems as described below (Ney 2001): Modeling Problem As this model suggests, a target word can be aligned to only a single source word. This restriction prohibits, for instance in Figure 1, “teitadake” from being mapped to both “could” and “you”, but allows only “could” to be mapped, and the other remaining source word, “you”, is treated as a zero fertility word. Och et al. (1999) introduced the concept of a translation template that could capture the phrase level correspondence, though the model relied on the HMM based translation model and could not be directly applied to fertility models such as the IBM Model 4. Training Problem Training for the various parameters, t, n, p1 , d1 , d&gt;1 relies on the EM algorithm, which optimizes the log-likelihood of the model over a given bilingual corpus. The EM algorithm can find an optimal solution, although it cannot assure finding the globally best one. As the number of parameters is larger than those of speech it will become e"
2002.tmi-papers.20,W01-1408,0,0.209859,"em is a critical issue for the success of statistical machine translation. The decoder, or the search system, should induce the source string from a sequence of target words by utilizing clues from a large numbers of parameters. Basically, if the vocabulary size is 10,000 and the output sentence length is 10, then 1000010 possible candidates must be enumerated. In addition, since the source sentence length is unknown to the decoder, the search system should also infer the total length of output at the same time. For details of the search problem, refer to Germann et al. (2001); Knight (1999); Och et al. (2001). 3 Hierarchical Phrase Alignment Hierarchical phrase alignment, proposed by Imamura (2001), computes the correspondence of sub-trees between source language and target language parse trees based on partial parse results. A phrase alignment is defined as an equivalent sequence of words between bilingual sentences, and it may be a sequence of words representing noun phrases and/or verb phrases etc. For instance, the sentence pairs, E: J: I have just arrived in Kyoto . kyoto ni tsui ta bakari desu . consists of three phrase alignments: in Kyoto arrived in Kyoto have just arrived in Kyoto — — — k"
2002.tmi-papers.20,1999.mtsummit-1.34,1,0.807335,"Missing"
2002.tmi-papers.20,C00-2135,0,0.102903,"+train model is better for longer input length. The results above demonstrate that the translation model parameters derived from the hierarchical phrase alignment were better than those acquired only by EM-training on a given corpus. One of the advantages of using hierarchical phrase alignment is that it is neutral to language pairs: They can share the same parsing system with a simple algorithm for aligning texts. The next advantage is the robustness to the input, since the phrase alignments are extracted from partial parse results. These advantages were not available in alignment methods of Yamamoto & Matsumoto (2000); Kaji et al. (1992). In addition, hierarchical phrase alignment is different from other chunking methods bacause it can preserve the correspondence in bilingual texts. Although the proposed method here did not use the higher level structures in the hierarchically aligned phrases, it will be challenging to incorporate those alignments restricted by non-terminal correspondences. The quality of translation is expected to be improved by including the restricted alignments into training steps. This idea is based on pegging (Brown et al. 1993) or from the work of Och & Ney (2000), in which a subset"
2002.tmi-papers.20,P01-1030,0,\N,Missing
2003.mtsummit-papers.54,C02-1076,1,0.940109,"“o4 ” should be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-based Decoder Instead of decoding word-by-word and generating an output string word-by-word, as seen in beam search strategies, this paper presents an alternative strategy taken after the framework of example-based machine translation: Retrieve a translation example from a parallel corpus whose source part is similar to the input sentence, then slightly modify the target part of the example"
2003.mtsummit-papers.54,J93-2003,0,0.0952076,"ine translation framework. The decoder presented here is based on the greedy approach to the decoding problem, but the search is initiated from a similar translation extracted from a bilingual corpus. The experiments on multilingual translations showed that the proposed method was far superior to a word-by-word generation beam search algorithm. 1 Introduction E = NULL0 show1 me2 The framework of statistical machine translation formulates the problem of translating a sentence in a language J into another language E as the maximization problem of the conditional probability Eˆ = argmaxE P(E|J) (Brown et al., 1993). The application of the Bayes Rule resulted in Eˆ = argmaxE P(E)P(J|E). The former term P(E) is called a language model, representing the likelihood of E. The latter term P(J|E) is called a translation model, representing the generation probability from E into J. Under this concept, Brown et al. (1993) presented a translation model where a source sentence is mapped to a target sentence with the notion of word alignment1 . Although it has been successfully applied to similar language pairs, such as French– English and German–English, little success has been achieved for drastically different l"
2003.mtsummit-papers.54,C00-1019,0,0.0234736,"earch process from the combined phrasal examples if the similarity scores of any examples are below a certain threshold. The example-based decoder can share an alternative view point: it is an example-based translation system, but uses statistically acquired knowledge to generate translations. Conventional example-based MT systems consist of three parts: the extraction of examples into storage, retrieval and the modification of examples when given an input. They can store examples either by sentence (Sumita, 2001), by fragment or by phrase (Nagao, 1984; Watanabe and Maruyama, 1994; Way, 1999; Brown, 2000; Richardson et al., 2001), and adjust fetched similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is,"
2003.mtsummit-papers.54,P98-2158,0,0.0894867,"Missing"
2003.mtsummit-papers.54,P01-1030,0,0.476307,"to merging statistical and example-based machine translation (Nagao, 1984). J= A=( 1 The source/target sentences are the channel model’s source/target that correspond to the translation system’s output/input. the3 uindo1 no2 shinamono3 7 0 4 one4 o4 0 in5 the6 window7 mise5 tekudasai6 1 1 ) Figure 1: Example of word alignment Given an input, the decoder first finds some translation examples whose source part is similar to the input. Second, it modifies the retrieved translation using the greedy search algorithm, a hill-climbing approach to find a (hopefully good) translation as introduced by Germann et al. (2001). Translation experiments were carried out between four different languages pairs, Chinese, English, Japanese and Korean, and it was verified that in any directions, the proposed decoder was superior to the beam search based decoder. The following section briefly reviews the word alignment based statistical machine translation, then proposes the example-based decoder. Section 4 shows experiments on various language pairs, followed by discussion. 2 Statistical Machine Translation Word alignment based statistical translation represents bilingual correspondence by the notion of word alignment A,"
2003.mtsummit-papers.54,W02-1018,0,0.0187635,"with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is, such as the syntaxbased translation model (Yamada and Knight, 2001) or the phrase-based translation model (Marcu and Wong, 2002), the more quality improvement will be expected. Furthermore, the example-based decoder can be adapted to the error correction framework with more accurate translation models. The greedy decoding process can be initiated from the translations from other systems, such as a rule-based MT system or an example-based MT system, instead of from translation examples extracted solely from a bilingual corpus. 6 Conclusion This paper presented an example-based decoding algorithm for statistical machine translation, which can offer both of the benefits of example-based and statistical machine translation"
2003.mtsummit-papers.54,P01-1050,0,0.354659,".4/42.8 10.5/63.4 16.3/51.7 14.5/39.5 14.0/51.5 19.5/42.5 15.0/32.5 non-matched 28.2/38.6 30.3/24.9 25.7/29.5 39.1/26.8 33.4/26.2 31.8/32.8 37.6/31.4 35.8/28.1 50.3/10.9 31.9/28.7 31.3/31.9 27.7/12.3 total 21.4/48.8 23.1/24.9 20.0/37.6 31.0/34.1 27.8/29.4 24.7/36.9 28.4/42.2 29.2/36.1 38.2/20.6 24.9/37.6 26.7/36.1 22.7/20.2 tions, not from merely guessed sentences by an input string. In general, the greedy method is strongly influenced by the initial state of the search, but our method provides the strong bias especially required for long distance language pairs, such as Japanese and English. Marcu (2001) proposed a slightly different approach in which translation examples are extracted phrase-by-phrase into a translation memory and the search process is initiated by concatenating the phrases found in the memory. Both share similar input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: input: reference: beam: example: retrieved: 銀行 の 前 で バッグ を ひったくら れ まし た i was robbed of my bag in front of the bank my bag was stolen in the front o"
2003.mtsummit-papers.54,P02-1038,0,0.12391,"ne of the easiest problems, while the English-to-Chinese will be the hardest direction to translate. 4.3 Evaluation The translation experiments were carried out on 510 sentences selected randomly from the test set. Two decoding algorithms were tested. One is the example-based decoder as explained in Section 3, and the other is a left-to-right output generation beam search algorithm as presented by Watanabe and Sumita (2002). The metrics for this evaluation were as follows: WER: Word-error-rate, which penalizes the edit distance (insertion/deletion/substitution) against reference translations (Och and Ney, 2002). English Japanese 167,163 980,790 1,148,428 15,641 21,896 5,547 9,220 35.35 24.06 Korean 1,269,888 13,395 4,191 20.34 PER: Position independent WER, which penalizes without considering positional disfluencies (Och and Ney, 2002). BLEU: BLEU score, which computes the ratio of the n-gram for the translation results found in reference translations (Papineni et al., 2002). Contrary to the above error metrics, the higher scores indicates better translations. SE: Subjective evaluation ranks ranging from A to D (A : perfect, B : fair, C : acceptable and D : nonsense), judged by a native speaker. The"
2003.mtsummit-papers.54,W01-1408,0,0.424899,"tistical and example-based machine translation (Nagao, 1984). J= A=( 1 The source/target sentences are the channel model’s source/target that correspond to the translation system’s output/input. the3 uindo1 no2 shinamono3 7 0 4 one4 o4 0 in5 the6 window7 mise5 tekudasai6 1 1 ) Figure 1: Example of word alignment Given an input, the decoder first finds some translation examples whose source part is similar to the input. Second, it modifies the retrieved translation using the greedy search algorithm, a hill-climbing approach to find a (hopefully good) translation as introduced by Germann et al. (2001). Translation experiments were carried out between four different languages pairs, Chinese, English, Japanese and Korean, and it was verified that in any directions, the proposed decoder was superior to the beam search based decoder. The following section briefly reviews the word alignment based statistical machine translation, then proposes the example-based decoder. Section 4 shows experiments on various language pairs, followed by discussion. 2 Statistical Machine Translation Word alignment based statistical translation represents bilingual correspondence by the notion of word alignment A,"
2003.mtsummit-papers.54,P02-1040,0,0.0729813,"search algorithm as presented by Watanabe and Sumita (2002). The metrics for this evaluation were as follows: WER: Word-error-rate, which penalizes the edit distance (insertion/deletion/substitution) against reference translations (Och and Ney, 2002). English Japanese 167,163 980,790 1,148,428 15,641 21,896 5,547 9,220 35.35 24.06 Korean 1,269,888 13,395 4,191 20.34 PER: Position independent WER, which penalizes without considering positional disfluencies (Och and Ney, 2002). BLEU: BLEU score, which computes the ratio of the n-gram for the translation results found in reference translations (Papineni et al., 2002). Contrary to the above error metrics, the higher scores indicates better translations. SE: Subjective evaluation ranks ranging from A to D (A : perfect, B : fair, C : acceptable and D : nonsense), judged by a native speaker. The scores are evaluated by the ratio of A ranked sentences, A+B for either A or B ranks, and A+B+C for either A, B or C ranks. We have evaluated only a languageto-English and a language-to-Japanese assuming that they are translations for Japanese-toEnglish and English-to-Japanese, respectively 2. For all the languages, 16 reference translations were created for the non-s"
2003.mtsummit-papers.54,W01-1402,0,0.0162967,"from the combined phrasal examples if the similarity scores of any examples are below a certain threshold. The example-based decoder can share an alternative view point: it is an example-based translation system, but uses statistically acquired knowledge to generate translations. Conventional example-based MT systems consist of three parts: the extraction of examples into storage, retrieval and the modification of examples when given an input. They can store examples either by sentence (Sumita, 2001), by fragment or by phrase (Nagao, 1984; Watanabe and Maruyama, 1994; Way, 1999; Brown, 2000; Richardson et al., 2001), and adjust fetched similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is, such as the syntaxbased t"
2003.mtsummit-papers.54,W01-1401,1,0.800277,"problem by combining two different unit sizes, by allowing a decoder to initiate the greedy search process from the combined phrasal examples if the similarity scores of any examples are below a certain threshold. The example-based decoder can share an alternative view point: it is an example-based translation system, but uses statistically acquired knowledge to generate translations. Conventional example-based MT systems consist of three parts: the extraction of examples into storage, retrieval and the modification of examples when given an input. They can store examples either by sentence (Sumita, 2001), by fragment or by phrase (Nagao, 1984; Watanabe and Maruyama, 1994; Way, 1999; Brown, 2000; Richardson et al., 2001), and adjust fetched similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assign"
2003.mtsummit-papers.54,takezawa-etal-2002-toward,1,0.936794,"be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-based Decoder Instead of decoding word-by-word and generating an output string word-by-word, as seen in beam search strategies, this paper presents an alternative strategy taken after the framework of example-based machine translation: Retrieve a translation example from a parallel corpus whose source part is similar to the input sentence, then slightly modify the target part of the example"
2003.mtsummit-papers.54,C00-2123,0,0.032521,"ted by the structural differences: i.e., English takes an SVO structure while Japanese usually takes the form of SOV. In addition, insertion and deletion occur very frequently as seen in the example. For instance, there exists no corresponding Japanese morphemes for “the3 ” and “the6 ”. Therefore, they should be inserted when translating from Japanese. Similarly, Japanese morphemes “no2 ” and “o4 ” should be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-bas"
2003.mtsummit-papers.54,C02-1050,1,0.906411,"ccur very frequently as seen in the example. For instance, there exists no corresponding Japanese morphemes for “the3 ” and “the6 ”. Therefore, they should be inserted when translating from Japanese. Similarly, Japanese morphemes “no2 ” and “o4 ” should be deleted. Both the intricate alignments and the insertion/deletion of words lead to a computationally expensive process when decoding by a wordby-word beam search algorithm as presented by Tillmann and Ney (2000). Due to its complexity, many pruning strategies have to be introduced, such as beam pruning (Och et al., 2001), fertility pruning (Watanabe and Sumita, 2002) or word-for-word translation pruning (Garc´ıa-Vaera et al., 1998), so that the search system can output results in a reasonable time. However, search errors become inevitable under the restricted search space. As Akiba et al. (2002) pointed out, though there exist some correlations between translation quality and the probabilities assigned by the translation model, the beam search was often unable to find good translations. 3 Example-based Decoder Instead of decoding word-by-word and generating an output string word-by-word, as seen in beam search strategies, this paper presents an alternativ"
2003.mtsummit-papers.54,P01-1067,0,0.0538691,"ed similar translation samples while translating. The difference with the proposed method lies in the process of transforming examples to match the input sentence. A conventional example-based MT system basically uses bilingual dictionaries, while the example-based decoder uses statistical translation models to adjust examples. The adaptation of the statistical model is justified by the correlation between the quality of translations and the probability assigned by the model (Akiba et al., 2002). Therefore, the more accurate the translation model is, such as the syntaxbased translation model (Yamada and Knight, 2001) or the phrase-based translation model (Marcu and Wong, 2002), the more quality improvement will be expected. Furthermore, the example-based decoder can be adapted to the error correction framework with more accurate translation models. The greedy decoding process can be initiated from the translations from other systems, such as a rule-based MT system or an example-based MT system, instead of from translation examples extracted solely from a bilingual corpus. 6 Conclusion This paper presented an example-based decoding algorithm for statistical machine translation, which can offer both of the"
2004.iwslt-evaluation.2,J90-2002,0,0.724612,"imal solution due to the enormous search space. However, SMT can sort translations in the order of their quality according to its statistical models. We show two different EBMT systems here, briefly explain each system, and then compare them. Finally, we ex1. Introduction There are two main strategies used in corpus-based translation: 1. Example-Based Machine Translation (EBMT) [1]: EBMT uses the corpus directly. EBMT retrieves the translation examples that are best matched to an input expression and then adjusts the examples to obtain the translation. 2. Statistical Machine Translation (SMT) [2]: SMT learns statistical models for translation from corpora and dictionaries and then searches for the best translation at run-time according to the statistical models for language and translation. By using the IWSLT04 task, this paper describes two endeavors that are independent at this moment: (a) a hybridization of EBMT and statistical models, and (b) a new approach for SMT, phrase-based HMM. (a) is used in the “unrestricted” Japanese-to-English track (Section 2), and (b) is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technolog"
2004.iwslt-evaluation.2,W01-1401,1,0.863919,"on is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven transDucer), which exploits DP-matching between word sequences. Let’s illustrate the process with a simple sample below. Suppose we are translating a Japanese sentence into English. The Japanese input sentence (1-j) is translated into the English sentence (1-e) by utilizing the English sentence (2-e), whose source sentence (2-j) is similar to (1-j). The common parts are unchanged, and the different portions, shown in bold face, are substituted by consulting a bilingual dictionary. ;;; A Japanese input (1-j) iro/ga/ki/ni/iri/masen ;;; the most similar example in co"
2004.iwslt-evaluation.2,P91-1024,1,0.754066,"source sentence of examples from a bilingual corpus. For this, we use DP-matching, which tells us the edit distance between word sequences while giving us the matched portions between the input and the example. The edit distance is calculated as follows. The count of the inserted words, the count of the deleted words, and the semantic distance of the substituted words are summed. Then, this total is normalized by the sum of the lengths of the input and the source part of translation example. The semantic distance between two substituted words is calculated by using the hierachy of a thesaurus[4]. Our language resources in addition to a bilingual corpus are a bilingual dictionary, which is used for generating target sentences, and thesauri of both languages, which are used for incorporating the semantic distance between words into the distance between word sequences. Furthermore, lexical resources are also used for word alignment. Table 1: Resources used for two EBMTs in IWSLT04 unresticted Japanese-to-English track. bilingual corpus bilingual dictionary thesaurus grammar D3 travel domain (20K) in-house in-house N.A. HPAT travel domain (20K) in-house in-house in-house D3 achieves a go"
2004.iwslt-evaluation.2,C94-1015,0,0.0154509,"is used in “supplied” Japanese-to-English and Chinese-toEnglish tracks (Section 3). In addition, paraphrasing technologies, which are not used in the IWSLT04 task but boost translation performance, are also introduced in Section 4. 13 plain the selector used to determine the best from multiple translations based on SMT models. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial"
2004.iwslt-evaluation.2,P03-1057,1,0.787621,"s. of the same syntactic category. Imamura [6] subsequently proposed HPA-based translation (HPAT). HPAed bilingual trees include all information necessary to automatically generate transfer patterns. Translation is done according to transfer patterns using the TDMT engine [7]. First, the source part of transfer patterns are utilized, and source structure is obtained. Second, structural changes are performed by mapping source patterns to target patterns. Finally, lexical items are inserted by referring to a bilingual dictionary, and then a conventional generation is performed. Finally, Imamura [8] proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules. BLEU was utilized to measure translation quality for the feedback process, and the hillclimbing algorithm was applied in searching for the combinatorial optimization. Utilizing the features of this task, incorrect/redundant rules were removed from the initial solution, which contains all rules acquired from the training corpus. Our experiments showed a considerable improvement in MT quality. 2.2. Two EBMTs 2.2.1. D3, DP-based EBMT Sumita [3] proposed D3 (Dp-match Driven tran"
2004.iwslt-evaluation.2,C02-1076,1,0.866736,"7.00 70.00 77.60 83.40 16.60 HPAT 38.60 59.80 77.40 83.40 16.60 SELECT 59.80 73.00 82.40 87.80 12.20 DIFF. +2.80 +3.00 +4.80 +4.40 -4.40 Next, the relationship between translation quality of element systems and gain by the selector was analyzed. Table 5 shows that the proposed selector reduces the number of low-quality translations (ranked “D”) while it increases the number of high-quality translations (ranked “S” to “B”). 2.3. SMT-based Selector We proposed an SMT-based method of automatically selecting the best translation among outputs generated by multiple machine translation (MT) systems [9]. Conventional approaches to the selection problem include a method that automatically selects the output to which the highest probability is assigned according to a language model (LM). [10] These existing methods have two problems. First, they do not check whether information on source sentences is adequately translated into MT outputs, although they do check the fluency of MT outputs. Second, they do not take the statistical behavior of assigned scores into consideration. The proposed approach scores MT outputs by using not only the language but also a translation model (TM). To conduct a s"
2004.iwslt-evaluation.2,hogan-frederking-1998-evaluation,0,0.0713962,"e drastic reduction in mWER has been demonstrated (Table 6). However, the quality with the small corpus is not so bad in the subjective evaluation shown in Table 7. We conjecture that adequacy is not low even with the supplied corpus, and the translation become similar to native English, that is, its fluency improves as the size of corpus increases. 2.4. Results 2.4.1. Selecting Effect 2.5. Discussion As shown in Table 4, all of the metrics taken together show that the proposed selector outperforms both element transRelated works have proposed ways to merge MT outputs from multiple MT systems [12] in order to output better translations. When the source language and the target language have similar sentence structures, this merging apGood: easy to understand, with either some unimportant information missing or flawed grammar; (C) Fair: broken, but understandable with effort; (D) Unacceptable: important information has been translated incorrectly. 15 translations with additional constraints [17, 18, 19]:  P (¯fi |¯ ea i ) P (f |e) ≈ Table 7: ATR’s Overall Subjective Evaluation - IWSLT supplied corpus. S S,A S,A,B S,A,B,C D D3 34.80 47.40 62.60 73.40 26.60 HPAT 25.20 44.20 70.40 80.40 19"
2004.iwslt-evaluation.2,P01-1050,0,0.0620593,"age and the target language have different sentence structures, such as English and Japanese, we often have translations whose structures are different from each other for a single input sentences. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sent"
2004.iwslt-evaluation.2,P01-1030,0,0.0810071,"nces. Thus, the authors regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihoo"
2004.iwslt-evaluation.2,2003.mtsummit-papers.54,1,0.856638,"s regard the merging approach as less suitable than the approach of selecting. Hybridization can be implemented in several arichitectures, for example, SMT followed by EBMT, SMT and EBMT in parallel, and so on. Which archtecture is best is still an interesting open question. In addition to the merging and selecting approaches, a modification approach can be taken. For example, Marcu [14] proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding [15]. Watanabe et al. [16] proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. ¯|e) is further decomposed into three terms: The term P (f , ¯f , e ¯ ¯|e) = P (f |¯f , e ¯, e)P (¯f |¯ P (f , f , e e, e)P (¯ e|e) (5) The first term of Equation 5 represents the probability that a segmented input sentence ¯f can be reordered and generated as the input text of f . The second term indicates the ¯ and translation probability of the two phrase sequences of e ¯f . The last term is the likelihood of the phrase-segmen"
2004.iwslt-evaluation.2,N03-1017,0,0.00448749,"ndling long Japanese input. The latter was attributed to the fact that we tuned our parameter to mWER and we exploited phrase models as well. Table 8: Evaluation - IWSLT Chinese-to-English supplied task. System Top Our Bottom (15) where count(¯ e, f¯) is the cooccurrence frequency of the two phrases e¯ and f¯. The basic idea of Equation 15 is to capture the bilingual correspondence while considering two directions. Additional phrases were exhaustively induced based on the intersection/union of the viterbi word alignments of the two directional models, P (e|f ) and P (f |e), computed by GIZA++ [17]. After the extraction of phrase translation pairs, their monolingual phrase lexicons were extracted and used as the possible segmentation for the source and target sentences. mWER 45.59 46.99 61.69 Fluency 38.20 38.20 25.04 Adequacy 33.38 29.50 29.06 4. Other Features of C3 This section introduces another feature of C3: paraphrasing and filtering corpora, which are not used in the IWSLT04 task but are useful for boosting MT performance. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. Specifically, theis variety makes it m"
2004.iwslt-evaluation.2,2003.mtsummit-papers.53,0,0.0727491,"Missing"
2004.iwslt-evaluation.2,W03-1001,0,0.0212046,"Missing"
2004.iwslt-evaluation.2,W04-3216,0,0.0140311,"regarded as the distortion tion 5, the term P (f |¯f , e probability of how a phrase segmented sentence ¯f will be reordered to form the source sentence f . Instead, we model this as the likelihood of a particular phrase segment ¯fj observed in f : ¯, e) ∝ P (f |¯f , e P (¯f |f )  P (¯fj |f ) ≈ Equation 12 can be regarded as a Hidden Markov Model in ¯ j in the lattice F ¯ is treated as an which each source phrase F ¯ observation emitted from a state Ei , a target phrase, in the ¯ as shown in Figure 2. lattice E, (8) (9) j The use of the phrase-based HMM structure has already been proposed in [20] in the context of aligning documents and abstracts. In their approach, jump probabilities were explicitly encoded as the state transitions that roughly corresponded to the alignment probabilities in the context of the word-based statistical translation model. The use of the explicit jump or alignment probabilities served for the completeness of the translation modeling at the cost of the enormous search space needed to train the phrase-based HMM structure. The segmentation model is realized as the unigram posterior probability of the phrase ngram model presented in Section 3.1. To briefly sum"
2004.iwslt-evaluation.2,J03-1002,0,0.00549884,"rocedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (14) To overcome the problem of local convergence often observed in the EM algorithm [21], we use the lexicon model from the GIZA++ [22] training as the initial parameters for the phrase translation model. In addition, the phrase ngram model and the phrase segmentation models are individually trained over the monolingual corpus and remained fixed during the HMM iterations. 3.8. Results The results appear strange in two points: (1) Our proposal didn’t work well for the Japanese-to-English track but did work well for the Chinese-to-English track; (2) Our proposal achieved high fluency but marked low adequacy. 3.6. Phrase Segment Induction Equations 13 and 14 involve summation over all possible contexts, either in its left-hand-s"
2004.iwslt-evaluation.2,P02-1038,0,0.0308911,"e segmentation model, and the phrase translation model – Equation 4 can be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure ge"
2004.iwslt-evaluation.2,P03-1021,0,0.00794031,"an be rewritten as  P (¯fj |f )P (¯fj |¯ ei )P (¯ ei |¯ ei ) (12) P (f |e) ≈ ¯,¯ e f j,i ¯ and ¯f are expanded If the phrase segmented sentences e ¯ and F, ¯ then into the corresponding lattice structures of E Therefore, the Forward-Backward algorithm can be for17 where P rj (e, f ) are the subcomponents of translation models, such as the phrase ngram model or the language model, and λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input"
2004.iwslt-evaluation.2,W02-1021,0,0.0253988,"λj is the weight for each model. The weighting parameters, λj , can be efficiently computed based either on the maximum likelihood criterion [23] by IIS or GIS algorithms or on the minimum error rate criterion [24] by some unconstrained optimization algorithms, such as the Downhill Simplex Method [25]. mulated to solve the recursions α(eii21 , fjj12 ) = i 1 −2 i =1  j j α(eii1 −1 , fj 2 ) 1 j fj2 ∩fj 2 =∅ 1 1 ×P (eii21 |eii1 −1 )P (fjj12 |eii21 )P (fjj12 |f ) β(eii21 , fjj12 ) = l   i =i2 +2 j (13) j  β(eii2 +1 , fj 2 ) 1 The decoder is taken after the word-graph-based decoder [26], which allows the multi-pass decoding strategies to incorporate complicated submodel structures. The first pass of the decoding procedure generates the word-graph, or the lattice, of translations for an input sentence by using a beam search. On the first pass, the submodels of all phrase-based HMM translation models were integrated with the wordbased trigram language model and the class 5-gram model. The second pass uses A* strategy to search for the best path of translation on the generated word-graph. j fj2 ∩fj 2 =∅ 1  1 2 j  j ×P (eii2 +1 |eii1 )P (fj 2 |eii2 +1 )P (fj 2 |f ) 1 1 (1"
2004.iwslt-evaluation.2,shimohata-sumita-2002-automatic,1,0.850547,"xtract good transfer patterns for HPAT, and to estimate the parameters for SMT. 3.7. Decoder The decision rule to compute the best translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created o"
2004.iwslt-evaluation.2,2002.tmi-tutorials.2,0,0.02481,"st translation is based on the log-linear combinations of all subcomponents of translation models as presented in [23]. 1  ˆ = argmax λj log P rj (e, f ) (16) e Z(f ) j e We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. 18 4.1. Paraphrasing for providing the Ruigo-Shin-Jiten. Three methods have been investigated for automatic paraphrasing. (1) Shimohata et al. [27] grouped sentences by the equivalence of the translation and extract rules of paraphrasing by DP-matching. (2) Finch et al. [28] clustered sentences in a paraphrase corpus to obtain pairs that are similar to each other for training SMT models. Then by using the models, the decoder generates a paraphrase. (3) Finch et al. [29] developed a paraphraser based on data-oriented parsing, which utilizes synatactic information within an examplebased framework. The experimental results indicate that the EBMT based on normalization of the source side had increased coverage [30] and that the SMT created on the normalized target sentences had a reduced word-error rate [31]. Finch et al. [32] demonstrated that the expansion of refer"
2004.iwslt-evaluation.2,C04-1017,1,0.875016,"Missing"
2004.iwslt-evaluation.2,E03-1029,1,0.895696,"Missing"
2004.iwslt-evaluation.2,W02-1611,1,\N,Missing
2004.iwslt-evaluation.2,watanabe-etal-2002-statistical,1,\N,Missing
2005.iwslt-1.15,J04-4002,0,0.0250193,") roughly means the word stem (inflectional endings). For example, “Would it be possible to ship it to Japan” becomes “Woul+ it be poss+ to ship it to Japa+” by prefix-4, and “+ould it be +ible to ship it to +apan” by suffix-4, where “+” at the end or beginning of a word denotes deletion. Prefix-4 and suffix-4 are likely to contribute to word alignment and language modeling, respectively. 3.2. Phrase-based Features Our system adopts a phrase-based translation model represented by phrase-based features, which are based on phrase translation pairs extracted by the method proposed by Och and Ney [4]. First, many-to-many word alignment is set by using both one-to-many and many-to-one word alignments generated by GIZA++ toolkit. In the experiment, we used prefix-4 for word-to-word alignment. Using prefix-4 produced better translations than the original form in preliminary experiments. Next, phrase pairs consistent with word alignment are extracted. The words in a legal phrase pair are only aligned to each other and not to words outside. Hereafter, we use count(˜ e) and count(f˜, e˜) to denote the number of extracted phrase e˜ and extracted phrase pair ( f˜, e˜), respectively. We used the f"
2005.iwslt-1.15,N03-1017,0,0.0336733,"xtracted source/target phrases # of source/target phrases appearing in the corpus • Phrase pair extraction probability, i.e., # of sentences phrase pairs extracted # of sentences phrase pairs appearing in the corpus • Adjusted Dice coefficient, which is an extension of the measure proposed in [5], i.e., Dice(f˜, e˜)log(count(f˜, e˜) + 1) 3.3. Word-level Features We used the following word-level features, where w(f |e) =  count(f, e) ,  f  count(f , e) I is the number of words in the translation and J is the number of words in the input sentence. • Lexical weight pw (f˜|˜ e) and pw (˜ e|f˜) [6], where pw (f˜|˜ e) = maxa ·  J  1 |{i|(i, j) ∈ a)}| j=1 ∀(i,j)∈a w(fj |ei ) (I˜ + 1)J˜ j w(f˜j |˜ ei ) i • Viterbi IBM Model 1 score p M1 (f˜|˜ e) and pM1 (˜ e|f˜), where pM1 (f˜|˜ e) = J˜  1 (I˜ + 1)J˜ max w(f˜j |˜ ei ) j i • Noisy OR gate pN OR (f˜|˜ e) and pN OR (˜ e|f˜) [7], where e) = pN OR (f˜|˜   (1 − (1 − w(f˜j |˜ ei ))) j i e, f˜) where • Deletion penalty p del (˜ pdel (˜ e, f˜) =  ˜ del(˜ eI1 , f˜j ) j 2 • Phrase extraction probability of source/target, i.e., J˜  I˜  1 ˜ del(˜ eI1 , f˜j ) =   1  0 i does not exist s.t. w(˜ ei |f˜j ) &gt; threshold otherwise. 3.4. Lexical"
2005.iwslt-1.15,N04-1033,0,0.0174663,"posed in [5], i.e., Dice(f˜, e˜)log(count(f˜, e˜) + 1) 3.3. Word-level Features We used the following word-level features, where w(f |e) =  count(f, e) ,  f  count(f , e) I is the number of words in the translation and J is the number of words in the input sentence. • Lexical weight pw (f˜|˜ e) and pw (˜ e|f˜) [6], where pw (f˜|˜ e) = maxa ·  J  1 |{i|(i, j) ∈ a)}| j=1 ∀(i,j)∈a w(fj |ei ) (I˜ + 1)J˜ j w(f˜j |˜ ei ) i • Viterbi IBM Model 1 score p M1 (f˜|˜ e) and pM1 (˜ e|f˜), where pM1 (f˜|˜ e) = J˜  1 (I˜ + 1)J˜ max w(f˜j |˜ ei ) j i • Noisy OR gate pN OR (f˜|˜ e) and pN OR (˜ e|f˜) [7], where e) = pN OR (f˜|˜   (1 − (1 − w(f˜j |˜ ei ))) j i e, f˜) where • Deletion penalty p del (˜ pdel (˜ e, f˜) =  ˜ del(˜ eI1 , f˜j ) j 2 • Phrase extraction probability of source/target, i.e., J˜  I˜  1 ˜ del(˜ eI1 , f˜j ) =   1  0 i does not exist s.t. w(˜ ei |f˜j ) &gt; threshold otherwise. 3.4. Lexical Reordering Features We used the following features to control the reordering of phrases: • Distortion model d(a i − bi−1 ) = exp−|ai −bi−1 −1 |, where ai denotes the starting position of the foreign phrase translated into the i-th English phrase, and b i−1 denotes the end position of"
2005.iwslt-1.15,N04-1021,0,0.050617,"Missing"
2005.iwslt-1.15,W02-1021,0,0.0178786,"on of the foreign phrase translated into the i-th English phrase, and b i−1 denotes the end position of the foreign phrase translated into the (i − 1)-th English phrase [6]. • Right monotone model P R (˜ e, f˜) (and left monotone ˜ model PL (˜ e, f )) inspired by Och’s scheme [8], where PR (f˜, e˜) = countR , count(f˜, e˜) and countR denotes the number of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the"
2005.iwslt-1.15,P97-1047,0,0.0296466,"umber of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam se"
2005.iwslt-1.15,J03-1005,0,0.0276819,"r of right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search"
2005.iwslt-1.15,2003.mtsummit-papers.53,0,0.0219439,"right connected monotone phrases. 3.5. Other Features The following additional features are used. • number of words that constitute a translation • number of phrases that constitute a translation 4. Decoder The decoder is based on word graph [9] and uses a multi-pass strategy to generate n-best translations. It generates hypothesized translations in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search sta"
2005.iwslt-1.15,C04-1006,0,0.0208502,"s in a left-to-right order by combining phrase translations for a source sentence. The first pass of our decoding algorithm generates a word graph, a compact representation of hypothesized translations, using a breadth-first beam search, as in [10][11][12][13]. Then, n-best translations are extracted from the generated word graph using A ∗ search. The search space for a beam search is constrained by restricting the reordering of source phrases. We have window size constraints that restrict the number of words skipped before selecting a segment of the source sequence [6][12]. An ITG-constraint [14] is also implemented that prohibits the extension of a hypothesis that violates ITG constraints, which will be useful for language pairs with drastic reordering, such as Japanese-to-English and Korean-to-English translations. During the beam search stage, three kinds of pruning are performed to further reduce the search space [11]. First, observation pruning limits the number of phrase translation candidates to a maximum of N candidates. Second, threshold pruning is performed by computing the most likely partial hypothesis and by discarding hypotheses whose probability is lower than the maximu"
2005.iwslt-1.15,W05-0834,0,0.0109953,"e fly and then integrated with the preceding score for beam pruning. We estimated future cost as described in [13]. Exact costs for the phrase-based features and word level features can be calculated for each extracted phrase pair. For the language model features, their costs were approximated by using only output words contained by each phrase pair. The upper bound of lexical reordering feature costs can be computed beforehand by considering the possible permutations of phrase pairs for a given input. After generating a word graph, it is then pruned using the posterior probabilities of edges [15] to further reduce the number of duplicate translations for A ∗ search. An edge is pruned if its posterior score is lower than the highest posterior score in the graph by a certain amount. 5. Experiments To validate the use of the reportedly effective features, we conducted translation experiments using all features introduced in Section 3. Also, we conducted comparable experiments in both supplied and unrestricted data tracks to study the effectiveness of additional language resources. English data sets IWSLT (supplied) ATR WEB Gigaword Corpus size (words) 190,177 1,100,194 8,482,782 1,799,53"
2005.iwslt-1.15,2004.iwslt-evaluation.13,0,0.0125922,".5 28.6 Chinese IWSLT LDC 56.6 462 56.1 449 50.7 432 Table 4: Input language perplexity of trigram trained by supplied corpora and IWSLT datasets are similar, WEB is closer to IWSLT than Gigaword, and that LDC is very different from IWSLT. Since the collection is enormous in Gigaword, the vocabulary set is first limited to that observed in the English part of supplied corpus and the ATR database. Then for decoding, an actual n-gram language model is estimated on the fly by constraining the vocabulary set to that observed in a given test set. 5.3. Other Setups Following one of the best systems [17] in IWSLT 2004, feature function scaling factors λ j are trained using NIST scores [18] in a loss function of minimum error rate training, and development set 1 (CSTAR) was used for it. For Japanese and Korean, ITG constraints of lexical reordering were applied, and for Arabic and Chinese, simple window size constraints up to 7 were used. 5.4. Results Table 5 summarizes the overall results of the supplied/unrestricted data tracks. The scores of the table are obtained by the comparable conditions for each language pair while some are not the same as those released by the organizer. “m unrestric"
2005.iwslt-1.15,W03-1506,0,\N,Missing
2005.iwslt-1.15,P02-1040,0,\N,Missing
2005.iwslt-1.15,P02-1038,0,\N,Missing
2005.iwslt-1.15,J03-1002,0,\N,Missing
2005.iwslt-1.15,2006.iwslt-evaluation.14,1,\N,Missing
2005.iwslt-1.15,P03-1021,0,\N,Missing
2006.iwslt-evaluation.14,J03-1002,0,0.00515357,"on-terminals are always instantiated with phrase translation pairs. Thus, we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4. 2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method ) from a senexhaustively extracts phrase pairs (fjj+m , ei+n i J I tence pair (f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model,"
2006.iwslt-evaluation.14,J04-4002,0,0.069757,"we will be able to reduce the number of rules induced from a bilingual corpus, which, in turn, help reducing the decoding complexity. Note that we do not imply arbitrary synchronous-CFGs are transformed into the target normalized form. The form simply restricts the grammar extracted from a bilingual corpus explained in Section 2.4. 2.4. Training The phrase extraction algorithm is based on those presented by [3]. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ [6], in both directions and by combining the results based on a heuristic [7]. Second, phrase translation pairs are extracted from the word aligned corpus [3]. This method ) from a senexhaustively extracts phrase pairs (fjj+m , ei+n i J I tence pair (f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models. 1. A phrase pair (f¯, e¯) const"
2006.iwslt-evaluation.14,W05-1507,0,0.0304273,"boxed indices indicate non-terminal alignment. One of the major differences to the algorithm presented in [4] is the restriction of the target normalized form in the last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder effici"
2006.iwslt-evaluation.14,N06-1033,0,0.0183208,"e last step. 2.5. Decoding by Top-down Parsing Decoding is performed by parsing on the source-side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved sy"
2006.iwslt-evaluation.14,P02-1038,0,0.663191,"onal phrase-based model. In addition, our reranking algorithm further boosted the performance. = argmax P r(eI1 |f1J ) (1) eI1 = argmax P eI1 e′ I1 ′ P M  I J λ h (e , f ) m m 1 1 m=1 (2) P M J ′I′ exp m=1 λm hm (e 1 , f1 ) exp In this framework, the posterior probability P r(eI1 |f1J ) is directly maximized using a log-linear combination of feature functions hm (eI1 , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to f"
2006.iwslt-evaluation.14,P03-1021,0,0.0700907,"king algorithm further boosted the performance. = argmax P r(eI1 |f1J ) (1) eI1 = argmax P eI1 e′ I1 ′ P M  I J λ h (e , f ) m m 1 1 m=1 (2) P M J ′I′ exp m=1 λm hm (e 1 , f1 ) exp In this framework, the posterior probability P r(eI1 |f1J ) is directly maximized using a log-linear combination of feature functions hm (eI1 , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach i"
2006.iwslt-evaluation.14,N03-1017,0,0.214202,"s dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likelihood approach [1] or on a direct error minimization approach [2]. This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. 1. Introduction This paper describes the NTT statistical machine translation system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based"
2006.iwslt-evaluation.14,P05-1033,0,0.752177,"system which is experimented in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2006. 2.2. Hierarchical Phrase-based Approach In the phrase-based translation approach [3], the input foreign sentence is segmented into phrases, f¯1K , mapped into corresponding English-side e¯K 1 , then, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efficiently, first, by simplifying the grammar so that the target-side takes a phrase-prefixed form, namely target normalized form. Our simplified grammar drastically reduces the number of rules extracted from a bilingual corpu"
2006.iwslt-evaluation.14,P06-1098,1,0.921865,"hen, reordered to form the output English sentence. The approach is able to capture phrase-wise local-reordering, or possibly neighboring phrase reordering, but does not account for long-distance reordering of phrases. In the hierarchical phrase-based translation approach [4], translation is constituted by hierarchically combining phrases with the help of non-terminals embedded in phrases themselves. Each non-terminal represented in each phrase can capture reordering of phrases. Based on the hierarchical phrase-based modeling, we adopted the left-to-right target generation method described in [5]. The method is able to generate translations efficiently, first, by simplifying the grammar so that the target-side takes a phrase-prefixed form, namely target normalized form. Our simplified grammar drastically reduces the number of rules extracted from a bilingual corpus empirically presented in [5]. Second, translation is generated in a left-to-right manner, similar to a phrase-based approach, using an Earley-style top-down parsing on the source-side. Coupled with the tarOur system consists of two parts. A hierarchical phrasebased translation system that generates a large n-best list. The"
2006.iwslt-evaluation.14,W06-3119,0,0.0424367,"side and by combining the projected target-side. A conventional method of parsing is a CKY-based method in which ordering is governed by the span-size of the source words [4]. One of the problem is the high computational complexity when integrated with ngram language model of the target-side especially when the ngram’s order is quite high [8]. The complexity lies on the possible “holes” in the target-side. One of the solution is to perform a binarization so that the target-side will not contain holes [9]. We applied an Earley-style top-down parsing approach described in [5] that is similar to [10]. The basic idea is to perform a top-down parsing in order so that the projected target-side is generated in a left-to-right manner. The search is guided with a push-down automaton which keeps track of the span-size of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most-likely translation. Our decoding algorithm can be regarded as an instance of Earley algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right"
2006.iwslt-evaluation.14,P96-1041,0,0.0444429,"e hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases [4]: 2.6. Feature Functions Feature functions evaluated during the decoding procedure is summarized as count-based models, lexicon-based models, language model, reordering models and length-based models. 1. A phrase pair (f¯, e¯) constitutes a rule: X → f¯, e¯ 96 2.6.3. Language Model 2.6.1. Count-based Models hφ (f1J |eI1 , D) and hφ (eI1 |f1J , D) estisentences f1J and eI1 over a derivaWe used mixed-cased 5-gram language model estimated with modified Kneser-Ney smoothing [11]: Y pn (ei |ei−4 ei−3 ei−2 ei−1 ) (11) hlm (eI1 ) = log Main feature functions mate the likelihood of two tion tree D. We assume that the production rules in D are independent of each other: Y φ(γ|α) (4) hφ (f1J |eI1 , D) = log i 2.6.4. Reordering Models hγ,αi∈D In order to limit the reorderings, two feature functions are employed: X height(Di ) (12) hheight (eI1 , f1J , D) = φ(γ|α) is estimated through the relative frequency on a given bilingual corpus. count(γ, α) φ(γ|α) = P γ count(γ, α) (5) Di ∈back(D) hwidth (eI1 , f1J , D) = where count(·) represents the cooccurrence frequency of rules γ"
2006.iwslt-evaluation.14,P02-1034,0,0.0145487,"titute a translation: j=1 max t(fj |ei ) &lt; τdel 0≤i≤I  (9) The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold τdel . Likewise, an insertion model is integrated that penalizes the inserted English words that do not account for any foreign words in an input: hins (eI1 , f1J ) = I  X i=1 max t(ei |fj ) &lt; τins 0≤j≤J  (14) hr (D) = rule(D) (15) This section explains our discriminative reranking method which further improves the quality of baseline MT system. Our reranking method basically follows the parse reranking method explained in [12]. We first generate a n-best list of candidate outputs (translations) from a baseline MT system, the hierarchical phrase-based translation described in Section 2. Then, a reranking model is trained by a ranking voted perceptron on a development set. Finally, in the process of decoding, we re-rank the n-best list of test data fed from the baseline MT using the trained reranking model. We adopted the above method, [12], with a BLEU-score-based weight update scheme. The reranking setting of MT is an ordinal regression procedure in each output pairs, similar to the parse reranking task, which can"
2006.iwslt-evaluation.14,takezawa-etal-2002-toward,0,0.0312538,"running GIZA++ in both directions, and by refining word alignment with a heuristic. Third, from three distinctly preprocessed corpora, rules are extracted using the algorithm presented in 2.4. In this step, preprocessed corpora are recovered into their original form. When recovered, punctuation marks on the source-side were removed together with corresponding word alignments. The idea is to induce better word alignments by considering non-punctuation corpus, together with punctuation preserved corpus. 4. Tasks The experiments were carried out on the Basic Travel Expression Corpus (BTEC) task [13]. BTEC is a multilingual corpus in traveling domain which was collected from phrase books for tourists. In the IWSLT 2006 open data track, the subsets of BTEC consists of training set and three development sets (Dev1 through Dev3) indicated in Table 1. Another development set (Dev4) and the final test set were provided in this track1 . The translation pairs set up for the task are: Arabic-to-English, Italian-to-English, Japanese-toEnglish and Chinese-to-English. The task description for the IWSLT 2006 evaluation campaign can be summarized as follows: 1. Spoken language, instead of written text"
2006.iwslt-evaluation.14,W06-3115,1,0.730654,"ey algorithm, but the predicted rule’s “dot” is moved synchronized with the left-to-right ordering of the projected target-side, not the left-to-right ordering on the source-side. The use of target normalized form further simplify the decodig procedure. Since the rule form does not allow any holes for the target-side, the integration with ngram language model is straightforward: the prefixed phrases are simply concatenated and intersected. Our decoder is based on an in-house developed phrasebased decoder which uses a bit vector to represent uncovered foreign word positions for each hypothesis [14]. We basically replaced the bit vector structure to the stack structure: Almost no modification was required for the word graph structure and the beam search strategy implemented for a phrase-based modeling, since the target-side’s prefixed phrases are simply concatenated. The use of a stack structure directly models a synchronous-CFG formalism realized as a push-down automation, while the bit vector implementation is conceptualized as a finite state transducer. (3) where X is a non-terminal, γ is a source-side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target-s"
2006.iwslt-evaluation.14,2005.iwslt-1.8,0,0.0199399,"erings. 5.2. Results on Hierarchical Phrase-based Translation We first compared our baseline hierarchical phrase-based translation against an in-house developed phrase-based translation that performed quite well for the shared task of “Workshop on Statistical Machine Translation” [14]. Table 3 shows the number of phrases and rules extracted from each task. The grammar size for our hierarchical phrase-based system is almost twice as large as the size of the phrase table for our phrase-based system. The phrase-based system employs a lexicalized reordering model to capture phrase-wise reordering [15]. For the hierarchical phrase-based system, spansize for each non-terminal was constrained to 7 for all tasks. Window-size constraints were set to 7 in the phrase-based system. As indicated in Table 4, our hierarchical phrasebased system outperforms the phrase-based system in all tasks. mPER 56.65 54.15 48.13 41.57 55.12 52.17 59.72 57.71 53.70 5.3. Results on Reranking Table 5 shows the reranking results for IWSLT2006. The rows of ‘1-best’ in Table 5 show the performance of our baseline MT system, hierarchical phrase-based system (contrast1 system). Then, the rows of ‘SC’ display the performa"
2007.iwslt-1.16,N07-1008,0,0.0150061,"parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with the ASR’s n-best list’s confidence measures. This paper is organized as follows: The overview of our decoder is presented in Section 2. We will describe the feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems s"
2007.iwslt-1.16,P03-1021,0,0.0149653,"e feature functions experimented in [3] together with additional features. The reranking system is described in Section 3. The reranker is biased to use a slightly different feature set to avoid over training. Both systems share the same online training algorithm, but differ in that the decoder’s parameters are updated based on the dynamically generated candidate list, whereby the reranking training is based on a fixed translation candidate list. Section 4 presents the results for the evaluation campaign of IWSLT 2007. 2. Machine Translation System We use a linear feature combination approach [8] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of ca"
2007.iwslt-1.16,P05-1033,0,0.107755,"by seeking a maximum solution: eˆ = argmax w⊤ · h(f, e) (1) e where h(f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the ngram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. Under this maximization scenario, our system composed of two steps: The first step is a decoder that can efficiently generate k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic str"
2007.iwslt-1.16,N03-1017,0,0.00424325,"te k-best list of candidate translations in a left-to-right manner [1] based on the hierarchical phrase-based translation[9]. The second step rerank the k-best list using a reranking voted perceptron[2]. 2.1. Hierarchical Phrase-based Translation We use the hierarchical phrase-based translation approach, in which non-terminals are embedded in each phrase [9]. A translation is generated by hierarchically combining phrases using the non-terminals. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [10]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Based on hierarchical phrase-based modeling, we adopted the left-to-right target generation method [1] which performed better than a phrase-based system in the last year’s evaluation[2]. This method is able to generate translations efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-t"
2007.iwslt-1.16,P98-2230,0,0.0172839,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,W06-3119,0,0.0336527,"s efficiently, first, by simplifying the grammar so that the target side takes a phrase-prefixed form, namely a target normalized form: X → γ, ¯bβ, ∼ (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals. ¯bβ is a corresponding target side where ¯b is a string of terminals, or a phrase, and β is a (possibly empty) string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. Second, a translation is generated in a left-to-right manner, similar to phrase-based decoding using Earley-style topdown parsing on the source side [11, 1, 12]. The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the restcost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure, at the expense for expressiveness. Since the rule form does not allow any holes in the target side, the integration with an n-gram language model is"
2007.iwslt-1.16,2004.iwslt-evaluation.13,0,0.0146149,"nd intersected with an n-gram. 2.2. Features 2.2.1. Baseline Features The hierarchical phrase-based translation system employs standard real valued value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in both directions, h(γ|¯bβ) and h(¯bβ|γ), estimated by relative counts, count(γ, ¯bβ) [9]. • Word-based lexically weighted models of hlex (γ|¯bβ) and hlex (¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of"
2007.iwslt-1.16,W06-3108,0,0.0191772,"(¯bβ|γ) using lexical translation models[9]. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models [13]. • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling [1]. 2.2.2. Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system [3]. The features are designed with decoding efficiency in mind and are based on the word alignment structure preserved in hierarchical phrase translation pairs [14]. When hierarchical phrases are extracted, the word alignment is preserved. If multiple ei−1 ei f j−1 ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. X1 X2 f j−1 fj f j+1 f j+3 X3 f j+2 Figure 2: Example hierarchical features. word alignments are observed with the same source and target sides, only the most frequently observed word alignment is kept to reduce the grammar size. Using the word alignment structure inside hierarchical phrases, we employs following feature set. • Word pair features directly capture the source/target word co"
2007.iwslt-1.16,P06-1098,1,0.894326,"parse binary features — of the order of millions — are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both sys"
2007.iwslt-1.16,P04-1007,0,0.0263178,"(ei−1 , fj−1 ), (ei , fj+1 )). • Insertion/deletion features are integrated in which no word alignment is associated in the target/source side. Inserted words are associated with all the words in the source sentence, such as (ei+1 , f1 ), ..., (ei+1 , fJ ) for the non-aligned word ei+1 with the source sentence f1J in Figure 1. In the same way, we use hierarchical phrase-wise deletion features by associating each inserted source word in a phrase to all the target words in the same phrase. • Target bigram features are also included to directly capture the fluency as in the n-gram language model [15], such as (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... in Figure 1. • Hierarchical features capture dependencies the source words in a parent phrase to the source words in child phrases, such as (fj−1 , fj ), (fj−1 , fj+1 ), (fj+3 , fj ), (fj+3 , fj+1 ), (fj , fj+2 ) and (fj+1 , fj+2 ) as indicated by the arrows in Figure 2. The hierarchical features are extracted only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt"
2007.iwslt-1.16,E99-1010,0,0.0399514,"ed only for those source words that are aligned with the target side to limit the feature size. Algorithm 1 Online Training Algorithm for decoder T 1: 2: 3: 4: 5: 6: 7: 8: 9: Training data: T = {(f t , et )}t=1 m-best oracles: O = {}Tt=1 i=0 for n = 1, ..., N do for t = 1, ..., T do C t ← bestk (f t ; wi ) Ot ← oraclem (Ot ∪ C t ; et ) wi+1 = update wi using C t w.r.t. Ot i=i+1 end for end for PN T i w return i=1 NT In order to achieve the generalization capability, we introduce normalized tokens for each surface form [3]. • Word class/part-of-speech/named entity. Words are clustered by mkcls [16]. The part-of-speech (POS) and named entity (NE) tags are also integrated to capture linguistic characteristics when taggers are available. A unique word class is assigned to each surface form. However, multiple POS/NE are potentially assigned to each surface word. In our approach, we do not disambiguate labels, but simply collect a surface word to multiple tags dictionary. Those tags are integrated by first running a tagger on the training data. Then, a surface form to POS/NE dictionary is generated by collecting all possible tags for each word. • Synsets from WordNet. In order to represent s"
2007.iwslt-1.16,D07-1080,1,0.562399,"ken Language Translation (IWSLT) 2007. 1. Introduction This paper presents NTT Statistical Machine Translation System evaluated in the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that"
2007.iwslt-1.16,P06-1091,0,0.0676827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P05-1012,0,0.28923,"WSLT) 2007. Our system is composed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a s"
2007.iwslt-1.16,P06-2098,0,0.0623452,"sed of two steps: First, k-best translation candidates are generated using an efficient decoder for hierarchical phrase-based translation [1]. Next, the large kbest translation is reordered using a reranking voted perceptron [2]. Both systems employ a large number of sparse features — of the order of millions — to achieve a state of the art performance [3]. The large number of parameters are trained using an efficient online training algorithm: The decoder employs an online large-margin training method [4] that has been successfully applied in dependency parsing [5] or joint labeling/chunking [6]. The reranker uses a reranking voted perceptron which gave significant improvement in the last year’s IWSLT 2006 evaluation [2]. Both systems are tuned using approximated BLEU as an objective function that scales the sentence-wise BLEU to a document-wise BLEU. Domain mismatch is handled by a simple task adaptation scheme by selecting training data that resembles a test set [7]. In order to handle the ASR’s error prone input, we decoded all the nbest translations and let the reranker choose the right translation by treating the individually translated list as a single k-best list combined with"
2007.iwslt-1.16,P06-1096,0,0.119827,"nese and Japanese. We consider all possible combination of those token types. For example, an English/Arabic word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its"
2007.iwslt-1.16,P02-1040,0,0.07649,"rmalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. As discussed above, the POS/NE/synsets labels are assigned by a one-to-many dictionary. Then, each surface form is expanded to all possible labels, then, all possible features are extracted. 2.3. Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms [17, 18] in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, e.g. BLEU [19]. In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of (f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot are updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores to each reference translation. Therefore, possible oracle translations are mai"
2007.iwslt-1.16,W04-3201,0,0.0272233,"Liang et at. [18] presented a similar updating strategy in which parameters were updated toward an oracle translation found in C t , but ignored potentially better translations discovered in the past iterations. A new wi+1 is computed using the k-best list C t with respect to the oracle translations Ot (line 5). After N iterations, the algorithm returns an averaged weight vector to avoid overfitting (line 9). When updating parameters in line 5, we use the Margin Infused Relaxed Algorithm (MIRA) [4] which is an online version of the large-margin training algorithm for structured classification [20] that has been successfully used for dependency parsing [5] and joint-labeling/chunking [6]. Line 5 of the weight vector update procedure in Algorithm 1 is reAlgorithm 2 Online Training Algorithm for Reranker placed by the solution of: X 1 ˆ i+1 = argmin ||wi+1 − wi ||2 + C ξ(ˆ e, e′ ) w wi+1 2 ′ eˆ,e subject to si+1 (f t , eˆ) − si+1 (f t , e′ ) + ξ(ˆ e, e′ ) ≥ L(ˆ e , e′ ; et ) ′ ξ(ˆ e, e ) ≥ 0 ∀ˆ e ∈ Ot , ∀e′ ∈ C t (3)  ⊤ where si (f t , e) = wi · h(f t , e). ξ(·) is a non-negative slack variable and C ≥ 0 is a constant to control the influence to the objective function. A larger C implies"
2007.iwslt-1.16,P02-1034,0,0.0176638,"ain the best oracle translations O1T = eˆ1 , ..., eˆT that is treated as a “bed” document. The approximated BLEU for a hypothesized translation e′ for the training instance (f t , et ) is computed over the bed O1T except for eˆt , which is replaced by e′ : BLEU({ˆ e1 , ..., eˆt−1 , e′ , eˆt+1 , ..., eˆT }; E) The loss computed by the approximated BLEU measures the document-wise loss of substituting the correct translation eˆt Our reranking system is basically identical to the system presented in the last year’s IWSLT 2006 evaluation [2] that is based on the parse reranking method explained in [21]. We first generate n-best lists of candidate translations from the decoder, then train reranking model using the development set with additional features by ranking voted perceptron. Finally, during the testing, we rerank the k-best list of test data from the decoder by the parameters for the reranking. A separately trained reranking model is used for the ASR’s nbest list. The reranker selects the best translation out of the merged k · n-best list generated by translating all the sentences in the n-best list. 3.1. Features The reranking system employs a slightly different feature set from the"
2007.iwslt-1.16,takezawa-etal-2002-toward,0,0.068604,"n line 8 is based on an perceptron algorithm with the update amount scaled by the loss function L(·).  wn = wn + L(Rj , Ri ; et ) · h(f t , Rj ) − h(f t , Ri ) (5) As our loss function, we employed the difference of the approximated BLEU in Section 2.4, but used a set of 1-best translations from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-Englis"
2007.iwslt-1.16,2005.mtsummit-papers.11,0,0.00566131,"ions from the decoder as our bed document, instead of oracle translations. The idea is to directly measure the gain or loss by selecting the translation different from the original 1-best translation of the decoder. 4. Evaluation 4.1. Data The major training data comes from IWSLT supplied data, a subset of BTEC[22]. We also used common bilingual data either in the public domain or from the LDC as indicated in Table 1. Additional data for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists large"
2007.iwslt-1.16,P03-1010,0,0.0124565,"ata for Arabic/English and Chinese/English comes from a set of LDC bilingual news data, lexicon and the named entity list. For Italian/English, a portion of EuroParl [23] was extracted. Additional data for Japanese/English come from the news data and misJapanese-to-English 1,055,144 10,811,003 8,646,894 384,236 254,442 NiCT, others Table 2: The source language perplexity for the “clean” development and test set. dev set test set Arabic-to-English 561.96 214.99 Italian-to-English 277.24 271.39 51.29 13.45 Japanese-to-English Chinese-to-English 188.49 73.18 cellaneous text data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decode"
2007.iwslt-1.16,H05-1059,0,0.0194319,"data supplied by NiCT [24], together with textbook-like data, a lexicon and a named entity list in the public domain 1 . The corpus statistics is presented in Table 1. Since there exists larger mismatch with the IWSLT condition, we extracted texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, mi"
2007.iwslt-1.16,W03-1506,0,0.274806,"d texts that do not contain any digits by discarding sentences that match the regular expression, “[09]”. We used a development set of 4, 5 and 5b for estimating parameters both of the decoder and the reranker, since those data include ASR’s outputs. Tokenization/tagging are performed by the following tools: English data is POS tagged by a MaxEnt-based tool [25] for use in the decoder, and by a rule-based Brill’s POS tagger for reranking. Arabic data is tokenized by simply isolating Arabic scripts. Italian data is POS tagged by treetagger [26]. Japanese/Chinese texts are POS tagged/NE chunked [27]. After tokenization, we removed all the punctuation marks in the source side of bilingual data and lowercased the texts. The English side of the bilingual data is case/punctuation preserved. 4.2. Task Adaptation As discussed in Section 4.1, we extracted bilingual data from various sources, ranging from in-domain travel related data to out-of-domain news, miscellaneous texts and lexicons. Their characteristics are very different from the style in the IWSLT development and test conditions. Table 2 shows the development/test set perplexity of the source side language computed by the trigram of t"
2007.iwslt-1.16,N06-1014,0,0.0286367,"Missing"
2007.iwslt-1.16,2006.iwslt-evaluation.14,1,\N,Missing
2007.iwslt-1.16,C98-2225,0,\N,Missing
2008.iwslt-evaluation.13,D07-1080,1,0.906376,"Missing"
2008.iwslt-evaluation.13,2006.iwslt-evaluation.14,1,0.898254,"ted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document"
2008.iwslt-evaluation.13,2007.iwslt-1.16,1,0.843688,"entences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of sparse binary features for reranking, as well as a real-valued feature (decoder score). • Lexical translation probabilities in phrase pairs 3.3.1. Word alignment features • Word-based insertion/deletion penalties We use source-target word pairs extracted by separately running IBM Model 1 in both direction [4]. In addition to source-target word unigram pairs, we used pairs of targetside word bigram and their corresponding source-side words in terms of the word alignment. We also include POS-based features, target-side word surfaces are replaced with their POS tags in the word alignment features above. Target-side (English) POS tags are automatically annotated by Brill Tagger. • Word 5-gram language model scores • Reordering penalties • Length penalties on both words and hierarchical phrases 3. Reranking Component Our reranking component is based on Ranking SVMs [1]. Each decoder k-best translation"
2008.iwslt-evaluation.13,P03-1021,0,0.0231724,"2 - work in the experiments, because they failed to capture useful context information in the current condition. We discuss these features using distinctive examples between reranker selections and decoder 1-bests. This paper is organized as follows: Section 2 brieﬂy describes our SMT decoder. Section 3 describes our reranking component and sparse features for reranking. Section 4 presents the results for the evaluation campaign of IWSLT 2008, followed by discussion in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hie"
2008.iwslt-evaluation.13,J07-2003,0,0.0606697,"on in Section 5. 2. Machine Translation Component 2.1. Statistical Machine Translation We use a linear feature combination approach [6] in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax w&gt; · h(f, e) (1) e where h(f, e) is a feature vector. w is a weight vector that scales the contribution from each feature. Feature weights (i.e. elements of w) are optimized based on minimum error rate training [6]. 2.2. Hierarchical Phrase-based Approach Our SMT component employs the hierarchical phrase-based approach [7], in which the translation model is based on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by"
2008.iwslt-evaluation.13,N03-1017,0,0.0076853,"ased on a stochastic synchronous context-free grammar (SCFG). A translation is generated by hierarchically combining phrases using non-terminals. Each production rule of SCFG takes the following form. X → hγ, α, ∼i (2) In the notation above, X is a non-terminal symbol, γ is a source-side string of terminal and non-terminal symbols, and α is a target-side one. γ and α share the same number of non-terminals whose one-to-one mapping is deﬁned by ∼. Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by"
2008.iwslt-evaluation.13,J03-1002,0,0.00281422,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,J04-4002,0,0.0283074,"hrases that is not directly modeled by a conventional phrase-based approach [8]. The non-terminal embedded phrases are learned from a bilingual corpus without a linguistically motivated syntactic structure. Our decoder and rule extraction procedure is based on Hiero [7]. The decoder is an in-house developed CKY-based Proceedings of IWSLT 2008, Hawaii - U.S.A. one. Rules in forms of (2) are extracted using phrase pairs obtained by the phrase extraction algorithm [8]. The phrase extraction uses many-to-many word alignment, derived from heuristics on one-to-many word alignment in both directions [9, 10]. Using the extracted phrases, SCFG production rules are accumulated by ﬁnding “holes” in extracted contiguous phrases: • For a phrase pair (f¯, e¯), a rule X → hf¯, e¯i is extracted. • For a rule X → hγ, αi and a phrase pair (f¯, e¯) s.t. γ = γ1 f¯γ2 and α = α1 e¯α2 , a rule X → hγ1 X k γ2 , α1 X k α2 i is extracted. where boxed indices non-terminals. k indicate one-to-one mapping between 2.3. Decoder features Features used in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSL"
2008.iwslt-evaluation.13,P02-1040,0,0.0764379,"in our machine translation component are realvalued scores derived from the translation and language models. These features are those used as baseline features in our IWSLT 2006 evaluation [3]: • Hierarchical phrase translation probabilities We employs Pegasos1 , a fast optimization algorithm for linear-kernel SVMs. It uses only k samples to calculate subgradient for optimization, so learning time of Pegasos does not depend on the training data size [5]. 3.2. Approximated BLEU We use approximated BLEU [3] to choose the best translation candidates, for optimizing the reranker in terms of BLEU [11]. The approximated BLEU is independently calculated on each translation candidate for each sentence in reranker training data during pre-processing, although original BLEU required document-wise calculation and is not suitable for sentence-level reranking. Given 1-best translation outputs for T input sentences O1T = {e11 , ..., eT1 }, the approximated BLEU on i-best translation candidate for t-th input sentence eti is calculated by substituting et1 with eti , i.e. the BLEU on the sentence set t t+1 T {e11 , ..., et−1 1 , ei , e1 , ..., e1 }. 3.3. Reranker features We use a large number of spar"
2008.iwslt-evaluation.13,W03-1506,0,0.0710488,"Missing"
2008.iwslt-evaluation.13,C08-1137,0,\N,Missing
2008.iwslt-evaluation.13,2007.iwslt-1.8,0,\N,Missing
2008.iwslt-evaluation.13,P07-1002,0,\N,Missing
2008.iwslt-evaluation.13,W06-3110,0,\N,Missing
2008.iwslt-evaluation.13,P06-1066,0,\N,Missing
2008.iwslt-evaluation.13,P07-2045,0,\N,Missing
2008.iwslt-evaluation.13,N04-1022,0,\N,Missing
2008.iwslt-evaluation.13,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.13,W06-3119,0,\N,Missing
2008.iwslt-evaluation.13,I08-1066,0,\N,Missing
2008.iwslt-evaluation.13,2006.iwslt-evaluation.22,0,\N,Missing
2009.iwslt-papers.3,P02-1038,0,0.263208,"tained by our proposed methods is more stable in various conditions than that obtained by MERT. Our experimental results on the FrenchEnglish WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data. 1. Introduction The state of the art statistical machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach. This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT"
2009.iwslt-papers.3,P03-1021,0,0.0865125,"on the FrenchEnglish WMT08 shared task show that degrade of our proposed methods is smaller than that of MERT in case of small training data or out-of-domain test data. 1. Introduction The state of the art statistical machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach. This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization a"
2009.iwslt-papers.3,2001.mtsummit-papers.68,0,0.120111,"l machine translation systems have been modeled by the log-linear approach which is a generalization of the noizy-channel approach. This approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation. Support vector machines (SVM) have proven to be power† This research was conducted as an internship program 2008"
2009.iwslt-papers.3,P05-1033,0,0.0849286,"Missing"
2009.iwslt-papers.3,P05-1034,0,0.0712134,"Missing"
2009.iwslt-papers.3,W00-1303,0,0.04046,"fective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation. Support vector machines (SVM) have proven to be power† This research was conducted as an internship program 2008 of NTT. Watanabe now belongs to National Institute of Information and Communications Technology (NICT) in Japan. ∗ Taro - 144 - ful tools for many tasks in natural language processing [6][7]. The core of the form consists of a smooth convex regularizer such as 12 ||w||2 and the empirical risk term of hinge loss. In this paper we present an approach to optimize the parameter of the log-liner model using the primal form of structural SVM [12]. We expect the convex regularizer or the factor of enlarging the margin (between the reference and the incorrect translation) of SVM to reduce the overfitting problem and enhance generalization ability. Using the BLEU scores to define the hinge loss, our proposed method also inherits the advantages of MERT, which enhance the BLEU scores of tra"
2009.iwslt-papers.3,D07-1080,1,0.888167,"ˆelsi−1 .m−li−1 .m } i−1 .b−li .b xi = { lil.m−l } i−1 .m end if until no more intersections add(I , xi1 ) add(I , max(I ) + 2) xbest = argminx∈I Obj(w )d, Cs1 , {rj , ˆ e∗j , fj }sj=1 ) w+ = (xbest − ) delete(I , max(I ) + 2) end for return w + (x − S e∗s }1 ) − BLEU({rs , ˆ es }1 )}. Q × {BLEU({rs , ˆ 1-slack formulation with margin-rescaling is constructed usS ing the corpus-wise BLEU loss function Δ({ˆ e∗s , ˆ es }1 ): λ argmin w2 + ξ. w,ξ≥0 2 (7) eS ) ∈ CS : s.t. ∀(ˆ e1 , · · · , ˆ S 1 S w, δhs  ≥ Δ({ˆ e∗s , ˆ es }1 ) − ξ. S s=1 Unlike the margin infused relaxed algorithm (MIRA) [9] and S-slack formulation, we can directly apply the corpus-wise BLEU to the SVM objective function without approximating the BLEU scores. 4.3. Optimization Algorithm 4.3.1. Line-search Algorithm Next we describe extended Och’s line-search algorithms for S-slack and 1-slack formulation of the Structural SVM. These pseudocodes for the line-search to optimize parameter w are given by Algorithm 3,4. In Algorithm 3,4 we can find the range of values along the direction vector d to which each candidate translation is assigned the best score. Algorithm 3 is the line-search algorithm for S-slack formul"
2009.iwslt-papers.3,P05-1012,0,0.13106,"Missing"
2009.iwslt-papers.3,P06-2098,0,0.029702,"Missing"
2009.iwslt-papers.3,2005.eamt-1.36,0,0.401502,"ce. In the second subsection we define loss function Δ(rs , ˆ es ) using the BLEU scores, and the third subsection extends Och’s line-search algorithm as the optimization algorithm for the SVM-based objective function. 4.1. Selecting the Configuration in the K-best list For structural SVM, we need to label a correct candidate translation for each source sentence from its K-best list of candidates, which is called a configuration. Since the BLEU scores are not cumulative, we cannot efficiently select the best configuration from the K-best list. So we approximate it by a greedy search algorithm [14]. This algorithm considers the impact on the training set score when selecting an alternative translation by subtracting the statistics for the current configuration choice from the accumulated statistics and adding those for the alternative and selects the translation which results in the highest score. Repeat this process and continue untill there are no configuration changes. The configuration obtained by this algorithm specifies the correct candidate for each K-best list, and the BLEU scores are the upper bound for the BLEU scores on the training set. 4.2. Loss Function for Rescaling 4.2.1"
2009.iwslt-papers.3,2006.iwslt-evaluation.14,1,0.930093,"Missing"
2009.iwslt-papers.3,P07-2045,0,0.0638114,"s approach has achieved a lot of great advances because it has provided a natural extention to integrate many useful components [1]. To estimate the weights toward these components according to their performance, minimum error rate training (MERT) [2] was introduced by Och (2003). MERT improves statistical machine translation quality by optimizing the parameter of the log-linear function by using such automatic translation evaluation metrics as the BLEU scores [3]. To train a small number of real-valued features used on a standard phrase-based statistical machine translation system like Moses [16], MERT with BLEU-based objective function is very effective due to line-search algorithm proposed by Och (2003). However, MERT tends to overfit to training data because its objective function consists of no regularizer. To enhance generalization ability, we would like to use other state-of-the-art machine learning techniques for machine translation. Support vector machines (SVM) have proven to be power† This research was conducted as an internship program 2008 of NTT. Watanabe now belongs to National Institute of Information and Communications Technology (NICT) in Japan. ∗ Taro - 144 - ful too"
2009.iwslt-papers.3,J03-1002,0,0.00413523,"i = argminˆes ∈Cs ˆe∗s { ˆelsi−1 .m−li−1 .m } i−1 .b−li .b xi = { lil.m−l } i−1 .m end if until no more intersections add(I , xi1 ) end for add(I , max(I ) + 2) xbest = argminx∈I Obj(w )d, CS1 , {rs , ˆ e∗s , fs }S1 ) return w + (xbest − )d + (x − • Three orientation types reordering model[17]: p(m|f , e) , p(s|f , e) , p(d|f , e) to capture the lexicalized information. • Word , Phrase penalty: To control the target length and the average length of the phrases. In this paper, we trained these small number of features and phrases were extracted using a typical approach [16] that ran GIZA++ [18]. We used a Katz smoothing 5-gram language model that was created using the SRILM toolkit [19]. 5.2. Data Set For experiments we used the French-English data provided for the Europarl-based WMT08 shared task. Europarl corpus was collected from the proceedings of European Parliament [20]. This training corpus contains about 1.3 M sentences. Parameters were tuned over the provided development set (dev2006) that consisted of 2000 sentences with one reference. We used two open test sets: Europarl test 2008, consisting of 2000 sentences with one reference, and News newstest 2008 (out-of-domain), co"
2009.iwslt-papers.3,2005.mtsummit-papers.11,0,0.00816147,"g model[17]: p(m|f , e) , p(s|f , e) , p(d|f , e) to capture the lexicalized information. • Word , Phrase penalty: To control the target length and the average length of the phrases. In this paper, we trained these small number of features and phrases were extracted using a typical approach [16] that ran GIZA++ [18]. We used a Katz smoothing 5-gram language model that was created using the SRILM toolkit [19]. 5.2. Data Set For experiments we used the French-English data provided for the Europarl-based WMT08 shared task. Europarl corpus was collected from the proceedings of European Parliament [20]. This training corpus contains about 1.3 M sentences. Parameters were tuned over the provided development set (dev2006) that consisted of 2000 sentences with one reference. We used two open test sets: Europarl test 2008, consisting of 2000 sentences with one reference, and News newstest 2008 (out-of-domain), consisting of 1500 sentences. Table 1 shows these contents in more detail. Table 1: The Data statistics For the 1-slack formulation we should calculate slopes m and intercepts b the same as the S-slack formulation, but, to avoid bias toward the line-search procedure by sentencewise BLEU,"
2009.iwslt-papers.3,N09-1025,0,0.107757,"Missing"
2009.iwslt-papers.3,W08-0304,0,0.108941,"Missing"
2009.iwslt-papers.3,D08-1076,0,0.0713832,"Missing"
2009.iwslt-papers.3,C08-1074,0,0.0530567,"Missing"
2009.iwslt-papers.3,J93-2003,0,0.0179364,"Missing"
2009.iwslt-papers.3,P02-1040,0,\N,Missing
2009.iwslt-papers.3,2005.iwslt-1.8,0,\N,Missing
2010.iwslt-evaluation.18,N03-1017,0,0.00288212,"words by replacing them with known words that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous cont"
2010.iwslt-evaluation.18,P05-1033,0,0.0604414,"that have the same lemmas but different inflections. The structure of the remainder of the paper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY alg"
2010.iwslt-evaluation.18,koen-2004-pharaoh,0,0.0968916,"aper is as follows: Section 2 describes each of the components that we used in our approach, Section 3 and Section 4 describe our implementation of the DIALOG translation systems and the BTEC French-English translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language m"
2010.iwslt-evaluation.18,P03-1021,0,0.0147245,"translation systems in detail and evaluate the performance of our systems, and the conclusion is given in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., i"
2010.iwslt-evaluation.18,J07-2003,0,0.0812834,"en in Section 5. 2. System Components 2.1. Machine Translation Systems We applied two machine translation models in our approach: a standard phrase-based model [1] and a hierarchical phrasebased model [2]. 2.1.1. CleopATRa We used a phrase-based translation system, that is similar to Pharaoh [3], a beam search decoder based on a log-linear model, CleopATRa, which is comprised of a language model, a translation model, a distortion model and word penalty. The feature weights are tuned using MERT [4]. 2.1.2. Linparse The hierarchical phrase-based translation system, Linparse, is similar to Hiero [5], and is based on a weighted syn139 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 chronous context-free grammar (CFG) and uses a CKY algorithm with cube-pruning for efficient search. The feature functions consist of a language model, a hierarchical phrase translation model, and phrase penalty. The feature weights are also tuned using MERT [4]. 2.2. Integration of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-"
2010.iwslt-evaluation.18,W10-1760,1,0.791009,"on of Multiple Segmentation Schemes The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-driven NLP applications like Machine Translation (MT). In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units. We use an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches [6]. Word segmentations that are consistent with the phrasal segmentations of SMT translation models are learned from the SMT training corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language. Successive characters aligned to the same target words are merged into a larger source language unit. Therefore, the granularity of the translation unit is defined in the given bitext context. In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn"
2010.iwslt-evaluation.18,P07-1040,0,0.0206781,"er the same surface string but differ only in the segmentation of the source language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorith"
2010.iwslt-evaluation.18,N04-1022,0,0.0213991,"language phrase. Therefore, the more often such a translation pair is learned by different iterative models, the more often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm"
2010.iwslt-evaluation.18,2006.amta-papers.25,0,0.017203,"often the respective target language expression will be exploited by the SMT decoder. The translation of unseen data using the merged translation models is carried out by (1) characterizing the input text and (2) applying the SMT decoding in a standard way. 2.3. System Combination A lattice-based system combination approach is applied in our model. We follow the traditional system combination approach [7, 8]. An MBR-CN framework is applied. The minimum Bayes-risk (MBR) decoder [9] is used to select the best single output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argm"
2010.iwslt-evaluation.18,P02-1034,0,0.0171555,"output to be used as the skeleton by minimizing the translation edit rate (TER) [10]. Then, the confusion network (CN) is built using the skeleton as the backbone which determines the word order of the combination. The other hypotheses are then aligned to the backbone based on the TER metric. The decoder of the CN uses only the word posterior probability, a 4-gram language model and the length penalty as the log-linear feature functions in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, w"
2010.iwslt-evaluation.18,P05-1012,0,0.0195886,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D07-1080,1,0.900751,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,D08-1024,0,0.0536492,"ns in a search process through a beam search algorithm. 2.4. SVM Reranking 2.4.1. Ranking Model Learning Our ranking algorithm is based on a ranking approach of [11] ˆ from a large in which we seek the maximum scored output e n-best list ˆ = argmax w> · h(e, f ) e (1) e∈GEN(f ) where GEN(·) is an n-best list, a set of candidate translations, generated from the input sentence f . h(·) defines mapping from input/output sentence pair to feature functions, and w is a weight vector. In training the parameter vector w, we employed an online large-margin learning for structured output classification [12, 13, 14] based on the margin infused relaxed algorithm (MIRA) [15]. First, we generate a large n-best list e for m input sentences f1...m . For each iteration, we randomly choose an input sentence fi and its corresponding ni -best list ei . We seek a maximum scored hypothesized translation eij using the current weight w w> · h(eij ) − b(eij ) (2) where h(eij ) and b(eij ) are a feature vector representation and the BLEU score for eij , respectively. Then, we update 140 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 w by the value of w0 wh"
2010.iwslt-evaluation.18,2008.iwslt-evaluation.13,1,0.855795,"n using the loss biased maximization in Equation 2 largely inspired by [14]. For the loss function lij and the underlying BLEU score b(·), we applied document scaled BLEU which computes BLEU by replacing one translation ei1 with another eij in a set of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignme"
2010.iwslt-evaluation.18,N03-1033,0,0.0440416,"t of 1-best translations {ei1 }i=1...m [13]. Oracle translations are selected with respect to b(·). When multiple oracle translations are found, we select the one which maximizes ∆h(eij ) · w [14]. 2.4.2. Feature Functions for Re-ranking We used a large number of sparse binary features together with real valued features from decoders as described in [17]. Word pair features We used all possible pairs of source word and target word as our primary features. POS pairs were also extracted by replacing source words and target words with their corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-c"
2010.iwslt-evaluation.18,N06-1014,0,0.0794113,"ir corresponding POS tags annotated by the Stanford tagger [18]. In addition, we used simple 4-letter prefix and 4-letter suffix normalized words as the word pair features. N-gram features In order to directly capture fluency, we extracted n-gram features in the target side from unigram to trigram. As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the"
2010.iwslt-evaluation.18,P08-1012,0,0.0154613,". As in word pair features, n-gram features with POS/4letter normalization were also used as our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from"
2010.iwslt-evaluation.18,P03-1054,0,0.00620711,"our feature set. Alignment features We used fine grained word pair features by running a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” la"
2010.iwslt-evaluation.18,P08-1067,0,0.0126776,"a word aligner which heuristically combines posterior distribution from symmetrically agreed HMM models in two directions [19]. For our heuristic combination method, we introduced ITG-constraints, instead of thresholding, by assigning zero weights to binary branching rules, and the log of posterior probabilities for bi-lexical rules. For faster Viterbi alignment computation, we employed a fast span pruning method of [20]. Syntactic features We also included syntactic features by running the Stanford parser [21] on both sides. The feature set employed in our ranking model was mainly taken from [22], namely, “Rule” and “Parent” for the rules used in the parsed tree with/without its parent category, “Word edges” for the category and span with neighboring terminal words and “NGram tree” for the minimum tree structure spanning a bigram. Context features The DIALOG task preserves dialog context between two speakers. We directly encoded the structure as our feature set by including pairs of words between words from the current translated utterance and bags of words (BOW) from the previously “translated” last utterance from both speakers. The BOWs were collected from the n-best list of the tra"
2010.iwslt-evaluation.18,2009.iwslt-evaluation.12,1,0.830023,"DIALOG corpus, the BTEC corpus and the DEVSET corpus. All the data in the DEVSET for the BTEC task, using on single reference, was included for training. Only the devset for DIALOG was reserved for development testing. All of our experiment results presented in this paper are based on this testset. In total, we had around 35K sentence pairs for training. The devset used for MERT is sampled from all of the DEVSET for BTEC. In the last year’s IWSLT campaign, we introduced a devset sampling technique in which the development data were sampled from training data that are similar to the input text [24]. The similarity is measured by the BLEU using the input sentences as references. This year, we sampled from bilingual data with multiple reference translations, rather than from large amounts of DIALOG data with single reference translations, in order to avoid overfitting. We extracted 500 sentences for each translation direction. During MERT, only the training corpus for DIALOG and BTEC were used to train the translation model, but all of the data was used to build final translation model. Some pre-processing was also carried out on the corpus before training. First, in order to avoid ambigu"
2010.iwslt-evaluation.18,I08-4033,1,0.828764,"ces are split if multiple sentences are found in one line. At the end of translation, these multiple sentences are concatenated into a single line. We also did some normalization to the text. For English text, all the words were lowercased, any hyphens or commas were removed from between numeral words and tokenized using the standard tools provided by the Moses toolkit1 . The Chinese word segmentation originally provided contained inconsistencies and was not usable to build the translation model. The Chinese word segmentation was therefore redone using three methods: character-based, Achilles [25] and ICTCLAS2 . We will explain the usage of different segmentation standards in the next section. Basically, the numeral words in Chinese can be written either using Chinese characters or Arabic numbers. We converted all of the Arabic numbers to Chinese characters using a simple set of heuristics. Our translation model was built from data containing the punctuation for both source and target languages. In the official testing, the test data is provided without punctuation to remain consistent with the format of ASR output. So, before sending the test data for translation, we restored the punc"
2014.iwslt-evaluation.20,2012.eamt-1.60,0,0.0123428,"statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model. 1. Introduction In the IWSLT 2014 machine translation evaluation campaign, the NICT team participated in the TED [1] translation shared-task for Chinese-English. This paper describes the machine translation approach adopted for this campaign. Our system was a combination of phrase-based and hierarchical SMT systems. The combination was performed by reranking the n-best hypotheses from these systems. A loglinear model which used the hypothesis scores of the component systems as features was used to calculate the score used in reranking. Additional features were also added into the log-linear model, for example features from a neural network model, or talk-level language model scores. In addition to system co"
2014.iwslt-evaluation.20,P14-1129,0,0.135961,"mple features from a neural network model, or talk-level language model scores. In addition to system combination, we put emphasis on language modeling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual tra"
2014.iwslt-evaluation.20,J03-1002,0,0.00988323,"ling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models train"
2014.iwslt-evaluation.20,2005.mtsummit-papers.11,0,0.0128404,"rom this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models trained on the English LDC Gigaword dataset, a collection of approximately 4 billion words of international newswire text. 2.1. Pre-processing The English data was tokenized by applying the EUROPARL tokenizer [5]. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post processing based on experiments carried out for the 2010 IWSLT shared evaluation [6]. 2.2. Post-processing The output of the translation system was subject to the following post-processing steps which were carried out in the following order: 1. In all"
2014.iwslt-evaluation.20,P03-1021,0,0.0455445,"Missing"
2014.iwslt-evaluation.20,2013.iwslt-evaluation.1,0,0.0146617,"cores of the component systems (MERT) [10]. The weights for the models were tuned using the development data supplied for the task. 3.5. Evaluation We evaluated each of these systems on the IWSLT 2013 test set, and the results are shown in Table 3.5. The evaluation in all of the experiments in this report was carried out on tokenized, lowercase data, using the “multi-bleu.perl” evaluation script included in release version 2.1 of the MOSES toolkit. The systems are roughly comparable in performance, and about 1.5 BLEU percentage points higher than the caseinsensitive MOSES baseline reported in [11], we believe this can be explained by differences in the tokenization used for evaluation, and also by differences in the development sets used for tuning. We found that when tuned and evaluated on different data sets, the relative rankings of the systems may vary. 4. Methodology 4.1. Language Modeling 4.1.1. Neural Network Model We implemented the neural network joint models proposed in [2] and used the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale o"
2014.iwslt-evaluation.20,D13-1140,0,0.0203032,"d the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale of the neural network. We found the settings used in [2] gave rise the highest performance, and we therefore adopted these settings in our system. These settings were: 11-word source context, 3-word target context, 192-unit shared embedding layer, and two additional 512unit hidden layers. We set both input and output vocabulary size to 32000. The neural network was implemented using the NPLM toolkit [12]. The results are shown in Table 4.3. The gain using from this approach was approximately 0.5 BLEU points. This was lower than the gains reported in [2], however, in their experiments the neural network was directly integrated into the decoding process. We integrated monolingual neural network model into the OCTAVIAN decoder, however, the experiments were not completed due to time limitations. 4.1.2. Gigaword We combined language models trained on the source of the parallel TED corpus, and the Gigaword newswire corpus by linear interpolation. The interpolated language model was then used direc"
2014.iwslt-evaluation.20,P96-1041,0,0.370762,"Missing"
2014.iwslt-evaluation.20,P07-2045,0,0.0124426,"the following order: 1. In all experiments, the out of vocabulary words (OOVs) were passed through the translation process unchanged, some of these OOVs were Chinese and some English. For the primary submission, we took 139 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the decision to delete only those OOVs containing Chinese characters not included in the ASCII character set and leave words containing only ASCII characters in the output. 2. The output was de-tokenized using the de-tokenizer included with the MOSES toolkit [7]. 3. The output was re-cased using the re-casing tool supplied with the MOSES toolkit. We trained the recasing tool on cased text from the TED talk training data. 3. The Base Systems 3.1. Decoders Our submission used two SMT systems within a system combination framework; these systems were: 1. OCTAVIAN, an in-house phrase-based decoder. 2. A hierarchical version of the MOSES decoder [7]. The OCTAVIAN decoder used in these experiments is an in-house phrase-based statistical machine translation decoder that can operate in a similar manner to the publicly available MOSES decoder [7]. The base dec"
2020.coling-main.271,D19-1371,0,0.0127032,"shi et al., 2019). Their method trains bidirectional long short-term memories (BiLSTMs) (Hochreiter and Schmidhuber, 1997) on annotated data, and runs the CKY algorithm to find the globally optimal coordinate structures in a sentence. 4.2 Dataset We evaluate our method on GENIA treebank beta (Tateisi et al., 2005), which is a biomedical-domain corpus that consists of abstracts taken from the MEDLINE database, and contains syntactic annotations, 1 When the similarity takes a negative value, we multiply the square by -1. We used word embeddings pre-trained in bio-domain corpora, namely SciBERT (Beltagy et al., 2019) and Biowordvec (Yijia et al., 2019). For ELMo, we used the model trained on PubMed, available at https://allennlp.org/elmo. 3 When there is a word decomposed into subwords, we create the word vector from the mean of the subword vectors. 4 The span of the second conjunct is determined by the path of the edit graph; once the path reaches the right-most column, we stop the operation and regard the last vertex as the span of the second conjunct. 5 We tune our hyper-parameters, namely, the skip score and normalization value, on the extended Penn Treebank (Ficler and Goldberg, 2016a). 6 Note that t"
2020.coling-main.271,Q17-1010,0,0.00770191,"but not the RARE-mediated signal.” A diagonal edge represents the alignment between two words at the top and right of the edge. In this example, three pairs of words (“the–the,” “retinoid-induced–RARE-mediated,” and “program–signal”) are aligned. The vertical and horizontal edges represent a skipping operation, which indicates that the words are not aligned. To calculate the similarities of the words, we use the square of the cosine similarity of the word embeddings.1 We used three different word embedding methods, namely, BERT (Devlin et al., 2019), ELMo (Peters et al., 2018), and FastText2 (Bojanowski et al., 2017) to investigate the impacts of the embedding methods. For BERT and ELMo, we input a whole sentence containing a conjunction, and use the last layer of the hidden states as the contextualized word embeddings.3 The largest difference between the approach by Shimbo and Hara (2007) and our method is that they use coordination-annotated data to train the feature weights, whereas our method does not. This difference requires some modifications in their algorithm: Because we do not have access to the gold span of the conjuncts, we need to consider all possible candidates of conjuncts within the outer"
2020.coling-main.271,N19-1423,0,0.0435968,"articularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination bo"
2020.coling-main.271,P16-1079,0,0.0805813,"ges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and"
2020.coling-main.271,D16-1003,0,0.102405,"ges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and"
2020.coling-main.271,P09-1109,1,0.833289,"et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creative Commons Attribut"
2020.coling-main.271,J94-4001,0,0.813011,"Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is"
2020.coling-main.271,P16-1101,0,0.017409,"scientific literature, coordination is a common syntactic structure and is frequently used to describe technical terminologies. These coordinate structures often involve ellipsis, a linguistic phenomenon in which certain redundant words inferable from the context are omitted. For instance, the phrase “prostate cancer and breast cancer cells” conjoins two cell names, “prostate cancer cell” and “breast cancer cell,” with the token “cell” eliminated from the first conjunct. This phenomenon raises significant challenges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not"
2020.coling-main.271,N18-1202,0,0.118081,"the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in iden"
2020.coling-main.271,D07-1064,0,0.387012,"et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creat"
2020.coling-main.271,I05-2038,0,0.297837,"f words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3043 Proceedings of the 28th International Conference on Computational Linguistics, pages 3043–3049 Barcelona, Spain (Online), December 8-13, 2020 2 Related Studies The goal of our method is to identify the coordination boundaries of noun phrases, includ"
2020.coling-main.271,I17-1027,1,0.837084,"ion (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and H"
2020.coling-main.271,N19-1343,1,0.805816,"t of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2"
2021.acl-long.275,C18-1139,0,0.0137256,"g learning rate ητ = η0 /(1 + γ · τ ), where τ is the index of the current epoch. For ACE2004, ACE2005, and GENIA, the initial learning rates η0 are 0.2, 0.2, and 0.1, and the decay rates γ are 0.01, 0.02, and 0.02 respectively. We set the weight decay rate, the momentum, the batch size, and the number of epochs to be 10−8 , 0.5, 32, and 100 respectively, especially we use batch size 64 on the GENIA dataset. We clip the gradient exceeding 5. Besides, we also conduct experiments to evaluate the performance of our model with contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019)"
2021.acl-long.275,N18-1079,0,0.0165581,"ll possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits its applicability. Wang et al. (2018) instead proposed to use a transition-based constituency parser to incrementally build constituency forest, its linear time complexity ensures it can handle longer sentences. 5 Conclusion In this paper, we proposed a simple"
2021.acl-long.275,W16-2922,0,0.0122812,"can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word embeddings initialization, we utilize 100dimensional pre-trained GloVe (Pennington et al., 2014) for the ACE2004 and the ACE2005 datasets, and use 200-dimensional biomedical domain word embeddings1 (Chiu et al., 2016) for the GENIA dataset. Moreover, we randomly initialize 30dimensional vectors for character embeddings. The hidden state dimension of character-level LSTM dc is 100, i.e., 50 in each direction, thus the dimension of token representation dx is 200. We apply dropout (Srivastava et al., 2014) on token representations before feeding it into the encoder. The hidden state dimension of the three-layered LSTM is 600 for ACE2004 and ACE2005, i.e., 300 in each direction, and 400 for GENIA. Choosing a different dimension is because the maximal depth of entity nesting m is different. We apply layer norma"
2021.acl-long.275,N19-1423,0,0.00801778,"nt descent (SGD), with a decaying learning rate ητ = η0 /(1 + γ · τ ), where τ is the index of the current epoch. For ACE2004, ACE2005, and GENIA, the initial learning rates η0 are 0.2, 0.2, and 0.1, and the decay rates γ are 0.01, 0.02, and 0.02 respectively. We set the weight decay rate, the momentum, the batch size, and the number of epochs to be 10−8 , 0.5, 32, and 100 respectively, especially we use batch size 64 on the GENIA dataset. We clip the gradient exceeding 5. Besides, we also conduct experiments to evaluate the performance of our model with contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo an"
2021.acl-long.275,N16-1030,0,0.0169756,"at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method. 1 PER ROLE ROLE PER Figure 1: An example of nested NER. Introduction Named entity recognition (NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, whe"
2021.acl-long.275,doddington-etal-2004-automatic,0,0.037815,"ion function. We optimize our model by minimizing the sum of the negative log-likelihoods of all levels. L=− m X log p (y l |Hl ) (10) l=1 On the decoding stage, we iteratively apply the Viterbi algorithm (Forney, 1973) at each level to search the most probable label sequences. ˆ l = arg max p (y 0 |Hl ) y (11) y 0 ∈Y n The pseudocodes of the training and the decoding algorithms with max or logsumexp potential function can be found in Algorithms 1 and 2, respectively. 3 Experiments 3.1 Datasets We conduct experiments on three nested named entity recognition datasets in English, i.e., ACE2004 (Doddington et al., 2004), ACE2005 (Walker et al., 2006) and GENIA (Kim et al., 2003). We divide all these datasets into tran/dev/test split by following Shibuya and Hovy (2020) and Wang et al. (2020). The dataset statistics can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word"
2021.acl-long.275,D09-1015,0,0.145613,"Missing"
2021.acl-long.275,P19-1585,0,0.0578994,"(NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventi"
2021.acl-long.275,N18-1131,0,0.16333,"h contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our"
2021.acl-long.275,P19-1511,0,0.0237053,"Missing"
2021.acl-long.275,D15-1102,0,0.024461,"ese l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits i"
2021.acl-long.275,N19-1308,0,0.0155588,"m. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal"
2021.acl-long.275,2020.acl-main.571,0,0.0565698,"2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our Method (max)[B+F] Our Method (logsumexp)[B+F] 84.51 85.94"
2021.acl-long.275,P16-1101,0,0.0330506,"ion for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method. 1 PER ROLE ROLE PER Figure 1: An example of nested NER. Introduction Named entity recognition (NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided in"
2021.acl-long.275,P19-1527,0,0.0242444,"Missing"
2021.acl-long.275,D17-1276,0,0.0193825,"ting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits its applicability. Wang et al. (2018) instead proposed to use a transition-based constituency parser to incrementally build constitu"
2021.acl-long.275,D14-1162,0,0.0864412,"ll these datasets into tran/dev/test split by following Shibuya and Hovy (2020) and Wang et al. (2020). The dataset statistics can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word embeddings initialization, we utilize 100dimensional pre-trained GloVe (Pennington et al., 2014) for the ACE2004 and the ACE2005 datasets, and use 200-dimensional biomedical domain word embeddings1 (Chiu et al., 2016) for the GENIA dataset. Moreover, we randomly initialize 30dimensional vectors for character embeddings. The hidden state dimension of character-level LSTM dc is 100, i.e., 50 in each direction, thus the dimension of token representation dx is 200. We apply dropout (Srivastava et al., 2014) on token representations before feeding it into the encoder. The hidden state dimension of the three-layered LSTM is 600 for ACE2004 and ACE2005, i.e., 300 in each direction, and 400 for"
2021.acl-long.275,W95-0107,0,0.136025,"o propose three different selection strategies for fully leveraging information among hidden states. Besides, Shibuya and Hovy (2020) proposed to recognize entities from outermost to inner. We empirically demonstrate that extracting the innermost entities first results in better performance. This may due to the fact that some long entities do not contain any inner entity, so using outermostfirst encoding mixes these entities with other short entities at the same levels, therefore leading encoder representations to be dislocated. In this paper, we convert entities to the IOBES encoding scheme (Ramshaw and Marcus, 1995), and solve nested NER through applying CRF level by level. Our contributions are considered as fourfold, (a) we design a novel nested NER algorithm to explicitly exclude the influence of the best path through using a different potential function at each level, (b) we propose three different selection strategies for fully utilizing information among hidden states, (c) we empirically demonstrate that recognizing entities from innermost to outer results in better performance, (d) and we provide extensive experimental results to demonstrate the effectiveness and efficiency of our proposed method"
2021.acl-long.275,2020.acl-demos.38,0,0.0134811,"al information, then selecting chunks in the original order is sufficient, thus our dynamic selecting mechanism can only slightly improve the model performance. 3.5 nested outermost entities at the same level would dislocate the encoding representation. Furthermore, even if we use the outermost-first encoding scheme, our method is superior to Shibuya and Hovy (2020), which further demonstrates the effectiveness of excluding the influence of the best path. 3.6 Time Complexity and Speed The time complexity of encoder is O (n), and because we employ the same tree reduction acceleration trick4 as Rush (2020), the time complexity of CRF is reduced to O (log n), therefore the overall time complexity is O (n + m · log n). Even our model outperforms slightly worse than Wang et al. (2020), the training and inference speed of our model is much faster than them, as shown in Table 4, since we do not need to stack the decoding component to 16 layers. Especially, when we increase the batch size to 64, the decoding speed is more than two times faster than their model. Method Batch Size Training Decoding Wang et al. (2020) 16 32 64 1,937.16 3,632.64 6,298.85 3,626.53 4,652.05 5,113.85 Our Method 16 32 64 4,1"
2021.acl-long.275,2020.tacl-1.39,0,0.0610903,"not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventional conditional random field (CRF) (Lafferty et al., 2001) and then exclude the obtained best paths from the search space. To accelerate computation, they also designed an algorithm to efficiently compute the partition function with the best path excluded. Moreover, because they search the outermost entities first, performing the second-best path s"
2021.acl-long.275,D18-1309,0,0.0332534,"Missing"
2021.acl-long.275,D18-1019,0,0.0639049,"ERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our Method (max)[B+F] Our Method (logsume"
2021.acl-long.275,D18-1124,0,0.0249544,"Missing"
2021.acl-long.275,2020.acl-main.525,0,0.174513,"in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventional conditional ran"
2021.acl-long.275,P17-1114,0,0.0249752,"e other is the tokens in recognized entities, to model the interaction among them. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and"
2021.acl-long.275,P18-1030,0,0.0116537,"s naive. These observations further demonstrate our dynamic chunk selection strategies are capable of learning more meaningful representations. 4 Related Work Existing NER algorithms commonly employ various neural networks to leverage more morphological and contextual information to improve performance. For example, to handle the out-ofvocabulary issue through introducing morphological features, Huang et al. (2015) proposed to employ manual spelling feature, while Ma and Hovy (2016) and Lample et al. (2016) suggested introducing CNN and LSTM to build word representations from character-level. Zhang et al. (2018) and Chen et al. (2019) introduced global representation to enhance encoder capability of encoding contextual information. Layered Model As a layered model, Ju et al. (2018) dynamically update span-level representations for next layer recognition according to recognized inner entities. Fisher and Vlachos (2019) proposed a merge and label method to enhance this idea further. Recently, Shibuya and Hovy (2020) designed a novel algorithm to efficiently learn and decode the second-best path on the span of detected entities. Luo and Zhao (2020) build two different graphs, one is the original token s"
2021.acl-long.275,D19-1034,0,0.0118739,"ties, to model the interaction among them. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested en"
2021.acl-srw.33,2020.emnlp-main.42,0,0.0325354,"Missing"
2021.acl-srw.33,D19-5622,0,0.084177,"Results of AER on each layer. The value in bold indicates the best performance. Transformer w/o. Positional Embedding Transformer + LPSI w/o. Positional Embedding Transformer + Local Attn w/o. Positional Embedding De→En 34.42 17.01 34.83 33.94 34.77 33.84 Ja→En 29.48 15.40 29.44 28.89 30.19 29.58 Table 6: Results on IWSLT’14 German to English (De→En) and ASPEC Japanese to English (Ja→En) for effectiveness of learning word order. ’w/o. Positional Embedding’ indicates removing positional embedding from the models. The local attention mask is applied only to the encoder following a prior study (Cui et al., 2019). 4.3.3 Therefore, the relationship between BLEU and AER does not seem to be significantly correlated. Table 5 shows that the effectiveness of synchronous latent phrase structure for two layers from the top in terms of AER. In the penultimate layer, while synchronous constrain by MSE contributed to the improvement of AER, but synchronous constrain by rank loss conversely worsened AER. However, rank loss resulted in a significant improvement AER in the third and fourth layers. In the final layer, both synchronous constraints by MSE and rank loss result in the worse AER. It suggests that the qua"
2021.acl-srw.33,R19-1028,0,0.0125137,"he explainability of translation through 321 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 321–330 August 5–6, 2021. ©2021 Association for Computational Linguistics our detailed analysis in word alignment task. 2 2.1 Related Work NMT with Supervised Tree Structure In the previous work, it is reported that supervised phrase structures (Eriguchi et al., 2017; Nguyen et al., 2020) and dependency structures (Ma et al., 2019; Deguchi et al., 2019) can help the performance of MT. However, these approaches require an annotated corpus of syntactic structures. In addition, such syntactic annotation is done on wordlevel granularities, which might not be the best tokenization for MT tasks due by language mismatch or out-of-vocabulary problem, and often BPE (Sennrich et al., 2016), is employed to alleviate the problem. However, the application of BPE to grammatical information might require a different approach for each language. 2.2 Latent Grammar Induction with Neural Machine Translation Shen et al. (2018a) introduce the concept called ”syn"
2021.acl-srw.33,N13-1073,0,0.0838318,"Missing"
2021.acl-srw.33,P17-2012,0,0.0194376,"We also show that the induced phrase structures and synchronous structures can enhance the explainability of translation through 321 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 321–330 August 5–6, 2021. ©2021 Association for Computational Linguistics our detailed analysis in word alignment task. 2 2.1 Related Work NMT with Supervised Tree Structure In the previous work, it is reported that supervised phrase structures (Eriguchi et al., 2017; Nguyen et al., 2020) and dependency structures (Ma et al., 2019; Deguchi et al., 2019) can help the performance of MT. However, these approaches require an annotated corpus of syntactic structures. In addition, such syntactic annotation is done on wordlevel granularities, which might not be the best tokenization for MT tasks due by language mismatch or out-of-vocabulary problem, and often BPE (Sennrich et al., 2016), is employed to alleviate the problem. However, the application of BPE to grammatical information might require a different approach for each language. 2.2 Latent Grammar Inducti"
2021.acl-srw.33,N19-1114,0,0.0244241,"due by language mismatch or out-of-vocabulary problem, and often BPE (Sennrich et al., 2016), is employed to alleviate the problem. However, the application of BPE to grammatical information might require a different approach for each language. 2.2 Latent Grammar Induction with Neural Machine Translation Shen et al. (2018a) introduce the concept called ”syntactic distance” which represents the syntactic relation of word pairs. Similarly, Shen et al. (2018c) introduce ordered neurons which allows to learn long-term or short-term information by a novel gating mechanism and activation function. Kim et al. (2019) apply amortized variational inference for recurrent neural network grammar to learn the phrase structures in an unsupervised fashion. Wang et al. (2019) add an extra constraint to the multi-head self-attention mechanism in order to encourage the attention heads to follow phrase structures. Shen et al. (2020) introduce the constrained multi-head self-attention mechanism that allows to induct phrase and dependency structure at the same time. These works successfully learn to induce phrase structure from language modeling task without extra linguistic resources. It is described in (Htut et al.,"
2021.acl-srw.33,2005.iwslt-1.8,0,0.359767,"Missing"
2021.acl-srw.33,N19-1205,0,0.0192917,"res can enhance the explainability of translation through 321 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 321–330 August 5–6, 2021. ©2021 Association for Computational Linguistics our detailed analysis in word alignment task. 2 2.1 Related Work NMT with Supervised Tree Structure In the previous work, it is reported that supervised phrase structures (Eriguchi et al., 2017; Nguyen et al., 2020) and dependency structures (Ma et al., 2019; Deguchi et al., 2019) can help the performance of MT. However, these approaches require an annotated corpus of syntactic structures. In addition, such syntactic annotation is done on wordlevel granularities, which might not be the best tokenization for MT tasks due by language mismatch or out-of-vocabulary problem, and often BPE (Sennrich et al., 2016), is employed to alleviate the problem. However, the application of BPE to grammatical information might require a different approach for each language. 2.2 Latent Grammar Induction with Neural Machine Translation Shen et al. (2018a) introduce"
2021.acl-srw.33,P11-2093,0,0.156616,"us settings in our preliminary experiments, and this setting achieved the best performance. 324 IWSLT’14 ASPEC Europarl v7 Train 160,239 1,255,372 1,905,695 Valid 7,283 1,790 997 Test 6,750 1,812 508 Transformer w/. Synchronous Attn Transformer + LPSI w/. SynchMSE w/. SynchRank Table 1: Number of sentences in each dataset. datasets. We train the translation models on the IWSLT’14 German-English and ASPEC JapaneseEnglish (Nakazawa et al., 2016) datasets. We use the prepare iwslt14.sh for IWSLT’14 GermanEnglish and follow the instruction of constructing the baseline system of WAT 2 , but KyTea (Neubig et al., 2011) is used as the tokenizer for Japanese sentences. These datasets are applied BPE. Table 1 shows the detailed data statistics. To compare the effectiveness of synchronous latent phrase structure, we run additional baselines without latent phrase induction but with synchronous constraints applied to the attention weights. We run inference with a beam size of 5 and report the quality of translation of our models with BLEU (Papineni et al., 2002). 4.2.2 Constituency Parsing Task In this experiment, we did not apply BPE and English data was parsed using Stanford CoreNLP version 4.1.0 3 , and thus t"
2021.acl-srw.33,2006.iwslt-papers.7,0,0.170937,"Missing"
2021.acl-srw.33,D19-1098,0,0.0156242,"ation of BPE to grammatical information might require a different approach for each language. 2.2 Latent Grammar Induction with Neural Machine Translation Shen et al. (2018a) introduce the concept called ”syntactic distance” which represents the syntactic relation of word pairs. Similarly, Shen et al. (2018c) introduce ordered neurons which allows to learn long-term or short-term information by a novel gating mechanism and activation function. Kim et al. (2019) apply amortized variational inference for recurrent neural network grammar to learn the phrase structures in an unsupervised fashion. Wang et al. (2019) add an extra constraint to the multi-head self-attention mechanism in order to encourage the attention heads to follow phrase structures. Shen et al. (2020) introduce the constrained multi-head self-attention mechanism that allows to induct phrase and dependency structure at the same time. These works successfully learn to induce phrase structure from language modeling task without extra linguistic resources. It is described in (Htut et al., 2019) that translation task is a conditional language modeling task with many supervisory signals and is suitable for deriving phrase structure. Unfortun"
2021.acl-srw.33,N19-4009,0,0.0139256,"x2 ) and sign(x) is sign function. Therefore, the overall objective L is represented by: L = Ltrans + λLsync (11) P where Ltrans = − Ji log p(yi |x, y &lt;i ) where Ltrans is the objective of machine translation task and λ ≥ 0 is hyper parameter to control the degree of the synchronous constraint Lsync . x and y are source and target sentences, respectively. 4 Experiments We train our proposed models using the training objective in Equation 11 and evaluate them on three tasks: translation, constituency parsing, and word alignment. We implement models within the Fairseq sequence modeling toolkit (Ott et al., 2019). 4.1 C   (l) (l) (l) (l) hinge di − dj , d˜i − d˜j i d(l) is projected syntactic distance in lth decoder layer and computed as: (l) L X X We employ the transformer iwslt de en align fairseq configuration for German-English dataset and the transformer align fairseq configuration for Japanese-English dataset. We use two MHA layers from the bottom to induct the phrase structures, and two encoder-decoder MHA layers from the top to synchronize the encoder and decoder syntactic distances 1 . The hyper parameters are set as look back range M = 5 and temperature τ = 1.0 1 . The synchronous constrai"
2021.acl-srw.33,P02-1040,0,0.109364,"16) datasets. We use the prepare iwslt14.sh for IWSLT’14 GermanEnglish and follow the instruction of constructing the baseline system of WAT 2 , but KyTea (Neubig et al., 2011) is used as the tokenizer for Japanese sentences. These datasets are applied BPE. Table 1 shows the detailed data statistics. To compare the effectiveness of synchronous latent phrase structure, we run additional baselines without latent phrase induction but with synchronous constraints applied to the attention weights. We run inference with a beam size of 5 and report the quality of translation of our models with BLEU (Papineni et al., 2002). 4.2.2 Constituency Parsing Task In this experiment, we did not apply BPE and English data was parsed using Stanford CoreNLP version 4.1.0 3 , and thus the number of tokens in each sentence is preserved. The latent phrase structure is obtained by force decoding; we feed the gold target sentences from the test set into the word-wise trained MT models. We report unlabeled F-measure (UF) as the quality of English latent phrase structures, inducted from the bottom syntactic distances, with scoring script Evalb 4 . Here, UF is an F-measure that ignores constituency tags and evaluates only by brack"
2021.acl-srw.33,P16-1162,0,0.154595,"s grammar expresses the complex relationships between source and target languages and incorporates phrase structure to enable more linguistically accurate translation. A similar idea could be employed for NMT to achieve improved performance on those distant language pairs. However, grammatical information annotation demands high human resources. In addition, such grammatical annotation is done on word-level granularities, which might not be the best tokenization for MT tasks due by language mismatch or out-of-vocabulary problem, and often sub-word tokenization, e.g., Byte-Pair-Encoding (BPE) (Sennrich et al., 2016), is employed to alleviate the problem. As a result, it is difficult to incorporate grammatical information into NMT that handle multiple languages simultaneously. Recently, there have been researches on unsupervised learning of phrase structure without relying on human annotations. Although these phrase structures learned in an unsupervised fashion are very close to the human annotation (Shen et al., 2018a,c), there exists no model which incorporates phrase structures as latent information to improve the performance and explainability of translation. In this work, we introduce an approach to"
2021.acl-srw.33,P19-1282,0,0.0181742,"duplicated our model correctly aligns them with ’um’ compared with FastAlign. Therefore, The synchronous constraints by MSE and rank loss indicate that only alignments with high confidence are provided. Furthermore, as can be seen from the precision values in this Table 4, there are no false alignments in synchronous constrain by rank loss, and definite explainability of translation is achieved. In other words, the synchronization constraint favors precision over recall, which may make the AER worse, but it can provide a reliable explanation for human. The prior study (Jain and Wallace, 2019; Serrano and Smith, 2019) conclude that the attentions have not explainability. However, our attention is constrained by the syntactic distance, it can explain the relation between source and target sentence following the constituency tree. We will work it as the future works. 6 Conclusion This paper introduces the approach to improve the performance and explainability of MT. In the MT task, our model improves the quality of translation even through distant language pairs. In the alignment task, we demonstrate that synchronous 328 constraint for syntactic distance can produce high precisional alignments to interpret M"
2021.acl-srw.33,P18-1108,0,0.127795,"nularities, which might not be the best tokenization for MT tasks due by language mismatch or out-of-vocabulary problem, and often sub-word tokenization, e.g., Byte-Pair-Encoding (BPE) (Sennrich et al., 2016), is employed to alleviate the problem. As a result, it is difficult to incorporate grammatical information into NMT that handle multiple languages simultaneously. Recently, there have been researches on unsupervised learning of phrase structure without relying on human annotations. Although these phrase structures learned in an unsupervised fashion are very close to the human annotation (Shen et al., 2018a,c), there exists no model which incorporates phrase structures as latent information to improve the performance and explainability of translation. In this work, we introduce an approach to incorporate the phrase structure explicitly into Transformer (Vaswani et al., 2017). The approach can split into two steps; first, latent phrase structures are induced in an unsupervised fashion for the source and target sides (Shen et al., 2018a); second, the two induced latent phrase structures are synchronously agreed with each other through an attention mechanism (Deguchi et al., 2021). Experiments on"
2021.acl-srw.33,I05-1054,0,0.121559,"nolingually, we find that the induced phrase structures enhance the explainability of translation through the synchronization constraint. 1 Introduction Although machine translation (MT) has achieved improved performance using neural machine translation (NMT), the translation qualities for distant languages are still poor (Johnson et al., 2017). As a way to tackle the problem, statistical MT (SMT) incorporates synchronous grammar to achieve more linguistically accurate translations, in which complex structural relations between source and target languages are expressed using phrase structure (Wong et al., 2005). The synchronous grammar expresses the complex relationships between source and target languages and incorporates phrase structure to enable more linguistically accurate translation. A similar idea could be employed for NMT to achieve improved performance on those distant language pairs. However, grammatical information annotation demands high human resources. In addition, such grammatical annotation is done on word-level granularities, which might not be the best tokenization for MT tasks due by language mismatch or out-of-vocabulary problem, and often sub-word tokenization, e.g., Byte-Pair-"
2021.acl-srw.34,2020.crac-1.3,0,0.0358749,"used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary. Aloraini and Poesio (2020) considered word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classification task. To the best of our knowledge, our approach is the first work that formalizes ZP identification as a QA task. In recent years, approaches for solving various tasks as QA-based span prediction problems have been proposed. Li et al. (2020) made questions corresponding to NER entity tags. Then, their model predicted the entity span giving the question and a sentence as QA tasks to tackle the nested NER problem. In the coreference resol"
2021.acl-srw.34,N19-1423,0,0.0907288,"Science and Technology 2 NTT Communication Science Laboratories, NTT Corporation {iwata.sei.is6,taro}@is.naist.jp masaaki.nagata.et@hco.ntt.co.jp Abstract When identifying a ZP from the sentence where the argument is omitted, the predicate information is the key. The ZP identification is solved in many previous works as a labeling task for input sentence tokens (Aloraini and Poesio, 2020; Song et al., 2020) or nodes in a parse tree (Xiang et al., 2013; Takeno et al., 2015). In this study, we treat ZP identification as an instance of span prediction tasks inspired by the QA method proposed in Devlin et al. (2019). There are two steps to solve the ZP identification in our approach. 1) Given a predicate as a query, our model extracts each argument, such as subject or object, as the answer from the input sentence. 2) If our model cannot extract any corresponding argument from the input sentence, the model predicts whether or not it is a ZP. In the above example, given a predicate 気に入った “like”, our model should predict that the subject argument is 私は “I” in the sentence and the object argument is a ZP. By explicitly providing predicates as queries in this way, our approach allows the model to capture info"
2021.acl-srw.34,2020.acl-main.622,0,0.0295219,"word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classification task. To the best of our knowledge, our approach is the first work that formalizes ZP identification as a QA task. In recent years, approaches for solving various tasks as QA-based span prediction problems have been proposed. Li et al. (2020) made questions corresponding to NER entity tags. Then, their model predicted the entity span giving the question and a sentence as QA tasks to tackle the nested NER problem. In the coreference resolution task, Wu et al. (2020) generated queries based on each mention and extracted the text spans of coreferences as answers to given queries. Nagata et al. (2020) improved the performance of word alignment task by giving a word in the source language sentence as a question and predicting its corresponding word span in the target language sentence. 3 Our argument span prediction is inspired by BERT fine-tuning for the QA task (Devlin et al., 2019). Inputs follow a BERT style formulated as “[CLS] query [SEP] sentence [SEP]”, where [CLS] is a special token to output the classification result and [SEP] denotes the boundary"
2021.acl-srw.34,P13-1081,0,0.0341127,"the object argument (OBJ) is omitted from the second sentence because Japanese speakers can predict from the context that the OBJ is “it”, and the omission is natural for the Japanese speakers. Downstream tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labelin"
2021.acl-srw.34,P11-1081,0,0.0276918,"f the baseline and QAZP, our proposal model for three sentences in a Japanese ZP identification task. Each line represents either the prediction of one of the both models, or the Gold data for the argument of a predicate covered by the lines. The first and third examples are predictions for SBJ arguments, and the second example is a prediction result for the OBJ arguments by the baseline and QAZP. Model Baseline QAZP Arg SBJ ALL SBJ ALL Arg span accuracy 88.7 88.3 ZP F1 71.5 71.4 80.6 80.5 ZP pre 72.5 72.6 81.2 81.0 ZP recall 70.5 70.3 80.6 80.4 Integer Linear Programming as in the method of (Iida and Poesio, 2011). Example 3 is the case when the predictions of both models are incorrect. In this example sentence, the gold ZP class is the first person “speaker”, but it is impossible to identify the ZP without knowing the context before and after the input sentence. We expect our model will capture context information by extending the input unit to multiple sentences instead of a single sentence. Table 5: Argument(Arg) span accuracy and ZP detection on OntoNotes5.0. for “pro” class. The row of ALL indiciataes the value for SBJ, OBJ and IO2 arguments. cates お 話し “speak” and 復習 “review” are different. While"
2021.acl-srw.34,2020.acl-main.519,0,0.0429663,"ch Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary. Aloraini and Poesio (2020) considered word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classification task. To the best of our knowledge, our approach is the first work that formalizes ZP identification as a QA task. In recent years, approaches for solving various tasks as QA-based span prediction problems have been proposed. Li et al. (2020) made questions corresponding to NER entity tags. Then, their model predicted the entity span giving the question and a sentence as QA tasks to tackle the nested NER problem. In the coreference resolution task, Wu et al. (2020) generated queries based on each mention and extracted the text spans of coreferences as answers to given queries. Nagata et al. (2020) improved the performance of word alignment task by giving a word in the source language sentence as a question and predicting its corresponding word span in the target language sentence. 3 Our argument span prediction is inspired by BERT"
2021.acl-srw.34,2020.emnlp-main.41,1,0.867501,"Missing"
2021.acl-srw.34,2020.acl-main.482,0,0.0181744,"tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary. Aloraini and Poesio (2020) considered word positions before or after each VP node as ZP location candidates and predicted whether the candidate has ZP or not as a binary classifi"
2021.acl-srw.34,D15-1156,1,0.832364,") is omitted from the second sentence because Japanese speakers can predict from the context that the OBJ is “it”, and the omission is natural for the Japanese speakers. Downstream tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August 5–6, 2021. ©2021 Association for Computational Linguistics ZP resolution and ZP identification by treating it as sequence labeling on every word boundary."
2021.acl-srw.34,D19-1085,0,0.0219258,"linguistics. JA こ の ケ ー キ は 美 味 し い 。私は (pro-OBJ) 気に入った． EN This cake is delicious. I like (it). In the Japanese example above, the object argument (OBJ) is omitted from the second sentence because Japanese speakers can predict from the context that the OBJ is “it”, and the omission is natural for the Japanese speakers. Downstream tasks involving pro-drop languages could easily suffer from the existence of ZPs. In the machine translation task, it has been reported that supplementing the ZP information when translating from pro-drop languages to non-pro-drop languages improves the performance (Wang et al., 2019). 2 Related work Most of the researchers considered the ZP detection or ZP identification as a labeling task. Xiang et al. (2013) and Takeno et al. (2015) used parse trees as input and detected empty categories, including ZPs, by labeling a node representing the maximal projection of a predicate, namely IP or VP. Song et al. (2020) proposed jointly learning 331 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 331–336 August"
2021.calcs-1.18,P16-1162,0,0.0435963,"To implement the language classifier, we prepared two monolingual corpora of Tatar and Russian. Given the lack of pure Tatar texts without CS in modern texts4 , we employed Tatar translation of Qur’an5 (19,691 words with duplication) translated in 1912 that contains no Russian loanwords in order to avoid noise to train the classifier. Its Russian counterpart6 (21,256 words with duplication) was translated by the Ministry of Awqaf, Egypt. The training process is as follows. First, the words collected from the dataset were automatically divided into subwords by the Byte Pair Encoding algorithm (Sennrich et al., 2016). Banerjee and Bhattacharyya (2018) reports that, unlike Morfessor, BPE can flexibly solve the OOV problem because some subwords are character-level segments. In our case, due to the meagerness of the monolingual training data, we employed BPE to avoid the OOV problem. Then, assuming that longer subwords are less ambiguous with respect to labels to be assigned, we took the longest match to make it easier to distinguish between Russian and Tatar; for this reason, the subword merge operation was repeated until no further merge was possible. The obtained subwords are then represented in subword e"
2021.calcs-1.18,2020.calcs-1.5,0,0.0317841,"nqedәn qә baxlagan ide. translit.: Bezneñ klassta qızlar sigezençedän eçä ba¸sla˘gan ide. “In our class, girls used to start drinking by the eighth grade.” 3 Related Work Code-Switching. Even though CS has attracted researchers in NLP, the lack of resource has been a major difficulty, because CS is an exclusively colloquial linguistic phenomenon and CS texts are seldom recorded. Jose et al. (2020) enumerates a list of available CS datasets at the time of the publication. In terms of both the availability of datasets and the popularity of research, CS language pairs in trend are Hindi–English (Srivastava et al., 2020, Singh and Lefever, 2020), Spanish– English (Alvarez-Mellado, 2020, Claeser et al., 2018), Arabic varieties and Modern Standard Arabic (Hamed et al., 2019). As for the studies of intra-word CS in other languages, Mager et al. (2019) for German–Turkish and Spanish–Wixarika, Nguyen and Cornips (2016) for Dutch–Limburgish, and Yirmibe¸so˘glu and Eryi˘git (2018) for Turkish–English have a similar approach to ours. The differences from ours are that Mager et al. (2019) employs segRNN (Lu et al., 2016) for segmentation and language identification, and that Nguyen and Cornips (2016) uses Morfessor ("
2021.calcs-1.18,W18-6115,0,0.0375752,"Missing"
2021.eacl-main.323,W14-3348,0,0.0239705,"mber of the object pairs was 11,607 and 10,612 in the settings A and B, respectively. We set the batch size to eight and terminated the training when the best validation score (specifically, the CIDEr score) did not exceed for 20 epochs. For the optimizer, we used Adam with the recommended hyperparameters (Kingma and Ba, 2015). Evaluation. In the evaluation, we set the maximum decoding length to 20. Our model decoded captions by using greedy search and unique-object decoding, described in Section 2.4. The evaluation metrics we used were BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). 3.3 Comparison with the State-of-the-Art Results Table 2 lists the results of our model compared with the previous state-of-the-art results. To avoid evaluating cherry-picked scores, we computed the mean and standard deviation of five results obtained with different seeds8 . Our method outperforms the previous approaches in terms of all evaluation metrics. These results confirm the effectiveness of our simple method. 8 In all the experiments, we specified a seed of 0, 1, 2, 3, 4 for each run. 3696 A B gate pseudoL unique image"
2021.eacl-main.323,W04-1013,0,0.0573135,"in the pairs. The number of the object pairs was 11,607 and 10,612 in the settings A and B, respectively. We set the batch size to eight and terminated the training when the best validation score (specifically, the CIDEr score) did not exceed for 20 epochs. For the optimizer, we used Adam with the recommended hyperparameters (Kingma and Ba, 2015). Evaluation. In the evaluation, we set the maximum decoding length to 20. Our model decoded captions by using greedy search and unique-object decoding, described in Section 2.4. The evaluation metrics we used were BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). 3.3 Comparison with the State-of-the-Art Results Table 2 lists the results of our model compared with the previous state-of-the-art results. To avoid evaluating cherry-picked scores, we computed the mean and standard deviation of five results obtained with different seeds8 . Our method outperforms the previous approaches in terms of all evaluation metrics. These results confirm the effectiveness of our simple method. 8 In all the experiments, we specified a seed of 0, 1, 2, 3, 4 for each run."
2021.eacl-main.323,K19-1009,0,0.014115,"nsidering images: is sitting on followed cat in both (c) and (e). 4 Related Work There has been considerable research with different settings and approaches to describe scenes that have no image–sentence pairs. Novel object captioning (Hendricks et al., 2016; Venugopalan et al., 2017; Anderson et al., 2018a; Agrawal et al., 2019) attempted describing unseen objects in captions. 3699 They incorporated an image classifier or object detector trained on objects not included in image– sentence pairs. Lu et al. (2018) tested captioning models on the generation of unseen combinations of objects, and Nikolaus et al. (2019) extended this to the unseen combinations of objects, attributes, and relations. In both settings, only the combinations were unseen, but each word in the combinations appeared in the training data. Semisupervised approaches utilized caption retrieval models to automatically collect the corresponding captions for unannotated images to augment image– sentence pairs (Liu et al., 2018; Kim et al., 2019). The above work was evaluated on the scenes where correct descriptions partially overlapped with those in the training image–sentence pairs. However, there can be scenes with no such overlap due t"
2021.eacl-main.323,P02-1040,0,0.114114,"e same sampling on each object in the pairs. The number of the object pairs was 11,607 and 10,612 in the settings A and B, respectively. We set the batch size to eight and terminated the training when the best validation score (specifically, the CIDEr score) did not exceed for 20 epochs. For the optimizer, we used Adam with the recommended hyperparameters (Kingma and Ba, 2015). Evaluation. In the evaluation, we set the maximum decoding length to 20. Our model decoded captions by using greedy search and unique-object decoding, described in Section 2.4. The evaluation metrics we used were BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). 3.3 Comparison with the State-of-the-Art Results Table 2 lists the results of our model compared with the previous state-of-the-art results. To avoid evaluating cherry-picked scores, we computed the mean and standard deviation of five results obtained with different seeds8 . Our method outperforms the previous approaches in terms of all evaluation metrics. These results confirm the effectiveness of our simple method. 8 In all the experiments, we specified a seed of 0, 1, 2,"
2021.eacl-main.323,P18-1238,0,0.0145311,"ious work; thus, we used the object detector trained on OpenImagesv2 (Krasin et al., 2017) to compare with Feng et al. (2019) and that trained on OpenImages-v4 (Kuznetsova et al., 2020) to compare with Laina et al. (2019). Note that these object detectors were not trained on MS COCO images. Following the previous work, we refrained from using the detected bounding boxes and their features. Training Text. Following the previous work, we used the Shutterstock image description corpus (SS) (Feng et al., 2019) and the training split captions (without images) of Google’s Conceptual Captions (GCC) (Sharma et al., 2018) for comparison with Feng et al. (2019) and Laina et al. (2019), respectively. SS consists of 2.3M image descriptions crawled from Shutterstock, an online stock photography website; GCC consists of 3.3M image descriptions crawled from the web. Note that these sentences are not the descriptions of the images in MS COCO. 3.2 Implementation Details Image Encoder. For a fair comparison with the previous work, we employed different image encoders depending on the compared method: Inception-v4 (Szegedy et al., 2017) in the settings of Feng et al. (2019) and ResNet-101 (He et al., 2016a,b) in the set"
2021.eacl-main.323,Q14-1006,0,0.0530427,"rformance. These results confirm the importance of careful alignment in word-level details.1 1 Introduction Image captioning is a task to describe images in natural languages. This is a fundamental challenge with regard to automatically retrieving and summarizing the visual information in a human-readable form. Recently, considerable progress has been made (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018b) owing to the development of neural networks and a large number of annotated 1 Code will be available at https://github.com/ ukyh/RemovingSpuriousAlignment image–sentence pairs (Young et al., 2014; Lin et al., 2014; Krishna et al., 2017). However, these pairs are limited in their coverage of scenes2 , and scaling them is difficult owing to the cost of manual annotation. Unsupervised image captioning (Feng et al., 2019) aims to describe scenes that have no corresponding image–sentence pairs, without requiring additional annotation of the pairs. The only available resources are images and sentences drawn from different sources and object labels detected from the images. Although it is highly challenging, unsupervised image captioning has the potential to cover a broad range of scenes by"
2021.findings-acl.164,K18-2005,0,0.0214902,"number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experiments respectively. 4.3 Evaluation NER experiments are evaluated by using F1 scores, and POS tagging experiments are evaluated with accuracy scores. All of our experiments were run 4 times with different random seeds, and the averaged scores are reported in the following tables. Our models3 are implemented with deep learning framework"
2021.findings-acl.164,Q16-1026,0,0.580398,"ly than Cui and Zhang (2019). Notably, the model of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperfor"
2021.findings-acl.164,D19-1422,0,0.0595106,"ploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term label dependency relations through multiple refinements layers. Individually picking up the most likely label at each time step is undoubtedly critical, however, considering the entire historical progress is also indispensable. We find that the locally normalized attention, which Cui and Zhang (2019) used to leverage information from label embeddings, can eventually hurt performance. Since it only considers the current time step but ignores labels at other time s"
2021.findings-acl.164,N19-1423,0,0.180674,"g rate ητ = η0 /(1 + 0.075 · τ ), where τ is the index of the current epoch, and the initial learning rate η0 for Chinese experiments without contextual word representations is 0.05, and for all the other experiments we use 0.1. The weight decay rate is 10−8 , the momentum is 0.15, the batch size is 10, the number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experiments respectively. 4.3 Evaluat"
2021.findings-acl.164,C18-1161,0,0.0258358,"Missing"
2021.findings-acl.164,D19-1096,0,0.0263898,"Missing"
2021.findings-acl.164,P19-1027,0,0.0111793,", each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recentl"
2021.findings-acl.164,D19-1399,0,0.169071,"named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term labe"
2021.findings-acl.164,N16-1030,0,0.747727,"ity, while O signifies this word is outside any named entity. In the case of POS tagging, each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created"
2021.findings-acl.164,D19-1099,0,0.0456713,"Missing"
2021.findings-acl.164,D18-1149,0,0.0286425,"he widespread use of contextual word representations, e.g., ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), and BERT (Devlin et al., 2019), greatly improves the performance of NER models and they are accepted as new fundamental techniques of natural language processing. Intuitively speaking, the refinement mechanism provides the models with additional chances to revise previous decisions. In existing work, this method was successfully applied to various tasks, e.g., text classification (Yu et al., 2017), sequential labeling (Cui and Zhang, 2019; Lyu et al., 2019), machine translation (Lee et al., 2018), and question answering (Nema et al., 2019). Our work is not the first attempt of introducing refinement mechanism to sequential labeling tasks. Cui and Zhang (2019) relied on locally normalized attention to softly refine hidden representations layer by layer, while Liu et al. (2019a) chose to discretely filter out target-irrelevant semantic aspects and thus could be considered as a hard refinement mechanism. 6 Conclusion Motivated by the structured attention, we enhanced the previous refinement mechanism by replacing the locally normalized attention with our globally normalized attention. Ex"
2021.findings-acl.164,P16-1101,0,0.630049,"is word is outside any named entity. In the case of POS tagging, each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target"
2021.findings-acl.164,D17-1282,0,0.103833,"l of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 score with BERT. More"
2021.findings-acl.164,2020.acl-main.611,0,0.138687,"We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets We conduct exper"
2021.findings-acl.164,2020.acl-main.519,0,0.207212,"We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets We conduct exper"
2021.findings-acl.164,P19-1532,0,0.334171,"word representation already provides rich enough morphological information, thus careful character embeddings initialization can only bring little benefit. On the OntoNotes 5.0 ChiTable 2: Experimental results on the OntoNotes 5.0 English dataset. Checkmark X in the “EK” column indicates that external knowledge is utilized in that model. [E] and [B] stands for ELMo and BERT respectively. Bold and underlined numbers indicate the best and the second-best results respectively. Model EK P R F1 Huang et al. (2015) Lample et al. (2016) Ma and Hovy (2016) Zhang et al. (2018) Chiu and Nichols (2016) Liu et al. (2019a) Yan et al. (2019) Liu et al. (2019b) Our Method X X X - 90.70 90.81 88.83 90.94 91.21 91.57 91.62 91.80 91.33 91.96 90.76 Jie and Lu (2019) [E] Yan et al. (2019)[E] Our Method [E] X - 92.60 93.19 92.40 92.62 92.89 Devlin et al. (2019) [B] Li et al. (2020b) [B] Yu et al. (2020) [B] Our Method [B] - 92.33 93.7 92.66 94.61 93.3 92.98 92.8 93.04 93.5 93.23 Model EK P R F1 Zhang and Yang (2018) Mengge et al. (2019) Gui et al. (2019a) Gui et al. (2019b) Yan et al. (2019) Li et al. (2020a) Our Method Our Method (init) X X X X X X - 76.35 76.78 76.40 76.13 75.28 75.49 71.56 72.54 72.60 73.68 72.39"
2021.findings-acl.164,P19-1233,0,0.180243,"word representation already provides rich enough morphological information, thus careful character embeddings initialization can only bring little benefit. On the OntoNotes 5.0 ChiTable 2: Experimental results on the OntoNotes 5.0 English dataset. Checkmark X in the “EK” column indicates that external knowledge is utilized in that model. [E] and [B] stands for ELMo and BERT respectively. Bold and underlined numbers indicate the best and the second-best results respectively. Model EK P R F1 Huang et al. (2015) Lample et al. (2016) Ma and Hovy (2016) Zhang et al. (2018) Chiu and Nichols (2016) Liu et al. (2019a) Yan et al. (2019) Liu et al. (2019b) Our Method X X X - 90.70 90.81 88.83 90.94 91.21 91.57 91.62 91.80 91.33 91.96 90.76 Jie and Lu (2019) [E] Yan et al. (2019)[E] Our Method [E] X - 92.60 93.19 92.40 92.62 92.89 Devlin et al. (2019) [B] Li et al. (2020b) [B] Yu et al. (2020) [B] Our Method [B] - 92.33 93.7 92.66 94.61 93.3 92.98 92.8 93.04 93.5 93.23 Model EK P R F1 Zhang and Yang (2018) Mengge et al. (2019) Gui et al. (2019a) Gui et al. (2019b) Yan et al. (2019) Li et al. (2020a) Our Method Our Method (init) X X X X X X - 76.35 76.78 76.40 76.13 75.28 75.49 71.56 72.54 72.60 73.68 72.39"
2021.findings-acl.164,L18-1008,0,0.0245265,"(Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of 0.5. For encoding and refinement layers, the dimension of the hidden state dh of bidirectional LSTMs is 600, i.e., 300 in each direction. We apply dropout (l) on hidden states ht with a rate of 0.5 before feeding into refinement layers. The number of refinement layers L is just 1. We optimize our model by app"
2021.findings-acl.164,D19-1326,0,0.056138,"Missing"
2021.findings-acl.164,D14-1162,0,0.0940471,"of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of 0.5. For encoding and refinement layers, the dimension of the hidden state dh of bidirectional LSTMs is 600, i.e., 300 in each direction. We apply dropout (l) on hidden states ht with a rate of 0."
2021.findings-acl.164,N18-1202,0,0.241169,"ent (SGD) with decaying learning rate ητ = η0 /(1 + 0.075 · τ ), where τ is the index of the current epoch, and the initial learning rate η0 for Chinese experiments without contextual word representations is 0.05, and for all the other experiments we use 0.1. The weight decay rate is 10−8 , the momentum is 0.15, the batch size is 10, the number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experi"
2021.findings-acl.164,W13-3516,0,0.0423898,"Missing"
2021.findings-acl.164,W95-0107,0,0.625899,"lish 38,219 / 5,527 / 5,462 12,544 / 2,003 / 2,078 45 50 Table 1: Dataset statistics, where the “Sentences” column displays the number of sentences in train/dev/test split respectively, the |Y |column displays the number of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token repres"
2021.findings-acl.164,W09-1119,0,0.111709,"2 12,544 / 2,003 / 2,078 45 50 Table 1: Dataset statistics, where the “Sentences” column displays the number of sentences in train/dev/test split respectively, the |Y |column displays the number of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of"
2021.findings-acl.164,2020.acl-demos.38,0,0.0284906,"We apply the Viterbi algorithm (Forney, 1973) to efficiently search for the most probable label sequences on the decoding stage. ˆ = arg max p (y 0 |h(L+1) ) y (14) y 0 ∈Y n 3.4 Complexity and Implementation Tricks One concern regarding our proposed method is its computational complexity, as it requires to compute not only the partition function but also the marginal probability. Calculating the partition function, as in Equation 8, is the well-known bottleneck of CRF computation. And this is commonly achieved through reducing potential matrices by applying matrix multiplications. Similar to Rush (2020), we make use of the associative property of matrix multiplication to accelerate computation. The product of multiplying matrices A, B, C, and D is equivalent to the product of AB and CD. Leveraging the power of GPU to compute AB and CD in parallel, and recursively applying this trick, we can reduce the time complexity of obtainP|B| ing the partition function from O ( i=1 |x|i ) to P|B| O ( i=1 log |x|i ), where |x|i is the length of i-th sentence in batch B. Moreover, instead of padding the sequence length |xi |out to the nearest power of two as Rush (2020) does, we pre-compile argument indic"
2021.findings-acl.164,D17-1283,0,0.114274,"019). Notably, the model of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 scor"
2021.findings-acl.164,D18-1279,0,0.0300428,"Missing"
2021.findings-acl.164,N18-1089,0,0.0439197,"Missing"
2021.findings-acl.164,2020.acl-main.577,0,0.129141,"to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 score with BERT. Moreover, on the OntoNotes 5.0 Chinese dataset, our model constantly outperforms the best previous work (Jie and Lu, 2019) by 0.65 F1 score without utilizing external knowledge. Besides, we can notice initializing character embeddings with our trick remarkably improves model performance by 0.76 F1 score on the OntoNotes 4.0 Chinese dataset, even this improvement reduces to only 0.00 and 0.20 F1 scores on ELMo and BERT experiments."
2021.findings-acl.164,D17-1056,0,0.0258509,"ll possible spans and to utilize a biaffine classifier to assign category labels to them. Besides, the widespread use of contextual word representations, e.g., ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), and BERT (Devlin et al., 2019), greatly improves the performance of NER models and they are accepted as new fundamental techniques of natural language processing. Intuitively speaking, the refinement mechanism provides the models with additional chances to revise previous decisions. In existing work, this method was successfully applied to various tasks, e.g., text classification (Yu et al., 2017), sequential labeling (Cui and Zhang, 2019; Lyu et al., 2019), machine translation (Lee et al., 2018), and question answering (Nema et al., 2019). Our work is not the first attempt of introducing refinement mechanism to sequential labeling tasks. Cui and Zhang (2019) relied on locally normalized attention to softly refine hidden representations layer by layer, while Liu et al. (2019a) chose to discretely filter out target-irrelevant semantic aspects and thus could be considered as a hard refinement mechanism. 6 Conclusion Motivated by the structured attention, we enhanced the previous refineme"
2021.findings-acl.164,P18-1030,0,0.266715,"ge processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term label dependency relations through multiple refinements layers. Individually picking up the most likely label at"
2021.findings-acl.164,P18-1144,0,0.01654,"to O (maxi log |x|i ). We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets"
2021.naacl-main.438,W15-4319,0,0.048961,"Missing"
2021.naacl-main.438,2020.lrec-1.773,0,0.0611393,"Missing"
2021.naacl-main.438,P11-1038,0,0.0383248,"al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated catego"
2021.naacl-main.438,W15-4313,0,0.0214366,"ion of pe- Saito et al. (2017) extracted formal-informal word culiar expressions is most similar to our types of pairs from unlabeled Twitter data based on semanvariant forms and Kaji et al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et a"
2021.naacl-main.438,D14-1011,0,0.0999202,"emmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to distributing tweet URLs has th"
2021.naacl-main.438,W04-3230,0,0.164412,"Missing"
2021.naacl-main.438,D08-1108,0,0.0604526,"al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated categories for a detailed evaluation of English LN systems. It includes phrasal abbreviation (e.g., “idk” pseudo training data from English raw tweets using noise insertion operations to achieve comparable for “I don’t know”), repetition (e.g., “soooo” for “so”), and phonetic transformation (e.g., “hackd” performance without manually annotated data to an existing LN system. for “hacked”). For Chinese, Li and Yarowsky (2008) classified informal words in Chinese webpages into four 19 types: homophone (informal words with similar Pinyin pronunciation is shown in “hi”. 5539 7 Conclusion We presented a publicly available Japanese UGT corpus annotated with morphological and normalization information. Our corpus enables the performance comparison of existing and future systems and identifies the main remaining issues of MA and LN of UGT. Experiments on our corpus demonstrated the limited performance of the existing systems for non-general words and non-standard forms mainly caused by two types of difficult examples: co"
2021.naacl-main.438,P11-2093,0,0.0259792,"entence IDs and annotation information, including word boundaries, POS, lemmas, standard forms of non-standard word tokens, and word categories. We will release the annotation information that enables BCCWJ applicants to replicate the full BQNC data from the original BCCWJ data.3 Using the BQNC, we evaluated two existing Japanese morphological analysis (MA) is a fundamental and important task that involves word segmentation, part-of-speech (POS) tagging and lemmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available a"
2021.naacl-main.438,D15-1211,0,0.0186263,"classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated categories for a detailed evaluation of English LN systems. It includes phrasal abbreviation (e.g., “idk” pseudo training data from English ra"
2021.naacl-main.438,I17-1094,0,0.0705833,"o¯ ‘oh’ and サラサラ〜 was normalized to サラサラ sarasara ‘smoothly’). However, we assessed these as errors based on our criterion that interjections have no (non-)standard forms and the BCCWJ guidelines that regards onomatopoeia with and without long sound insertion as different lemmas. malization errors into two types: complicated variant forms and unknown words of specific vocabulary types such as emoticons and neologisms/slang. The effective use of linguistic resources may be required to build more accurate systems, for example, discovering variant form candidates from large raw text similar to (Saito et al., 2017), and constructing/using term dictionaries of specific vocabulary types. 6 Related Work UGT Corpus for MA and LN Hashimoto et al. (2011) developed a Japanese blog corpus with morphological, grammatical, and sentiment information, but it contains only 38 non-standard forms and 102 misspellings as UGT-specific examples. Osaki et al. (2017) constructed a Japanese Twitter corpus annotated with morphological information and standard word forms. Although they published tweet URLs along with annotation information, we could only restore parts of sentences because of the deletion of the original tweet"
2021.naacl-main.438,P19-3032,0,0.0256965,"Missing"
2021.naacl-main.438,L18-1109,0,0.0396716,"Missing"
2021.naacl-main.438,I13-1015,0,0.0257161,"irs of formal and informal words on Twitter. variants. Ikeda et al. (2010)’s classification of pe- Saito et al. (2017) extracted formal-informal word culiar expressions is most similar to our types of pairs from unlabeled Twitter data based on semanvariant forms and Kaji et al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “befo"
2021.naacl-main.438,D13-1007,0,0.0741565,"Missing"
2021.naacl-main.438,C14-1167,0,0.0805651,"anese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to distributing tweet URLs has the disadvantage that"
2021.naacl-main.438,I13-1019,0,0.0948969,"h (POS) tagging and lemmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to di"
2021.starsem-1.20,W18-4912,0,0.0122904,"11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is called “next sentence prediction task” (NSP). In the process of pretraining using NSP, the model is presented with pairs of sentences. The model predicts whether the second sentence is the actual subsequent sentence. NSP enables BERT to represent a pair of sentences by packing them together as a single sequence. Some studies have focused on discourse structure in AMR framework. Donatelli et al. (2018) enhances AMR by annontating tense and aspect phenomena at discourse-level. The work by O’Gorman et al. (2018) targets relations of sentences and provides annotation of coreference in multi-sentence AMR corpus. Yet, neither the structure of the complex sentence constructions nor the coherence relations between subordinate and matrix clauses have been much of a concern in this framework. 3 GHSHQGHQFWUHH FKDUWHUHG DGYFO QVXEM GLVUXSWHG PDUN 6LQFH QVXEMSDVV IOLJKWV FRPSDQ SODQH WKH D FRS ZHUH DLUOLQH 3DWWHUQ0DWFKLQJ FKDUWHUHG DGYFO QVXEM ZHUH PDUN 6LQFH QVXEMSDVV GLVUXSWHG IOLJKWV FRPSDQ S"
2021.starsem-1.20,P14-1134,0,0.0326336,"mong AMR parsers which are aware of syntactic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics,"
2021.starsem-1.20,D15-1198,0,0.0183329,"tic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok"
2021.starsem-1.20,C18-1048,0,0.0121876,"HUHG DSODQHWRIOWKHH[HFXWLYHVEDFNWRWKH:HVW&RDVW /H[LFDO 6QWDFWLF3URFHVVLQJ Related Works While our focus is on clause-level relation of complex sentence constructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is"
2021.starsem-1.20,P17-1014,0,0.0328637,"a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics FDXVH EHOLHYH $5* $5* JLUO $5* VHHP $5* ER $5* EHOLHYH VHHP $5*"
2021.starsem-1.20,W13-2322,0,0.048075,"sentences, various types of complex sentence are used in human language. This characteristics makes it challenging for existing AMR parsers to capture its structure correctly. Among AMR parsers which are aware of syntactic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in commo"
2021.starsem-1.20,2020.lrec-1.497,0,0.0616752,"Missing"
2021.starsem-1.20,L18-1266,0,0.0220631,"Missing"
2021.starsem-1.20,C18-1313,0,0.0380173,"Missing"
2021.starsem-1.20,2020.acl-main.119,0,0.0111753,"ouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics FDXVH EHOLHYH $5* $5* JLUO $5* VHHP $5* ER $5* EHOLHYH VHHP $5* $5* $5* ER UHOLDEOH UHOLDEOH GRPDLQ $5* JLUO $5*"
2021.starsem-1.20,J05-1004,0,0.158861,"Wikipedia corpus, establishing a new baseline for future works. The developed complex sentence patterns and the corresponding AMR descriptions will be made public1. 1 6 9 ORQJ DGYFO QVXEM 6 9 PDUN DV 9 $5* DGYPRG FRQGLWLRQ 6 DGYPRG DVORQJDV DV 9 RS $5* 6 Figure 1: Representation of as long as-construction in dependency tree (left) and AMR graph (right). that dependency trees and semantic role labeling structures have a strong correlation in that nsubj and dobj can be used interchangeably for ARG0 and ARG1 role (Xia et al., 2019). Since AMR is annotated based on PropBank frames (Palmer et al., 2005), the same could be said for AMR structures. This holds to be true for a simple sentence, which is basically a matrix clause, comprised of a predicate and its arguments. However, it is not always the case with complex sentence constructions, each of which consists of a matrix clause and one or more subordinate clause(s). Consider Figure 1 which shows both dependency and AMR representation of a complex sentence with a subordinator as long as. While variables S’s and V’s are interchangeable between the representations, predicative relations and subordinator itself are expressed quite diﬀerently."
2021.starsem-1.20,E17-1035,0,0.013225,"rse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics FDXVH EHOLHYH $5* $5* JLUO $5* VHHP $5* ER $5* EHOLHYH VHHP $5* $5* $5* ER UH"
2021.starsem-1.20,A00-2015,1,0.472139,"(hereinafter referred to as“ skeletal AMRs ”). Then, we provide a pattern matcher which captures clausal relations between a superordinate and subordinate clauses in a complex sentence. Our pattern matching approach faces the problem of syntactic and semantic ambiguities. When a complex sentence has more than one subordinate clause, we need to determine which pair of clauses are related. Consider the following example where two subordinate clauses appear in a single sentence. 199-192783-6849434_0102.3) While there has been studies regarding the syntactic scope of a subordinate clause such as Utsuro et al. (2000), this problem is beyond the scope of this paper. We rely on the output of the dependency parser, which we employ in our pattern matching system, to decide which pair of clauses are syntactically related. Meanwhile, when a subordinator itself is ambiguous between several senses, we need to select the correct type of coherence relation between the clauses. Sentences in (2) show usages of a subordinator since, which is semantically ambiguous between causal and temporal senses. (2) a. Since there is responsibility, we are not afraid. (AMR: bolt12_6455_6561.15) b. Also since he turned 80, people h"
2021.starsem-1.20,prasad-etal-2008-penn,0,0.0730739,"ructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is called “next sentence prediction task” (NSP). In the process of pretraining using NSP, the model is presented with pairs of sentences. The model predicts whether the second"
2021.starsem-1.20,N15-1040,0,0.0236498,"and one or more subordinate clause(s). Consider Figure 1 which shows both dependency and AMR representation of a complex sentence with a subordinator as long as. While variables S’s and V’s are interchangeable between the representations, predicative relations and subordinator itself are expressed quite diﬀerently. Compared to uniform structures of simple sentences, various types of complex sentence are used in human language. This characteristics makes it challenging for existing AMR parsers to capture its structure correctly. Among AMR parsers which are aware of syntactic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) u"
2021.starsem-1.20,N19-1075,0,0.0152462,"through training classiﬁcation models on data derived from AMR and Wikipedia corpus, establishing a new baseline for future works. The developed complex sentence patterns and the corresponding AMR descriptions will be made public1. 1 6 9 ORQJ DGYFO QVXEM 6 9 PDUN DV 9 $5* DGYPRG FRQGLWLRQ 6 DGYPRG DVORQJDV DV 9 RS $5* 6 Figure 1: Representation of as long as-construction in dependency tree (left) and AMR graph (right). that dependency trees and semantic role labeling structures have a strong correlation in that nsubj and dobj can be used interchangeably for ARG0 and ARG1 role (Xia et al., 2019). Since AMR is annotated based on PropBank frames (Palmer et al., 2005), the same could be said for AMR structures. This holds to be true for a simple sentence, which is basically a matrix clause, comprised of a predicate and its arguments. However, it is not always the case with complex sentence constructions, each of which consists of a matrix clause and one or more subordinate clause(s). Consider Figure 1 which shows both dependency and AMR representation of a complex sentence with a subordinator as long as. While variables S’s and V’s are interchangeable between the representations, predic"
2021.starsem-1.20,D15-1136,0,0.0153082,"(Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online)"
2021.starsem-1.20,D19-1392,0,0.0260054,"Missing"
2021.starsem-1.20,2020.acl-demos.14,0,0.0127711,"not ﬁnd a copular clause with complex sentence patterns alone. To avoid redundancy of creating additional patterns substituting V’s with copulas for each entry in dictionary, we make an extra pattern outside that describes a copula-complement structure: 3For “John is tall.”, UD treats “tall” as a head of “is”. 216 ^5(*(;AQVXEM` V 4.1 Experimental Setup F 7$*^5(*(;A1eA-` For creating the dataset, we use the lateset release of AMR corpus (LDC2020T02), which provides 59.2k pairs of sentences and AMR graphs. To extract complex sentence constructions from the corpus, we use the Stanza pipeline (Qi et al., 2020) for lexical and syntactic processing of the sentences and employ our pattern matcher with all patterns in the dictionary. In order to check whether the corresponding AMR graph describes the relation we want for each class, we look for alignments between sentence tokens and AMR graphs45. Finally, we split the sentence to obtain a pair of clauses and a subordinator. While the data derived from AMR corpus can be regarded as “supervised”, the amount is relatively small with the total of 1,933 pairs of subordinate and matrix clauses. As it consumes time and money to create more supervised data, we"
2021.starsem-1.20,P17-1093,0,0.0117107,"WKHFRPSDQFKDUWHUHG DSODQHWRIOWKHH[HFXWLYHVEDFNWRWKH:HVW&RDVW /H[LFDO 6QWDFWLF3URFHVVLQJ Related Works While our focus is on clause-level relation of complex sentence constructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pr"
2021.starsem-1.20,E17-1027,0,0.0121815,"HIOLJKWVZHUHGLVUXSWHGWKHFRPSDQFKDUWHUHG DSODQHWRIOWKHH[HFXWLYHVEDFNWRWKH:HVW&RDVW /H[LFDO 6QWDFWLF3URFHVVLQJ Related Works While our focus is on clause-level relation of complex sentence constructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin e"
2021.starsem-1.20,D19-1586,0,0.0126766,"ne speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is called “next sentence prediction task” (NSP). In the process of pretraining using NSP, the model is presented with pairs of sentences. The model predicts whether the second sentence is the actual subsequent sent"
2021.wnut-1.9,2021.findings-acl.84,0,0.0123695,"ons that aggregate character-level edit operations. Recently, text editing models based on Transformer and BERT (Malmi et al., 2019; Mallinson et al., 2020; Stahlberg and Kumar, 2020) have been proposed for monolingual sequence transduction tasks, such as grammatical error correction and text normalization for speech synthesis, because of their sample-efficient and fast inference characteristics 74 compared to sequence-to-sequence models. References Data Synthesis. Data synthesis and augmentation methods have been explored for various NLP tasks, to increase the diversity of training examples (Feng et al., 2021) and for lexical normalization to address the deficiency of training data. Ikeda et al. (2016) synthesized Japanese formal-informal sentence pairs by hand-crafted rules to convert standard forms to nonstandard forms. Zhang et al. (2017) synthesized training data for Chinese informal word detection by random substitution of formal words in segmented sentences by informal words in a dictionary of formal-informal word pairs. To train statistical and neural MT models for Turkish text normalization, Çolako˘glu et al. (2019) generated a pseudo-parallel corpus where nonstandard words in original twee"
2021.wnut-1.9,2021.naacl-main.438,1,0.897268,"ers. For this reason, the problem of Japanese lexical normalization has been solved by predicting word boundaries, part-of-speech (POS) tags, and normalized word forms simultaneously (Sasano et al., 2013; Saito et al., 2014). Similarly to previous work, we tackle the joint task comprising Japanese word Segmentation, POS tagging, and lexical Normalization (SPN). A critical problem in lexical normalization is the lack of labeled data. Manual annotation of normalized forms is a time-consuming task; therefore, the size of the available annotated corpora is quite small (Kaji and Kitsuregawa, 2014; Higashiyama et al., 2021). A prospective solution to this problem 2 Task Definition As shown in Table 2, a training instance for the SPN task is defined as a pair, comprising a sentence x = (x1 , . . . , xn ) and its label sequence t = {(fj , lj , pj , Sj )}m j=1 , where n and m (≤ n) are the numbers of characters and words in x, fj and lj are the indexes of the first and last character in j-th word wj , and pj is the POS tag of wj . The set of standard forms Sj is equal to the empty set ∅ when wj is a standard form, whereas Sj consists of one or more standard forms when wj is a nonstandard form. A system is required"
2021.wnut-1.9,2020.findings-emnlp.111,0,0.0245681,"(2008) extracted formal-informal word pairs using websearched sentences defining informal words and a conditional log-linear ranking model. Wang and Text Editing. Text editing methods have also been applied to English lexical normalization. Chrupała (2014) used character embeddings based on a recurrent neural network LM and trained CRFs to predict character-level edit operations. Min and Mott (2015) proposed an LSTM-based model to perform word-level edit operations that aggregate character-level edit operations. Recently, text editing models based on Transformer and BERT (Malmi et al., 2019; Mallinson et al., 2020; Stahlberg and Kumar, 2020) have been proposed for monolingual sequence transduction tasks, such as grammatical error correction and text normalization for speech synthesis, because of their sample-efficient and fast inference characteristics 74 compared to sequence-to-sequence models. References Data Synthesis. Data synthesis and augmentation methods have been explored for various NLP tasks, to increase the diversity of training examples (Feng et al., 2021) and for lexical normalization to address the deficiency of training data. Ikeda et al. (2016) synthesized Japanese formal-informal sente"
2021.wnut-1.9,P82-1020,0,0.785309,"Missing"
C02-1050,J93-2003,0,0.0164084,"English translation. It was also observed that the bidirectional method was better for English-to-Japanese translation. 1 Introduction The statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of a translation target text given a translation source text. According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned transl"
C02-1050,2001.mtsummit-papers.22,0,0.0203283,"the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application. This paper p"
C02-1050,P01-1030,0,0.286203,"of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application. This paper presents two decoding methods, one is the right-to-left decoding based on the left-toright beam search algorithm, which generates outputs from the end of a sentence. The second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one. The experimental results of Japanese and En"
C02-1050,J99-4005,0,0.0380221,"ated from the cept ( j0 ). • NULL Translation Model — p1 : A fixed probability of inserting a NULL word after determining each target word f . For details, refer to Brown et al. (1993). 2.2 Search Problem The search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given f and the model, P(f|e) = P a P(f, a|e) and P(e). For the space of a is extremely large, |a|l+1 , where the l is the output length, an approximation of P(f|e) &apos; P(f, a|e) is used when exploring the possible candidates of translation. This problem is known to be NP-Complete (Knight, 1999), for the re-ordering property in the model further complicates the search. One of the solution is the left-to-right generation of output by consuming input words in any-order. Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al. (1996), Och et al. (2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al. (2001), though they all based on almost linearly Translation Model Lexical Q Model t( f j |ei ) Head Non-Head Fertility Q Model n(φi |ei ) Distortion Model Q d ( j − cρi"
C02-1050,W01-1408,0,0.341697,"According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffe"
C02-1050,1999.mtsummit-1.34,1,0.903591,"Missing"
C02-1050,takezawa-etal-2002-toward,1,0.793701,"he quality of translation. The similar statement can hold for postfix languages, such as Japanese, where emphasis is placed around the end of a sentence. For such languages, right-to-left decoding will be suitable but left-toright decoding will degrade the quality of translation. The bidirectional decoding is expected to take the benefits of both of the directions, and will show the best results in any kind of languages. 4 Experimental Results The corpus for this experiment consists of 172,481 bilingual sentences of English and Japanese extracted from a large-scale travel conversation corpus (Takezawa et al., 2002). The statistics of the corpus are shown in Table 1. The database was split into three parts: a training set of 152,183 sentence pairs, a validation set of 10,148, and a test set of 10,150. The translation models, both for the Japanese-toEnglish (J-E) and English-to-Japanese (E-J) translation, were trained toward IBM Model 4 on the training set and cross-validated on validation set to terminate the iteration by observing perplexity. In modeling IBM Model 4, POSs were used as word classes. From the viterbi alignments of the training corpus, A list of possible insertion of zero fertility words w"
C02-1050,C00-2123,0,0.408326,"el model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the"
C02-1050,P97-1047,0,0.122269,"Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words. There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English. Germann et al. (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar probl"
C02-1050,2002.tmi-papers.20,1,0.77024,"exity. In modeling IBM Model 4, POSs were used as word classes. From the viterbi alignments of the training corpus, A list of possible insertion of zero fertility words were extracted with frequency more than 10, around 1,300 sequences of words for both of the JE and E-J translations. The test set consists of 150 Japanese sentences varying by the sentence length of 6, 8 and 10. The translation was carried out by three decoding methods:left-to-right, right-toleft and bidirectional one. The translation results were evaluated by worderror-rate (WER) and position independent worderror-rate (PER) (Watanabe et al., 2002; Och et al., 2001). The WER is the measure by penalizing insertion/deletion/replacement by 1. The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection. It has been also evaluated by subjective evaluation (SE) with the criteria ranging from A(perfect) to D(nonTable 1: Statistics on a travel conversation corpus Japanese English # of sentences 172,481 # of words 1,186,620 1,005,080 vocabulary size 22,801 15,768 6.88 5.83 avg. sentence length 26.16 36.92 3-gram perplexity Table 3: Comparison of"
C02-1076,J93-2003,0,0.0139183,"ection system is much better than that of each component MT system. Conventional approaches to the selection problem include methods (Callison-Burch and Flournoy, 2001; Kaki et al., 1999) that automatically select the output assigned the highest probability P(t) (hereafter, LM-score), according to a language model (LM) for the translation target language. As a preliminary experiment, the authors applied this LM-score to selecting the best among the outputs from the three J-E MT systems. In order to make a comparison, the authors also used a score based on a translation model (TM) called IBM4 (Brown et al., 1993) (hereafter, TM-score) and a score based on the product of the TM-score and the LM-score (hereafter, TM3LM-score) to select the best output. Table 1 shows the results of this preliminary experiment. The oating number indicates the di erence between the performance for Rank A of each selection system and that of D3 (the best MT system, i.e., with the highest performance for Rank A). The LMscore and TM-score based selections did not Ideal Selection System TDMT HPAT 80 70 60 50 40 30 20 I-subj computer-of system engineer am I&apos;m a computer engineer.&quot; (2) I&apos;m a computer systems engineer. (TRN1) (3"
C02-1076,2001.mtsummit-papers.12,0,0.419313,"ntences ranked as A or B to the total number of sentences translated by each MT system (hereafter, performance for Rank A+B). The right-hand group of bars indicates the ratio of the number of sentences ranked as A, B, or C to the total number of sentences translated by each MT system (hereafter, performance for Rank A+B+C). The black bars indicate the performance of the ideal selection system. As Figures 2 and 3 show, the performance of the ideal J-E and E-J selection system is much better than that of each component MT system. Conventional approaches to the selection problem include methods (Callison-Burch and Flournoy, 2001; Kaki et al., 1999) that automatically select the output assigned the highest probability P(t) (hereafter, LM-score), according to a language model (LM) for the translation target language. As a preliminary experiment, the authors applied this LM-score to selecting the best among the outputs from the three J-E MT systems. In order to make a comparison, the authors also used a score based on a translation model (TM) called IBM4 (Brown et al., 1993) (hereafter, TM-score) and a score based on the product of the TM-score and the LM-score (hereafter, TM3LM-score) to select the best output. Table 1"
C02-1076,C96-1070,0,0.0120845,"m of automatically selecting the best among outputs from multiple machine translation (MT) systems (Figure 1). In combinations of multiple MT systems, some component MT systems can translate a source sentence well while others cannot well. In such a case, correct selection of the best can obviously boost performance. ATR has been developing such multiple MT systems, including three Japanese-to-English (J-E) MT systems: TDMT (Furuse and Iida, Input MTa MTb MTc Outputa Outputb Outputc 1996), D3 (Sumita, 2001), and SMT (Watanabe et al., 2002), and three English-to-Japanese (EJ) MT systems: TDMT (Furuse and Iida, 1996), HPAT (Imamura, 2002), and SMT (Watanabe et al., 2002). In order to evaluate each MT system, the MT outputs were manually assigned one of four ranks1 , A, B, C, and D, by native speakers of the target language. The ideal selection for J-E MT systems is the highestranked outputs from the three J-E MT systems: TDMT, D3, and SMT. The ideal selection for E-J MT systems is the highest-ranked outputs from the three E-J MT systems: TDMT, HPAT, and SMT. Figure 2 shows the individual performances of the three J-E MT systems and the ideal selection system derived from their combination. Figure 3 shows"
C02-1076,2002.tmi-papers.9,0,0.0817741,"he best among outputs from multiple machine translation (MT) systems (Figure 1). In combinations of multiple MT systems, some component MT systems can translate a source sentence well while others cannot well. In such a case, correct selection of the best can obviously boost performance. ATR has been developing such multiple MT systems, including three Japanese-to-English (J-E) MT systems: TDMT (Furuse and Iida, Input MTa MTb MTc Outputa Outputb Outputc 1996), D3 (Sumita, 2001), and SMT (Watanabe et al., 2002), and three English-to-Japanese (EJ) MT systems: TDMT (Furuse and Iida, 1996), HPAT (Imamura, 2002), and SMT (Watanabe et al., 2002). In order to evaluate each MT system, the MT outputs were manually assigned one of four ranks1 , A, B, C, and D, by native speakers of the target language. The ideal selection for J-E MT systems is the highestranked outputs from the three J-E MT systems: TDMT, D3, and SMT. The ideal selection for E-J MT systems is the highest-ranked outputs from the three E-J MT systems: TDMT, HPAT, and SMT. Figure 2 shows the individual performances of the three J-E MT systems and the ideal selection system derived from their combination. Figure 3 shows the individual perform"
C02-1076,P00-1056,0,0.0630269,"xpression (BE) corpus (Takezawa et al., 2002), which is split into three parts: a training set of 125,537 sentence pairs, a veri cation set of 9,872 pairs, and a test set of 10,023 pairs. The full corpus C in training translation target language model and translation model is the training set. Ten subsets of the full corpus were Preparing multiple RTs for each component MT system enables the second method to be extended so as to select the best output according to the multiple comparison. 5 used for the rst proposed method. The translation model and language model are learned by using GIZA++ (Och and Ney, 2000) and the CMU-Cambridge Toolkit (Clarkson and Rosenfeld, 1997), respectively. The translation model is learned from IBM 1 to 4, including the HMM model, as suggested by Och and Ney (2000), and its training loop was terminated when the perplexity for the validation set indicated the lowest scores. The word classes used in TM learning are the part-of-speech (POS) classes in TDMT. The P-value used for the multiple comparison test is 0.05. Four sets of about ve hundred pairs of English and Japanese sentences were randomly selected from the test set. The English sentences in the four sets were trans"
C02-1076,W01-1401,1,0.785248,"Missing"
C02-1076,takezawa-etal-2002-toward,1,0.814887,"i 3 Experimental Comparison 3.1 Experimental Method The authors evaluated the proposed methods in order to answer the following question: Which selection system improves performance best in comparison with that of the best MT system i.e. the MT systems with the highest performance as shown in Figures 2 and 3? In order to answer the above question, the authors used a set of three J-E component MT systems (TDMT, D3, and SMT) and a set of three E-J component MT systems (TDMT, HPAT, and SMT). Bilingual English and Japanese data were from ATR broad-coverage bilingual basic expression (BE) corpus (Takezawa et al., 2002), which is split into three parts: a training set of 125,537 sentence pairs, a veri cation set of 9,872 pairs, and a test set of 10,023 pairs. The full corpus C in training translation target language model and translation model is the training set. Ten subsets of the full corpus were Preparing multiple RTs for each component MT system enables the second method to be extended so as to select the best output according to the multiple comparison. 5 used for the rst proposed method. The translation model and language model are learned by using GIZA++ (Och and Ney, 2000) and the CMU-Cambridge Tool"
C02-1076,2002.tmi-papers.20,1,0.903029,"Missing"
C02-1164,C00-2163,0,\N,Missing
C02-1164,J93-2003,0,\N,Missing
C02-1164,C00-2123,0,\N,Missing
C02-1164,H94-1014,0,\N,Missing
C02-1164,P00-1056,0,\N,Missing
C04-1015,C02-1076,1,0.882262,"own et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solution due to the enormous search space (Watanabe and Sumita, 2003). Statistical MT could generate high-quality translations if it succeeded in finding a globally optimal solution. Therefore, the models employed by statistical MT are superior indicators of the quality of machine translation. Using this feature, Akiba et al. (2002) achieved selection of the best translation among those output by multiple MT engines. This paper presents an example-based MT method based on syntactic transfer, which selects the best translation by using models of statistical MT. This method is roughly structured using two modules (Figure 2). One is an example-based syntactic transfer module. This module constructs Transfer Rules Example-based Syntactic Transfer Statistical Generation Thesaurus Translation Dictionary Preprocessing Postprocessing Input Sentence Output Sentence Translation Model Language Model Figure 2: Structure of Proposed"
C04-1015,J93-2003,0,0.00358261,"g case relations or idiomatic expressions. However, when some examples conflict during reE = NULL0 show1 J= A= ( me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1 ) Figure 1: Example of Word Alignment between English and Japanese (Watanabe and Sumita, 2003) trieval, example-based MT selects the best example scored by the similarity between the input and the source part of the example. This implies that example-based MT does not check whether the translation of the given input sentence is correct or not. On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solution due to the enormous search space (Watanabe and Sumita, 2003). Statistical MT could generate high-quality translations if it succeeded in finding a globally optimal solution. Therefore, the models employed by statistical MT are superior indicators of the quality of machine translation. Using this feature, Akiba et al. (200"
C04-1015,P01-1030,0,0.0451743,"shared nodes of the target tree, so it can improve translation speed. Therefore, bottom-up generation is suitable for tasks that require real-time processing, such as spoken dialogue translation. 5 Discussion We incorporated example-based MT in models of statistical MT. However, some methods to obtain initial solutions of statistical MT by examplebased MT have already been proposed. For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001). Watanabe and Sumita (2003) proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. The difference between our method and these methods involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT. Even though example-based MT can output appropriate translations to some degree, our method assumes that the candidates contain a globally optimal solution. This means that the upper bound of MT quality is li"
C04-1015,P03-1057,1,0.874225,"ds involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT. Even though example-based MT can output appropriate translations to some degree, our method assumes that the candidates contain a globally optimal solution. This means that the upper bound of MT quality is limited by the example-based transfer, so we have to improve this stage in order to further improve MT quality. For instance, example-based MT can be improved by applying an optimization algorithm that uses an automatic evaluation of MT quality (Imamura et al., 2003). 6 Conclusions This paper demonstrated that example-based MT can be improved by incorporating it in models of statistical MT. The example-based MT used in this paper is based on syntactic transfer, so word reordering is achieved in the transfer module. Using this feature, the best translation was selected by using only a lexicon model and an n-gram language model. In addition, bottom-up generation achieved faster translation speed by using the tree structure of the target sentence. Acknowledgements The authors would like to thank Kadokawa Publishers, who permitted us to use the hierarchy of R"
C04-1015,2002.tmi-papers.9,1,0.88153,"because the example-based transfer generates syntactically correct candidates for the most appropriate translation. The rest of this paper is organized as follows: Section 2 describes the example-based syntactic transfer, Section 3 describes the statistical generation, Section 4 evaluates an experimental system that uses this method, and Section 5 compares other hybrid methods of example-based and statistical MT. 2 Example-based Syntactic Transfer The example-based syntactic transfer used in this paper is a revised version of the Hierarchical Phrase Alignment-based Translator (HPAT, refer to (Imamura, 2002)). This section gives an overview with an example of Japanese-to-English machine translation. 2.1 Transfer Rules Transfer rules are automatically acquired from bilingual corpora by using hierarchical phrase alignment (HPA; (Imamura, 2001)). HPA parses bilingual sentences and acquires corresponding syntactic nodes of the source and target sentences. The transfer rules are created from their node correspondences. Figure 3 shows an example of the transfer rules. Variables, such as X and Y in Figure 3, denote non-terminal symbols that correspond between source and target grammar. The set of transf"
C04-1015,P01-1050,0,0.0174084,"essary for improving MT quality. Finally, focusing on translation speed, the worst time for Bottom-up generation was dramatically faster than that for All Search. Bottom-up generation effectively uses shared nodes of the target tree, so it can improve translation speed. Therefore, bottom-up generation is suitable for tasks that require real-time processing, such as spoken dialogue translation. 5 Discussion We incorporated example-based MT in models of statistical MT. However, some methods to obtain initial solutions of statistical MT by examplebased MT have already been proposed. For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001). Watanabe and Sumita (2003) proposed a decoding algorithm in which translations that are similar to the input sentence are retrieved from bilingual corpora and then modified by greedy decoding. The difference between our method and these methods involves whether modification is applied. Our approach simply selects the best translation from candidates that are output from examplebased MT"
C04-1015,J03-1002,0,0.00490061,"sic Travel Expression Corpus (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into subsets for training and testing as shown in Table 1. Transfer Rules Transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed. The number of rules was 24,310. Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). Compared Methods We compared the following four methods. • Baseline (Example-based Transfer only) The best translation that had the same semantic distance was randomly selected from the S &lt;s&gt; bus will leave at 11 o’clock TM: -7.13 LM: -14.30 bus will start at 11 o’clock TM: -8.03 LM: -13.84 &lt;/s&gt; the bus will leave at 11 o’clock TM: -7.13 LM: -13.54 XNP bus TM: -0.07 LM: -0.0 the bus TM: -0.07 LM: -1.94 a bus TM: -0.07 LM: -2.11 n-best YVP leaves at 11 o’clock TM: -"
C04-1015,P02-1040,0,0.0702122,"Missing"
C04-1015,P91-1024,1,0.748447,"ock NP -&gt; X6 11 11 Figure 4: Example of Syntactic Transfer Process (Bold frames are syntactic nodes mentioned in text) that do not correspond between the source and target sentences (e.g., the determiner ‘a’ or ‘the’) are automatically inserted or eliminated by the target grammar (cf. NP node represented by a bold frame). Namely, transfer rules work in a manner similar to the functions of distortion, fertility, and NULL in IBM models. 2.3 Usage of Source Examples Example-based transfer utilizes the source examples for disambiguation of mapping and parsing. Specifically, the semantic distance (Sumita and Iida, 1991) is calculated between the source examples and the headwords of the input sentence, and the transfer rules that contain the nearest example are used to construct the target tree structure. The semantic distance between words is defined as the distance from the leaf node to the most specific common abstraction (MSCA) in a thesaurus (Ohno and Hamanishi, 1984). For example, if the input phrase “ie (home) ni kaeru (return)” is given, Rules 1 to 3 in Figure 3 are used for the syntactic transfer, and three target nodes are generated without any disambiguation. However, when we compare the source exa"
C04-1015,takezawa-etal-2002-toward,1,0.73883,"sentence and end-of-sentence, and the n-best list is re-sorted. As a result, the translation “The bus will leave at 11 o’clock” is obtained from the tree of Figure 4. Bottom-up generation calculates the probabilities of shared nodes only once, so it effectively uses tree information. 4 Evaluation In order to evaluate the effect when models of statistical MT are integrated into example-based MT, we compared various methods that changed the statistical generation module. 4.1 Experimental Setting Bilingual Corpus The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002; Kikui et al., 2003). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into subsets for training and testing as shown in Table 1. Transfer Rules Transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed. The number of rules was 24,310. Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram mode"
C04-1015,2003.mtsummit-papers.53,0,0.0163217,"is determined from the product of the translation model and the language model in the same manner as statistical MT. In other words, when F and E denote the channel target and channel source sequence, ˆ that satrespectively, the output word sequence E isfies the following equation is searched for. Set Name Training Test ˆ = argmax P (E|F ) E Item # of Sentences # of Words # of Sentences # of Words English Japanese 152,170 886,708 1,007,484 510 2,973 3,340 E = argmax P (E)P (F |E). E (1) We only utilize the lexicon model as the translation model in this paper, similar to the models proposed by Vogel et al. (2003). Namely, when f and e denote the channel target and channel source word, respectively, the translation probability is computed by the following equation. P (F |E) =  j t(fj |ei ). (2) i The IBM models include other models, such as fertility, NULL, and distortion models. As we described in Section 2.2, the quality of machine translation is maintained using only the lexicon model because syntactical correctness is already preserved by example-based transfer. For the language model, we utilize a standard word n-gram model. 3.2 Bottom-up Generation We can construct word graphs by serializing th"
C04-1015,2003.mtsummit-papers.54,1,0.938164,"ngual corpus as a database and retrieves examples that are similar to an input sentence. Then, a translation is generated by modifying the target part of the examples while referring to translation dictionaries. Most example-based MT systems employ phrases or sentences as the unit for examples, so they can translate while considering case relations or idiomatic expressions. However, when some examples conflict during reE = NULL0 show1 J= A= ( me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1 ) Figure 1: Example of Word Alignment between English and Japanese (Watanabe and Sumita, 2003) trieval, example-based MT selects the best example scored by the similarity between the input and the source part of the example. This implies that example-based MT does not check whether the translation of the given input sentence is correct or not. On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering. Therefore, when it is applied to a language pair in which the word order is quite different (e.g., English and Japanese, Figure 1), it becomes difficult to find a globally optimal solut"
C04-1030,J90-2002,0,0.427447,"significant improvements compared to the unconstrained search. 1 Introduction In statistical machine translation, we are given a source language (‘French’) sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language (‘English’) sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we will choose the sentence with the highest probability: © ª eˆI1 = argmax P r(eI1 |f1J ) eI1 = argmax eI1 © ª P r(eI1 ) · P r(f1J |eI1 ) This decomposition into two knowledge sources is known as the source-channel approach to statistical machine translation (Brown et al., 1990). It allows an independent modeling of target language model P r(eI1 ) and translation model P r(f1J |eI1 ). The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. It can be further decomposed into alignment and lexicon model. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. An alternative to the classical sourcechannel approach is the direct"
C04-1030,J99-4005,0,0.599269,"ls or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). In this paper, we will investigate the reordering problem for phrase-based translation approaches. As the word order in source and target language may differ, the search algorithm has to allow certain reorderings. If arbitrary reorderings are allowed, the search problem is NP-hard (Knight, 1999). To obtain an efficient search algorithm, we can either restrict the possible reorderings or we have to use an approximation algorithm. Note that in the latter case we cannot guarantee to find an optimal solution. The remaining part of this work is structured as follows: in the next section, we will review the baseline translation system, namely the alignment template approach. Afterward, we will describe different reordering constraints. We will begin with the IBM constraints for phrase-based translation. Then, we will describe constraints based on inversion transduction grammars (ITG). In t"
C04-1030,N03-1017,0,0.32893,"Missing"
C04-1030,W02-1018,0,0.0915666,"Missing"
C04-1030,P02-1038,1,0.858868,"lation model P r(f1J |eI1 ). The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. It can be further decomposed into alignment and lexicon model. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. An alternative to the classical sourcechannel approach is the direct modeling of the posterior probability P r(eI1 |f1J ). Using a loglinear model (Och and Ney, 2002), we obtain: Ã P r(eI1 |f1J ) = exp ! M X λm hm (eI1 , f1J ) · Z(f1J ) m=1 Here, Z(f1J ) denotes the appropriate normalization constant. As a decision rule, we obtain: ( eˆI1 = argmax eI1 M X ) λm hm (eI1 , f1J ) m=1 This approach is a generalization of the source-channel approach. It has the advantage that additional models or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measu"
C04-1030,W99-0604,1,0.401442,"nslation system, namely the alignment template approach. Afterward, we will describe different reordering constraints. We will begin with the IBM constraints for phrase-based translation. Then, we will describe constraints based on inversion transduction grammars (ITG). In the following, we will call these the ITG constraints. In Section 4, we will present results for two Japanese–English translation tasks. 2 Alignment Template Approach In this section, we give a brief description of the translation system, namely the alignment template approach. The key elements of this translation approach (Och et al., 1999) are the alignment templates. These are pairs of source and target language phrases with an alignment within the phrases. The alignment templates are build at the level of word classes. This improves the generalization capability of the alignment templates. We use maximum entropy to train the model scaling factors (Och and Ney, 2002). As feature functions we use a phrase translation model as well as a word translation model. Additionally, we use two language model feature functions: a word-based trigram model and a class-based five-gram model. Furthermore, we use two heuristics, namely the wor"
C04-1030,P03-1021,0,0.209602,"= exp ! M X λm hm (eI1 , f1J ) · Z(f1J ) m=1 Here, Z(f1J ) denotes the appropriate normalization constant. As a decision rule, we obtain: ( eˆI1 = argmax eI1 M X ) λm hm (eI1 , f1J ) m=1 This approach is a generalization of the source-channel approach. It has the advantage that additional models or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm. Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). In this paper, we will investigate the reordering problem for phrase-based translation approaches. As the word order in source and target language may differ, the search algorithm has to allow certain reorderings. If arbitrary reorderings are allowed, the search problem is NP-hard (Knight, 1999). To obtain an efficient search algorithm, we can either restrict the possible reorderings or we have to use an approximation algorithm. Note that in the latter case we cannot guarantee to find an optimal solution. The remaining part of this work is structured as follows: in the next section, we will"
C04-1030,P02-1040,0,0.113983,"eletion operations that have to be performed to convert the generated sentence into the reference sentence. PER (position-independent word error rate). A shortcoming of the WER is that it requires a perfect word order. The word order of an acceptable sentence can be different from that of the target sentence, so that the WER measure alone could be misleading. The PER compares the words in the two sentences ignoring the word order. BLEU. This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences (Papineni et al., 2002). The BLEU score measures accuracy, i.e. large BLEU scores are better. NIST. This score is similar to BLEU. It is a weighted n-gram precision in combination with a penalty for too short sentences (Doddington, 2002). The NIST score measures accuracy, i.e. large NIST scores are better. Note that for each source sentence, we have as many as 16 references available. We compute all the preceding criteria with respect to multiple references. 4.3 System Comparison In Table 3 and Table 4, we show the translation results for the BTEC task. First, we observe that the overall quality is rather high on th"
C04-1030,takezawa-etal-2002-toward,1,0.296878,"to the left of the current position jc , e.g. positions (a) and (d). Somewhere in between there has to be an covered position j whose successor position j + 1 is uncovered, e.g. (b) and (c). Therefore, any reordering that violates Equation 1 generates the pattern on the left-hand side in Figure 4, thus it violates the ITG constraints. 4 Results 4.1 Corpus Statistics To investigate the effect of reordering constraints, we have chosen two Japanese–English tasks, because the word order in Japanese and English is rather different. The first task is the Basic Travel Expression Corpus (BTEC) task (Takezawa et al., 2002). The corpus statistics are shown in Table 1. This corpus consists of phrasebook entries. The second task is the Spoken Language DataBase (SLDB) task (Morimoto et al., 1994). This task consists of transcription of spoken dialogs in the domain of hotel reservation. Here, we use domain-specific training data in addition to the BTEC corpus. The corpus statistics of this additional corpus are shown in Table 2. The development corpus is the same for both tasks. 4.2 Evaluation Criteria WER (word error rate). The WER is computed as the minimum number of substitution, insertion and deletion operations"
C04-1030,J03-1005,1,0.779472,"a sequence of word classes as used in the alignment templates. 3.1 IBM Constraints In this section, we describe restrictions on the phrase reordering in spirit of the IBM constraints (Berger et al., 1996). First, we briefly review the IBM constraints at the word level. The target sentence is produced word by word. We keep a coverage vector to mark the already translated (covered) source positions. The next target word has to be the translation of one of the first k uncovered, i.e. not translated, source positions. The IBM constraints are illustrated in Figure 1. For further details see e.g. (Tillmann and Ney, 2003). For the phrase-based translation approach, we use the same idea. The target sentence is produced phrase by phrase. Now, we allow skipping of up to k phrases. If we set k = 0, we obtain a search that is monotone at the phrase level as a special case. Q(1, ∅, $) = 1  with inversion target positions without inversion target positions The search problem can be solved using dynamic programming. We define a auxiliary function Q(j, S, e). Here, the source position j is the first unprocessed source position; with unprocessed, we mean this source position is neither translated nor skipped. We use th"
C04-1030,J97-3002,0,0.799973,"the language model history. The symbol $ is used to mark the sentence start and the sentence end. The extension to higher-order n-gram language models is straightforward. We use M to denote the maximum phrase length in the source language. We obtain the following dynamic programming equations: source positions source positions Figure 2: Illustration of monotone and inverted concatenation of two consecutive blocks. setting k = 0 results in a search algorithm that is monotone at the phrase level. 3.2 ITG Constraints Q(j, S, e) = max In this section, we describe the ITG conn straints (Wu, 1995; Wu, 1997). Here, we interj−1 max max Q(j 0 , S, e0 ) · p(fj 0 |˜ e) · p(˜ e|e0 ), pret the input sentence as a sequence of blocks. e0 ,˜ e j−M ≤j 0 <j o 0 0 , e0 ) · p(f j +l−1 |˜ 0 ) , In the beginning, each alignment template is a max Q(j, S e ) · p(˜ e |e j0 block of its own. Then, the reordering process (j 0 ,l)∈S 0 S=S 0 {(j 0 ,l)}  can be interpreted as follows: we select two consecutive blocks and merge them to a single max0 Q(j 0 , S 0 , e) j−M ≤j <j block by choosing between two options: either S 0 :S=S 0 ∪{(j 0 ,j−j 0 )}∧|S 0 |<k keep the target phrases in monotone order or Q(J + 2, ∅, $) ="
C04-1030,P03-1019,1,0.776962,"entence word by word or phrase the position to be translated next jn . Then, by phrase. The idea is to start with the beam it is not allowed to move from an uncovered search decoder for unconstrained search and position to a covered one. modify it in such a way that it will produce Now, we sketch the proof that these cononly reorderings that do not violate the ITG straints are equivalent to the ITG constraints. constraints. Now, we describe one way to obIt is easy to see that the constraint in Equatain such a decoder. It has been pointed out tion 1 avoids the pattern on the left-hand side in (Zens and Ney, 2003) that the ITG conin Figure 4. To be precise: after placing the straints can be characterized as follows: a refirst two phrases at (b,1) and (d,2), it avoids ordering violates the ITG constraints if and the placement of the third phrase at (a,3). only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as Similarly, the constraint in Equation 2 avoid a subsequence. This means, if we select four the pattern on the right-hand side in Figcolumns and the corresponding rows from the ure 4. Therefore, if we enforce the constraints alignment matrix and we obtain one of the two in Equation 1 and Equation 2, we"
C04-1168,J93-2003,0,0.00536183,"where Pam (X|J) is the acoustic model likelihood of the observations given the recognized sentence J; Plm (J), the source language model probability; and P (X), the probability of all acoustic observations. In the experiment we generated a set of N best hypotheses, J1N = {J1 , J2 , · · · , JN } 1 and each Ji is determined by Ji = arg max Pam (X|J)Plm (J) J∈Ωi where Ωi is the set of all possible source sentences excluding all higher ranked J k ’s, 1 ≤ k ≤ i − 1. The conversion from J to E in Fig. 1 is the machine translation process. According to the statistical machine translation formalism (Brown et al., 1993), the translation process b such that is to search for the best sentence E b = arg max P (E|J) = arg max P (J|E)P (E) E E E where P (J|E) is a translation model characterizing the correspondence between E and J; P (E), the English language model probability. In the IBM model 4, the translation model P (J|E) is further decomposed into four submodels: • Lexicon Model – t(j|e): probability of a word j in the Japanese language being translated into a word e in the English language. 1 Hereafter, J1 is called the single-best hypothesis of speech recognition; J1N , the N -best hypotheses. • Fertility"
C04-1168,W02-0706,0,0.0559183,"Missing"
C04-1168,W02-1018,0,0.0201343,"nvestigate the effect of different features for improving speech translation. In addition to the above seven features, the following features are also incorporated. • Part-of-speech language models: English part-of-speech language models were used. POS dependence of a translated English sentence is an effective constraint in pruning English sentence candidates. In our experiments 81 part-of-speech tags and a 5gram POS language model were used. • Length model P (l|E, J): l is the length (number of words) of a translated English sentence. • Jump weight: Jump width for adjacent cepts in Model 4 (Marcu and Wong, 2002). • Example matching score: The translated English sentence is matched with phrase translation examples. A score is derived based on the count of matches (Watanabe and Sumita, 2003). • Dynamic example matching score: Similar to the example matching score but phrases were extracted dynamically from sentence examples (Watanabe and Sumita, 2003). each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. b R) is a translation “distortion” or an D(E, objective translation assessment. The following four metrics were used specifically in this study: • BLEU (Papineni et al., 20"
C04-1168,niessen-etal-2000-evaluation,0,0.0383394,"weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Altogether, we used M (=12) different features. In section 3, we review Powell’s algorithm (Press et al., 2000) as our tool to optimize model parameters, λM 1 , based on different objective translation metrics. 3 • NIST : An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor which again penalizes short translation sentences. Parameter Optimization Based on Translation Metrics • mWER (Niessen et al., 2000): Multiple reference word error rate, which computes the edit distance (minimum number of insertions, deletions, and substitutions) between test and reference sentences. The denominator in Eq. 1 can be ignored since the normalization is applied equally to every hypothesis. Hence, the choice of the best translab out of all possible translations, E, is tion, E, independent of the denominator, b = arg max E E M X λi logPi (X, E) • mPER: Multiple reference position independent word error rate, which computes the edit distance without considering the word order. (2) i=1 where we write features, fi"
C04-1168,J03-1002,0,0.0259349,"ses the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extracted phrase translations from the bilingual sentences (Watanabe and Sumita, 2003). The decoder used the IBM Model 4 with a trigram language model and a 5-gram part-of-speech language model. The training of IBM model 4 was implemented by the GIZA++ package (Och and Ney, 2003). 4.2 Model Training In order to quantify translation improvement by features from speech recognition and machine translation respectively, we built four log-linear models by adding features successively. The four models are: • Standard translation model(stm): Only features from the IBM model 4 (M =5) described in section 2 were used in the loglinear models. We did not perform parameter optimization on this model. It is equivalent to setting all the λM 1 to 1. This model was the standard model used in most statistical machine translation system. It is referred to as the baseline model. • Optim"
C04-1168,P03-1021,0,0.490288,"nslation E th utterance is produced by the (Eq. 2), where E ∈ Cl . Let R = {R1 , · · · , RL } be the set of translation references for all utterances. Human translators paraphrased 16 reference sentences for The BLEU score and NIST score are calculated by the tool downloadable 2 . Because the objective function in the model (Eq. 3) is not smoothed function, we used Powell’s search method to find a solution. The Powell’s algorithm used in this work is similar as the one from (Press et al., 2000) but we modified the line optimization codes, a subroutine of Powell’s algorithm, with reference to (Och, 2003). Finding a global optimum is usually difficult in a high dimensional vector space. To make sure that we had found a good local optimum, we restarted the algorithm by using various initializations and used the best local optimum as the final solution. 4 Experiments 4.1 Corpus & System The data used in this study was the Basic Travel Expression Corpus (BTEC) (Kikui et al., 2003), consisting of commonly used sentences listed in travel guidebooks and tour conversations. The corpus were designed for developing multiple language speech-to-speech translation systems. It contains four different langu"
C04-1168,P02-1040,0,0.110685,"rcu and Wong, 2002). • Example matching score: The translated English sentence is matched with phrase translation examples. A score is derived based on the count of matches (Watanabe and Sumita, 2003). • Dynamic example matching score: Similar to the example matching score but phrases were extracted dynamically from sentence examples (Watanabe and Sumita, 2003). each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. b R) is a translation “distortion” or an D(E, objective translation assessment. The following four metrics were used specifically in this study: • BLEU (Papineni et al., 2002): A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Altogether, we used M (=12) different features. In section 3, we review Powell’s algorithm (Press et al., 2000) as our tool to optimize model parameters, λM 1 , based on different objective translation metrics. 3 • NIST : An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor which again penalizes short translation sentences. Parameter Optimization Based on Translation Metrics • mW"
C04-1168,W02-1021,0,0.0119337,"d test corpus #01 were used for training, development and test respectively. The statistics of corpus is shown in table 1. The speech recognition engine used in the experiments was an HMM-based, large vocabulary continuous speech recognizer. The acoustic HMMs were triphone models with 2,100 states in total, using 25 dimensional, short-time spectrum features. In the first and second pass of decoding, a multiclass word bigram of a lexicon of 37,000 words plus 10,000 compound words was used. A word trigram was used in rescoring the results. The machine translation system is a graphbased decoder (Ueffing et al., 2002). The first pass of the decoder generates a word-graph, a compact representation of alternative translation candidates, using a beam search based on the scores of the lexicon and language models. In the second pass an A* search traverses the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extracted phrase translation"
C04-1168,2003.mtsummit-papers.54,1,0.913845,"peech language models: English part-of-speech language models were used. POS dependence of a translated English sentence is an effective constraint in pruning English sentence candidates. In our experiments 81 part-of-speech tags and a 5gram POS language model were used. • Length model P (l|E, J): l is the length (number of words) of a translated English sentence. • Jump weight: Jump width for adjacent cepts in Model 4 (Marcu and Wong, 2002). • Example matching score: The translated English sentence is matched with phrase translation examples. A score is derived based on the count of matches (Watanabe and Sumita, 2003). • Dynamic example matching score: Similar to the example matching score but phrases were extracted dynamically from sentence examples (Watanabe and Sumita, 2003). each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. b R) is a translation “distortion” or an D(E, objective translation assessment. The following four metrics were used specifically in this study: • BLEU (Papineni et al., 2002): A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Altogether, we"
C12-2071,N12-1047,0,0.0686776,"U optimizes the expected BLEU, a loss more approximate towards Corpus-BLEU compared with the generalized hinge loss, and it utilizes the projection distance metric instead of L2 as with MIRA. Further, ELBUU is a MERT-like batch mode which ultraconservatively updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al., 2007) or parts of examples (Chiang et al., 2008). The batch mode has some advantages over online mode: more accurate sentence-wise BLEU towards Corpus-BLEU (Watanabe, 2012) and more promising experimental performance (Cherry and Foster, 2012). Additionally, our method is similar to (Liu et al., 2012). However, the main difference is that ours is a global training method instead of a local training method. 4 Experiments and Results 4.1 Experimental Setting We conduct our translation experiments on two language pairs: Chinese-to-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST"
C12-2071,D08-1024,0,0.588794,"mize Eq. 3. Motivated by (Och, 2003; Smith and Eisner, 2006; Zens et al., 2007), we use the expected loss to substitute the direct loss in Eq. 3 and we obtain the objective function as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with pseudo-document. 3.2 Distance Metric Based on Projection Euclidean distance ( L2 norm) is usually employed as in MIRA (Watanabe et al., 2007; Chiang et al., 2008). In this section we will specifically investigate another metric for ultraconservative update in SMT. In log-linear based translation models, since the decoding strategy is the maximal posterior probability, the translation results are the same for the weight W and its positive multiplication (see Eq. 2). Therefore, for a translation decoder, we wish that the distance of two weights satisfies the"
C12-2071,P11-2031,0,0.031614,"EU. Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012. 723 1 Introduction Minimum error rate training (Och, 2003), MERT, is an important component of statistical machine translation (SMT), and it has been the most popular method for tuning parameters for SMT systems. One of its major contributions is the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al."
C12-2071,W02-1001,0,0.0637373,"ng corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average the weights as (Collins, 2002). The norm of the averaged weight may nolonger be equal to 1, but it is irrelevant for testing, see discussion in Section 3.2. 728 Methods MERT ELBUU dev06(Dev) 28.85 28.67 test08 19.68 20.23 test09 21.36 21.72 test10 23.35 23.90+ test11 23.65 24.18+ Table 2: Comparison of two tuning methods, MERT and ELBUU, on Spanish-to-English translation tasks. + means the ELBUU method is significantly better than MERT with confidence p &lt; 0.05. Distance metrics L2 Projection NIST02(Dev) 29.95 30.06 NIST03 27.09 27.36 NIST04 29.65 29.89 NIST05 26.79 27.03 NIST06 25.98 26.30 NIST08 19.54 19.79 Table 3: Compa"
C12-2071,W04-3250,0,0.0518064,", test10, test11. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average the weights as (Collins, 2002). The norm of the averaged weight may nolonger be equal to 1, but it is irrelevant for testing, see discussion in Section 3.2. 728 Methods MERT ELBUU dev06(Dev) 28.85 28.67 test08 19.68 20.23 test09 21.36 21.72 test10 23.35 23.90+ test11 23.65 24.18+ Table 2: Comparison of two tuning methods, MERT and ELBUU, on Spanish-to-English translation tasks. + means the ELBUU method is significantly better than MERT with confidence p &lt; 0.05. Distance metrics L2 Projection NIST02(Dev) 29.95 30.06 NIST03 27.09 27.36 NIST04 29.65 29.89 NIST"
C12-2071,P07-2045,0,0.0187117,"test10 23.35 23.90+ test11 23.65 24.18+ Table 2: Comparison of two tuning methods, MERT and ELBUU, on Spanish-to-English translation tasks. + means the ELBUU method is significantly better than MERT with confidence p &lt; 0.05. Distance metrics L2 Projection NIST02(Dev) 29.95 30.06 NIST03 27.09 27.36 NIST04 29.65 29.89 NIST05 26.79 27.03 NIST06 25.98 26.30 NIST08 19.54 19.79 Table 3: Comparison of two distance metrics L2 and projection on Chinese-to-English translation tasks. The translation system is a phrase-based translation model (Koehn et al., 2003) and we use the open source toolkit MOSES (Koehn et al., 2007) as its implementation. In the experiments, the default setting is used for MOSES. The baseline tuning method is the standard algorithm MERT and the k-best-list size is set as 100 for tuning. For ELBUU, we empirically set α = 3.0 as (Och, 2003), η = 1, ε = 10−5 , K = 20, and we do not tune them further. We tune λ on NIST05 with λ = 1.0 for the Chinese-to-English translation tasks and we do not tune it again for the Spanish-to-English translation tasks. 4.2 Results Table 1 and Table 2 give the main results of ELBUU compared with the baseline MERT on Chinese-to-English and Spanish-to-English tra"
C12-2071,N03-1017,0,0.175345,"sk, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST03, NIST04, NIST05, NIST06, and NIST08. For the Spanish-to-English task, all the datasets are from WMT2011: the training data is the first 200k sentence pairs of Europarl corpus; the development set is dev06; and the test datasets are test07, test08,test09, test10, test11. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average the weights as (Collins, 2002). The norm of the averaged weight m"
C12-2071,P09-1019,0,0.0699435,"imization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al. (2008) and Kumar et al. (2009), to achieve a more stable loss function between optimization steps. We propose an expected loss-based ultraconservative update method, in which an expected loss is minimized using an ultraconservative update strategy (Crammer and Singer, 2003; Crammer et al., 2006). In the optimization step, we iteratively learn the weight which should not only minimize the error rates as in MERT but also not be far from the weight learned at the previous optimization step. Instead of using the L2 in Euclidean space to describe the distances between the two weights as in the Margin Infused Relaxed Algorithm ("
C12-2071,D12-1037,1,0.833347,"pus-BLEU compared with the generalized hinge loss, and it utilizes the projection distance metric instead of L2 as with MIRA. Further, ELBUU is a MERT-like batch mode which ultraconservatively updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al., 2007) or parts of examples (Chiang et al., 2008). The batch mode has some advantages over online mode: more accurate sentence-wise BLEU towards Corpus-BLEU (Watanabe, 2012) and more promising experimental performance (Cherry and Foster, 2012). Additionally, our method is similar to (Liu et al., 2012). However, the main difference is that ours is a global training method instead of a local training method. 4 Experiments and Results 4.1 Experimental Setting We conduct our translation experiments on two language pairs: Chinese-to-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST03, NIST04, NIST05, NIST06, and NIST08. For the Spanish-to-"
C12-2071,D08-1076,0,0.0578676,"k et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al. (2008) and Kumar et al. (2009), to achieve a more stable loss function between optimization steps. We propose an expected loss-based ultraconservative update method, in which an expected loss is minimized using an ultraconservative update strategy (Crammer and Singer, 2003; Crammer et al., 2006). In the optimization step, we iteratively learn the weight which should not only minimize the error rates as in MERT but also not be far from the weight learned at the previous optimization step. Instead of using the L2 in Euclidean space to describe the distances between the two weights as in the Margin Inf"
C12-2071,P03-1021,0,0.678266,"ative update, in which the combination of an expected task loss and the distance from the parameters in the previous round are minimized with a variant of gradient descent. Experiments on test datasets of both Chinese-to-English and Spanish-toEnglish translation show that our method can achieve improvements over MERT under the Moses system. KEYWORDS: statistical machine translation; tuning; minimum error rate training; ultraconservative update; expected BLEU. Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012. 723 1 Introduction Minimum error rate training (Och, 2003), MERT, is an important component of statistical machine translation (SMT), and it has been the most popular method for tuning parameters for SMT systems. One of its major contributions is the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists."
C12-2071,P00-1056,0,0.0906742,"o-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test set for tuning hyperparameter λ in Eq. 6; and the test datasets are NIST03, NIST04, NIST05, NIST06, and NIST08. For the Spanish-to-English task, all the datasets are from WMT2011: the training data is the first 200k sentence pairs of Europarl corpus; the development set is dev06; and the test datasets are test07, test08,test09, test10, test11. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. We train a 4-gram language model on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric (Papineni et al., 2002) and we use mteval-v13a.pl as the evaluation tool. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). 3 At the end of tuning, we average th"
C12-2071,P02-1040,0,0.0924311,"lish translation show that our method can achieve improvements over MERT under the Moses system. KEYWORDS: statistical machine translation; tuning; minimum error rate training; ultraconservative update; expected BLEU. Proceedings of COLING 2012: Posters, pages 723–732, COLING 2012, Mumbai, December 2012. 723 1 Introduction Minimum error rate training (Och, 2003), MERT, is an important component of statistical machine translation (SMT), and it has been the most popular method for tuning parameters for SMT systems. One of its major contributions is the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may"
C12-2071,D09-1147,0,0.193527,"s the use of an evaluation metric, such as BLEU (Papineni et al., 2002), as a direct loss function during its optimization procedure by interchanging decoding and optimization steps in each round. While MERT is successful in practice, it is known to be unstable (Clark et al., 2011). At the optimization step in each round, MERT tries to repeatedly optimize a loss function defined by the k-best candidate lists. Since new k-best lists are generated and merged with the previously generated lists at each round, the optimization objective function may change drastically between two adjacent rounds (Pauls et al., 2009), and the optimized weights of these two rounds may also be far from each other. Motivated by the above observation, this paper investigates a new tuning approach under the kbest lists framework, instead of the lattices or hypergraphs framework as Macherey et al. (2008) and Kumar et al. (2009), to achieve a more stable loss function between optimization steps. We propose an expected loss-based ultraconservative update method, in which an expected loss is minimized using an ultraconservative update strategy (Crammer and Singer, 2003; Crammer et al., 2006). In the optimization step, we iterative"
C12-2071,P06-2101,0,0.28711,"ain the following objective function:  n  d(W, Wk ) + λLosser r or ri ; ˆe( f i ; W ) i=1 , (3) where d(W, Wk ) is a distance function of a pair of weights and it is used to penalize a weight far away from Wk . Losser r or is the objective function of MERT as defined in Eq. 1. λ ≥ 0 is the regularization penalty. When λ → ∞ Eq. 3 goes back to the objective function of MERT. 725 Because the first term d in Eq. 3 is not piecewise linear in respect to W , the exact line search routine in MERT does not hold anymore. Generally, it is not easy to directly minimize Eq. 3. Motivated by (Och, 2003; Smith and Eisner, 2006; Zens et al., 2007), we use the expected loss to substitute the direct loss in Eq. 3 and we obtain the objective function as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with ps"
C12-2071,N12-1026,1,0.836237,". However, there are also some differences between them. ELBUU optimizes the expected BLEU, a loss more approximate towards Corpus-BLEU compared with the generalized hinge loss, and it utilizes the projection distance metric instead of L2 as with MIRA. Further, ELBUU is a MERT-like batch mode which ultraconservatively updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al., 2007) or parts of examples (Chiang et al., 2008). The batch mode has some advantages over online mode: more accurate sentence-wise BLEU towards Corpus-BLEU (Watanabe, 2012) and more promising experimental performance (Cherry and Foster, 2012). Additionally, our method is similar to (Liu et al., 2012). However, the main difference is that ours is a global training method instead of a local training method. 4 Experiments and Results 4.1 Experimental Setting We conduct our translation experiments on two language pairs: Chinese-to-English and Spanishto-English. For the Chinese-to-English task, the training data is FBIS corpus consisting of about 240k sentence pairs; the development set is NIST02 evaluation data; the test set NIST05 is used as the development test se"
C12-2071,D07-1080,1,0.957646,"ction as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with pseudo-document. 3.2 Distance Metric Based on Projection Euclidean distance ( L2 norm) is usually employed as in MIRA (Watanabe et al., 2007; Chiang et al., 2008). In this section we will specifically investigate another metric for ultraconservative update in SMT. In log-linear based translation models, since the decoding strategy is the maximal posterior probability, the translation results are the same for the weight W and its positive multiplication (see Eq. 2). Therefore, for a translation decoder, we wish that the distance of two weights satisfies the following property: the smaller the distance between them is, the more similar the translation results decoded with them are. However, L2 norm does not satisfy this property. In"
C12-2071,D07-1055,0,0.083524,"ive function:  n  d(W, Wk ) + λLosser r or ri ; ˆe( f i ; W ) i=1 , (3) where d(W, Wk ) is a distance function of a pair of weights and it is used to penalize a weight far away from Wk . Losser r or is the objective function of MERT as defined in Eq. 1. λ ≥ 0 is the regularization penalty. When λ → ∞ Eq. 3 goes back to the objective function of MERT. 725 Because the first term d in Eq. 3 is not piecewise linear in respect to W , the exact line search routine in MERT does not hold anymore. Generally, it is not easy to directly minimize Eq. 3. Motivated by (Och, 2003; Smith and Eisner, 2006; Zens et al., 2007), we use the expected loss to substitute the direct loss in Eq. 3 and we obtain the objective function as follows: d(W, Wk ) + n X λX n i=1 e∈ci Losser r or (ri ; e)Pα (e |f i ; W ), with Pα (e |f i ; W ) = P exp[αW · h( f i , e)] e′ ∈c i exp[αW · h( f i , e′ )] (4) , where α &gt; 0 is a real number, each h( f i , e) is a feature vector, and d is a distance metric defined on a pair of weights. Losser r or (ri ; e) in Eq. 4 is a sentence-wise direct loss, and in this paper we used a variant of sentence BLEU proposed by Chiang et al. (2008) which smoothes BLEU statistics with pseudo-document. 3.2 D"
C14-1180,W12-2703,0,0.0345425,"al., 2006; Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual tuples also known as minimal translation units (MTUs). Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural networks with factorizations to model bilingual tuples in a continuous space. Although the authors reported some gains over the n-gram model in machine translation tasks, these models can only capture a limited amount of context and remain a kind of n-gram model. In language modeling, experimental results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate in speech recognition even though it is harder to train properly. Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose recurrent neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translation system. Our RNNTSMs are capable of modeling long-span context and have better generalization. Compared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contribu"
C14-1180,D13-1106,0,0.023599,"ns the combination of six models, as described in Table 4. We can observe that: (1) The combination of the proposed RNNTSMs only trained on the speechstyle data can essentially enhance the baselines by 1.2-1.8 BLEU points. (2) The improvements over the RNNLMs are significant on the English-German task but these improvements are modest on the EnglishFrench task. Note that the factorized RNNTSMs and the RNNLMs in the large task are also only trained the speech-style parallel corpus. In future work, we will train them on a bigger corpus, which can be expected to further increase the performance (Auli et al., 2013; Wu et al., 2012). 6 Conclusion Most prior neural network-based translation models either employ feed-forward neural networks to explicitly integrate source information via word-to-word alignment, or use recurrent neural networks in which source information is implicitly represented with a compressed vector. In this paper, we present recurrent neural network-based tuple sequence models (RNNTSMs) to compute probabilities of bilingual tuples in continuous space. One of major advantages is their potential to capture long-span history compared with feed-forward neural networks. In addition, our m"
C14-1180,P11-2031,0,0.0765904,"Missing"
C14-1180,J81-4005,0,0.782139,"Missing"
C14-1180,W09-0438,0,0.0667539,"Missing"
C14-1180,C04-1022,0,0.0632748,"Missing"
C14-1180,P11-1105,0,0.0377789,"Missing"
C14-1180,P13-2071,0,0.0781107,"hrase-based translation systems (Koehn et al., 2003) rely on language model and lexicalized reordering model to capture lexical dependencies that span phrase boundaries. Their translation models, however, do not explicitly model context dependencies between translation units. To address this limitation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be effective in helping the phrase-based translation models overcome the phrasal independence assumption (Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006; Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual tuples also known as minimal translation units (MTUs). Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural networks with factorizations to model bilingual tuples in a continuous space. Although the authors reported some gains over the n-gram model in machine translation tasks, these models can only capture a limited amount of context and remain a kind of n-gram mode"
C14-1180,2012.iwslt-evaluation.1,0,0.0406003,"ores of the RNNTSMs, the factorized RNNTSM (fRNNTSM), the fRNNTSMsource (sfRNNTSM), the fRNNTSMtarget with the word-bilingual-tuple-I and their combination. The numbers in the parentheses are the absolute improvements over the Baseline. In Equation 7, ψ stands for one of the connection weights in the neural networks and η is the learning rate. After each iteration, it uses validation data for stopping and controlling the learning rate. Usually, our RNNs needs 10 to 20 iterations. ψ new = ψ previous − η × ∂L ∂ψ (7) 5 Experiments We experiment with two language pairs on the IWSLT2012 data sets (Federico et al., 2012), with English as source and French, German as target. The IWSLT data comes from TED speecheds, given by leaders in various fields and covering an open set of topics in technology, entertainment, design, and many others. In the following experiments, the IWSLT dev2010 set is used as the tuning set, the tst2010, tst2011, and tst2012 as the test sets. Phrase-based translation systems are constructed as baselines using standard settings (GIZA++ alignment, grow-diag-final-and, lexical reordering models, SRILM, and MERT optimizer) in the MOSES toolkit (Koehn et al., 2007). The proposed models are u"
C14-1180,J13-1005,0,0.0217327,"Missing"
C14-1180,D13-1176,0,0.0876826,"Missing"
C14-1180,N03-1017,0,0.0110605,"ides in addition to bilingual tuples; (iii) we investigate heuristic rules to decompose phrasal bilingual tuples to word bilingual tuples for reducing the out-of-tuple-vocabulary rate and providing fine-grained tuple sequence model; (iv) we integrate the proposed models into the state-of-the-art phrase-based translation system (MOSES) as a supplement of the work in (Son et al., 2012) that is a complete n-gram translation system. 2 Related Work The n-gram translation model (Marino et al., 2006) is a Markov model over phrasal bilingual tuples and can improve the phrase-based translation system (Koehn et al., 2003) by providing contextual dependencies between phrase pairs. To further improve the n-gram translation model, Crego and Yvon (2010) explored factored bilingual n-gram language models. Durrani et al. (2011) proposed a joint sequence model for the translation and reordering probabilities. Zhang et al. (2013) explored multiple decomposition structures as well as dynamic bidirectional decomposition. Since neural networks advance the state of the art in the fields of image processing, acoustic modeling (Seide et al., 2011), language modeling (Bengio et al., 2003), natural language processing (Collob"
C14-1180,P07-2045,0,0.00880799,"he IWSLT2012 data sets (Federico et al., 2012), with English as source and French, German as target. The IWSLT data comes from TED speecheds, given by leaders in various fields and covering an open set of topics in technology, entertainment, design, and many others. In the following experiments, the IWSLT dev2010 set is used as the tuning set, the tst2010, tst2011, and tst2012 as the test sets. Phrase-based translation systems are constructed as baselines using standard settings (GIZA++ alignment, grow-diag-final-and, lexical reordering models, SRILM, and MERT optimizer) in the MOSES toolkit (Koehn et al., 2007). The proposed models are used to re-score n-best lists produced by the baseline systems. The n-best size is set to at most 1000 for each test sentence. During the n-best re-scoring, the weights are re-tuned on the dev2010 data set with MERT optimizer4 . The proposed RNN-based models are evaluated on a small task and a large task. For the parameters of all the RNN-based models, we set the number of hidden neurons in the hidden layer to 480 and classes in the output layer to 300. 5.1 Small Task In the small task, the training data only contains the speech-style bi-text, i.e., the human translat"
C14-1180,J06-4004,0,0.438921,"Missing"
C14-1180,J03-1002,0,0.00839837,"Missing"
C14-1180,P06-2093,0,0.0258858,"ntal results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate in speech recognition even though it is harder to train properly. Therefore, in this paper we take the advantages of RNN and tuple sequence model and propose recurrent neural network-based tuple sequence models (RNNTSMs) to improve phrase-based translation system. Our RNNTSMs are capable of modeling long-span context and have better generalization. Compared with such related studies as (Schwenk et al., 2006; Son et al., 2012), our main contributions can This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The IWSLT workshop aims at translating TED speeches (http://www.ted.com), a collection of public lectures covering a variety of topics. 1908 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1908–1917, Dublin, Ireland, August 23-29 2014. be summarized as: (i) our models c"
C14-1180,D07-1045,0,0.0219815,"is limitation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be effective in helping the phrase-based translation models overcome the phrasal independence assumption (Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006; Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual tuples also known as minimal translation units (MTUs). Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural networks with factorizations to model bilingual tuples in a continuous space. Although the authors reported some gains over the n-gram model in machine translation tasks, these models can only capture a limited amount of context and remain a kind of n-gram model. In language modeling, experimental results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate in speech recognition even though it is harder"
C14-1180,C12-2104,0,0.0337656,"Missing"
C14-1180,D13-1170,0,0.00635582,"Missing"
C14-1180,N12-1005,0,0.117861,"et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be effective in helping the phrase-based translation models overcome the phrasal independence assumption (Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006; Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual tuples also known as minimal translation units (MTUs). Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural networks with factorizations to model bilingual tuples in a continuous space. Although the authors reported some gains over the n-gram model in machine translation tasks, these models can only capture a limited amount of context and remain a kind of n-gram model. In language modeling, experimental results in (Mikolov et al., 2011; Arisoy et al., 2012; Sundermeyer et al., 2013) showed that recurrent neural networks (RNNs) outperform feed-forward neural networks in both perplexity and word error rate in speech recognition even though it is harder to train properly."
C14-1180,2007.mtsummit-papers.67,0,0.13036,"Missing"
C14-1180,C12-1173,1,0.83274,"of six models, as described in Table 4. We can observe that: (1) The combination of the proposed RNNTSMs only trained on the speechstyle data can essentially enhance the baselines by 1.2-1.8 BLEU points. (2) The improvements over the RNNLMs are significant on the English-German task but these improvements are modest on the EnglishFrench task. Note that the factorized RNNTSMs and the RNNLMs in the large task are also only trained the speech-style parallel corpus. In future work, we will train them on a bigger corpus, which can be expected to further increase the performance (Auli et al., 2013; Wu et al., 2012). 6 Conclusion Most prior neural network-based translation models either employ feed-forward neural networks to explicitly integrate source information via word-to-word alignment, or use recurrent neural networks in which source information is implicitly represented with a compressed vector. In this paper, we present recurrent neural network-based tuple sequence models (RNNTSMs) to compute probabilities of bilingual tuples in continuous space. One of major advantages is their potential to capture long-span history compared with feed-forward neural networks. In addition, our models can well add"
C14-1180,P13-1017,0,0.0485583,"Missing"
C14-1180,N13-1002,0,0.0973927,"n systems (Koehn et al., 2003) rely on language model and lexicalized reordering model to capture lexical dependencies that span phrase boundaries. Their translation models, however, do not explicitly model context dependencies between translation units. To address this limitation, Marino et al. (2006) and Crego and Yvon (2010) proposed n-gram-based translation systems to capture dependencies across phrasal boundaries. The n-gram translation models have been shown to be effective in helping the phrase-based translation models overcome the phrasal independence assumption (Durrani et al., 2013; Zhang et al., 2013). Most of the n-gram translation models (Marino et al., 2006; Durrani et al., 2013; Zhang et al., 2013) employed Markov (n-gram) model over sequence of bilingual tuples also known as minimal translation units (MTUs). Recently, some pioneer studies (Schwenk et al., 2007; Son et al., 2012) proposed feed-forward neural networks with factorizations to model bilingual tuples in a continuous space. Although the authors reported some gains over the n-gram model in machine translation tasks, these models can only capture a limited amount of context and remain a kind of n-gram model. In language modeli"
C16-1134,D07-1090,0,0.0356033,"anually translated 3,000/5,000 sentences from other data sources on the Web for each language pair. All hyperparameters for each method are optimized using the development data and final evaluation is performed using the test data. During word alignment, IBM Model 1 (Brown et al., 1993) and HMM alignment (Vogel et al., 1996) were performed using one-best preordered source sentences and corresponding target sentences. The phrase table was built according to the alignment results, and shared with all decoding methods. For the English language model, a 4-gram model with stupid backoff smoothing (Brants et al., 2007) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reordering model (Zens and Ney, 2006) enhanced by a state-of-the-art preordering method based on bracketing transduction grammar (Nakagawa, 2015). We used similar decoding strategy and other basic feature functions to Moses (Koehn et al., 2007) except some neural lexical features such as NNJM (De"
C16-1134,J93-2003,0,0.0611718,"istic characteristics when compared with English. For the training data, we used a parallel corpus by mining from the Web using an in-house crawler. The corpus contains 9.5M sentences and 160M words on average, at least 8.0M sentences and 140M words for each language pair. For the development/test data, we separately sampled and manually translated 3,000/5,000 sentences from other data sources on the Web for each language pair. All hyperparameters for each method are optimized using the development data and final evaluation is performed using the test data. During word alignment, IBM Model 1 (Brown et al., 1993) and HMM alignment (Vogel et al., 1996) were performed using one-best preordered source sentences and corresponding target sentences. The phrase table was built according to the alignment results, and shared with all decoding methods. For the English language model, a 4-gram model with stupid backoff smoothing (Brants et al., 2007) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of"
C16-1134,W16-2213,0,0.0255664,"Missing"
C16-1134,P14-1129,0,0.0197194,"7) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reordering model (Zens and Ney, 2006) enhanced by a state-of-the-art preordering method based on bracketing transduction grammar (Nakagawa, 2015). We used similar decoding strategy and other basic feature functions to Moses (Koehn et al., 2007) except some neural lexical features such as NNJM (Devlin et al., 2014). Only one-best preordering candidate is used for the baseline system. We chose the best distortion limit of the baseline system for each language pair by the BLEU (Papineni et al., 2002) score on the development data. We also compared the reranking method (Li et al., 2007), which translates all preordering candidates using conventional PBMT (our baseline system) and chose one with the best score. To do that, we used simple linear interpolation between decoder’s score D and preordering confidence C with a hyperparameter λ as follows: Score(C, D) := λ · C + (1 − λ) · D. (4) We varied the number"
C16-1134,P08-1115,0,0.0138755,"by Algorithm 1 guarantee that all integrated preordering candidates are represented as a path over the lattice, and also guarantee that all words in the source sentence appear only once in any paths over the lattice. In addition, the preordering lattice often includes extra paths which represents other preordering candidates not in the original candidate set A. Thus, the decoding algorithm described in the next section actually explores more preordering candidates when compared with a decoding algorithm which relies only on A. The preordering lattice is similar to the word lattice structure (Dyer et al., 2008), but all edges in the preordering lattice represent specific words in one source sentence. Daiber et al. (2016) also described a similar structure to our lattice using finite-state transducer (FST), and applied determinization and minimization to compress the lattice. On the other hand, we introduced more simple algorithm described in Algorithm 1 to achieve similar compression. 3 Decoding Algorithm over the Preordering Lattice We also intoduce a simple decoding algorithm to generate translations directly using the preordering lattice. Our algorithm runs by traversing the lattice in a left-to-"
C16-1134,D08-1089,0,0.0309713,"ple preordering candidates, and it is trivial to employ existing preordering methods into our system. In our experiments for translating diverse 11 languages into English, the proposed method outperforms conventional phrase-based decoder in terms of translation qualities under comparable or faster decoding time. 1 Introduction One of the main problem of phrase-based statistical machine translation (PBMT) (Koehn et al., 2003; Och and Ney, 2004) is handling the difference of word orders between source and target languages. Decoding-time reordering models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig et al., 2012; Nakagawa, 2015) change word order"
C16-1134,W13-0805,0,0.0177935,"to integrating these methods in a single system while comprehending their interactions. In this study, we propose a new phrase-based decoding method which employs multiple preordering candidates for a source sentence. Our method first encodes multiple preordering candidates as a compact graph structure (we call it preordering lattice), and generates translations by a single-pass traversal on the preordering lattice which can take into account all preordering candidates. Several previous work proposed decoding methods using graph structures with respect to preordering (Niehues and Kolss, 2009; Herrmann et al., 2013a; Herrmann et al., 2013b); however, these methods are tightly integrated with a specific graph structure defined on top of the methods themselves. Another previous work focused on multi-source translation based on a confusion network of multiple source 1419 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1419–1428, Osaka, Japan, December 11-17 2016. Figure 1: Generating preordering lattice from multiple preordering candidates. sentences (Schroeder et al., 2009; Jiang et al., 2011); however, the derived confusion network is to"
C16-1134,2013.iwslt-papers.11,0,0.0333811,"to integrating these methods in a single system while comprehending their interactions. In this study, we propose a new phrase-based decoding method which employs multiple preordering candidates for a source sentence. Our method first encodes multiple preordering candidates as a compact graph structure (we call it preordering lattice), and generates translations by a single-pass traversal on the preordering lattice which can take into account all preordering candidates. Several previous work proposed decoding methods using graph structures with respect to preordering (Niehues and Kolss, 2009; Herrmann et al., 2013a; Herrmann et al., 2013b); however, these methods are tightly integrated with a specific graph structure defined on top of the methods themselves. Another previous work focused on multi-source translation based on a confusion network of multiple source 1419 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1419–1428, Osaka, Japan, December 11-17 2016. Figure 1: Generating preordering lattice from multiple preordering candidates. sentences (Schroeder et al., 2009; Jiang et al., 2011); however, the derived confusion network is to"
C16-1134,W10-1736,0,0.0235847,"ng models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig et al., 2012; Nakagawa, 2015) change word orders of source sentence to be close to the target sentence before starting the decoding process. These methods can use global information in the source sentence and may generate grammatically correct reordering results. However, previous PBMT methods with preordering usually take only one-best preordered sentence and it is difficult to avoid the noise of the input caused by the errors from preordering methods. One of the trivial way to avoid preordering errors is to obtain N -best preordering candidates, translate each candidate one-by-one a"
C16-1134,W11-1004,0,0.0214653,"ct to preordering (Niehues and Kolss, 2009; Herrmann et al., 2013a; Herrmann et al., 2013b); however, these methods are tightly integrated with a specific graph structure defined on top of the methods themselves. Another previous work focused on multi-source translation based on a confusion network of multiple source 1419 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1419–1428, Osaka, Japan, December 11-17 2016. Figure 1: Generating preordering lattice from multiple preordering candidates. sentences (Schroeder et al., 2009; Jiang et al., 2011); however, the derived confusion network is too constrained to represent variation of preordering, and it cannot take the advantage of alternative reordering in the multiple source sentences. Compared with above previous works, our method is more generic in that the preordering lattice is constructed based only on the word permutations of the source sentence which are generated from arbitrary and independent preordering methods, and the preordering lattice guarantees that all preordering candidates are compactly encoded in the graph structure. In addition, we show that our preordering lattice"
C16-1134,N03-1017,0,0.0575986,"dering is performed. As a result, its runtime is very fast and implementing the algorithm becomes easy. Our system does not depend on specific preordering methods as long as they output multiple preordering candidates, and it is trivial to employ existing preordering methods into our system. In our experiments for translating diverse 11 languages into English, the proposed method outperforms conventional phrase-based decoder in terms of translation qualities under comparable or faster decoding time. 1 Introduction One of the main problem of phrase-based statistical machine translation (PBMT) (Koehn et al., 2003; Och and Ney, 2004) is handling the difference of word orders between source and target languages. Decoding-time reordering models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consi"
C16-1134,2005.iwslt-1.8,0,0.0225035,"ing methods as long as they output multiple preordering candidates, and it is trivial to employ existing preordering methods into our system. In our experiments for translating diverse 11 languages into English, the proposed method outperforms conventional phrase-based decoder in terms of translation qualities under comparable or faster decoding time. 1 Introduction One of the main problem of phrase-based statistical machine translation (PBMT) (Koehn et al., 2003; Och and Ney, 2004) is handling the difference of word orders between source and target languages. Decoding-time reordering models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig"
C16-1134,P07-2045,0,0.0069552,"odel, a 4-gram model with stupid backoff smoothing (Brants et al., 2007) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reordering model (Zens and Ney, 2006) enhanced by a state-of-the-art preordering method based on bracketing transduction grammar (Nakagawa, 2015). We used similar decoding strategy and other basic feature functions to Moses (Koehn et al., 2007) except some neural lexical features such as NNJM (Devlin et al., 2014). Only one-best preordering candidate is used for the baseline system. We chose the best distortion limit of the baseline system for each language pair by the BLEU (Papineni et al., 2002) score on the development data. We also compared the reranking method (Li et al., 2007), which translates all preordering candidates using conventional PBMT (our baseline system) and chose one with the best score. To do that, we used simple linear interpolation between decoder’s score D and preordering confidence C with a hyperparameter λ a"
C16-1134,P07-1091,0,0.176726,"5) change word orders of source sentence to be close to the target sentence before starting the decoding process. These methods can use global information in the source sentence and may generate grammatically correct reordering results. However, previous PBMT methods with preordering usually take only one-best preordered sentence and it is difficult to avoid the noise of the input caused by the errors from preordering methods. One of the trivial way to avoid preordering errors is to obtain N -best preordering candidates, translate each candidate one-by-one and select the most probable result (Li et al., 2007; Zhu, 2014). This method has an obvious problem on computation time because the decoding process is executed N times. Another way to resolve preordering errors is combining a preordering method and decoding-time reordering models. However, it is not trivial to integrating these methods in a single system while comprehending their interactions. In this study, we propose a new phrase-based decoding method which employs multiple preordering candidates for a source sentence. Our method first encodes multiple preordering candidates as a compact graph structure (we call it preordering lattice), and"
C16-1134,D08-1076,0,0.0474618,"rdering confidence C with a hyperparameter λ as follows: Score(C, D) := λ · C + (1 − λ) · D. (4) We varied the number of preordering candidates (1, 2, 4, 8, 16, 32, 64-bests) for the proposed method and the reranking method, and chose the one with the best BLEU on the development data. For the reranking method, we trained two variants by differentiating distortion limits, a system sharing the same limit with the PBMT baseline and those with 0, in order to examine the effects of preordering and decoding-time reordering. For all methods, we used lattice-based Minimum Error-rate Training (MERT) (Macherey et al., 2008) to optimize weights of features. Evaluation is carried out by BLEU using all test data, and subjective evaluation with 7-grade (0 to 6) Likert scale about translation acceptance using 400 randomly selected samples from the test data in each language pair. 4.2 Results and Discussion Figure 2 shows the number of nodes in actual preordering lattices generated from each source sentence in Japanese-English test data under 64-best preordering candidates. Upper group in this graph shows the number of unmerged nodes in which nodes are not shared when combining multiple preordering candidates in Algor"
C16-1134,P15-1021,1,0.915681,", 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig et al., 2012; Nakagawa, 2015) change word orders of source sentence to be close to the target sentence before starting the decoding process. These methods can use global information in the source sentence and may generate grammatically correct reordering results. However, previous PBMT methods with preordering usually take only one-best preordered sentence and it is difficult to avoid the noise of the input caused by the errors from preordering methods. One of the trivial way to avoid preordering errors is to obtain N -best preordering candidates, translate each candidate one-by-one and select the most probable result (Li"
C16-1134,D12-1077,1,0.850514,"., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig et al., 2012; Nakagawa, 2015) change word orders of source sentence to be close to the target sentence before starting the decoding process. These methods can use global information in the source sentence and may generate grammatically correct reordering results. However, previous PBMT methods with preordering usually take only one-best preordered sentence and it is difficult to avoid the noise of the input caused by the errors from preordering methods. One of the trivial way to avoid preordering errors is to obtain N -best preordering candidates, translate each candidate one-by-one and select the most pr"
C16-1134,W09-0435,0,0.0287787,"wever, it is not trivial to integrating these methods in a single system while comprehending their interactions. In this study, we propose a new phrase-based decoding method which employs multiple preordering candidates for a source sentence. Our method first encodes multiple preordering candidates as a compact graph structure (we call it preordering lattice), and generates translations by a single-pass traversal on the preordering lattice which can take into account all preordering candidates. Several previous work proposed decoding methods using graph structures with respect to preordering (Niehues and Kolss, 2009; Herrmann et al., 2013a; Herrmann et al., 2013b); however, these methods are tightly integrated with a specific graph structure defined on top of the methods themselves. Another previous work focused on multi-source translation based on a confusion network of multiple source 1419 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1419–1428, Osaka, Japan, December 11-17 2016. Figure 1: Generating preordering lattice from multiple preordering candidates. sentences (Schroeder et al., 2009; Jiang et al., 2011); however, the derived"
C16-1134,J04-4002,0,0.636057,"As a result, its runtime is very fast and implementing the algorithm becomes easy. Our system does not depend on specific preordering methods as long as they output multiple preordering candidates, and it is trivial to employ existing preordering methods into our system. In our experiments for translating diverse 11 languages into English, the proposed method outperforms conventional phrase-based decoder in terms of translation qualities under comparable or faster decoding time. 1 Introduction One of the main problem of phrase-based statistical machine translation (PBMT) (Koehn et al., 2003; Och and Ney, 2004) is handling the difference of word orders between source and target languages. Decoding-time reordering models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations o"
C16-1134,P02-1040,0,0.101324,"tem. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reordering model (Zens and Ney, 2006) enhanced by a state-of-the-art preordering method based on bracketing transduction grammar (Nakagawa, 2015). We used similar decoding strategy and other basic feature functions to Moses (Koehn et al., 2007) except some neural lexical features such as NNJM (Devlin et al., 2014). Only one-best preordering candidate is used for the baseline system. We chose the best distortion limit of the baseline system for each language pair by the BLEU (Papineni et al., 2002) score on the development data. We also compared the reranking method (Li et al., 2007), which translates all preordering candidates using conventional PBMT (our baseline system) and chose one with the best score. To do that, we used simple linear interpolation between decoder’s score D and preordering confidence C with a hyperparameter λ as follows: Score(C, D) := λ · C + (1 − λ) · D. (4) We varied the number of preordering candidates (1, 2, 4, 8, 16, 32, 64-bests) for the proposed method and the reranking method, and chose the one with the best BLEU on the development data. For the reranking"
C16-1134,E09-1082,0,0.0244578,"ph structures with respect to preordering (Niehues and Kolss, 2009; Herrmann et al., 2013a; Herrmann et al., 2013b); however, these methods are tightly integrated with a specific graph structure defined on top of the methods themselves. Another previous work focused on multi-source translation based on a confusion network of multiple source 1419 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1419–1428, Osaka, Japan, December 11-17 2016. Figure 1: Generating preordering lattice from multiple preordering candidates. sentences (Schroeder et al., 2009; Jiang et al., 2011); however, the derived confusion network is too constrained to represent variation of preordering, and it cannot take the advantage of alternative reordering in the multiple source sentences. Compared with above previous works, our method is more generic in that the preordering lattice is constructed based only on the word permutations of the source sentence which are generated from arbitrary and independent preordering methods, and the preordering lattice guarantees that all preordering candidates are compactly encoded in the graph structure. In addition, we show that our"
C16-1134,C96-2141,0,0.235072,"h English. For the training data, we used a parallel corpus by mining from the Web using an in-house crawler. The corpus contains 9.5M sentences and 160M words on average, at least 8.0M sentences and 140M words for each language pair. For the development/test data, we separately sampled and manually translated 3,000/5,000 sentences from other data sources on the Web for each language pair. All hyperparameters for each method are optimized using the development data and final evaluation is performed using the test data. During word alignment, IBM Model 1 (Brown et al., 1993) and HMM alignment (Vogel et al., 1996) were performed using one-best preordered source sentences and corresponding target sentences. The phrase table was built according to the alignment results, and shared with all decoding methods. For the English language model, a 4-gram model with stupid backoff smoothing (Brants et al., 2007) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reo"
C16-1134,C04-1073,0,0.0677985,"Decoding-time reordering models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig et al., 2012; Nakagawa, 2015) change word orders of source sentence to be close to the target sentence before starting the decoding process. These methods can use global information in the source sentence and may generate grammatically correct reordering results. However, previous PBMT methods with preordering usually take only one-best preordered sentence and it is difficult to avoid the noise of the input caused by the errors from preordering methods. One of the trivial way to avoid preordering errors is to obtain N -best preordering candidates, translate each"
C16-1134,W06-3108,0,0.0516109,"as they output multiple preordering candidates, and it is trivial to employ existing preordering methods into our system. In our experiments for translating diverse 11 languages into English, the proposed method outperforms conventional phrase-based decoder in terms of translation qualities under comparable or faster decoding time. 1 Introduction One of the main problem of phrase-based statistical machine translation (PBMT) (Koehn et al., 2003; Och and Ney, 2004) is handling the difference of word orders between source and target languages. Decoding-time reordering models (Koehn et al., 2005; Zens and Ney, 2006; Galley and Manning, 2008) measure positional relationship between each phrase at the decoding time. However, reordering models have a common problem in that it is difficult to take global information in the source sentence into account, and as a result the decoder may generate grammatically incorrect word orders. In addition, using reordering models demands a complicated decoding algorithm, in which the decoder has to consider concatenations of source phrases with arbitrary orders. On the other hand, preordering methods (Xia and McCord, 2004; Isozaki et al., 2010; Neubig et al., 2012; Nakaga"
C16-1134,2008.iwslt-papers.8,0,0.0259025,"the confidence of preordering candidates, which are used as an additional feature during decoding. After numbers of preliminary experiments, we adopted the product of maximum preordering confidence score and the ratio of phrase length based on our preliminary studies: |p| · max γ(n, p), n {I Cn , if p ⊂ Lattice(An ) γ(n, p) := −∞, otherwise, f (p) := (2) (3) where p represents an arbitrary path over the preordering lattice. −∞ means the decoder never choose the path p, and this formulation corresponds to IsCandidatePath condition in Algorithm 2. Compared with the conventional decoding method (Zens and Ney, 2008), the proposed method can eliminate some complex score calculations, e.g., rest cost estimation and decoding-time reorderings, because each path in the reordering lattice holds complete information of the word order. As a result, the proposed method makes the decoding algorithm simpler than the conventional method. 1422 4 Experiments 4.1 Experimental settings We evaluated our proposed method under the settings of translating into English. We chose 11 language pairs consisting of 6 European languages (Fr/De/It/Pt/Ru/Es) and 5 Asian languages (Ar/Zh/Ja/Ko/Tr), which have different linguistic cha"
C16-1134,W14-7004,0,0.0133756,"ders of source sentence to be close to the target sentence before starting the decoding process. These methods can use global information in the source sentence and may generate grammatically correct reordering results. However, previous PBMT methods with preordering usually take only one-best preordered sentence and it is difficult to avoid the noise of the input caused by the errors from preordering methods. One of the trivial way to avoid preordering errors is to obtain N -best preordering candidates, translate each candidate one-by-one and select the most probable result (Li et al., 2007; Zhu, 2014). This method has an obvious problem on computation time because the decoding process is executed N times. Another way to resolve preordering errors is combining a preordering method and decoding-time reordering models. However, it is not trivial to integrating these methods in a single system while comprehending their interactions. In this study, we propose a new phrase-based decoding method which employs multiple preordering candidates for a source sentence. Our method first encodes multiple preordering candidates as a compact graph structure (we call it preordering lattice), and generates t"
D07-1080,P05-1012,0,0.310578,"ptimized toward a set of good translations found in the k-best list across iterations. The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss. The parameters are trained using the 764 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 764–773, Prague, June 2007. 2007 Association for Computational Linguistics Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006). MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006). Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum error training with a small number of features. This paper is organized as follows: First, Section 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined"
D07-1080,J04-4002,0,0.0244586,"achine translation by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant"
D07-1080,2004.iwslt-evaluation.13,0,0.0101531,"concatenated and intersected with n-gram. 3 Features 3.1 Baseline Features The hierarchical phrase-based translation system employs standard numeric value features: • n-gram language model to capture the fluency of the target side. • Hierarchical phrase translation probabilities in ¯ and h(bβ|γ), ¯ both directions, h(γ|bβ) estimated ¯ by relative counts, count(γ, bβ). • Word-based lexically weighted models of ¯ and hlex (bβ|γ) ¯ hlex (γ|bβ) using lexical translation models. • Word-based insertion/deletion penalties that penalize through the low probabilities of the lexical translation models (Bender et al., 2004). • Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b). 3.2 Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system. We may use any binary features, such as   English word “violate” and Arabic    1 word “tnthk” appeared in e and f . h( f, e) =     0 otherwise. The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pai"
D07-1080,P05-1033,0,0.854373,"ber of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional s"
D07-1080,P06-1121,0,0.0405344,"training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional small feature set. Bangalore et al. (2006) trained"
D07-1080,N03-1017,0,0.244563,"by using a large number of features with an online large-margin training algorithm. The millions of parameters were tuned only on a small development set consisting of less than 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a"
D07-1080,P03-1021,0,0.59948,"on 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined including numeric features for our baseline system. Section 4 introduces an online large-margin training algorithm using MIRA with our key components. The experiments are presented in Section 5 followed by discussion in Section 6. 2 Statistical Machine Translation We use a log-linear approach (Och, 2003) in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: eˆ = argmax wT · h( f, e) (1) e where h( f, e) is a large-dimension feature vector. w is a weight vector that scales the contribution from each feature. Each feature can take any real value, such as the log of the n-gram language model to represent fluency, or a lexicon model to capture the word or phrase-wise correspondence. 2.1 Hierarchical Phrase-based SMT Chiang (2005) introduced the hierarchical phrasebased translation approach, in which non-terminals are emb"
D07-1080,P02-1040,0,0.107918,"ed as “@@@@/@/@@”. We consider all possible combination of those token types. For example, the word pair feature (violate, tnthk) is normalized and expanded to (viol+, tnthk), (viol+, tnth+), (violate, tnth+), etc. using the 4-letter prefix token type. 4 Online Large-Margin Training Algorithm 1 is our generic online training algorithm. The algorithm is slightly different from other online training algorithms (Tillmann and Zhang, 2006; Liang et al., 2006) in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, i.e. BLEU (Papineni et al., 2002). In line 3, a k-best list is generated by bestk (·) using the current weight vector wi for the training instance of ( f t , et ). Each training instance has multiple (or, possibly one) reference translations et for the source sentence f t . Using the k-best list, m-best oracle translations Ot is updated by oraclem (·) for every iteration (line 4). Usually, a decoder cannot generate translations that exactly match the reference translations due to its beam search pruning and OOV. Thus, we cannot always assign scores for each reference translation. Therefore, possible oracle translations are ma"
D07-1080,P04-1007,0,0.0559747,"of phrase translation pairs, but it is trivial to define the features over hierarchical phrases. X1 X2 f j−1 fj f j+3 X3 f j+1 f j+2 Figure 2: Example hierarchical features. same way, we will be able to include deletion features where a non-aligned source word is associated with the target sentence. However, this would lead to complex decoding in which all the translated words are memorized for each hypothesis, and thus not integrated in our feature set. 3.2.3 Target Bigram Features Target side bigram features are also included to directly capture the fluency as in the n-gram language model (Roark et al., 2004). For instance, bigram features of (ei−1 , ei ), (ei , ei+1 ), (ei+1 , ei+2 )... are observed in Figure 1. 3.2.4 Hierarchical Features In addition to the phrase motivated features, we included features inspired by the hierarchical structure. Figure 2 shows an example of hierarchical phrases in the source side, consisting of X 1 → E E D D D E f j−1 X 2 f j+3 , X 2 → f j f j+1 X 3 and X 3 → f j+2 . Hierarchical features capture the dependency of the source words in a parent phrase to the source words in child phrases, such as ( f j−1 , f j ), ( f j−1 , f j+1 ), ( f j+3 , f j ), ( f j+3 , f j+1 )"
D07-1080,P06-2098,0,0.147558,"st list across iterations. The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss. The parameters are trained using the 764 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 764–773, Prague, June 2007. 2007 Association for Computational Linguistics Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006). MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006). Experiments were carried out on an Arabicto-English translation task, and we achieved significant improvements over conventional minimum error training with a small number of features. This paper is organized as follows: First, Section 2 introduces the framework of statistical machine translation. As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005). In Section 3, a set of binary sparse features are defined including numeric features for our baseline system. Section"
D07-1080,W04-3201,0,0.02528,"nslation are created, which amount to m × k large-margin constraints. In this online training, only active features constrained by Eq. 3 are kept and updated, unlike offline training in which all possible features have to be extracted and selected in advance. The Lagrange dual form of Eq. 3 is:   1 X maxα(·)≥0 − || α(ˆe, e′ ) h( f t , eˆ ) − h( f t , e′ ) ||2 2 eˆ ,e′ X + α(ˆe, e′ )L(ˆe, e′ ; et ) eˆ,e′ Margin Infused Relaxed Algorithm The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) is an online version of the large-margin training algorithm for structured classification (Taskar et al., 2004) that has been successfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006). The basic idea is to keep the norm of the updates to the weight vector as small as possible, considering a margin at least as large as the loss of the incorrect classification. Line 5 of the weight vector update procedure in Algorithm 1 is replaced by the solution of: X ˆ i+1 = argmin ||wi+1 − wi ||+ C ξ(ˆe, e′ ) w wi+1 eˆ ,e′ subject to si+1 ( f t , eˆ ) − si+1 ( f t , e′ ) + ξ(ˆe, e′ ) ≥ L(ˆe, e′ ; et ) ξ(ˆe, e′ ) ≥ 0 ∀ˆe ∈ Ot , ∀e′ ∈ Ct (3) n oT where si ( f"
D07-1080,P06-1091,0,0.438964,"an 1K sentences. Experiments on Arabic-toEnglish translation indicated that a model trained with sparse binary features outperformed a conventional SMT system with a small number of features. 1 Introduction The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006). However, it does not scale well with a large number of features of the order of millions. Tillmann and Zhang (2006), Liang et al. (2006) and Bangalore et al. (2006) introduced sparse binary features for statistical machine translation trained on a large training corpus. In this framework, the problem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing. However, the use of a large number of features did not provide any significant improvements over a conventional small feature set. Bangalore et al. (2006) trained the lexical choice model by using Conditional Random Fields (CRF) realized on a WFST. Their modeling was reduced to M"
D07-1080,P06-1098,1,0.329931,"oice model by using Conditional Random Fields (CRF) realized on a WFST. Their modeling was reduced to Maximum Entropy Markov Model (MEMM) to handle a large number of features which, in turn, faced the labeling bias problem (Lafferty et al., 2001). Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm. Since the decoding is still expensive, their online training approach is approximated by enlarging a merged kbest list one-by-one with a 1-best output. Liang et al. (2006) introduced an averaged perceptron algorithm, but employed only 1-best translation. In Watanabe et al. (2006a), binary features were trained only on a small development set using a variant of voted perceptron for reranking k-best translations. Thus, the improvement is merely relative to the baseline translation system, namely whether or not there is a good translation in their k-best. We present a method to estimate a large number of parameters — of the order of millions — using an online training algorithm. Although it was intuitively considered to be prone to overfitting, training on a small development set — less than 1K sentences — was sufficient to achieve improved performance. In this method,"
D07-1080,P98-2230,0,0.0117577,"string of non-terminals. ∼ defines one-to-one mapping between non-terminals in γ and β. The use of phrase b¯ as a prefix maintains the strength of the phrasebase framework. A contiguous English side with a (possibly) discontiguous foreign language side preserves phrase-bounded local word reordering. At the same time, the target normalized framework still combines phrases hierarchically in a restricted manner. 2.3 Left-to-Right Target Generation Decoding is performed by parsing on the source side and by combining the projected target side. We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006). The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure. Since the rule form does not allow any holes for the target side, the integration with an n-g"
D07-1080,W06-3108,0,0.164848,"Word/hierarchical-phrase length penalties. • Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b). 3.2 Sparse Features In addition to the baseline features, a large number of binary features are integrated in our MT system. We may use any binary features, such as   English word “violate” and Arabic    1 word “tnthk” appeared in e and f . h( f, e) =     0 otherwise. The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pairs (Zens and Ney, 2006). When hierarchical phrases are extracted, the word alignment is preserved. If multiple word alignments are observed 766 ei−1 f j−1 ei ei+1 fj ei+2 ei+3 f j+1 f j+2 ei+4 f j+3 Figure 1: An example of sparse features for a phrase translation. with the same source and target sides, only the frequently observed word alignment is kept to reduce the grammar size. 3.2.1 Word Pair Features Word pair features reflect the word correspondence in a hierarchical phrase. Figure 1 illustrates an example of sparse features for a phrase translation pair f j , ..., f j+2 and ei , ..., ei+3 1 . From the word al"
D07-1080,W06-3119,0,0.0385186,"one mapping between non-terminals in γ and β. The use of phrase b¯ as a prefix maintains the strength of the phrasebase framework. A contiguous English side with a (possibly) discontiguous foreign language side preserves phrase-bounded local word reordering. At the same time, the target normalized framework still combines phrases hierarchically in a restricted manner. 2.3 Left-to-Right Target Generation Decoding is performed by parsing on the source side and by combining the projected target side. We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006). The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner. The search is guided with a push-down automaton, which keeps track of the span of uncovered source word positions. Combined with the rest-cost estimation aggregated in a bottom-up way, our decoder efficiently searches for the most likely translation. The use of a target normalized form further simplifies the decoding procedure. Since the rule form does not allow any holes for the target side, the integration with an n-gram language model is straightforward: the prefixed phr"
D07-1080,P06-1096,0,\N,Missing
D07-1080,J03-1002,0,\N,Missing
D07-1080,2006.iwslt-evaluation.14,1,\N,Missing
D07-1080,C98-2225,0,\N,Missing
D11-1137,D07-1101,0,0.0228192,"ocal features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser. In the field of SMT, for an approximation of MAP decoding, Li et al. (2009) proposed variational decoding and Kumar et al. (2009) pres"
D11-1137,P05-1022,0,0.903388,"baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative mo"
D11-1137,J07-2003,0,0.019011,"of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram lan1482 guage model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best V"
D11-1137,C96-1058,0,0.523394,", 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; 1479 Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates. In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al. (2009)’s approach which reranks only k-best candidates. Forests usually encode better candidates more"
D11-1137,N10-1115,0,0.0368683,"ecoding. The search algorithm in the reranking stage can be performed using dynamic programming algorithm. Our variational reranking is aimed at selecting a candidate from a forest, which is correct both in local and global. Our experimental results show more significant improvements than conventional approaches, such as k-best and forest generative reranking. In the future, we plan to investigate more appropriate generative models for reranking. PPAttachment is one of the most difficult problems for a natural language parser. We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. As we mentioned in Section 5.4, we also plan to incorporate semi-supervised learning into our framework, which may potentially improve our reranking performance. Acknowledgments We would like to thank Graham Neubig and Masashi Shimbo for their helpful comments and to the anonymous reviewers for their effort of reviewing our paper and giving valuable comments. This work was supported in part by Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Research Fellowship"
D11-1137,W05-1506,0,0.441204,"er factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram lan1482 guage model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best Viterbi algorithm except an additional loop in line 4. Line 4 references outgoing edge e′ of node h from a set of outgo"
D11-1137,P10-1110,0,0.158821,"Missing"
D11-1137,P08-1067,0,0.247023,"The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner,"
D11-1137,P10-1001,0,0.173547,"2008). Moreover, our reranking uses not only a generative model obtained from training data, but also a sentence specific generative model learned from a forest. In the reranking stage, we use linearly combined model of these models. We call this variational reranking model. The model proposed in this paper is factored in the third-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Viterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010). This algorithm enables us an exact 1-best reranking without any approximation. We summarize our contributions in this paper as follows. • To extend k-best to forest generative reranking. • We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009). • To obtain 1-best tree in the reranking stage, we Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics .top.0,8 propose an exact"
D11-1137,P08-1068,0,0.0309923,"hows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in 1486 Table 10 shows the comparison of the performance of variational reranking (16-best forests) with that of other systems. Our method outperforms supervised parsers with second-order features, and achieves comparable results compared to a parser with thirdorder features (Koo and Collins, 2010). We can not directly compare our method with semi-supervised parsers such as Koo et al. (2008)’s semi-sup and Suzuki et al. (2009), because ours does not use additional unlabeled data for training. The model trained from unlabeled data can be easily incorporated into our reranking framework. We plan to investigate semi-supervised learning in future work. Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 b"
D11-1137,P09-1019,0,0.105002,"locally and globally appropriate candidate from a forest. Table 7 shows the parsing time (on 2.66GHz Quad-Core Xeon) of the baseline k-best, generative reranking and variational reranking parsers (java implemented). The variational reranking parser contains the following procedures. 1. 2. 3. 4. k-best forest creation (baseline) Estimation of variational model Forest pruning Search with the third-order model Our reranking parser incurred little overhead to the Table 5: The comparison of the decoding frameworks: MBR decoding seeks a candidate which has the highest accuracy scores over a forest (Kumar et al., 2009). Variational decoding is performed based on Eq.8. XX XXX Eval Unlabeled XXX Decoding XX baseline 91.9 MBR (8-best forest) 91.99 Variational (8-best forest) 92.17 Table 7: The parsing time (CPU second per sentence) and accuracy score of the baseline k-best, generative reranking and variational reranking parsers k baseline generative variational 2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76) 4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81) 8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87) 16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89) 32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89) 64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)"
D11-1137,P09-1067,0,0.366974,"hird-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Viterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010). This algorithm enables us an exact 1-best reranking without any approximation. We summarize our contributions in this paper as follows. • To extend k-best to forest generative reranking. • We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009). • To obtain 1-best tree in the reranking stage, we Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics .top.0,8 propose an exact 1-best search algorithm with the third-order model. In experiments on English Penn Treebank data, we show that our proposed methods bring significant improvement to dependency parsing. Moreover, our variational reranking framework achieves consistent improvement, compared to conventional approaches, such as simple k-best a"
D11-1137,P05-1010,0,0.0396665,"n rule to dependency parsing. For dependency parsing, we can choose to model q ∗ as the tri-sibling and grandsibling generative models in section 3. 2 In case of dependency parsing, Titov and Henderson (2006) proposed that a loss function is simply defined using a dependency attachment score. 3 In SMT, a marginalization of all derivations which yield a paticular translation needs to be carried out for each translation. This makes the MAP decoding NP-hard in SMT. This variational approximate framework can be applied to other tasks collapsing spurious ambiguity, such as latent-variable parsing (Matsuzaki et al., 2005). Algorithm 2 DP-ML Estimation(HG(x)) 100 1: run inside and outside algorithm on HG(x) 2: for v ∈ V do 3: for e ∈ IE(v) do 4: ctsib = pe · α(v)/β(top) 5: for u ∈ tails(e) do 6: ctsib = ctsib · β(u) 7: for e′ ∈ IE(u) do 8: cgsib = pe · pe′ · α(v)/β(top) 9: for u′ ∈ tails(e)  u do 10: cgsib = cgsib · β(u′ ) 11: for u′′ ∈ tails(e′ ) do 12: cgsib = cgsib · β(u′′ ) 13: for u′′ ∈ tails(e′ ) do 14: c2 (u′′ |C(u′′ ))+ = cgsib 15: c2 (C(u′′ ))+ = cgsib 16: for u ∈ tails(e) do 17: c1 (u|C(u))+ = ctsib 18: c1 (C(u))+ = ctsib 19: MLE estimate q1∗ , q2∗ using formula Eq.14 = Unlabeled Accuracy 98 k=100 97"
D11-1137,E06-1011,0,0.138728,"b .sib .v g.. .h .sib .v Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order second-order (sibling) third-order (tri-sibling) third-order (grandsibling) McDonald et al. (2005) Eisner (1996a) McDonald et al. (2005) tri-sibling model Model 2 (Koo and Collins, 2010) grandsibling model (Sangati et al., 2009) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the order of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate"
D11-1137,P05-1012,0,0.710609,"g 2-nd term wt(h),t(sib),d 3-rd term wt(v),t(h),t(sib),d wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d wt(h),t(sib),t(tsib),d t(h),t(sib),t(tsib),d — — — — — — t(h),wt(sib),t(g),d wt(h),t(sib),t(g),d t(h),t(sib),t(g),d — — .. h .tsib .sib .v g.. .h .sib .v Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order second-order (sibling) third-order (tri-sibling) third-order (grandsibling) McDonald et al. (2005) Eisner (1996a) McDonald et al. (2005) tri-sibling model Model 2 (Koo and Collins, 2010) grandsibling model (Sangati et al., 2009) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the"
D11-1137,D08-1022,0,0.0136474,"ctsib as the posterior weight for computing expected count c1 of events in the tri-sibling model q1∗ . Lines 16-18 compute c1 for all events occuring in a hyperedge e. The expected count c2 needed for the estimation of grandsibling model q2∗ is extracted in lines 7-15. c2 for a grandsibling model must be extracted over two hyperedges e and e′ because it needs grandparent information. Lines 8-12 show the algorithm to compute the posterior weight cgsib of e and e′ , which 1484 is similar to that to compute the posterior weight of rules of tree substitution grammars used in treebased MT systems (Mi and Huang, 2008). Lines 13-15 compute expected counts c2 of events occuring over two hyperedges e and e′ . Finally, line 19 estimates q1∗ and q2∗ using the form in Eq.14. Li et al. (2009) assumes n-gram locality of the forest to efficiently train the model, namely, the baseline n-gram model has larger n than that of variational n-gram model. In our case, grandsibling locality is not embedded in the forest generated from the baseline parser. Therefore, we need to reference incoming hyperedges of tail nodes in line 7. y ∗ of Eq.12 may be locally appropriate but globally inadequate because q ∗ only approximates"
D11-1137,P03-1021,0,0.0907227,"e show the reductions list for each term of two models in Table 2. The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details. The final prediction is performed using a loglinear interpolated model. It interpolates the baseline discriminative model and two (tri-sibling and grandsibling) generative models. yˆ = argmax 2 ∑ log qn (top(y))θn y∈G(x) n=1 + log p(y|x)θbase (8) (5) where θ are parameters to adjust the weight of each = q1 (dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir) term in prediction. These parameters are tuned using MERT algorithm (Och, 2003) on development data = q1 (tag(v)|h, sib, tsib, dir) using a criterion of accuracy maximization. The rea×q1 (wrd(v)|tag(v), h, sib, tsib, dir) son why we chose MERT is that it effectively tunes ×q1 (dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir) dense parameters with a line search algorithm. 1481 q1 (v|h, sib, tsib, dir) Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word, POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition; later reductions throw away more and more"
D11-1137,W09-3839,0,0.304606,"achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; 1479 Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selec"
D11-1137,D09-1058,0,0.0295312,"scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in 1486 Table 10 shows the comparison of the performance of variational reranking (16-best forests) with that of other systems. Our method outperforms supervised parsers with second-order features, and achieves comparable results compared to a parser with thirdorder features (Koo and Collins, 2010). We can not directly compare our method with semi-supervised parsers such as Koo et al. (2008)’s semi-sup and Suzuki et al. (2009), because ours does not use additional unlabeled data for training. The model trained from unlabeled data can be easily incorporated into our reranking framework. We plan to investigate semi-supervised learning in future work. Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 baseline 3 3 4 0 4 5 6 4 11 11 8 8 12"
D11-1137,C10-1123,0,0.0187849,"using G(x), the conditional probability p(y|x) is typically derived as follows: p(y|x) = eγ·s(x,y) eγ·s(x,y) =∑ γ·s(x,y) Z(x) y∈G(x) e (2) where s(x, y) is the score function shown in Eq.1 and γ is a scaling factor to adjust the sharpness of the distribution and Z(x) is a normarization factor. 2.1 Hypergraph Representation We propose to encode many hypotheses in a compact representation called dependency forest. While there may be exponentially many dependency trees, the forest represents them in polynomial space. A dependency forest (or tree) can be defined as a hypergraph data strucure HG (Tu et al., 2010). Figure 1 shows an example of a hypergraph for a dependency tree. A shaded hyperedge e is defined as the following form: e : ⟨(I1,2 , girl3,5 , with5,8 ), saw1,8 ⟩. 1480 . .. .e . .saw1,8 . :V ... . . g. irl3,5 . :N .I1,2.:N . . . . . . . . . . . .a3,4.:D . . . . .with5,8 . :P . . . 6,8 :N . . .telescope . . .a .:D . . . . . 6,7 Figure 1: An example of dependency tree for a sentence “I saw a girl with a telescope”. The node saw1,8 is a head node of e. The nodes, I1,2 , girl3,5 and with5,8 , are tail nodes of e. The hyperedge e is an incoming edge for saw1,8 and outgoing edge for each of I1,2"
D11-1137,W03-3023,1,0.78739,"tion of variational decoding and generative reranking. We call this framework variational reranking. Table 4: The statistics of forests and 20-best lists on development data: this shows the average number of hyperedges and nodes per sentence and oracle scores. forest 20-best pruning threshold ρ = 10−3 — ave. num of hyperedges 180.67 255.04 ave. num of nodes 135.74 491.42 oracle scores 98.76 96.78 5 Experiments Experiments are performed on English Penn Treebank data. We split WSJ part of the Treebank into sections 02-21 for training, sections 22 for development, sections 23 for testing. We use Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factor γ = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters θ ar"
D12-1003,N07-1026,0,0.0604188,"Missing"
D12-1003,C10-1003,0,0.604363,"ered when computing bilingual relations, which have been neglected in previous methods. In addition to the co-occurrence-based graph construction, we propose a similarity graph, which also takes into account context similarities between words. The main contributions of this paper are as follows: • We propose a bilingual lexicon extraction method that captures co-occurrence relations with all the seeds, including indirect relations, using graph-based label propagation. In our experiments, we confirm that the proposed method outperforms conventional context-similarity-based methods (Rapp, 1999; Andrade et al., 2010), and works well even if the coverage of a seed lexicon is low. • We propose a similarity graph which represents context similarities between words. In our experiments, we confirm that a similarity graph is more effective than a co-occurrence-based graph. 2 Context-Similarity-based Extraction Method The bilingual lexicon extraction from comparable corpora was pioneered in (Rapp, 1995; Fung, 1995). 25 The popular similarity-based methods consist of the following steps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of e"
D12-1003,W11-1203,0,0.623374,"ertain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification. Step 2. Calculating context similarities: The contexts which are expressed in two different languages are mapped into the same space. Previous methods generally use a seed bilingual lexicon for this mapping. After that, similarities are calculated b"
D12-1003,C02-2020,0,0.0683871,"ニア (piranha)”. There are three context words for the query. However, the information on co-occurrence with “淡水 (freshwater)” disappears after the context vector is mapped, because the seed lexicon does not include “淡水 (freshwater)”. The same thing happens with the English word “piranha”. As a result, the pair of “ピラニア (piranha)” and “anaconda” could be wrongly identified as a translation pair. Some previous work focused on the problem of seed lexicon limitation. Morin and Prochasson (2011) complemented the seed lexicon with bilingual lexicon extracted from parallel sentences. Koehn and Knight (2002) used identically-spelled words in two languages as a seed lexicon. However, the method is not applicable for language pairs with different types of characters such as English and Japanese. Hazem et al. (2011) exploited k-nearest words for a query, which is very sensitive to the parameter k. Some previous work did not require any seed lexicon. Rapp (1995) proposed a computationally demanding matrix permutation method which maximizes a similarity between co-occurrence matrices in two languages. Ismail and Manandhar (2010) introduced a similarity measure between two words in different languages"
D12-1003,I05-1062,0,0.0123874,"d captures relations with all the seeds including indirect relations by propagating seed information. Moreover, we proposed using similarity graphs in propagation process in addition to cooccurrence graphs. Our experiments showed that the proposed method outperforms conventional contextsimilarity-based methods (Rapp, 1999; Andrade et al., 2010), and the similarity graphs improve the performance by clustering synonyms into the same translation. We are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in (Daille and Morin, 2005; Morin et al., 2007). In addition, indirect relations have also been used in other tasks, such as paraphrase acquisition from bilingual parallel corpora (Kok and Brockett, 2010). We will utilize their random walk approach or other graph-based techniques such as modified adsorption (Talukdar and Crammer, 2009) for generating seed distributions. We are also planning an end-toend evaluation, for instance, by employing the extracted bilingual lexicon into an MT system. Acknowledgments We thank anonymous reviewers of EMNLP-CoNLL 2012 for helpful suggestions and comments on a first version of this"
D12-1003,P11-1061,0,0.0421256,"i (or vj ). Then, in edge pruning, we preserve the edges with top 100 weight for each vertex. 3.2 Seed Propagation LP is a graph-based technique which transfers the labels from labeled data to unlabeled data in order to infer labels for unlabeled data. This is primarily used when there is scarce labeled data but abundant unlabeled data. LP has been successfully applied in common natural language processing tasks such as word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), multi-class lexicon acquisition (Alexandrescu and Kirchhoff, 2007), and part-of-speech tagging (Das and Petrov, 2011). LP iteratively propagates label information from any vertex to nearby vertices through weighted edges, and then a label distribution for each vertex is generated where the weights of all labels add up to 1. We adopt LP to obtain relations with all bilingual seeds including indirect relations by treating each seed as a label. First, each translated seed is assigned to a label, and then the labels are propagated in the graph described in Section 3.1. The seed distribution for each word is initialized as follows:  if vi ∈ Vs and z = vi  1 0 0 if vi ∈ Vs and z = vi , qi (z) =  u(z) otherwise"
D12-1003,P11-2071,0,0.0913553,"Missing"
D12-1003,C02-1166,0,0.042563,"ニア (piranha)”. There are three context words for the query. However, the information on co-occurrence with “淡水 (freshwater)” disappears after the context vector is mapped, because the seed lexicon does not include “淡水 (freshwater)”. The same thing happens with the English word “piranha”. As a result, the pair of “ピラニア (piranha)” and “anaconda” could be wrongly identified as a translation pair. Some previous work focused on the problem of seed lexicon limitation. Morin and Prochasson (2011) complemented the seed lexicon with bilingual lexicon extracted from parallel sentences. Koehn and Knight (2002) used identically-spelled words in two languages as a seed lexicon. However, the method is not applicable for language pairs with different types of characters such as English and Japanese. Hazem et al. (2011) exploited k-nearest words for a query, which is very sensitive to the parameter k. Some previous work did not require any seed lexicon. Rapp (1995) proposed a computationally demanding matrix permutation method which maximizes a similarity between co-occurrence matrices in two languages. Ismail and Manandhar (2010) introduced a similarity measure between two words in different languages"
D12-1003,W11-1204,0,0.0705797,"Missing"
D12-1003,C94-2178,0,0.0448485,"onservatism” in English. The proposed methods merge different senses by propagating seeds through these polysemous words in only one language side. This is why translation pairs could have wrong seed distributions and then the proposed methods could not identify correct translation pairs. We will leave this word sense disambiguation problem for future work. 6 Related Work Besides the comparable corpora approach discussed in Section 2, many alternatives have been proposed for bilingual lexicon extraction. The first is a method that finds translation pairs in parallel corpora (Wu and Xia, 1994; Fung and Church, 1994; Och and Ney, 2003). However, large parallel corpora are only available for a few language pairs and for limited domains. Moreover, even the large parallel corpora are relatively smaller than comparable corpora. The second is a method that exploits the Web. Lu et al. (2004) extracted translation pairs by mining web anchor texts and link structures. As an alternative, mixed-language web pages are exploited by first retrieving texts including both source and target languages from the web by using a search engine or simple rules, and then extracting translation pairs from the mixed-language text"
D12-1003,W97-0119,0,0.0676358,"The bilingual lexicon extraction from comparable corpora was pioneered in (Rapp, 1995; Fung, 1995). 25 The popular similarity-based methods consist of the following steps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneit"
D12-1003,P98-1069,0,0.941339,"generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in context"
D12-1003,W95-0114,0,0.749213,"rities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Na"
D12-1003,W09-1117,0,0.116105,"eps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a contex"
D12-1003,P04-1067,0,0.821213,"Missing"
D12-1003,P08-1088,0,0.774795,"sually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learni"
D12-1003,W11-1206,0,0.10853,"arity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference o"
D12-1003,H05-1061,0,0.00755926,"rpora are relatively smaller than comparable corpora. The second is a method that exploits the Web. Lu et al. (2004) extracted translation pairs by mining web anchor texts and link structures. As an alternative, mixed-language web pages are exploited by first retrieving texts including both source and target languages from the web by using a search engine or simple rules, and then extracting translation pairs from the mixed-language texts utilizing various clues: Zhang and Vines (2004) used cooccurrence statistics, Cheng et al. (2004) used cooccurrences and context similarity information, and Huang et al. (2005) used phonetic, semantic and frequency-distance features. Lin et al. (2008) proposed a method for extracting parenthetically translated terms, where a word alignment algorithm is used for establishing the correspondences between in-parenthesis and pre-parenthesis words. However, those methods cannot find translation pairs when they are not connected with each other through link structures, or when they do not co-occur in the same text. Transliteration is a completely different way for bilingual lexicon acquisition, in which a word in one language is converted into another language using phonet"
D12-1003,C10-2055,0,0.242532,"seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 24–36, Jeju Island"
D12-1003,W02-0902,0,0.880473,"act word translation pairs with a high-context similarity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of tex"
D12-1003,N10-1017,0,0.0168808,"Missing"
D12-1003,C10-1070,0,0.0987065,"2 Context-Similarity-based Extraction Method The bilingual lexicon extraction from comparable corpora was pioneered in (Rapp, 1995; Fung, 1995). 25 The popular similarity-based methods consist of the following steps: modeling contexts, calculating context similarities, and finding translation pairs. Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (An"
D12-1003,C10-2070,0,0.563014,"decessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification. Step 2. Calculating context similarities: The contexts which are expressed in two different languages are mapped into the same space. Previous methods generally use a"
D12-1003,P08-1113,0,0.0531508,"Missing"
D12-1003,W11-1205,0,0.171411,"rs with a high-context similarity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 20"
D12-1003,P07-1084,0,0.0601746,"h all the seeds including indirect relations by propagating seed information. Moreover, we proposed using similarity graphs in propagation process in addition to cooccurrence graphs. Our experiments showed that the proposed method outperforms conventional contextsimilarity-based methods (Rapp, 1999; Andrade et al., 2010), and the similarity graphs improve the performance by clustering synonyms into the same translation. We are planning to investigate the following open problems in future work: word sense disambiguation and translation of compound words as described in (Daille and Morin, 2005; Morin et al., 2007). In addition, indirect relations have also been used in other tasks, such as paraphrase acquisition from bilingual parallel corpora (Kok and Brockett, 2010). We will utilize their random walk approach or other graph-based techniques such as modified adsorption (Talukdar and Crammer, 2009) for generating seed distributions. We are also planning an end-toend evaluation, for instance, by employing the extracted bilingual lexicon into an MT system. Acknowledgments We thank anonymous reviewers of EMNLP-CoNLL 2012 for helpful suggestions and comments on a first version of this paper. We also thank"
D12-1003,P05-1049,0,0.0557205,"Missing"
D12-1003,J03-1002,0,0.0104573,"ocuments of all the domains. The Japanese texts were segmented and part-ofspeech tagged by ChaSen7 , and the English texts were tokenized and part-of-speech tagged by TreeTagger (Schmid, 1994). Next, function words were removed since function words with little semantic information spuriously co-occurred with many words. As a result, the number of distinct words in Japanese corpus and English corpus amounted to 1,111,302 and 4,099,8258 , respectively. We employed seed lexicons from two sources: (1) EDR bilingual dictionary (EDR, 1990), (2) automatic word alignments generated by running GIZA++ (Och and Ney, 2003) with the NTCIR parallel data consisting of 3,190,654 parallel sentences. From each source, we extracted pairs of nouns appearing in our corpus. From (2), we excluded word pairs where the average of 2-way translation proba6 SECTION G of IPC code indicates the physics domain. http://chasen-legacy.sourceforge.jp/ 8 The English words contain words in tables or mathematical formula but the Japanese words do not because the data format differs between English and Japanese. This is why the number of English words is larger than that of Japanese words, even though the number of English documents is s"
D12-1003,P11-1133,0,0.285386,"tors in accordance with the notion that importance varies by context positions. Gaussier et al. (2004) mapped context vectors via latent classes to capture synonymy and polysemy in a seed lexicon. Fiˇser et al. (2011) and Kaji (2005) calculated 2-way similarities. Step 3. Finding translation pairs: A pair of words is treated as a translation pair when their context similarity is high. Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (D´ejean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004). 2.1 Problems from Previous Works Most of previous methods used a seed bilingual lexicon for mapping modeled contexts in two different languages into the same space. The mapping heavily relies on the entries in a given bilingual lexicon. Therefore, if the coverage of the seed lexicon is low, 26 the context vectors become sparser and its discriminative capability becomes lower, leading to extraction of incorrect translation equivalents. Consider the example in Figure 1, where a context-similarity-based method and our proposed method find tra"
D12-1003,P95-1050,0,0.887097,"ntext similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon. Some of the previous methods tried to alleviate the problem of the limited seed lexicon size (Koehn and Knight, 2002; Morin and Prochasson, 2011; Hazem et al., 2011), while others did not require any seed lexicon (Rapp, 1995; Fung, 1995; Haghighi et al., 2008; Ismail and Manandhar, 2010; Daum´e III and Jagarlamudi, 2011). However, they suffer the problems of high computational cost (Rapp, 1995), sensitivity to parameters (Hazem et al., 2011), low accuracy (Fung, 1995; Ismail and Manandhar, 2010), and ineffectiveness for language pairs with 1 Although Vuli´c et al. (2011) regarded document-aligned texts such as texts on Wikipedia as comparable corpora, we do not limit comparable corpora to these kinds of texts. 24 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Comp"
D12-1003,P99-1067,0,0.751917,", the automatic building of bilingual lexicons from corpora is one of the issues that have attracted many researchers. As a solution, a number of previous works proposed extracting bilingual lexicons from comparable corpora, in which documents were not direct translations but shared a topic or domain1 . The use of comparable corpora is motivated by the fact that large parallel corpora are only available for a few language pairs and for limited domains. Most of the previous methods are based on assumption (I), that a word and its translation tend to appear in similar contexts across languages (Rapp, 1999). Based on this assumption, many methods calculate word similarity using context and then extract word translation pairs with a high-context similarity. We call these methods context-similaritybased methods. The context similarities are usually computed using a seed bilingual lexicon (e.g. a general bilingual dictionary) by mapping contexts expressed in two different languages into the same space. In the mapping, information not represented by the seed lexicon is discarded. Therefore, the context-similarity-based methods could not find accurate translation pairs if using a small seed lexicon."
D12-1003,C04-1089,0,0.494708,"dow (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification. Step 2. Calculating context similarities: The contexts which are expressed in two different language"
D12-1003,P11-2084,0,0.397648,"Missing"
D12-1003,1994.amta-1.26,0,0.191355,"ans “right” and “conservatism” in English. The proposed methods merge different senses by propagating seeds through these polysemous words in only one language side. This is why translation pairs could have wrong seed distributions and then the proposed methods could not identify correct translation pairs. We will leave this word sense disambiguation problem for future work. 6 Related Work Besides the comparable corpora approach discussed in Section 2, many alternatives have been proposed for bilingual lexicon extraction. The first is a method that finds translation pairs in parallel corpora (Wu and Xia, 1994; Fung and Church, 1994; Och and Ney, 2003). However, large parallel corpora are only available for a few language pairs and for limited domains. Moreover, even the large parallel corpora are relatively smaller than comparable corpora. The second is a method that exploits the Web. Lu et al. (2004) extracted translation pairs by mining web anchor texts and link structures. As an alternative, mixed-language web pages are exploited by first retrieving texts including both source and target languages from the web by using a search engine or simple rules, and then extracting translation pairs from"
D12-1003,C98-1066,0,\N,Missing
D12-1003,J98-4003,0,\N,Missing
D12-1037,P08-1024,0,0.14643,"mprovements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due"
D12-1037,D08-1024,0,0.623546,"introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the d"
D12-1037,P05-1033,0,0.0879558,"r experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times using MERT, and then se406 Methods Global method Local method Steps Decoding Retrieval Local training Seconds 2.0 +0.6 +0.3 NIST02 NIST05 27.07 27.75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set used to tune λ for MBUU and EBUU, and NIST06 and NIST08 are test"
D12-1037,D11-1004,0,0.149234,"e global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are"
D12-1037,P10-1064,0,0.0295431,"Missing"
D12-1037,2005.eamt-1.19,0,0.0784023,"arns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the test sentences. Our method resorts to some translation examples, which is similar as example-based translation or translation memory (Watanabe and Sumita, 2003; He et al., 2010; Ma et al., 2011). Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them 409 to discriminatively learn local weights. Similar to (Hildebrand et al., 2005; L¨u et al., 2007), our method also employes IR methods to retrieve examples for a given test set. Their methods utilize the retrieved examples to acquire translation model and can be seen as the adaptation of translation model. However, ours uses the retrieved examples to tune the weights and thus can be considered as the adaptation of tuning. Furthermore, since ours does not change the translation model which needs to run GIZA++ and it incrementally trains local weights, our method can be applied for online translation service. 7 Conclusion and Future Work This paper proposes a novel local"
D12-1037,D11-1125,0,0.403742,"or statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of th"
D12-1037,N03-1017,0,0.114511,"Missing"
D12-1037,P07-2045,0,0.014159,".75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set used to tune λ for MBUU and EBUU, and NIST06 and NIST08 are test sets. + means the local method is significantly better than MERT with p &lt; 0.05. lect the best-performing one as our baseline for the following experiments. As Table 1 indicates, our baseline In-Hiero is comparable to the phrase-based MT (Moses) and the hierarchical phrase-based MT (Moses hier) implemented in Moses, an open source MT toolkit2 (Koehn et al., 2007). Both of these systems are with default setting. All three systems are trained by MERT with 100 best candidates. To compare the local training method in Algorithm 2, we use a standard global training method, MERT, as the baseline training method. We do not compare with Algorithm 1, in which retraining is performed for each input sentence, since retraining for the whole test set is impractical given that each sentence-wise retraining may take some hours or even days. Therefore, we just compare Algorithm 2 with MERT. 5.2 Runtime Results To run the Algorithm 2, we tune the baseline weight Wb on"
D12-1037,W04-3250,0,0.0660269,"h modified Kneser-Ney smoothing (Chen and Goodrj . Due to the existence of L2 norm in objective man, 1998). In our experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times using MERT, and then se406 Methods Global method Local method Steps Decoding Retrieval Local training Seconds 2.0 +0.6 +0.3 NIST02 NIST05 27.07 27.75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EB"
D12-1037,C10-1075,0,0.091706,"009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of these methods is dependent on the choice of a development set, which may potentially lead to an unstable translation performance for testing. As referred in our experiment, the BLEU points on NIST08 are 402 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 402–411, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics  1, 0  h ( f 1 , e11 )  h ( f 1 , e12 )  2,0  h("
D12-1037,D07-1036,0,0.0396223,"Missing"
D12-1037,P11-1124,0,0.197989,"Missing"
D12-1037,P00-1056,0,0.0922694,"Missing"
D12-1037,P02-1038,0,0.282749,"method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. We propose efficient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007;"
D12-1037,P03-1021,0,0.66567,"meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and un"
D12-1037,P02-1040,0,0.0913028,"e eˆ(fj ; W ) is defined in Equation (1), and tence pair. We train a 4-gram language model on Error(rj , e) is the sentence-wise minus BLEU (Pa- the Xinhua portion of the English Gigaword corpineni et al., 2002) of a candidate e with respect to pus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodrj . Due to the existence of L2 norm in objective man, 1998). In our experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times usi"
D12-1037,D09-1147,0,0.080957,"arable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li"
D12-1037,2003.mtsummit-papers.54,1,0.859025,"nd the incremental training in line 5 of Algorithm 2. 3 Acquiring Training Examples In line 4 of Algorithm 2, to retrieve training examples for the sentence ti , we first need a metric to retrieve similar translation examples. We assume that the metric satisfy the property: more similar the test sentence and translation examples are, the better translation result one obtains when decoding the test sentence with the weight trained on the translation examples. The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence, (Watanabe and Sumita, 2003) defined a metric based on the combination of edit distance and TF-IDF (Manning and Sch¨utze, 1999) as follows: dist(f1 , f2 ) = θ × edit-dist(f1 , f2 )+ (1 − θ) × tf-idf(f1 , f2 ), (2) where θ(0 ≤ θ ≤ 1) is an interpolation weight, fi (i = 1, 2) is a word sequence and can be also considered as a document. In this paper, we extract similar examples from training data. Like examplebased translation in which similar source sentences have similar translations, we assume that the optimal translation weights of the similar source sentences are closer. 4 Incremental Training Based on Ultraconservati"
D12-1037,D07-1080,1,0.964911,"tion Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipel"
D12-1037,N09-2006,0,0.20149,"ts efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of"
D12-1037,C08-1074,0,\N,Missing
D12-1077,W05-0909,0,0.0281214,"dition is true, and 0 otherwise. An example of this is given in Figure 2 (b). To calculate an accuracy measure for ordering F 0 , we ﬁrst calculate the maximum loss for the sentence, which is equal to the total number of non-equal rank comparisons in the sentence5 F J−1 ∑ Lt (F 0 ) , max Lt (F˜ 0 ) F˜ 0 which will take a value between 0 (when F 0 has maximal loss), and 1 (when F 0 matches one of the oracle orderings). In Figure 2 (b), Lt (F 0 ) = 2 and max Lt (F˜ 0 ) = 8, so At (F 0 ) = 0.75. J ∑ 4.3 Chunk Fragmentation Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al., 2011) is chunk fragmentation. This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a diﬀerent position in the reordered sentence to read it in the target order. One way to measure the number of continuous chunks is considering 0 whether each word pair fj0 and fj+1 is discon0 tinuous (the rank of fj+1 is not equal to or one greater than fj0 ) 0 discont(fj0 , fj+1 )="
D12-1077,P10-2033,0,0.013484,"ansforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual p"
D12-1077,J07-2003,0,0.86214,"ng framework results in signiﬁcant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ord"
D12-1077,P11-2031,0,0.00445512,"xperiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F 0 and oracle F 0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ , but evaluated on the target sentence E instead of the reordered sentence F 0 . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types orig 3-step 3-step+φpos 3-step+φcf g lader lader+φpos lader+φcf g Chunk 61.22 63.51 64.28 65.76 73.19 73.97 75.06 en-ja τ BLEU 73.46 21.87 72.55 21.45 72.11 21.45 75.32 21.67 78.44 23.11 79.24 23.32 80.53 23.36 RIBES 68.25 67.66 67.44 68.47 69.86 69.78 70.89 Chunk 66.42 67.17 67.56 67.23 75.14 75.49 75.14 ja-en τ BLEU 72.99 18.34 73.01 17.78 74.21 18.18 74.06 18.18 79.14 19.54 78.79 19.89 77.80 19.35 RIBES 65.36 64.42 64.65 64.93 66.93 67.24 66.12 Table"
D12-1077,P05-1066,0,0.768146,"in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual gram"
D12-1077,W02-1001,0,0.0148034,"ions by selecting the derivation with the largest model score. From an implementation point of view, this can be done by ﬁnding the derivation that minimizes L(Dk |Fk , Ak ) − αS(Dk |Fk , w), where α is a constant small enough to ensure that the eﬀect of the loss will always be greater than the eﬀect of the score. Finally, if the model parse D˙ k has a loss that ˆ k , we is greater than that of the oracle parse D update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning. To perform this full process, given a source sentence Fk , alignment Ak , and model weights w we need to be able to eﬃciently calculate scores, calculate losses, and create parse forests for derivations Dk , the details of which will be explained in the following sections. 5.2 Scoring Derivation Trees First, we must consider how to eﬃciently assign scores S(D|F, w) to a derivation or forest during parsing. The most stand"
D12-1077,D11-1018,0,0.622387,"les the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable. As a learning framework, we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy. We propose a variety of features, and demonstrate that learning can succeed when no linguistic informati"
D12-1077,N10-1128,0,0.0575155,"ring ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero"
D12-1077,I11-1087,1,0.815752,"n, 2004). RM-train RM-test TM/LM Tune Test sent. 602 555 329k 1166 1160 word (ja) 14.5k 11.2k 6.08M 26.8k 28.5k word (en) 14.3k 10.4k 5.91M 24.3k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). except φpos and φcf g . In addition, we test systems with φpos and φcf g added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Eﬀect of Pre-ordering of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10−3 (chosen through cross-validation). We test our systems on Japanese-English and English-Japanese translation using data from the Kyot"
D12-1077,C10-1043,0,0.357752,"ure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our a"
D12-1077,P09-1104,0,0.0189689,"urthermore, we assume that the score S(D|F ) is the weighted sum of a number of feature functions deﬁned over D and F ∑ S(D|F, w) = wi φi (D, F ) i where φi is the ith feature function, and wi is its corresponding weight in weight vector w. Given this model, we must next consider how to learn the weights w. As the ﬁnal goal of our model is to produce good reorderings F 0 , it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings. 3 BTGs cannot reproduce all possible reorderings, but can handle most reorderings occurring in natural translated text (Haghighi et al., 2009). 845 Figure 2: An example of (a) the ranking function r(fj ), (b) loss according to Kendall’s τ , (c) loss according to chunk fragmentation. 4 Evaluating Reorderings Before we explain the learning algorithm, we must know how to distinguish whether the F 0 produced by the model is good or bad. This section explains how to calculate oracle reorderings, and assign each F 0 a loss and an accuracy according to how well it reproduces the oracle. 4.1 Calculating Oracle Orderings In order to calculate reordering quality, we ﬁrst deﬁne a ranking function r(fj |F, A), which indicates the relative posit"
D12-1077,D10-1092,0,0.110239,"Missing"
D12-1077,W10-1736,0,0.679774,"ing a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the"
D12-1077,D11-1017,0,0.0374907,"cketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a nonempty substring f .2 The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f 0 . For each terminal node, no reordering occurs and f is equal to f 0 . 1 The semi-supervised method of Katz-Brown et al. (2011) also optimizes reordering accuracy, but requires manually annotated parses as seed data. 2 In the original BTG framework used in translation, terminals produce a bilingual substring pair f /e, but as we are only interested in reordering the source F , we simplify the model by removing the target substring e. For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str, the reordered strings will be concatenated in order as f 0 = f 01 f 02 , and if the non-terminal symbol is inv, the reordered strings will be concat"
D12-1077,I11-1005,0,0.255321,"Missing"
D12-1077,P03-1054,0,0.00645291,"dering (chunk, τ ) and translation (BLEU, RIBES) results for each system. Bold numbers indicate no signiﬁcant diﬀerence from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004). RM-train RM-test TM/LM Tune Test sent. 602 555 329k 1166 1160 word (ja) 14.5k 11.2k 6.08M 26.8k 28.5k word (en) 14.3k 10.4k 5.91M 24.3k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). except φpos and φcf g . In addition, we test systems with φpos and φcf g added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Eﬀect of Pre-ordering of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limi"
D12-1077,N03-1017,0,0.0461607,"012. 2012 Association for Computational Linguistics Figure 1: An example with a source sentence F reordered into target order F 0 , and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering. the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for"
D12-1077,2005.iwslt-1.8,0,0.165383,"ored over the parse tree. Using this model in the pre-ordering framework results in signiﬁcant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse s"
D12-1077,P07-2045,0,0.00429164,"ranslation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F 0 and oracle F 0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ , but evaluated on the target sentence E instead of the reordered sentence F 0 . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types orig 3-step 3-step+φpos 3-step+φcf g lader lader+φpos lader+φcf g Chunk 61.22 63.51 64.28 65.76 73.19 73.97 75.06 en-ja τ BLEU 73.46 21.87 72.55 21.45 72.11 21.45 75.32 21.67 78.44 23.11 79.24 23.32 80.53 23.36 RIBES 68.25 67.66 67.44 68.47 69.86 69.78 70.89 Chunk 66.42 67.17 67.56 67.23 75.14 75.49 75.14 ja-en τ BLEU 72.99 18.34 73.01 17.78 74.21 18.18 74.06 18.18 79.14 19.54 78.79 19.89 77.80 19.35 RIBES 65.36 64.42 64.65 64.93 66.93 67.24 66.12 Table 2: Reordering (chunk, τ ) and translation (BLEU, RIB"
D12-1077,W04-3250,0,0.117398,"Missing"
D12-1077,P07-1091,0,0.20119,"translation (Figure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being si"
D12-1077,P06-1096,0,0.0852397,"Missing"
D12-1077,P11-2093,1,0.714814,"niﬁcant diﬀerence from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004). RM-train RM-test TM/LM Tune Test sent. 602 555 329k 1166 1160 word (ja) 14.5k 11.2k 6.08M 26.8k 28.5k word (en) 14.3k 10.4k 5.91M 24.3k 26.7k Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM). except φpos and φcf g . In addition, we test systems with φpos and φcf g added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging,8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse. 6.1 Eﬀect of Pre-ordering of pre-ordering: original order with F 0 ← F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3step), and the proposed model with latent derivations (lader).7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10−3 (chosen through cross-validation). We test our sy"
D12-1077,E99-1010,0,0.0604002,"term), while l and r are the leftmost and rightmost indices of the span that d covers. c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes. All features are intersected with the node label s, so each feature described below corresponds to three diﬀerent features (or two for features applicable to only non-terminal nodes). • φlex : Identities of words in positions fl , fr , fc , fc+1 , fl−1 , fr+1 , fl fr , and fc fc+1 . • φclass : Same as φlex , but with words abstracted to classes. We use the 50 classes automatically generated by Och (1999)’s method that are calculated during alignment in standard SMT systems. • φbalance : For non-terminals, features indicating whether the length of the left span 848 (c − l + 1) is lesser than, equal to, or greater than the length of the right span (r − c). • φtable : Features, bucketed by length, that indicate whether “fl . . . fr ” appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases o"
D12-1077,P02-1040,0,0.101341,"easonable burden in terms of time and memory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007). 6 Experiments Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall’s τ and chunk fragmentation (Talbot et al., 2011) comparing the system F 0 and oracle F 0 calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall’s τ , but evaluated on the target sentence E instead of the reordered sentence F 0 . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types orig 3-step 3-step+φpos 3-step+φcf g lader lader+φpos lader+φcf g Chunk 61.22 63.51 64.28 65.76 73.19 73.97 75.06 en-ja τ BLEU 73.46 21.87 72.55 21.45 72.11 21.45 75.32 21.67 78.44 23.11 79.24"
D12-1077,2007.tmi-papers.21,0,0.0861613,"works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training 844 a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent vari"
D12-1077,2011.mtsummit-papers.36,0,0.0181648,"s for systems trained to optimize chunk fragmentation (Lc ) or Kendall’s τ (Lt ). that spanned constituent boundaries (as long as the phrase frequency was high). Finally, as Section 6.2 shows in detail, the ability of lader to maximize reordering accuracy directly allows for improved reordering and translation results. It can also be seen that incorporating POS tags or parse trees improves accuracy of both lader and 3-step, particularly for EnglishJapanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al., 2011b). We also tested Moses’s implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching lader in accuracy, but with a signiﬁcant decrease in decoding speed. Further, when pre-ordering with lader and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements. 6.2 Eﬀect of Training Loss Table 3 shows results when one of three losses is optimized during training: c"
D12-1077,W11-2102,0,0.294296,"d fj2 are assigned the same rank. We can now deﬁne measures of reordering accuracy for F 0 by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F 0 where all words are in strictly ascending order, which we will call oracle orderings. 4.2 The ﬁrst measure of reordering accuracy that we will consider is Kendall’s τ (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements fj01 and fj02 of the reordered sentence, where j1 < j2 . Because j1 < j2 , fj01 comes before fj02 in the reordered sentence, the ranks should be r(fj01 ) ≤ r(fj02 ) in order to produce the correct ordering. Based on this criterion, we ﬁrst deﬁne a loss Lt (F 0 ) that will be higher for orderings that are further from the oracle. Speciﬁcally, we take the sum of all pairwise orderings that do not follow the expected order Lt (F ) = J−1 ∑ J ∑ where δ(·) is an indicator function that is 1 when its condition"
D12-1077,D09-1105,0,0.607314,"ic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training 844 a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative r"
D12-1077,D07-1080,1,0.843185,"1, which corresponds to the total number of comparisons made in calculating the loss6 Ac (F 0 ) = 1 − Lc (F 0 ) . J +1 In Figure 2 (c), Lc (F 0 ) = 3 and J + 1 = 6, so Ac (F 0 ) = 0.5. 5 Learning a BTG Parser for Reordering Now that we have a deﬁnition of loss over reorderings produced by the model, we have a clear learning objective: we would like to ﬁnd reorderings F 0 with low loss. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007). 5.1 Learning Algorithm Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, ﬁnding a derivation with high model score (the model parse) and a derivation with 6 It should be noted that for sentences of length one or sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation. 847 minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3). In order to create both of these parses eﬃciently, we ﬁrst create a parse forest"
D12-1077,J97-3002,0,0.73298,"ization, including features such as those that utilize the existence of a span in the phrase table. Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy, which proves important for achieving good translations.1 3 Training a Reordering Model with Latent Derivations In this section, we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning. 3.1 Space of Reorderings The model we present here is based on the bracketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a nonempty substring f .2 The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f 0 . For each terminal node, no reordering occurs and f is equal to f 0 . 1 The semi-supervised method of Katz-Brown et al. (2011) also optimizes reor"
D12-1077,C04-1073,0,0.932492,"sebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we present a method for inducing a parser for"
D12-1077,N09-1028,0,0.450117,"anslation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, tr"
D12-1077,P01-1067,0,0.229152,"in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods. 1 Introduction Finding the appropriate word ordering in the target language is one of the most diﬃcult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004). In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The ﬁrst author is now aﬃliated with the Nara Institute of Science and Technology. decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system. In this work, we pre"
D12-1077,W07-0401,0,0.0103136,"ss of reordering and translation (Figure 1). Reordering ﬁrst deterministically transforms F into F 0 , which contains the same words as F but is in the order of E. Translation then transforms F 0 into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required. This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are deﬁned over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima’an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve men"
D12-1077,W06-3119,0,0.0728907,"as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases of frequency one are removed. • φpos : Same as φlex , but with words abstracted to language-dependent POS tags. • φcf g : Features indicating the label of the spans fl . . . fr , fl . . . fc , and fc+1 . . . fr in a supervised parse tree, and the intersection of the three labels. When spans do not correspond to a span in the supervised parse tree, we indicate “no span” with the label “X” (Zollmann and Venugopal, 2006). Most of these features can be calculated from only a parallel corpus, but φpos requires a POS tagger and φcf g requires a full syntactic parser in the source language. As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools. 5.3 Finding Losses for Derivation Trees The above features φ and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time. However, during training, it is also necessary to ﬁnd"
D12-1077,D11-1045,0,\N,Missing
D14-1017,P07-2045,0,0.0140636,"Missing"
D14-1017,J93-2003,0,0.0941501,"tion tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 = Statistical word alignment with posterior regularization framework (xs , xt ) Z = → − → q (− y |x) xs Given a bilingual sentence x = where and xt denote a source and target sentence, respectively, the bilingual se"
D14-1017,P14-1139,0,0.0127301,"methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we d"
D14-1017,N06-1014,0,0.190871,"ika-cho, Soraku-gun, Kyoto, Japan Abstract of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each"
D14-1017,N12-1047,0,0.0375542,"Missing"
D14-1017,C04-1032,0,0.023836,"Yokohama, Japan 2 National Institute of Information and Communication Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan Abstract of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically diffe"
D14-1017,P11-1043,0,0.0127509,"to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function wo"
D14-1017,J03-1002,0,0.0214356,"K 4.91K 4.57K NTCIR10 Japanese English 2.02M 53.4M 49.4M 114K 183K 2K 73K 67.3K 4.38K 5.04K 8.6K 334K 310K 10.4K 12.7K Figure 1: Precision Recall graph in Hansard French-English Figure 2: Precision Recall graph in KFTT Figure 3: AER in Hansard French-English Figure 4: AER in KFTT 156 Table 2: Results of word alignment evaluation with the heuristics-based method (GDF) method symmetric f2f c2c f2c precision 0.4595 0.4633 0.4606 0.4630 KFTT recall AER 0.5942 48.18 0.5997 47.73 0.5964 48.02 0.5998 47.74 F 0.5182 0.5227 0.5198 0.5226 Table 3: Results of translation evaluation by AER and F-measure (Och and Ney, 2003). Since there exists no distinction for sure-possible alignments in the KFTT data, we use only sure alignment for our evaluation, both for the FrenchEnglish and the Japanese-English tasks. Table 2 summarizes our results. The baseline method is symmetric constraint (Ganchev et al., 2010) shown in Table 2. The numbers in bold and in italics indicate the best score and the second best score, respectively. The differences between f2f,f2c and baseline in KFTT are statistically significant at p &lt; 0.05 using the signtest, but in hansard corpus, there exist no significant differences between the basel"
D14-1017,P02-1040,0,0.0919616,"may be treated as content words, based on the previous work of Setiawan et al. (2007). Experiments on word alignment tasks showed better alignment qualities measured by F-measure and AER on both the Hansard task and KFTT. We also observed large gain in BLEU, 0.2 on average, when compared with the previous posterior regularization method under NTCIR10 task. As our future work, we will investigate more precise methods for deciding function words and content words for better alignment and translation qualities. Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May,"
D14-1017,P08-1112,0,0.0467484,"Missing"
D14-1017,P07-1090,0,0.373945,"t the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we differentiate between content words and function words by frequency in bilingual data, following Setiawan et al. (2007). Experimental results show that the proposed methods achieved better alignment qualities on the French-English Hansard data and the JapaneseEnglish Kyoto free translation task (KFTT) measured by AER and F-measure. In translation evaluations, we achieved statistically significant gains Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agre"
D14-1017,C96-2141,0,0.299019,"ffectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 = Statistical word alignment with posterior regularization framework (xs , xt ) Z = → − → q (− y |x) xs Given a bilingual sentence x = where and xt denote a source and target sentence, respectively, the bilingual sentence is aligned by a manyto"
D14-1017,D11-1125,0,0.0492598,"Missing"
D14-1017,N03-1017,0,0.0362618,"ith each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Languag"
D14-1017,E12-1045,0,\N,Missing
D14-1019,J92-4003,0,0.555798,"do move label X tentatively to cluster K compute F (C) for this exchange move label X to cluster with maximum F (C) do until the cluster mapping does not change 3.1 Optimization Criterion The generative probability in each rule of the form of Equation (6) can be approximated by clustering nonterminal symbols as follows: p(Y, Z|X) ≈ p(Y |c(Y )) · p(Z|c(Z)) ·p(c(Y ), c(Z)|c(X)) (7) Table 1: Outline of syntax-label clustering method where we map a syntax label X to its equivalence cluster c(X). This can be regarded as the clustering criterion usually used in a class-based n-gram language model (Brown et al., 1992). If each label on the right-hand side of a synchronous rule (4) is independent of each other, we can factor the joint model as follows: where C denotes all clusters and N denotes all syntax labels. For Equation (11), the last summation is equivalent to the sum of the occurrences of all syntax labels, and canceled out by the first summation. K in the third summation considers clusters in a synchronous rule whose left-hand side label is X, and we let ch(X) denote a set of those clusters. The second summation equals ∑ K∈C N (K) · log N (K). As a result, Equation (11) simplifies to p(Y, Z|X) ≈ p("
D14-1019,P07-2045,0,0.00610793,"and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU score and the rule number for We did experiments with the SAMT (Zollmann and Venugopal, 2006) model with the Moses (Koehn et al., 2007). For the SAMT model, we conducted experiments with two label sets. One is extracted from the phrase structure parses and the other is extended with CCG4 . We applied the proposed method (+clustering) and the baseline method (+coarsening), which uses the Hanneman 3 LDC2003E14 Using the relax-parse with option SAMT 4 for IWSLT07 and FBIS and SAMT 2 for NIST08 in the Moses 4 168 each cluster number when using the IWSLT07 dataset. Since the +clustering maximizes the likelihood of synchronous rules, it can introduce appropriate rules adapted to training data given a fixed number of clusters. For e"
D14-1019,N12-1047,0,0.015901,"n zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis 4.2 Experiment design Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU score and the rule number for We did experiments with the SAM"
D14-1019,J07-2003,0,0.806338,"on (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is a set of synchronous rules. Each synchronous rule in R takes the form X → ⟨α, β, ∼⟩ 165 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 165–171, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics The clustering of Hanneman and Lavie proved successful in decreasing"
D14-1019,P06-1077,0,0.0160689,"rectly maximizing the likelihood of synchronous rules, whereas previous work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonte"
D14-1019,P10-1146,0,0.0858067,"distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is a set of synchronous rules."
D14-1019,P11-1065,0,0.0387772,"Missing"
D14-1019,P06-1121,0,0.0544737,"evious work considered only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targe"
D14-1019,P11-2093,0,0.0346993,"tuning set has seven English references and the test set has six English references. For zh-en data we prepared two kind of data. The one is extracted from FBIS3 , which is a collection of news articles. The other is 1 M sentences extracted rondomly from NIST Open MT 2008 task (NIST08). We use the NIST Open MT 2006 for tuning and the MT 2003 for testing. The tuning and test sets have four English references. Table 2 shows the details for each corpus. Each corpus is tokenized, put in lower-case, and sentences with over 40 tokens on either side are removed from the training data. We use KyTea (Neubig et al., 2011) to tokenize the Japanese data and Stanford Word Segmenter (Tseng et al., 2005) to tokenize the Chinese data. We parse the English data with the Berkeley parser (Petrov and Klein, 2007). Label Rule F(C) SD 70 5,460 80 80 2.1 M 60 M 32 M 38 M -1.5 e+10 -7.9 e+09 526 154 70 7,328 80 12 M 120 M 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The lang"
D14-1019,W08-0509,0,0.0184457,"M 32 M 38 M -1.5 e+10 -7.9 e+09 526 154 70 7,328 80 12 M 120 M 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis 4.2 Experiment design Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation tim"
D14-1019,P02-1040,0,0.088965,"12 M 120 M 100 M -2.6 e+10 218 Table 4: SAMT grammars on zh-en experiments label-collapsing algorithm described in Section 2, for syntax-label clustering to the SAMT models with CCG. The number of clusters for each clustering was set to 80. The language models were built using SRILM Toolkits (Stolcke, 2002). The language model with the IWSLT07 is a 5-gram model trained on the training data, and the language model with the FBIS and NIST08 is a 5gram model trained on the Xinhua portion of English GigaWord. For word alignments, we used MGIZA++ (Gao and Vogel, 2008). To tune the weights for BLEU (Papineni et al., 2002), we used the n-best batch MIRA (Cherry and Foster, 2012). 5 Results and analysis 4.2 Experiment design Tables 3 and 4 present the details of SAMT grammars with each label set learned by the experiments using the IWSLT07 (ja-en), FBIS and NIST08 (zh-en), which include the number of syntax labels and synchronous rules, the values of the objective (F (C)), and the standard deviation (SD) of the number of labels assigned to each cluster. For NIST08 we applied only the + clustering because the + coarsening needs a huge amount of computation time. Table 5 shows the differences between the BLEU scor"
D14-1019,N13-1029,0,0.643228,"ho, Soraku-gun, Kyoto, JAPAN {hideya.mino, taro.watanabe, eiichiro.sumita}@nict.go.jp Abstract which relies on two nonterminal labels. One problem in adding syntax labels to Hiero-style rules is that only partial phrases are assigned labels. It is common practice to extend labels by using the idea of combinatory categorial grammar (CCG) (Steedman, 2000) on the problem. Although this extended syntactical information may improve the coverage of rules and syntactic correctness in translation, the increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic distributions of labels in synchronous rules and showed that performance improved. In the present work, we follow the idea of labelset coarsening and propose a new method to group syntax labels. First, as an optimization criterion, we use the logarithm of the likelihood of synchronous rules instead of the similarity of probabilistic distributions of syntax labels. Second, we use exchange clustering (Uszkoreit and Brants, 2008), which is faster than the agglomerativeclustering algorithm used in the previous work. We tested our"
D14-1019,N06-1031,0,0.0258994,"d only the similarity of probabilistic distributions of labels. We tested the proposed method on Japanese-English and Chinese-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is"
D14-1019,N03-1017,0,0.0178391,"Missing"
D14-1019,P08-1086,0,0.0580322,"he increased grammar size causes serious speed and data-sparseness problems. To address these problems, Hanneman and Lavie (2013) coarsen syntactic labels using the similarity of the probabilistic distributions of labels in synchronous rules and showed that performance improved. In the present work, we follow the idea of labelset coarsening and propose a new method to group syntax labels. First, as an optimization criterion, we use the logarithm of the likelihood of synchronous rules instead of the similarity of probabilistic distributions of syntax labels. Second, we use exchange clustering (Uszkoreit and Brants, 2008), which is faster than the agglomerativeclustering algorithm used in the previous work. We tested our proposed method on JapaneseEnglish and Chinese-English translation tasks and observed gains comparable to those of previous work with similar reductions in grammar size. Recently, syntactic information has helped significantly to improve statistical machine translation. However, the use of syntactic information may have a negative impact on the speed of translation because of the large number of rules, especially when syntax labels are projected from a parser in syntax-augmented machine transl"
D14-1019,W06-3119,0,0.604595,"se-English translation tasks and found order-of-magnitude higher clustering speeds for reducing labels and gains in translation quality compared with previous clustering method. 1 Introduction In recent years, statistical machine translation (SMT) models that use syntactic information have received significant research attention. These models use syntactic information on the source side (Liu et al., 2006; Mylonakis and Sima’an, 2011), the target side (Galley et al., 2006; Huang and Knight, 2006) or both sides (Chiang, 2010; Hanneman and Lavie, 2013) produce syntactically correct translations. Zollmann and Venugopal (2006) proposed syntax-augmented MT (SAMT), which is a MT system that uses syntax labels of a parser. The SAMT grammar directly encodes syntactic information into the synchronous contextfree grammar (SCFG) of Hiero (Chiang, 2007), 2 Syntax-Augmented Machine Translation SAMT is an instance of SCFG G, which can be formally defined as G = (N , S, Tσ , Tτ , R) where N is a set of nonterminals, S ∈ N is a start label, Tσ and Tτ are the source- and targetside terminals, and R is a set of synchronous rules. Each synchronous rule in R takes the form X → ⟨α, β, ∼⟩ 165 Proceedings of the 2014 Conference on Em"
D14-1019,N07-1051,0,\N,Missing
D15-1143,N10-1028,0,0.0896146,"nsduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to p"
D15-1143,P09-1088,0,0.107802,"a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model o"
D15-1143,W10-1703,0,0.0126229,"ions, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as additional training data, and increment the corresponding customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model. 6 Experiments 6.1 Comparison with Previous Bayesian Model First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary corpus for German/Spanish/French-to-English pairs (Callison-Burch et al., 2010) and NTCIR10 corpus for Japanese-English (Goto et al., 2013) for the translation model. All sentences are lowercased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) w"
D15-1143,J14-1007,0,0.0220988,"Missing"
D15-1143,P11-2031,0,0.0259328,"here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that the gains were achieved with the comparable grammar size. When comparing German-English and Japanese-English language pairs, there are no significant differences between Back and Gen. The combination of our Back with future score during slice sampling (+future) achieved further gains over the slice sampling without future scores, and slightly decrese the grammar size, compared to Back. However, there are still no significant difference between Back+future and Gen on German-Eng"
D15-1143,C10-2021,0,0.0174855,"ing et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the"
D15-1143,P13-1077,0,0.0150821,"lating the detailed balance, its time complexity of O(|f |3 |e|3 ) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let Σ denote a set of terminal symbols in the source langua"
D15-1143,W06-3105,0,0.425911,"ecreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A"
D15-1143,D08-1033,0,0.045527,"Missing"
D15-1143,P10-4002,0,0.0242682,"or those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to s"
D15-1143,D11-1125,0,0.0169852,"65.5k fr-en 1.54M 1.83M 55.6M 65.5k 72.5k 61.9k 70.5k ja-en 1.80M 2.03M 27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better t"
D15-1143,D07-1103,0,0.194885,"basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu,"
D15-1143,N03-1017,0,0.199013,"ilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to score every phrase pair created during decoding with ba"
D15-1143,P07-2045,0,0.0145853,"Missing"
D15-1143,D12-1021,0,0.0497587,"Missing"
D15-1143,D12-1088,0,0.038562,"Missing"
D15-1143,W02-1018,0,0.0414167,"ntly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom"
D15-1143,P11-1064,1,0.480702,"s sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sam"
D15-1143,J03-1002,0,0.0330693,"Missing"
D15-1143,P03-1021,0,0.0510498,"heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pairs that are selected at least once in the sample. During this extraction process, we limit the source or target terminal symbol size of phrase pairs to 5. For each extracted rule or phase pair, we compute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple features is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data (Och, 2003). We use the following six features; the joint model probability Pmodel is calculated by Equation (2) for rules and by Equation (5) for phrase pairs. The joint posterior probability Pposterior (f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count estimation, motivated by Neubig et al. (2011) 4 . The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probabil"
D15-1143,C12-1176,0,0.200354,"granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Euro"
D15-1143,D12-1089,0,0.0958848,"a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg"
D15-1143,N13-1038,0,0.0306641,"tion. If the Score(rspi ) is less than usp , we prune the rspi from cube. Similar to Blunsom and Cohn (2010), if the span sp is not in the current derivation, the rules with low probability are pruned acd denotes a rule in cording to Equation (14). Let rsp d with span sp, P (d|u) is calculated by: sp∈d d ) P (rsp ∑ rj ∈rsp P (rj )I(usp &lt; Score(rj )) |φ | p| ∏ [θp ]dpp ∏ |φ (14) P (usp |d) = Beta(usp ; a, 1.0), ∏ pruning is conducted against the score denoted by the equation 10 , which is very similar to Xiao et al. (2012).3 For faster bi-parsing, we run sampling in parallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter resampling in a hierarchical Pitman-Yor process (Teh, 2006). In particular, we resample ⟨dp , θp"
D15-1143,P02-1040,0,0.0940086,"27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that"
D15-1143,D14-1180,0,0.0144786,"ion. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f |3 |e|3 ) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xia"
D15-1143,P06-1124,0,0.367232,"2: Derivation tree generated from the hierarchical back-off model we reach phrase pairs which are generated without any back-offs. Let a discount parameter be dp , a strength parameter be θp , and a base measure be Gp0 . More formally, the generative process is represented as follows: GX ∼ Prule (dr , θr , Gphrase ), Gphrase ∼ Pphrase (dp , θp , GX ), X → ⟨s/t⟩ ∼ Gphrase , X → ⟨α/β⟩ ∼ GX , (4) where s is source side terminals and t is target side terminals in phrase pair ⟨s/t⟩. Pphrase is composed of three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor process (Teh, 2006). model: We draw a phrase pair ⟨s/t⟩ with the probability similar to Equation (2): ck − dp · |φpk | , θ p + np (5) where ck is the numbers of customers of a phrase pair pk and np is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation (2). back-off: We will back off to smaller phrases using a rule generated by Prule as follows: θp + dp · |φp | cback + γb · Gb · θp + np cback + cbase + γb ·Prule (dr , θr , Gphrase ) ∏ · Pphrase (dp , θp , GX ), X∈⟨α/β⟩ base: As an alternative to the back-off state, we may rea"
D15-1143,J97-3002,0,0.677402,"007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) m"
D15-1143,J07-2003,0,\N,Missing
D15-1209,W13-2201,0,0.0199167,"ing the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus)1 with it’s humanannotated word alignment. The Chinese word “HE ZHANG,” denoted wr , which means river custodian, only occurs once in the whole corpus. We performe"
D15-1209,H93-1039,0,0.474031,"Missing"
D15-1209,J93-2003,0,0.167506,"and BLEU scores of end-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects"
D15-1209,2013.iwslt-evaluation.1,0,0.0234888,"Missing"
D15-1209,P07-1003,0,0.0292356,"essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted"
D15-1209,D08-1033,0,0.0481387,"Missing"
D15-1209,N13-1073,0,0.0288713,"st statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish"
D15-1209,P08-1112,0,0.0529177,"Missing"
D15-1209,P07-2045,0,0.00687099,"nd converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Wat"
D15-1209,2005.mtsummit-papers.11,0,0.0582682,"GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings"
D15-1209,N06-1014,0,0.168812,"e pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a rea"
D15-1209,P11-1064,1,0.897734,"Missing"
D15-1209,C00-2163,0,0.833427,"nd-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a"
D15-1209,J03-1002,0,0.26461,"d word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Gra"
D15-1209,P03-1021,0,0.015174,"d was tested on two language pairs: Chinese-English and JapaneseEnglish (Table 2). Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks. GIZA++ and our own implementation of standard EM were used as baselines. 4.1 Experimental Settings The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server i"
D15-1209,C02-1145,0,0.0241296,"nMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We imp"
D15-1209,P14-1072,0,0.0607884,"ing phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)"
D15-1209,P13-1083,1,0.86032,"el 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1 (Och and Ney, 2003). T"
D15-1209,J10-3007,0,0.035402,"Missing"
D15-1209,P12-1033,0,0.0623327,"r et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)"
D15-1209,P14-2122,1,0.835056,"ly based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)                    (c) Figure 1: Examples of supervised word alignment. (a) gold alignment; (b) standard EM (GIZA++); (c) Leave-one-out alignment (proposed). 2011), also attempt to address the issue of overfitting, however EM algorithms related to the proposed method have been shown to be more efficient (Wang et al., 2014). 3 Methodology This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation. The main notation used in this section is shown in Table 1. 3.1 Standard EM for IBM Models 1, 2 and HMM Model To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data. In order to maximize the likelihood of the alignment model θ given the data S, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney,"
D15-1209,P11-1125,1,0.848372,"settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision"
D15-1209,N12-1026,1,0.860338,"07). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1"
D15-1209,P10-1049,0,0.0224644,"e propose a leave-one-out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of proba"
D15-1209,W12-3158,0,0.0196466,"out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al.,"
E03-1048,P02-1040,0,0.0780651,"ere an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the RED rank, and the BLEU score. While RED accords with HUMAN, BLEU fails to agree with HUMAN in the EJ evaluation. One reason for this is that the BLEU score favors SAT translations in that they are more similar to the reference translation from the viewpoint of Ngrams. Table 1 Quality Evaluation of Three MTs 5 (2) BLEU score: The MT translations are scored based on the precision of N-grams in an entire set of multiple reference translations (Papineni et al., 2002). It ranges from 1.0 (best) down to 0.0 (worst). (3) Estimated TOEIC score: It is important to interpret MT performance from the viewpoint of a language proficiency test such as TOEIC 4 . A translator compared MT translations with human ones, then, MT&apos;s proficiency is estimated by regression analysis (Sugaya et al., 2000). It ranges from 10 (lowest) to 990 points (perfect). 3.3 Results Table 1 wraps up the results. So far, SMT has been applied mainly to language pairs of similar European languages. Skeptical opinions dominate about Average is calculated: A, B, C, and D are assigned values of 4"
E03-1048,2001.mtsummit-papers.3,1,0.832851,"e and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Verbmobil experiment (Ney, 2001) where an SMT system scored highest. We are studying these interesting contradictory observations. Let&apos;s consider the relationships among the HUMAN rank, the"
E03-1048,shimohata-sumita-2002-automatic,1,0.89347,"Missing"
E03-1048,C02-1076,1,0.819515,"ging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the source and target correspondences from the semantic point of view are maintained in a state-of-the-art translation system. However, the second assumption does not necessarily hold. To solve this problem, Akiba et al. (2002) used not only a language model but also a translation model of SMT derived from a corpus, and Sumita et al. (2002) exploited a corpus whose sentences are converted into semantic class sequences. These two selectors outperformed conventional selectors using the target N-gram in our experiments. 5 Paraphrasing and Filtering This section introduces another feature of C3 : paraphrasing and filtering corpora. The large variety of possible translations in a corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for"
E03-1048,1995.tmi-1.17,0,0.00827994,"w the HUMAN rank, as described above. Table 2. Sample of Translation Variety [B] Is the payment cash? Or is it the credit card? [A] Would you like to pay in cash or with a credit card? [C] Could you cash or credit card? In our experiment, while D3 , HPAT, and SAT for the E-to-J direction have A-ratios of 0.62, 0.55, and 0.53, respectively, the ideal selection would have an interestingly high A-ratio of 0.79. Thus, we could obtain a large increase in accuracy if it were possible to select the best one of the three different translations for each input sentence. Unlike other approaches such as (Brown and Frederking, 1995), we do not merge multiple results into a single one but we select the best one because the large difference between multiple translations for distant language pairs such as Japanese and English makes merging infeasible. Methods using N-gram statistics of a target language corpus have been proposed before (Brown and Frederking, 1995; Callison-Burch et al., 2001). They are based on the assumptions that (1) the naturalness of the translations is effective for selecting good translations because they are sensitive to the broken target sentences due to errors in translation processes, and (2) the"
E03-1048,2001.mtsummit-papers.12,0,0.0767102,"Missing"
E03-1048,W02-1611,1,0.895885,"Missing"
E03-1048,suyaga-etal-2002-proposal,0,0.0182927,"or example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the coun"
E03-1048,2002.tmi-tutorials.2,0,0.0456358,"corpus causes difficulty in building machine translation on the corpus. For example, the variety makes it harder to estimate the parameters for SAT, to find appropriate translation examples for D3 , to extract good transfer patterns for HPAT. We propose ways to overcome these problems by paraphrasing corpora through automated processes or filtering corpora by abandoning inappropriate expressions. Two methods have been investigated for automatic paraphrasing. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation"
E03-1048,W01-1401,1,0.84948,"stical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phra"
E03-1048,2002.tmi-papers.9,1,0.841258,"slation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual trees, transfer patterns are generated. According to the patterns, the source phrase structure is obtained and converted to generate target sentences (Imamura 2002) SAT (Word-based SMT): Watanabe et al. (2002b) implemented SAT dealing with Japanese and English on top of a word-based SMT framework (Brown et al. 1993). 3 Competition on the Same Corpus 3.1 Resources In our competitive evaluation of the MT systems, we used the BTEC corpus &apos;, which is a collection of Japanese sentences and their English translations typically found in phrasebooks for tourists. The size is about 150 thousand sentence pairs. A quality evaluation was done using a test set consisting of 345 sentences selected randomly from the above corpus, and the remaining sentences were used f"
E03-1048,1983.tc-1.13,0,0.350529,"Missing"
E03-1048,E03-1029,1,0.834211,"g. (1) Shimohata et al. (2002a) group sentences by the equivalence of the translation and extract rules of paraphrasing by DPmatching. (2) Finch et al. (2002) cluster sentences in a handcrafted paraphrase corpus (Sugaya et al., 2002) to obtain pairs that are similar to each other for training SMT models, then by using the models the decoder generates a paraphrase. The experimental results indicate that (i) the EBMT based on normalization had increased coverage (Shimohata et al., 2002b) and (ii) the SMT created on the normalized sentences had a reduced word-error-rate (Watanabe et al., 2002a). Imamura et al. (2003) proposed a calculation that measures the literalness of a translation pair and called it TCR. After the word alignment of a translation pair, TCR is calculated as the rate of the aligned word count over the count of words in the translation pair. After abandoning the non-literal parts of the corpus, the acquisition of HPAT transfer patterns is done. The effect has been confirmed by an improvement in translation quality. 6 Conclusion Our project, called C3 , places corpora at the center of speech-to-speech technology. Good performance in translation components is demonstrated in the experiment"
E03-1048,1999.mtsummit-1.34,1,0.836105,"ezawa et al., 2002). 171 We used bilingual dictionaries and thesauri of about fifty thousand words for the travel domain. 3.2 Evaluation Measures We used the measures below. The BLEU score and the RED rank are measured by referring to the test corpus, i.e., a set of input sentences and their multiple reference translations; the HUMAN rank and the estimated TOEIC score are judged by bilingual translators. (1) Average of Ranks 2 : HUMAN rank: In our evaluation, 9 translators who are native speakers of the target language ranked the MT translations into 4 ranks: A, B, C, and D, from good to bad (Sumita et al., 1999). 3 RED rank: An automatic ranker is learned as a decision tree from HUMAN-ranked examples. It exploits edit-distances between MT and multiple reference translations (Akiba et al., 2001). the effectiveness or applicability of SMT to dissimilar language pairs. However, we implemented SMT for translation between Japanese and English. They are dissimilar in many points, such as word order and lexical systems. We found that SAT, which is an SMT, worked in both J-to-E and Eto-J directions. The EBMT systems, HPAT and D 3 , surpassed SAT in the HUMAN rank. This is the reverse result obtained in a Ver"
E03-1048,W01-1405,0,0.048248,"g a high-quality translation subsystem for a speechto-speech translation system. This paper introduces recent progress in C3 . Sections 2 and 3 demonstrate a competition between multiple machine translation systems developed in our project, and Sections 4 and 5 explain the features that differentiate our project from other corpus-based projects. 2 Three Corpus-based MT Systems There are two main strategies in corpus-based machine translation: (i) Example-Based Machine Translation (EBMT; Nagao, 1984; Somers, 1999) and (ii) Statistical Machine Translation (SMT; Brown et al., 1993; Knight, 1997; Ney, 2001; Alshawi et al., 2000). C3 is developing both technologies in parallel and blending them. In this paper, we introduce three different machine translation systems: Di, HPAT, and SAT. The three MT systems are characterized by different translation units. D3 , HPAT, and SAT use sentences, phrases, and words, respectively. D3 (Sentence-based EBMT): It retrieves the most similar example by DP-matching of the input and example sentences and adjusts the gap between the input and the retrieved example by using dictionaries. (Sumita 2001) HPAT (Phrase-based EBMT): Based on phrasealigned bilingual tree"
E03-1048,C02-1050,1,0.897796,"Missing"
E03-1048,J90-2002,0,\N,Missing
E03-1048,takezawa-etal-2002-toward,1,\N,Missing
I05-1042,N03-1017,0,0.0172003,"irs and that the relative positional features in a word group are useful for reordering the local word orders. Moreover, the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model. 1 Introduction For decades, many research eﬀorts have contributed to the advance of statistical machine translation. Such an approach to machine translation has proven successful in various comparative evaluations. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most statistical machine translation systems still consider surface forms and rarely use linguistic knowledge about the structure of the languages involved[8]. In this paper, we address the question of the eﬀectiveness of morpho-syntactic features such as parts-of-speech, base forms, and relative positions in a chunk or an agglutinated word for improving the quality of statistical machine translations. Basically, we take a statistical machine translation model based on an IBM model that consists of a language model and a separate translation mo"
I05-1042,W99-0604,0,0.0338372,"irs and that the relative positional features in a word group are useful for reordering the local word orders. Moreover, the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model. 1 Introduction For decades, many research eﬀorts have contributed to the advance of statistical machine translation. Such an approach to machine translation has proven successful in various comparative evaluations. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most statistical machine translation systems still consider surface forms and rarely use linguistic knowledge about the structure of the languages involved[8]. In this paper, we address the question of the eﬀectiveness of morpho-syntactic features such as parts-of-speech, base forms, and relative positions in a chunk or an agglutinated word for improving the quality of statistical machine translations. Basically, we take a statistical machine translation model based on an IBM model that consists of a language model and a separate translation mo"
I05-1042,P00-1056,0,0.0149711,"irs and that the relative positional features in a word group are useful for reordering the local word orders. Moreover, the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model. 1 Introduction For decades, many research eﬀorts have contributed to the advance of statistical machine translation. Such an approach to machine translation has proven successful in various comparative evaluations. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most statistical machine translation systems still consider surface forms and rarely use linguistic knowledge about the structure of the languages involved[8]. In this paper, we address the question of the eﬀectiveness of morpho-syntactic features such as parts-of-speech, base forms, and relative positions in a chunk or an agglutinated word for improving the quality of statistical machine translations. Basically, we take a statistical machine translation model based on an IBM model that consists of a language model and a separate translation mo"
I05-1042,N04-1033,0,0.0129812,"irs and that the relative positional features in a word group are useful for reordering the local word orders. Moreover, the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model. 1 Introduction For decades, many research eﬀorts have contributed to the advance of statistical machine translation. Such an approach to machine translation has proven successful in various comparative evaluations. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most statistical machine translation systems still consider surface forms and rarely use linguistic knowledge about the structure of the languages involved[8]. In this paper, we address the question of the eﬀectiveness of morpho-syntactic features such as parts-of-speech, base forms, and relative positions in a chunk or an agglutinated word for improving the quality of statistical machine translations. Basically, we take a statistical machine translation model based on an IBM model that consists of a language model and a separate translation mo"
I05-1042,J93-2003,0,0.00588121,"using morpho-syntactic information [6,8]. But most statistical machine translation systems still consider surface forms and rarely use linguistic knowledge about the structure of the languages involved[8]. In this paper, we address the question of the eﬀectiveness of morpho-syntactic features such as parts-of-speech, base forms, and relative positions in a chunk or an agglutinated word for improving the quality of statistical machine translations. Basically, we take a statistical machine translation model based on an IBM model that consists of a language model and a separate translation model [5]: eI1 = argmaxeI1 P r(f1J |eI1 )P r(eI1 ) (1) The translation model links the source language sentence to the target language sentence. The target language model describes the well-formedness of the target language sentence. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 474–485, 2005. c Springer-Verlag Berlin Heidelberg 2005  Empirical Study of Utilizing Morph-Syntactic Information in SMT 475 One of the main problems in statistical machine translation is to learn the less ambiguous correspondences between the words in the source and target languages from the bilingual training data. When"
I05-1042,W02-1021,0,0.0288734,"Missing"
I05-1042,J04-2003,0,0.113877,"a word group are useful for reordering the local word orders. Moreover, the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model. 1 Introduction For decades, many research eﬀorts have contributed to the advance of statistical machine translation. Such an approach to machine translation has proven successful in various comparative evaluations. Recently, various works have improved the quality of statistical machine translation systems by using phrase translation [1,2,3,4] or using morpho-syntactic information [6,8]. But most statistical machine translation systems still consider surface forms and rarely use linguistic knowledge about the structure of the languages involved[8]. In this paper, we address the question of the eﬀectiveness of morpho-syntactic features such as parts-of-speech, base forms, and relative positions in a chunk or an agglutinated word for improving the quality of statistical machine translations. Basically, we take a statistical machine translation model based on an IBM model that consists of a language model and a separate translation model [5]: eI1 = argmaxeI1 P r(f1J |eI1 )P r(e"
I05-1042,niessen-etal-2000-evaluation,0,0.0244044,"Missing"
I05-1042,2003.mtsummit-papers.54,1,0.839728,"Missing"
I05-1042,P02-1038,0,0.077426,"Missing"
I05-1042,P02-1040,0,0.0705084,"Missing"
I05-1042,takezawa-etal-2002-toward,0,0.0204597,"Missing"
I05-1042,C00-2163,0,\N,Missing
I05-1042,J92-4003,0,\N,Missing
I13-1032,D08-1024,0,0.179727,"grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in previous works. One of"
I13-1032,P05-1033,0,0.0461075,"n is attributed to the Eq.5 in (Zhong and Kwok, 2011). pairs. Test sets 2003, 2004 and 2008 are used as the development set, development test (devtest) set and test set, respectively; and all of them contain 16 references. A 5-gram language model is trained on the training data with the SRILM toolkit, and word alignment is obtained with GIZA++. In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline decoder, and we use the state of the art tuning methods MERT and PRO as our comparison methods4 . Based on our in-house decoder, we implement three translation models with different feature sets: default features (default); default features plus rule id features (+id) ; and default features plus group features of rule id (+group). On the IWSLT training data, the number of rule id features is 500K, i.e. d = 500K, which is significantly greater than the number of bilingual sentences 30K. Our proposed tuning method is with the following setting by tuning on the dev-test set: λ1 ="
I13-1032,D11-1125,0,0.0685367,"pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so"
I13-1032,W04-3250,0,0.0161007,"on tasks, whose training data consists of about 30K bilingual sentence 3 The reason is attributed to the Eq.5 in (Zhong and Kwok, 2011). pairs. Test sets 2003, 2004 and 2008 are used as the development set, development test (devtest) set and test set, respectively; and all of them contain 16 references. A 5-gram language model is trained on the training data with the SRILM toolkit, and word alignment is obtained with GIZA++. In our experiments, the translation performances are measured by the case-insensitive BLEU4 metric. The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) as our baseline decoder, and we use the state of the art tuning methods MERT and PRO as our comparison methods4 . Based on our in-house decoder, we implement three translation models with different feature sets: default features (default); default features plus rule id features (+id) ; and default features plus group features of rule id (+group). On the IWSLT training data, the number of rule id features is 500K, i.e. d = 500K, which is significantly greater than the number of bilingual sentences 30K. Our propo"
I13-1032,C10-1075,0,0.259742,"er-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT translation task, as will be shown in our experiments later. Therefore, enlarging a tuning set is not always a sufficient solution for robust tuning, since it would be impractical to create a large scale tuning set with these requirements. We propose a novel tuning method by grouping a large number of features to leverage the above pitfalls. Instead of directly taking the large number of atomic features into translation model, we firstly learn their group structure on the training"
I13-1032,P02-1038,0,0.1062,"a result, we face an over-fitting problem, which limits the generalization abilities of the learned models. Based on our analysis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done whi"
I13-1032,P03-1021,0,0.104864,"sis, we propose a novel method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, wh"
I13-1032,P02-1040,0,0.0863901,"atomic features into translation model, we firstly learn their group structure on the training data to alleviate their serious sparsity. Then, we tune the translation model consisting of grouped features on a multi-reference development set to ensure robust tuning. Unlike unsupervised clustering methods such as k-means (MacQueen, 1967) for feature clustering, we group the features with the OSCAR (Octagonal Shrinkage and Clustering Algorithm for Regression) method (Bondell and Reich, 2008), which directly relates the objective of feature grouping to translation evaluation metrics such as BLEU (Papineni et al., 2002) and thus grouped features are optimized with respect to BLEU. Due to the large number of features and large number of training examples, efficient grouping is not simple. We apply the online gradient projection method under the FOBOS (forwardbackward splitting) framework (Duchi and Singer, 2009) to accelerate feature grouping. We employ a large number of features by treating each translation rule in a synchronous-CFG as a single feature. Experiments on IWSLT Chineseto-English translation tasks show that, with the help of grouping these features, our method can overcome the above pitfalls and"
I13-1032,P12-1002,0,0.0603095,"pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile. As a result, the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT translation task, as will be sh"
I13-1032,P09-1054,0,0.0310359,"t features setting. Its main reason, as presented in Section 1, may be that multiple references and closeness5 of tuning sets are much helpful for translation tasks. Further, the id features do not achieve improvements and even decreases 0.9 BLEU scores when tuned on the development set, due to its serious sparsity. However, after grouping id features, the groups learned by our method can alleviate the feature sparsity and thus significantly obtain gains of 0.7 BLEU scores over default feature setting. Further, we implement another tuning method6 for comparison, i.e. L1 regularization method (Tsuruoka et al., 2009) based on the ranking loss L(W ) defined in Eq.1. We tune the translation 4 Both of them are derived from the Moses toolkit: http://www.statmt.org/moses/. 5 If the tuning set and test set are close enough or identically distributed, it is possible to get gains by sparse discriminative features without using feature grouping(Chiang et al., 2009). 6 It is similar to dtrain implemented in the cdec toolkit: http://cdec-decoder.org/, except that it does not use the distributed learning framework. Methods Tuning set Feature set MERT PRO PRO PRO L1 L1 OSCAR dev dev train dev train dev – default defau"
I13-1032,D07-1080,1,0.803531,"method based on feature grouping via OSCAR to overcome these pitfalls. Our feature grouping is implemented within an online learning framework and thus it is efficient for a large scale (both for features and examples) of learning in our scenario. Experiment results on IWSLT translation tasks show that the proposed method significantly outperforms the state of the art tuning methods. 1 Introduction Since the introduction of log-linear based SMT (Och and Ney, 2002), tuning has been a hot topic. Various methods have been explored: their objectives are either error rates (Och, 2003), hinge loss (Watanabe et al., 2007; Chiang et al., 2008) or ranking loss (Hopkins and May, 2011), and they are either batch training or online training methods. In this paper, we consider tuning translation models with a large number of features such as lexical, n-gram level and rule level features, where the number of features is largely greater than the number of bilingual sentences. Practically, existing tuning methods such as PRO and MIRA might This joint work was done while the first author visited NICT. be applied in our scenario, however, they will suffer from some pitfalls as well, which have been less investigated in"
I13-1032,D11-1081,0,0.367302,"ll suffer from some pitfalls as well, which have been less investigated in previous works. One of pitfalls is that these features are so sparse that many features which are potentially useful for a test set may not be included in a given tuning set, and many useless features for testing will be over tuned on the developement set meanwhile. As a result, the generalization abilities of features are limited due to the mismatch between the testing data and the tuning data, and over-fitting occurs. One practice is to tune translation models on a larger tuning set, such as the entire training data (Xiao et al., 2011; Simianer et al., 2012), in the hope that more features would be included during tuning. However, tuning robust weights for translation models has additional requirements to a tuning set. Firstly, multiple reference translations in the tuning data are helpful for better tuning, especially when testing data contains multiple reference translations. Secondly, the closeness between the tuning set and a test set is also important for better testing performance (Li et al., 2010). These requirements can explain why tuning on the training data leads to unsatisfactory performance on the IWSLT transla"
I13-1032,N09-1025,0,\N,Missing
J16-1001,W05-0909,0,0.00337933,"l accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures. We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defined as the geometric mean of n-gram precisions (usually for n from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly high evaluation scores. For a single reference sentence e and a corresponding system output eˆ , we can define cn (ˆe ) as the"
J16-1001,N12-1062,0,0.0483162,"Missing"
J16-1001,J96-1002,0,0.0773772,"Missing"
J16-1001,P08-1024,0,0.0799565,"Missing"
J16-1001,D08-1023,0,0.393701,"ords e, and fires a feature for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two can be calculated from source and local target context, but target bigrams require target bigram context and are thus non-local features. One final variety of features that has proven useful is syntax-based features (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and hierarchical phrase-based translations do not directly consider syntax (in the linguistic sense) in the construction of the models, so introducing this information in the form of features has a potential for benefit. One way to introduce this information is to parse the input sentence before translation, and use the information in the parse tree in the calculation of features. For example, we can count the number of times a phrase or translation rule matches, or partially matches (Marton and Resnik 2008), a span with a particular label"
J16-1001,J93-2003,0,0.0683852,"Missing"
J16-1001,W11-2103,0,0.0597144,"Missing"
J16-1001,W08-0304,0,0.0693471,"Missing"
J16-1001,N10-1080,0,0.0554388,"Missing"
J16-1001,D10-1059,0,0.0220325,"ce (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT over these sorts of packed data structures by observing the fact that the envelopes used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), allowing for exact calculation of the full envelope for all hypotheses in a lattice or hypergraph using polynomial-time dynamic programming (the forward algorithm or inside algorithm, respectively). There has also been work to improve the accuracy of the k-best approximation by either sampling k-best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decoding to find derivations that achieve the reference translation, and adding them to the k-best list (Liang, Zhang, and Zhao 2012). The second weakness of MERT is that it has no concept of regularization, causing it to overfit the training data if there are too many features, and there have been several attempts to incorporate regularization to ameliorate this problem. Cer, Jurafsky, and Manning (2008) propose a method to incorporate regularization by not choosing the plateau in the loss curve that minimizes the loss itself, but choosing the point considering the loss"
J16-1001,N12-1047,0,0.0128094,"bed in Section 3.4, possibly with the addition of a regularizer, is also a relatively standard problem in the machine learning literature. Methods to solve Equation (25) include sequential minimization optimization (Platt 1999), dual coordinate descent (Hsieh et al. 2008), as well as the quadratic program solvers used in standard SVMs (Joachims 1998). It should also be noted that there have also been several attempts to apply marginbased online learning algorithms explained in Section 6.3, but in a batch setting where the whole training corpus is decoded before each iteration of optimization (Cherry and Foster 2012; Gimpel and Smith 2012). We will explain these methods in more detail later, but it should be noted that the advantage of using these methods in a batch 27 Computational Linguistics Volume 42, Number 1 setting mainly lies in simplicity; for online learning it is often necessary to directly implement the optimization procedure within the decoder, whereas in a batch setting the implementation of the decoding and optimization algorithm can be performed separately. 5.4 Ranking and Linear Regression Optimization The rank-based loss described in Section 3.5 is essentially the combination of multipl"
J16-1001,J07-2003,0,0.0707349,"he sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts, which are represented as hidden variables, which together form a derivation. For example, in phrase-based translation (Koehn, Och, and Marcu 2003), the hidden variables will be the alignment between the phrases of the source and target sentences, and in tree-based translation models (Yamada and Knight 2001; Chiang 2007), the hidden variables will represent the latent tree structure used to generate the translation. We will define D( f ) to be the space of possible derivations that can be acquired from source sentence f , and d ∈ D( f ) to be one of those derivations. Any particular derivation d will correspond to exactly one e ∈ E ( f ), although the opposite is not true (the derivation uniquely determines the translation, but there can be multiple derivations corresponding to a particular translation). We also define tuple he, di consisting of a target sentence and its corresponding derivation, and T ( f )"
J16-1001,N09-1025,0,0.0253903,"Missing"
J16-1001,D08-1024,0,0.0189905,"Missing"
J16-1001,W12-3159,0,0.0192693,"he partial hypothesis he∗(i) , d∗(i) i by the greatest margin (the point of “maximum violation”). Search-aware tuning (Liu and Huang 2014) is a method that is able to consider search errors using an arbitrary optimization method. It does so by defining an evaluation measure for not only full sentences, but also partial derivations that occur during the search process, and optimizes parameters for k-best lists of partial derivations. Finally, there has also been some work on optimizing features not of the model itself, but parameters of the search process, using the downhill simplex algorithm (Chung and Galley 2012). Using this method, it is possible to adjust the beam width, distortion penalty, or other parameters that actually affect the size and shape of the derivation space, as opposed to simply rescoring hypotheses within it. 9. Conclusion In this survey article, we have provided a review of the current state-of-the-art in machine translation optimization, covering batch optimization, online optimization, expansions to large scale data, and a number of other topics. While these optimization algorithms have already led to large improvements in machine translation accuracy, the task of MT optimization"
J16-1001,Q14-1031,0,0.0261986,"Missing"
J16-1001,P11-2031,0,0.0170828,"om restarts, the results will generally change over multiple training runs, with the changes often being quite significant. Some research has shown that this randomness can be stabilized somewhat by improving the ability of the line-search algorithm to find a globally good solution by choosing random seeds more intelligently (Moore and Quirk 2008; Foster and Kuhn 2009) or by searching in directions that consider multiple features at once, instead of using the simple coordinate ascent as described in Figure 4 (Cer, Jurafsky, and Manning 2008). Orthogonally to actual improvement of the results, Clark et al. (2011) suggest that because randomness is a fundamental feature of MERT and other optimization algorithms for MT, it is better experimental practice to perform optimization multiple times, and report the resulting means and standard deviations over various optimization runs. It is also possible to optimize the MERT objective using other optimization algorithms. For example, Suzuki, Duh, and Nagata (2011) present a method for using particle swarm optimization, a distributed algorithm where many “particles” are each associated with a parameter vector, and the particle updates its vector in a way such"
J16-1001,2012.amta-papers.4,0,0.0361923,"Missing"
J16-1001,W02-1001,0,0.348853,"major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apart in many ways, and as a result requires a number of unique design decisions not necessary in other frameworks (as summarized in Table 1). The first is the search space that must be considered. The search space in MT is generally too large to expand exhaustively, so it is necessary to decide which subset of all the possible hypotheses should be used in optimization. In addition, the evaluation of MT accuracy is not straightforward, with automatic evaluation measures for MT still being researched to this day. From t"
J16-1001,P04-1015,0,0.012725,"eously in decoding (Sankaran, Sarkar, and Duh 2013). 8.4 Search and Optimization As mentioned in Section 2.4, because MT decoders perform approximate search, they may make search errors and not find the hypothesis that achieves the highest model score. There have been a few attempts to consider this fact in the optimization process. For example, in the perceptron algorithm of Section 6.2 it is known that the convergence guarantees of the structured perceptron no longer hold when using approximate search. The first method that can be used to resolve this problem is the early updating strategy (Collins and Roark 2004; Cowan, Ku˘cerov´a, and Collins 2006). The early updating strategy is a variety of bold updates, where the decoder output e∗(i) must be exactly equal to the reference e(i) . Decoding proceeds as normal, but the moment the correct hypothesis e(i) can no longer be produced by any hypothesis in the search space (i.e., a search error has occurred), search is stopped and update is performed using only the partial derivation. The second method is the max-violation perceptron (Huang, Fayong, and Guo 2012; Yu et al. 2013). In the max-violation perceptron, forced decoding is performed to acquire a der"
J16-1001,W06-1628,0,0.0446193,"Missing"
J16-1001,D13-1107,0,0.0514763,"Missing"
J16-1001,P09-1064,0,0.0637746,"Missing"
J16-1001,E14-1042,0,0.0183584,"Missing"
J16-1001,N15-1106,0,0.0450051,"Missing"
J16-1001,N12-1017,0,0.0139369,"cally translated using a ma N chine translation system to acquire MT results Eˆ = eˆ (i) i=1 , which are then compared to the corresponding references. The closer the MT output is to the reference, the better it is deemed to be, according to automatic evaluation. In addition, as there are often 9 Computational Linguistics Volume 42, Number 1 many ways to translate a particular sentence, it is also possible to perform evaluation with multiple references created by different translators. There has also been some work on encoding a huge number of references in a lattice, created either by hand (Dreyer and Marcu 2012) or by automatic paraphrasing (Zhou, Lin, and Hovy 2006). One major distinction between optimization measures is whether they are calculated on the corpus level or the sentence level. Corpus-level measures are calculated by taking statistics over the whole corpus, whereas sentence-level measures are calculated by measuring sentence-level accuracy, and defining the corpus-level accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this dist"
J16-1001,D09-1114,0,0.0268945,"ing, and Gaussian kernels (Nguyen, Mahajan, and He 2007), or the n-spectrum string kernel for finding associations between the source and target strings (Wang, Shawe-Taylor, and Szedmak 2007). Neural networks are another popular method for modeling nonlinearities, and it has been shown that neural networks can effectively be used to calculate new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target"
J16-1001,P08-2010,0,0.132413,"and overcoming this problem is the main obstacle to applying nonlinear models to MT (or structured learning in general). A number of countermeasures to this problem exist: Reranking: The most simple and commonly used method for incorporating nonlinearity, or other highly nonlocal features that cannot be easily incorporated in search, is through the use of reranking (Shen, Sarkar, and Och 2004). In this case, a system optimized using a standard linear model is used to create a k-best list of outputs, and this k-best list is then reranked using the nonlinear model (Nguyen, Mahajan, and He 2007; Duh and Kirchhoff 2008). Because we are now only dealing with fully expanded hypotheses, scoring becomes trivial, but reranking also has the major downsides of potentially missing useful hypotheses not included in the k-best list,9 and requiring time directly proportional to the size of the k-best list. Local Nonlinearity: Another possibility is to first use a nonlinear function to calculate local features, which are then used as part of the standard linear model (Liu et al. 2013). Alternatively, it is possible to treat feature-value pairs as new binary features (Clark, Dyer, and Lavie 2014). In this case, all effec"
J16-1001,P12-1001,0,0.0457195,"Missing"
J16-1001,N10-1033,0,0.032939,"Missing"
J16-1001,W09-0426,0,0.343238,"reasonable. 2.3.3 Summary features. Although sparse features are useful, training of sparse features is an extremely difficult optimization problem, and at this point there is still no method that has been widely demonstrated as being able to robustly estimate the parameters of millions of features. Because of this, a third approach of first training the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ"
J16-1001,W12-3160,0,0.0168486,"T1 Tt=+21 w(t) end procedure Figure 11 Online learning with synchronous update. 38 . Mix parameters Neubig and Watanabe Optimization for Statistical Machine Translation PS where s=1 µs = 1. As µs , it is possible to use a uniform distribution, or a weight proportional to the number of online updates performed at each shard (McDonald, Hall, and Mann 2010; Simianer, Riezler, and Dyer 2012). It should be noted that this algorithm can be considered a variety of the MapReduce framework, allowing for relatively straightforward implementation using parallel processing infrastructure such as Hadoop (Eidelman 2012). Simianer, Riezler, and Dyer (2012) propose another method for mixing parameters that, instead of averaging at each iteration, chooses to preserve only the parameters that have been learned over all shards, and sets all the remaining parameters to zero, allowing for a simple sort of feature selection. In particular, we define a S × M matrix h i&gt; ¯ (t+1) = w ¯ 1(t+1) |. . . |w ¯ S(t+1) , ¯ s(t+1) at each shard as w that combines the parameters w takes the L2 norm of each matrix column, and averages the columns with high norm values while setting the rest to zero. 7.1.2 Asynchronous Update. Whi"
J16-1001,P13-1110,0,0.03306,"Missing"
J16-1001,P02-1001,0,0.0859128,"(22) as pi,k = PK exp(si,k ) k0 =1 exp(si,k0 ) (53) Next, we define the expectation of the n-gram (gn ∈ ek ) frequency as cn,i,k , the expectation of the number of n-gram matches as mn,i,k , and the expectation of the reference length as ri,k . These values can be calculated as: cn,i,k = |{gn ∈ ek } |· pi,k (i) mn,i,k = |{gn ∈ ek } ∩ {gn ∈ e(i) } |· pi,k ri,k = |e(i) |· pi,k It should be noted that although these equations apply to k-best lists, it is also possible to calculate statistics over lattices or forests using dynamic programming algorithms and tools such as the expectation semiring (Eisner 2002; Li and Eisner 2009). 30 Neubig and Watanabe Optimization for Statistical Machine Translation xBLEU is calculated from these expected sufficient statistics: xBLEU = 4 Y n=1 P N PK i=1 k=1 mn,i,k P N PK i=1 k=1 cn,i,k ! 14 PN PK ·φ 1− i=1 k=1 ri,k PN P K i=1 k=1 c1,i,k ! (54) where φ(x) is the brevity penalty. Compared to the risk minimization in Equation (24), we define our optimization problem as the maximization of xBLEU: `xBLEU (F, E, C; γ, w) = − xBLEU (55) It is possible to calculate a gradient for xBLEU, allowing for optimization using gradient-based optimization methods, and we explain"
J16-1001,N13-1025,0,0.032251,"Missing"
J16-1001,W09-0439,0,0.0251881,"a number of extensions to the MERT framework have been proposed to resolve these problems. The first weakness of MERT is the randomness in the optimization process. Because each iteration of the training algorithm generally involves a number of random restarts, the results will generally change over multiple training runs, with the changes often being quite significant. Some research has shown that this randomness can be stabilized somewhat by improving the ability of the line-search algorithm to find a globally good solution by choosing random seeds more intelligently (Moore and Quirk 2008; Foster and Kuhn 2009) or by searching in directions that consider multiple features at once, instead of using the simple coordinate ascent as described in Figure 4 (Cer, Jurafsky, and Manning 2008). Orthogonally to actual improvement of the results, Clark et al. (2011) suggest that because randomness is a fundamental feature of MERT and other optimization algorithms for MT, it is better experimental practice to perform optimization multiple times, and report the resulting means and standard deviations over various optimization runs. It is also possible to optimize the MERT objective using other optimization algori"
J16-1001,D08-1089,0,0.0144523,"ommon to add a word penalty feature that measures the length of translation e to compensate for this. Similarly, phrase penalty or rule penalty features express the trade-off between longer or shorter derivations. There exist other features that are dependent on the underlying MT system model. Phrase-based MT heavily relies on the distortion probabilities that are computed by the distance on the source side of target-adjacent phrase pairs. More refined lexicalized reordering models estimate the parameters from the training data based on the relative distance of two phrase pairs (Tillman 2004; Galley and Manning 2008). 2.3.2 Sparse features. Although dense features form the foundation of most SMT systems, in recent years the ability to define richer feature sets and directly optimize the system using rich features has been shown to allow for significant increases in accuracy. On the other hand, large and sparse feature sets make the MT optimization problem significantly harder, and many of the optimization methods we will cover in the rest of this survey are aimed at optimizing rich feature sets. The first variety of sparse features that we can think of are phrase features or rule features, which count the"
J16-1001,D11-1004,0,0.0365469,"Missing"
J16-1001,D13-1201,0,0.0171802,"on to ameliorate this problem. Cer, Jurafsky, and Manning (2008) propose a method to incorporate regularization by not choosing the plateau in the loss curve that minimizes the loss itself, but choosing the point considering the loss values for a few surrounding plateaus, helping to avoid points that have a low loss but are surrounded by plateaus with higher loss. It is also possible to incorporate regularization into MERT-style line search using an SVM-inspired marginbased objective (Hayashi et al. 2009) or by using scale-invariant regularization methods such as L0 or a scaled version of L2 (Galley et al. 2013). 26 Neubig and Watanabe Optimization for Statistical Machine Translation The final weakness of MERT is that it has computational problems when scaling to large numbers of features. When using only a standard set of 20 or so features, MERT is able to perform training in reasonable time, but the number of line searches, and thus time, required in Algorithm 4 scales linearly with the number of features. Thus training of hundreds of features is time-consuming, and there are no published results training standard MERT on thousands or millions of features. It should be noted, however, that Galley e"
J16-1001,N13-1048,0,0.0136588,", which is generally initialized to a value η(1) and gradually reduced according to a function update(· ) as learning progresses. One standard method for updating η(t) according to the following formula η(t+1) ← η(1) 1 + t/T (70) allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the parameters are updated and we obtain w(t+1) . Within this framework, in the perceptron algorithm η(t) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in translation for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization term Ω(w) is not differentiable, such as L1 regularization, it is a common practice to use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 1 w(t+ 2 ) ← w(t) − η(t+1) ∆`(F˜ (t) , E˜ (t) , C˜ (t) ; w(t) ) (71) w(t+1) ← arg min 1 kw − w(t+ 2 ) k22 + η(t+1) λΩ(w) 2 w (72) 1 36 Neubig and Watanabe Optimization for Statistical Machine Translation First, we perform updates without considering the"
J16-1001,D11-1083,0,0.01934,"lume 42, Number 1 each feature function can be decomposed over each step, and Equation (1) can be expressed by heˆ , dˆ i = arg max he,di∈T ( f ) j−1 |w| X i wi |d| X j−1 hi (dj , ρi (d1 )) (9) j=1 where hi (dj , ρi (d1 )) is a feature function for the jth step decomposed from the global feature function of hi ( f , e, d). As mentioned in the previous section, non-local features require information that cannot be calculated directly from the rule itself, and j−1 ρi (d1 ) is a variable that defines the residual information to score this ith feature funcj−1 tion using the partial derivation d1 (Gesmundo and Henderson 2011; Green, Cer, and Manning 2014). For example, in phrase-based translation, for an n-gram language j −1 model feature, ρi (d1 ) will be the n − 1 word suffix of the partial translation (Koehn, Och, and Marcu 2003). The local feature functions, such as phrase translation probabilij −1 ties in Section 2.3.1, require no context from partial derivations, and thus ρi (d1 ) = ∅. The problem of decoding is treated as a search problem in which partial derivations ˙ in Equation (9) are enumerated to form hypotheses or states. In d˙ together with ρi (d) phrase-based MT, search is carried out by enumerati"
J16-1001,D13-1111,0,0.0246035,"Missing"
J16-1001,D09-1023,0,0.0216448,"ontributes to the denominator. Thus, intuitively, the softmax objective prefers parameter settings that assign high scores to the oracle translations, and lower scores to any other members of c(i) that are not oracles. It should be noted that this loss can be calculated from a k-best list by iterating over the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward–backward or inside–outside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006; Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w. This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to error, it also has the advantage of being different"
J16-1001,N12-1023,0,0.0463339,"e˙ (i) , e¯ (i) ) − w&gt; ∆h( f (i) , e˙ (i) , d˙ , e¯ (i) , d¯ ) (37) which is the largest margin in the k-best list. Explaining more intuitively, this criterion provides a bias towards selecting hypotheses with high error, making the learning algorithm work harder to correctly classify very bad hypotheses than it does for hypotheses that are only slightly worse than the oracle. Inference methods that consider the loss as in Equations (35) and (36) are called loss-augmented inference (Taskar et al. 2005) methods, and can minimize losses with respect to the candidate with the largest violation. Gimpel and Smith (2012) take this a step further, defining a structured ramp loss that additionally considers Equations (28) and (29) within this framework. 5. Batch Methods Now that we have explained the details of calculating loss functions used in machine translation, we turn to the actual algorithms used in optimizing using these loss functions. In this section, we cover batch learning approaches to MT optimization. Batch learning works by considering the entire training data on every update of the parameters, in contrast to online learning (covered in the following section), which considers only part of the dat"
J16-1001,E14-1047,0,0.0144677,"ing large-scale optimization, nonlinear models, domain-dependent optimization, and the effect of MT evaluation measures or search on optimization. Finally, we discuss the current state of affairs in MT optimization, and point out some unresolved problems that will likely be the target of further research in optimization for MT. 1. Introduction Machine translation (MT) has long been both one of the most promising applications of natural language processing technology and one of the most elusive. However, over approximately the past decade, huge gains in translation accuracy have been achieved (Graham et al. 2014), and commercial systems deployed for hundreds of language pairs are being used by hundreds of millions of users. There are many reasons for these advances in the accuracy and coverage of MT, but among them two particularly stand out: statistical machine translation (SMT) techniques that make it possible to learn statistical models from data, and massive increases in the amount of data available to learn SMT models. ∗ 8916-5 Takayama-cho, Ikoma, Nara, Japan. E-mail: neubig@is.naist.jp. ∗∗ 6-10-1 Roppongi, Minato-ku, Tokyo, Japan. E-mail: tarow@google.com. This work was mostly done while the se"
J16-1001,W14-3360,0,0.0168226,"ecomes an individual choice, and thus the ranking loss is the sum of these individual losses. As the binary classifier, it is possible to use perceptron, hinge, or softmax losses between the correct and incorrect answers. It should be noted that standard ranking techniques make a hard decision between candidates with higher and lower error, which can cause problems when the ranking by error does not correlate well with the ranking measured by the model. The crossentropy ranking loss solves this problem by softly fitting the model distribution to the distribution of ranking measured by errors (Green et al. 2014). 3.6 Mean Squared Error Loss Finally, mean squared error loss is another method that does not make a hard zero– one decision between the better and worse candidates, but instead attempts to directly estimate the difference in scores (Bazrafshan, Chung, and Gildea 2012). This is done by first finding the difference in errors between the two candidates ∆ err(e(i) , e∗ , e) and defining the loss as the mean squared error of the difference between the inverse of the difference in the errors and the difference in the model scores5 : `mse (F, E, C; w) = 1 N (C) N X X X i=1 he∗ ,d∗ i∈o(i) he,di∈c(i)"
J16-1001,P13-1031,0,0.0142589,"and gradually reduced according to a function update(· ) as learning progresses. One standard method for updating η(t) according to the following formula η(t+1) ← η(1) 1 + t/T (70) allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the parameters are updated and we obtain w(t+1) . Within this framework, in the perceptron algorithm η(t) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in translation for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization term Ω(w) is not differentiable, such as L1 regularization, it is a common practice to use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 1 w(t+ 2 ) ← w(t) − η(t+1) ∆`(F˜ (t) , E˜ (t) , C˜ (t) ; w(t) ) (71) w(t+1) ← arg min 1 kw − w(t+ 2 ) k22 + η(t+1) λΩ(w) 2 w (72) 1 36 Neubig and Watanabe Optimization for Statistical Machine Translation First, we perform updates without considering the regularization term in Equation (71). Second, the"
J16-1001,D14-1130,0,0.0147892,"ecomes an individual choice, and thus the ranking loss is the sum of these individual losses. As the binary classifier, it is possible to use perceptron, hinge, or softmax losses between the correct and incorrect answers. It should be noted that standard ranking techniques make a hard decision between candidates with higher and lower error, which can cause problems when the ranking by error does not correlate well with the ranking measured by the model. The crossentropy ranking loss solves this problem by softly fitting the model distribution to the distribution of ranking measured by errors (Green et al. 2014). 3.6 Mean Squared Error Loss Finally, mean squared error loss is another method that does not make a hard zero– one decision between the better and worse candidates, but instead attempts to directly estimate the difference in scores (Bazrafshan, Chung, and Gildea 2012). This is done by first finding the difference in errors between the two candidates ∆ err(e(i) , e∗ , e) and defining the loss as the mean squared error of the difference between the inverse of the difference in the errors and the difference in the model scores5 : `mse (F, E, C; w) = 1 N (C) N X X X i=1 he∗ ,d∗ i∈o(i) he,di∈c(i)"
J16-1001,N13-1035,0,0.017328,"meters, it is useful to make the distinction between in-domain translation (when the model training data matches the test domain) and cross-domain translation (when the model training data mismatches the test domain). In cross-domain translation, fewer long rules will be used, and translation probabilities will be less reliable, and the parameters must change accordingly to account for this (Pecina, Toral, and van Genabith 2012). It has also been shown that building TMs for several domains and tuning the parameters to maximize translation accuracy can improve MT accuracy on the target domain (Haddow 2013). Another option for making the distinction between in-domain and out-of-domain data is by firing different features for in-domain and out-of-domain training data, allowing for the learning of different weights for different domains (Clark, Lavie, and Dyer 2012). 8.3 Evaluation Measures and Optimization In the entirety of this article, we have assumed that optimization for MT aims to reduce MT error defined using an evaluation measure, generally BLEU. However, as mentioned in Section 2.5, evaluation of MT is an active research field, and there are many alternatives in addition to BLEU. Thus, i"
J16-1001,W11-2130,0,0.0496357,"Missing"
J16-1001,N07-2015,0,0.063938,"Missing"
J16-1001,2009.iwslt-papers.3,1,0.788497,"training data if there are too many features, and there have been several attempts to incorporate regularization to ameliorate this problem. Cer, Jurafsky, and Manning (2008) propose a method to incorporate regularization by not choosing the plateau in the loss curve that minimizes the loss itself, but choosing the point considering the loss values for a few surrounding plateaus, helping to avoid points that have a low loss but are surrounded by plateaus with higher loss. It is also possible to incorporate regularization into MERT-style line search using an SVM-inspired marginbased objective (Hayashi et al. 2009) or by using scale-invariant regularization methods such as L0 or a scaled version of L2 (Galley et al. 2013). 26 Neubig and Watanabe Optimization for Statistical Machine Translation The final weakness of MERT is that it has computational problems when scaling to large numbers of features. When using only a standard set of 20 or so features, MERT is able to perform training in reasonable time, but the number of line searches, and thus time, required in Algorithm 4 scales linearly with the number of features. Thus training of hundreds of features is time-consuming, and there are no published re"
J16-1001,P12-1031,0,0.0291857,"Summary features. Although sparse features are useful, training of sparse features is an extremely difficult optimization problem, and at this point there is still no method that has been widely demonstrated as being able to robustly estimate the parameters of millions of features. Because of this, a third approach of first training the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according t"
J16-1001,2009.mtsummit-posters.8,0,0.0230133,"-the-loop optimization is prohibitive, so Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide than generating new reference sentences. Finally, there is also some work on optimizing multiple evaluation metrics at one time. The easiest way to do so is to simply use the linear interpolation of two or more metrics as the error function (Dyer et al. 2009; He and Way 2009; Servan and Schwenk 2011): ˆ = error(E, E) L X ˆ ρi errori (E, E) (78) i=1 where L is the number of error functions, and ρi is a manually set interpolation coefficient for its respective error function. There are also more sophisticated methods based on the idea of optimizing towards Pareto-optimal hypotheses (Duh et al. 43 Computational Linguistics Volume 42, Number 1 2012), which achieve errors lower than all other hypotheses on at least one evaluation measure, ˆ &lt; errori (E, E0 )} pareto(E, E ) = {Eˆ ∈ E : ∀E0 ∈E ∃i errori (E, E) (79) To incorporate this concept of Pareto optimality into o"
J16-1001,D11-1125,0,0.0228166,"Missing"
J16-1001,P07-1019,0,0.0120787,"K+ algorithm (Chappelier and Rajman 1998) on the source side and generating partial derivations for progressively longer source spans. Because of the enormous ˙ in each partial derivation, beam search search space brought about by maintaining ρi (d) is used to heuristically prune the search space. As a result, the search is inexact because of the search error caused by heuristic pruning, in which the best scoring hypothesis is not necessarily optimal in terms of given model parameters. The search is efficiently carried out by merging equivalent states encoded as ρ (Koehn, Och, and Marcu 2003; Huang and Chiang 2007), and the space is succinctly represented by compact data structures, such as graphs (Ueffing, Och, and Ney 2002) (or lattices) in phrase-based MT (Koehn, Och, and Marcu 2003) and hypergraphs (Klein and Manning 2004) (or packed forests) in tree-based MT (Huang and Chiang 2007). These data structures may be directly used as compact representations of all derivations for optimization. However, using these data structures directly can be unwieldly, and thus it is more common to obtain a k-best list as an approximation of the derivation space. Figure 1(a) shows an example of k-best English transla"
J16-1001,N12-1015,0,0.0318911,"Missing"
J16-1001,2006.amta-papers.8,0,0.0149369,"Missing"
J16-1001,J10-4005,0,0.0445813,"tween MT evaluation and optimization. Finally, we conclude in Section 9, overviewing the methods described, making a brief note about which methods see the most use in actual systems, and outlining some of the unsolved problems in the optimization of MT systems. 2. Machine Translation Preliminaries and Definitions Before delving into the details of actual optimization algorithms, we first introduce preliminaries and definitions regarding MT in general and the MT optimization problem in particular. We focus mainly on the aspects of MT that are relevant to optimization, and readers may refer to Koehn (2010) or Lopez (2008) for more details about MT in general. 2.1 Machine Translation Machine translation is the problem of automatically translating from one natural language to another. Formally, we define this problem by specifying F to be the collection of all source sentences to be translated, f ∈ F as one of the sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts,"
J16-1001,P07-2045,0,0.0118186,"nd h(· ) are M-dimensional, and bm is an M-dimensional vector where the m-th element is 1 and the rest of the elements are zero. For the T iterations, we decide the dimension m of the feature vector (line 6), and for each possible weight vector w(j) + γbm choose the γ ∈ R that minimizes `error (· ) using line search (line 7). Then, among the γ for each of the M search dimensions, we perform an update using γˆ that affords the largest reduction in error (lines 9 and 10). This algorithm can be deemed a variety of steepest descent, which is a standard method used in most implementations of MERT (Koehn et al. 2007). Another alternative is a variant of coordinate descent (e.g., Powell’s method), in which search and update is performed in each dimension. One feature of MERT is that it is known to easily fall into local optima of the error function. Because of this, it is standard to choose R starting points (line 4), perform ˆ that optimization starting at each of these starting points, and finally choose the w minimizes the loss from the weights acquired from each of the R random restarts. The R starting points are generally chosen so that one of the points is the best w from the previous iteration, and"
J16-1001,N03-1017,0,0.0562814,"Missing"
J16-1001,W07-0733,0,0.0253917,"nd Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as we will summarize subsequently. One relatively simple way of performing domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target la"
J16-1001,P09-1019,0,0.0301324,"an one dimension, or all dimensions at a single time. However, as MERT remains a fundamentally computationally hard problem, this method takes large amounts of time for larger training sets or feature spaces. It should be noted that instability in MERT is not entirely due to the fact that search is random, but also due to the fact that k-best lists are poor approximations of the whole space of possible translations. One way to improve this approximation is by performing MERT over an exponentially large number of hypotheses encoded in a translation lattice (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT over these sorts of packed data structures by observing the fact that the envelopes used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), allowing for exact calculation of the full envelope for all hypotheses in a lattice or hypergraph using polynomial-time dynamic programming (the forward algorithm or inside algorithm, respectively). There has also been work to improve the accuracy of the k-best approximation by either sampling k-best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decodin"
J16-1001,2008.eamt-1.14,0,0.0566391,"uch as the Parzen window, binning, and Gaussian kernels (Nguyen, Mahajan, and He 2007), or the n-spectrum string kernel for finding associations between the source and target strings (Wang, Shawe-Taylor, and Szedmak 2007). Neural networks are another popular method for modeling nonlinearities, and it has been shown that neural networks can effectively be used to calculate new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with da"
J16-1001,D08-1088,0,0.0544982,"Missing"
J16-1001,C10-1075,0,0.0242919,"the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as we will summarize subsequently. One relatively simple way of performing domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target language or vice versa, then using this data for optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is possible to reflect post-edited translations back into the o"
J16-1001,D09-1005,0,0.220713,"s can be calculated from a k-best list by iterating over the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward–backward or inside–outside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006; Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w. This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to error, it also has the advantage of being differentiable, allowing for easier optimization. To define this error, we define a scaling parameter γ ≥ 0 and use it in the calculation of each hypothesis’s probability pγ,w (e, d |f , c) = P exp(γw&gt; h( f , e, d)) &gt; 0 0 he0 ,d0 i∈c exp(γw h( f , e , d ))"
J16-1001,D11-1085,0,0.0167481,"imple way of performing domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target language or vice versa, then using this data for optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is possible to reflect post-edited translations back into the optimization process as new indomain training data (Mathur, Mauro, and Federico 2013; Denkowski, Dyer, and Lavie 2014). Once adaptation data have been chosen, it is necessary to decide how to use the data. The most straightforward way is to simply use these in-domain data in optimization, but if the data set is small it is preferable to combine both in- and out-ofdomain data to achieve more robust parameter estimates. This is essentially equivalent to t"
J16-1001,P06-1096,0,0.01512,"Missing"
J16-1001,C04-1072,0,0.013434,"is defined as the longest reference with a length shorter than or equal to eˆ (i) . 2.5.2 BLEU+1. One thing to notice here is that BLEU is calculated by taking statistics over the entire corpus, and thus it is a corpus-level measure. There is nothing inherently preventing us from calculating BLEU on a single sentence, but in the single-sentence case it is common for the number of matches of higher order n-grams to become zero, resulting in a BLEU score of zero for the entire sentence. One common solution to this problem is the use of a smoothed version of BLEU, commonly referred to as BLEU+1 (Lin and Och 2004). In BLEU+1, we add one to the numerators and denominators of each n-gram of order greater than one c0n (ˆe ) = |{gn ∈ eˆ } |+ δ(n &gt; 1) m0n (e, eˆ ) = |{gn ∈ eˆ } ∩ {g0 n ∈ e} |+ δ(n &gt; 1) where δ(· ) is a function that takes a value of 1 when the corresponding statement is true. We can then re-define a sentence-level BLEU using these smoothed counts BLEU’(e, eˆ ) = 1 4  0 Y m ({e1 , . . . , eM }, eˆ ) 4 n n =1 c0n (ˆe ) · BP(e, eˆ ) (12) and the corpus-level evaluation can be re-defined as the average of sentence level evaluations ˆ = 1 BLEU’(E, E) N N X BLEU’(e(i) , eˆ (i) ) (13) i=1 It has"
J16-1001,D12-1037,1,0.931558,"izing other losses such as those based on probabilistic models (Section 5.2), error margins (Section 5.3), ranking (Section 5.4), and risk (Section 5.5). 5.1 Error Minimization 5.1.1 Minimum Error Rate Training Overview. Minimum error rate training (MERT) (Och 2003) is one of the first, and is currently the most widely used, method for MT optimization, and focuses mainly on direct minimization of the error described in Section 3.1. Because error is not continuously differentiable, MERT uses optimization methods that do not require the calculation of a gradient, such as iterative line search 7 Liu et al. (2012) propose a method to avoid over-aggressive moves in parameter space by considering the balance between increase in the evaluation score and the similarity with the parameters on the previous iteration. 21 Computational Linguistics 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: Volume 42, Number 1 procedure MERT(F, E, C) ˆ ←∅ w for r ∈ {1 . . . R} do w(1) ∼ RM . Initialize randomly for t ∈ {1 . . . T} do . Until convergence for m ∈ {1 . . . M} do . For each dimension γˆ m ← arg minγ `error (F, E, C; w(t) + γbm ) . Search end for . Descent γˆ ← arg minγˆ m `error (F, E, C; w(t) + γˆ"
J16-1001,D14-1209,0,0.0277259,"Missing"
J16-1001,P13-1078,1,0.733513,"g one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if we enumerate all possible derivations in T ( f ) and rank each derivation by the model. We assume that a derivation is composed of a set of steps d = d1 , d2 , · · · , d|d| (8) where each dj is a step—for example, a phrase pair in phrase-based MT or a synchronous rule in tree-based MT—ordered in a particular way. We also assume that 7 Computational Linguistics Volume 42, Number"
J16-1001,C12-2071,1,0.926133,"izing other losses such as those based on probabilistic models (Section 5.2), error margins (Section 5.3), ranking (Section 5.4), and risk (Section 5.5). 5.1 Error Minimization 5.1.1 Minimum Error Rate Training Overview. Minimum error rate training (MERT) (Och 2003) is one of the first, and is currently the most widely used, method for MT optimization, and focuses mainly on direct minimization of the error described in Section 3.1. Because error is not continuously differentiable, MERT uses optimization methods that do not require the calculation of a gradient, such as iterative line search 7 Liu et al. (2012) propose a method to avoid over-aggressive moves in parameter space by considering the balance between increase in the evaluation score and the similarity with the parameters on the previous iteration. 21 Computational Linguistics 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: Volume 42, Number 1 procedure MERT(F, E, C) ˆ ←∅ w for r ∈ {1 . . . R} do w(1) ∼ RM . Initialize randomly for t ∈ {1 . . . T} do . Until convergence for m ∈ {1 . . . M} do . For each dimension γˆ m ← arg minγ `error (F, E, C; w(t) + γbm ) . Search end for . Descent γˆ ← arg minγˆ m `error (F, E, C; w(t) + γˆ"
J16-1001,I13-1032,1,0.943906,"g one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if we enumerate all possible derivations in T ( f ) and rank each derivation by the model. We assume that a derivation is composed of a set of steps d = d1 , d2 , · · · , d|d| (8) where each dj is a step—for example, a phrase pair in phrase-based MT or a synchronous rule in tree-based MT—ordered in a particular way. We also assume that 7 Computational Linguistics Volume 42, Number"
J16-1001,P13-2067,0,0.0231633,"s on the effect of the metric used in optimization on human assessments of the generated translations (Cer, Manning, and Jurafsky 2010; Callison-Burch et al. 2011). These studies showed the rather surprising result that despite the fact that other evaluation measures had proven superior to BLEU with regards to post facto correlation with human evaluation, a BLEU-optimized system proved superior to systems tuned using other metrics. Since this result, however, there have been other reports stating that systems optimized using other metrics such as TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT (Lo et al. 2013) achieve superior results to BLEU-optimized systems. There have also been attempts to directly optimize not automatic, but human evaluation measures of translation quality (Zaidan and Callison-Burch 2009). However, the cost of performing this sort of human-in-the-loop optimization is prohibitive, so Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide t"
J16-1001,D08-1076,0,0.0316358,"according to a particular γ (Figure 6a). After finding the envelope, for each line that participates in the envelope, we can calculate the sufficient statistics necessary for calculating the loss `error (· ) and error error(· ). For example, given the envelope in Figure 6a, Figure 6b is an example of the sentence-wise loss with respect to γ. The envelope shown in Equation (41) can also be viewed as the problem of finding a convex hull in computational geometry. A standard and efficient algorithm for finding a convex hull of multiple lines is the sweep line algorithm (Bentley and Ottmann 1979; Macherey et al. 2008) (see Figure 7). Here, we assume L is a set of the lines corresponding to the K translation candidates in c(i) , each line l ∈ L is expressed as ha(l), b(l), γ(l)i with intercept a(l) = a( f (i) , e, d), and slope b(l) = b( f (i) , e, d). Furthermore, we define γ(l) as an intersection initialized to −∞. S ORT L INES (L) in Figure 3 sorts the lines in the order of their slope b(l), and if two lines lk1 have the same slope, lk2 chooses the one with the larger intercept a(lk1 ) &gt; a(lk2 ) and deletes the other. We next process the sorted set of lines L0 (|L0 |≤ K) in order of ascending slope (line"
J16-1001,P08-1114,0,0.0208734,"e for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two can be calculated from source and local target context, but target bigrams require target bigram context and are thus non-local features. One final variety of features that has proven useful is syntax-based features (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and hierarchical phrase-based translations do not directly consider syntax (in the linguistic sense) in the construction of the models, so introducing this information in the form of features has a potential for benefit. One way to introduce this information is to parse the input sentence before translation, and use the information in the parse tree in the calculation of features. For example, we can count the number of times a phrase or translation rule matches, or partially matches (Marton and Resnik 2008), a span with a particular label, based on the assumption"
J16-1001,W13-2237,0,0.017363,"Missing"
J16-1001,P05-1012,0,0.151284,"Missing"
J16-1001,N10-1069,0,0.0269632,"Missing"
J16-1001,C08-1074,0,0.208528,"optima of the error function. Because of this, it is standard to choose R starting points (line 4), perform ˆ that optimization starting at each of these starting points, and finally choose the w minimizes the loss from the weights acquired from each of the R random restarts. The R starting points are generally chosen so that one of the points is the best w from the previous iteration, and the remaining R − 1 have each element of w chosen randomly and uniformly from some interval, although it has also been shown that more intelligent choice of initial points can result in better final scores (Moore and Quirk 2008). 5.1.2 Line Search for MERT. Although the majority of this process is relatively straightforward, the line search in Line 7 of Figure 4 requires a bit more explanation. In this step, we would like to choose the γ that results in the ordering of hypotheses in c(i) that achieves the lowest error. In order to do so, MERT uses an algorithm that allows for 22 Neubig and Watanabe Optimization for Statistical Machine Translation exact enumeration of which of the K candidates in c(i) will be chosen for each value of γ. Concretely, we define  &gt; arg max w(j) + γbm h( f (i) , e, d) (38) he,di∈c(i) &gt; ="
J16-1001,C12-1121,0,0.0296408,"Missing"
J16-1001,P13-2003,0,0.0395626,"Missing"
J16-1001,W14-3316,0,0.0229636,"see that even after over ten years, MERT is still the dominant optimization algorithm. However, starting in WMT 2013, we can see a move to systems based on MIRA, and to a lesser extent ranking, particularly in the most competitive systems. In these systems, the preferred choice of an optimization algorithm seems to be MERT when using up to 20 features, and MIRA when using a large number of features (up to several hundred). There are fewer examples of systems using large numbers of features (tens of thousands, or millions) in actual competitive systems, with a few exceptions (Dyer et al. 2009; Neidert et al. 2014; Wuebker et al. 2014). In the case when a large number of sparse features are used, it is most common to use a softmax or risk-based objective and gradient-based optimization algorithms, often combining the features into summary features and performing a final tuning pass with MERT. The fact that algorithms other than MERT are seeing adoption in competitive systems for shared tasks is a welcome sign for the future of MT optimization research. However, there are still many open questions in the field, a few of which can be outlined here: Stable Training with Millions of Features: At the moment"
J16-1001,W07-0710,0,0.0679013,"Missing"
J16-1001,P03-1021,0,0.209352,"Linguistics Computational Linguistics Volume 42, Number 1 Within the SMT framework, there have been two revolutions in the way we mathematically model the translation process. The first was the pioneering work of Brown et al. (1993), who proposed the idea of SMT, and described methods for estimation of the parameters used in translation. In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training corpus. The second major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apart in many ways"
J16-1001,P02-1038,0,0.156287,"tion for Computational Linguistics Computational Linguistics Volume 42, Number 1 Within the SMT framework, there have been two revolutions in the way we mathematically model the translation process. The first was the pioneering work of Brown et al. (1993), who proposed the idea of SMT, and described methods for estimation of the parameters used in translation. In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training corpus. The second major advance in SMT is the discriminative training framework proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for MT, optimized to maximize either the probability of getting the correct sentence from a k-best list of candidates, or to directly achieve the highest accuracy over the entire corpus. By describing the scoring function for MT as a flexibly parameterizable loglinear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002). However, within the general framework of structured prediction, MT stands apa"
J16-1001,J03-1002,0,0.0233519,"t vector w from the set of possible weight vectors RM .1 Optimization is also widely called tuning in the SMT literature. In addition, because of the exponentially large number of possible translations in E ( f ) that must be considered, it is necessary to take advantage of the problem structure, making MT optimization an instance of structured learning. 2.2 Model Construction The first step of creating a machine translation system is model construction, in which translation models (TMs) are extracted from a large parallel corpus. The TM is usually created by first aligning the parallel text (Och and Ney 2003), using this text to extract multi-word phrase pairs or synchronous grammar rules (Koehn, Och, and Marcu 2003; Chiang 2007), and scoring these rules according to several features explained in more detail in Section 2.3. The construction of the TM is generally performed first in a manner that does not directly consider the optimization of translation accuracy, followed by an optimization step that explicitly considers the accuracy achieved by the system.2 In this survey, we focus on the optimization step, and thus do not cover elements of model construction that do not directly optimize an obje"
J16-1001,P02-1040,0,0.110893,"res is whether they are calculated on the corpus level or the sentence level. Corpus-level measures are calculated by taking statistics over the whole corpus, whereas sentence-level measures are calculated by measuring sentence-level accuracy, and defining the corpus-level accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures. We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defin"
J16-1001,D09-1147,0,0.0513919,"Missing"
J16-1001,C12-1135,0,0.057601,"Missing"
J16-1001,C12-2091,0,0.0158071,"chair Figure 1 Example of a k-best list, lattice, and forest. Another class of decoding problem is forced decoding, in which the output from a decoder is forced to match with a reference translation of the input sentence. In phrasebased MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing approximate matching of the target side (Liang, Zhang, and Zhao 2012). It is also possible to create a neighborhood of a forced decoding derivation by adding additional hyperedges to the true derivation, which allows for efficient generation of negative examples for discriminative learning a"
J16-1001,P13-2060,0,0.0178879,"ring kernel for finding associations between the source and target strings (Wang, Shawe-Taylor, and Szedmak 2007). Neural networks are another popular method for modeling nonlinearities, and it has been shown that neural networks can effectively be used to calculate new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is a"
J16-1001,W10-1748,0,0.153366,"that calculates not the expectation of the error itself, but the expectation of the sufficient statistics used in calculating the error. In contrast to sentence-level approximations or formulations such as linear BLEU, the expectation of the sufficient statistics can be calculated directly on the corpus level. Because of this, by maximizing the evaluation derived by these expected statistics, it is possible to directly optimize for a corpus-level error, in a manner similar to MERT (Pauls, Denero, and Klein 2009). When this is applied to BLEU in particular, this measure is often called xBLEU (Rosti et al. 2010, 2011) and the required sufficient statistics include n-gram counts and matched n-gram counts. We define the kth translation candidate in c(i) as hek , dk i, its score as si,k = γw&gt; h( f (i) , ek , dk ), and the probability in Equation (22) as pi,k = PK exp(si,k ) k0 =1 exp(si,k0 ) (53) Next, we define the expectation of the n-gram (gn ∈ ek ) frequency as cn,i,k , the expectation of the number of n-gram matches as mn,i,k , and the expectation of the reference length as ri,k . These values can be calculated as: cn,i,k = |{gn ∈ ek } |· pi,k (i) mn,i,k = |{gn ∈ ek } ∩ {gn ∈ e(i) } |· pi,k ri,k ="
J16-1001,W11-2119,0,0.0440587,"Missing"
J16-1001,2010.amta-papers.31,0,0.0123991,"all number of these k(k − 1)/2 hypotheses for use in optimization, which has been shown empirically to allow for increases in training speed without decreases in accuracy. For example, Hopkins and May (2011) describe a method dubbed pairwise ranking optimization that selects 5,000 pairs randomly for each sentence, and among these random pairs using the 50 with the largest difference in error for training the classifier. Other selection heuristics—for example, avoiding training on candidate pairs with overly different scores (Nakov, Guzm´an, and Vogel 2013), or performing Monte Carlo sampling (Roth et al. 2010; Haddow, Arun, and Koehn 2011)—are also possible and potentially increase accuracy. Recently, there has also been a method proposed that uses an efficient ranking SVM formulation that alleviates the need for this sampling and explicitly performs ranking over all pairs (Dreyer and Dong 2015). The mean squared error loss described in Section 3.6, which is similar to ranking loss in that it will prefer a proper ordering of the k-best list, is much easier to optimize. This loss can be minimized using standard techniques for solving least-squared-error linear regression (Press et al. 2007). 5.5 Ri"
J16-1001,2012.amta-papers.14,0,0.0513567,"Missing"
J16-1001,C10-2124,0,0.0524463,"Missing"
J16-1001,N13-1115,0,0.429063,"Missing"
J16-1001,N13-1034,0,0.0211213,", training of sparse features is an extremely difficult optimization problem, and at this point there is still no method that has been widely demonstrated as being able to robustly estimate the parameters of millions of features. Because of this, a third approach of first training the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if w"
J16-1001,N04-1023,0,0.090152,"Missing"
J16-1001,P12-1002,0,0.0423328,"Missing"
J16-1001,P06-2101,0,0.208252,"are not oracles. It should be noted that this loss can be calculated from a k-best list by iterating over the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward–backward or inside–outside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss In contrast to softmax loss, which can be viewed as a probabilistic version of zero–one loss, risk defines a probabilistic version of the translation error (Smith and Eisner 2006; Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w. This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to error, it also has the advantage of being differentiable, allowing for easier optimization. To define this error, we define a scaling parameter γ ≥ 0 and use it in the calculation of each hypothesis’s probability pγ,w (e, d |f , c) = P exp(γw&gt; h( f ,"
J16-1001,2006.amta-papers.25,0,0.0903222,"cy, and defining the corpus-level accuracy as the average of the sentence-level accuracies. All optimization algorithms that are applicable to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. The most commonly used MT evaluation measure BLEU (Papineni et al. 2002) is defined on the corpus level, and we will cover it in detail as it plays an important role in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization algorithms that require sentence-level evaluation measures. We discuss the role of evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defined as the geometric mean of n-gram precisions (usually for n from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly high evaluation scores. For a single reference sentence e and a corresponding system outpu"
J16-1001,E12-1013,0,0.0488057,"Missing"
J16-1001,2012.amta-papers.17,0,0.0766389,"Missing"
J16-1001,2011.eamt-1.33,0,0.0184665,"feature spaces. It should be noted that instability in MERT is not entirely due to the fact that search is random, but also due to the fact that k-best lists are poor approximations of the whole space of possible translations. One way to improve this approximation is by performing MERT over an exponentially large number of hypotheses encoded in a translation lattice (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT over these sorts of packed data structures by observing the fact that the envelopes used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), allowing for exact calculation of the full envelope for all hypotheses in a lattice or hypergraph using polynomial-time dynamic programming (the forward algorithm or inside algorithm, respectively). There has also been work to improve the accuracy of the k-best approximation by either sampling k-best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decoding to find derivations that achieve the reference translation, and adding them to the k-best list (Liang, Zhang, and Zhao 2012). The second weakness of MERT is that it has no concept of regularizat"
J16-1001,I11-1073,0,0.0579941,"Missing"
J16-1001,D13-1083,0,0.0144122,"ing corpus, but with respect to a subset of data sampled from the corpus. This has consequences for the calculation of translation quality when using a corpus-level evaluation measure such as BLEU. For example, when choosing an oracle for oracle-based optimization methods, the oracles chosen when considering the entire corpus will be different from the oracles chosen when considering a mini-batch. In general, the amount of difference between the corpus-level and mini-batch level oracles will vary depending on the size of a mini-batch, with larger mini-batches providing a better approximation (Tan et al. 2013; Watanabe 2012). Thus, when using smaller batches, especially single sentences, it is necessary to use methods to approximate the corpus-level error function as covered in the next two sections. 6.1.1 Approximation with a Pseudo-Corpus. The first method to approximate the corpuslevel evaluation measure relies on creating a pseudo-corpus, and using it to augment the statistics used in the mini-batch error calculation (Watanabe et al. 2007). Specifically, n oN given the training data hF, Ei = hf (i) , e(i) i , we define its corresponding pseudoi=1  (i) N corpus E¯ = e¯ i=1 . E¯ could be, for e"
J16-1001,N04-4026,0,0.0168543,"s, and it is common to add a word penalty feature that measures the length of translation e to compensate for this. Similarly, phrase penalty or rule penalty features express the trade-off between longer or shorter derivations. There exist other features that are dependent on the underlying MT system model. Phrase-based MT heavily relies on the distortion probabilities that are computed by the distance on the source side of target-adjacent phrase pairs. More refined lexicalized reordering models estimate the parameters from the training data based on the relative distance of two phrase pairs (Tillman 2004; Galley and Manning 2008). 2.3.2 Sparse features. Although dense features form the foundation of most SMT systems, in recent years the ability to define richer feature sets and directly optimize the system using rich features has been shown to allow for significant increases in accuracy. On the other hand, large and sparse feature sets make the MT optimization problem significantly harder, and many of the optimization methods we will cover in the rest of this survey are aimed at optimizing rich feature sets. The first variety of sparse features that we can think of are phrase features or rule"
J16-1001,P06-1091,0,0.0312915,"g to a function update(· ) as learning progresses. One standard method for updating η(t) according to the following formula η(t+1) ← η(1) 1 + t/T (70) allows for a guarantee of convergence (Collins et al. 2008). In Equation (69), the parameters are updated and we obtain w(t+1) . Within this framework, in the perceptron algorithm η(t) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in translation for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization term Ω(w) is not differentiable, such as L1 regularization, it is a common practice to use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 1 w(t+ 2 ) ← w(t) − η(t+1) ∆`(F˜ (t) , E˜ (t) , C˜ (t) ; w(t) ) (71) w(t+1) ← arg min 1 kw − w(t+ 2 ) k22 + η(t+1) λΩ(w) 2 w (72) 1 36 Neubig and Watanabe Optimization for Statistical Machine Translation First, we perform updates without considering the regularization term in Equation (71). Second, the regularization term is applied in Equ"
J16-1001,P13-2072,0,0.0148718,"for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search algorithms (Turian, Wellington, and Melamed 2006), or by using the trees to induce features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as w"
J16-1001,D08-1065,0,0.0296565,"06). The motivation for cooling is that if we start with a large T, the earlier steps using a smoother function will allow us to approach the global optimum, and the later steps will allow us to approach the actual error function. It should be noted that in Equation (24), and the discussion up to this point, we have been using not the corpus-based error, but the sentence-based error err(e(i) , e). There have also been attempts to make the risk minimization framework applicable to corpus-level error error(· ), specifically BLEU. We will discuss two such methods. 5.5.1 Linear BLEU. Linear BLEU (Tromble et al. 2008) provides an approximation for corpus-level BLEU that can be divided among sentences. Linear BLEU uses a Taylor expansion to approximate the effect that the sufficient statistics of any particular sentence will have on corpus-level BLEU. We define r as the total length of the reference translations, c as the total length of the candidates, and cn and mn (1 ≤ n ≤ 4) as the translation candidate’s number of n-grams, and number of n-grams that match the reference respectively. Taking the equation for corpus-level BLEU (Papineni et al. 2002) and assuming that the n-gram counts are approximately eq"
J16-1001,P07-1004,0,0.0720857,"Missing"
J16-1001,W02-1021,0,0.0686007,"Missing"
J16-1001,2005.eamt-1.36,0,0.0394239,"o(i) ← {he, di ∼ c(i) } or o(i) ← ∅ repeat for i ∈ P ERMUTE ({1, . . . , N} ) do . Random order o(i) ← ∅ s←∞ for k ∈ {1, . . . ,K} do (1) (i−1) s0 ← error E, {o1 , . . . , o1 (i) (i+1) , ck , o1 (N) , . . . , o1 }  if s0 &lt; s then . Update the oracle (i) (i) o ← {ck } s ← s0 else if s0 = s then . Same error value (i) o(i) ← o(i) ∪ {ck } end if end for end for until convergence . If O doesn’t change, converged return O end procedure Figure 2 Greedy search for an oracle. However, when using a corpus-level error function we need a slightly more sophisticated method, such as the greedy method of Venugopal and Vogel (2005). In this method (Figure 2), the oracle is first initialized either as an empty set or by randomly picking from the candidates. Next, we iterate randomly through the translation candidates in c(i) , try replacing the current oracle o(i) with the candidate, and check the change in the error function (Line 9), and if the error decreases, replace the oracle with the tested candidate. This process is repeated until there is no change in O. 4.3 Selecting Oracles for Margin-Based Methods Considering the hinge loss of Equation (30), the 1-best and oracle candidates are acquired according to Equation"
J16-1001,N07-2047,0,0.0806296,"Missing"
J16-1001,N12-1026,1,0.922215,"ith respect to a subset of data sampled from the corpus. This has consequences for the calculation of translation quality when using a corpus-level evaluation measure such as BLEU. For example, when choosing an oracle for oracle-based optimization methods, the oracles chosen when considering the entire corpus will be different from the oracles chosen when considering a mini-batch. In general, the amount of difference between the corpus-level and mini-batch level oracles will vary depending on the size of a mini-batch, with larger mini-batches providing a better approximation (Tan et al. 2013; Watanabe 2012). Thus, when using smaller batches, especially single sentences, it is necessary to use methods to approximate the corpus-level error function as covered in the next two sections. 6.1.1 Approximation with a Pseudo-Corpus. The first method to approximate the corpuslevel evaluation measure relies on creating a pseudo-corpus, and using it to augment the statistics used in the mini-batch error calculation (Watanabe et al. 2007). Specifically, n oN given the training data hF, Ei = hf (i) , e(i) i , we define its corresponding pseudoi=1  (i) N corpus E¯ = e¯ i=1 . E¯ could be, for example, either t"
J16-1001,D07-1080,1,0.831583,"er, and Dyer (2012) also propose features using the “shape” of translation rules, transforming a rule X → hne X 1 pas, did not X 1 i (5) into a string simply indicating whether each word is a terminal (T) or non-terminal (N) N → hT N T, T T Ni (6) Count-based features can also be extended to cover other features of the translation, such as phrase or rule bigrams, indicating which phrases or rules tend to be used together (Simianer, Riezler, and Dyer 2012). Another alternative for the creation of features that are sparse, but less sparse than features of phrases or rules, are lexical features (Watanabe et al. 2007). Lexical features, 6 Neubig and Watanabe Optimization for Statistical Machine Translation similar to lexical weighting, focus on the correspondence between the individual words that are included in a phrase or rule. The simplest variety of lexical features remembers which source words f are aligned with which target words e, and fires a feature for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe"
J16-1001,J97-3002,0,0.0677689,"ull the presidency in full delegation (b) lattice (c) forest the chair Figure 1 Example of a k-best list, lattice, and forest. Another class of decoding problem is forced decoding, in which the output from a decoder is forced to match with a reference translation of the input sentence. In phrasebased MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing approximate matching of the target side (Liang, Zhang, and Zhao 2012). It is also possible to create a neighborhood of a forced decoding derivation by adding additional hyperedges to the true derivation, which allows f"
J16-1001,P10-1049,0,0.0633434,"Missing"
J16-1001,2014.iwslt-evaluation.22,0,0.0376691,"ver ten years, MERT is still the dominant optimization algorithm. However, starting in WMT 2013, we can see a move to systems based on MIRA, and to a lesser extent ranking, particularly in the most competitive systems. In these systems, the preferred choice of an optimization algorithm seems to be MERT when using up to 20 features, and MIRA when using a large number of features (up to several hundred). There are fewer examples of systems using large numbers of features (tens of thousands, or millions) in actual competitive systems, with a few exceptions (Dyer et al. 2009; Neidert et al. 2014; Wuebker et al. 2014). In the case when a large number of sparse features are used, it is most common to use a softmax or risk-based objective and gradient-based optimization algorithms, often combining the features into summary features and performing a final tuning pass with MERT. The fact that algorithms other than MERT are seeing adoption in competitive systems for shared tasks is a welcome sign for the future of MT optimization research. However, there are still many open questions in the field, a few of which can be outlined here: Stable Training with Millions of Features: At the moment, there is still no st"
J16-1001,P11-2074,0,0.014221,"dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature, and can be expressed as follows hsum ( f , e, d) = w&gt; sparse hsparse ( f , e, d) (7) There has also been work that splits sparse features into not one, but multiple groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding Given an input sentence f , the task of decoding is defined as an inference problem of finding the best scoring derivation heˆ , dˆ i according to Equation (1). In general, the inference is intractable if we enumerate all possible derivations in T ( f ) and rank each derivation by the model. We assume that a derivation is composed of a set of steps d = d1 , d2 , · · · , d|d| (8) where each dj is a step—for example, a phrase pair in phrase-based MT or a synchronous rule in tree-based MT—ordered in a particular way. We also assume that 7 Computational Linguistics"
J16-1001,D11-1081,0,0.0885023,"re sparse, but less sparse than features of phrases or rules, are lexical features (Watanabe et al. 2007). Lexical features, 6 Neubig and Watanabe Optimization for Statistical Machine Translation similar to lexical weighting, focus on the correspondence between the individual words that are included in a phrase or rule. The simplest variety of lexical features remembers which source words f are aligned with which target words e, and fires a feature for each pair. It is also possible to condition lexical features on the surrounding context in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two can be calculated from source and local target context, but target bigrams require target bigram context and are thus non-local features. One final variety of features that has proven useful is syntax-based features (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and hierarchical phrase-based translations do not directly consider syntax (in the linguistic sense) in the const"
J16-1001,D13-1026,0,0.0149338,"eters will result in overfitting, learning parameters that heavily favor using these memorized multi-word phrases, which will not be present in a separate test set. 1 It should be noted that although most work on MT optimization is concerned with linear models (and thus we will spend the majority of this article discussing optimization of these models), optimization using non-linear models is also possible, and is discussed in Section 8.1. 2 It should also be noted there have been a few recent attempts to jointly perform rule extraction and optimization, doing away with this two-step process (Xiao and Xiong 2013). 4 Neubig and Watanabe Optimization for Statistical Machine Translation The traditional way to solve this problem is to train the TM on a large parallel corpus on the order of hundreds of thousands to tens of millions of sentences, then perform optimization of parameters on a separate set of data consisting of around one thousand sentences, often called the development set. When learning the weights for larger feature sets, however, a smaller development set is often not sufficient, and it is common to perform cross-validation, holding out some larger portion of the training set for parameter"
J16-1001,P01-1067,0,0.0850855,"lated, f ∈ F as one of the sentences, and E ( f ) as the collection of all possible target language sentences that can be obtained by translating f . Machine translation systems perform this translation process by dividing the translation of a full sentence into the translation and recombination of smaller parts, which are represented as hidden variables, which together form a derivation. For example, in phrase-based translation (Koehn, Och, and Marcu 2003), the hidden variables will be the alignment between the phrases of the source and target sentences, and in tree-based translation models (Yamada and Knight 2001; Chiang 2007), the hidden variables will represent the latent tree structure used to generate the translation. We will define D( f ) to be the space of possible derivations that can be acquired from source sentence f , and d ∈ D( f ) to be one of those derivations. Any particular derivation d will correspond to exactly one e ∈ E ( f ), although the opposite is not true (the derivation uniquely determines the translation, but there can be multiple derivations corresponding to a particular translation). We also define tuple he, di consisting of a target sentence and its corresponding derivation"
J16-1001,D13-1112,0,0.0876147,"port fully the chair . X will X . the X X will support X X the X of X will X support X fully fully the presidency chinese support delegation china in full the presidency in full delegation (b) lattice (c) forest the chair Figure 1 Example of a k-best list, lattice, and forest. Another class of decoding problem is forced decoding, in which the output from a decoder is forced to match with a reference translation of the input sentence. In phrasebased MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be resolved by preserving the prefix of partial derivations (Yu et al. 2013), or by allowing approximate matching of the target side (Liang, Zhang, and Zhao 2012). It"
J16-1001,D09-1006,0,0.0239333,"e rather surprising result that despite the fact that other evaluation measures had proven superior to BLEU with regards to post facto correlation with human evaluation, a BLEU-optimized system proved superior to systems tuned using other metrics. Since this result, however, there have been other reports stating that systems optimized using other metrics such as TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT (Lo et al. 2013) achieve superior results to BLEU-optimized systems. There have also been attempts to directly optimize not automatic, but human evaluation measures of translation quality (Zaidan and Callison-Burch 2009). However, the cost of performing this sort of human-in-the-loop optimization is prohibitive, so Zaidan and Callison-Burch (2009) propose a method that re-uses partial hypotheses in evaluation. Saluja, Lane, and Zhang (2012) also propose a method for incorporating binary good/bad input into optimization, with the motivation that this sort of feedback is easier for human annotators to provide than generating new reference sentences. Finally, there is also some work on optimizing multiple evaluation metrics at one time. The easiest way to do so is to simply use the linear interpolation of two or"
J16-1001,D07-1055,0,0.0643423,"Missing"
J16-1001,N09-2006,0,0.0244941,"w(1) ∼ RM . Initialize randomly for t ∈ {1 . . . T} do . Until convergence for m ∈ {1 . . . M} do . For each dimension γˆ m ← arg minγ `error (F, E, C; w(t) + γbm ) . Search end for . Descent γˆ ← arg minγˆ m `error (F, E, C; w(t) + γˆ m bm ) m (t+1) (t) w ← w + γˆ b . Update end for ˆ then if `error (F, E, C; w(T+1) ) &lt; `error (F, E, C; w) ˆ ← w(T+1) w end if end for ˆ return w end procedure Figure 4 Minimum error rate training (MERT). inspired by Powell’s method (Och 2003; Press et al. 2007), or the Downhill-Simplex method (Nelder-Mead method) (Press et al. 2007; Zens, Hasan, and Ney 2007; Zhao and Chen 2009). The algorithm for MERT using line search is shown in Figure 4. Here, we assume that w and h(· ) are M-dimensional, and bm is an M-dimensional vector where the m-th element is 1 and the rest of the elements are zero. For the T iterations, we decide the dimension m of the feature vector (line 6), and for each possible weight vector w(j) + γbm choose the γ ∈ R that minimizes `error (· ) using line search (line 7). Then, among the γ for each of the M search dimensions, we perform an update using γˆ that affords the largest reduction in error (lines 9 and 10). This algorithm can be deemed a varie"
J16-1001,I11-1072,0,0.0216474,"forming domain adaptation is by selecting a subset of the training data that is similar to the data that we want to translate (Li et al. 2010). This can be done by selecting sentences that are similar to our test corpus, or even selecting adaptation data for each individual test sentence (Liu et al. 2012). If no parallel data exist in the target domain, it has also been shown that first automatically translating data from the source to the target language or vice versa, then using this data for optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li et al. 2011; Zhao et al. 2011) In addition, in a computer-assisted translation scenario, it is possible to reflect post-edited translations back into the optimization process as new indomain training data (Mathur, Mauro, and Federico 2013; Denkowski, Dyer, and Lavie 2014). Once adaptation data have been chosen, it is necessary to decide how to use the data. The most straightforward way is to simply use these in-domain data in optimization, but if the data set is small it is preferable to combine both in- and out-ofdomain data to achieve more robust parameter estimates. This is essentially equivalent to the standard domain-"
J16-1001,W06-1610,0,0.0263346,"Missing"
J16-1001,W06-3601,0,\N,Missing
J16-1001,D11-1035,0,\N,Missing
J16-1001,W05-0836,0,\N,Missing
J16-1001,W10-1711,0,\N,Missing
J16-1001,2010.iwslt-evaluation.22,0,\N,Missing
J16-1001,W14-3302,0,\N,Missing
levin-etal-2000-lessons,P99-1073,0,\N,Missing
levin-etal-2000-lessons,W00-0203,1,\N,Missing
levin-etal-2000-lessons,P97-1035,0,\N,Missing
N12-1026,J96-1002,0,0.0382032,"-cho, Soraku-gun, Kyoto, 619-0289 JAPAN {taro.watanabe}@nict.go.jp Abstract Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Lear"
N12-1026,P08-1024,0,0.0624277,"Missing"
N12-1026,J93-2003,0,0.0573833,"Missing"
N12-1026,P05-1022,0,0.150898,"Missing"
N12-1026,D08-1064,0,0.270643,"ltiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer et al., 2006), in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the ranking loss. Learning is efficiently parallelized by splitting training data among shards and by merging parameters in each round (McDo"
N12-1026,D08-1024,0,0.645463,"Missing"
N12-1026,J07-2003,0,0.578713,"the optimum parameters under corpus-BLEU without a 257 5 Experiments Experiments were carried out on the NIST 2008 Chinese-to-English Open MT task. The training data consists of nearly 5.6 million bilingual sentences and additional monolingual data, English Gigaword, for 5-gram language model estimation. MT02 and MT06 were used as our tuning and development testing, and MT08 as our final testing with all data consisting of four reference translations. We use an in-house developed hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT (Chiang, 2007). The system employs 14 features, consisting of standard Hiero-style features (Chiang, 2007), and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by in"
N12-1026,P11-2031,0,0.0604021,"ed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not l"
N12-1026,J05-1003,0,0.0174882,"le translations are selected by minimizing the task loss, { }N { }N ℓ( e′ ∈ NB EST(w; f i ) i=1 , ei i=1 ) 8: i.e. negative BLEU, with respect to a set of reference translations e. In order to compute oracles with corpus-BLEU, we apply a greedy search strategy over n-bests (Venugopal, 2005). Equation 5 can be easily interpreted as a constant loss “1” for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by 1 N ∑ ZO (w; f, e) ZN (w; f ) Algorithm 2 Stochastic Gradient Descent 1: k = 1, w1 ← 0 2: for t = 1, ..., T do 3: Choose Bt = {bt1 , ..., btK } from D 4: for b ∈ Bt do 5: Compute n-bests and oracles of b 6: Set learning rate ηk 7: wk+ 1 ← wk − ηk ∇(wk ; b) 2 ▷ Our proposed algorithm solve } Eq. 12 or 16 { wk+1 ← min 1, ∥w1/ k ←k+1 10: end for 11: end for 12: return wk √ λ ∥2 k+ 1 2 wk+ 1 2 9: (line 5) using a batch local corpus-BLEU (Haddow et al., 2011). Then, we optimize an approximated objective function λ arg min ∥w∥22 + ℓ(w; b) 2 w (7) by replacing D with b in the objectiv"
N12-1026,W11-2139,0,0.0144192,"sting with all data consisting of four reference translations. We use an in-house developed hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT (Chiang, 2007). The system employs 14 features, consisting of standard Hiero-style features (Chiang, 2007), and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences and by keeping the target side of the inter3 The other major difference is the use of a simpler learning 1 , which was very slow in our preliminary studies. λk 4 Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. 5 We used liblinear (Fan et al., 2008) at http://w"
N12-1026,D11-1004,0,0.147242,") relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the"
N12-1026,N10-1112,0,0.0267231,"Missing"
N12-1026,W11-2130,0,0.640638,"ly used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer et al., 2006), in which each parameter update is further rescaled considering the tradeoff between the amount of updates to the parameters and the"
N12-1026,D11-1125,0,0.265408,"ctly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. Setting the larger batch size implies the more accurate corpus-BLEU, but at the cost of slower convergence of SGD. Therefore, we propose an optimized update method inspired by the passive-aggressive algorithm (Crammer et al., 2006), in which each p"
N12-1026,P07-1019,0,0.0275236,"ar with the solver type of 3. rate, 258 MERT PRO MIRA-L ORO-Lhinge O-ORO-Lhinge ORO-Lsoftmax O-ORO-Lsoftmax MT06 31.45† 31.76† 31.42† 29.76 32.06 30.77 31.16† MT08 24.13† 24.43† 24.15† 21.96 24.95 23.07 23.20 Table 1: Translation results by BLEU. Results without significant differences from the MERT baseline are marked †. The numbers in boldface are significantly better than the MERT baseline (both measured by the bootstrap resampling (Koehn, 2004) with p > 0.05). 40 35 30 25 BLEU sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework (Huang and Chiang, 2007) with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our optimization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with varied objectives, hinge loss or softmax loss3 . MERT learns parameters from forests (Kumar et al., 2009) with 4 restarts and 8 random directions in each iteration. We experimented on a variant of PRO4 , in which the objective in Eq. 4 with the"
N12-1026,P07-2045,0,0.00643525,"st translations in each iteration. The hyperparameter λ for PRO and ORO was set to 10−5 , selected from among {10−3 , 10−4 , 10−5 }, and 102 for MIRA, chosen from {10, 102 , 103 } by preliminary testing on MT06. Both decoding and learning are parallelized and run on 8 cores. Each online learning took roughly 12 hours, and PRO took one day. It took roughly 3 days for MERT with 20 iterations. Translation results are measured by case sensitive BLEU. Table 1 presents our main results. Among the parameters from multiple iterations, we report the outputs that performed the best on MT06. With Moses (Koehn et al., 2007), we achieved 30.36 and 23.64 BLEU for MT06 and MT08, respectively. We denote the “O-” prefix for the optimized parameter updates discussed in Section 4.1, and the “-L” suffix 20 15 MIRA-L MT02 MT08 ORO-L MT02 MT08 O-ORO-L MT02 MT08 10 5 0 0 5 10 15 20 25 30 iteration Figure 1: Learning curves for three algorithms, MIRA-L, ORO-Lhinge and O-ORO-Lhinge . for parameter mixing by line search as described in Section 4.2. The batch size was set to 16 for MIRA and ORO. In general, our PRO and MIRA settings achieved the results very comparable to MERT. The hinge-loss and softmax objective OROs were lo"
N12-1026,W04-3250,0,0.143142,"Missing"
N12-1026,P09-1019,0,0.133746,"5). 40 35 30 25 BLEU sected rules. n-bests are generated from the precomputed forests on the fly using the forest rescoring framework (Huang and Chiang, 2007) with additional non-local features, such as 5-gram language models. We compared four algorithms, MERT, PRO, MIRA and our proposed online settings, online rank optimization (ORO). Note that ORO without our optimization methods in Section 4 is essentially the same as Pegasos, but differs in that we employ the algorithm for ranking structured outputs with varied objectives, hinge loss or softmax loss3 . MERT learns parameters from forests (Kumar et al., 2009) with 4 restarts and 8 random directions in each iteration. We experimented on a variant of PRO4 , in which the objective in Eq. 4 with the hinge loss of Eq. 5 was solved in each iteration in line 4 of Alg. 1 using an off-the-shelf solver5 . Our MIRA solves the problem in Equation 13 in line 7 of Alg. 2. For a systematic comparison, we used our exhaustive oracle translation selection method in Section 3 for PRO, MIRA and ORO. For each learning algorithm, we ran 30 iterations and generated duplicate removed 1,000-best translations in each iteration. The hyperparameter λ for PRO and ORO was set"
N12-1026,D09-1005,0,0.0247259,"be}@nict.go.jp Abstract Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is"
N12-1026,P06-1096,0,0.452015,"Missing"
N12-1026,N10-1069,0,0.125314,"iterations, but keep only n-bests in each iteration for faster training and for memory saving. Thus, the optimum ρ obtained by the line search may be suboptimal in terms of the training objective, but potentially better than averaging for minimizing the final task loss. ∑ subject to (f,e)∈b τf ≤ ηk . Similar to Eq. 15, the parameter update by Ψ(·) is rescaled by its Lagrange multipliers τf in place of the uniform scale of 1/|b| in the sub-gradient of Eq. 10. 4.2 Line Search for Parameter Mixing For faster training, we employ an efficient parallel training strategy proposed by McDonald et al. (2010). The training data D is split into S disjoint shards, {D1 , ..., DS }. Each shard learns its own parameters in each single epoch t and performs parameter mixing by averaging parameters across shards. We propose an optimized parallel training in Algorithm 3 which performs better mixing with respect to the task loss, i.e. negative BLEU. In line 1 5, wt+ 2 is computed by averaging wt+1,s from all the shards after local training using their own data Ds . Then, the new parameters wt+1 are obtained by linearly interpolating with the parameters from the previous epoch wt . The linear interpolation w"
N12-1026,C08-1074,0,0.0366164,"dicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an online fashion. As proposed by Haddow et al. (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentencewise score (Chiang et al., 2008a), and optimization for sentence-BLEU does not always achi"
N12-1026,P02-1038,0,0.149705,"gorithms, such as MIRA and PRO. 2 Algorithm 1 MERT 1: 2: 3: 4: 5: 6: Initialize w1 for t = 1, ..., T do ▷ Or, until convergence Generate n-bests using wt Learn new wt+1 by Powell’s method end for return wT +1 Statistical Machine Translation SMT can be formulated as a maximization problem of finding the most likely translation e given an input sentence f using a set of parameters θ (Brown et al., 1993) eˆ = arg max p(e|f ; θ). (1) e Under this maximization setting, we assume that p(·) is represented by a linear combination of feature functions h(f, e) which are scaled by a set of parameters w (Och and Ney, 2002) eˆ = arg max w⊤ h(f, e). (2) mated by search over the n-bests merged across iterations. The merged n-bests are also used in the line search procedure to efficiently draw the error surface for efficient computation of the outer minimization of Eq. 3. 3 Online Rank Learning 3.1 Rank Learning Instead of the direct task loss minimization of Eq. 3, we would like to find w by solving the L2 regularized constrained minimization problem e Each element of h(·) is a feature function which captures different aspects of translations, for instance, log of n-gram language model probability, the number of t"
N12-1026,P03-1021,0,0.885033,"ose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al.,"
N12-1026,P02-1040,0,0.0976003,"n is further optimized by a passive-aggressive algorithm. Learning is efficiently parallelized and line search is performed in each round when merging parameters across parallel jobs. Experiments on the NIST Chinese-to-English Open MT task indicate significantly better translation results. 1 Introduction The advancement of statistical machine translation (SMT) relies on efficient tuning of several or many parameters in a model. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al., 2002). MERT has been successfully used in practical applications, although, it is known to be unstable (Clark et al., 2011). To overcome this instability, it requires multiple runs from random starting points and directions (Moore and Quirk, 2008), or a computationally expensive procedure by linear programming and combinatorial optimization (Galley and Quirk, 2011). We present an online learning algorithm based on stochastic gradient descent (SGD) with a larger batch size (Shalev-Shwartz et al., 2007). Like Hopkins and May (2011), we optimize ranking in nbest lists, but learn parameters in an onlin"
N12-1026,W11-2119,0,0.0825212,"but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. Training is performed by SGD with a parameter projection method (Shalev-Shwartz et al., 2007). Slower training incurred by the larger batch size MT06 batch size MIRA-L ORO-Lhinge O-ORO-Lhinge ORO-Lsoftmax O-ORO-Lsoftmax 1 31.28† 31.32† 31.44† 25.10 31.15† 4 31.53† 30.69 31.54† 31.66† 31.17† MT08 8 31.63† 29.61 31.35† 31.31† 30.90 16 31.42† 29.76 32.06 30.77 31.16† 1 23.46 23.63 23.72 19.27 23.62 4 23.97† 23.12 24.02† 23.59 23.31 8 24.58 22.07 24.28† 23.50 23.03 16 24.15† 21.96 24.95 23.07 23.20 Table 3: Translation results with varied batch size. for"
N12-1026,P06-2101,0,0.44267,"-0289 JAPAN {taro.watanabe}@nict.go.jp Abstract Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each iteration is further optimized by a passive-aggressive algorithm. Learning is efficiently paralleliz"
N12-1026,W04-3201,0,0.419167,"e of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 JAPAN {taro.watanabe}@nict.go.jp Abstract Many alternative methods have been proposed based on the algorithms in machine learning, such as averaged perceptron (Liang et al., 2006), maximum entropy (Och and Ney, 2002; Blunsom et al., 2008), Margin Infused Relaxed Algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008b), or pairwise rank optimization (PRO) (Hopkins and May, 2011). They primarily differ in the mode of training; online or MERT-like batch, and in their objectives; max-margin (Taskar et al., 2004), conditional loglikelihood (or softmax loss) (Berger et al., 1996), risk (Smith and Eisner, 2006; Li and Eisner, 2009), or ranking (Herbrich et al., 1999). We present an online learning algorithm for statistical machine translation (SMT) based on stochastic gradient descent (SGD). Under the online setting of rank learning, a corpus-wise loss has to be approximated by a batch local loss when optimizing for evaluation measures that cannot be linearly decomposed into a sentence-wise loss, such as BLEU. We propose a variant of SGD with a larger batch size in which the parameter update in each ite"
N12-1026,P09-1054,0,0.0698891,"Missing"
N12-1026,2005.eamt-1.36,0,0.0516407,"nslations, we follow the convention of approximating the domain of translation by n-bests. Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the nbests other than those in O RACLE(·). Oracle translations are selected by minimizing the task loss, { }N { }N ℓ( e′ ∈ NB EST(w; f i ) i=1 , ei i=1 ) 8: i.e. negative BLEU, with respect to a set of reference translations e. In order to compute oracles with corpus-BLEU, we apply a greedy search strategy over n-bests (Venugopal, 2005). Equation 5 can be easily interpreted as a constant loss “1” for choosing a wrong translation under current parameters w, which is in contrast with the direct task-loss used in max-margin approach to structured output learning (Taskar et al., 2004). As an alternative, we would also consider a softmax loss (Collins and Koo, 2005) represented by 1 N ∑ ZO (w; f, e) ZN (w; f ) Algorithm 2 Stochastic Gradient Descent 1: k = 1, w1 ← 0 2: for t = 1, ..., T do 3: Choose Bt = {bt1 , ..., btK } from D 4: for b ∈ Bt do 5: Compute n-bests and oracles of b 6: Set learning rate ηk 7: wk+ 1 ← wk − ηk ∇(wk ;"
N12-1026,D07-1080,1,0.889827,"for a larger batch size. As discussed in Section 3, the smaller batch size means frequent updates to parameters and a faster convergence, but potentially leads to a poor performance since the corpus-BLEU is approximately computed in a local batch. Our optimized update algorithms address the problem by adjusting the tradeoff between the amount of update to parameters and the loss, and perform better for larger batch sizes with a more accurate corpusBLEU. 6 Related Work Our work is largely inspired by pairwise rank optimization (Hopkins and May, 2011), but runs in an online fashion similar to (Watanabe et al., 2007; Chiang et al., 2008b). Major differences come from the corpus-BLEU computation used to select oracle translations. Instead of the sentence-BLEU used by Hopkins and May (2011) or the corpus-BLEU statistics accumulated from previous translations generated by different parameters (Watanabe et al., 2007; Chiang et al., 2008b), we used a simple batch local corpus-BLEU (Haddow et al., 2011) in the same way as an online approximation to the objectives. An alternative is the use of a Taylor series approximation (Smith and Eisner, 2006; Rosti et al., 2011), which was not investigated in this paper. T"
N12-1026,D11-1081,0,0.0622567,"hypergraph-based toolkit for training and decoding with synchronousCFGs (SCFG) for hierarchical phrase-bassed SMT (Chiang, 2007). The system employs 14 features, consisting of standard Hiero-style features (Chiang, 2007), and a set of indicator features, such as the number of synchronous-rules in a derivation. Two 5-gram language models are also included, one from the English-side of bitexts and the other from English Gigaword, with features counting the number of out-of-vocabulary words in each model (Dyer et al., 2011). For faster experiments, we precomputed translation forests inspired by Xiao et al. (2011). Instead of generating forests from bitexts in each iteration, we construct and save translation forests by intersecting the source side of SCFG with input sentences and by keeping the target side of the inter3 The other major difference is the use of a simpler learning 1 , which was very slow in our preliminary studies. λk 4 Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. 5 We used liblinear (Fan et al., 2008) at http://www. csie.ntu.edu.tw/˜cjlin/liblinear with the solver type of 3. rate, 258 MERT PRO MIRA-L O"
P03-1039,J93-2003,0,0.00826573,"Missing"
P03-1039,W02-1018,0,0.0296906,"e now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach is suitable for inputs with less deviation from a training corpus, while the"
P03-1039,W99-0604,0,0.0676192,"ys be march hello hello i ’d like to change my reservation on march nineteenth 二 三 分 待っ て下さい 今 電話 中 な ん です wait a couple of minutes i ’m telephoning now is this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other han"
P03-1039,P02-1040,0,0.0699314,"Missing"
P03-1039,takezawa-etal-2002-toward,1,0.761178,"this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach i"
P03-1039,C00-2123,0,0.0624808,"del 4 parameters were used as the initial parameters for training. We directly applied the Lexicon Model and Fertility Model to the chunk-based translation model but set other parameters as uniform. 3.4 Table 1: Basic Travel Expression Corpus # of sentences # of words vocabulary size # of singletons 3-gram perplexity + weight × Decoding 2. Generate hypothesized output by consuming input chunks in arbitrary order and combining possible output chunks in left-to-right order. The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000). In addition, an example-based method is also introduced, which generates candidate chunks by looking up the viterbi chunking and alignment from a training corpus. Since the combination of all possible chunks is computationally very expensive, we have introduced the following pruning and scoring strategies. beam pruning: Since the search space is enormous, we have set up a size threshold to maintain partial hypotheses for both of the above two stages. We also incorporated a threshold for scoring, which allows partial hypotheses with a certain score to be processed. example-based scoring: Inpu"
P03-1039,2002.tmi-papers.20,1,0.840222,"ephoning now is this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach i"
P03-1039,P01-1067,0,\N,Missing
P06-1098,P05-1066,0,0.0109351,"n Chinese engineers , construction design and construction methods of the recipient from . The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief measures to the village and the city of Niigata . The Health and Welfare Ministry in that the Japanese people in the village are made law . The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata . Figure 3: Sample translations from two systems: Phrase and Normalized-2 as a set of rules that reorders the foreign language to match with English language sequentially. Collins et al. (2005) presented a method with hand-coded rules. Our method directly learns such serialization rules from a bilingual corpus without linguistic clues. The translation quality presented in Section 5 are rather low due to the limited size of the bilingual corpus, and also because of the linguistic difference of two languages. As our future work, we are in the process of experimenting our model for other languages with rich resources, such as Chinese and Arabic, as well as similar language pairs, such as French and English. Additional feature functions will be also investigated that were proved success"
P06-1098,W05-1507,0,0.0389301,"n-terminals in γ and α that are associated with ∼. Chiang (2005) proposed a hierarchical phrasebased translation model, a binary synchronousCFG, which restricted the form of production rules as follows: The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J 3+3(n−1) ) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. • Only two types of non-terminals allowed: S and X. • Both of the strings γ and α must contain at least one terminal item. • Rules may have at most two non-terminals but non-terminals cannot be adjacent for the foreign language side γ. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebas"
P06-1098,N03-1017,0,0.316076,"1I , f1J ) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated phrases are reordered to form f1J . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordere"
P06-1098,W04-3250,0,0.0419224,"Missing"
P06-1098,P02-1038,0,0.0250315,"pled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models. Our model was experimented on a Japanese-to-English newswire translation task, and showed statistically significant performance improvements against a phrase-based translation system. 1 where hm (e1I , f1J ) is a feature function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated"
P06-1098,J03-1002,0,0.00440966,"ls but non-terminals cannot be adjacent for the foreign language side γ. As an alternative to a binarized form, we present a target-normalized hierarchical phrasebased translation model. The model is a class of a hierarchical phrase-based model, but constrained so that the English part of the right-hand side The production rules are induced from a bilingual corpus with the help of word alignments. To alleviate a data sparseness problem, glue rules are 778 added that prefer combining hierarchical phrases in a serial manner: D E S → S 1 X2 , S 1 X2 (5) E D (6) S → X1 , X1 model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Koehn et al., 2003). Second, phrase translation pairs are extracted from the word alignment corpus (Koehn et al., 2003). The method exhaustively extracts phrase j+m J I pairs ( f j , ei+n i ) from a sentence pair ( f1 , e1 ) that do not violate the word alignment constraints a: where boxed indices indicate non-terminal’s linkages represented in ∼. Our model is based on Chiang (2005)’s framework, but further restricts the form of production rules so that the aligned right-hand side α follows a GNF-like structure: D E ¯ ∼ X"
P06-1098,N04-4026,0,0.119691,"Missing"
P06-1098,J93-2003,0,0.00796207,"or is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated phrases are reordered to form f1J . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an En1 Introduction In a classical statistical machine translation, a foreign language sentence f1J ="
P06-1098,P05-1033,0,0.605252,"ram language model is straightforward, since the model generates a translation in left-to-right order. Our decoder is based on an Earley-style top down parsing on the foreign language side. The projected English-side is generated in left-to-right order synchronized with the derivation of the foreign language side. The decoder’s implementation is taken after a decoder for an existing phrase-based model with a simple modification to account for production rules. Experimental results on a Japanese-toEnglish newswire translation task showed significant improvement against a phrase-based modeling. Chiang (2005) introduced a hierarchical phrasebased translation model that combined the strength of the phrase-based approach and a synchronous-CFG formalism (Aho and Ullman, 1969): A rewrite system initiated from a start symbol which synchronously rewrites paired nonterminals. Their translation model is a binarized synchronous-CFG, or a rank-2 of synchronousCFG, in which the right-hand side of a production rule contains at most two non-terminals. The form can be regarded as a phrase translation pair with at most two holes instantiated with other phrases. The hierarchically combined phrases provide a sort"
P06-1098,P03-1010,0,0.00564793,"f D, and height(Di ) and width(Di ) refer the height and width of subtree Di , respectively. In Figure 1(b), for instance, a rule of X 1 with non-terminals X 2 and X 4 , two rules X 2 and X 3 spanning two terminal symbols should be backtracked to proceed to X 4 . The rationale is that positive scaling factors prefer a deeper structure whereby negative scaling factors prefer a monotonized structure. 4.5 The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to Table 1) (Utiyama and Isahara, 2003). From one-toone aligned sentences, 1,500 sentence pairs were sampled for a development set and a test set1 . Since the bilingual corpus is rather small, especially for the newspaper translation domain, Japanese/English dictionaries consisting of 1.3M entries were added into a training set to alleviate an OOV problem2 . Word alignments were annotated by a HMM translation model (Och and Ney, 2003). After Length-based Models Three trivial length-based feature functions were used in our experiment. hl (e1I ) = I (21) hr (D) = rule(D) (22) h p (D) = phrase(D) (23) 1 Japanese sentences were segment"
P06-1098,J97-3002,0,0.208821,"ectively. ∼ is a one-to-one correspondence for the non-terminals appeared in γ and α. Starting from an initial non-terminal, each rule rewrites non-terminals in γ and α that are associated with ∼. Chiang (2005) proposed a hierarchical phrasebased translation model, a binary synchronousCFG, which restricted the form of production rules as follows: The integration with a ngram language model further increases the cost of decoding especially when incorporating a higher order ngram, such as 5-gram. In the hierarchical phrase-based model (Chiang, 2005), and an inversion transduction grammar (ITG) (Wu, 1997), the problem is resolved by restricting to a binarized form where at most two non-terminals are allowed in the righthand side. However, Huang et al. (2005) reported that the computational complexity for decoding amounted to O(J 3+3(n−1) ) with n-gram even using a hook technique. The complexity lies in memorizing the ngram’s context for each constituent. The order of ngram would be a dominant factor for higher order ngrams. • Only two types of non-terminals allowed: S and X. • Both of the strings γ and α must contain at least one terminal item. • Rules may have at most two non-terminals but no"
P06-1098,P03-1019,0,0.100533,"ure function, such as a ngram language model or a translation model. When decoding, the denominator is dropped since it depends only on f1J . Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). This modeling allows the integration of various feature functions depending on the scenario of how a translation is constituted. A phrase-based translation model is one of the modern approaches which exploits a phrase, a contiguous sequence of words, as a unit of translation (Koehn et al., 2003; Zens and Ney, 2003; Tillman, 2004). The idea is based on a word-based source channel modeling of Brown et al. (1993): It assumes that e1I is segmented into a sequence of K phrases e¯ 1K . Each phrase e¯ k is transformed into f¯k . The translated phrases are reordered to form f1J . One of the benefits of the modeling is that the phrase translation unit preserves localized word reordering. However, it cannot hypothesize a long-distance reordering required for linguistically divergent language pairs. For instance, when translating Japanese to English, a Japanese SOV structure has to be reordered to match with an E"
P06-1098,P03-1021,0,\N,Missing
P09-2086,D07-1090,0,0.558593,"responds to its node position in the unigram table, we can remove the word ids for the first order. Our implementation merges across different orders of N -grams, then separates into multiple tables such as word ids, smoothed probabilities, back-off coefficients, and pointers. The starting positions of different orders are memorized to allow access to arbitrary orders. To store N -gram counts, we use three tables for word ids, counts and pointers. We share the same tables for word ids and pointers with additional probability and back-off coefficient tables. To support distributed computation (Brants et al., 2007), we further split the N -gram data into “shards” by hash values of the first bigram. Unigram data are shared across shards for efficiency. 3 an imaginary super root node emits 10 for its only child, i.e., the root node. After the root node, its first child or node 1 follows. Since (M + 1)0s and M 1s are emitted for a trie with M nodes, LOUDS occupies 2M + 1 bits. We define a basic operation on the bit string. sel1 (i) returns the position of the i-th 1. We can also define similar operations over zero bit strings, sel0 (i). Given selb , we define two operations for a node x. parent(x) gives x’"
P09-2086,D07-1021,0,0.0512316,"xperimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N -gram counts. By using 8-bit floating point quantization 1 , N -gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008). Introduction There has been an increase in available N -gram data and a large amount of web-scaled N -gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lossy one if other conditions are the same. In addtion, a lossless approach can easly combined with pruning. Therefore, lossless representation of N -gram is a key issue even for lossy approaches. Raj and Whittaker (2003) showed a general N gram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by"
P09-2086,P08-1058,0,0.0116879,"f probability pointer Figure 1: Data structure for language model and LOUDS succinctly represents it by a 2M + 1 bit string. The space is further reduced by considering the N -gram structure. We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts. We experimented with English Web 1T 5-gram from LDC consisting of 25 GB of gzipped raw text N -gram counts. By using 8-bit floating point quantization 1 , N -gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008). Introduction There has been an increase in available N -gram data and a large amount of web-scaled N -gram data has been successfully deployed in statistical machine translation. However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them. Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time. However, the lossy approaches may reduce accuracy, and tuning is necessary. A lossless approach is obviously better than a lo"
P09-2086,W07-0712,0,\N,Missing
P11-1064,N10-1028,0,0.636111,"tributions, and thus the parameters for the Pitman-Yor process will be different for each distribution. Further, as ll and lr must be smaller than l, Pt,l no longer contains itself as a base measure, and is thus not deficient. An example of the actual discount values learned in one of the experiments described in Section 7 is shown in Figure 2. It can be seen that, as expected, the discounts for short phrases are lower than 636 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase co"
P11-1064,P09-1088,0,0.778629,"nts. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to genera"
P11-1064,J93-2003,0,0.0978707,"Missing"
P11-1064,W10-1703,0,0.0131603,"he proposed method on translation tasks from four languages, French, German, Spanish, and Japanese, into English. 638 TM (en) TM (other) LM (en) Tune (en ) Tune (other) Test (en) Test (other) de-en 1.80M 1.85M 52.7M 49.8k 47.2k 65.6k 62.7k es-en 1.62M 1.82M 52.7M 49.8k 52.6k 65.6k 68.1k fr-en 1.35M 1.56M 52.7M 49.8k 55.4k 65.6k 72.6k ja-en 2.38M 2.78M 44.7M 68.9k 80.4k 40.4k 48.7k Table 1: The number of words in each corpus for TM and LM training, tuning, and testing. 7.1 Experimental Setup The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We use the news commentary corpus for training the TM, and the news commentary and Europarl corpora for training the LM. For Japanese, we use data from the NTCIR patent translation task (Fujii et al., 2008). We use the first 100k sentences of the parallel corpus for the TM, and the whole parallel corpus for the LM. Details of both corpora can be found in Table 1. Corpora are tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of o"
P11-1064,W07-0403,0,0.436106,"able that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-"
P11-1064,J07-2003,0,0.350869,"a fraction of the size of most heuristic extraction methods. Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k senFor future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also examine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context. Acknowledgments This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists. References Figure 4: The effect of corpus size on the accuracy (a) and phrase table size (b) for each method (Japanese-English). tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus size"
P11-1064,N10-1081,0,0.293077,"rates from the symbol distribution Px , then from the phrase distribution Pt , while HIER generates directly from Pt , which falls back to divide-and-conquer based on Px when necessary. It can be seen that while Pt in FLAT only generates minimal phrases, Pt in HIER generates (and thus memorizes) phrases at all levels of granularity. 4.1 Length-based Parameter Tuning There are still two problems with HIER, one theoretical, and one practical. Theoretically, HIER contains itself as its base measure, and stochastic process models that include themselves as base measures are deficient, as noted in Cohen et al. (2010). Practically, while the Pitman-Yor process in HIER shares the parameters s and d over all phrase pairs in the model, long phrase pairs are much more sparse those of long phrases. In particular, phrase pairs of length up to six (for example, |e |= 3, |f |= 3) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max(|e|, |f |) ≤ 3 cause significant improvements in BLEU score, while using larger phrases results in diminishing returns. Figure 2: Learned discount va"
P11-1064,P05-1066,0,0.0803727,"Missing"
P11-1064,P08-2007,0,0.0326897,"Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pf lat (he, f i; θx , θt ) is parameterized by a phrase table θt and a symbol distribution θx . We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px (x; θx ). x can take the values TERM, REG , or INV . 2. According to the x take the following actions. (a) If x = TERM, generate a phrase pair from the phrase table Pt (he, f i; θt ). (b) If x = REG, a regular ITG rule, generate phrase pairs he1 , f1 i and he2 , f2 i from"
P11-1064,P10-1147,0,0.261523,"the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one importan"
P11-1064,W06-3105,0,0.0185751,"lso for phrases that, while not directly included in the model, are composed of two high probability child phrases. It should be noted that while for FLAT and HIER Pt can be used directly, as HLEN learns separate models for each length, we must combine these probabilities into a single value. We do this by setting Pt (he, f i) = Pt,l (he, f i)c(l)/ L ∑ c(˜l) ˜ l=1 for every phrase pair, where l = |e |+ |f |and c(l) is the number of phrases of length l in the sample. We call this model-based extraction method MOD. 5.3 Sample Combination As has been noted in previous works, (Koehn et al., 2003; DeNero et al., 2006) exhaustive phrase extraction tends to out-perform approaches that use syntax or generative models to limit phrase boundaries. DeNero et al. (2006) state that this is because generative models choose only a single phrase segmentation, and thus throw away many good phrase pairs that are in conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the jo"
P11-1064,D08-1033,0,0.563293,"Missing"
P11-1064,P06-1121,0,0.17041,"ds. Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k senFor future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also examine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context. Acknowledgments This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists. References Figure 4: The effect of corpus size on the accuracy (a) and phrase table size (b) for each method (Japanese-English). tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus sizes, the results from all three methods are comparable, with insign"
P11-1064,D07-1103,0,0.0694819,"mentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, not one. Adaptor gramma"
P11-1064,P07-2045,0,0.00525629,"Missing"
P11-1064,N03-1017,0,0.562956,"not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size. 1 Introduction The training of translation models for phrasebased statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual training data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally generated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieve"
P11-1064,2005.iwslt-1.8,0,0.00647738,"tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets. We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT, HIER and HLEN models, with a baseline of using word alignments from GIZA ++ and heuristic phrase extraction. Decoding is performed using Moses (Koehn and others, 2007) using the phrase tables learned by each method under consideration, as well as standard bidirectional lexical reordering probabilities (Koehn et al., 2005). Maximum phrase length is limited to 7 in all models, and for the LM we use an interpolated Kneser-Ney 5-gram model. For GIZA ++, we use the standard training regimen up to Model 4, and combine alignments with grow-diag-final-and. For the proposed models, we train for 100 iterations, and use the final sample acquired at the end of the training process for our experiments using a single sample6 . In addition, 6 For most models, while likelihood continued to increase gradually for all 100 iterations, BLEU score gains plateaued after 5-10 iterations, likely due to the strong prior information Al"
P11-1064,N06-1014,0,0.0794575,"f |; λ) 1 M0 (he, f i) =(Pm1 (f |e)Puni (e)Pm1 (e|f )Puni (f )) 2 . Ppois is the Poisson distribution with the average length parameter λ. As long phrases lead to sparsity, we set λ to a relatively small value to allow us to bias against overly long phrases4 . Pm1 is the word-based Model 1 (Brown et al., 1993) probability of one phrase given the other, which incorporates word-based alignment information as prior knowledge in the phrase translation probability. We take the geometric mean5 of the Model 1 probabilities in both directions to encourage alignments that are supported by both models (Liang et al., 2006). It should be noted that while Model 1 probabilities are used, they are only soft constraints, compared with the hard constraint of choosing a single word alignment used in most previous phrase extraction approaches. For Pbu , if g is the non-null phrase in e and f , we calculate the probability as follows: Pbu (he, f i) = Puni (g)Ppois (|g|; λ)/2. Note that Pbu is divided by 2 as the probability is considering null alignments in both directions. 4 Hierarchical ITG Model While in FLAT only minimal phrases were memorized by the model, as DeNero et al. (2008) note We choose 10−2 , 10−3 , or 10−"
P11-1064,W02-1018,0,0.126279,"Missing"
P11-1064,W07-0715,0,0.0636315,"n conflict with this segmentation. Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approximating the integral over various parameter configurations in Equation (1). In MOD, we do this by taking the average of the joint probability and span probability features, and re-calculating the conditional probabilities from the averaged joint probabilities. 6 Related Work In addition to the previously mentioned phrase alignment techniques, there has also been a significant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsupervised generative model. A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work differs in that it uses Bayesian techniques instead of MDL, and works on two languages, n"
P11-1064,W99-0604,0,0.0831392,"on which value gives the best performance on the development set. 5 The probabilities of the geometric mean do not add to one, but we found empirically that even when left unnormalized, this provided much better results than the using the arithmetic mean, which is more theoretically correct. 3 and we confirm in the experiments in Section 7, using only minimal phrases leads to inferior translation results for phrase-based SMT. Because of this, previous research has combined FLAT with heuristic phrase extraction, which exhaustively combines all adjacent phrases permitted by the word alignments (Och et al., 1999). We propose an alternative, fully statistical approach that directly models phrases at multiple granularities, which we will refer to as HIER. By doing so, we are able to do away with heuristic phrase extraction, creating a fully probabilistic model for phrase probabilities that still yields competitive results. Similarly to FLAT, HIER assigns a probability Phier (he, f i; θx , θt ) to phrase pairs, and is parameterized by a phrase table θt and a symbol distribution θx . The main difference from the generative story of the traditional ITG model is that symbols and phrase pairs are generated i"
P11-1064,W09-3804,0,0.435467,"be seen that, as expected, the discounts for short phrases are lower than 636 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable convergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algorithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 1010 times smaller than that of the best hypothesis in the bucket. One important implementation detail that is different from previous models is the management of phrase counts. As a phrase pair ta may have been generated from two smaller component phrases tb and tc , when a sample containing ta is removed from the distribution, it may also be necessary to decrement the counts of tb and tc as well. The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solu"
P11-1064,P06-1124,0,0.832206,"tribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strategy to generate phrase pairs that do not exist (or are given low probability) in the phrase distribution. We combine this model with the Bayesian nonparametric Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), realizing ITG-based divide and conquer through a novel formulation where the Pitman-Yor process uses two copies of itself as a 632 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632–641, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics base measure. As a result of this modeling strategy, phrases of multiple granularities are generated, and thus memorized, by the Pitman-Yor process. This makes it possible to directly use probabilities of the phrase model as a replacement for the phrase table generated by heuri"
P11-1064,J97-3002,0,0.466135,"le values of the hidden parameters: ∫ P (e|f , hE, Fi) = P (e|f , θ)P (θ|hE, Fi). (1) θ If θ takes the form of a scored phrase table, we can use traditional methods for phrase-based SMT to find P (e|f , θ) and concentrate on creating a model for P (θ|hE, Fi). We decompose this posterior probability using Bayes law into the corpus likelihood and parameter prior probabilities Wong (2002), DeNero et al. (2008), inter alia), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase alignments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary reordering is allowed (DeNero and Klein, 2008). The traditional flat ITG generative probability for a particular phrase (or sentence) pair Pf lat (he, f i; θx , θt ) is parameterized by a phrase table θt and a symbol distribution θx . We use the following generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distribution Px (x;"
P11-1064,P08-1012,0,0.360881,"t with these alignments. However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating In this paper, we propose the first unsupervised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a nonterminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuristic extraction methods. In the proposed model, at each branch in the tree, we first attempt to generate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and con"
P11-1125,P89-1018,0,0.60505,"in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules"
P11-1125,W10-1703,0,0.114529,"during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley’s algorithm. 1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation (WMT10) in four directions, {Czech, French, German, Spanish}-toEnglish (Callison-Burch et al., 2010), and we found comparable performance to the conventional confusion network based system combination in two language pairs, and statistically significant improvements in the others. First, we will review the state-of-the-art method which is a system combination framework based on confusion networks (§2). Then, we will introduce a novel system combination method based on confusion forest (§3) and present related work in consensus translations (§4). Experiments are presented in Section 5 followed by discussion and our conclusion. .*. I.. .saw . t.he . . f. orest . . . .I. w . alked . .the . b. l"
P11-1125,J07-2003,0,0.621752,".VBN . . . .NP@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail nodes, corresponding to the left-hand side and the right-hand s"
P11-1125,W07-0414,0,0.0168751,"which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Papineni et al., 2002). We vary the horizontal orders, h = 1, 2, ∞ with vertical orders of v = 3, 4"
P11-1125,P81-1022,0,0.659596,"ts the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to those found in the parse trees. Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earley’s algorithm. 1249 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1249–1257, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computa"
P11-1125,A94-1016,0,0.111864,"st derivation d: dˆ = arg max w⊤ · h(d, F ) (1) d∈D where h(d, F ) is a set of feature functions scaled by weight vector w. We use cube-pruning (Chiang, 2007; Huang and Chiang, 2007) to approximately intersect with non-local features, such as n-gram language models. Then, k-best derivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even wi"
P11-1125,J99-4004,0,0.0212069,"roach for constructing a confusion forest: First, MT outputs are parsed. Second, 1251 [TOP → S•, 0] Figure 3: The deductive system for Earley’s generation algorithm a grammar is learned by treating each hyperedge as an instance of a CFG rule. Third, a forest is generated from the unique root symbol of the extracted grammar through non-terminal rewriting. 3.1 Forest Generation Given the extracted grammar, we apply a variant of Earley’s algorithm (Earley, 1970) which can generate strings in a left-to-right manner from the unique root symbol, TOP. Figure 3 presents the deductive inference rules (Goodman, 1999) for our generation algorithm. We use capital letters X ∈ N to denote non-terminals and x ∈ T for terminals. Lowercase Greek letters α, β and γ are strings of terminals and non-terminals (T ∪ N )∗ . u and v are weights associated with each item. The major difference compared to Earley’s parsing algorithm is that we ignore the terminal span information each non-terminal covers and keep track of the height of derivations by h. The scanning step will always succeed by moving the dot to the right. Combined with the prediction and completion steps, our algorithm may potentially generate a spuriousl"
P11-1125,D08-1011,0,0.235716,"ivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a"
P11-1125,W99-0623,0,0.0846419,"ute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space. 1 Introduction System combination techniques take the advantages of consensus among multiple systems and have been widely used in fields, such as speech recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The"
P11-1125,W05-1506,0,0.13737,"N.N . . .was . . . . .forest . . I.. . .the .D.T .VBN . . . .NP@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail nodes, corresponding to t"
P11-1125,P07-1019,0,0.0780672,"@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail nodes, corresponding to the left-hand side and the right-hand side of an instance of a r"
P11-1125,P05-3026,0,0.0241321,"acted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors. Even with parsing errors, we can still take a tree fragment-level consensus as long as a parser is consistent in that similar syntactic mistakes would be made for similar hypotheses. Rosti et al. (2007a) describe a re-generation approach to consensus translation in which a phrasal translation table i"
P11-1125,W01-1812,0,0.0430929,"BD@ V . 2.1 .PRP . .D.T .N.N . . .was . . . . .forest . . I.. . .the .D.T .VBN . . . .NP@2.2 . . . .saw . .walked . f . ound . .J.J [X → α • xβ, h] : u [X → αx • β, h] : u Predict: [X → α • Yβ, h] [Y → •γ, h + 1] : u u Y → γ ∈ G, h &lt; H .DT@2.2.1 . .NN@2.2.2 . Complete: . .NN . . .blue . . . .forest . g. reen . . . . .the . .the . . . . . t.rees . .forest [X → α • Yβ, h] : u [Y → γ•, h + 1] : v [X → αY • β, h] : u ⊗ v Goal: Figure 2: An example packed forest representing hypotheses in Figure 1(a). ments among parse trees. The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005) and machine translation (Chiang, 2007; Huang and Chiang, 2007). More formally, a hypergraph is a pair ⟨V, E⟩ where V is the set of nodes and E is the set of hyperedges. Each node in V is represented as X @p where X ∈ N is a non-terminal symbol and p is an address (Shieber et al., 1995) that encapsulates each node id relative to its parent. The root node is given the address ϵ and the address of the first child of node p is given p.1. Each hyperedge e ∈ E is represented as a pair ⟨head(e), tails(e)⟩ where head(e) ∈ V is a head node and tails(e) ∈ V ∗ is a list of tail"
P11-1125,P03-1054,0,0.0181285,"al symbols assigned to each parse tree before extracting rules. Here, we replace each non-terminal symbol by the state representation of Earley’s algorithm corresponding to the sequence of prediction steps starting from TOP. Figure 4(a) presents an example parse tree with each symbol replaced by the Earley’s state in Figure 4(b). For example, the label for VBD is replaced by •S + NP : •VP + •VBD : NP which corresponds to the prediction steps of TOP → •S, S → NP • VP and VP → •VBD NP. The context represented in the Earley’s state is further limited by the vertical and horizontal Markovization (Klein and Manning, 2003). We define the vertical order v in which the label is limited to memorize only v previous prediction steps. For instance, setting v = 1 yields NP : •VP + •VBD : NP in our example. Likewise, we introduce the horizontal order h which limits the number of sibling labels memorized on the left and the right of the dotted label. Limiting h = 1 implies that each deductive step is encoded with at most three symbols. No limits in the horizontal and vertical Markovization orders implies memorizing of all the deductions and yields a confusion forest representing the union of parse trees through the gram"
P11-1125,P09-1019,0,0.0211587,"escribed in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S → X, S → S X and X → x. k-best translations are extracted from the forest using the forest-based algorithms in Section 3.3. 5.2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language models hilm (d): English Gigaword Fourth edition1 , the English side of French-English 109 corpus and the news commentary English data2 . The count based features ht (d) and he (d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hm s (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combi"
P11-1125,P98-1116,0,0.0104244,"translation table is constructed from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is const"
P11-1125,A00-2023,0,0.05299,"ucted from the MT outputs aligned with an input source sentence. New translations are generated by decoding the source sentence again using the newly extracted phrase table. Our grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing"
P11-1125,N09-2003,0,0.0142979,"th system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b). 5.3 Results Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Papineni et al., 2002). We vary the horizontal orders, h = 1, 2, ∞ with vertical orders of v = 3, 4, ∞. Systems without statistically significant differences from the best result (p"
P11-1125,P09-1065,0,0.0382463,"r grammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding. In terms of generation, our approach is an instance of statistical generation (Langkilde and Knight, 1998; Langkilde, 2000). Instead of generating forests from semantic representations (Langkilde, 2000), we generate forests from a CFG encoding the consensus among parsed hypotheses. Liu et al. (2009) present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs. Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems. While our work is similar in that a new forest is constructed by sharing rules among systems, although their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al., 2010). 1253"
P11-1125,E09-1061,0,0.0154029,"034 Table 1: WMT10 system combination tuning/testing data 5 Experiments 5.1 Setup We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English (Callison-Burch et al., 2010). The data is summarized in Table 1. The system outputs are retokenized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003), and lower-cased. We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007). Input to our system is a collection of hypergraphs, a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3. Our baseline, also implemented in cicada, is a confusion network-based system combination method (§2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al., 2008) and merges multiple networks into a large single network. After performing epsilon removal, the network is transformed into a forest by parsing"
P11-1125,D07-1105,0,0.0143057,".2 Features The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language models hilm (d): English Gigaword Fourth edition1 , the English side of French-English 109 corpus and the news commentary English data2 . The count based features ht (d) and he (d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hm s (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additio"
P11-1125,E06-1005,0,0.343573,"reen . t.rees . .. . . .the . . . . . .forest . .was . .found . (a) Pairwise alignment using the first starred hypothesis as a skeleton. .saw .I . . .the . . .ϵ ϵ. . alked w .blue f. orest .trees f. ound . . . . .ϵ .ϵ .green .was ϵ. (b) Confusion network from (a) .saw .I . . .ϵ . .the . ϵ. . alked w .blue f. orest .was .found . . . . .green .trees .ϵ .ϵ (c) Incrementally constructed confusion network 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from t"
P11-1125,P08-1023,0,0.168136,"ch recognition (Fiscus, 1997; Mangu et al., 2000) or parsing (Henderson and Brill, 1999). One of the state-of-the-art system combination methods for MT is based on confusion networks, which are compact graph-based structures representing multiple hypotheses (Bangalore et al., 2001). Confusion networks are constructed based on string similarity information. First, one skeleton or We present a novel method for system combination which exploits the syntactic similarity of system outputs. Instead of constructing a string-based confusion network, we generate a packed forest (Billot and Lang, 1989; Mi et al., 2008) which encodes exponentially many parse trees in a polynomial space. The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus. We employ a grammar-based method to generate the confusion forest: First, system outputs are parsed. Second, a set of rules are extracted from the parse trees. Third, a packed forest is generated using a variant of Earley’s algorithm (Earley, 1970) starting from the unique root symbol. New hypotheses are selected by searching the best derivation in the forest. The grammar, a set of rules, is limited to th"
P11-1125,P04-1063,0,0.0242747,"ek ˆ for the best derivation d: dˆ = arg max w⊤ · h(d, F ) (1) d∈D where h(d, F ) is a set of feature functions scaled by weight vector w. We use cube-pruning (Chiang, 2007; Huang and Chiang, 2007) to approximately intersect with non-local features, such as n-gram language models. Then, k-best derivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005). 4 Related Work Consensus translations have been extensively studied with many granularities. One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004). Frederking and Nirenburg (1994) proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al., 2001; Matusov et al., 2006; He et al., 2008; Jayaraman and Lavie, 2005; Sim et al., 2007). Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest. The confusion forest approach may suffer from parsing errors such as the confusion network construction influe"
P11-1125,J03-1002,0,0.00509528,"rred hypothesis as a skeleton. .saw .I . . .the . . .ϵ ϵ. . alked w .blue f. orest .trees f. ound . . . . .ϵ .ϵ .green .was ϵ. (b) Confusion network from (a) .saw .I . . .ϵ . .the . ϵ. . alked w .blue f. orest .was .found . . . . .green .trees .ϵ .ϵ (c) Incrementally constructed confusion network 2 Combination by Confusion Network The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference. Matusov et al. (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003), is used to align the hypotheses. Sim et al. (2007) introduced TER (Snover et al., 2006) to measure the edit-based alignment. Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which serves as a building block for aligning the rest of the hypotheses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The netwo"
P11-1125,P02-1040,0,0.0828041,"vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al., 2009). We use three lower-cased 5-gram language models hilm (d): English Gigaword Fourth edition1 , the English side of French-English 109 corpus and the news commentary English data2 . The count based features ht (d) and he (d) count the number of terminals and the number of hyperedges in d, respectively. We employ M confidence measures hm s (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al., 2007a). Following Macherey and Och (2007), BLEU (Papineni et al., 2002) correlations are also incorporated in our system combination. Given M system outputs e1 ...eM , M BLEU scores are computed for d using each of the system outputs em as a reference ) ( 4 ∑ 1 hm log ρn (e, em ) b (d) = BP (e, em ) · exp 4 n=1 where e = yield(d) is a terminal yield of d, BP (·) and ρn (·) respectively denote brevity penalty and n-gram precision. Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for computing ρn (·) with a compact state representation (Li and Khudanpur, 2009). Our baseline confusion network system has an additional penalty feature, hp (m), w"
P11-1125,N07-1029,0,0.571826,"ork, not only the 1250 Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously inserted words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharin"
P11-1125,P07-1040,0,0.766415,"ork, not only the 1250 Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produce a longer sentence with spuriously inserted words, as in “I saw the blue trees was found” in Figure 1(c). Rosti et al. (2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network. 3 Combination by Confusion Forest The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus. Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharin"
P11-1125,W08-0329,0,0.116632,"heses. Other hypotheses are aligned against the skeleton using the pairwise alignment. Figure 1(b) illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton. The network consists of several arcs, each of which represents an alternative word at that position, including the empty symbol, ϵ. This pairwise alignment strategy is prone to spurious insertions and repetitions due to alignment errors such as in Figure 1(a) in which “green” in the third hypothesis is aligned with “forest” in the skeleton. Rosti et al. (2008) introduces an incremental method so that hypotheses are aligned incrementally to the growing confusion network, not only the 1250 Figure 1: An example confusion network construction skeleton hypothesis. In our example, “green trees” is aligned with “blue forest” in Figure 1(c). The confusion network construction is largely influenced by the skeleton selection, which determines the global word reordering of a new hypothesis. For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice. This large grammatical difference may produ"
P11-1125,2006.amta-papers.25,0,0.137982,"cations Technology 3-5 Hikaridai, Keihanna Science City, 619-0289 JAPAN {taro.watanabe,eiichiro.sumita}@nict.go.jp Abstract backbone sentence is selected. Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates. The alignment method is either model-based (Matusov et al., 2006; He et al., 2008) in which a statistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie, 2005; Sim et al., 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al., 2006). The new translation hypothesis is generated by selecting the best path through the network. The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules t"
P11-1125,C98-1112,0,\N,Missing
P11-1125,N10-1141,0,\N,Missing
P11-1125,2005.eamt-1.20,0,\N,Missing
P12-1018,P02-1051,0,0.0205265,"ey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumer"
P12-1018,I08-1033,0,0.0173092,"ed by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word"
P12-1018,N10-1028,0,0.511558,"r)I(as,S,u,U )I(aS,t,U,v ) s≤S≤t u≤U ≤v + X X Px (inv)I(as,S,U,v )I(aS,t,u,U ) s≤S≤t u≤U ≤v where Px (str) and Px (inv) are the probability of straight and inverted ITG productions. While the exact calculation of these probabilities can be performed in O(n6 ) time, where n is the 2 Pt can be specified according to Bayesian statistics as described by Neubig et al. (2011). 168 length of the sentence, this is impractical for all but the shortest sentences. Thus it is necessary to use methods to reduce the search space such as beamsearch based chart parsing (Saers et al., 2009) or slice sampling (Blunsom and Cohn, 2010).3 In this section we propose the use of a look-ahead probability to increase the efficiency of this chart parsing. Taking the example of Saers et al. (2009), spans are pushed onto a different queue based on their size, and queues are processed in ascending order of size. Agendas can further be trimmed based on a histogram beam (Saers et al., 2009) or probability beam (Neubig et al., 2011) compared to the best hypothesis a ˆ. In other words, we have a queue discipline based on the inside probability, and all spans ak where I(ak ) &lt; cI(ˆ a) are pruned. c is a constant describing the width of th"
P12-1018,P09-1088,0,0.0977356,"normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). 167 enough information to allow for effective alignment with its corresponding elements in eI1 . While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more i"
P12-1018,W07-0735,0,0.0131682,"is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of wor"
P12-1018,J93-2003,0,0.0971778,"ir. We represent our target and source sentences as eI1 and f J1 . ei and fj represent single elements of the target and source sentences respectively. These may be words in word-based alignment models or single characters in character-based alignment models.1 We define our alignment as aK 1 , where each element is a span ak = hs, t, u, vi indicating that the target string es , . . . , et and source string fu , . . . , fv are aligned to each-other. 3.1 One-to-Many Alignment The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1 |f J1 , aK 1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2"
P12-1018,2002.tmi-papers.3,0,0.0215762,"in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods f"
P12-1018,W08-0336,0,0.0274774,"Missing"
P12-1018,D09-1075,0,0.0227118,"ion (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptual"
P12-1018,P06-3003,0,0.165382,"ter-based) Model 1 probability, which can be efficiently calculated using the dynamic programming algorithm described by Brown et al. (1993). However, for reasons previously stated in Section 3, these methods are less satisfactory when performing character-based alignment, as the amount of information contained in a character does not allow for proper alignment. 5.2 Substring Co-occurrence Priors Instead, we propose a method for using raw substring co-occurrence statistics to bias alignments towards substrings that often co-occur in the entire training corpus. This is similar to the method of Cromieres (2006), but instead of using these cooccurrence statistics as a heuristic alignment criterion, we incorporate them as a prior probability in a statistical model that can take into account mutual exclusivity of overlapping substrings in a sentence. We define this prior probability using three counts over substrings c(e), c(f ), and c(e, f ). c(e) and c(f ) count the total number of sentences in which the substrings e and f occur respectively. c(e, f ) is a count of the total number of sentences in which the substring e occurs on the target side, and f occurs on the source side. We perform the calcula"
P12-1018,D08-1033,0,0.0307241,"Missing"
P12-1018,W11-2107,0,0.014441,"riments, although it does indicate that we must have access to tokenized data for the development set. 7 171 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU. The differences between the evaluation metrics are due to the fact that character-based translation of"
P12-1018,H05-1085,0,0.0216227,"ollection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine"
P12-1018,P09-1104,0,0.122759,"when co-occurrence counts are used. More importantly, they allow for more aggressive beam pruning, increasing sampling speed from 1.3 sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6 sent/s for Japanese. 7 Conclusion and Future Directions This paper demonstrated that character-based translation can act as a unified framework for handling difficult problems in translation: morphology, compound words, transliteration, and segmentation. One future challenge includes scaling training up to longer sentences, which can likely be achieved through methods such as the heuristic span pruning of Haghighi et al. (2009) or sentence splitting of Vilar et al. (2007). Monolingual data could also be used to improve estimates of our substring-based prior. In addition, error analysis showed that wordbased translation performed better than characterbased translation on reordering and lexical choice, indicating that improved decoding (or pre-ordering) and language modeling tailored to character-based translation will likely greatly improve accuracy. Finally, we plan to explore the middle ground between word-based and character based translation, allowing for the flexibility of character-based translation, while usin"
P12-1018,N07-1018,0,0.0294521,"ng, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak ) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O∗ that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2 , compared with the n3 amortized time of the tic-tac-toe pruning 3 Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this had no significant effect on results, so we omit the Metropolis-in-Gibbs step for experiments. algorithm described by (Zhang et al., 2008a). During the calculation of the phrase generation probabilities Pt , we save the best inside probability I ∗ for each monolingual span. Ie∗ (s, t) = max If∗ (u, v) = max {˜ a=h˜ s,t˜,˜ u,˜ v i;˜ s=s,t˜=t} Pt (˜ a) {˜ a=h˜ s,t˜,˜ u,˜ v i;˜ u=u,˜ v =v} Pt (˜ a) For each language independently, we calculate forward probabilities α and backward probabilities β. For example, αe (s) is the maximum probabilit"
P12-1018,N03-1016,0,0.0120405,"is unwise to ignore competing hypotheses during beam pruning. Particularly, the alignment “les/1960s” competes with the high-probability alignment “les/the,” so intuitively should be a good candidate for pruning. However its probability is only slightly higher than “ann´ees/1960s,” which has no competing hypotheses and thus should not be trimmed. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I(ak ), but also the outside probability O(ak ), the probability of generating all spans other than ak , as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak ) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O∗ that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2 , compared with the n3 amortized time of the tic-tac-toe pruning 3 Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson e"
P12-1018,N03-1017,0,0.0487988,"n et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1 |f J1 , aK 1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many alignment methods to be effective, each fj must contain 1 Some previous work has also performed alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). 167 enough information to allow for effective alignment with its corresponding elements in eI1 . While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there hav"
P12-1018,W04-3250,0,0.0751149,"35.45 en-fi 13.22 / 58.50 / 27.03 13.12 / 59.27 / 27.09 04.58 / 35.09 / 11.76 12.14 / 59.02 / 25.31 en-fr 32.19 / 69.20 / 52.39 31.66 / 69.61 / 51.98 10.31 / 42.84 / 25.06 27.74 / 67.44 / 48.56 en-ja 20.79 / 27.01 / 38.41 20.26 / 28.34 / 38.34 01.48 / 00.72 / 06.67 17.90 / 28.46 / 35.71 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004). source and target were 100 characters or less,6 the total size of which is shown in Table 1. In characterbased translation, white spaces between words were treated as any other character and not given any special treatment. Evaluation was performed on tokenized and lower-cased data. For alignment, we use the GIZA++ implementation of one-to-many alignment7 and the pialign implementation of the phrasal ITG models8 modified with the proposed improvements. For GIZA++, we used the default settings for word-based alignment, but used the HMM model for character-based alignment to allow for alignmen"
P12-1018,2005.mtsummit-papers.11,0,0.090289,"ed enhancements to the model. Finally, we perform a qualitative analysis, which finds that character-based translation can handle unsegmented text, conjugation, and proper names in a unified framework with no additional processing. 2 Related Work on Data Sparsity in SMT As traditional SMT systems treat all words as single tokens without considering their internal structure, major problems of data sparsity occur for less frequent tokens. In fact, it has been shown that there is a direct negative correlation between vocabulary 166 size (and thus sparsity) of a language and translation accuracy (Koehn, 2005). Sparsity causes trouble for alignment models, both in the form of incorrectly aligned uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regu"
P12-1018,N03-2016,0,0.0319786,"logical analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), alth"
P12-1018,N04-4015,0,0.0188944,"f garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named en"
P12-1018,P11-1140,0,0.0221195,"e segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and K"
P12-1018,W02-1018,0,0.0382455,"alignment using morphological analyzers to normalize or split the sentence into morpheme streams (Corston-Oliver and Gamon, 2004). 167 enough information to allow for effective alignment with its corresponding elements in eI1 . While this is often the case in word-based models, for characterbased models this assumption breaks down, as there is often no clear correspondence between characters. 3.2 Many-to-Many Alignment On the other hand, in recent years, there have been advances in many-to-many alignment techniques that are able to align multi-element chunks on both sides of the translation (Marcu and Wong, 2002; DeNero et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). Many-to-many methods can be expected to achieve superior results on character-based alignment, as the aligner can use information about substrings, which may correspond to letters, morphemes, words, or short phrases. Here, we focus on the model presented by Neubig et al. (2011), which uses Bayesian inference in the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the pr"
P12-1018,P11-1090,0,0.014234,"nsduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual decoding. In this framework, trainin"
P12-1018,P11-1064,1,0.701097,"of traditional word-based systems using only character strings. We draw upon recent advances in many-to-many alignment, which allows for the automatic choice of the length of units to be aligned. As these units may be at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search pr"
P12-1018,C10-1092,0,0.0075172,"1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach is attractive conceptually, previous research"
P12-1018,C00-2162,0,0.113372,"d uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophistica"
P12-1018,J03-1002,0,0.00795056,"traditional SMT systems treat all words as single tokens without considering their internal structure, major problems of data sparsity occur for less frequent tokens. In fact, it has been shown that there is a direct negative correlation between vocabulary 166 size (and thus sparsity) of a language and translation accuracy (Koehn, 2005). Sparsity causes trouble for alignment models, both in the form of incorrectly aligned uncommon words, and in the form of garbage collection, where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more diffi"
P12-1018,P02-1040,0,0.101074,"atmt.org/moses/ 11 We chose this set-up to minimize the effect of tuning criterion on our experiments, although it does indicate that we must have access to tokenized data for the development set. 7 171 6.2 Quantitative Evaluation Table 2 presents a quantitative analysis of the translation results for each of the proposed methods. As previous research has shown that it is more difficult to translate into morphologically rich languages than into English (Koehn, 2005), we perform experiments translating in both directions for all language pairs. We evaluate translation quality using BLEU score (Papineni et al., 2002), both on the word and character level (with n = 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. It can be seen that character-based translation with all of the proposed alignment improvements greatly exceeds character-based translation using one-to-many alignment, confirming that substringbased information is necessary for accurate alignments. When compared with word-based translation, character-based translation achieves better, comparable, or inferior results on character-based BLEU, comparable or inferior results on METEOR, and inferior results on word-based BLEU. The"
P12-1018,W09-3804,0,0.613151,"els by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model. We do this by defining prior probabilities based on these substring counts within the Bayesian phrasal ITG framework. An evaluation on four language pairs with differing morphological properties shows that for distant language pairs, character-based SMT can achieve translation accuracy comparable to word-based systems. In addition, we perform ablation studies, showing that thes"
P12-1018,P08-1084,0,0.0198035,"n the phrasal inversion transduction grammar (ITG, Wu (1997)) framework. ITGs are a variety of synchronous context free grammar (SCFG) that allows for many-to-many alignment to be achieved in polynomial time through the process of biparsing, which we explain more in the following section. Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides. It should be noted that there are other many-to-many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages (Snyder and Barzilay, 2008; Naradowsky and Toutanova, 2011), but these have generally been applied to single words or short phrases, and it is not immediately clear that they will scale to aligning full sentences. 4 Look-Ahead Biparsing In this work, we experiment with the alignment method of Neubig et al. (2011), which can achieve competitive accuracy with a much smaller phrase table than traditional methods. This is important in the character-based translation context, as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual dec"
P12-1018,P11-1024,0,0.0120071,"oblem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt wi"
P12-1018,P06-1122,0,0.012896,"s in one language are incorrectly aligned to large segments of the sentence in the other language (Och and Ney, 2003). Unknown words are also a problem during the translation process, and the default approach is to map them as-is into the target sentence. This is a major problem in agglutinative languages such as Finnish or compounding languages such as German. Previous works have attempted to handle morphology, decompounding and regularization through lemmatization, morphological analysis, or unsupervised techniques (Nießen and Ney, 2000; Brown, 2002; Lee, 2004; Goldwater and McClosky, 2005; Talbot and Osborne, 2006; Mermer and Akın, 2010; Macherey et al., 2011). It has also been noted that it is more difficult to translate into morphologically rich languages, and methods for modeling target-side morphology have attracted interest in recent years (Bojar, 2007; Subotin, 2011). Another source of data sparsity that occurs in all languages is proper names, which have been handled by using cognates or transliteration to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliter"
P12-1018,2009.eamt-1.3,0,0.0804142,"for languages with explicit word The first author is now affiliated with the Nara Institute of Science and Technology. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation techniques to character-based translation for less similar language pairs. In this paper, we propose improvements to the alignment process tailored to character-based machine translation, and demonstrate that it is, in fact, possible to achieve translation accuracies that ap165 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,"
P12-1018,W07-0705,0,0.459683,"d eI1 is assumed to be a word in the source and target languages. However, the definition of a “word” is often problematic. The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). Even for languages with explicit word The first author is now affiliated with the Nara Institute of Science and Technology. These difficulties occur because we are translating sequences of words as our basic unit. On the other hand, Vilar et al. (2007) examine the possibility of instead treating each sentence as sequences of characters to be translated. This method is attractive, as it is theoretically able to handle all sparsity phenomena in a single unified framework, but has only been shown feasible between similar language pairs such as Spanish-Catalan (Vilar et al., 2007), Swedish-Norwegian (Tiedemann, 2009), and ThaiLao (Sornlertlamvanich et al., 2008), which have a strong co-occurrence between single characters. As Vilar et al. (2007) state and we confirm, accurate translations cannot be achieved when applying traditional translation"
P12-1018,C96-2141,0,0.510173,"ces as eI1 and f J1 . ei and fj represent single elements of the target and source sentences respectively. These may be words in word-based alignment models or single characters in character-based alignment models.1 We define our alignment as aK 1 , where each element is a span ak = hs, t, u, vi indicating that the target string es , . . . , et and source string fu , . . . , fv are aligned to each-other. 3.1 One-to-Many Alignment The most well-known and widely-used models for bitext alignment are for one-to-many alignment, including the IBM models (Brown et al., 1993) and HMM alignment model (Vogel et al., 1996). These models are by nature directional, attempting to find the alignments that maximize the conditional probability of the target sentence P (eI1 |f J1 , aK 1 ). For computational reasons, the IBM models are restricted to aligning each word on the target side to a single word on the source side. In the formalism presented above, this means that each ei must be included in at most one span, and for each span u = v. Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al., 2003). However, in order for one-to-many align"
P12-1018,J97-3002,0,0.511044,"th of units to be aligned. As these units may be at the character, subword, word, or multi-word phrase level, we conjecture that this will allow for better character alignments than one-to-many alignment techniques, and will allow for better translation of uncommon words than traditional word-based models by breaking down words into their component parts. We also propose two improvements to the manyto-many alignment method of Neubig et al. (2011). One barrier to applying many-to-many alignment models to character strings is training cost. In the inversion transduction grammar (ITG) framework (Wu, 1997), which is widely used in many-to-many alignment, search is cumbersome for longer sentences, a problem that is further exacerbated when using characters instead of words as the basic unit. As a step towards overcoming this difficulty, we increase the efficiency of the beam-search technique of Saers et al. (2009) by augmenting it with look-ahead probabilities in the spirit of A* search. Secondly, we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model. We do this by defining prior probabilities based on these substring"
P12-1018,P05-1059,0,0.00986268,"arly, the alignment “les/1960s” competes with the high-probability alignment “les/the,” so intuitively should be a good candidate for pruning. However its probability is only slightly higher than “ann´ees/1960s,” which has no competing hypotheses and thus should not be trimmed. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I(ak ), but also the outside probability O(ak ), the probability of generating all spans other than ak , as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for wordbased ITGs (Zhang and Gildea, 2005). As the calculation of the actual outside probability O(ak ) is just as expensive as parsing itself, it is necessary to approximate this with heuristic function O∗ that can be calculated efficiently. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2 , compared with the n3 amortized time of the tic-tac-toe pruning 3 Applying beam-search before sampling will sample from an improper distribution, although Metropolis-in-Gibbs sampling (Johnson et al., 2007) can be used to compensate. However, we found that this h"
P12-1018,P08-1012,0,0.710838,"n to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach"
P12-1018,W08-0335,0,0.0813058,"n to improve translation (Knight and Graehl, 1998; Kondrak et al., 2003; Finch and Sumita, 2007), and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed (Al-Onaizan and Knight, 2002). Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries, such as Chinese, Japanese, and Thai. A number of works have dealt with this word segmentation problem in translation, mainly focusing on Chinese-to-English translation (Bai et al., 2008; Chang et al., 2008; Zhang et al., 2008b; Chung and Gildea, 2009; Nguyen et al., 2010), although these works generally assume that a word segmentation exists in one language (English) and attempt to optimize the word segmentation in the other language (Chinese). We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions. Character-based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework, requiring no language specific tools such as morphological analyzers or word segmenters. However, while the approach"
P12-1018,corston-oliver-gamon-2004-normalizing,0,\N,Missing
P12-1018,P10-3006,0,\N,Missing
P12-1018,I08-8003,0,\N,Missing
P12-1018,J98-4003,0,\N,Missing
P12-1069,W06-2922,0,0.42502,"Missing"
P12-1069,P04-1015,0,0.203063,"wn (beam 8, pred 5) and shift-reduce (beam 8) and MST(2nd) parsers in Table 1. 0.4 0.2 0 0 10 20 30 40 50 length of input sentence 60 70 Figure 5: Scatter plot of parsing time against sentence length, comparing with top-down, 2nd-MST and shiftreduce parsers (beam size: 8, pred size: 5) we used the information of words and fine-grained POS-tags for features. We also implemented and experimented Huang and Sagae (2010)’s arc-standard shift-reduce parser. For the 2nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012). We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. The weighted prediction and stack-based models of topdown parser were jointly trained. 8.1 Results for English Data During training, we fixed the prediction size and beam size to 5 and 16, respectively, judged by pre663 top-down (beam:8, pred:5) shift-reduce (beam:8) 2nd-MST oracle (sh+mst) oracle (top+sh) oracle (top+mst) oracle (top+sh+mst) acc"
P12-1069,P81-1022,0,0.771692,", 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 . . . nn where n0 is a special root node $, a directed graph is defined as GW = (VW , AW ) where VW = {0, 1, . . . , n} is a set of (indices of) nodes and AW ⊆"
P12-1069,P99-1059,0,0.203438,"pushes them into hypo in line 9 of Algorithm 1. 6 Time Complexity Our proposed top-down algorithm has three kinds of actions which are scan, comp and predict. Each scan and comp actions occurs n times when parsing a sentence with the length n. Predict action also occurs n times in which a child node is selected from 662 n ∑ i= n(n + 1) . (11) 2 n2 As for prediction is the most dominant factor, the time complexity of the algorithm is O(n2 ) and that of the algorithm with beam search is O(n2 ∗ b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003)"
P12-1069,N10-1115,0,0.0762616,"Missing"
P12-1069,D11-1137,1,0.712619,"Missing"
P12-1069,P10-1110,0,0.439528,"rsing with Top-down Prediction Katsuhiko Hayashi† , Taro Watanabe‡ , Masayuki Asahara§ , Yuji Matsumoto† † Nara Institute of Science and Technology Ikoma, Nara, 630-0192, Japan ‡ National Institute of Information and Communications Technology Sorakugun, Kyoto, 619-0289, Japan § National Institute for Japanese Language and Linguistics Tachikawa, Tokyo, 190-8561, Japan katsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jp masayu-a@ninjal.ac.jp, matsu@is.naist.jp Abstract To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 ∗ b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achieves competitive results with other data-driven parsing algorithms. This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the E"
P12-1069,C04-1040,0,0.0334442,"ttom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 2 Definition of Dependency Graph 1 Introduction A dependency graph is defined as follows. Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the predic"
P12-1069,W07-2416,0,0.0235581,"e results are almost 664 the same as those of the English results. 8.3 Analysis of Results Table 4 shows two interesting results, on which topdown parser is superior to either shift-reduce parser or 2nd-MST parser. The sentence No.717 contains an adverbial clause structure between the subject and the main verb. Top-down parser is able to handle the long-distance dependency while shift-reudce parser cannot correctly analyze it. The effectiveness on the clause structures implies that our head-driven parser may handle non-projective structures well, which are introduced by Johansonn’s head rule (Johansson and Nugues, 2007). The sentence No.127 contains a coordination structure, which it is difficult for bottom-up parsers to handle, but, top-down parser handles it well because its top-down prediction globally captures the coordination. 9 Conclusion This paper presents a novel head-driven parsing algorithm and empirically shows that it is as practical as other dependency parsing algorithms. Our head-driven parser has potential for handling nonprojective structures better than other non-projective dependency algorithms (McDonald et al., 2005; Attardi, 2006; Nivre, 2008b; Koo et al., 2010). We are in the process of"
P12-1069,P07-1022,0,0.0198248,"length n. Predict action also occurs n times in which a child node is selected from 662 n ∑ i= n(n + 1) . (11) 2 n2 As for prediction is the most dominant factor, the time complexity of the algorithm is O(n2 ) and that of the algorithm with beam search is O(n2 ∗ b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc"
P12-1069,W89-0206,0,0.583899,". 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the tra"
P12-1069,P10-2035,0,0.537808,"Missing"
P12-1069,P10-1001,0,0.138001,"Missing"
P12-1069,D10-1125,0,0.0267333,"information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 . . . nn where n0 is a special root node $, a directed graph is defined as GW = (VW , AW ) where VW = {0, 1, . . . , n} is a set of (indices of) nodes and AW ⊆ VW × VW is a set of directed arcs. The set of arcs is a set of pairs (x, y) where x is a head and y is a dependent of x. x →∗ l denotes a path from x to l. A directed graph GW = (VW , AW ) is well-formed if and only"
P12-1069,E06-1011,0,0.214123,"t they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 . . . nn where n0 is a special root node $, a directed graph is defined as GW = (VW , AW ) where VW = {0, 1, . . . , n} is a set of (indices of) nodes and AW ⊆ VW × VW is a set of directed arcs. The set of arcs is a set of pairs (x, y) where x is a head and y is a dependent of x. x →∗ l denotes a path from x to l. A directed graph GW = (VW , AW ) is well"
P12-1069,H05-1066,0,0.130502,"Missing"
P12-1069,J03-1006,0,0.035985,"-order one for weighted prediction. 661 Pred takes either predx or predy . Beam search is performed based on the following linear order for the two states p and p′ at the same step, which have (cfw , cin ) and (c′ fw , c′ in ) respectively: p ≻ p′ iff cfw < c′ fw or cfw = c′ fw ∧ cin < c′ . (9) a node sequence in the input queue. Thus, the algorithm takes the following times for prediction: n + (n − 1) + · · · + 1 = i in We prioritize the forward cost over the inside cost since forward cost pertains to longer action sequence and is better suited to evaluate hypothesis states than inside cost (Nederhof, 2003). 5.4 FIRST Function for Lookahead Top-down backtrack parser usually reduces backtracking by precomputing the set FIRST(·) (Aho and Ullman, 1972). We define the set FIRST(·) for our top-down dependency parser: FIRST(t’) = {ld.t|ld ∈ lmdescendant(Tree, t’) Tree ∈ Corpus} (10) where t’ is a POS-tag, Tree is a correct dependency tree which exists in Corpus, a function lmdescendant(Tree, t’) returns the set of the leftmost descendant node ld of each nodes in Tree whose POS-tag is t’, and ld.t denotes a POS-tag of ld. Though our parser does not backtrack, it looks ahead when selecting possible chil"
P12-1069,W03-3017,0,0.098252,"rted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoN"
P12-1069,W04-0308,0,0.433655,"hms. This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 2 Definition of Dependency Graph 1 Introduction A dependency graph is defined as follows. Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley"
P12-1069,J08-4003,0,0.154551,"”. We follow a convention and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example, we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1. parser constructs left and right children of a head node in a left-to-right direction by scanning the head node prior to its right children. Figure 2 shows an example for parsing a sentence “I saw a girl”. 4 Correctness To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). The correct deductive system is both sound and complete. Theorem 4.1 The deductive system in Figure 1 is correct for the class of dependency forest. Proof 4.1 To show soundness, we show that Gp0 = (VW , ∅), which is a directed graph defined by the axiom, is well-formed and projective, and that every transition preserves this property. • ROOT: The node 0 is a root in Gp0 , and the node 0 is on the top of stack of p0 . The two pred actions put a word onto the top of stack, and predict an arc from root or its descendant to the child. The comp actions add the predicted arcs which include no ar"
P12-1069,J95-2002,0,0.086641,"e cost of a top tree s0 in stack S. We define these costs using a combination of stack-based model and weighted prediction model. The forward and inside costs of the combination model are as follows: { fw fw c = cfw s + cp (5) in in in c = cs + cp in where cfw s and cs are a forward cost and an inside in cost for stack-based model, and cfw p and cp are a forward cost and an inside cost for weighted prediction model. We add the following tuple of costs to a state: in fw in (cfw s , cs , cp , cp ). For each action, we define how to efficiently calculate the forward and inside costs3 , following Stolcke (1995) and Huang and Sagae (2010)’s works. In either case of predx or predy , fw (cfw s , , cp , ) fw (cfw s + λ, 0, cp + cp (s0 .h, nk ), 0) where λ= m−1 ∑ θs · fs,predx (i, h, j, S) if predx θs · fs,predy (i, h, j, S) if predy (6) In the case of scan, cp (h, ly , ly+1 ) in fw in (cfw s , cs , cp , cp ) in fw in (cfw s + ξ, cs + ξ, cp , cp ) y=1 +cp (h, −, r1 ) + { cp (h, ry , ry+1 ). where y=1 This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)’s definition are calculated from ri"
P12-1069,W03-3023,1,0.78688,"wn. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combinat"
P12-1069,D08-1059,0,0.367636,"er presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 2 Definition of Dependency Graph 1 Introduction A dependency graph is defined as follows. Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley pred"
P12-1069,P11-2033,0,0.155303,"Missing"
P13-1078,1997.tmi-1.19,0,0.396779,"Missing"
P13-1078,D08-1024,0,0.0472501,"runing method as the log-linear model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is called as a preference"
P13-1078,P05-1033,0,0.522224,"ng; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into fea"
P13-1078,J07-2003,0,0.830393,", e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a linear component which captures nonlocal (or state dependent) features and a non-linear component (i.e., neural nework) which encodes loMost statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interprete"
P13-1078,P11-2031,0,0.0314142,") as 10 and 30, and M axIter as 16 and 20 in Algorithm 2, for Chinese-to-English and Japanese-to-English tasks, respectively. Although there are several parameters in AdNN which may limit its practicability, according to many of our internal studies, most parameters are insensitive to AdNN except λ and M axIter, which are common in other tuning toolkits such as MIRA and can be tuned5 on a development test dataset. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al. (2011) for fairer comparisons. For AdNN, we report the averaged scores of five post-training runs, but both pre-training and training are performed only once. 5.2 NIST08 17.42+ 18.33+ 18.20+ 19.42 test4 23.68+ 23.66+ 24.03+ 24.45 Table 2: The BLEU comparisons between AdNNHiero-E and Log-linear translation models on the Chinese-to-English and Japanese-to-English tasks. + means the comparison is significant over AdNN-Hiero-E with p &lt; 0.05. these features are not dependent on the translation states, they are computed and saved to memory when loading the translation model. During decoding, we just look"
P13-1078,W09-0438,0,0.0124299,"computational times for the features in  the hidden units, i.e. σ M · h0 (r) + B . Since 5 For easier tuning, we tuned these two parameters on a given development test set without post-training in Algorithm 2. 797 Chinese-to-English NIST05 NIST06 L-Hiero 25.57+ 25.27+ 25.93 AdNN-Hiero-E 26.37 AdNN-Hiero-D 26.21 26.07 Japanese-to-English test2 test3 L-Hiero 24.38 25.55 AdNN-Hiero-E 25.14+ 26.32+ AdNN-Hiero-D 24.42 25.46 model (Bengio et al., 2003); POS, Chunking, NER, and SRL (Collobert and Weston, 2008); Parsing (Collobert and Weston, 2008; Socher et al., 2011); and Machine transliteration (Deselaers et al., 2009). Our work is, of course, highly motivated by these works. Unlike these works, we propose a variant neural network, i.e. additive neural networks, starting from SMT itself and taking both of the model definition and its inference (decoding) together into account. Our variant of neural network, AdNN, is highly related to both additive models (Buja et al., 1989) and generalized additive neural networks (Potts, 1999; Waal and Toit, 2007), in which an additive term is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposab"
P13-1078,P08-2010,0,0.66897,", Taro Watanabe2 , Eiichiro Sumita2 , Tiejun Zhao1 1 School of Computer Science and Technology Harbin Institute of Technology (HIT), Harbin, China 2 National Institute of Information and Communication Technology (NICT) 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan {lmliu |tjzhao}@mtlab.hit.edu.cn {taro.watanabe |eiichiro.sumita}@nict.go.jp Abstract On the one hand, features are required to be linear with respect to the objective of the translation model (Nguyen et al., 2007), but it is not guaranteed that the potential features be linear with the model. This induces modeling inadequacy (Duh and Kirchhoff, 2008), in which the translation performance may not improve, or may even decrease, after one integrates additional features into the model. On the other hand, it cannot deeply interpret its surface features, and thus can not efficiently develop the potential of these features. What may happen is that a feature p does initially not improve the translation performance, but after a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing,"
P13-1078,D11-1125,0,0.0808631,"near model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is called as a preference pair for f . Following PRO, w"
P13-1078,P02-1040,0,0.103298,"ts are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phrase-based translation (Chiang, 2005) for our baseline system, which shares the similar setting as Hiero (Chiang, 2005), e.g. beam-size=100, kbest-size=100, and is denoted as L-Hiero to emphasize its log-linear model. We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit. On the same experiment settings, the performance of L-Hiero is comparable Training Algorithm Algorithm 2 Training Algorithm Input: M axIter, a dev set, parameters (e.g. λ"
P13-1078,N03-1017,0,0.050145,"(news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-sampling (Koehn, 2004b). We use an in-house developed hierarchical phr"
P13-1078,P07-2045,0,0.00938653,"-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the model, and it can be tuned by the toolkit MERT (Och, 2003). Different from Brown’s generative model (Brown et al., 1993), the loglinear model does not assume strong independency holds, and allows arbitrary features to be integrated into the model easily. In other words, it can transform complex language translation into feature engineering: it can achieve high translati"
P13-1078,C12-2104,0,0.00594537,"model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking"
P13-1078,koen-2004-pharaoh,0,0.192436,"fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li"
P13-1078,W04-3250,0,0.540761,"fter a nonlinear operation, e.g. log(p), it does. The reason is not because this feature is useless but the model does not efficiently interpret and represent it. Situations such as this confuse explanations for feature designing, since it is unclear whether such a feature contributes to a translation or not. A neural network (Bishop, 1995) is a reasonable method to overcome the above shortcomings. However, it should take constraints, e.g. the decoding efficiency, into account in SMT. Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search (Koehn, 2004a). In the search procedure, frequent computation of the model score is needed for the search heuristic function, which will be challenged by the decoding efficiency for the neural network based translation model. Further, decoding with non-local (or state-dependent) features, such as a language model, is also a problem. Actually, even for the (log-) linear model, efficient decoding with the language model is not trivial (Chiang, 2007). In this paper, we propose a variant of neural networks, i.e. additive neural networks (see Section 3 for details), for SMT. It consists of two components: a li"
P13-1078,2012.amta-papers.17,0,0.553532,"presenting each word as a feature vector (Collobert and Weston, 2008). Because of the thousands of parameters and the non-convex objective in our model, efficient training is not simple. We propose an efficient training methodology: we apply the mini-batch conjugate sub-gradient algorithm (Le et al., 2011) to accelerate the training; we also propose pre-training and post-training methods to avoid poor local minima. The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model (Duh and Kirchhoff, 2008; Sokolov et al., 2012). On both Chinese-to-English and Japanese-toEnglish translation tasks, experiment results show that our model can leverage the shortcomings suffered by the log-linear model, and thus achieves significant improvements over the log-linear based translation. 2 a collection of synchronous rules for Hiero grammar (Chiang, 2005), or phrase pairs in Moses (Koehn et al., 2007); h(f, e, d) = (h1 (f, e, d), h2 (f, e, d), · · · , hK (f, e, d))> is a K-dimensional feature vector defined on the tuple hf, e, di; W = (w1 , w2 , · · · , wK )> is a Kdimensional weight vector of h, i.e., the parameters of the m"
P13-1078,N12-1005,0,0.0332495,"erm is either a linear model or a neural network. Unlike additive models and generalized additive neural networks, our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity. The idea of the neural network in machine translation has already been pioneered in previous works. Casta˜no et al. (1997) introduced a neural network for example-based machine translation. In particular, Son et al. (2012) and Schwenk (2012) employed a neural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of consid"
P13-1078,W07-0710,0,0.699181,"Missing"
P13-1078,P10-1040,0,0.0218209,"ine σ as a multilayer neural network. Again for the example shown in Figure 1, the model score defined in Eq. (5) for the pair he2 , d2 i can be represented as follows: because they empirically perform well in the loglinear model. For the local feature vector h0 in Eq (5), we employ word embedding features as described in the following subsection. 3.3 Word Embedding features for AdNN Word embedding can relax the sparsity introduced by the lexicalization in NLP, and it improves the systems for many tasks such as language model, named entity recognition, and parsing (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011). Here, we propose embedding features for rules in SMT by combining word embeddings. Firstly, we will define the embedding for the source side α of a rule r : X → hα, γi. Let VS be the vocabulary in the source language with size |VS |; Rn×|VS |be the word embedding matrix, each column of which is the word embedding (ndimensional vector) for the corresponding word in VS ; and maxSource be the maximal length of α for all rules. We further assume that the α for all rules share the same length as maxSource; otherwise, we add maxSource − |α |words “N U LL” to the end of α to obtai"
P13-1078,P00-1056,0,0.0517744,"Chinese-to-English task, the training data is the FBIS corpus (news domain) with about 240k sentence pairs; the development set is the NIST02 evaluation data; the development test set is NIST05; and the test datasets are NIST06, and NIST08. For the Japanese-to-English task, the training data with 300k sentence pairs is from the NTCIR-patent task (Fujii et al., 2010); the development set, development test set, and two test sets are averagely extracted from a given development set with 4000 sentences, and these four datasets are called test1, test2, test3 and test4, respectively. We run GIZA++ (Och and Ney, 2000) on the training corpus in both directions (Koehn et al., 2003) to obtain the word alignment for each sentence pair. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we train a 4-gram language model for the Chinese-toEnglish task on the Xinhua portion of the English Gigaword corpus and a 4-gram language model for the Japanese-to-English task on the target side of its training data. In our experiments, the translation performances are measured by case-sensitive BLEU4 metric4 (Papineni et al., 2002). The significance testing is performed by paired bootstrap re-samplin"
P13-1078,P02-1038,0,0.904636,"a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks. 1 Introduction Recently, great progress has been achieved in SMT, especially since Och and Ney (2002) proposed the log-linear model: almost all the stateof-the-art SMT systems are based on the log-linear model. Its most important advantage is that arbitrary features can be added to the model. Thus, it casts complex translation between a pair of languages as feature engineering, which facilitates research and development for SMT. Regardless of how successful the log-linear model is in SMT, it still has some shortcomings. This joint work was done while the first author visited NICT. 791 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801, c Sof"
P13-1078,D07-1080,1,0.791639,"rch strategy and cube pruning method as the log-linear model. 4 Algorithm 1 Mini-batch conjugate subgradient Input: θ1 , T , CGIter, batch-size, k-best-list 1: for all t such that 1 ≤ t ≤ T do 2: Sample mini-batch preference pairs with size batch-size from k-best-list 3: Calculate some quantities for CG, e.g. training objective Obj, subgradient ∆, according to Eq. (6) defined over the sampled preference pairs 4: θt+1 = CG(θt , Obj, ∆, CGIter) 5: end for Output: θT +1 Training Method 4.1 Training Objective For the log-linear model, there are various tuning methods, e.g. MERT (Och, 2003), MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011) and so on, which iteratively optimize a weight such that, after re-ranking a k-best list of a given development set with this weight, the loss of the resulting 1-best list is minimal. In the extreme, if the k-best list consists only of a pair of translations hhe∗ , d∗ i, he0 , d0 ii, the desirable weight should satisfy the assertion: if the BLEU score of e∗ is greater than that of e0 , then the model score of he∗ , d∗ i with this weight will be also greater than that of he0 , d0 i. In this paper, a pair he∗ , e0 i for a source sentence f is c"
P13-1078,P10-1076,0,0.011635,"ural network to model the phrase translation probability on the rule level hα, γi instead of the bilingual sentence level hf, ei as in Eq. (5), and thus they did not go beyond the log-linear model for SMT. There are also works which exploit non-linear models in SMT. Duh and Kirchhoff (2008) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model’s expressive abilities; Sokolov et al. (2012) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective. Instead of considering the reranking task in SMT, Xiao et al. (2010) employed a boosting method for the system combination in SMT. Unlike their post-processing models (either a re-ranking or a system combination model) in SMT, we propose a non-linear translation model which can be easily incorporated into the existing SMT framework. NIST08 18.33+ 19.42 19.54 test4 23.66 24.45+ 23.73 Table 3: The effect of different feature setting on AdNN model. + means the comparison is significant over AdNN-Hiero-D with p &lt; 0.05. both test sets. In addition, to investigate the effect of different feature settings on AdNN, we alternatively design another setting for h0 in Eq."
P13-1078,J93-2003,0,\N,Missing
P13-1078,P03-1021,0,\N,Missing
P13-1079,W07-0403,0,0.0221244,"n adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012)."
P13-1079,P05-1033,0,0.0914726,"ethod in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data"
P13-1079,P11-2031,0,0.0169713,"tences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the GIZA-batch, a single model is estimated from a single, merged corpus. Since pialign cannot handle large data, we did not experiment on the largest LDC data set. 5. Pialign-adaptive: Alignment and phrase pairs extraction are same to Pialign-batch, while translation probabilities are estimated"
P13-1079,P08-2007,0,0.01791,"ods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each IT"
P13-1079,W07-0717,0,0.0271745,"slation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified"
P13-1079,N10-1028,0,0.0143001,"explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds"
P13-1079,N03-1017,0,0.0141628,"phrase extraction method in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using"
P13-1079,P07-2045,0,0.00605503,"h and Pialign-adaptive were not run on the largest data set. the feature values. Pialign is used with default parameters. The parameter ’samps’ is set to 5, which indicates 5 samples are generated for a sentence pair. The IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the featu"
P13-1079,P08-1024,0,0.0181494,"a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism whi"
P13-1079,D07-1090,0,0.0188054,"Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs"
P13-1079,D09-1079,0,0.025601,"‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based me"
P13-1079,N12-1047,0,0.0243501,"IWSLT data consists of roughly 2, 000 sentences and 3, 000 sentences each from the HIT and BTEC for development purposes, and the test data consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the"
P13-1079,N10-1062,0,0.0176112,"lassifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead (Levenberg et al., 2010; Gonz´alezRubio et al., 2011). However, their methods usually require numbers of hyperparameters, such as mini-batch size, step size, or human judgment to determine the quality of phrases, and still rely on a heuristic phrase extraction method in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain ou"
P13-1079,I08-2089,0,0.0241935,"al phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned tog"
P13-1079,W11-2122,0,0.0170791,"(Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incrementally updated by using a succinct data structure with a interpolation technique (Levenberg and Osborne, 2009; Levenberg et al., 2011). In the case of the previous work on translation modeling, mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table (Foster and Kuhn, 2007). Under this framework, the training data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed."
P13-1079,P12-1048,0,0.0787144,"done while the first author visited NICT. 802 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802–810, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent batch oriented methods, an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation pe"
P13-1079,P06-1124,0,0.0502351,"is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008).  More formally, P he, f i; θx , θt are the probability of phrase pairs he, f i, which is parameterized by a phrase pair distribution θt and a symbol distribution θx . θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), which is expressed as  θt ∼ P Y d, s, Pdac (1) where d is the discount parameter, s is the strength parameter, and , and Pdac is a prior probability which acts as a fallback probability when a phrase pair is not in the model. Under this model, the probability for a phrase pair found in a bilingual corpus hE, F i can be represented by the following equation using the Chinese restaurant process (Teh, 2006):  P hei , fi i; hE, F i = Figure 1: A word alignment (a), and its hierarchical derivation (b). c. If x = IN V , follow a similar process as b, but concatenate f1 and f2 in reverse order he"
P13-1079,D07-1036,0,0.0197915,"keeps growing. Consequently, how to effectively use those data and improve translation performance becomes a challenging issue. This joint work was done while the first author visited NICT. 802 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802–810, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics by a feature set to potentially reflect the domain information. The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence (Lu et al., 2007). Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set (Su et al., 2012). Regardless of underlying methods, either classifier-based or featurebased method, the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection. Usually the domain similar to a given development data is usually assigned higher weights. Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT. Compared to traditional frequent ba"
P13-1079,2012.amta-papers.18,0,0.0920973,"on a large data set, and it requires us to re-train every time new training data is available. Even if we can handle the large computation cost, improvement is not guaranteed every time we perform batch tuning on the newly updated training data obtained from divergent domains. Traditional domain adaption methods for SMT are also not adequate in this scenario. Most of them have been proposed in order to make translation systems perform better for resource-scarce domains when most training data comes from resourcerich domains, and ignore performance on a more generic domain without domain bias (Wang et al., 2012). As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data. Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus. This paper proposes a new phrase table combination method. First, phrase pairs are extracted from each domain without interfering with other domains. In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of Neubig et al. (2011) to p"
P13-1079,P11-1064,1,0.92446,"bias (Wang et al., 2012). As an alternative, incremental learning may resolve the gap by incrementally adding data sentence-by-sentence into the training data. Since SMT systems trend to employ very large scale training data for translation knowledge extraction, updating several sentence pairs each time will be annihilated in the existing corpus. This paper proposes a new phrase table combination method. First, phrase pairs are extracted from each domain without interfering with other domains. In particular, we employ the nonparametric Bayesian phrasal inversion transduction grammar (ITG) of Neubig et al. (2011) to perform phrase table extraction. Second, extracted phrase tables are combined as if they are drawn from a hierarchical Pitman-Yor process, in which the phrase tables represented as tables in the Chinese restaurant process (CRP) are hierarchically chained by treating each of the previously learned phrase tables as prior to the current one. Thus, we can easily update the chain of phrase tables by appending the newly extracted phrase table and by treating the chain of the previous ones as its prior. Experiment results indicate that our method can achieve better translation performance when th"
P13-1079,J97-3002,0,0.0172822,"ence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by sli"
P13-1079,P12-1018,1,0.903351,"(Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules, and each ITG derivation corresponds to the alignment of a sentence pair (Wu, 1997). Translation probabilities of ITG phrasal align803 ments can be estimated in polynomial time by slightly limiting word reordering (DeNero and Klein, 2008).  More formally, P he, f i; θx , θt are the probability of phrase pairs he, f i, which is parameterized by a phrase pair distribution θt and a symbol distribution θx . θx is a Dirichlet prior, and θt is estimated with the Pitman-Yor process (Pitman and Yor, 1"
P13-1079,2007.mtsummit-papers.68,0,0.0386744,"ining data is first divided into several parts, and phase pairs are extracted with some sub-domain features. Then all the phrase pairs and features are tuned together with different weights during decoding. As a way to choose the right domain for the domain adaption, a classifier-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a m"
P13-1079,P08-1012,0,0.0167267,"er-based method and a feature-based method have been proposed. Classification-based methods must at least add an explicit label to indicate which domain the current phrase pair comes from. This is traditionally done with an automatic domain classifier, and each input sentence is classified into its corresponding domain (Xu et al., 2007). As an alternative to the classification-based approach, Wang et al. (2012) employed a featurebased approach, in which phrase pairs are enriched 3 Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently, phrase alignment with ITGs (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008) and parameter estimation with Gibbs sampling (DeNero and Klein, 2008; Blunsom and Cohn, 2010) are popular. Here, we employ a method proposed by Neubig et al. (2011), which uses parametric Bayesian inference with the phrasal ITGs (Wu, 1997). It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA++ and heuristic phrase extraction methods. It has also been proved successful in adjusting the phrase length granularity by applying character-based SMT with more sophisticated inference (Neubig et al., 2012). ITG is a synchronou"
P13-1079,J03-1002,0,0.00573107,"phrase pair would be boosted by the fallback probabilities. Pitman-Yor process is also employed in n-gram language models which are hierarchically represented through the hierarchical Pitman-Yor process with switch priors to integrate different domains in all the levels (Wood and Teh, 2009). Our work incrementally combines the models from different domains by directly employing the hierarchical process through the base measures. 5 Corpus HIT BTEC Domain 1 Domain 2 Domain 3 Domain 4 Domain 5 News News Magazine Magazine Finance 1. GIZA-linear: Phase pairs are extracted in each domain by GIZA++ (Och and Ney, 2003) and the ”grow-diag-final-and” method with a maximum length 7. The phrase tables from various domains are linearly combined by averaging the feature values. 2. Pialign-linear: Similar to GIZA-linear, but we employed the phrasal ITG method described in Section 3 using the pialign toolkit 3 (Neubig et Experiment 1 http://code.google.com/p/plda/ In particular, they come from LDC catalog number: LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47, LDC2006E26, in this order. 3 http://www.phontron.com/pialign/ 2 We evaluate the proposed approach on the Chinese-to-English translation task with three data"
P13-1079,J04-4002,0,0.0224078,"rely on a heuristic phrase extraction method in each phrase table update. achieve at least comparable results to batch training methods, with a significantly less computational overhead. The rest of the paper is organized as follows. In Section 2, we introduce related work. In section 3, we briefly describe the translation model with phrasal ITGs and Pitman-Yor process. In section 4, we explain our hierarchical combination approach and give experiment results in section 5. We conclude the paper in the last section. 2 Related Work Bilingual phrases are cornerstones for phrasebased SMT systems (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) and existing translation systems often get ‘crowd-sourced’ improvements (Levenberg et al., 2010). A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains, such as domain adaptation and incremental learning for SMT. The translation model and language model are primary components in SMT. Previous work proved successful in the use of large-scale data for language models from diverse domains (Brants et al., 2007; Schwenk and Koehn, 2008). Alternatively, the language model is incremental"
P13-1079,P02-1040,0,0.0878579,"ata consists of 1, 000 sentences. For the FBIS and LDC task, we used NIST MT 2002 and 2004 for development and testing purposes, consisting of 878 and 1, 788 sentences respectively. We employ Moses, an open-source toolkit for our experiment (Koehn et al., 2007). SRILM Toolkit (Stolcke, 2002) is employed to train 4-gram language models on the Xinhua portion of Gigaword corpus, while for the IWLST2012 data set, only its training set is used. We use batch-MIRA (Cherry and Foster, 2012) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU-4 metric (Papineni et al., 2002). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al., 2011). al., 2011). Extracted phrase pairs are linearly combined by averaging the feature values. 3. GIZA-batch: Instead of splitting into each domain, the data set is merged as a single corpus and then a heuristic GZA-based phrase extraction is performed, similar as GIZA-linear. 4. Pialign-batch: Similar to the GIZA-batch, a single model is estimated from a single, merged corpus. Since pialign cannot handle large data, we did not experiment on"
P13-1083,D09-1037,0,0.0182123,"ls: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computationa"
P13-1083,P05-1067,0,0.0375111,"syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they ar"
P13-1083,P07-1035,0,0.105138,"Eiichiro Sumita† , Hiroya Takamura‡ , Manabu Okumura‡ † National Institute of Information and Communications Technology {akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp † Precision and Intelligence Laboratory, Tokyo Institute of Technology {takamura, oku}@pi.titech.ac.jp Abstract [Example 1] 1 あなた は インターネット が 利用 でき ない Japanese POS: noun particle This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. [Example 2] particle noun verb auxiliary verb You can not use the Internet . 私 Japane"
P13-1083,D09-1071,0,0.0564114,"Missing"
P13-1083,P06-1121,0,0.0231334,"guage into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceeding"
P13-1083,I11-1136,0,0.0659352,"Missing"
P13-1083,D11-1125,0,0.0332039,"Missing"
P13-1083,W06-3601,0,0.0610141,"Missing"
P13-1083,P10-1145,0,0.0147666,"d method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the othe"
P13-1083,N03-1017,0,0.00724269,"e included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, th"
P13-1083,C12-1120,0,0.0368962,"Missing"
P13-1083,J03-1002,0,0.0064088,"lish sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the fo"
P13-1083,P03-1021,0,0.159428,"Missing"
P13-1083,W04-3250,0,0.192157,"Missing"
P13-1083,P02-1040,0,0.0860707,"Missing"
P13-1083,W02-2016,0,0.0772343,"Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency str"
P13-1083,P05-1034,0,0.0379419,"between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed withou"
P13-1083,D07-1072,0,0.16601,"nd parameters (e.g., ϕ′k and ϕ′′k ) for each information. Specifically, x′t and ϕ′k are introduced for the surface form of aligned words, and x′′t and ϕ′′k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “利用” generates the string “利用+use+verb” as the observation in the joint model, while it generates “利用”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities πks and observation ′ distributions (ϕsk , ϕks ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st , and we use the pair (st , zt ) as a state representation. β|γ ∼ GEM(γ), πk |α0 , β ∼ DP(α0 , β), ϕk ∼ H, ϕ′k ∼ H ′ , zt′ |zt ∼ Multinomial(πzt ), xt |zt ∼ F (ϕzt ), x′t |zt ∼ F ′ (ϕ′zt"
P13-1083,W11-2119,0,0.0290777,"Missing"
P13-1083,C04-1090,0,0.0333421,"s represent syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal"
P13-1083,P06-1077,0,0.0845012,"s in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “"
P13-1083,P08-1066,0,0.025975,"S tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the l"
P13-1083,P09-1063,0,0.0382929,"Missing"
P13-1083,N12-1045,0,0.0450049,"Missing"
P13-1083,D08-1022,0,0.0195747,"ations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual"
P13-1083,P11-1125,1,0.791018,"Missing"
P13-1083,N12-1026,1,0.875093,"Missing"
P13-1083,P08-1064,0,0.0187394,"te two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Asso"
P13-1083,P11-1084,0,0.0126965,"joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851, c Sofia, Bulgaria, August 4"
P13-1083,P11-1087,0,\N,Missing
P13-1083,P07-2045,0,\N,Missing
P14-1138,D13-1106,0,0.0157846,"Dependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to ∗ The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are su"
P14-1138,P06-1009,0,0.083905,"Missing"
P14-1138,J93-2003,0,0.11607,"nformation and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN a-tamura@ah.jp.nec.com, {taro.watanabe, eiichiro.sumita}@nict.go.jp Abstract This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical"
P14-1138,P11-1042,0,0.0616453,"Rumelhart et al., 1986), which unfolds the network in time (j) and computes gradients over time steps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) presented an unsupervised alignment model based on contrastive estimation (CE) (Smith and Eisner, 2005). CE seeks to discriminate observed data from its neighborhood, 1473 which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible alignments of the bilingual sentences, which are given as training data (T ), and those of the full translation search space (Ω) as the observed data and its neighborhood, respectively. We introduce this idea to a ranking loss with margin as { loss(θ) = max 0, 1 − ∑ + ∑ EΦ [sθ (a|f + , e+ )] (f + ,e+ )∈T } EΦ [sθ (a|f + , e− )] , (11) ("
P14-1138,P08-1112,0,0.0350066,"Missing"
P14-1138,D13-1176,0,0.0436696,"al Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to ∗ The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are supervised models. Unfortunately,"
P14-1138,N03-1017,0,0.1479,"Missing"
P14-1138,W04-3250,0,0.0844871,"Missing"
P14-1138,N12-1005,0,0.0140666,"pair (f1J , eI1 ) can be found as a ˆJ1 = argmax p(f1J , aJ1 |eI1 ). aJ 1 (3) For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as 1471 sN N (aJ1 |f1J , eI1 ) = J ∏ ta (aj − aj−1 |c(eaj−1 )) j=1 ·tlex (fj , eaj |c(fj ), c(eaj )), (4) t lex ( fj , ea |f j-1 , eaa -1+1 ) j+1 j Output Layer The computations in the hidden and output layer are as follows2 : j j O× z1 +BO z1 Hidden Layer"
P14-1138,N06-1014,0,0.527699,"the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f . It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other. Based on this motivation, our directional models are also simultaneously trained. Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This constraint prevents each model from overfitting to a particular direction and leads to global optimizat"
P14-1138,C04-1032,0,0.059982,"ch that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f . It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other. Based on this motivation, our directional models are also simultaneously trained. Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This constraint prevents each model from overfitting to a particular direction and leads"
P14-1138,W03-0301,0,0.426192,"Missing"
P14-1138,H05-1011,0,0.0550795,"Missing"
P14-1138,J03-1002,0,0.0908667,"a. All the data in BT EC is word-aligned, and the training data in Hansards is unlabeled data. In F BIS, we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data (N IST 03 and N IST 04). We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable. We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer. The IBM Model 4 was trained by previously presented model sequence schemes (Och and Ney, 2003): 15 H 5 35 45 , i.e., five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ (IBM 4). For the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer |z1 |to 100, and the window size of contexts to 5. Hence, |z0 |is 300 (30×5×2). Following Yang et al. (2013), the FFNN-based model was trained by the supervised approach described in Section 2.2 (F F N Ns ). For the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer |yj |to 100. Thus, |xj |is 60 ("
P14-1138,P03-1021,0,0.0506179,"Missing"
P14-1138,P02-1040,0,0.0976276,"cause the supervised models are adversely affected by errors in the automatically generated training data. This is especially true when the quality of training data, i.e., the performance of IBM 4, is low. 5.3 Word Alignment Results 5.4 Machine Translation Results Table 2 shows the alignment performance by the F1-measure. Hereafter, M ODEL(R) and M ODEL(I) denote the M ODEL trained from gold standard alignments and word alignments found by the IBM Model 4, respectively. In Hansards, all models were trained from ranTable 3 shows the translation performance by the case sensitive BLEU4 metric11 (Papineni et al., 2002). Table 3 presents the average BLEU of three different MERT runs. In N T CIR and F BIS, each alignment model was trained from the ranAlignment IBM 4 F F N Ns (I) RN Ns (I) RN Ns+c (I) RN Nu RN Nu+c F F N Ns (R) RN Ns (R) RN Ns+c (R) Table 2: measure) 7 BT EC 0.4859 0.4770 0.5053+ 0.5174+ 0.5307+ 0.5562+ 0.8224 0.8798+ 0.8921+ Hansards 0.9029 0.9020 0.9068 0.9202+ 0.9037 0.9275+ - Word alignment performance (F1- http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ 8 http://chasen-legacy.sourceforge.jp/ 9 http://nlp.stanford.edu/software/ segmenter.shtml 10 Due to high computational cost, we did not use al"
P14-1138,D13-1140,0,0.0306444,"can be found as a ˆJ1 = argmax p(f1J , aJ1 |eI1 ). aJ 1 (3) For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as 1471 sN N (aJ1 |f1J , eI1 ) = J ∏ ta (aj − aj−1 |c(eaj−1 )) j=1 ·tlex (fj , eaj |c(fj ), c(eaj )), (4) t lex ( fj , ea |f j-1 , eaa -1+1 ) j+1 j Output Layer The computations in the hidden and output layer are as follows2 : j j O× z1 +BO z1 Hidden Layer htanh(H× z0 +BH) z1 = f"
P14-1138,C96-2141,0,0.874258,"nsures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to ∗ The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed"
P14-1138,P13-1017,0,0.335221,"ted by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a"
P14-1138,P05-1044,0,0.0543265,"ps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) presented an unsupervised alignment model based on contrastive estimation (CE) (Smith and Eisner, 2005). CE seeks to discriminate observed data from its neighborhood, 1473 which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible alignments of the bilingual sentences, which are given as training data (T ), and those of the full translation search space (Ω) as the observed data and its neighborhood, respectively. We introduce this idea to a ranking loss with margin as { loss(θ) = max 0, 1 − ∑ + ∑ EΦ [sθ (a|f + , e+ )] (f + ,e+ )∈T } EΦ [sθ (a|f + , e− )] , (11) (f + ,e− )∈Ω where Φ is a set of all possible alignments given (f , e), EΦ [sθ ] is the expected value of"
P14-1138,takezawa-etal-2002-toward,1,0.52196,"Missing"
P14-1138,H05-1010,0,0.0207941,"Missing"
P14-1138,P12-1033,0,0.0339331,"Missing"
P14-1138,2007.iwslt-1.1,0,\N,Missing
P14-1138,P07-2045,0,\N,Missing
P15-1113,P05-1022,0,0.426589,"Missing"
P15-1113,A00-2018,0,0.65318,"Missing"
P15-1113,H91-1060,0,0.0470421,"Missing"
P15-1113,D14-1082,0,0.109218,"der large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1 http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for transition-based dependency parsing in which limited contexts are considered in the feature representation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-reduce parsing was first presented by Mayberry and Miikkulainen (1999) in which the stack representation was treated as a hidden state of an RNN. In their study, the hidden state is updated recurrently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the i"
P15-1113,J05-1003,0,0.0182229,"e the best results for each hidden state dimension. Experiments Settings We conducted experiments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 4401151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8 . The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to consider OOVs for testing: They were simply mapped to a special token hunki when looking up their corresponding word representation vector. Similarly, when"
P15-1113,P04-1015,0,0.436603,"nd hidlei, respectively3 . 5 Parameter Estimation  X X Let θ = Hsh , Qsh , · · · ∈ RM be an M dimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j ∗ where the difference of the scores is the largest:   j ∗ j = arg min ρθ (y0 ) − max ρθ (d) . (9) j 10 can be intuitively considered an expected mistake suffered at the maximum violated step j ∗ , which is measured by the Viterbi violation in Equation 9. Note that if we replace EB˜j ∗ [ρθ ] with maxd∈Bj ∗ ρθ (d) in Equation 10, it is exactly the same as the max-violation objective (Huang et al.,"
P15-1113,P97-1003,0,0.561303,"Missing"
P15-1113,J03-4003,0,0.631116,"Missing"
P15-1113,P14-1129,0,0.0927518,"Missing"
P15-1113,P14-1022,0,0.0329376,"that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delaye"
P15-1113,D13-1137,0,0.00902901,"t  ≥ η02 , it is always decayed6 . In our preliminary studies, AdaGrad eventually becomes very conservative to update parameters when training longer iterations. AdaDec fixes the problem by ignoring older histories of sub-gradients in G, which is reflected in the learning rate η. In each update, we employ `1 regularization through FOBOS (Duchi and Singer, 2009) using a hyperparameter λ ≥ 0 to control the fitness in Equation 16 and 17. For testing, we found that taking the average of the pa1 PT rameters over period T +1 t=0 θ t under training iterations T was very effective as demonstrated by Hashimoto et al. (2013). Parameter estimation is performed in parallel by distributing training instances asynchronously Or, setting pθ (d∗ ) = 1 for the Viterbi derivation d∗ = arg maxd∈Bj ∗ ρθ (d) and zero otherwise. 6 Note that AdaGrad is a special case of AdaDec with γ = 1 and  = 0. 1173 5 6 6.1 dev test in each shard and by updating locally copied parameters using the sub-gradients computed from the distributed mini-batches (Dean et al., 2012). The sub-gradients are broadcast asynchronously to other shards to reflect the updates in one shard. Unlike Dean et al. (2012), we do not keep a central storage for mode"
P15-1113,N03-1014,0,0.256072,"ural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1 http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for tran"
P15-1113,N07-1051,0,0.152499,"Missing"
P15-1113,P04-1013,0,0.840703,"vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 1990) in backward direction (Henderson, 2004). The stack structure is also modeled similarly to RNNs, and its top item is updated using the previously constructed hidden representations saved in the 1169 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1169–1179, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics stack. The representations from both the stack and queue are combined with the representations propagated from the partially parsed tree structure inspired by the recursive neura"
P15-1113,P06-1055,0,0.258141,"Missing"
P15-1113,P82-1020,0,0.859454,"Missing"
P15-1113,N12-1015,0,0.124816,"s for the three actions is 9×m2 +m×m0 +6×m+3 for each non-terminal label X. The scores for a 1172 finish action and an idle action are defined analogous to the unary-X action with special labels for X, hfinishi and hidlei, respectively3 . 5 Parameter Estimation  X X Let θ = Hsh , Qsh , · · · ∈ RM be an M dimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j ∗ where the difference of the scores is the largest:   j ∗ j = arg min ρθ (y0 ) − max ρθ (d) . (9) j 10 can be intuitively considered an expected mistake suffered at the maximum viol"
P15-1113,P03-1054,0,0.0727829,"Missing"
P15-1113,D12-1096,0,0.0716293,"Missing"
P15-1113,D14-1081,0,0.0175063,"., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1 http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the"
P15-1113,J93-2004,0,0.0597693,"Missing"
P15-1113,P05-1010,0,0.125817,"Missing"
P15-1113,W05-1513,0,0.196083,"3; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insuffi"
P15-1113,P13-1045,0,0.764865,"mpete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features. Recently Socher et al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 199"
P15-1113,P14-1138,1,0.767336,"t g t 2 (15) (16) 1 θ t ← arg min kθ − θ t− 1 k22 + λη &gt; t abs(θ). 2 2 θ (17) d∈Bj Then, we define the following hinge-loss function: o Compared with AdaGrad, the squared sum of the n ∗ L(w, y; B, θ) = max 0, 1 − ρθ (y0j ) + EB˜j ∗ [ρθ ] ,sub-gradients decays over time using a constant (10) wherein we consider the subset of sub-derivations ˜j ∗ ⊂ Bj ∗ consisting of those scored higher than B ∗ ρθ (y0j ): n o ˜j ∗ = d ∈ Bj ∗ ρθ (d) &gt; ρθ (y j ∗ ) (11) B 0 exp(ρθ (d)) 0 ˜j ∗ exp(ρθ (d )) d0 ∈B X EB˜j ∗ [ρθ ] = pθ (d)ρθ (d). pθ (d) = P (12) (13) ˜j ∗ d∈B Unlike Huang et al. (2012) and inspired by Tamura et al. (2014), we consider all incorrect sub˜j ∗ through the expected derivations found in B 4 score EB˜j ∗ [ρθ ] . The loss function in Equation 3 Since h1j and qn are constants for the finish and idle acX tions, we enforce Hun = 0 and QX un = 0 for those special actions. 4 We can use all the sub-derivations in Bj ∗ ; however, our ˜j ∗ was better. preliminary studies indicated that the use of B 0 &lt; γ ≤ 1 in Equation 14. The learning rate in Equation 15 is computed element-wise and bounded by a constant  ≥ 0, and if we set  ≥ η02 , it is always decayed6 . In our preliminary studies, AdaGrad eventually be"
P15-1113,N03-1033,0,0.115222,"ments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 4401151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8 . The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to consider OOVs for testing: They were simply mapped to a special token hunki when looking up their corresponding word representation vector. Similarly, when assigning possible POS tags in shift actions, they fell back to their corresponding “word si"
P15-1113,P10-1040,0,0.0741815,"Missing"
P15-1113,P14-1069,0,0.437019,"in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features. Recently Socher et al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased pa"
P15-1113,W09-3825,0,0.834691,"ategory splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various"
P15-1113,P13-1043,0,0.68089,"manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗ The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relation"
W00-0203,levin-etal-2000-lessons,1,0.873434,"Missing"
W06-3115,2004.iwslt-evaluation.13,0,0.016641,"3,979 23,186,379 es-en 17,221,890 16,601,306 16,540,767 12,677,192 21,709,212 fr-en 16,176,075 15,635,900 15,610,319 11,645,404 20,760,539 en-de 17,596,764 17,052,808 16,936,710 12,218,997 23,066,052 en-es 17,237,723 16,597,274 16,530,810 12,688,773 21,698,267 en-fr 16,220,520 15,658,940 15,613,755 11,653,242 20,789,570 Table 2: Number of phrases extracted from differently preprocessed corpora. lower stem prefix4 merged de-en 37,711,217 46,550,101 53,429,522 80,260,191 es-en 61,161,868 75,610,696 78,193,818 111,153,303 fr-en 56,025,918 68,210,968 70,514,377 103,523,206 lexicon model t( f |e) (Bender et al., 2004): hdel (e1I , f1J ) = J  X j=1 max t( f j |ei ) < τdel 0≤i≤I  (4) The deletion model simply counts the number of words whose lexicon model probability is lower than a threshold τdel . Likewise, we also added an insertion model hins (e1I , f1J ) that penalizes the spuriously inserted English words using a lexicon model t(e |f ). For the hierarchical phrase-based model, we employed the same feature set except for the distortion model and the lexicalized reordering model. 3 Phrase Extraction from Different Word Alignment en-de 38,142,663 46,749,195 53,647,033 80,666,414 en-es 60,619,435 75,473,"
W06-3115,P05-1033,0,0.568763,"language, i.e. English, e1I = e1 , e2 , ..., eI by seeking a maximum likelihood solution of eˆ 1I = argmax Pr(e1I |f1J ) (1) e1I  I , f J) λ h (e m m m=1 1 1 = argmax P (2) P M ′ I′ , f J ) I ′ exp λ h (e I e1 ′ m m m=1 e 1 1 exp P M 1 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005). Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, In this framework, the posterior probability Pr(e1I |f1J ) is directly maximized using a log-linear combination of feature functions hm (e1I , f1J ), such as a ngram language model or a translation model. When decoding, the denominator is dropped"
W06-3115,N03-1017,0,0.292419,"Models We used a log-linear approach (Och and Ney, 2002) in which a foreign language sentence f1J = f1 , f2 , ... fJ is translated into another language, i.e. English, e1I = e1 , e2 , ..., eI by seeking a maximum likelihood solution of eˆ 1I = argmax Pr(e1I |f1J ) (1) e1I  I , f J) λ h (e m m m=1 1 1 = argmax P (2) P M ′ I′ , f J ) I ′ exp λ h (e I e1 ′ m m m=1 e 1 1 exp P M 1 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-based translation in which translation is realized as a set of paired production rules (Chiang, 2005). Section 2 discusses those two models and details extraction algorithms, decoding algorithms and feature functions. We also explored three types of corpus preprocessing in Section 3. As expected, different tokenization would lead to different word alignments which, in turn, resulted in the divergence of the extracted phrase/rule size. In our method, In this framework, the posterior probability Pr(e1I |f1J ) is directly maximized using a log-linear combinati"
W06-3115,P02-1038,0,0.483885,"resent two translation systems experimented for the shared-task of “Workshop on Statistical Machine Translation,” a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronousCFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrasebased model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora. 2 Translation Models We used a log-linear approach (Och and Ney, 2002) in which a foreign language sentence f1J = f1 , f2 , ... fJ is translated into another language, i.e. English, e1I = e1 , e2 , ..., eI by seeking a maximum likelihood solution of eˆ 1I = argmax Pr(e1I |f1J ) (1) e1I  I , f J) λ h (e m m m=1 1 1 = argmax P (2) P M ′ I′ , f J ) I ′ exp λ h (e I e1 ′ m m m=1 e 1 1 exp P M 1 1 Introduction We contrasted two translation methods for the Workshop on Statistical Machine Translation (WMT2006) shared-task. One is a phrase-based translation in which a phrasal unit is employed for translation (Koehn et al., 2003). The other is a hierarchical phrase-b"
W06-3115,J03-1002,0,0.0144663,"h sentence. In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.1 2.3 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively exj+m tracts phrase pairs ( f j , ei+n i ) from a sentence pair I J ( f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003): • Re"
W06-3115,J04-4002,0,0.0486513,"e top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.1 2.3 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively exj+m tracts phrase pairs ( f j , ei+n i ) from a sentence pair I J ( f1 , e1 ) that do not violate the word alignment constraints a. In the hierarchical phrase-based model, production rules are accumulated by computing “holes” for extracted contiguous phrases (Chiang, 2005): Our phrase-based model uses a standard pharaoh feature functions listed as follows (Koehn et al., 2003): • Relative-count based phrase translation probabilities in both directions. • Lexically weight"
W06-3115,P06-1098,1,0.825285,"ociated with ∼. existing hypothesis, new hypothesis is generated by consuming a phrase translation pair that covers untranslated foreign word positions. The score for the newly generated hypothesis is updated by combining the scores of feature functions described in Section 2.3. The English side of the phrase is simply concatenated to form a new prefix of English sentence. In the hierarchical phrase-based model, decoding is realized as an Earley-style top-down parser on the foreign language side with a beam search strategy synchronized with the cardinality of already translated foreign words (Watanabe et al., 2006). The major difference to the phrase-based model’s decoder is the handling of non-terminals, or holes, in each rule. 2.1 2.3 Phrase/Rule Extraction The phrase extraction algorithm is based on those presented by Koehn et al. (2003). First, manyto-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004). Second, phrase translation pairs are extracted from the word aligned corpus (Koehn et al., 2003). The method exhaustively exj+m tracts phrase pairs"
W06-3115,N04-1021,0,\N,Missing
W06-3115,P03-1021,0,\N,Missing
W18-6314,D11-1033,0,0.835577,"pic in the machine translation (MT) field. Recent research has found that data noise has a bigger impact on neural machine translation (NMT) than on statistical machine translation (Khayrallah and Koehn, 2018), but learning what data quality (or noise) means in NMT and how to make NMT training robust to data noise remains an open research question. On the other hand, a rich body of MT data research focuses on domain data relevance and selection for domain adaptation purpose. As a result, effective and successful methods have been published and shown to work for both SMT and NMT. For example, (Axelrod et al., 2011) introduce a metric for measuring the data relevance to a domain by using n-gram language models (LM). (van der Wees et al., 2017) employ a neuralnetwork version of it and propose a graduallyrefining strategy to dynamically schedule data during NMT training. In these methods, a large amount of in-domain data are used to help measure data domain relevance. • How to measure noise? • How does noise dynamically interact with the training progress? • How to denoise the model training with a small, trusted parallel dataset? In the denoising scenario, the trusted data would be the counterpart of in-d"
W18-6314,W17-4712,0,0.0274715,"raining. It uses a small amount of trusted data to help models measure noise in a sentence pair. The noise is defined based on comparison between a pair of a noisy NMT model and another, slightly denoised NMT model, inspired by the contrastive in-domain LM vs out-of-domain LM idea. It employs online data selection to sort sentence pairs by noise level so that the model is trained on gradually noise-reduced data batches. We show that language model based domain data selection method as is does not work well whereas the proposed approach is quite effective in denoising NMT training. 134 4 2017; Britz et al., 2017; Matsoukas et al., 2009). In the context of denoising, the quality that the ordering uses would be the amount of noise in a sentence pair, not (only) how much the data fits the domain of interest. SMT models tend to be fairly robust to data noise and denoising in SMT seems to have been a lightly studied topic. For example, (Mediani, 2017) uses a small, clean seed corpus and designs classifier filter to identify noisy data with lexical features; and also there is a nice list of works accumulated over years, compiled on the SMT Research Survey Wiki1 . The importance of NMT denoising has been in"
W18-6314,K16-1031,0,0.12152,", (van der Wees et al., 2017) employ a neural-network version of it along with a dynamic data selection idea and achieve better domain data selection outcome. (Mansour et al., 2011) compute the CED using IBM translation Model 1 and achieve the best domain data selection/filtering effect for SMT combined with LM selection; The case of partial or misalignments with a bilingual scoring mechanism rather than LMs is also discussed. Another effective method to distinguish domain relevance is to build a classifier. A small amount of trusted parallel data is used in classifier training. For example, (Chen and Huang, 2016) use semisupervised convolutional neural networks (CNNs) as LMs to select domain data. Trusted data is used to adapt the classifier/selector. (Chen et al., 2016) introduce a bilingual data selection method that uses CNNs on bitokens; The method uses parallel trusted data and is targeted at selecting data to improve SMT; In addition to domain relevance, the work also examines its noise-screening capability; The method is tried on NMT and does not seem to improve. Previous work on domain data selection has shown that the order in which data are scheduled matters a lot for NMT training, a researc"
W18-6314,P18-1008,1,0.810911,"training loss term (thus the training process). 5 Our Approach We first define how to measure noise with the help of the small trusted dataset. Then we use it to control the schedule of the data batches to train the NMT model. Online NMT Training We usually train NMT models with online optimization, e.g., stochastic gradient descent. At a time step t, we have an NMT model p(y|x; θt ) translating from sentence x to y with parameterization θt . The model choice could be, for example, RNN-based (Wu et al., 2016), CNNbased (Gehring et al., 2017), Transformer model (Vaswani et al., 2017) or RNMT+ (Chen et al., 2018). To move p(y|x; θt ) to next step, t + 1, a random data batch bt is normally used to compute the cross entropy loss. The prediction accuracy of p(y|x; θt ) does not depend on the data of this batch alone, but on all data the model has seen so far. 1 The Denoising Problem 5.1 Incremental denoising with trusted data e trained on noisy data D, e Given a model p(y|x; θ) a practical way to denoise it with a small amount b would be to simply fine-tune the of trusted data D model on the trusted data, considering that a small trusted dataset alone is not enough to reliably train an NMT model from scr"
W18-6314,W18-2709,0,0.367368,"different quality that has been shown to affect NMT performance in particular. In MT, the use of web crawl, automatic methods for parallel data mining, sentence alignment provide us with parallel data of variable quality from many points of view: sentence breaking, poor sentence alignments, translations, domain adequacy, tokenization and so forth. To deal with such data noise, a commonly used practice is (static) data filtering with simple heuristics or classification. The NMT community increasingly realizes that this type of quality matters for general NMT translation accuracy. For example, (Khayrallah and Koehn, 2018) studies the types of data noise and their impact on NMT; WMT 2018 introduces a Parallel Corpus Filtering task on noisy webcrawled data. Unfortunately, the ingredients that made domain data selection methods successful have not been studied in the NMT denoising context. Specifically, Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neur"
W18-6314,N18-1136,0,0.0628559,"Missing"
W18-6314,W04-3250,0,0.0770732,"tstrapped test at p < 0.05, P3 is significantly better than P2, P3 than P1, P2 than P1, on all test sets. W3 is significantly better than W1 on n2014. P2 vs P3 shows that the online denoising approach reduced the training noise further more and gains +1.2 n2014 BLEU, +1.9 d2015 BLEU and +2.2 patent BLEU, on top of incremental denoising on trusted data. On the WMT dataset, W2 vs W3 shows that, even though the trusted data does not directly help, the online denoising helps by +0.7 n2014 BLEU, +0.6 d2015 BLEU and +0.4 patent BLEU. We carried out paired bootstrapped statistical significance test (Koehn, 2004) between systems, at p < 0.05, P3 is significantly better than P2, P3 than P1, P2 than P1, across all test sets; W3 is significantly better than W1 only on n2014. seems slightly better than the WMT one in discerning noisier sentence pairs. We speculate this is because the noisy Paracrawl data “amplifies” the contrastive effect of the pair of models. 6.4 +7.5 BLEU on patent. The Paracrawl experiments and the above rating ranking curves (Figure 1) indicate the power of simple incremental denoising on trusted data (Section 5.1) when the background data is very noisy. In NMT domain adaptation lite"
W18-6314,D17-1155,0,0.078132,"Missing"
W18-6314,D17-1147,0,0.0837062,"Missing"
W18-6314,2011.iwslt-papers.5,0,0.0428863,"cross entropy difference (CED) between an in-domain and an out-of-domain language models. For example, (Moore and Lewis, 2010) selects LM training data with CED according to an in-domain LM and a generic LM. (Axelrod et al., 2011) propose the contrastive data selection idea to select parallel domain data. It ranks data by the bilingual CED that is computed, for each language, with a generic n-gram LM and a domain one. Even more recently, (van der Wees et al., 2017) employ a neural-network version of it along with a dynamic data selection idea and achieve better domain data selection outcome. (Mansour et al., 2011) compute the CED using IBM translation Model 1 and achieve the best domain data selection/filtering effect for SMT combined with LM selection; The case of partial or misalignments with a bilingual scoring mechanism rather than LMs is also discussed. Another effective method to distinguish domain relevance is to build a classifier. A small amount of trusted parallel data is used in classifier training. For example, (Chen and Huang, 2016) use semisupervised convolutional neural networks (CNNs) as LMs to select domain data. Trusted data is used to adapt the classifier/selector. (Chen et al., 2016"
W18-6314,D09-1074,0,0.0592484,"mall amount of trusted data to help models measure noise in a sentence pair. The noise is defined based on comparison between a pair of a noisy NMT model and another, slightly denoised NMT model, inspired by the contrastive in-domain LM vs out-of-domain LM idea. It employs online data selection to sort sentence pairs by noise level so that the model is trained on gradually noise-reduced data batches. We show that language model based domain data selection method as is does not work well whereas the proposed approach is quite effective in denoising NMT training. 134 4 2017; Britz et al., 2017; Matsoukas et al., 2009). In the context of denoising, the quality that the ordering uses would be the amount of noise in a sentence pair, not (only) how much the data fits the domain of interest. SMT models tend to be fairly robust to data noise and denoising in SMT seems to have been a lightly studied topic. For example, (Mediani, 2017) uses a small, clean seed corpus and designs classifier filter to identify noisy data with lexical features; and also there is a nice list of works accumulated over years, compiled on the SMT Research Survey Wiki1 . The importance of NMT denoising has been increasingly realized. (Kha"
W18-6314,P10-2041,0,0.119934,"133–143 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64014 zh zh-gloss en 2 gongche zhan zai nali? bus stop is where? Where is the bus stop? For bus 81. Related Research One line of research that is related to our work is data selection for machine translation. It has been mostly studied in the domain adaptation context. Under this context, a popular metric to measure domain relevance of data is based on cross entropy difference (CED) between an in-domain and an out-of-domain language models. For example, (Moore and Lewis, 2010) selects LM training data with CED according to an in-domain LM and a generic LM. (Axelrod et al., 2011) propose the contrastive data selection idea to select parallel domain data. It ranks data by the bilingual CED that is computed, for each language, with a generic n-gram LM and a domain one. Even more recently, (van der Wees et al., 2017) employ a neural-network version of it along with a dynamic data selection idea and achieve better domain data selection outcome. (Mansour et al., 2011) compute the CED using IBM translation Model 1 and achieve the best domain data selection/filtering effec"
W18-6314,W18-6319,0,0.0606779,"Missing"
W18-6314,E17-2045,0,0.0549295,"hod that uses CNNs on bitokens; The method uses parallel trusted data and is targeted at selecting data to improve SMT; In addition to domain relevance, the work also examines its noise-screening capability; The method is tried on NMT and does not seem to improve. Previous work on domain data selection has shown that the order in which data are scheduled matters a lot for NMT training, a research that is relevant to curriculum learning (Bengio et al., 2009) in machine learning literature. (van der Wees et al., 2017) show the effectiveness of a nice “gradually-refining” dynamic data schedule. (Sajjad et al., 2017) find the usefulness of a similar idea, called model stacking for NMT domain adaptation. Data ordering could be viewed as a way of data weighting, which can be also done by example weighting/mixing, e.g., (Wang et al., Table 1: A noisy sentence pair. a part of the English sentence does not align to anything on the Chinese side, yet the pair contains some translation and the sentences are fluent. An LM-based domain-data selection method would generally treat it as a suitable domain example for building a travel NMT model and may not consider this noise. A simple data filtering method based on l"
watanabe-etal-2002-statistical,N01-1009,0,\N,Missing
watanabe-etal-2002-statistical,J93-2003,0,\N,Missing
watanabe-etal-2002-statistical,C00-2123,0,\N,Missing
watanabe-etal-2002-statistical,P01-1008,0,\N,Missing
watanabe-etal-2002-statistical,1999.mtsummit-1.34,1,\N,Missing
watanabe-etal-2002-statistical,shimohata-sumita-2002-automatic,1,\N,Missing
