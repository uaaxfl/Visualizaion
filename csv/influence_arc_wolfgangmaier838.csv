2009.jeptalnrecital-court.40,W01-1807,0,0.0400543,"Missing"
2009.jeptalnrecital-court.40,E99-1008,0,0.0745236,"Missing"
2009.jeptalnrecital-court.40,2000.iwpt-1.8,0,0.0978387,"Missing"
2009.jeptalnrecital-court.40,W05-1502,0,0.0371231,"Missing"
2009.jeptalnrecital-court.40,W08-2307,0,0.0374122,"Missing"
2009.jeptalnrecital-court.40,J91-3002,0,0.181186,"Missing"
2009.jeptalnrecital-court.40,C08-2026,0,0.0385433,"Missing"
2009.jeptalnrecital-court.40,C02-1028,0,0.0362691,"Missing"
2020.sigdial-1.38,D18-2029,0,0.021668,"ue actions treating slots s and values v in S˜ as individual γ: X ˜ (a, a0 ) = δact,act 0 + CM m(a, ˜ a0 , γ) (8) 15,000 1.00 0.99 3.79 3.81 4.36 4.43 20,000 1.00 0.97 3.86 3.73 4.15 4.62 25,000 1.00 0.98 3.77 3.85 4.37 4.30 30,000 1.00 0.96 3.71 3.87 4.49 4.41 ∈S˜ a0 35,000 0.99 0.96 3.73 3.84 4.42 4.46 40,000 1.00 0.94 3.77 3.77 4.35 4.75 A concept match of two dialogue actions a and is thus defined by ˜ (a, a0 ) CM CM (a, a0 ) = (9) ˜ 1 + |S| and the concept match rate by |C| 1 X CMR = CM (a, a0 ) . |C| (10) i=1 Cosine Similarity and angular similarity The Universal Sentence Encoder (USE) (Cer et al., 2018) is a generic sentence encoder which employs two measures for the computation of the distances between encoded sentences, namely cosine similarity and angular similarity: BERTscore The BERTscore (Zhang* et al., 2020) is an automatic evaluation metric used for text generation that has shown a high correlation with human ratings. Given a function β which returns the BERT embedding (Devlin et al., 2018) for a given token, recall and precision along with the F1-score are computed for a reference p and a candidate p0 as 1 X RBERT = max β(pi )&gt; β(p0j ) , (14) |p |p ∈p p0j ∈p0 i 1 X max β(pi )&gt; β(p0j"
2020.sigdial-1.38,A00-2027,0,0.458263,"Missing"
2020.sigdial-1.38,W11-2019,0,0.0353477,"ponse directly based on text input thus combining user input interpretation, dialogue context integration, and dialogue response selection in one model. Absolute measures to evaluate the performance of these behaviour models through the interaction with real or simulated users are, for example, task success or dialogue length (Gaˇsi´c and Young, 2014; Lemon and Pietquin, 2007; Daubigney et al., 2012; Levin and Pieraccini, 1997; Young et al., 2013; Su et al., 2016; Ultes et al., 2015; Wen et al., 2017). Other measures are user satisfaction (Walker et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2015). Text comparison metrics like BLEU (Papineni et al., 2002) have been adopted from machine translation to"
2020.sigdial-1.38,W14-4337,0,0.0261985,"Statistical Spoken Dialogue System Toolkit (Ultes et al., 2017b) with an agendabased user simulator (Schatzmann and Young, 2009). For each trial, a GP-SARSA (Gaˇsi´c and Young, 2014) policy model was trained—a learning algorithm known for its high sample-efficiency— with dialogues in the Cambridge restaurants domain about finding restaurants in Cambridge, UK. The domain comprises three slots used as search constraints (area, price range, food type). For belief state tracking—updating the probability distribution over all possible dialogue states in each turn—the focus belief tracker is used (Henderson et al., 2014). Prompts were generated using the SC-LSTM (Wen et al., 2015) natural language generator implementation of PyDial. To ensure consistency, the standardised Environment 1 from Casanueva et al. (2017) is used. The interaction quality is estimated using a BiLSTM with self-attention as described by Ultes (2019). For each trial of the task success and the interaction quality set-ups, a policy was trained with 40,000 dialogues and evaluated after each 1,000 training dialogues with 100 evaluation dialogues. The absolute performance of each set-up in terms of task success rate (TSR), average interactio"
2020.sigdial-1.38,P16-1094,0,0.0210517,"., 2015; Wen et al., 2017). Other measures are user satisfaction (Walker et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2015). Text comparison metrics like BLEU (Papineni et al., 2002) have been adopted from machine translation to evaluate how well the system response matches the one in the database, e.g., (Li et al., 2016b; Sordoni et al., 2015). This way of evaluating has been criticised widely as a system response that is different from the one in the data base can still be a valid system response simply leading to a different subsequent dialogue. Furthermore, the BLEU score evaluation hardly correlates with human judgements (Liu et al., 2016; Novikova et al., 2017). Dismissing text similarit"
2020.sigdial-1.38,D16-1127,0,0.0297631,"., 2015; Wen et al., 2017). Other measures are user satisfaction (Walker et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2015). Text comparison metrics like BLEU (Papineni et al., 2002) have been adopted from machine translation to evaluate how well the system response matches the one in the database, e.g., (Li et al., 2016b; Sordoni et al., 2015). This way of evaluating has been criticised widely as a system response that is different from the one in the data base can still be a valid system response simply leading to a different subsequent dialogue. Furthermore, the BLEU score evaluation hardly correlates with human judgements (Liu et al., 2016; Novikova et al., 2017). Dismissing text similarit"
2020.sigdial-1.38,D16-1230,0,0.0511482,"Missing"
2020.sigdial-1.38,W15-4640,0,0.0193168,"., 2017). Other measures are user satisfaction (Walker et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2015). Text comparison metrics like BLEU (Papineni et al., 2002) have been adopted from machine translation to evaluate how well the system response matches the one in the database, e.g., (Li et al., 2016b; Sordoni et al., 2015). This way of evaluating has been criticised widely as a system response that is different from the one in the data base can still be a valid system response simply leading to a different subsequent dialogue. Furthermore, the BLEU score evaluation hardly correlates with human judgements (Liu et al., 2016; Novikova et al., 2017). Dismissing text similarity measures as not use"
2020.sigdial-1.38,D17-1238,0,0.0443608,"Missing"
2020.sigdial-1.38,P02-1040,0,0.108904,"et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2015). Text comparison metrics like BLEU (Papineni et al., 2002) have been adopted from machine translation to evaluate how well the system response matches the one in the database, e.g., (Li et al., 2016b; Sordoni et al., 2015). This way of evaluating has been criticised widely as a system response that is different from the one in the data base can still be a valid system response simply leading to a different subsequent dialogue. Furthermore, the BLEU score evaluation hardly correlates with human judgements (Liu et al., 2016; Novikova et al., 2017). Dismissing text similarity measures as not useful for dialogue evaluation, however, is overhasty and shor"
2020.sigdial-1.38,P16-1056,0,0.0390256,"Missing"
2020.sigdial-1.38,N15-1020,0,0.0534788,"Missing"
2020.sigdial-1.38,P16-1230,1,0.935656,"Missing"
2020.sigdial-1.38,W19-5902,1,0.810791,"of 19 down to −T which is consistent with related work in which binary task success (TS) was used to define the reward as: RT S = −T + 1T S · 20 , 0.8 0.2 4.1.1 Policy Training For the evaluation, two policies are trained to reflect two different set-ups. One set-up uses the conventional task success as main reward component as heavily used within the literature (Gaˇsi´c and Young, 2014; Vandyke et al., 2015; Su et al., 2016, e.g.) and the other set-up uses the interaction quality (IQ) (Schmitt and Ultes, 2015) representing user satisfaction as described by Ultes et al. (Ultes et al., 2017a; Ultes, 2019). IQ is defined on a five-point scale from five (satisfied) down to one (extremely unsatisfied). To derive a reward from this value, RIQ = −T + (iq − 1) · 5 0.9 Total Match Rate # Training Dialogues 0.978 0.989 0.984 1.000 Trial 0 10,000 20,000 30,000 40,000 2 Table 2: Similarity measures for testing convergence of each trial (random seed) for RT S empmloying task success and RIQ employing interaction quality for Creal . ment using the PyDial Statistical Spoken Dialogue System Toolkit (Ultes et al., 2017b) with an agendabased user simulator (Schatzmann and Young, 2009). For each trial, a GP-SA"
2020.sigdial-1.38,W15-4649,1,0.716768,"s then transferred into text by a natural language generator. An implicit behaviour model uses a neural network to learn a text response directly based on text input thus combining user input interpretation, dialogue context integration, and dialogue response selection in one model. Absolute measures to evaluate the performance of these behaviour models through the interaction with real or simulated users are, for example, task success or dialogue length (Gaˇsi´c and Young, 2014; Lemon and Pietquin, 2007; Daubigney et al., 2012; Levin and Pieraccini, 1997; Young et al., 2013; Su et al., 2016; Ultes et al., 2015; Wen et al., 2017). Other measures are user satisfaction (Walker et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et a"
2020.sigdial-1.38,P17-4013,1,0.921605,"Missing"
2020.sigdial-1.38,P97-1035,0,0.656738,"aviour model uses a neural network to learn a text response directly based on text input thus combining user input interpretation, dialogue context integration, and dialogue response selection in one model. Absolute measures to evaluate the performance of these behaviour models through the interaction with real or simulated users are, for example, task success or dialogue length (Gaˇsi´c and Young, 2014; Lemon and Pietquin, 2007; Daubigney et al., 2012; Levin and Pieraccini, 1997; Young et al., 2013; Su et al., 2016; Ultes et al., 2015; Wen et al., 2017). Other measures are user satisfaction (Walker et al., 1997; Chu-Carroll and Nickerson, 2000; Dzikovska et al., 2011; Ultes et al., 2015; Wen et al., 2016; Ultes et al., 2017a) or quality of interaction (M¨oller et al., 2008; Schmitt and Ultes, 2015). All are often acquired through interaction-based studies1 . Others have employed corpus-based evaluation by comparing textual system responses with transcriptions of actual interaction as absolute evaluation criterion where the response in the corpus is treated as ground truth (Serban et al., 2016; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2015). Text comparison metrics like BLEU (Papineni et"
2020.sigdial-1.38,N16-1015,0,0.0392986,"Missing"
2020.sigdial-1.38,D15-1199,0,0.0417222,"Missing"
2020.sigdial-1.38,E17-1042,1,0.895432,"Missing"
2021.sigdial-1.42,2020.findings-emnlp.347,0,0.027663,"s principal reward components. Previous work on RL-based dialogue policy learning focused either on TS or US as the principal reward component. Task success can be computed (Schatzmann and Young, 2009; Gaˇsi´c et al., 2013, e.g.) or estimated (El Asri et al., 2014b; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016) only when information about the task and underlying goal are known in advance. Integrating US into the reward by using the PARADISE (Walker et al., 1997) framework (Walker, 2000; Rieser and Lemon, 2008; El Asri et al., 2013, e.g.) or through a measure called response quality (Bodigutla et al., 2020, e.g.). Both are not suitable for this research as PARADISE directly incorporates task knowledge and response quality incorporates functionality of back-end services. Ultes et al. (2017a; 2019) showed that a pretrained interaction quality reward estimator can lead to a policy that is able to produce successful dialogues while achieving higher user satisfaction. This has been shown across different domains, including the domain that is used in this work. However, success declines with increasing noise in the communication channel, increasing differences in domain structure, and less co-operati"
2021.sigdial-1.42,D18-1547,1,0.841966,"Missing"
2021.sigdial-1.42,W17-5520,1,0.811218,"nnotated system-user-exchanges from the Let’s Go bus information system (Raux et al., 2006; Eskenazi et al., 2008) of Carnegie Mellon University in Pittsburgh, PA. The system provided information about bus schedules and connections to actual users with real needs and was live from 2006 until 2016. Each turn of these 200 dialogues has been annotated with IQ (representing the quality of the dialogue up to the current turn) by three experts. The final IQ label has been assigned using the median of the three individual labels. Subsequent work applied deep neural networks achieving an UAR of 0.45 (Rach et al., 2017) and a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) achieving an UAR of 0.54 (Ultes, 2019). Previous work has used the LEGO corpus with a full IQ feature set (which includes additional partly domain-related information) achieving an UAR in a turn-wise cross-validation setup of 0.55 using ordinal regression (El Asri et al., 2014a), 0.53 using a two-level SVM approach (Ultes and Minker, 2013), and 0.51 using a hybrid-HMM (Ultes and Minker, 2014). Human performance on the same task is 0.69 UAR (Schmitt and Ultes, 2015). Multi-objective Reinforcement Learning The task of reinforcement Le"
2021.sigdial-1.42,rieser-lemon-2008-automatic,0,0.0581693,"ed dialog policy and (2) analysing the performance and learned behaviour when blending TS and IQ as principal reward components. Previous work on RL-based dialogue policy learning focused either on TS or US as the principal reward component. Task success can be computed (Schatzmann and Young, 2009; Gaˇsi´c et al., 2013, e.g.) or estimated (El Asri et al., 2014b; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016) only when information about the task and underlying goal are known in advance. Integrating US into the reward by using the PARADISE (Walker et al., 1997) framework (Walker, 2000; Rieser and Lemon, 2008; El Asri et al., 2013, e.g.) or through a measure called response quality (Bodigutla et al., 2020, e.g.). Both are not suitable for this research as PARADISE directly incorporates task knowledge and response quality incorporates functionality of back-end services. Ultes et al. (2017a; 2019) showed that a pretrained interaction quality reward estimator can lead to a policy that is able to produce successful dialogues while achieving higher user satisfaction. This has been shown across different domains, including the domain that is used in this work. However, success declines with increasing n"
2021.sigdial-1.42,schmitt-etal-2012-parameterized,1,0.761124,"atisfied) down to 1 (extremely unsatisfied). The input consists of domain-independent interaction parameters that incorporate turn-level information from the automatic speech recognition (ASR) output and the preceding system action. Furthermore, temporal features are computed by taking sums, means or counts of the turn-based information for a window of the last three system-user-exchanges1 and the complete dialogue. Ultes et al. (2017a, 2015) use a feature set of 16 parameters to train a support vector machine (SVM) (Vapnik, 1995; Chang and Lin, 2011) with linear kernel using the LEGO corpus (Schmitt et al., 2012) achieving an unweighted average recall2 (UAR) of 0.44 in a dialog-wise cross-validation setup. The LEGO corpus consists of 200 dialogues with a total of 4,885 annotated system-user-exchanges from the Let’s Go bus information system (Raux et al., 2006; Eskenazi et al., 2008) of Carnegie Mellon University in Pittsburgh, PA. The system provided information about bus schedules and connections to actual users with real needs and was live from 2006 until 2016. Each turn of these 200 dialogues has been annotated with IQ (representing the quality of the dialogue up to the current turn) by three exper"
2021.sigdial-1.42,P16-1230,1,0.870603,"Missing"
2021.sigdial-1.42,W19-5902,1,0.763728,"ng, dialogue state tracking, dialogue policy execution, and natural language generation. For many years, research on modular spoken dialogue systems has rendered this decision making task of finding the optimal policy as a reinforcement learning (RL) problem that optimises an expected long-term future reward. The principal reward component has previously been either task success (TS) (Gaˇsi´c and Young, 2014; Daubigney et al., 2012; Levin and Pieraccini, 1997; Young et al., 2013; Su et al., 2016, 2015; Lemon and Pietquin, 2007; Ultes et al., 2018) or user satisfaction (US) (e.g. Walker, 2000; Ultes, 2019) independently. The goal of this paper is to apply both, TS and US, as principal reward components at the same time and to gain insights into the learned dialogue behaviour. This requires a learning setup that allows multiple principle reward components simultaneously and an analysis method with a structured procedure to probe learned dialog policies. This is achieved through a multi-objective reinforcement learning (MORL) setup (Ultes et al., 2017b) and an analysis method that builds upon work from Ultes and Maier (2020). The chosen MORL setup employs a linear reward scalarisation that combin"
2021.sigdial-1.42,W17-5509,1,0.87458,"Missing"
2021.sigdial-1.42,W18-5032,1,0.891372,"Missing"
2021.sigdial-1.42,W15-4649,1,0.876413,"Missing"
2021.sigdial-1.42,2020.sigdial-1.38,1,0.908065,"Pietquin, 2007; Ultes et al., 2018) or user satisfaction (US) (e.g. Walker, 2000; Ultes, 2019) independently. The goal of this paper is to apply both, TS and US, as principal reward components at the same time and to gain insights into the learned dialogue behaviour. This requires a learning setup that allows multiple principle reward components simultaneously and an analysis method with a structured procedure to probe learned dialog policies. This is achieved through a multi-objective reinforcement learning (MORL) setup (Ultes et al., 2017b) and an analysis method that builds upon work from Ultes and Maier (2020). The chosen MORL setup employs a linear reward scalarisation that combines the principal reward components TS and interaction quality (IQ) (Schmitt and Ultes, 2015)—a more objective measure for modelling US. The two main contributions of this work are (1) a universal behaviour analysis method that aims at investigating the influence of multiple learning objectives on the learned dialog policy and (2) analysing the performance and learned behaviour when blending TS and IQ as principal reward components. Previous work on RL-based dialogue policy learning focused either on TS or US as the princi"
2021.sigdial-1.42,W13-4018,1,0.763246,"ue up to the current turn) by three experts. The final IQ label has been assigned using the median of the three individual labels. Subsequent work applied deep neural networks achieving an UAR of 0.45 (Rach et al., 2017) and a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) achieving an UAR of 0.54 (Ultes, 2019). Previous work has used the LEGO corpus with a full IQ feature set (which includes additional partly domain-related information) achieving an UAR in a turn-wise cross-validation setup of 0.55 using ordinal regression (El Asri et al., 2014a), 0.53 using a two-level SVM approach (Ultes and Minker, 2013), and 0.51 using a hybrid-HMM (Ultes and Minker, 2014). Human performance on the same task is 0.69 UAR (Schmitt and Ultes, 2015). Multi-objective Reinforcement Learning The task of reinforcement Learning (RL) is to find the optimal policy π ∗ that maximises a potentially delayed objective (the reward function r) (Sutton and Barto, 1998). In multi-objective reinforcement learning (MORL), the objective function consist of multiple dimensions so that a reward r becomes a vector r = (r1 , r2 , . . . , rm ), where m is the number of objectives. A scalarisation function f uses weights w for the diff"
2021.sigdial-1.42,W14-4328,1,0.784469,"IQ label has been assigned using the median of the three individual labels. Subsequent work applied deep neural networks achieving an UAR of 0.45 (Rach et al., 2017) and a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) achieving an UAR of 0.54 (Ultes, 2019). Previous work has used the LEGO corpus with a full IQ feature set (which includes additional partly domain-related information) achieving an UAR in a turn-wise cross-validation setup of 0.55 using ordinal regression (El Asri et al., 2014a), 0.53 using a two-level SVM approach (Ultes and Minker, 2013), and 0.51 using a hybrid-HMM (Ultes and Minker, 2014). Human performance on the same task is 0.69 UAR (Schmitt and Ultes, 2015). Multi-objective Reinforcement Learning The task of reinforcement Learning (RL) is to find the optimal policy π ∗ that maximises a potentially delayed objective (the reward function r) (Sutton and Barto, 1998). In multi-objective reinforcement learning (MORL), the objective function consist of multiple dimensions so that a reward r becomes a vector r = (r1 , r2 , . . . , rm ), where m is the number of objectives. A scalarisation function f uses weights w for the different objectives to map the vector representation to a"
2021.sigdial-1.42,P17-4013,1,0.818649,"Pieraccini, 1997; Young et al., 2013; Su et al., 2016, 2015; Lemon and Pietquin, 2007; Ultes et al., 2018) or user satisfaction (US) (e.g. Walker, 2000; Ultes, 2019) independently. The goal of this paper is to apply both, TS and US, as principal reward components at the same time and to gain insights into the learned dialogue behaviour. This requires a learning setup that allows multiple principle reward components simultaneously and an analysis method with a structured procedure to probe learned dialog policies. This is achieved through a multi-objective reinforcement learning (MORL) setup (Ultes et al., 2017b) and an analysis method that builds upon work from Ultes and Maier (2020). The chosen MORL setup employs a linear reward scalarisation that combines the principal reward components TS and interaction quality (IQ) (Schmitt and Ultes, 2015)—a more objective measure for modelling US. The two main contributions of this work are (1) a universal behaviour analysis method that aims at investigating the influence of multiple learning objectives on the learned dialog policy and (2) analysing the performance and learned behaviour when blending TS and IQ as principal reward components. Previous work on"
2021.sigdial-1.42,W12-1819,1,0.770554,"s on multi-objective reinforcement learning and interaction quality modelling: Interaction Quality Estimation The interaction quality (IQ) (Schmitt and Ultes, 2015) represents a less subjective variant of user satisfaction: instead of being acquired from users directly, experts annotate pre-recorded dialogues to avoid the large variance that is often encountered when users rate their dialogues directly (Schmitt and Ultes, 2015). Interaction quality shows a good correlation with user satisfaction (Ultes et al., 2013) and fulfils the requirements necessary for its application in dialog systems (Ultes et al., 2012, 2016). Estimating IQ has been cast as a turn-level classification problem where the target classes are the distinct IQ values ranging from 5 (satisfied) down to 1 (extremely unsatisfied). The input consists of domain-independent interaction parameters that incorporate turn-level information from the automatic speech recognition (ASR) output and the preceding system action. Furthermore, temporal features are computed by taking sums, means or counts of the turn-based information for a window of the last three system-user-exchanges1 and the complete dialogue. Ultes et al. (2017a, 2015) use a fe"
2021.sigdial-1.42,N13-1064,1,0.781034,"in Sections 5 and 6. 2 Preliminaries The presented work builds upon previously published approaches on multi-objective reinforcement learning and interaction quality modelling: Interaction Quality Estimation The interaction quality (IQ) (Schmitt and Ultes, 2015) represents a less subjective variant of user satisfaction: instead of being acquired from users directly, experts annotate pre-recorded dialogues to avoid the large variance that is often encountered when users rate their dialogues directly (Schmitt and Ultes, 2015). Interaction quality shows a good correlation with user satisfaction (Ultes et al., 2013) and fulfils the requirements necessary for its application in dialog systems (Ultes et al., 2012, 2016). Estimating IQ has been cast as a turn-level classification problem where the target classes are the distinct IQ values ranging from 5 (satisfied) down to 1 (extremely unsatisfied). The input consists of domain-independent interaction parameters that incorporate turn-level information from the automatic speech recognition (ASR) output and the preceding system action. Furthermore, temporal features are computed by taking sums, means or counts of the turn-based information for a window of the"
2021.sigdial-1.42,P97-1035,0,0.701459,"e of multiple learning objectives on the learned dialog policy and (2) analysing the performance and learned behaviour when blending TS and IQ as principal reward components. Previous work on RL-based dialogue policy learning focused either on TS or US as the principal reward component. Task success can be computed (Schatzmann and Young, 2009; Gaˇsi´c et al., 2013, e.g.) or estimated (El Asri et al., 2014b; Su et al., 2015; Vandyke et al., 2015; Su et al., 2016) only when information about the task and underlying goal are known in advance. Integrating US into the reward by using the PARADISE (Walker et al., 1997) framework (Walker, 2000; Rieser and Lemon, 2008; El Asri et al., 2013, e.g.) or through a measure called response quality (Bodigutla et al., 2020, e.g.). Both are not suitable for this research as PARADISE directly incorporates task knowledge and response quality incorporates functionality of back-end services. Ultes et al. (2017a; 2019) showed that a pretrained interaction quality reward estimator can lead to a policy that is able to produce successful dialogues while achieving higher user satisfaction. This has been shown across different domains, including the domain that is used in this w"
C10-1061,W07-1506,0,0.204133,"candidate to be used for data-driven parsing. We evaluate our parser with a grammar extracted from the German NeGra treebank. Our experiments show that datadriven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality. However, given the expressivity restrictions of PCFG, work on data-driven parsing has mostly excluded non-local dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded, while in NeGra, resp. TIGER, tree transformations are applied which resolve the crossing branches (K¨ubler, 2005; Boyd, 2007, e.g.). Especially for these treebanks, such a transformation is questionable, since it is non-reversible and implies information loss. 1 Introduction Data-driven parsing has largely been dominated by Probabilistic Context-Free Grammar (PCFG). The use of PCFG is tied to the annotation principles of popular treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., l"
C10-1061,J98-2004,0,0.0189416,"ternational Conference on Computational Linguistics (Coling 2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constitue"
C10-1061,P03-1013,0,0.191753,"ents with NeGra Our results are not directly comparable with PCFG parsing results, since LCFRS parsing is a 1 SIMPLE also proved to be infeasible to compute for the small set for the markovization settings v = 2 and h = 1 due to the greatly increased label set with this settings. harder task. However, since the EVALB metric coincides for constituents without crossing branches, in order to place our results in the context of previous work on parsing NeGra, we cite some of the results from the literature which were obtained using PCFG parsers2 : K¨ubler (2005) (Tab. 1, plain PCFG) obtains 69.4, Dubey and Keller (2003) (Tab. 5, sister-head PCFG model) 71.12, Rafferty and Manning (2008) (Tab. 2, Stanford parser with markovization v = 2 and h = 1) 77.2, and Petrov and Klein (2007) (Tab. 1, Berkeley parser) 80.1. Plaehn (2004) obtains 73.16 Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 81.27. The comparison shows that our system delivers competitive results. Additionally, when comparing this to PCFG parsing results, one has to keep in mind that LCFRS parse trees contain non-context-free infor"
C10-1061,N09-1061,0,0.0916083,"Missing"
C10-1061,P02-1018,0,0.0943343,". In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) have been established as a candidate for modeling both discontinuous constituents and nonprojective dependency trees as they occur in treebanks (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). LCFRS exte"
C10-1061,W06-1508,0,0.868586,"t of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maier and Kallmeyer, 2010). This article is mainly dedicated to the presentation of several methods for context summary estimation of parse items, and to an experimental evaluation of their usefulness. The estimates either act as figures-of-merit in a best-first parsing context or as estimates for A∗ parsing. Our evaluation shows that while our parser achieves a reasonable"
C10-1061,N03-1016,0,0.128145,"August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maie"
C10-1061,E09-1055,0,0.153123,"d in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) have been established as a candidate for modeling both discontinuous constituents and nonprojective dependency trees as they occur in treebanks (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). LCFRS extend CFG such that non-terminals can span tuples of possibly non537 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set o"
C10-1061,P04-1042,0,0.216389,"l., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) h"
C10-1061,W10-4415,1,0.88234,"2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maier and Kallmeyer, 2010). This article is mainly dedicated to the presentation of several methods for context summary estimation of parse items, and to an experimental evaluation of their usefulness. The estimates either act as figures-of-merit in a best-first parsing context or as estimates for A∗ parsing. Our evaluation shows that while our parser achieves a reasonable speed already without estimates, the estimates lead to a great reduction of the number of produced items, all while preserving the output quality. Sect. 2 and 3 of the paper introduce probabilistic LCFRS and the parsing algorithm. Sect. 4 presents di"
C10-1061,W10-1407,1,0.875031,"and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maier and Kallmeyer, 2010). This article is mainly dedicated to the presentation of several methods for context summary estimation of parse items, and to an experimental evaluation of their usefulness. The estimates either act as figures-of-merit in a best-first parsing context or as estimates for A∗ parsing. Our evaluation shows that while our parser achieves a reasonable speed already without estimates, the estimates lead to a great reduction of the number of produced items, all while preserving the output quality. Sect. 2 and 3 of the paper introduce probabilistic LCFRS and the parsing al"
C10-1061,H94-1020,0,0.0513173,"cal dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded, while in NeGra, resp. TIGER, tree transformations are applied which resolve the crossing branches (K¨ubler, 2005; Boyd, 2007, e.g.). Especially for these treebanks, such a transformation is questionable, since it is non-reversible and implies information loss. 1 Introduction Data-driven parsing has largely been dominated by Probabilistic Context-Free Grammar (PCFG). The use of PCFG is tied to the annotation principles of popular treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and"
C10-1061,J03-1006,0,0.0835254,"2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an"
C10-1061,N07-1051,0,0.0227591,"e small set for the markovization settings v = 2 and h = 1 due to the greatly increased label set with this settings. harder task. However, since the EVALB metric coincides for constituents without crossing branches, in order to place our results in the context of previous work on parsing NeGra, we cite some of the results from the literature which were obtained using PCFG parsers2 : K¨ubler (2005) (Tab. 1, plain PCFG) obtains 69.4, Dubey and Keller (2003) (Tab. 5, sister-head PCFG model) 71.12, Rafferty and Manning (2008) (Tab. 2, Stanford parser with markovization v = 2 and h = 1) 77.2, and Petrov and Klein (2007) (Tab. 1, Berkeley parser) 80.1. Plaehn (2004) obtains 73.16 Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 81.27. The comparison shows that our system delivers competitive results. Additionally, when comparing this to PCFG parsing results, one has to keep in mind that LCFRS parse trees contain non-context-free information about discontinuities. Therefore, a correct parse with our grammar is actually better than a correct CFG parse, evaluated with respect to a transformation o"
C10-1061,W08-1006,0,0.0949213,"Missing"
C10-1061,A97-1014,0,0.821568,"PCFG is tied to the annotation principles of popular treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated"
C10-1061,P87-1015,0,0.871053,"sing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) have been established as a candidate for modeling both discontinuous constituents and nonprojective dependency trees as they occur in treebanks (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). LCFRS extend CFG such that non-terminals can span tuples of possibly non537 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that d"
C10-1061,W11-3805,0,\N,Missing
C10-1061,N10-1035,0,\N,Missing
C10-1061,E95-1034,0,\N,Missing
C10-1061,E91-1005,0,\N,Missing
C10-1061,J03-4003,0,\N,Missing
C10-1061,W08-1007,0,\N,Missing
C10-1061,P03-1054,0,\N,Missing
C10-1061,W08-1004,0,\N,Missing
C10-1061,P00-1058,0,\N,Missing
C10-1061,P11-2037,0,\N,Missing
C10-1061,N10-1049,0,\N,Missing
C10-1061,H05-1066,0,\N,Missing
C10-1061,W07-2460,0,\N,Missing
C10-1061,W08-1005,0,\N,Missing
C10-1061,W11-2913,1,\N,Missing
C10-1061,D11-1036,0,\N,Missing
C10-1061,P01-1007,0,\N,Missing
C10-1061,P83-1021,0,\N,Missing
C10-1061,W10-1409,0,\N,Missing
C10-1061,W10-4407,0,\N,Missing
C10-1061,N06-1022,0,\N,Missing
C10-1061,W98-0105,0,\N,Missing
E09-1047,J94-4001,0,0.309468,"Missing"
E09-1047,P92-1003,0,0.519481,"Missing"
E09-1047,C02-1131,0,0.0357558,"Missing"
E09-1047,W08-1005,0,0.0522758,"). BitPar is an efficient implementation of an Earley style parser that uses bit vectors. However, BitPar cannot handle pre-bracketed input. For this reason, we used LoPar for the experiments where such input was required. LoPar, as it is used here, is a pure PCFG parser, which allows the input to be partially bracketed. We are aware that the results that can be obtained by pure PCFG parsers are not state of the art as reported in the shared task of the ACL 2008 Workshop on Parsing German (K¨ubler, 2008). While BitPar reaches an F-score of 69.76 (see next section), the best performing parser (Petrov and Klein, 2008) reaches an Fscore of 83.97 on T¨uBa-D/Z (but with a different split of training and test data). However, our experiments require certain features in the parsers, namely the capability to provide n-best analyses and to parse pre-bracketed input. To our knowledge, the parsers that took part in the shared task do not provide these features. Should they become available, the methods presented here could be applied to such parsers. We see no reason why our The Treebank The data source used for the experiments is the T¨ubingen Treebank of Written German (T¨uBaD/Z) (Telljohann et al., 2005). T¨uBa-D"
E09-1047,P05-1022,0,0.303414,"Missing"
E09-1047,C04-1024,0,0.14491,"Missing"
E09-1047,J05-1003,0,0.31486,"phenomena are particularly hard for pure PCFG parsing, due to the independence assumption inherent in the statistical models for PCFGs. Sentence (4) has the following Viterbi parse: methods should not be able to improve the results of these parsers further. Since we are interested in parsing coordinations, all experiments are conducted with gold POS tags, so as to abstract away from POS tagging errors. Although the treebank contains morphological information, this type of information is not used in the experiments presented here. The reranking experiments were conducted using the reranker by Collins and Koo (2005). This reranker uses a set of candidate parses for a sentence and reranks them based on a set of features that are extracted from the trees. The reranker uses a boosting method based on the approach by Freund et al. (1998). We used a similar feature set to the one Collins and Koo used; the following types of features were included: rules, bigrams, grandparent rules, grandparent bigrams, lexical bigrams, two-level rules, two-level bigrams, trigrams, head-modifiers, PPs, and distance for headmodifier relations, as well as all feature types involving rules extended by closed class lexicalization."
E09-1047,telljohann-etal-2004-tuba,1,0.905351,"Missing"
E09-1047,P07-1086,0,0.815425,"error analysis of his WSJ parsing results that coordination is one of the most frequent cases of incorrect parses, particularly if the conjuncts involved are complex. He manages to reduce errors for simple cases of NP coordination by introducing a special phrasal category of base NPs. In the experiments presented above, no explicit distinction is made between simple and complex cases of coordination, and no transformations are performed on the treebank annotations used for training. Our experiment 1, reranking 50-best parses, is similar to the approaches of Charniak and Johnson (2005) and of Hogan (2007). However, it differs from their experiments in two crucial ways: 1) Compared to Charniak and Johnson, who use 1.1 mio. features, our feature set is appr. five times larger (more than 5 mio. features), with the same threshold of at least five occurrences in the training set. 2) Both Hogan and Charniak and Johnson use special features for coordinate structures, such as a Boolean feature for marking parallelism (Charniak and Johnson) or for distinguishing between coordination of base NPs and coordination of complex conjuncts (Hogan), while our approach refrains from such special-purpose features"
E09-1047,J03-4003,0,\N,Missing
E09-1047,W08-1008,1,\N,Missing
J13-1006,E91-1005,0,0.786968,"Missing"
J13-1006,W98-0105,0,0.112274,"we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a nonterminal can span not only a single string but a tuple of strings of size k ≥ 1. k is thereby called its fan-out. We will notate LCFRS with the syntax of Simple Range Concatenation Grammars (SRCG) (Boullier 1998b), a formalism that is equivalent to LCFRS. A third formalism that is equivalent to LCFRS is Multiple Context-Free Grammar (MCFG) (Seki et al. 1991). Definition 1 (LCFRS) A Linear Context-Free Rewriting System (LCFRS) is a tuple N, T, V, P, S where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules (1) (1) (m) (m) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) · · · Am (X1 , . . . , Xdim("
J13-1006,2000.iwpt-1.8,0,0.110128,"= {0, . . . , n}. 2. We call a pair l, r ∈ Pos(w) × Pos(w) with l ≤ r a range in w. Its yield l, r(w) is the substring wl+1 . . . wr . 3. For two ranges ρ1 = l1 , r1 , ρ2 = l2 , r2 , if r1 = l2 , then the concatenation of ρ1 and ρ2 is ρ1 · ρ2 = l1 , r2 ; otherwise ρ1 · ρ2 is undefined. 4.  ∈ (Pos(w) × Pos(w))k is a k-dimensional range vector for w iff Aρ  = l1 , r1 , . . . , lk , rk  where li , ri  is a range in w for 1 ≤ i ≤ k. ρ We now define instantiations of rules with respect to a given input string. This definition follows the definition of clause instantiations from Boullier (2000). An instantiated rule is a rule in which variables are consistently replaced by ranges. Because we need this definition only for parsing our specific grammars, we restrict ourselves to ε-free rules containing only variables. Definition 7 (Rule instantiation) ) → Let G = (N, T, V, P, S) be an ε-free monotone LCFRS. For a given rule r = A(α A1 (x1 ) · · · Am (xm ) ∈ P (0 < m) that does not contain any terminals, 1. an instantiation with respect to a string w = t1 . . . tn consists of a function f : V → {i, j |1 ≤ i ≤ j ≤ |w|} such that for all x, y adjacent in one of the  , f (x) · f (y)"
J13-1006,W07-1506,0,0.490627,"Missing"
J13-1006,W08-1004,0,0.0245996,"ight be interesting as well, given that non-projectivity is the dependency-counterpart to discontinuity in constituency parsing. A meaningful comparison is difficult to do for the following reasons, however. Firstly, dependency parsing deals with relations between words, whereas in our case words are not considered in the parsing task. Our grammars take POS tags for a given and construct syntactic trees. Also, dependency conversion algorithms generally depend on the correct identification of linguistic head words (Lin 1995). We cannot rely on grammatical function labels, such as, for example, Boyd and Meurers (2008). Therefore we would have to use heuristics for the dependency conversion of the parser output. This would introduce additional noise. Secondly, the resources one obtains from our PLCFRS parser and from dependency parsers (the probabilistic LCFRS and the trained dependency parser) are quite different because the former contains non-lexicalized internal phrase structure identifying meaningful syntactic categories such as VP or NP while the latter is only concerned with relations between lexical items. A comparison would concentrate only on relations between lexical items and the rich phrase str"
J13-1006,P11-2037,0,0.077393,"Missing"
J13-1006,W10-1409,0,0.133075,"Missing"
J13-1006,J98-2004,0,0.103572,"y the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concernin"
J13-1006,N06-1022,0,0.0313949,"ures 26 and 27 show the average number of items produced by the parser and the parsing times for different sentence lengths. The results indicate that the estimates have the desired effect of preventing unnecessary items from being produced. This is reflected in a significantly lower parsing time. The different behavior of the LR and the LN estimate raises the question of the trade-off between maintaining optimality and obtaining a higher parsing speed. In 112 Kallmeyer and Maier PLCFRS Parsing other words, it raises the question of whether techniques such as pruning or coarseto-fine parsing (Charniak et al. 2006) would probably be superior to A∗ parsing. A first implementation of a coarse-to-fine approach has been presented by van Cranenburgh (2012). He generates a CFG from the treebank PLCFRS, based on the idea of Barth´elemy et al. (2001). This grammar, which can be seen as a coarser version of the actual PLCFRS, is then used for pruning of the search space. The problem that van Cranenburgh tackles is specific to PLCFRS: His PCFG stage generalizes over the distinction of labels by their fan-out. The merit of his work is an enormous increase in efficiency: Sentences with a length of up to 40 words ca"
J13-1006,W10-4407,0,0.0225309,"and yields output of competitive quality. There are three main directions for future work on this subject. r r On the symbolic side, LCFRS seems to offer more power than necessary. By removing symbolic expressivity, a lower parsing complexity can be achieved. One possibility is to disallow the use of so-called ill-nested LCFRS rules. These are rules where, roughly, the spans of two right-hand side non-terminals interleave in a cross-serial way. See the parsing ´ algorithm in Gomez-Rodr´ ıguez, Kuhlmann, and Satta (2010). Nevertheless, this seems to be too restrictive for linguistic modeling (Chen-Main and Joshi 2010; Maier and Lichte 2011). Our goal for future work is therefore to define reduced forms of ill-nested rules with which we get a lower parsing complexity. Another possibility is to reduce the fan-out of the extracted grammar. We have pursued the question whether the fan-out of the trees in the treebank can be reduced in a linguistically meaningful way in Maier, Kaeshammer, and Kallmeyer (2012). On the side of the probabilistic model, there are certain independence assumptions made in our model that are too strong. The main problem in respect is that, due to the definition of LCFRS, we have to d"
J13-1006,W11-2913,1,0.83313,"66.93 60.79 63.71 69.23 70.41 69.81 58.52 57.63 58.07 67.06 58.07 65.18 As for the work that aims to create crossing branches, Plaehn (2004) obtains 73.16 Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 83.97. The crucial difference between DPSG rules and LCFRS rules is that the former explicitly specify the material that can occur in gaps whereas LCFRS does not. Levy (2005), like us, proposes to use LCFRS but does not provide any evaluation results of his work. Very recently, Evang and Kallmeyer (2011) followed up on our work. They transform the Penn Treebank such that the trace nodes and co-indexations are converted into crossing branches and parse them with the parser presented in this article, obtaining promising results. Furthermore, van Cranenburgh, Scha, and Sangati (2011) and van Cranenburgh (2012) have also followed up on our work, introducing an integration of our approach with Data-Oriented Parsing (DOP). The former article introduces an LCFRS adaption of Goodman’s PCFG-DOP (Goodman 2003). For their evaluation, the authors use the same data as we do in Maier (2010), and obtain an"
J13-1006,N09-1061,0,0.435818,"re 6. The analysis (a) displaying nested dependencies has probability 0.16 and (b) (right-linear dependencies) has probability 0.042. 3. Parsing PLCFRS 3.1 Binarization Similarly to the transformation of a CFG into Chomsky normal form, an LCFRS can be binarized, resulting in an LCFRS of rank 2. As in the CFG case, in the transformation, we introduce a non-terminal for each right-hand side longer than 2 and split the rule into two rules, using this new intermediate non-terminal. This is repeated until all ´ right-hand sides are of length 2. The transformation algorithm is inspired by GomezRodr´ıguez et al. (2009) and it is also specified in Kallmeyer (2010). 3.1.1 General Binarization. In order to give the algorithm for this transformation, we  ∈ [(T ∪ V)∗ ]i by a vector x ∈ V j where all need the notion of a reduction of a vector α  . A reduction is, roughly, obtained by keeping all variables in α  variables in x occur in α that are not in x. This is defined as follows: Definition 5 (Reduction)  ∈ [(T ∪ V)∗ ]i and x ∈ V j for some i, j ∈ IN. Let N, T, V, P, S be an LCFRS, α  1 $ . . . $α  i be the string obtained from concatenating the components of α , Let w = α / (V ∪ T). separated by"
J13-1006,W08-1007,0,0.493997,"y mentioned, note that the full SX estimate and the SX estimate with span and sentence length are monotonic and allow for A∗ parsing. The other two estimates, which are both not monotonic, act as FOMs in a best-first parsing context. Consequently, they contribute to speeding up parsing but they decrease the quality of the parsing output. For further evaluation details see Section 6. 5. Grammars for Discontinuous Constituents 5.1 Grammar Extraction The algorithm we use for extracting an LCFRS from a constituency treebank with crossing branches has originally been presented in Maier and Søgaard (2008). It interprets the treebank trees as LCFRS derivation trees. Consider for instance the tree in Figure 22. The S node has two daughters, a VMFIN node and a VP node. This yields a rule S → VP VMFIN. The VP is discontinuous with two components that wrap around the yield of the VMFIN. Consequently, the LCFRS rule is S(XYZ) → VP(X, Z) VMFIN(Y). The extraction of an LCFRS from treebanks with crossing branches is almost immediate, except for the fan-out of the non-terminal categories: In the treebank, we can have the same non-terminal with different fan-outs, for instance a VP without a gap (fan-out"
J13-1006,E95-1034,0,0.205922,"Candito and Seddah 2010). A rather indirect effect is that morphological richness often relaxes word order constraints. The principal intuition is that a rich morphology encodes information that otherwise has to be conveyed by a particular word order. If, for instance, the case of a nominal complement is not provided by morphology, it has to be provided by the position of the complement relative to other complements in the sentence. Example (1) provides an example of case marking and free word order in German. In turn, in free word order languages, word order can encode information structure (Hoffman 1995). (1) a. der kleine Jungenom schickt seiner Schwesterdat den Briefacc sends his sister the letter the little boy b. Other possible word orders: (i) der kleine Jungenom schickt den Briefacc seiner Schwesterdat (ii) seiner Schwesterdat schickt der kleine Jungenom den Briefacc (iii) den Briefacc schickt der kleine Jungenom seiner Schwesterdat ¨ Sprache und Information, Universit¨atsstr. 1, D-40225 Dusseldorf, ¨ ∗ Institut fur Germany. E-mail: kallmeyer@phil.uni-duesseldorf.de. ¨ Sprache und Information, Universit¨atsstr. 1, D-40225 Dusseldorf, ¨ ∗∗ Institut fur Germany. E-mail: maierw@hhu.de. Sub"
J13-1006,C10-1061,1,0.247685,"that has successfully been used for data-driven parsing.1 The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sections 3 and 4 present the binarization algorithm, the parser, and the outside estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches. 1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG"
J13-1006,W06-1508,0,0.387499,"Missing"
J13-1006,N03-1016,0,0.245933,"-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We have implemented a CYK parser and we prese"
J13-1006,P03-1054,0,0.440556,"-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We have implemented a CYK parser and we prese"
J13-1006,E09-1055,0,0.0646792,"ation in a post- or preprocessing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004; Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter approach. Our work is motivated by the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfat"
J13-1006,P04-1042,0,0.166162,"which produce trees without crossing branches but provide a larger domain of locality than CFG— for instance, through complex labels (Hockenmaier 2003) or through the derivation 90 Kallmeyer and Maier CFG: PLCFRS Parsing • LCFRS: A A γ1 • γ2 • γ3 γ Figure 3 Different domains of locality. mechanism (Chiang 2003). The second class, to which we contribute in this paper, consists of approaches that aim at producing trees which contain non-local information. Some methods realize the reconstruction of non-local information in a post- or preprocessing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004; Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter approach. Our work is motivated by the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-te"
J13-1006,W10-1407,1,0.918098,"ssfully been used for data-driven parsing.1 The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sections 3 and 4 present the binarization algorithm, the parser, and the outside estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches. 1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG"
J13-1006,W12-4615,1,0.853416,"Missing"
J13-1006,W10-4415,1,0.866819,"side estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches. 1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a nonterminal can span not only a single string but a tuple of strings of size k ≥ 1. k is thereby called its fan-out. We will notate LCFRS with the syntax of Simple Range Concatenation Grammars (SRCG)"
J13-1006,H94-1020,0,0.279531,"pokriva. Of house-DET he repaired roof. “It is the roof of the house he repairs.” b. Gwon.han-ul ˘ nu.ga ka.ji.go iss.ji? Authority-OBJ who has not? “Who has no authority?” Discontinuous constituents are by no means limited to languages with freedom in word order. They also occur in languages with a rather fixed word order such as English, resulting from, for instance, long-distance movements. Examples (4a) and (4b) are examples from the Penn Treebank for long extractions resulting in discontinuous S categories and for discontinuous NPs arising from extraposed relative clauses, respectively (Marcus et al. 1994). (4) a. b. Long Extraction in English: (i) Those chains include Bloomingdale’s, which Campeau recently said it will sell. (ii) What should I do. Extraposed nominal modifiers (relative clauses and PPs) in English: (i) They sow a row of male-fertile plants nearby, which then pollinate the malesterile plants. (ii) Prices fell marginally for fuel and electricity. 1.2 Treebank Annotation and Data-Driven Parsing Most constituency treebanks rely on an annotation backbone based on Context-Free Grammar (CFG). Discontinuities cannot be modeled with CFG, because they require a larger domain of locality"
J13-1006,H05-1066,0,0.164117,"Missing"
J13-1006,J03-1006,0,0.613405,"ewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We ha"
J13-1006,P83-1021,0,0.722363,"ot contain any terminals, 1. an instantiation with respect to a string w = t1 . . . tn consists of a function f : V → {i, j |1 ≤ i ≤ j ≤ |w|} such that for all x, y adjacent in one of the  , f (x) · f (y) must be defined; we then define f (xy) = f (x) · f (y), elements of α 2.  )) → A1 ( f (x1 )) · · · Am ( f (xm )) is an if f is an instantiation of r, then A( f (α instantiated rule where f (x1 , . . . , xk  ) =  f (x1 ), . . . , f (xk ). We use a probabilistic version of the CYK parser from Seki et al. (1991). The algorithm is formulated using the framework of parsing as deduction (Pereira and Warren 1983; Shieber, Schabes, and Pereira 1995; Sikkel 1997), extended with weights (Nederhof 2003). In this framework, a set of weighted items representing partial parsing results is characterized via a set of deduction rules, and certain items (the goal items) represent successful parses. During parsing, we have to match components in the rules we use with portions of ] where A ∈ N and ρ  the input string. For a given input w, our items have the form [A, ρ is a range vector that characterizes the span of A. Each item has a weight in that encodes the Viterbi inside score of its best parse tree. More"
J13-1006,W08-1005,0,0.0155553,"lthough the results for LoPar are no surprise, given the similarity of the models implemented by our parser and the Stanford parser, it remains to be investigated why the lexicalization component of the Stanford parser does not lead to better results. In any case the comparison shows that on a data set without crossing branches, our parser obtains the results one would expect. A further data set to which we can provide a ¨ comparison is the PaGe workshop experimental data (Kubler and Penn 2008).7 Table 4 ¨ lists the results of some of the papers in Kubler and Penn (2008) on TIGER, namely, for Petrov and Klein (2008) (P&K), who use the Berkeley Parser (Petrov and Klein 2007); Rafferty and Manning (2008) (R&M), who use the Stanford parser (see above); and Hall and Nivre (2008) (H&N), who use a dependency-based approach (see next paragraph). The comparison again shows that our system produces good results. Again the performance gap between the Stanford parser and our parser warrants further investigation. 6 We have obtained the former parser from http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/ LoPar.html and the latter (Version 2.0.1) from http://nlp.stanford.edu/software/lex-parser.shtml. ¨ 7 Thanks to Sandr"
J13-1006,W08-1006,0,0.181191,"est data from which all category splits have been removed. This metric is equivalent to the corresponding PCFG metric for dim(A) = 1. Despite the shortcomings of such a measure (Rehbein and van Genabith 2007), it still allows to some extent a comparison to previous work in PCFG parsing (see also Section 7). Note that we provide the parser with gold POS tags in all experiments. 6.4 Markovization and Binarization We use the markovization settings v = 1 and h = 2 for all further experiments. The setting which has been reported to yield the best results for PCFG parsing of NeGra, v = 2 and h = 1 (Rafferty and Manning 2008), required a parsing time which was too high.4 Table 2 contains the parsing results for NeGraLCFRS using five different binarizations: Head-driven and KM are the two head-outward binarizations that use a head chosen on linguistic grounds (described in Section 5.2); L-to-R is another variant in which we always choose the rightmost daughter of a node as its head.5 Optimal reorders the left-hand side such that the fan-out of the binarized rules is optimized (described in Section 3.1.2). Finally, we also try a deterministic binarization (Deterministic) in which we binarize strictly from left to ri"
J13-1006,N10-1049,0,0.0549873,"Missing"
J13-1006,W07-2460,0,0.229117,"Missing"
J13-1006,A97-1014,0,0.936409,"iterature on this discussion. With a rather free word order, constituents and single parts of them can be displaced freely within the sentence. German, for instance, has a rich inflectional system and allows for a free word order, as we have already seen in Example (1): Arguments can be scrambled, and topicalizations and extrapositions underlie few restrictions. Consequently, discontinuous constituents occur frequently. This is challenging for syntactic description in general (Uszkoreit 1986; Becker, Joshi, and Rambow 1991; Bunt 1996; ¨ Muller 2004), and for treebank annotation in particular (Skut et al. 1997). In this paper, we address the problem of data-driven parsing of discontinuous constituents on the basis of German. In this section, we inspect the type of data we have to deal with, and we describe the way such data are annotated in treebanks. We briefly discuss different parsing strategies for the data in question and motivate our own approach. 1.1 Discontinuous Constituents Consider the sentences in Example (2) as examples for discontinuous constituents (taken from the German NeGra [Skut et al. 1997] and TIGER [Brants et al. 2002] treebanks). Example (2a) shows several instances of discont"
J13-1006,D11-1036,0,0.0632198,"Missing"
J13-1006,E12-1047,0,0.479869,"Missing"
J13-1006,W11-3805,0,0.193015,"Missing"
J13-1006,P87-1015,0,0.917852,"Missing"
kallmeyer-etal-2008-developing,schulte-im-walde-2002-subcategorisation,0,\N,Missing
kallmeyer-etal-2008-developing,C96-2120,0,\N,Missing
kallmeyer-etal-2008-developing,P89-1018,0,\N,Missing
kallmeyer-etal-2008-developing,P92-1010,0,\N,Missing
kallmeyer-etal-2008-developing,W08-2316,1,\N,Missing
kubler-etal-2008-compare,daum-etal-2004-automatic,0,\N,Missing
kubler-etal-2008-compare,brants-hansen-2002-developments,0,\N,Missing
kubler-etal-2008-compare,A97-1014,0,\N,Missing
kubler-etal-2008-compare,C04-1024,0,\N,Missing
kubler-etal-2008-compare,W06-1614,1,\N,Missing
kubler-etal-2008-compare,W07-1506,0,\N,Missing
kubler-etal-2008-compare,P03-1054,0,\N,Missing
kubler-etal-2008-compare,D07-1066,1,\N,Missing
kubler-etal-2008-compare,P06-3004,1,\N,Missing
kubler-etal-2008-compare,C04-1056,0,\N,Missing
kubler-etal-2008-compare,emms-2008-tree,0,\N,Missing
L16-1658,W15-3206,0,0.0586422,"Missing"
L16-1658,W14-3917,0,0.0198808,"and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work concentrates on Levantine and Egyptian Arabic (see, e.g., Elfardy and Diab (2012) and Elfardy et al. (2014)). An exception is Cotter"
L16-1658,dey-fung-2014-hindi,0,0.0622104,"the article. 2. 2.1. Code-Switching Linguistic Analysis of Code-Switching Code-switching1 is common phenomenon in multilingual communities wherein speakers switch from one language or dialect to another within the same context (Bullock and Toribio, 2009). Communities where commonly, more than one language, resp. dialect is spoken can be found around the world. Examples include India, where speakers switch between English and Hindi (among other local languages) 1 Note that for the purpose of this paper, we do not distinguish between code-switching and similar concepts such as codemixing. 4170 (Dey and Fung, 2014); the United States, where migrants from Spanish-speaking countries continue to use their native language alongside English (Poplack, 1980); Spain, where people switch between regional languages such as Basque and Spanish (Mu˜noa Barredo, 2003); Paraguay, where Spanish co-exists with Guarani (Estigarribia, 2015); and finally the Arab world, where speakers alternate between MSA and Dialectal Arabic. In the literature, three types of codes-switching are distinguished. In inter-sentential code-switching languages are switched between sentences. An instance of this type of switching is (1) (from M"
L16-1658,C12-2029,0,0.0448042,"Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work concentrates on Levantine and Egyptian Arabic (see, e.g., Elfardy and Diab (2012) and Elfardy et al. (2014)). An exception is Cotterell et al. (2014), who works on Algerian Arabic. In linguistics, Moroccan Arabic has found attention very early (Harrell, 1962; Harrell and Sobelman, 1966). Work on the computational processing of Darija, however, remains very scarce. To our knowledge, there is only the work of Tratz et al. (2013), who present a data collection and annotation environment for romanized Darija, and the work of Voss et al. (2014) who present an approach for finding romanized Darija in code-mixed tweets. 6. We have presented a corpus of Moroccan Arabic Darija, obt"
L16-1658,W14-3911,0,0.0382725,"ratz et al. (2013), Maharjan et al. (2015), Dey and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work concentrates on Levantine and Egyptian Arabic (see, e.g., Elfardy and Diab (2012"
L16-1658,N13-1131,0,0.0272079,"he ones described by Tratz et al. (2013), Maharjan et al. (2015), Dey and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work concentrates on Levantine and Egyptian Arabic (see, e.g.,"
L16-1658,W14-3912,1,0.889147,"Missing"
L16-1658,W15-1608,0,0.0945793,"y government since the independence which has worked hard to develop the country; and we will see that the thieves coming back (Independence Party, Constitutional Party, Liberal Party, and murderers). Figure 3: Sample forum post with annotated version and free translation  (meaning è @P (demonstrative pronoun), ¬Q»  worse). Words with mixed morphology are, e.g., úæÖß A « and úk XA«, which both exhibit the Darija future prefix A« on . et al., 2014; Nakov et al., 2014; Zampieri et al., 2014). A number of language resources has been created, such as the ones described by Tratz et al. (2013), Maharjan et al. (2015), Dey and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Mach"
L16-1658,W14-3916,0,0.0121732,"nal Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work concentrates on Levantine and Egyptian Arabic (see, e.g., Elfardy and Diab (2012) and Elfardy et al. (2014)). An exception is Cotterell et al. (2014), who works on Algerian Arabic. In linguistics, Moroccan Arabi"
L16-1658,D08-1102,0,0.154881,"umber of language resources has been created, such as the ones described by Tratz et al. (2013), Maharjan et al. (2015), Dey and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work conc"
L16-1658,D08-1110,0,0.255748,"umber of language resources has been created, such as the ones described by Tratz et al. (2013), Maharjan et al. (2015), Dey and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 2014), Support Vector Machines (Bar and Dershowitz, 2014), Markov Models (King et al., 2014) and n-gram based approaches (Shrestha, 2014; Bacatan et al., 2014) to Recurrent Neural Networks (Chang and Lin, 2014). POS tagging of code-switched text has also been investigated (Solorio and Liu, 2008b; Rodrigues and K¨ubler, 2013). Concerning the processing of Arabic in general, there is an ample body of research. For an overview, see Habash (2010). With regard to the processing of Dialectal Arabic, most of the existing work conc"
L16-1658,W14-3907,0,0.493506,"task in Arabic and no such tool is available for Darija, we leave the downloaded text units (”posts”) intact. Then we tokenize the text with a simple heuristic, delete all diacritic marks as usual in Arabic NLP (Habash, 2010), 4171 and Buckwalter transliterate the text. Finally, we store the data token-wise as pairs (original and transliteration) in a MySQL database. The size of the resulting corpus is 15 million tokens in total. It comprises a wide range of topics including politics, religion, sport and economics. Existing code-switched data sets are often highly skewed towards one language (Solorio et al., 2014), with a high percentage of the sentences not exhibiting code-switching at all. Also in our corpus, MSA is more prevalent than Darija. In order to obtain a resource that concentrates on codeswitching, we aim at minimizing the skewing by extracting only a subset of the data set that contains more instances of code-switching. The subset is extracted with the following iterative process. We first compile an initial seed list of 439 commonly used Darija words and phrases collected from the internet.2 Then we repeat the following steps. Each word and phrase in the seed list is formulated in a MySQL"
L16-1658,W13-2317,0,0.172144,"government is the only government since the independence which has worked hard to develop the country; and we will see that the thieves coming back (Independence Party, Constitutional Party, Liberal Party, and murderers). Figure 3: Sample forum post with annotated version and free translation  (meaning è @P (demonstrative pronoun), ¬Q»  worse). Words with mixed morphology are, e.g., úæÖß A « and úk XA«, which both exhibit the Darija future prefix A« on . et al., 2014; Nakov et al., 2014; Zampieri et al., 2014). A number of language resources has been created, such as the ones described by Tratz et al. (2013), Maharjan et al. (2015), Dey and Fung (2014), and the data set from the Shared Task at the First Workshop on Computational Approaches to Code-Switching at EMNLP 2014 (Solorio et al., 2014), with a particular focus on inter-operable annotation guidelines. A popular use for those resources can be found in approaches to automatic detection of codeswitching points in text. This task has mostly been treated as a sequence labeling problem. Different techniques have been applied, ranging from Naive Bayes (Solorio and Liu, 2008) over Conditional Random Fields (King and Abney, 2013; Elfardy et al., 20"
L16-1658,voss-etal-2014-finding,0,0.0585972,"regard to the processing of Dialectal Arabic, most of the existing work concentrates on Levantine and Egyptian Arabic (see, e.g., Elfardy and Diab (2012) and Elfardy et al. (2014)). An exception is Cotterell et al. (2014), who works on Algerian Arabic. In linguistics, Moroccan Arabic has found attention very early (Harrell, 1962; Harrell and Sobelman, 1966). Work on the computational processing of Darija, however, remains very scarce. To our knowledge, there is only the work of Tratz et al. (2013), who present a data collection and annotation environment for romanized Darija, and the work of Voss et al. (2014) who present an approach for finding romanized Darija in code-mixed tweets. 6. We have presented a corpus of Moroccan Arabic Darija, obtained from internet discussion forums and blogs, manually annotated for code-switching on token level without crowd-sourcing. With its 223k tokens, to our knowledge, it is currently the largest resource of its kind. In future work, we will additionally annotate the data with part-of-speech information. words are, e.g., words which otherwise are MSA. Last, note that the punctuation in the text unit is labeled as other. For the purpose of presentation, i.e., in"
L18-1315,W17-5522,0,0.0552152,"Missing"
L18-1315,L16-1119,1,0.92952,"Section 4 presents and discusses the experiments and results. Lastly, section 5 serves as the coda of the article. 2. Related Work In the past years, the speech processing communities have recognized that crowdsourcing is a promising solution to their strong need for data (Eskenazi et al., 2013). In the field of spoken dialog systems, crowdsourcing has been used for evaluation (Yang et al., 2010; Komarov et al., 2013) and acquiring training data for system components such as automatic speech recognition (ASR) (Rothwell et al., 2015; McGraw et al., 2010), natural language understanding (NLU) (Braunger et al., 2016; Wang et al., 2012; Misu, 2014), dialog management (Manuvinakurike et al., 2015; McGraw et al., 2010) and natural language generation (NLG) (Novikova et al., 2016; Mitchell et al., 2014). The evaluation of the crowdsourced data sets differs depending on research questions and application purposes. The literature that evaluates crowdsourcing approaches for acquiring training and test data can described as follows. Most of the research works compare different meaning representation modalities with which to elicit data for given meanings or tasks. Depending on the application of the acquired dat"
L18-1315,W17-5517,1,0.761107,"with task 2 it performs best. 4.2. Linguistic Evaluation In order to evaluate the usefulness of crowdsourcing we check the data set acquired by a previously evaluated crowdsourcing method against data acquired by an experimental study. Since our system usage data serves as a baseline for naturalness in user input, both data sets are examined and compared in terms of syntactic, lexical, and pragmatic criteria commonly used in literature. The criteria we examine also include those mentioned by literature for natural queries: politeness, full sentences, filler words and a higher number of words (Braunger et al., 2017). Table 1 presents the lexical properties of both, crowdsourced queries and real system usage queries (referred to as ”natural data”). A common measure of lexical diversity is the type-token ratio which is calculated by dividing the number of individual word types (lemmas) by the number of occurring word tokens. The standardized typetoken ratio (STTR) (Johnson, 1944) is commonly used to normalize the impact of the size of different data sets. Table 1 shows that there are no significant differences between 2005 crowdsourced queries and real system usage queries. The content-function word ratio"
L18-1315,W10-0701,0,0.0431425,"ch a communication style presupposes an understanding of all utterances which are associated with a specific semantic meaning. Thanks to the advent of statistical data-driven approaches, recent systems have been able to interpret freely spoken user input. However, obtaining suitable data for their training and evaluation is a significant challenge. In order to increase the amount of data for training and evaluation of natural language understanding (NLU), researchers are turning to crowdsourcing instead of collecting data from Wizard of Oz experiments or using handcrafted grammars (see, e.g., Callison-Burch and Dredze (2010)). While the benefits of crowdsourcing can include expedition, individuality of people, and low costs (see, e.g., Eskenazi et al. (2013)), it is often criticized for poor standards as it is difficult to control the quality of work when requesting complex tasks (Eskenazi et al., 2013). Since the community of crowd workers is acknowledged for being Internet savvy young adults, chiefly between ages 18 and 35, the findings are not representative for any target group. In addition, crowdsourced data intended to be used for improving SDS do not necessarily reflect real system usage. Therefore we inve"
L18-1315,P13-1025,0,0.0441636,"Missing"
L18-1315,mcgraw-etal-2010-collecting,0,0.035026,"wdsourced and system usage data and the design of our study. Section 4 presents and discusses the experiments and results. Lastly, section 5 serves as the coda of the article. 2. Related Work In the past years, the speech processing communities have recognized that crowdsourcing is a promising solution to their strong need for data (Eskenazi et al., 2013). In the field of spoken dialog systems, crowdsourcing has been used for evaluation (Yang et al., 2010; Komarov et al., 2013) and acquiring training data for system components such as automatic speech recognition (ASR) (Rothwell et al., 2015; McGraw et al., 2010), natural language understanding (NLU) (Braunger et al., 2016; Wang et al., 2012; Misu, 2014), dialog management (Manuvinakurike et al., 2015; McGraw et al., 2010) and natural language generation (NLG) (Novikova et al., 2016; Mitchell et al., 2014). The evaluation of the crowdsourced data sets differs depending on research questions and application purposes. The literature that evaluates crowdsourcing approaches for acquiring training and test data can described as follows. Most of the research works compare different meaning representation modalities with which to elicit data for given meanin"
L18-1315,W14-5003,0,0.367221,"ave recognized that crowdsourcing is a promising solution to their strong need for data (Eskenazi et al., 2013). In the field of spoken dialog systems, crowdsourcing has been used for evaluation (Yang et al., 2010; Komarov et al., 2013) and acquiring training data for system components such as automatic speech recognition (ASR) (Rothwell et al., 2015; McGraw et al., 2010), natural language understanding (NLU) (Braunger et al., 2016; Wang et al., 2012; Misu, 2014), dialog management (Manuvinakurike et al., 2015; McGraw et al., 2010) and natural language generation (NLG) (Novikova et al., 2016; Mitchell et al., 2014). The evaluation of the crowdsourced data sets differs depending on research questions and application purposes. The literature that evaluates crowdsourcing approaches for acquiring training and test data can described as follows. Most of the research works compare different meaning representation modalities with which to elicit data for given meanings or tasks. Depending on the application of the acquired data, e.g., NLG or NLU, different metrics are used in order to find the best elicitation method among the proposed. Novikova et al. (2016), e.g., compare two meaning 2002 representation moda"
L18-1315,moller-etal-2008-corpus,0,0.0608489,"Missing"
L18-1315,W16-6644,0,0.0235871,"rocessing communities have recognized that crowdsourcing is a promising solution to their strong need for data (Eskenazi et al., 2013). In the field of spoken dialog systems, crowdsourcing has been used for evaluation (Yang et al., 2010; Komarov et al., 2013) and acquiring training data for system components such as automatic speech recognition (ASR) (Rothwell et al., 2015; McGraw et al., 2010), natural language understanding (NLU) (Braunger et al., 2016; Wang et al., 2012; Misu, 2014), dialog management (Manuvinakurike et al., 2015; McGraw et al., 2010) and natural language generation (NLG) (Novikova et al., 2016; Mitchell et al., 2014). The evaluation of the crowdsourced data sets differs depending on research questions and application purposes. The literature that evaluates crowdsourcing approaches for acquiring training and test data can described as follows. Most of the research works compare different meaning representation modalities with which to elicit data for given meanings or tasks. Depending on the application of the acquired data, e.g., NLG or NLU, different metrics are used in order to find the best elicitation method among the proposed. Novikova et al. (2016), e.g., compare two meaning"
L18-1315,petrov-etal-2012-universal,0,0.101086,"Missing"
L18-1315,N16-1081,0,0.0345619,"Missing"
L18-1315,W15-4657,1,0.898876,"Missing"
maier-etal-2014-discosuite,D12-1133,0,\N,Missing
maier-etal-2014-discosuite,E12-1006,0,\N,Missing
maier-etal-2014-discosuite,J93-2004,0,\N,Missing
maier-etal-2014-discosuite,E06-1011,0,\N,Missing
maier-etal-2014-discosuite,W03-2404,0,\N,Missing
maier-etal-2014-discosuite,C10-1061,1,\N,Missing
maier-etal-2014-discosuite,C96-2120,0,\N,Missing
maier-etal-2014-discosuite,P11-2037,0,\N,Missing
maier-etal-2014-discosuite,H05-1066,0,\N,Missing
maier-etal-2014-discosuite,E14-1039,0,\N,Missing
maier-etal-2014-discosuite,seeker-kuhn-2012-making,0,\N,Missing
maier-etal-2014-discosuite,E12-1047,0,\N,Missing
maier-etal-2014-discosuite,emms-2008-tree,0,\N,Missing
maier-etal-2014-discosuite,Y95-1009,0,\N,Missing
N15-1134,E14-1039,0,0.0288793,"Missing"
N15-1134,D13-1034,0,0.0258249,"2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algo"
N15-1134,W05-1502,0,0.019724,"Missing"
N15-1134,W13-0808,0,0.0174264,"terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algorithms for automaton and parse table constructions, and section 3.3 presents the parsing algorithm. Section 4 concludes the a"
N15-1134,W09-3808,1,0.801885,"an instantiated clause, the instantiated LHS non-terminal may be replaced with the sequence of instantiated RHS terminals. The language of the grammar is the set of strings which can be reduced to the empty word, starting with S instantiated to the input string. See figure 1 for a sample LCFRS. 2.2 Thread Automata Thread automata (TA) (Villemonte de la Clergerie, 2002) are a generic automaton model which can be parametrized to recognize different mildly contextsensitive languages. The TA for LCFRS (LCFRSTA) implements a prefix-valid top-down incremental parsing strategy similar to the ones of Kallmeyer and Maier (2009) and Burden and Ljungl¨of (2005). An LCFRS-TA for some LCFRS G = (N, T, V, P, S) works as follows. The processing of a single rule is handled by a single thread which will traverse the LHS arguments of the rule. A thread is given by a pair p : X, where p ∈ {1, . . . , m}∗ with m the rank of G is the address, and X ∈ N ∪ {ret} ∪ C where ret ∈ / N is the content of the thread. An automaton state is given by a tuple hi, p, T i where T is a set of threads, the thread store, p is the address of the active thread, and i ≥ 0 indicates that i tokens have been recognized. We introduce a new start symbo"
N15-1134,J13-1006,1,0.655071,"xt-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the foll"
N15-1134,J13-2004,0,0.0240383,"by an automaton which is compiled offline. LR parsers were first introduced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Au"
N15-1134,W10-4415,1,0.778997,"ced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structu"
N15-1134,P98-2156,0,0.117529,"as Tree-Adjoining Grammar. In this paper, we present the first LRstyle parsing algorithm for Linear ContextFree Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG which has received considerable attention in the last years. 1 Introduction LR parsing is an incremental shift-reduce parsing strategy in which the transitions between parser states are guided by an automaton which is compiled offline. LR parsers were first introduced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative mor"
N15-1134,C08-2026,0,0.0235806,"which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algorithms for automaton and parse table constructions, and section 3.3 presents the parsing algorithm. Sectio"
N15-1134,P84-1073,0,0.75177,"r mildly context-sensitive formalisms, such as Tree-Adjoining Grammar. In this paper, we present the first LRstyle parsing algorithm for Linear ContextFree Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG which has received considerable attention in the last years. 1 Introduction LR parsing is an incremental shift-reduce parsing strategy in which the transitions between parser states are guided by an automaton which is compiled offline. LR parsers were first introduced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also"
N15-1134,E12-1047,0,0.23332,"Missing"
N15-1134,C02-1028,0,0.757526,"ven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algorithms for automaton and parse table constructions, and section 3.3 presents the parsing algorithm. Section 4 concludes the article. 2 Preliminaries 2.1 LCFRS In this paper, we restrict ourselves to string rewriting LCFRS and omit the more general definition (Weir, 1988). In LCFRS, a single non-terminal can span k ≥ 1 continuous blocks of a string. A CFG is simply a special case of"
P06-3004,A00-2018,0,0.0749695,"r on all treebank versions. Our results give an indication of which structures are favorable for parsing and which ones are not. 1 Introduction The Wall-Street-Journal part (WSJ) of the Penn Treebank (Marcus et al., 1994) plays a central role in research on statistical treebank-based parsing. It has not only become a standard for parser evaluation, but also the foundation for the development of new parsing models. For the English WSJ, high accuracy parsing models have been created, some of them using extensions to classical PCFG parsing such as lexicalization and markovization (Collins, 1999; Charniak, 2000; Klein and Manning, 2003). However, since most research has been limited to a single language (English) and to a single treebank (WSJ), the question of how portable the parsers and their extensions are across languages and across treebanks often remained open. In the present paper, our goal is to determine the effects of different annotation decisions on the results of plain PCFG parsing without extensions. Our motivation is two-fold: first, we want to present research on a language different from English, second, we want to investigate the influences of annotation schemes via a realistic com"
P06-3004,P03-1013,0,0.279631,"any wmaier@sfs.uni-tuebingen.de Abstract Only recently, there have been attempts to evaluate parsing results with respect to the properties and the language of the treebank that is used. Gildea (2001) investigates the effects that certain treebank characteristics have on parsing results, such as the distribution of verb subcategorization frames. He conducts experiments on the WSJ and the Brown Corpus, parsing one of the treebanks while having trained on the other one. He draws the conclusion that a small amount of matched training data is better than a large amount of unmatched training data. Dubey and Keller (2003) analyze the difficulties that German imposes on parsing. They use the NeGra treebank for their experiments and show that lexicalization, while highly effective for English, has no benefit for German. This result motivates them to create a parsing model for German based on sisterhead-dependencies. Corazza et al. (2004) conduct experiments with model 2 of Collins’ parser (Collins, 1999) and the Stanford parser (Klein and Manning, 2003) on two Italian treebanks. They report disappointing results which they trace back to the different difficulties of different parsing tasks in Italian and English"
P06-3004,P03-1054,0,0.109701,"le having trained on the other one. He draws the conclusion that a small amount of matched training data is better than a large amount of unmatched training data. Dubey and Keller (2003) analyze the difficulties that German imposes on parsing. They use the NeGra treebank for their experiments and show that lexicalization, while highly effective for English, has no benefit for German. This result motivates them to create a parsing model for German based on sisterhead-dependencies. Corazza et al. (2004) conduct experiments with model 2 of Collins’ parser (Collins, 1999) and the Stanford parser (Klein and Manning, 2003) on two Italian treebanks. They report disappointing results which they trace back to the different difficulties of different parsing tasks in Italian and English and to differences in annotation styles across treebanks. Most of the work on treebank-based statistical parsing exclusively uses the WallStreet-Journal part of the Penn treebank for evaluation purposes. Due to the presence of this quasi-standard, the question of to which degree parsing results depend on the properties of treebanks was often ignored. In this paper, we use two similar German treebanks, T¨uBa-D/Z and NeGra, and investi"
P06-3004,H94-1020,0,0.0194064,"ng results depend on the properties of treebanks was often ignored. In this paper, we use two similar German treebanks, T¨uBa-D/Z and NeGra, and investigate the role that different annotation decisions play for parsing. For these purposes, we approximate the two treebanks by gradually taking out or inserting the corresponding annotation components and test the performance of a standard PCFG parser on all treebank versions. Our results give an indication of which structures are favorable for parsing and which ones are not. 1 Introduction The Wall-Street-Journal part (WSJ) of the Penn Treebank (Marcus et al., 1994) plays a central role in research on statistical treebank-based parsing. It has not only become a standard for parser evaluation, but also the foundation for the development of new parsing models. For the English WSJ, high accuracy parsing models have been created, some of them using extensions to classical PCFG parsing such as lexicalization and markovization (Collins, 1999; Charniak, 2000; Klein and Manning, 2003). However, since most research has been limited to a single language (English) and to a single treebank (WSJ), the question of how portable the parsers and their extensions are acro"
P06-3004,A97-1014,0,0.8562,"eir extensions are across languages and across treebanks often remained open. In the present paper, our goal is to determine the effects of different annotation decisions on the results of plain PCFG parsing without extensions. Our motivation is two-fold: first, we want to present research on a language different from English, second, we want to investigate the influences of annotation schemes via a realistic comparison, i.e. use two different annotation schemes. Therefore, we take advantage of the availability of two similar treebanks of German, T¨uBa-D/Z (Telljohann et al., 2003) and NeGra (Skut et al., 1997). The strategy we adopt extends K¨ubler 19 Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 19–24, c Sydney, July 2006. 2006 Association for Computational Linguistics (2005). Treebanks and their annotation schemes respectively are compared using a stepwise approximation. Annotation components corresponding to certain annotation decisions are taken out or inserted, submitting each time the resulting modified treebank to the parser. This method allows us to investigate the role of single annotation decisions in two different environments. In section 2, we describe the annotati"
P06-3004,W01-0521,0,\N,Missing
P06-3004,J03-4003,0,\N,Missing
P09-2003,W05-1502,0,0.0317567,"Missing"
P09-2003,kallmeyer-etal-2008-developing,1,0.891948,"ible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radz"
P09-2003,J91-3002,0,0.0809759,"2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number names. Parsing algorithms for RCG have been introduced by Boullier (2000), who presents a directional top-down parsing algorithm using pseudocode, and Barth´elemy et al. (2001), who add an oracle to Boullier’s algorithm. The more restricted Definition 1. A RCG G = hN, T, V, P, Si consists of a) a finite set of predicates N with an arity function dim: N → N  {0} where S ∈ N is the start predicate with dim(S) = 1, b) disjoint finite sets of terminals T and variables V"
P09-2003,E91-1005,0,0.091443,"or the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number names. Parsing algorithms for RCG have been introduced by Boullier (2000), who presents a directional top-down parsing algorithm using pseudocode, and Barth´elemy et al. (2001), who add an oracle to Boullier’s algorithm. The more restricted Definition 1. A RCG G = hN, T, V, P, Si consists"
P09-2003,C08-2026,0,0.0192457,"on-terminals as late as possible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht,"
P09-2003,W01-1807,0,0.0343216,"s in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number na"
P09-2003,E99-1008,0,0.0336239,"gaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number names. Parsing algorithms for RCG have been introduced by Boullier (2000), who presents a directional top-down parsing algorithm using pseudocode, and Barth´elemy et al. (2001), who add an oracle to Boullier’s algorithm. The more restricted Definition 1. A RCG G = hN, T, V, P, Si consists of a) a finite set of predicates N with an arity function dim: N → N  {0} where S ∈ N is the start predicate with dim(S) = 1, b) disjoint finite sets of terminals T and variables V , c) a finite se"
P09-2003,2000.iwpt-1.8,0,0.488759,"hat we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are n"
P09-2003,C02-1028,0,\N,Missing
P09-2003,P87-1015,0,\N,Missing
P09-2003,P01-1007,0,\N,Missing
P15-1116,E14-1039,0,0.0667243,"Missing"
P15-1116,W07-1506,0,0.428456,"Missing"
P15-1116,P15-1147,0,0.19827,"Missing"
P15-1116,N10-1115,0,0.0419855,"s 1202–1212, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done online during parsing with a “swap” operation that allows to process input words out of order. This idea can be transferred, because also for e"
P15-1116,P11-2037,0,0.107021,"lowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) wollen VMFIN Das wollen wir umkehren That want we reverse “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done with Linear Conte"
P15-1116,P03-2006,0,0.0327909,"method is to annotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) wollen VMFIN Das wollen wir umkehren That want we reverse “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of"
P15-1116,P04-1015,0,0.376021,"operation that allows to process input words out of order. This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gradient method to train his parser. He reports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it"
P15-1116,W03-1005,0,0.468994,"Missing"
P15-1116,emms-2008-tree,0,0.0740848,"Missing"
P15-1116,W11-2913,0,0.231953,"P Das PDS wir umkehren PPER VVINF Figure 1: Example annotation with discontinuous constituents from TiGer Introduction Discontinuous constituents consist of more than one continuous block of tokens. They arise through phenomena which traditionally in linguistics would be analyzed as being the result of some kind of “movement”, such as extraposition or topicalization. The occurrence of discontinuous constituents does not necessarily depend on the degree of freedom in word order that a language allows for. They can be found, e.g., in almost equal proportions in English and German treebank data (Evang and Kallmeyer, 2011). Generally, discontinuous constituents are accounted for in treebank annotation. One annotation method consists of using trace nodes that denote the source of a movement and are co-indexed with the moved constituent. Another method is to annotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) wollen VMFIN Das wollen wir umkehren That want we reverse “This is"
P15-1116,D12-1095,0,0.202822,"release 2.2 (T I G ER), and the NeGra treebank (N E G RA). For T I G ER, we use the first half of the last 10,000 sentences for development and the second half for testing.3 We also recreate the split of Hall and Nivre (2008) (T I G ER HN), for which we split TiGer in 10 parts, assigning sentence i to part imod 10. The first of those parts is used for testing, the concatenation of the rest for training. 2 See Maier and Lichte (2011) for a formal account on gaps in treebanks. 3 This split, which corresponds to the split used in the SPMRL 2013 shared task (Seddah et al., 2013), was proposed in Farkas and Schmid (2012). We exclude sentences 46,234 and 50,224, because of annotation errors. Both contain nodes with more than one parent node. 1205 3.2 Experimental Setup Our parser is implemented in Java. We run all our experiments with Java 8 on an Intel Core i5, allocating 15 GB per experiment. All experiments are carried out with gold POS tags, as in previous work on shift-reduce constituency parsing (Zhang and Clark, 2009). Grammatical function labels are discarded. For the evaluation, we use the corresponding module of discodop.4 We report several metrics (as implemented in discodop): • Extended labeled bra"
P15-1116,P02-1018,0,0.155813,"xed with the moved constituent. Another method is to annotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) wollen VMFIN Das wollen wir umkehren That want we reverse “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of"
P15-1116,J13-1006,1,0.823591,"with the method usually used for PCFG parsing: For all maximal continuous parts of a discontinuous constituent, a separate node is introduced (Boyd, 2007). Subsequently, all nodes that do not cover the head child of the discontinuous constituent are removed. No further preprocessing or cleanup is applied. 68 66 64 1 2 4 8 62 60 2 4 6 8 10 12 14 16 18 20 Iteration Figure 5: N E G RA dev results (F1 ) for different beam sizes We perform 20 training iterations unless indicated otherwise. When training stops, we average the model (as in Daum´e III (2006)). We run further experiments with rparse5 (Kallmeyer and Maier, 2013) to facilitate a comparison with a grammar-based parser. 3.3 Results We start with discontinuous parsing experiments on N E G RA and T I G ER, followed by continuous parsing experiments, and a comparison to grammar-based parsing. 3.3.1 Discontinuous Parsing NeGra The first goal is to determine the effect of different beam sizes with BASELINE features and the C OMPOUND S WAPi operation. We run experiments with beam sizes 1, 2, 4 and 8; Fig. 5 shows the results obtained on the dev set after each iteration. Fig. 6 shows the average decoding speed during each iteration for each beam size (both smo"
P15-1116,P04-1042,0,0.484356,"nnotate discontinuities directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) wollen VMFIN Das wollen wir umkehren That want we reverse “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituen"
P15-1116,W12-4615,1,0.891802,"rsing times, LCFRS chart parsers employ different strategies to speed up search: Kallmeyer 1202 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can th"
P15-1116,W09-3811,0,0.407805,"nenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done online during parsing with a “swap” operation that allows to process input words out of order. This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gradient method to train"
P15-1116,P09-1040,0,0.602283,") and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done online during parsing with a “swap” operation that allows to process input words out of order. This idea can be transferred, because also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gra"
P15-1116,W07-2460,0,0.0757187,"Missing"
P15-1116,W05-1513,0,0.0640614,"also for every discontinuous constituency tree, one can find a continuous tree by reordering the terminals. Versley (2014) uses an adaptive gradient method to train his parser. He reports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present str"
P15-1116,P06-1023,0,0.244839,"directly by allowing for crossing branches. Fig. 1 shows an example for the latter approach with which we are concerned in this paper, namely, the annotation of (1). The tree contains a discontinuous VP due to the fact that the fronted pronoun is directly attached. (1) wollen VMFIN Das wollen wir umkehren That want we reverse “This is what we want to reverse” Several methods have been proposed for parsing such structures. Trace recovery can been framed as a separate pre-, post- or in-processing task to PCFG parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun, 2003; Levy and Manning, 2004; Schmid, 2006; Cai et al., 2011, among others); see particularly Schmid (2006) for more details. Directly annotated discontinuous constituents can be parsed with a dependency parser, given a reversible transformation from discontinuous constituency trees to non-projective dependency structures. Transformations have been proposed by Hall and Nivre (2008), who use complex edge labels that encode paths between lexical heads, and recently by Fern´andez-Gonz´alez and Martins (2015), who use edge labels to encode the attachment order of modifiers to heads. Direct parsing of discontinuous constituents can be done"
P15-1116,W13-4917,1,0.913197,"Missing"
P15-1116,W13-5701,0,0.224171,"Missing"
P15-1116,E12-1047,0,0.150481,"Missing"
P15-1116,W14-6104,0,0.515375,"of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1202–1212, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics and Maier (2013) use A∗ search; van Cranenburgh (2012) and van Cranenburgh and Bod (2013) use a coarse-to-fine strategy in combination with DataOriented Parsing; Angelov and Ljungl¨of (2014) use a novel cost estimation to rank parser items. Maier et al. (2012) apply a treebank transformation which limits the block degree and therewith also the parsing complexity. Recently Versley (2014) achieved a breakthrough with a EaFi, a classifier-based parser that uses an “easy-first” approach in the style of Goldberg and Elhadad (2010). In order to obtain discontinuous constituents, the parser uses a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): For every non-projective dependency tree, there is a projective dependency tree which can be obtained by reordering the input words. Non-projective dependency parsing can therefore be viewed as projective dependency parsing with an additional reordering of the input words. The reordering can be done o"
P15-1116,W09-3825,0,0.713995,"INARY-X-L and B INARYX-R reduce the first two elements on the stack to a new X constituent, with the lexical head coming from the left or the right child, respectively. F INISH removes the last element from the stack. We additionally use an I DLE transition, which can be applied any number of times after F INISH, to improve the comparability of analyses of different lengths (Zhu et al., 2013). The application of a transition is subject to restrictions. U NARY-X, e.g., can only be applied when there is at least a single item on the stack. We implement all restrictions listed in the appendix of Zhang and Clark (2009), and add additional restrictions that block transitions involving the root label when not having arrived at the end of a derivation. We do not use an underlying grammar to filter out transitions which have not been seen during training. For decoding, we use beam search (Zhang and Clark, 2011b). Decoding is started by putting the 1 As in other shift-reduce approaches, we assume that POS tagging is done outside of the parser. 1203 unigrams s0 tc, s0 wc, s1 tc, s1 wc, s2 tc, s2 wc, s3 tc, s3 wc, q0 wt, q1 wt, q2 wt, q3 wt, s0 lwc, s0 rwc, s0 uwc, s1 lwc, s1 rwc, s1 uwc bigrams s0 ws1 w, s0 ws1 c"
P15-1116,P11-1069,0,0.387464,"ports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluatio"
P15-1116,J11-1005,0,0.404207,"ports a parsing speed of 40-55 sent./sec. and results that surpass those reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluatio"
P15-1116,C12-1194,0,0.0369525,"Missing"
P15-1116,P13-1043,0,0.522616,"e reported for the above mentioned chart parsers. In (continuous) constituency parsing, incremental shift-reduce parsing using the structured perceptron is an established technique. While the structured perceptron for parsing has first been used by Collins and Roark (2004), classifier-based incremental shift-reduce parsing has been taken up by Sagae and Lavie (2005). A general formulation for the application of the perceptron algorithm to various problems, including shift-reduce constituency parsing, has been introduced by Zhang and Clark (2011b). Improvements have followed (Zhu et al., 2012; Zhu et al., 2013). A similar strategy has been shown to work well for CCG parsing (Zhang and Clark, 2011a), too. In this paper, we contribute a perceptron-based shift-reduce parsing architecture with beam search (following Zhu et al. (2013) and Bauer (2014)) and extend it such that it can create trees with crossing branches (following Versley (2014)). We present strategies to improve performance on discontinuous structures, such as a new feature set. Our parser is very fast (up to 640 sent./sec.), and produces accurate results. In our evaluation, where we pay particular attention to the parser performance on d"
P15-1116,P87-1015,0,\N,Missing
S18-1155,P10-1089,0,0.0137,"stical and knowledge-driven semantics. This classification depends on whether the assumption is that human knowledge is encapsulated in language man947 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 947–952 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Dataset Training Validation Test comparison of semantic similarity (Joubarne and Inkpen, 2011), information retrieval (Tandon and De Melo, 2010; Klein and Nelson, 2009), lexical disambiguation (Bergsma et al., 2009), improving general purpose NLP classifiers (Bergsma et al., 2010), and improving parsing performance (Pitler et al., 2010). Knowledge-driven approaches to the detection of semantic relations rely on manually constructed lexical and encyclopedic resources, such as ConceptNet (Speer et al., 2017), ImageNet (Russakovsky et al., 2015), WordNet (Miller and Fellbaum, 1998), Wiktionary, Open Mind Common Sense (Singh et al., 2002) and DBpedia (Mendes et al., 2012). In this work we follow a statistical based approach and show the strengths and weakness of the distributional semantics of the word vectors and ngram frequency counts in capturing the different types of"
S18-1155,N16-2002,0,0.012595,"with word embeddings is to formulate semantic relations in arithmetic fashion by creating a vector space in which words with similar contextual embeddings have closer vectors distance (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schtze, 2008; Mikolov et al., 2013c). The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes including capturing semantic relations (Gladkova et al., 2016; Attia et al., 2016). The Google n-gram corpus (Brants and Franz, 2006) is a collection of English word n-grams and their observed counts generated from 1 trillion words of texts from web pages. This corpus has been used in many different applications including estimating word-relatedness (Islam et al., 2012), This paper describes our system submission to the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Given two concepts and an attribute, the task is to determine whether the attribute is semantically related to one concept and not the other. In this work we assume that discri"
S18-1155,N13-1090,0,0.119332,"al application to the Firthian dictum “You shall know a word by the company it keeps” (Firth, 1957) which has become commonsense wisdom in lexical semantics. Features of the statistical model are extracted from unstructured data, such as words embeddings, n-gram counts, or directly from raw data. The basic idea with word embeddings is to formulate semantic relations in arithmetic fashion by creating a vector space in which words with similar contextual embeddings have closer vectors distance (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schtze, 2008; Mikolov et al., 2013c). The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes including capturing semantic relations (Gladkova et al., 2016; Attia et al., 2016). The Google n-gram corpus (Brants and Franz, 2006) is a collection of English word n-grams and their observed counts generated from 1 trillion words of texts from web pages. This corpus has been used in many different applications including estimating word-relatedn"
S18-1155,D14-1162,0,0.0768369,"in lexical semantics. Features of the statistical model are extracted from unstructured data, such as words embeddings, n-gram counts, or directly from raw data. The basic idea with word embeddings is to formulate semantic relations in arithmetic fashion by creating a vector space in which words with similar contextual embeddings have closer vectors distance (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schtze, 2008; Mikolov et al., 2013c). The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes including capturing semantic relations (Gladkova et al., 2016; Attia et al., 2016). The Google n-gram corpus (Brants and Franz, 2006) is a collection of English word n-grams and their observed counts generated from 1 trillion words of texts from web pages. This corpus has been used in many different applications including estimating word-relatedness (Islam et al., 2012), This paper describes our system submission to the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Giv"
S18-1155,C10-1100,0,0.0133334,"depends on whether the assumption is that human knowledge is encapsulated in language man947 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 947–952 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Dataset Training Validation Test comparison of semantic similarity (Joubarne and Inkpen, 2011), information retrieval (Tandon and De Melo, 2010; Klein and Nelson, 2009), lexical disambiguation (Bergsma et al., 2009), improving general purpose NLP classifiers (Bergsma et al., 2010), and improving parsing performance (Pitler et al., 2010). Knowledge-driven approaches to the detection of semantic relations rely on manually constructed lexical and encyclopedic resources, such as ConceptNet (Speer et al., 2017), ImageNet (Russakovsky et al., 2015), WordNet (Miller and Fellbaum, 1998), Wiktionary, Open Mind Common Sense (Singh et al., 2002) and DBpedia (Mendes et al., 2012). In this work we follow a statistical based approach and show the strengths and weakness of the distributional semantics of the word vectors and ngram frequency counts in capturing the different types of discriminative attributes. 2 # of triples 17,547 2,722 2,"
S18-1155,S18-1117,0,0.0412099,"encoded. The two resources are the Google n-gram counts and the Google News Word2Vec. Google n-gram counts. We use the Google 5-gram counts as provided by Google Books ngrams1 (Michel et al., 2011; Lin et al., 2012). Google News Word2Vec. This is a publicly available pre-trained word vector2 , built with the word2vec architecture (Mikolov et al., 2013b) from a news corpus of 100B words (3M vocabulary entries) with 300 dimensions, negative sampling, using continuous bag of words and window size of 5. Task and Data Description The goal of the shared task on Capturing Discriminative Attributes (Krebs et al., 2018) is to detect semantic difference between pairs of concepts, or in other words, determine whether a semantic property differentiates between two possibly related concepts. For example both ‘bear’ and ‘goat’ are animals, but only a ‘bear’ has ‘claws’. Therefore ‘claws’ is considered as a discriminative feature. The shared task data is formatted in triples that represent a ternary relation between two concepts (Word1 , Word2 ) on one hand and an attribute (Word3 ) on the other. Word3 is considered as a discriminative attribute if, and only if, it characterizes Word1 but not Word2 . For example,"
S18-1155,P12-3029,0,0.0122596,"butes. The basic idea with deep learning is to use hidden layers of neural nets to automatically capture the underlying factors that lead from the input to the output, eliminating the need for feature engineering. The system is trained on features extracted from two main publicly available resources that fall within the paradigm of unstructured data as no manual lexical or encyclopedic knowledge is encoded. The two resources are the Google n-gram counts and the Google News Word2Vec. Google n-gram counts. We use the Google 5-gram counts as provided by Google Books ngrams1 (Michel et al., 2011; Lin et al., 2012). Google News Word2Vec. This is a publicly available pre-trained word vector2 , built with the word2vec architecture (Mikolov et al., 2013b) from a news corpus of 100B words (3M vocabulary entries) with 300 dimensions, negative sampling, using continuous bag of words and window size of 5. Task and Data Description The goal of the shared task on Capturing Discriminative Attributes (Krebs et al., 2018) is to detect semantic difference between pairs of concepts, or in other words, determine whether a semantic property differentiates between two possibly related concepts. For example both ‘bear’ a"
S18-1155,mendes-etal-2012-dbpedia,0,0.0548022,"Missing"
W06-1614,A97-1014,0,\N,Missing
W06-1614,J06-2001,0,\N,Missing
W06-1614,J03-4003,0,\N,Missing
W06-1614,P03-1054,0,\N,Missing
W06-1614,P03-1014,0,\N,Missing
W06-1614,P06-3004,1,\N,Missing
W06-1614,C04-1056,0,\N,Missing
W06-1614,P03-1056,0,\N,Missing
W06-1614,P05-1039,0,\N,Missing
W06-1614,P03-1013,0,\N,Missing
W08-1701,P07-2004,1,0.842421,"has been used to implement efficient parsing algorithms for several formalisms, including TAG and RCG. Unfortunately, it does not include any built-in GUI and requires a good knowledge of the GNU build tools to compile parsers. This makes it relatively difficult to use. DyALog’s main quality lies in its efficiency in terms of parsing time and its capacity to handle very large resources. Unlike TuLiPA, it does not compute semantic representations. The closest approach to TuLiPA corresponds to the SemTAG system13 , which extends TAG parsers compiled with DyALog with a semantic calculus module (Gardent and Parmentier, 2007). Unlike TuLiPA, this system only supports TAG, and does not provide any graphical output allowing to easily check the result of parsing. Note that, for grammar designers mainly interested in TAG, SemTAG and TuLiPA can be seen as complementary tools. Indeed, one may use TuLiPA to develop the grammar and check specific syntactic structures thanks to its intuitive parsing environment. Once the grammar is stable, one may use SemTAG in batch processing to parse corpuses and build semantic representations using large grammars. This combination of these 2 systems is made easier by the fact that both"
W08-1701,C04-1044,0,0.144272,"having a frontier node marked for anchoring (i.e., lexicalization). At parsing time, the tree schemata are anchored according to the input string. This anchoring selects a subgrammar supposed to cover the input string. Unfortunately, this subgrammar may contain many trees that either do not lead to a parse or for which we know a priori that they cannot be combined within the same derivation (so we should not predict a derivation from one of these trees to another during parsing). As a result, the parser could have poor performance because of the many derivation paths that have to be explored. Bonfante et al. (2004) proposed to polarize the structures of the grammar, and to apply an automaton-based filtering of the compatible structures. The idea is the following. One compute polarities representing the needs/resources brought by a given tree (or tree tuple for TT-MCTAG). A substitution or foot node with category NP reflects a need for an NP (written NP-). In the same way, an NP root node reflects a resource of type NP (written NP+). Then you build an automaton whose edges correspond to trees, and states to polarities brought by trees along the path. The automaton is then traversed to extract all paths l"
W08-1701,C96-2120,0,0.0602791,"This environment is being used (i) at the University of T¨ubingen, in the context of the development of a TT-MCTAG for German describing both syntax and semantics, and (ii) at LORIA Nancy, in the development of an XTAG-based metagrammar for English. The German grammar, called GerTT (for German Tree Tuples), is released under a LGPL license for Linguistic Resources11 and is presented in (Kallmeyer et al., 2008). The test-suite currently used to check the grammar is hand-crafted. A more systematic evaluation of the grammar is in preparation, using the Test Suite for Natural Language Processing (Lehmann et al., 1996). Unfortunately, as mentioned above, the linguist has to move back-and-forth from the grammar/lexicon descriptions to the parser, i.e., each time the parser reports grammar errors, the linguist fixes these and then recomputes the XML files and then parses again. To avoid this tedious task of resources re-compilation, we started developing an Eclipse8 plug-in for the TuLiPA system. Thus, the linguist will be able to manage all these resources, and to call the parser, the metagrammar compiler, and the lexConverter from a common interface (see Fig. 6). Figure 6: TuLiPA’s eclipse plug-in. The moti"
W08-1701,2000.iwpt-1.8,0,0.68321,"t between formalisms (e.g., in terms of parsing complexity in practice), and would allow for a better sharing of resources (e.g., having a common lexicon, from which different features would be extracted depending on the target formalism). In this context, we present a parsing environment relying on a general architecture that can be used for parsing with mildly context-sensitive (MCS) formalisms1 (Joshi, 1987). Its underlying idea is to use Range Concatenation Grammar (RCG) as a pivot formalism, for RCG has been shown to strictly include MCS languages while being parsable in polynomial time (Boullier, 2000). Currently, this architecture supports tree-based grammars (Tree-Adjoining Grammars and MultiComponent Tree-Adjoining Grammars with Tree Tuples (Lichte, 2007)). More precisely, treebased grammars are first converted into equivalent RCGs, which are then used for parsing. The result of RCG parsing is finally interpreted to extract a derivation structure for the input grammar, as well as to perform additional processings (e.g., semantic calculus, extraction of dependency views). The paper is structured as follows. In section 2, we present the architecture of the TuLiPA parsing environment and sh"
W08-1701,C00-2087,0,0.0343331,"can be seen as complementary tools. Indeed, one may use TuLiPA to develop the grammar and check specific syntactic structures thanks to its intuitive parsing environment. Once the grammar is stable, one may use SemTAG in batch processing to parse corpuses and build semantic representations using large grammars. This combination of these 2 systems is made easier by the fact that both use the same input formats (a metagrammar in the XMG language and a text-based lexicon). This approach is the one being adopted for the development of a French TAG equipped with semantics. For Interaction Grammar (Perrier, 2000), there exists an engineering environment gathering the XMG metagrammar compiler and an eLEtrOstatic PARser (LEOPAR).14 This environment is being used to develop an Interaction Grammar for French. TuLiPA’s lexical disambiguation module For other formalisms, there exist state-of-the-art grammar engineering environments that have been used for many years to design large deep grammars for several languages. For Lexical Functional Grammar, one may cite the Xerox Linguistic Environment (XLE).15 For Head-driven Phrase Structure Grammar, the main available systems are the Linguistic Knowledge Base (L"
W08-1701,A92-1039,0,0.387077,"ars (namely Tree-Adjoining Grammars (TAG) and Multi-Component TreeAdjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not only of syntactic structures, but also of the corresponding semantic representations. It is used for the development of a tree-based grammar for German. 1 Introduction Grammars and lexicons represent important linguistic resources for many NLP applications, among which one may cite dialog systems, automatic summarization or machine translation. Developing such resources is known to be a complex task that needs useful tools such as parsers and generators (Erbach, 1992). Furthermore, there is a lack of a common framework allowing for multi-formalism grammar engineering. Thus, many formalisms have been proposed to model natural language, each coming with specific implementations. Having a common framework would facilitate the comparison 1 A formalism is said to be mildly context sensitive (MCS) iff (i) it generates limited cross-serial dependencies, (ii) it is polynomially parsable, and (iii) the string languages generated by the formalism have the constant growth property (e.g., n {a2 |n ≥ 0} does not have this property). Examples of MCS formalisms include T"
W08-1701,E03-1030,1,0.818722,"with respect to the input grammar. The use of RCG as a pivot formalism, and thus of an RCG parser as a core component of the system, leads to a modular architecture. In turns, this makes TuLiPA more easily extensible, either in terms of functionalities, or in terms of formalisms. 2.1 NPj name(j,john) 2 Range Concatenation Grammar as a pivot formalism VP Adding functionalities to the parsing environment As an illustration of TuLiPA’s extensibility, one may consider two extensions applied to the system recently. First, a semantic calculus using the syntax/semantics interface for TAG proposed by Gardent and Kallmeyer (2003) has been added. This interface associates each tree with flat semantic formulas. The arguments of these formulas are unification variables, which are co-indexed with features labelling the nodes of the syntactic tree. During classical TAG derivation, trees are combined, triggering unifications of the feature structures labelling nodes. As a result of these unifications, the arguments of the semantic formulas are unified (see Fig. 1). 2 7 is the only valid state and {proper., trans., det., noun.} the only compatible set of trees). 0 John 1 1 eats 2 intrans. 0 proper. 1 NP+ a3 2 2 det. S+ 3 4 c"
W08-1701,kallmeyer-etal-2008-developing,1,\N,Missing
W08-2316,W08-2316,1,0.0523041,"CG). Furthermore, for tree-based grammars, the parser computes not only syntactic analyses but also the corresponding semantic representations. 1 Introduction The starting point of the work presented here is the aim to implement a parser for a German TAG-based grammar that computes syntax and semantics. As a grammar formalism for German we chose a multicomponent extension of TAG called TT-MCTAG (Multicomponent TAG with Tree Tuples) which has been first introduced by Lichte (2007). With some additional constraints, TT-MCTAG is mildly context-sensitive (MCS) as shown by Kallmeyer and Parmentier (2008). Instead of implementing a specific TT-MCTAG parser we follow a more general approach by using Range Concatenation Grammars (RCG) as a pivot formalism for parsing MCS languages. Indeed the generative capacity of RCGs lies beyond MCS, while they stay parsable in polynomial time (Boullier, 1999). In this context, the TT-MCTAG (or TAG) is transformed into a strongly equivalent RCG that is then used for parsing. We have implemented the conversion into RCG, the RCG parser and the retrieval of the corresponding TTMCTAG analyses. The parsing architecture comes with graphical input and output interfa"
W08-2316,P89-1018,0,0.504997,"Missing"
W08-2316,C04-1044,0,0.262645,"Missing"
W08-2316,2000.iwpt-1.8,0,0.564791,"Missing"
W08-2316,E03-1030,1,0.897416,"Missing"
W08-2316,kallmeyer-etal-2008-developing,1,0.890865,"Missing"
W08-2316,W08-2308,1,0.773959,"0 , ni,1 i ∈ PD and for 1 ≤ j ≤ k − 1: hni,j , ni,j+1 i ∈ PD where this edge is labelled with ǫ. TT-MCTAG has been proposed to deal with free word order languages. An example from German is shown in Fig. 1. Here, the NPnom auxiliary tree 2 For a tree γ, Pγ is the parent relation on the nodes, i.e., hx, yi ∈ Pγ for nodes x, y in γ iff x is the mother of y. adjoins directly to verspricht (its head) while the NPacc tree adjoins to the root of a tree that adjoins to the root of a tree that adjoins to reparieren. For a more extended account of German word order using TT-MCTAG see Lichte (2007) and Lichte and Kallmeyer (2008). TT-MCTAG can be further restricted, such that at each point of the derivation the number of pending β-trees is at most k. This subclass is also called k-TT-MCTAG. Definition 2 (k-TT-MCTAG) A TT-MCTAG G = hI, A, N, T, Ai is of rank k (or a k-TT-MCTAG for short) iff for each derivation tree D licensed in G: (TT-k) There are no nodes n, h0 , . . . , hk , a0 , . . . , ak in D such that the label of ai is an argument tree of the label of hi and hhi , ni, hn, ai i ∈ + for 0 ≤ i ≤ k. PD TT-MCTAG in general are NP-complete (Søgaard et al., 2007) while k-TT-MCTAG are MCS (Kallmeyer and Parmentier, 20"
W09-3808,W05-1502,0,0.223393,"Missing"
W09-3808,W05-1506,0,0.0331572,"minal filter iff we can find an injec~ tive mapping fT : Term = {hk, li |φ(k)(l) ∈T and either k > i or (k = i and l > j)} → {pos + 1, . . . , n} such that We have presented an Earley-style algorithm for simple range concatenation grammar, formulated as deduction system. Furthermore, we have presented a set of filters on the chart reducing the number of items. An implementation and a test with grammars extracted from treebanks showed that reasonable parsing times can be achieved. We are currently working on a probabilistic k-best extension of our parser which resumes comparable work for PCFG (Huang and Chiang, 2005). Unfortunately, experiments with the Earley algorithm have shown that with grammars of a reasonable size for data-driven parsing (> 15, 000 clauses), an exhaustive parsing is no longer efficient, due to the highly ambiguous grammars. Algorithms using only passive items seem more promising in this context since they facilitate the application of A∗ parsing techniques. ~ 1. wfT (hk,li) = φ(k)(l) for all hk, li ∈ Term; 2. for all hk1 , l1 i, hk2 , l2 i ∈ Term with k1 = k2 and l1 < l2 : fT (hk2 , l2 i) ≥ fT (hk1 , l1 i) + (l2 − l1 ); 3. for all hk1 , l1 i, hk2 , l2 i ∈ Term with k1 < ~ 1 ) |− k2"
W09-3808,W06-1508,0,0.239538,"ar context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987), the equivalent multiple context-free grammars (MCFG) (Seki et al., 1991) and simple range concatenation grammars (sRCG) (Boullier, 1998) have recently attracted an increasing interest in the context of natural language processing. For example, Maier and Søgaard (2008) propose to extract simple RCGs from constituency treebanks with crossing branches while Kuhlmann and Satta (2009) propose to extract LCFRS from non-projective dependency treebanks. Another application area of this class of formalisms is biological computing (Kato et al., 2006). This paper addresses the symbolic parsing of sRCG/LCFRS. Starting from the parsing algorithms presented in Burden and Ljungl¨of (2005) and Villemonte de la Clergerie (2002), we propose an incremental Earley algorithm for simple RCG. The strategy is roughly like the one pursued in Villemonte de la Clergerie (2002). However, instead of the automaton-based formalization in Villemonte de la Clergerie’s work, we give a general formulation of an incremental Earley algorithm, using the framework of parsing as deduction. In order to reduce the search space, we introduce different types of filters on"
W09-3808,N03-1016,0,0.143096,"gument, we convert the item into a passive one: ~ → Ψ, ~ pos, hi, ji, ρ [B(ψ) ~B ] [B, ρ] ~ ~ = i, |ψ(i)| = j, |ψ| ~ ρ ~B (ψ) = ρ Complete: Whenever we have a passive B item we can use it to move the dot over the variable of the last argument of B in a parent A-clause that was used to predict it. (n − pos) dim(A) ~ ~ + (dim(A) − i) ≥ (|φ(i)| − j) + Σk=i+1 |φ(k)| ~ → . . . B(ξ) ~ . . . , pos, hk, li, ρ [B, ρ ~B ], [A(φ) ~A ] ~ → . . . B(ξ) ~ . . . , pos′ , hk, l + 1i, ρ [A(φ) ~] The length filter is applied to results of predict, resume, suspend and complete. A second filter, first proposed in Klein and Manning (2003), checks for the presence of required preterminals. In our case, we assume the where the dot in the antecedent A-item precedes ~ ρB |), the last range in ρ~B is the variable ξ(|~ ′ hpos, pos i, and for all 1 ≤ m < |~ ρB |: ρ~B (m) = 63 5 Conclusion and Future Work preterminals to be treated as terminals, so this filter amounts to checking for the presence of all terminals in the predicted part of a clause (the part to the right of the dot) in the remaining input. Furthermore, we check that the terminals appear in the predicted order and that the distance between two of them is at least the num"
W09-3808,E09-1055,0,0.0624657,"exactly one rule S(ε) → ε and S does not appear in any of the righthand sides of the rules in the grammar. A rule is an ε-rule if one of the arguments 1 Introduction Linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987), the equivalent multiple context-free grammars (MCFG) (Seki et al., 1991) and simple range concatenation grammars (sRCG) (Boullier, 1998) have recently attracted an increasing interest in the context of natural language processing. For example, Maier and Søgaard (2008) propose to extract simple RCGs from constituency treebanks with crossing branches while Kuhlmann and Satta (2009) propose to extract LCFRS from non-projective dependency treebanks. Another application area of this class of formalisms is biological computing (Kato et al., 2006). This paper addresses the symbolic parsing of sRCG/LCFRS. Starting from the parsing algorithms presented in Burden and Ljungl¨of (2005) and Villemonte de la Clergerie (2002), we propose an incremental Earley algorithm for simple RCG. The strategy is roughly like the one pursued in Villemonte de la Clergerie (2002). However, instead of the automaton-based formalization in Villemonte de la Clergerie’s work, we give a general formulat"
W09-3808,P87-1015,0,\N,Missing
W09-3808,C02-1028,0,\N,Missing
W09-3810,2000.iwpt-1.8,0,0.874802,". Furthermore, we transfer this property to grammars extracted from treebanks. 1 Introduction Discontinuous phrases are frequent in natural language, particularly in languages with a relatively free word order. Several formalisms have been proposed in the literature for modeling trees containing such phrases. These include nonprojective dependency grammar (Nivre, 2006), discontinuous phrase structure grammar (DPSG) (Bunt et al., 1987), as well as linear contextfree rewriting systems (LCFRS) (Vijay-Shanker et al., 1987) and the equivalent formalism of simple range concatenation grammar (sRCG) (Boullier, 2000). Kuhlmann (2007) uses LCFRS for non-projective dependency trees. DPSG have been used in Plaehn (2004) for data-driven parsing of treebanks with discontinuous constituent annotation. Maier and Søgaard (2008) extract sRCGs from treebanks with discontinuous constituent structures. Both LCFRS and sRCG can model discontinuities and allow for synchronous rewriting as well. We speak of synchronous rewriting when two or 2 Synchronous Rewriting Trees in German treebanks By synchronous rewriting we indicate the synchronous instantiation of two or more context-free derivation processes. As an example, c"
W09-3810,E87-1034,0,0.536484,"Missing"
W09-3810,P87-1015,0,\N,Missing
W09-3810,J07-2003,0,\N,Missing
W10-1407,W07-1506,0,0.207037,"the direct annotation of the discontinuous VP. S HD OC SB VP MO MO AVP AVP HD MO HD MO HD Noch nie habe ich so viel gewählt . ADV ADV VAFIN 1.Sg.Pres.Ind PPER 1.Sg.*.Nom ADV ADV VVPP $. Figure 2: A tree from NeGra Since in general, the annotation mechanisms for non-local dependencies lie beyond the expressivity of Context-Free Grammar, non-local information is inaccessible for PCFG parsing and therefore generally discarded. In NeGra/TIGER annotation, e.g., tree transformation algorithms are applied before parsing in order to resolve the crossing branches. See, e.g., K¨ubler et al. (2008) and Boyd (2007) for details. If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous"
W10-1407,emms-2008-tree,0,0.300149,"btained from the parser output, and B&lt; B B t1 t2 x y |τK |and |τA |nodes, respectively. For a T -mapping σ from τK to τA with the sets D, I, S and M, we compute them as follows. C t3 B &gt;B t4 z t1 t2 z x y dice(σ) = 1 − Figure 5: TDIST example jaccard (σ) = 1 − one from the corresponding treebank trees. Using these tuple sets, we compute labeled and unlabeled recall (LR/UR), precision (LP/UP), and the F1 measure (LF1 /UF1 ) in the usual way. Note that if k = 1, our metric is identical to its PCFG version. EVALB does not necessarily reflect parser output quality (Rehbein and van Genabith, 2007; Emms, 2008; K¨ubler et al., 2008). One of its major problems is that attachment errors are penalized too hard. As the second evaluation method, we therefore choose the tree-distance measure (henceforth TDIST) (Zhang and Shasha, 1989), which levitates this problem. It has been proposed for parser evaluation by Emms (2008). TDIST is an ideal candidate for evaluation of the output of a PLCFRS, since it the fact if trees have crossing branches or not is not relevant to it. Two trees τk and τA are compared on the basis of T -mappings from τk to τA . A T -mapping is a partial mapping σ of nodes of τk to nodes"
W10-1407,N09-1061,0,0.212024,"Missing"
W10-1407,W09-3808,1,0.873605,"s and SRCGs (Boullier, 1998). 60 Scan: 0 : [A, hhi, i + 1ii] Unary: A POS tag of wi+1 in : [B, ρ~] in + |log(p) |: [A, ρ~] p : A(~ ρ) → B(~ ρ) ∈ P inB : [B, ρ~B ], inC : [C, ρ~C ] inB + inC + log(p) : [A, ρ~A ] where p : A(ρ~A ) → B(ρ~B )C(ρ~C ) is an instantiated rule. Goal: [S, hh0, nii] Binary: Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice. It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are ε. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G´omez-Rodr´ıguez et al., 2009) and ε-components on LHS of rules can be removed (Boullier, 1998). We make the assumption that POS tagging is done before parsing. The POS tags are special non-terminals of fan-out 1. Consequently, the rules are either of the form A(a) → ε where A is a POS tag and a ∈ T or of the form A(~ α) → B(~x"
W10-1407,C10-1061,1,0.917711,"annotation, e.g., tree transformation algorithms are applied before parsing in order to resolve the crossing branches. See, e.g., K¨ubler et al. (2008) and Boyd (2007) for details. If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge pa"
W10-1407,N03-1016,0,0.684953,"quently, the rules are either of the form A(a) → ε where A is a POS tag and a ∈ T or of the form A(~ α) → B(~x) or A(~ α) → B(~x)C(~y ) where α ~ ∈ (V + )dim(A) , i.e., only the rules for POS tags contain terminals in their LHSs. The parser items have the form [A, ρ~], with A ∈ N and ρ~ a vector of ranges characterizing all components of the span of A. We specify the set of weighted parse items via the deduction rules in Fig. 3. Parsing time can be reduced by reordering the agenda during parsing such that those items are processed first which lead to a complete parse more quickly than others (Klein and Manning, 2003a). The parser uses for this purpose an admissible, but not monotonic estimate called LR estimate. It gives (relative to a sentence length) an estimate of the outside probability of some non-terminal A with a span of a certain length (the sum of the lengths of all the components of the span), a certain number of terminals to the left of the first and to the right of the last component and a certain number of terminals gaps in between the components of the A span, i.e., filling the gaps. A discussion of other estimates is presented at length in Kallmeyer and Maier (2010). 2.3 LCFRS for Modeling"
W10-1407,kubler-etal-2008-compare,1,0.888142,"Missing"
W10-1407,E09-1055,0,0.243423,"Missing"
W10-1407,H94-1020,0,0.097034,"Missing"
W10-1407,J03-1006,0,0.398935,"en defined in the same way (Kato et al., 2006). 1 MCFGs are equivalent to LCFRSs and SRCGs (Boullier, 1998). 60 Scan: 0 : [A, hhi, i + 1ii] Unary: A POS tag of wi+1 in : [B, ρ~] in + |log(p) |: [A, ρ~] p : A(~ ρ) → B(~ ρ) ∈ P inB : [B, ρ~B ], inC : [C, ρ~C ] inB + inC + log(p) : [A, ρ~A ] where p : A(ρ~A ) → B(ρ~B )C(ρ~C ) is an instantiated rule. Goal: [S, hh0, nii] Binary: Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice. It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are ε. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G´omez-Rodr´ıguez et al., 2009) and ε-components on LHS of rules can be removed (Boullier, 1998). We make the assumption that POS tagging is done before parsing. The POS tags are special non-terminals of fan-out 1. Consequently, the rules a"
W10-1407,N07-1051,0,0.0382311,"ed and are not put on the agenda. This blocking has an extremely beneficial effect on parser speed. However, it is paid by a worse recall, as experiments with smaller data sets have shown. A complete discussion of the effects of estimates, as well as a discussion of other possible optimizations, is presented in Kallmeyer and Maier (2010). Recall finally that LCFRS parses are more informative than PCFG parses – a lower score for LCFRS EVALB than for PCFG EVALB does not necessarily mean that the PCFG parse is “better”. 4.2 e.g., lies at 93.62 (Dice) and 87.87 (Jaccard). For the Berkeley Parser (Petrov and Klein, 2007), 94.72 and 89.87 is reported. We see that our results lie in them same range. However, Jaccard scores are lower since this normalization punishes a higher number of edit operations more severely than Dice. In order to meaningfully interpret which treebank properties are responsible for the fact that between the gold trees and the trees from the parser, the German data requires more tree edit operations than the English data, a TDIST evaluation of the output of an off-the-shelf PCFG parser would be necessary. This is left for future work. 4.3 Dependency Evaluation For the dependency evaluation"
W10-1407,W08-1006,0,0.151503,"Missing"
W10-1407,W07-2460,0,0.153041,"Missing"
W10-1407,A97-1014,0,0.951993,"requently. In (1), the discontinuity is caused by an extraposed relative clause. (1) wieder treffen alle Attribute zu, die auch again match all attributes V PART which also sonst immer passen otherwise always fit ‘Again, the same attributes as always apply.’ Another language with a rather free word order is Bulgarian. In (2), the discontinuity is caused by topicalization. (2) Himikali1 az kupuvam samo evtini t1 Pens1 I buy only expensive t1 ‘As for pens, I only buy expensive ones.’ Figure 1: A tree from T¨uBa-D/Z However, in a few other treebanks, such as the German NeGra and TIGER treebanks (Skut et al., 1997; Brants et al., 2002), crossing branches are allowed. This way, all dependents of a long-distance dependency can be grouped under a single node. 58 Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 58–66, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics Fig. 2 shows a tree from NeGra with the annotation of (3). (3) Noch nie habe ich so viel gew¨ahlt Yet never have I so much chosen ‘Never have I had that much choice.’ Note the direct annotation of the discontinuous VP. S HD OC SB VP MO MO AVP AVP"
W10-1407,P87-1015,0,0.941198,"on algorithms are applied before parsing in order to resolve the crossing branches. See, e.g., K¨ubler et al. (2008) and Boyd (2007) for details. If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge parser output quality, we use four dif"
W10-1407,J03-4003,0,\N,Missing
W10-1407,P00-1058,0,\N,Missing
W10-4415,P99-1065,0,0.0762209,"Missing"
W10-4415,N09-1061,0,0.274568,"Missing"
W10-4415,P02-1018,0,0.147066,"constituency parsing is comparable to the output quality of other state-of-the-art results, all while yielding structures that display discontinuous dependencies. 1 Introduction It is a well-known fact that Context-Free Grammar (CFG) does not provide enough expressivity to describe natural languages. For data-driven probabilistic CFG parsing, some of the information present in constituency treebanks, namely the annotation of non-local dependencies, cannot be captured by a CFG. It is therefore removed before learning a PCFG from the treebank and must be re-introduced in a post-processing step (Johnson, 2002; Levy and Manning, 2004). Non-projective dependencies also lie beyond the expressivity of CFG. Current dependency parsers are able to parse them (McDonald et al., 2005; Nivre et al., 2007). However, the corresponding parsing algorithms are not grammar-based. We propose to use a grammar formalism with an extended domain of locality that is able to capture the non-local dependencies both in constituency and dependency treebanks. We chose Linear Context-Free Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG that allows non-terminals to span tuples of discontinuous strings. T"
W10-4415,C10-1061,1,0.880244,"ntinuity and Non-Projectivity: Using Mildly Context-Sensitive Formalisms for Data-Driven Parsing where A ∈ N , ρ ~ ∈ (P os(w) × P os(w))dim(A) the vector of ranges characterizing all components of the span of A. We specify the set of weighted parse items via the deduction rules in Fig. 2. An instantiated rule is a rule where variables have been replaced with corresponding vectors of ranges. Our parser performs weighted deductive parsing, based on this deduction system. We use a chart C and an agenda A, both initially empty, and we proceed as in Fig. 3. For more details of the parser, see also Kallmeyer and Maier (2010). length of 25 words.1 This leads to a size of 14,858, resp. 1,651 sentences for the NeGra training, resp. test sets and to 13,935, resp. 1,300 sentences for the PDT training, resp. test set. 3.2 Grammar Extraction From all of our data sets, we extract PLCFRSs. For the constituent sets, we use the algorithm from Maier and Søgaard (2008), for the dependencies the algorithm from Kuhlmann and Satta (2009). For reasons of space, we restrict ourselves here to the examples in Fig. 4–6. 3 Experimental Setup 3.1 S Data VP Our data sources are the NeGra treebank (Skut et al., 1997) and the Prague Depen"
W10-4415,W09-3810,1,0.8051,"ranches as in the German Negra treebank) allow a straight-forward interpretation of the trees as LCFRS derivation structures, without the necessity of inducing linguistic knowledge.1 This considerably facilitates the extraction of probabilistic LCFRSs (Maier and Søgaard, 2008). The same holds for non-projective dependency structures, which can also straight-forwardly be interpreted as LCFRS derivation structures (Kuhlmann and Satta, 2009). Previous approaches that have used non-context-free formalisms for data-driven constituency parsing (Plaehn, 2004; Chiang, 2003) are either too restricted (Kallmeyer et al., 2009) or do not allow for an immediate interpretation of the treebank trees as derivation structures. Grammarbased non-projective dependency parsing has, to our knowledge, not been attempted at all. First results for PLCFRS constituency parsing with a detailed evaluation have been presented in Maier (2010). The contribution of this article is to present the first results for data-driven dependency parsing on the dependency version of the German NeGra treebank and on the Prague Dependency Treebank. Furthermore, we give greater detail on the parser and the experimental setup. We also additionally inv"
W10-4415,W06-1508,0,0.560149,"2. Here, the superscript is the vertical context and the subscript the horizontal context of the new non-terminal X. The probabilities are then computed based on the 123 Wolfgang Maier, Laura Kallmeyer S 1000 XS NN 900 XS VP ,NN Number of items (in 1000) 800 VP XVP PDS XVP ADV ,PDS XSVMFIN ,VP PDS NeGra LCFRS NeGra combined splits NeGra PCFG NeGra Dep PDT VMFIN XVP VAINF ,ADV NN ADV VAINF 700 600 500 400 300 200 100 Figure 10: Sample Markovization with v = 1, h = 2 0 6 8 frequencies of rules in the treebank, using a Maximum Likelihood estimator (MLE). Such an estimation has been used before (Kato et al., 2006). 3.6 NeGra LCFRS S sp. VP sp. S ◦ VP sp. NeGra PCFG NeGra Dep. PDT bin. 16,904 17,033 18,362 18,503 15,563 68,847 38,312 18 20 22 24 Figure 11: Number of items bin. lab. 4,142 4,179 4,952 4,995 3,898 49,085 24,119 Table 1: PLCFRSs extracted from training sets LP LR LF1 UP UR UF1 Tab. 1 contains the properties of the grammars: The number of rules in the unbinarized grammar, the number of rules in the binarized and markovized grammar and the number of labels (including POS tags) in the binarized and markovized grammar. 4 Experiments LCFRS 73.24 73.56 73.40 77.12 77.46 77.29 w/ category splits V"
W10-4415,P03-1054,0,0.0662375,"Missing"
W10-4415,E09-1055,0,0.469165,"span tuples of discontinuous strings. The reason why we think LCFRS particularly well-suited is that treebanks with a direct annotation of discontinuous constituents (with crossing branches as in the German Negra treebank) allow a straight-forward interpretation of the trees as LCFRS derivation structures, without the necessity of inducing linguistic knowledge.1 This considerably facilitates the extraction of probabilistic LCFRSs (Maier and Søgaard, 2008). The same holds for non-projective dependency structures, which can also straight-forwardly be interpreted as LCFRS derivation structures (Kuhlmann and Satta, 2009). Previous approaches that have used non-context-free formalisms for data-driven constituency parsing (Plaehn, 2004; Chiang, 2003) are either too restricted (Kallmeyer et al., 2009) or do not allow for an immediate interpretation of the treebank trees as derivation structures. Grammarbased non-projective dependency parsing has, to our knowledge, not been attempted at all. First results for PLCFRS constituency parsing with a detailed evaluation have been presented in Maier (2010). The contribution of this article is to present the first results for data-driven dependency parsing on the dependen"
W10-4415,P04-1042,0,0.371165,"rsing is comparable to the output quality of other state-of-the-art results, all while yielding structures that display discontinuous dependencies. 1 Introduction It is a well-known fact that Context-Free Grammar (CFG) does not provide enough expressivity to describe natural languages. For data-driven probabilistic CFG parsing, some of the information present in constituency treebanks, namely the annotation of non-local dependencies, cannot be captured by a CFG. It is therefore removed before learning a PCFG from the treebank and must be re-introduced in a post-processing step (Johnson, 2002; Levy and Manning, 2004). Non-projective dependencies also lie beyond the expressivity of CFG. Current dependency parsers are able to parse them (McDonald et al., 2005; Nivre et al., 2007). However, the corresponding parsing algorithms are not grammar-based. We propose to use a grammar formalism with an extended domain of locality that is able to capture the non-local dependencies both in constituency and dependency treebanks. We chose Linear Context-Free Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG that allows non-terminals to span tuples of discontinuous strings. The reason why we think LC"
W10-4415,W10-1407,1,0.852723,"dependency structures, which can also straight-forwardly be interpreted as LCFRS derivation structures (Kuhlmann and Satta, 2009). Previous approaches that have used non-context-free formalisms for data-driven constituency parsing (Plaehn, 2004; Chiang, 2003) are either too restricted (Kallmeyer et al., 2009) or do not allow for an immediate interpretation of the treebank trees as derivation structures. Grammarbased non-projective dependency parsing has, to our knowledge, not been attempted at all. First results for PLCFRS constituency parsing with a detailed evaluation have been presented in Maier (2010). The contribution of this article is to present the first results for data-driven dependency parsing on the dependency version of the German NeGra treebank and on the Prague Dependency Treebank. Furthermore, we give greater detail on the parser and the experimental setup. We also additionally investigate the effect on manually introduced category splits for PLCFRS constituency parsing. 1 Treebank trees in which non-local dependencies are annotated differently, such as with trace nodes in the Penn Treebank, could also be interpreted as LCFRS derivations given an appropriate transformation algo"
W10-4415,W07-2216,0,0.018926,"our result, we still lie in the same range. Plaehn (2004) also reports results for direct parsing of the discontinuous constituents using Probabilistic Discontinuous Phrase Structure Grammar (DPSG). See Maier (2010) for details. 4.2 Dependency Parsing In this section, we present the first grammar-based non-projective dependency parsing results. As Kuhlmann and Satta (2009) note, the principal advantage of grammar-based non-projective dependency parsing is that edge probabilities can be finetuned while staying polynomially parseable. This is not possible in the Maximum Spanning Tree approach (McDonald and Satta, 2007). For compar6 The results from the literature were obtained on sentences longer than 25 words and would most likely be better for our sentence length. ison of our dependency parser output, we report labeled and unlabeled attachment score and completely correct graphs (punctuation included). As markovization setting for the PDT set, we choose v = 2 and h = ∞. UAS LAS UComp LComp NeGra Grammar MST 78.98 87.96 71.84 82.62 32.65 42.16 25.03 29.56 PDT Grammar 51.44 67.09 14.92 9.46 MST 76.01 40.54 28.92 17.23 Table 4: Dependency parsing Tab. 4 contain the dependency parsing results for our parser a"
W10-4415,H05-1066,0,0.411279,"Missing"
W10-4415,J03-1006,0,0.363351,"tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules (1) (1) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) (m) · · · Am (X1 LCFRS and p : P → [0..1] a function such that ~ = 1. for all A ∈ N : ΣA(~x)→Φ∈P p(A(~x) → Φ) ~ 2.2 PLCFRS Parsing Our parser is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). We assume without loss of generality that our LCFRSs are binary (i.e., have rank 2) (G´omez-Rodr´ıguez et al., 2009) and do not contain rules where some of the LHS components are ε (Boullier, 1998; Seki et al., 1991). Our binarization algorithm is given in Section 3.4. Furthermore, we make the assumption that POS tagging is done before parsing. The POS tags are special non-terminals of fan-out 1. Scan: 0 : [A, hhi, i + 1ii] Unary: A POS tag of wi+1 in : [B, ρ~] in + |log(p) |: [A, ρ~] p : A(~ α) → B(~ α) ∈ P inB : [B, ρ~B ], inC : [C, ρ~C ] inB + inC + log(p) : [A, ρ~A ] where p : A(ρ~A ) →"
W10-4415,N07-1051,0,0.20918,"AINF(werden) → S1 (X1 X2 X3 ) → VP2 (X1 , X2 X3 ) → VP2 (X1 , X2 ) → → ε → ε ε ε VP2 (X1 , X3 ) VMFIN(X2 ) VP2 (X1 , X2 ) VAINF(X3 ) PROAV(X1 ) VVPP(X2 ) Figure 5: LCFRS rules for the tree in Fig. 4 3.3 Grammar Annotation Grammar annotation (i.e., manual enhancement of annotation information through category splitting) has previously been successfully employed in parsing German (Versley, 2005). In order to see if such modifications can have a beneficial effect in PLCFRS parsing, we will apply the following category splits to the Negra constituency data sets with unmodified labels (inspired by Petrov and Klein (2007)): We split the category S (“sentence”) into SRC (“relative clause”) and S (all other categories S). Relative clauses mostly occur in a very specific 1 This length restriction can be greatly alleviated by using an estimate of outside probabilities of parse items which speeds up parsing (Kallmeyer and Maier, 2010) 121 Wolfgang Maier, Laura Kallmeyer for all rules r = A(~ α) → A0 (α~0 ) . . . Am (α~m ) in P with m > 1 do remove r from P R := ∅ pick new non-terminals C1 , . . . , Cm−1 add the rule A(~ α) → A0 (α~0 )C1 (γ~1 ) to R where γ~1 is obtained by reducing α ~ with α~0 for all i, 1 ≤ i ≤ m"
W10-4415,W08-1006,0,0.0561134,".94 here 74.46 R&M 08 77.20 P&K 07 80.1 Table 3: Previous NeGra PCFG parsing Tab. 2 presents the constituent parsing results for both data sets with (LCFRS) and without (PCFG) crossing branches. For the sake of comparison, we 5 Note that our metric is equivalent to the corresponding PCFG metric for dim(A) = 1. 124 Discontinuity and Non-Projectivity: Using Mildly Context-Sensitive Formalisms for Data-Driven Parsing report PCFG parsing results from the literature6 in Tab. 3, namely for PCFG parsing with a plain vanilla treebank grammar (K¨ubler, 2005), for PCFG parsing with the Stanford parser (Rafferty and Manning, 2008) (markovization as in our parser), and for the current state-of-the-art, namely PCFG parsing with a latent variable model (Petrov and Klein, 2007). We see that the LCFRS parser output (which contains more information than the output of a PCFG parser) is competitive. The PCFG (1-LCFRS) parsing results are even closer to the ones of current systems. Recall that these are just first results, much optimization potential is left. Before we evaluate the experiments with category splits, we replace all split labels in the parser output with the corresponding original labels. The results show that the"
W10-4415,W07-2460,0,0.210488,"Missing"
W10-4415,A97-1014,0,0.873466,"Missing"
W10-4415,P87-1015,0,0.914137,"could also be interpreted as LCFRS derivations given an appropriate transformation algorithm. 119 Wolfgang Maier, Laura Kallmeyer The remainder of this paper is structured as follows. In the following section, we present the formalism and our parser. Sect. 3 is dedicated the experimental setup, Sect. 4 contains the experimental results. In Sect. 5, we present a conclusion. 2 A Parser for Probabilistic Linear Context-Free Rewriting Systems We notate LCFRS with the syntax of simple Range Concatenation Grammars (SRCG) (Boullier, 1998), a formalism that is equivalent to LCFRS. 2.1 PLCFRS A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules (1) (1) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) (m) · · · Am (X1 LCFRS and p : P → [0..1] a function such that ~ = 1. for all A ∈ N : ΣA(~x)→Φ∈P p(A(~x) → Φ) ~ 2.2 PLCFRS Parsing Our parser is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive pars"
W10-4415,J03-4003,0,\N,Missing
W10-4415,P00-1058,0,\N,Missing
W12-3624,P07-1086,0,0.445015,"n allows the retrieval of a considerable number of coordinate structures beyond the ones having a coordinating conjunction. 1 1.1 Introduction Motivation Coordination is a difficult topic, in terms of linguistic description and analysis as well as for NLP approaches. Most linguistic frameworks still struggle with finding an account for coordination that is descriptively fully adequate (Hartmann, 2000). This is also the reason why coordination is not adequately encoded in the annotation of major treebanks. From an NLP perspective, coordination is one of the major sources for errors in parsing (Hogan, 2007). If parsing of coordinate structures can be improved, overall parsing quality also benefits (K¨ubler et al., 2009). 166 And consequently, downstream NLP applications, such as question answering or machine translation, would benefit as well. However, since linguistic frameworks in general are challenged by the diverse phenomena of coordination, a consistent annotation of coordinate structures, clearly marking the phenomenon as such as well as its scope, is a difficult enterprise. Consequently, this makes the detection of conjuncts and their boundaries a highly non-trivial task. Nevertheless, a"
W12-3624,E09-1047,1,0.936618,"Missing"
W12-3624,H05-1105,0,0.0143213,"1 shows an example sentence with two coordinate structures, the inside one a coordinate noun phrase (NP) with 3 conjuncts, and the outside one a coordinate verb phrase (VP) with two complex conjuncts. These coordinate structures are labeled by ordinary phrasal categories such as VP and NP and can thus not be distinguished at the phrasal level from VPs and NP that do not involve coordination. There are approaches to improving parsing for coordinations, but most of these approaches are restricted to very narrow definitions such as coordinations of noun compounds such as “oil and gas resources” (Nakov and Hearst, 2005), coordinations of symmetrical NPs (Hogan, 2007; Shimbo and Hara, 2007), or coordinations of “A CC B” where A and B are conjuncts, and CC is an overt conjunction (K¨ubler et al., 2009). To our knowledge, there is no attempt at covering all coordination types. One goal of this paper is to demonstrate a wide range of coordination phenomena that have to be taken into account in a thorough treatment of coordinations. We additionally present a proposal for an enhanced annotation of coordination for the Penn Treebank. The annotation is focused on punctuation and allows for an in-depth investigation"
W12-3624,D07-1064,0,0.262261,"ne a coordinate noun phrase (NP) with 3 conjuncts, and the outside one a coordinate verb phrase (VP) with two complex conjuncts. These coordinate structures are labeled by ordinary phrasal categories such as VP and NP and can thus not be distinguished at the phrasal level from VPs and NP that do not involve coordination. There are approaches to improving parsing for coordinations, but most of these approaches are restricted to very narrow definitions such as coordinations of noun compounds such as “oil and gas resources” (Nakov and Hearst, 2005), coordinations of symmetrical NPs (Hogan, 2007; Shimbo and Hara, 2007), or coordinations of “A CC B” where A and B are conjuncts, and CC is an overt conjunction (K¨ubler et al., 2009). To our knowledge, there is no attempt at covering all coordination types. One goal of this paper is to demonstrate a wide range of coordination phenomena that have to be taken into account in a thorough treatment of coordinations. We additionally present a proposal for an enhanced annotation of coordination for the Penn Treebank. The annotation is focused on punctuation and allows for an in-depth investigation of coordinations, for example for linguistic treatments, but also for w"
W12-3624,telljohann-etal-2004-tuba,1,0.831414,"Missing"
W12-3624,J93-2004,0,\N,Missing
W12-4615,W07-1506,0,0.3462,"Missing"
W12-4615,W11-2913,1,0.869716,"ation labels is used in order to establish implicit edges. In other treebanks, e.g., the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks, crossing branches are allowed.1 This way, all parts of a discontinuous constituent can 1 The annotation differences between TIGER and NeGra are minor and can be neglected for the purpose of this work. be grouped under a single node. There is no fundamental difference between both representations: PTB-style annotation can be converted into a NeGra/TIGER-style annotation. This has been done in the Discontinuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011). For data-driven parsing with Probabilistic CFG (PCFG), the annotation information concerning discontinuities must be discarded, because it exceeds the expressivity of CFG. For NeGra, there exist two methods, namely (i) attaching nonhead daughters of discontinuous constituents to higher positions in the tree, such that the crossing branches disappear (the NeGra distribution contains a version of the treebank in which this transformation is readily carried out), or (ii) introducing an additional non-terminal node for each continuous part of a discontinuous constituent (Boyd, 2007). As an examp"
W12-4615,N10-1035,0,0.0586414,"Missing"
W12-4615,C10-1061,1,0.852132,"ssing branches removal for NeGra. Note that the argument structure is changed as a result of the removal of the crossing branches. discontinuities (Maier and Lichte, 2011). In LCFRS, a single non-terminal can span k ≥ 1 continuous blocks of a string. A CFG is simply a special case of an LCFRS in which k = 1. k is called the fan-out of the non-terminal, and a corresponding constituent is said to have block degree k. It has been shown that probabilistic data-driven parsing on the basis of Probabilistic LCFRS (PLCFRS) is feasible and gives good results while preserving discontinuity information (Kallmeyer and Maier, 2010; Maier, 2010; van Cranenburgh et al., 2011; Evang and Kallmeyer, 2011; van Cranenburgh, 2012; Maier, 2012). The major problem of PLCFRS parsing is its high computational complexity. A binarized PCFG can be parsed in O(n3 ), parsing a binarized LCFRS takes O(n3k ) (Seki et al., 1991), where k is the fan-out of the grammar (the maximal fan-out of any of its non-terminals). The parsers from the literature allow for an unbounded k. This leads to parsing times beyond practically acceptable values for sentences longer than 25 to 30 words. In this paper, our goal is to show that by restricting the b"
W12-4615,W06-1508,0,0.908376,". In the specialized deduction system, unary items now take the form [A, i, j] and binary items take the form [A, i, j, k, l], where A ∈ N and i, j, resp. k, l are spans dominated by A with 0 ≤ i &lt; j &lt; k &lt; l ≤ n. The goal item is [S, 0, n]. We replace the old Unary and Binary deduction rules in figure 4 with 14 new rules, one per production type. Figure 6 shows the new scan rule and the complete rules for type 1, type 6 and type 10, which should make the basic idea clear. Note that there is no need to refer to instantiations anymore. Our case-by-case strategy is similar to the one employed by Kato et al. (2006). As in our previous work, we specify the set 0 : [A, i, i + 1] A POS tag of wi+1 in : [B, i, j] in + |log(p) |: [A, i, j] where p : A(X) → B(X) ∈ P . Complete1: inB : [B, i, j, k, l], inC : [C, l, u] inB + inC + |log(p) |: [A, i, j, k, u] where p : A(X, Y Z) → B(X, Y )C(Z) ∈ P . Complete6: inB : [B, i, x, k, y], inC : [C, x, j, y, l] inB + inC + |log(p) |: [A, i, j, k, l] where p : A(XY, ZU ) → B(X, Z)C(Y, U ) ∈ P Complete10: Figure 6: Weighted CYK deduction rules for 2-LCFRS of parse items using the algorithm of weighted deductive parsing (WDP) (Nederhof, 2003). In WDP, one maintains a prior"
W12-4615,N03-1016,0,0.677656,"CYK deduction rules for 2-LCFRS of parse items using the algorithm of weighted deductive parsing (WDP) (Nederhof, 2003). In WDP, one maintains a priority queue of items, sorted by the resp. Viterbi inside scores. The topmost item is always processed first. WDP guarantees optimality, i.e., that the best parse is found. 5.2 A Novel Outside Estimate One can speed up parsing by adding to the Viterbi inside score of an item an estimate of its Viterbi outside score, in other words, an estimate of the cost of completion of the item to a complete parse. This has proven to be successful for both PCFG (Klein and Manning, 2003) and PLCFRS (Kallmeyer and Maier, 2010). As outside estimate, one uses the outside probability of a summary of the item, i.e., of an equivalence class of parse items. The difficulty for PLCFRS is to choose the summary such that optimality is maintained through the two estimate properties admissibility and monotonicity (Klein and Manning, 2003). Here, we present the novel LN estimate, which is based on a summary that records only the sum of the span lengths and the length of the entire sentence. It is the first practically computable estimate which allows for maintaining optimality. The estimat"
W12-4615,W10-1407,1,0.742211,"removal for NeGra. Note that the argument structure is changed as a result of the removal of the crossing branches. discontinuities (Maier and Lichte, 2011). In LCFRS, a single non-terminal can span k ≥ 1 continuous blocks of a string. A CFG is simply a special case of an LCFRS in which k = 1. k is called the fan-out of the non-terminal, and a corresponding constituent is said to have block degree k. It has been shown that probabilistic data-driven parsing on the basis of Probabilistic LCFRS (PLCFRS) is feasible and gives good results while preserving discontinuity information (Kallmeyer and Maier, 2010; Maier, 2010; van Cranenburgh et al., 2011; Evang and Kallmeyer, 2011; van Cranenburgh, 2012; Maier, 2012). The major problem of PLCFRS parsing is its high computational complexity. A binarized PCFG can be parsed in O(n3 ), parsing a binarized LCFRS takes O(n3k ) (Seki et al., 1991), where k is the fan-out of the grammar (the maximal fan-out of any of its non-terminals). The parsers from the literature allow for an unbounded k. This leads to parsing times beyond practically acceptable values for sentences longer than 25 to 30 words. In this paper, our goal is to show that by restricting the b"
W12-4615,J93-2004,0,0.0406121,"man NeGra treebank and the Discontinuous Penn Treebank in which all trees have block degree two. The experiments show that compared to previous work, our approach provides an enormous speed-up while delivering an output of comparable richness. 1 Introduction In many constituency treebanks, the syntactic annotation takes the form of Context-Free Grammar (CFG) derivation trees, i.e., of trees with no crossing branches. Discontinuous structures (Huck and Ojeda, 1987) cannot be modeled with CFG and are therefore handled by an additional mechanism in such an annotation. In the Penn Treebank (PTB) (Marcus et al., 1993), for instance, a combination of trace nodes and co-indexation labels is used in order to establish implicit edges. In other treebanks, e.g., the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks, crossing branches are allowed.1 This way, all parts of a discontinuous constituent can 1 The annotation differences between TIGER and NeGra are minor and can be neglected for the purpose of this work. be grouped under a single node. There is no fundamental difference between both representations: PTB-style annotation can be converted into a NeGra/TIGER-style annotation. This"
W12-4615,J03-1006,0,0.356855,"ilar to the one employed by Kato et al. (2006). As in our previous work, we specify the set 0 : [A, i, i + 1] A POS tag of wi+1 in : [B, i, j] in + |log(p) |: [A, i, j] where p : A(X) → B(X) ∈ P . Complete1: inB : [B, i, j, k, l], inC : [C, l, u] inB + inC + |log(p) |: [A, i, j, k, u] where p : A(X, Y Z) → B(X, Y )C(Z) ∈ P . Complete6: inB : [B, i, x, k, y], inC : [C, x, j, y, l] inB + inC + |log(p) |: [A, i, j, k, l] where p : A(XY, ZU ) → B(X, Z)C(Y, U ) ∈ P Complete10: Figure 6: Weighted CYK deduction rules for 2-LCFRS of parse items using the algorithm of weighted deductive parsing (WDP) (Nederhof, 2003). In WDP, one maintains a priority queue of items, sorted by the resp. Viterbi inside scores. The topmost item is always processed first. WDP guarantees optimality, i.e., that the best parse is found. 5.2 A Novel Outside Estimate One can speed up parsing by adding to the Viterbi inside score of an item an estimate of its Viterbi outside score, in other words, an estimate of the cost of completion of the item to a complete parse. This has proven to be successful for both PCFG (Klein and Manning, 2003) and PLCFRS (Kallmeyer and Maier, 2010). As outside estimate, one uses the outside probability"
W12-4615,W07-2460,0,0.493969,"Missing"
W12-4615,A97-1014,0,0.386495,"ous speed-up while delivering an output of comparable richness. 1 Introduction In many constituency treebanks, the syntactic annotation takes the form of Context-Free Grammar (CFG) derivation trees, i.e., of trees with no crossing branches. Discontinuous structures (Huck and Ojeda, 1987) cannot be modeled with CFG and are therefore handled by an additional mechanism in such an annotation. In the Penn Treebank (PTB) (Marcus et al., 1993), for instance, a combination of trace nodes and co-indexation labels is used in order to establish implicit edges. In other treebanks, e.g., the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks, crossing branches are allowed.1 This way, all parts of a discontinuous constituent can 1 The annotation differences between TIGER and NeGra are minor and can be neglected for the purpose of this work. be grouped under a single node. There is no fundamental difference between both representations: PTB-style annotation can be converted into a NeGra/TIGER-style annotation. This has been done in the Discontinuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011). For data-driven parsing with Probabilistic CFG (PCFG), the annotation information concerning"
W12-4615,W11-3805,0,0.240245,"Missing"
W12-4615,E12-1047,0,0.547251,"Missing"
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W13-5713,J98-4004,0,0.514173,"c.). So why not just do directional parsing with the unbinarized treebank grammar? It is common knowledge that vanilla treebank PCFGs do not perform well. This also holds for PLCFRS. Markovization has been proven to be an effective remedy for both PCFG and PLCFRS parsing. It can be achieved through the probability model itself (Collins, 1999) or by annotating the treebank grammar (Klein and Manning, 2003b; Kallmeyer and Maier, 2013): Instead of using a unique non-terminal as in deterministic binarization, one uses a single non-terminal and adorns it with the vertical (“parent annotation”, see Johnson (1998)) and horizontal context of the occurrence of the rule in the treebank. This augments the coverage of the grammar and helps to achieve better parsing results by adding a possibly infinite number of implicit non-binary rules. and provide a formulation of deterministic binarization using this notation. An implementation of the debinarization algorithm has been tested on the German NeGra treebank (Skut et al., 1997). First experimental results confirm that in practice, the debinarized grammar can perform better than than the plain treebank grammar. The remainder of the article is structured as fo"
W13-5713,W09-3808,1,0.748267,"he use of non-binary rules, such as directional CYK parsing or Earley parsing. The reasoning behind this assumption is as follows. Firstly, the longer the righthand side of a rule, the easier it is to check during parsing on a symbolic basis if it is possible to use the rule 1 See http://phil.hhu.de/rparse. See http://github.com/andreasvc/disco-dop. 3 For very long sentences, i.e. &gt; 60 words, the parser still has extreme memory requirements. 2 113 in a complete parse or not by checking the requirements of the rule with respect to the yet unparsed part of the input such as it is done, e.g., in Kallmeyer and Maier (2009).4 A comparable strategy, the F grammar projection estimate (Klein and Manning, 2003a), has been employed in PCFG parsing. Secondly, practical experience with Range Concatenation Grammar (RCG) (Boullier, 1998) and Grammatical Framework (GF) (Ranta, 2004) parsing points in the same direction. Lazy computation of instantiations in RCG parsing as done in the TuLiPA system (Kallmeyer et al., 2009) and the SYNTAXE parser (Boullier and Deschamp, 1988) (Boullier, p.c.) seem to be less effective with shorter right-hand sides of rules because less constraints can be collected at once. Practical experim"
W13-5713,J13-1006,1,0.916396,"ar retains the markovization information. The algorithm has been implemented and successfully applied to the German NeGra treebank. 1 Introduction Linear Context-Free Rewriting System (LCFRS), an extension of CFG in which non-terminals can cover more than a single span, can be used to model discontinuous constituents as well as non-projective dependencies (Maier and Lichte, 2011). It has therefore recently been exploited for direct data-driven parsing of such structures. See, e.g., Maier and Søgaard (2008), Maier and Kallmeyer (2010), van Cranenburgh (2011), van Cranenburgh et al. (2012), and Kallmeyer and Maier (2013). A major problem with data-driven probabilistic LCFRS (PLCFRS) parsing is the high parsing complexity. Given a binary grammar, CYK parsing can be done in O(n3k ) where k is the fanout of the grammar, that is, the maximum number of spans that a single non-terminal can cover (Seki et al., 1991). While for PCFG parsing, k is 1, for PLCFRS, k will typically be ≈ 5. Sentences with lengths around 23 to 25 words require very high, unpractical parsing times (20 minutes and more per sentence) (van Cranenburgh et al., 2011; Kallmeyer and Maier, 2013). One possibility to obtain faster parsing is to redu"
W13-5713,P09-2003,1,0.86361,"parser still has extreme memory requirements. 2 113 in a complete parse or not by checking the requirements of the rule with respect to the yet unparsed part of the input such as it is done, e.g., in Kallmeyer and Maier (2009).4 A comparable strategy, the F grammar projection estimate (Klein and Manning, 2003a), has been employed in PCFG parsing. Secondly, practical experience with Range Concatenation Grammar (RCG) (Boullier, 1998) and Grammatical Framework (GF) (Ranta, 2004) parsing points in the same direction. Lazy computation of instantiations in RCG parsing as done in the TuLiPA system (Kallmeyer et al., 2009) and the SYNTAXE parser (Boullier and Deschamp, 1988) (Boullier, p.c.) seem to be less effective with shorter right-hand sides of rules because less constraints can be collected at once. Practical experiments with the GF5 parser (Angelov, 2009), which implements an Earley strategy, indicate that certain optimizations loose their effect with binary grammars (Angelov, p.c.). So why not just do directional parsing with the unbinarized treebank grammar? It is common knowledge that vanilla treebank PCFGs do not perform well. This also holds for PLCFRS. Markovization has been proven to be an effecti"
W13-5713,N03-1016,0,0.713146,"require very high, unpractical parsing times (20 minutes and more per sentence) (van Cranenburgh et al., 2011; Kallmeyer and Maier, 2013). One possibility to obtain faster parsing is to reduce the fan-out of the grammar by reducing the number of gaps in the trees from which the grammar is extracted. This has been done by Maier et al. (2012) who transform the trees of the German TIGER treebank such that a grammar fan-out of 2 is guaranteed. For unrestricted PLCFRS parsing, other solutions have been implemented. The parser of Kallmeyer and Maier (2013)1 offers A∗ parsing with outside estimates (Klein and Manning, 2003a). With this technique the practical sentence length limit is shifted upwards by 7 to 10 words. Kallmeyer and Maier also propose non-admissible estimates which provide a greater speed-up but also let the results degrade. The parser of van Cranenburgh (2012)2 also does not maintain exact search. It implements a coarse-to-fine strategy in a more general PCFG is created from the treebank PLCFRS using the algorithm of Barth´elemy et al. (2001). The PCFG chart is then used to filter the PLCFRS chart. While this approach in principle removes the limit3 on sentence length, it also leads to degraded"
W13-5713,E09-1009,0,0.229809,"the F grammar projection estimate (Klein and Manning, 2003a), has been employed in PCFG parsing. Secondly, practical experience with Range Concatenation Grammar (RCG) (Boullier, 1998) and Grammatical Framework (GF) (Ranta, 2004) parsing points in the same direction. Lazy computation of instantiations in RCG parsing as done in the TuLiPA system (Kallmeyer et al., 2009) and the SYNTAXE parser (Boullier and Deschamp, 1988) (Boullier, p.c.) seem to be less effective with shorter right-hand sides of rules because less constraints can be collected at once. Practical experiments with the GF5 parser (Angelov, 2009), which implements an Earley strategy, indicate that certain optimizations loose their effect with binary grammars (Angelov, p.c.). So why not just do directional parsing with the unbinarized treebank grammar? It is common knowledge that vanilla treebank PCFGs do not perform well. This also holds for PLCFRS. Markovization has been proven to be an effective remedy for both PCFG and PLCFRS parsing. It can be achieved through the probability model itself (Collins, 1999) or by annotating the treebank grammar (Klein and Manning, 2003b; Kallmeyer and Maier, 2013): Instead of using a unique non-termi"
W13-5713,P03-1054,0,0.22096,"require very high, unpractical parsing times (20 minutes and more per sentence) (van Cranenburgh et al., 2011; Kallmeyer and Maier, 2013). One possibility to obtain faster parsing is to reduce the fan-out of the grammar by reducing the number of gaps in the trees from which the grammar is extracted. This has been done by Maier et al. (2012) who transform the trees of the German TIGER treebank such that a grammar fan-out of 2 is guaranteed. For unrestricted PLCFRS parsing, other solutions have been implemented. The parser of Kallmeyer and Maier (2013)1 offers A∗ parsing with outside estimates (Klein and Manning, 2003a). With this technique the practical sentence length limit is shifted upwards by 7 to 10 words. Kallmeyer and Maier also propose non-admissible estimates which provide a greater speed-up but also let the results degrade. The parser of van Cranenburgh (2012)2 also does not maintain exact search. It implements a coarse-to-fine strategy in a more general PCFG is created from the treebank PLCFRS using the algorithm of Barth´elemy et al. (2001). The PCFG chart is then used to filter the PLCFRS chart. While this approach in principle removes the limit3 on sentence length, it also leads to degraded"
W13-5713,P01-1007,0,0.0705194,"Missing"
W13-5713,W98-0105,0,0.0218917,"g on a symbolic basis if it is possible to use the rule 1 See http://phil.hhu.de/rparse. See http://github.com/andreasvc/disco-dop. 3 For very long sentences, i.e. &gt; 60 words, the parser still has extreme memory requirements. 2 113 in a complete parse or not by checking the requirements of the rule with respect to the yet unparsed part of the input such as it is done, e.g., in Kallmeyer and Maier (2009).4 A comparable strategy, the F grammar projection estimate (Klein and Manning, 2003a), has been employed in PCFG parsing. Secondly, practical experience with Range Concatenation Grammar (RCG) (Boullier, 1998) and Grammatical Framework (GF) (Ranta, 2004) parsing points in the same direction. Lazy computation of instantiations in RCG parsing as done in the TuLiPA system (Kallmeyer et al., 2009) and the SYNTAXE parser (Boullier and Deschamp, 1988) (Boullier, p.c.) seem to be less effective with shorter right-hand sides of rules because less constraints can be collected at once. Practical experiments with the GF5 parser (Angelov, 2009), which implements an Earley strategy, indicate that certain optimizations loose their effect with binary grammars (Angelov, p.c.). So why not just do directional parsin"
W13-5713,W12-4615,1,0.845386,"arsing can be done in O(n3k ) where k is the fanout of the grammar, that is, the maximum number of spans that a single non-terminal can cover (Seki et al., 1991). While for PCFG parsing, k is 1, for PLCFRS, k will typically be ≈ 5. Sentences with lengths around 23 to 25 words require very high, unpractical parsing times (20 minutes and more per sentence) (van Cranenburgh et al., 2011; Kallmeyer and Maier, 2013). One possibility to obtain faster parsing is to reduce the fan-out of the grammar by reducing the number of gaps in the trees from which the grammar is extracted. This has been done by Maier et al. (2012) who transform the trees of the German TIGER treebank such that a grammar fan-out of 2 is guaranteed. For unrestricted PLCFRS parsing, other solutions have been implemented. The parser of Kallmeyer and Maier (2013)1 offers A∗ parsing with outside estimates (Klein and Manning, 2003a). With this technique the practical sentence length limit is shifted upwards by 7 to 10 words. Kallmeyer and Maier also propose non-admissible estimates which provide a greater speed-up but also let the results degrade. The parser of van Cranenburgh (2012)2 also does not maintain exact search. It implements a coarse"
W13-5713,A97-1014,0,0.168423,"llmeyer and Maier, 2013): Instead of using a unique non-terminal as in deterministic binarization, one uses a single non-terminal and adorns it with the vertical (“parent annotation”, see Johnson (1998)) and horizontal context of the occurrence of the rule in the treebank. This augments the coverage of the grammar and helps to achieve better parsing results by adding a possibly infinite number of implicit non-binary rules. and provide a formulation of deterministic binarization using this notation. An implementation of the debinarization algorithm has been tested on the German NeGra treebank (Skut et al., 1997). First experimental results confirm that in practice, the debinarized grammar can perform better than than the plain treebank grammar. The remainder of the article is structured as follows. In the following section, we define treebank LCFRS and introduce our new rule representation. We furthermore introduce the binarization, resp. markovization algorithm. In section 3, we introduce our new debinarization algorithm. Section 4 presents the experimental evaluation and section 5 closes the article. 2 LCFRS 2.1 Definition In LCFRS (Vijay-Shanker et al., 1987), a single nonterminal can span k ≥ 1 c"
W13-5713,W11-3805,0,0.0308189,"Missing"
W13-5713,E12-1047,0,0.0285116,"Missing"
W13-5713,C02-1028,0,0.0445693,"the maximal fan-out of any of its non-terminals. The properties which distinguish a treebank LCFRS from a regular LCFRS are the requirements that 1. all non-terminal arguments are variables except in terminating lexical rules in which the only argument of the LHS consists of a single terminal, and 2. the ordering property. These properties allows us to notate rules in a more compact way. Let (N, T, V, P, S) be an LCFRS. A rule r ∈ P (1) (1) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) (m) (m) · · · Am (X1 , . . . , Xdim(Am ) ) 6 This is the ordering property which Villemonte de la Clergerie (2002) defines for RCGs. LCFRSs with this property are called monotone (Michaelis, 2001), MCFGs non-permuting (Kracht, 2003). 2. We write ϕ ~  x for some x ∈ N for the deletion of all occurrences of x in ϕ ~ , followed by the deletion of subvectors which become empty. 3. ϕ ~ ↓x for some x ∈ N denotes the splitting of all subvectors of ϕ ~ such that a vector boundary is introduced between all ϕ ~ [i][j] and ϕ ~ [i][j + 1], 1 ≤ i ≤ |~ ϕ|, 1 ≤ j ≤ |~ ϕ[i] |− 1 iff ϕ ~ [i][j] = x. Note that for every LCFRS, there is an equivalent LCFRS which fulfills the treebank LCFRS conditions (Kallmeyer, 2010). A p"
W13-5713,C10-1061,1,\N,Missing
W13-5713,J03-4003,0,\N,Missing
W13-5713,W10-4415,1,\N,Missing
W14-3912,P96-1041,0,0.013775,"are trained on the provided training data. Lexical probabilities: The lexical probability component consists of a dictionary for each label containing the words found under that label and their relative frequencies. Each word type and its count of tokens are added to the total for each respective label. After training, the probability of a given label emitting a word (i.e., P (word|label)) is derived from these counts. To handle out-of-vocabulary words, we use Chen-Goodman “one-count” smoothing, which approximates the probabilities of unknown words as compared to the occurrence of singletons (Chen and Goodman, 1996). Character n-gram probabilities: The characterbased n-gram model serves mostly as a back-off in case a word is out-of-vocabulary, in which case the lexical probability may not be reliable. However, it also provides important information in the case of mixed words, which may use morphology from one language added to a stem from the other one. In this setting, unigrams are not informative. For this reason, we select longer n-grams, with n ranging between 2 and 5. Character n-gram probabilities are calculated as follows: For each training set, the words in that training set are sorted into lists"
W14-3912,P05-1045,0,0.0577003,"Missing"
W14-3912,N13-1131,0,0.0839953,"rd that could exist in either language, e.g., no in the Spanish-English data). Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets. These methods are based on variants of character n-grams. Seminal work in this area is by Beesley (1988) and Grefenstette (1995). Lui and Baldwin (2014) showed that character n-grams also perform on Twitter messages. One of a few recent approaches working on individual words is by King et al. (2014), who worked on historical data; see also work by Nguyen and Dogruz (2013) and King and Abney (2013). Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools. All the information sources are weighted in the Markov model. One advantage of our approach is that it is languageindependent. We use the exact same architecture for all language pairs, and the only difference for the individual language pairs lies in a manual, non-exhaustive search for the best weights. Our results show that the approach works well for the one language pair with d"
W14-3912,W14-1303,0,0.0143217,"ons) also common. Named entities (ne) are also frequent, and accounting for them adds a significant complication to the task. Less common are mixed (to account for words that may e.g., apply L1 morphology to an L2 word), and ambiguous (to cover a word that could exist in either language, e.g., no in the Spanish-English data). Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets. These methods are based on variants of character n-grams. Seminal work in this area is by Beesley (1988) and Grefenstette (1995). Lui and Baldwin (2014) showed that character n-grams also perform on Twitter messages. One of a few recent approaches working on individual words is by King et al. (2014), who worked on historical data; see also work by Nguyen and Dogruz (2013) and King and Abney (2013). Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools. All the information sources are weighted in the Markov model. One advantage of our approach is that it is languageindependent. We use th"
W14-3912,D13-1084,0,0.0385764,"and ambiguous (to cover a word that could exist in either language, e.g., no in the Spanish-English data). Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets. These methods are based on variants of character n-grams. Seminal work in this area is by Beesley (1988) and Grefenstette (1995). Lui and Baldwin (2014) showed that character n-grams also perform on Twitter messages. One of a few recent approaches working on individual words is by King et al. (2014), who worked on historical data; see also work by Nguyen and Dogruz (2013) and King and Abney (2013). Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools. All the information sources are weighted in the Markov model. One advantage of our approach is that it is languageindependent. We use the exact same architecture for all language pairs, and the only difference for the individual language pairs lies in a manual, non-exhaustive search for the best weights. Our results show that the approach works well for th"
W14-3912,D11-1141,0,0.0161445,"Missing"
W14-3912,W14-3907,0,0.135251,"Missing"
W14-4204,W12-2108,0,0.0241609,"nce of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coordinates -120◦ , -55◦ and -29◦ , 30◦ (roughly delimiting Latin America), as well as -10◦ , 35◦ and 3◦ , 46◦ (roughly delimiting Spain). The download ran from July 2 to July 4, 2014. In a second step, we sorted the tweets a"
W14-4204,R13-1026,0,0.0218585,"for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). reaches an overall F-score of 67.72 on the fiveclass problem. On the two-class problem, human classification is outper"
W14-4204,W13-1729,0,0.0233276,"usses the results, and Section 8 concludes the article. 2 Related Work Research on language identification has seen a variety of methods. A well established technique is the use of character n-gram models. Cavnar and Trenkle (1994) build n-gram frequency “profiles” for several languages and classify text by matching it to the profiles. Dunning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, ˇ uˇrek and Kolkus (2009), who treat “noisy” and Reh˚ web text and therefore consider the particular influence of single words in discriminating between languages. Language i"
W14-4204,P11-1038,0,0.0746979,"Missing"
W14-4204,N10-1027,0,0.0172443,"ties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, ˇ uˇrek and Kolkus (2009), who treat “noisy” and Reh˚ web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and"
W14-4204,P12-3005,0,0.0318804,"le language, such as the regional varieties of Spanish. This task is especially challenging because the differences between 1 We are aware that there are natively Spanish-speaking communities elsewhere, such as on the Philippines, but we do not consider them in this study. 25 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 25–35, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied very recently. Zampieri et al. (2013) have addressed the task for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating"
W14-4204,W14-5307,0,0.311402,"Missing"
W14-4204,W14-1303,0,0.0629176,", 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coordinates -120◦ , -55◦ and -29◦ , 30◦ (roughly delimiting Latin America), as well as -10◦ , 35◦ and 3◦ , 46◦ (roughly delimiting Spain). The download ran from July 2 to July 4, 2014. In a second step, we sorted the tweets according to the respective countries. Twitter is not used to the same extent in all countries where Spanish is spoken. In the time 2 https://dev.twitter.com/docs/api/ stream"
W14-4204,U13-1003,0,0.380598,"word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). reaches an overall F-score of 67.72 on the fiveclass problem. On the two-class problem, human classification is outperformed by a large margin. The remainder of t"
W14-4204,vatanen-etal-2010-language,0,0.335234,"entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, ˇ uˇrek and Kolkus (2009), who treat “noisy” and Reh˚ web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesti"
W14-6101,D12-1133,0,0.0126291,"rser effectively performs the POS disambiguation. On these grounds, they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). Since the two treeban"
W14-6101,W07-1506,0,0.0291559,"reprocessing, we follow the standard practices from the parsing community. In both treebanks, punctuation and other material, such as parentheses, are not included in the annotation, but attached to a virtual root node. We attach the respective nodes to the tree using the algorithm described by Maier et al. (2012) so that every sentence corresponds to exactly one tree. In a nutshell, this algorithm uses the left and right terminal neighbors as attachment targets. In TiGer, we then remove the crossing branches using a two-stage process. In a first step, we apply the transformation described by Boyd (2007). This transformation introduces a new non-terminal for every continuous block of a discontinuous constituent. We keep a flag on each of the newly introduced nodes that indicates if it dominates the head daughter of the original discontinuous node. Subsequently, we delete all those nodes for which this flag is false.2 For both POS tagging and parsing, we use the same split for training, development, and test. We use the first half of the last 10 000 sentences in TiGer for development and the second half for testing. The remaining 40 472 sentences are used for training. Accordingly, in order to"
W14-6101,A00-1031,0,0.163514,"extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 1–14 Dublin, Ireland, August 23-29 201"
W14-6101,W10-1409,0,0.0220527,"sque, French, German, Hebrew, and Hungarian. However, note that Sz´ant´o and Farkas (2014) used the data from the SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotations. Both approaches found improvements for subsets of morphological features. Other works examine, also within a “pipeline” method, possibilities for ambiguity reduction through modification of tagsets, or of the lexicon by tagset reduction, or through word-clustering. Lakeland (2005) uses lexicalized parsing a` la Collins (1999). Similarly to the more recent work by Koo et al. (2008) or Candito and Seddah (2010), he addresses the question of how to optimally disambiguate for parsing on the lexical level by clustering. A word cluster is thereby seen as an equivalence class of words and assumes to a certain extent the function of a POS tag, but can be adapted to the training data. Le Roux et al. (2012) address the issue of data sparseness on the lexical level with PCFG parsing with the morphologically rich language Spanish. The authors use a reimplementation of the Berkeley parser. They show that parsing results can be improved by simplifying the POS tagset, as well as by lemmatization, since both appr"
W14-6101,I11-1141,0,0.0197709,"hing for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM description pronoun determiner, article preposition, postposition numeral tag CONJ PRT . X description conjunction particle punctuation everything else Table 1: The 12 tags of the Universal Tagset. hierarchical, features are decisive for the quality of POS tagging and note that a “pipeline” model does not take this into account since the parser effectively performs the POS disambiguation. On these grounds, they pr"
W14-6101,chrupala-etal-2008-learning,0,0.0307403,"Missing"
W14-6101,P99-1065,0,0.245797,"Missing"
W14-6101,E03-1052,0,0.0163896,"iscriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). Since the two treebanks differ in their morphological annotation, in this variant, the tags differ between the two treebanks: For TiGer, we have 783 possible complex"
W14-6101,W11-3802,0,0.0565312,"Missing"
W14-6101,gimenez-marquez-2004-svmtool,0,0.0798508,"Missing"
W14-6101,P08-1043,0,0.02008,"del does not take this into account since the parser effectively performs the POS disambiguation. On these grounds, they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes morphological information from the t"
W14-6101,I11-1136,0,0.0223608,"ng and note that a “pipeline” model does not take this into account since the parser effectively performs the POS disambiguation. On these grounds, they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also inclu"
W14-6101,P08-1068,0,0.0413383,"ic, the latter for Basque, French, German, Hebrew, and Hungarian. However, note that Sz´ant´o and Farkas (2014) used the data from the SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotations. Both approaches found improvements for subsets of morphological features. Other works examine, also within a “pipeline” method, possibilities for ambiguity reduction through modification of tagsets, or of the lexicon by tagset reduction, or through word-clustering. Lakeland (2005) uses lexicalized parsing a` la Collins (1999). Similarly to the more recent work by Koo et al. (2008) or Candito and Seddah (2010), he addresses the question of how to optimally disambiguate for parsing on the lexical level by clustering. A word cluster is thereby seen as an equivalence class of words and assumes to a certain extent the function of a POS tag, but can be adapted to the training data. Le Roux et al. (2012) address the issue of data sparseness on the lexical level with PCFG parsing with the morphologically rich language Spanish. The authors use a reimplementation of the Berkeley parser. They show that parsing results can be improved by simplifying the POS tagset, as well as by l"
W14-6101,W06-1614,1,0.869625,"Missing"
W14-6101,P10-1052,0,0.0507193,"Missing"
W14-6101,W12-3408,0,0.0366199,"Missing"
W14-6101,W12-4615,1,0.850612,"nsiderably in the syntactic annotation scheme. While TiGer uses a very flat annotation involving crossing branches, the annotations in T¨uBa-D/Z are more hierarchical, and long distance relations are modeled via grammatical function labels rather than via attachment. Figures 1 and 2 show examples. For preprocessing, we follow the standard practices from the parsing community. In both treebanks, punctuation and other material, such as parentheses, are not included in the annotation, but attached to a virtual root node. We attach the respective nodes to the tree using the algorithm described by Maier et al. (2012) so that every sentence corresponds to exactly one tree. In a nutshell, this algorithm uses the left and right terminal neighbors as attachment targets. In TiGer, we then remove the crossing branches using a two-stage process. In a first step, we apply the transformation described by Boyd (2007). This transformation introduces a new non-terminal for every continuous block of a discontinuous constituent. We keep a flag on each of the newly introduced nodes that indicates if it dominates the head daughter of the original discontinuous node. Subsequently, we delete all those nodes for which this"
W14-6101,J13-1008,0,0.123556,"been done on investigating different tagsets for individual languages. Collins et al. (1999) adapt the parser of Collins (1999) for the Czech Prague Dependency Treebank. Using an external lexicon to reduce data sparseness for word forms did not result in any improvement, but adding case to the POS tagset had a positive effect. Seddah et al. (2009) investigate the use of different parsers on French. They also investigate two tagsets with different granularity and come to the conclusion that the finer grained tagset leads to higher parser performance. The work that is closest to ours is work by Marton et al. (2013), who investigate the optimal POS tagset for parsing Arabic. They come to the conclusion that adding definiteness, person, number, gender, and lemma information to the POS tagset improve parsing accuracy. Both Dehdari et al. (2011) and Sz´ant´o and Farkas (2014) investigate automatic methods for selecting the best subset of morphological features, the former for Arabic, the latter for Basque, French, German, Hebrew, and Hungarian. However, note that Sz´ant´o and Farkas (2014) used the data from the SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotation"
W14-6101,N07-1051,0,0.212026,"cal variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 1–14 Dublin, Ireland, August 23-29 2014. Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on unknown words while TnT reaches the best performance for STTS and the RF-Tagger fo"
W14-6101,W08-1005,0,0.0214698,"d distinctions on POS level decrease parsing performance. 1 Introduction German is a non-configurational language with a moderately free word order in combination with a case system. The case of a noun phrase complement generally is a direct indicator of the phrase’s grammatical function. For this reason, a morphological analysis seems to be a prerequisite for a syntactic analysis. However, in computational linguistics, parsing was developed for English without the use of morphological information, and this same architecture is used for other languages, including German (K¨ubler et al., 2006; Petrov and Klein, 2008). An easy way of introducing morphological information into parsing, without modifying the architecture, is to attach morphology to the part-of-speech (POS) tagset. However, this makes POS tagging more complex and thus more difficult. In this paper, we investigate the following questions: 1) How well do the different POS taggers work with tagsets of a varying level of morphological granularity? 2) Do the differences in POS tagger performance translate into similar differences in parsing quality? Complementary POS tagging results and preliminary parsing results have been published in German in"
W14-6101,petrov-etal-2012-universal,0,0.165752,"ological granularity? 2) Do the differences in POS tagger performance translate into similar differences in parsing quality? Complementary POS tagging results and preliminary parsing results have been published in German in K¨ubler and Maier (2013). Our experiments are based on two different treebanks for German, TiGer (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2012). Both treebanks are based on the same POS tagset, the StuttgartT¨ubingen Tagset (STTS) (Schiller et al., 1995). We perform experiments with three variants of the tagset: The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is"
W14-6101,W96-0213,0,0.0617186,". (2008) overcome this deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM description pronoun determiner, article preposition, postposition numeral tag CONJ PRT . X description conjunction particle punctuation everything else Table 1: The 12 tags of the Universal Tagset. hierarchical, features are decisive for the quality of POS tagging and note that a “pipeline” model does not take this into account since the parser effectively perf"
W14-6101,C08-1098,0,0.0165306,"et, the StuttgartT¨ubingen Tagset (STTS) (Schiller et al., 1995). We perform experiments with three variants of the tagset: The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License detail"
W14-6101,W09-3824,0,0.0671138,"Missing"
W14-6101,J13-1004,0,0.0149644,"be seen as an equivalence class of words. Since in the “pipeline” approach, the parse tree is built on POS tags, it is possible that a POS tagset is optimal from a linguistic point of view, but that its behavior is not optimal with respect to parsing results, because relevant lexical information is hidden from the parse tree by the POS tagset. While Koo et al. (2008) overcome this deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM descri"
W14-6101,E14-1015,0,0.0302323,"Missing"
W14-6101,W00-1308,0,0.071398,"deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM description pronoun determiner, article preposition, postposition numeral tag CONJ PRT . X description conjunction particle punctuation everything else Table 1: The 12 tags of the Universal Tagset. hierarchical, features are decisive for the quality of POS tagging and note that a “pipeline” model does not take this into account since the parser effectively performs the POS disambiguation. On t"
W14-6101,N03-1033,0,0.00776892,"ith three variants of the tagset: The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 First Joint Workshop on Statistical Parsin"
W14-6101,W09-3820,0,0.0250328,"dy mentioned, a POS tag can be seen as an equivalence class of words. Since in the “pipeline” approach, the parse tree is built on POS tags, it is possible that a POS tagset is optimal from a linguistic point of view, but that its behavior is not optimal with respect to parsing results, because relevant lexical information is hidden from the parse tree by the POS tagset. While Koo et al. (2008) overcome this deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag"
W14-6101,J03-4003,0,\N,Missing
W16-0906,E14-1039,0,0.0605887,"Missing"
W16-0906,P11-2037,0,0.0213466,"seen in Fig. 1, which shows the treebank annotation of (1). The connection between Dar¨uber and its reference participle nachgedacht is made by grouping both words under a single VP node. Parsing discontinuous constituents is a challenge, since approaches that produce context-free derivations cannot be used. For treebanks in which discontinuities are represented by traces, various approaches have been presented, mostly based on the extension of a CFG parser with a pre-, post- or in-processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings of DiscoNLP 2016, pages 47–57, c San Diego, California, June 17, 2016. 2016 Association"
W16-0906,W03-1005,0,0.0614596,", as it is done in the German TIGER and NeGra treebanks. This can be seen in Fig. 1, which shows the treebank annotation of (1). The connection between Dar¨uber and its reference participle nachgedacht is made by grouping both words under a single VP node. Parsing discontinuous constituents is a challenge, since approaches that produce context-free derivations cannot be used. For treebanks in which discontinuities are represented by traces, various approaches have been presented, mostly based on the extension of a CFG parser with a pre-, post- or in-processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings of DiscoNLP 2016, pages"
W16-0906,D12-1095,0,0.0230389,"/POS pairs remaining on the queue (”full queue features”). Furthermore, we adapt swap importance weighting from Maier (2015). During training, all updates that concern a swap transition are counted twice. We do the same for S KIP S HIFT-i. 4 Experiments We implement the tree transformations with the corresponding oracles, as well as the S KIP S HIFT-i operation within uparse, the publicly available implementation of the parser of Maier (2015).4 4.1 Data and Setup In order to facilitate a comparison, we use the same data as Maier (2015), we use the TIGER treebank release 2.2 with the splits of Farkas and Schmid (2012) and Hall and Nivre (2008). In the former, the first half of the last 10,000 sentences are used for development and the second half for testing, and the rest is left for training. In the latter, the treebank is split into ten parts, such that sentence i is put into part i mod 10. The first of those parts is used for testing, the concatenation of the rest for training. As usual, 4 we attach the material which is not included in the annotation (mainly punctuation) to the tree itself. We run the training for 20 iterations using beam size 4. Other parameters are indicated later. The results are re"
W16-0906,P15-1147,0,0.155311,"Missing"
W16-0906,N10-1115,0,0.0364296,"alez and Martins (2015). Recently, Versley (2014) and Maier (2015) have exploited a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): One can convert every non-projective dependency tree into a projective one by reordering its words. Non-projective dependency parsing can therefore be cast as projective dependency parsing with an additional online reordering operation which allows for the input to be processed out-of-order (“swap”). The same holds for the parsing of discontinuous constituency trees. While Versley (2014) adapts the “easy-first” strategy of Goldberg and Elhadad (2010) to work with a swap operation, Maier (2015) extends the shift-reduce approach of Zhu et al. (2013) correspondingly. Note that the idea of processing linear precedence and immediate dominance for discontinuous parsing separately has also been explored in a grammar-based context by Nederhof and Vogler (2014). In this paper, we build on the work of Maier (2015) and make two contributions. Firstly, we introduce a new parser transition S KIP S HIFT-i which in comparison to the swap operation reduces the amount of decisions required to be taken in order to produce a discontinuous constituent and th"
W16-0906,N12-1015,0,0.108594,"Missing"
W16-0906,P02-1018,0,0.0681434,"er a single node, as it is done in the German TIGER and NeGra treebanks. This can be seen in Fig. 1, which shows the treebank annotation of (1). The connection between Dar¨uber and its reference participle nachgedacht is made by grouping both words under a single VP node. Parsing discontinuous constituents is a challenge, since approaches that produce context-free derivations cannot be used. For treebanks in which discontinuities are represented by traces, various approaches have been presented, mostly based on the extension of a CFG parser with a pre-, post- or in-processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings"
W16-0906,J13-1006,1,0.735165,"presented, mostly based on the extension of a CFG parser with a pre-, post- or in-processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings of DiscoNLP 2016, pages 47–57, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics or Cohen and Gildea (2015). Even with advanced approaches such as the latter, the high parsing complexity with such approaches is a major bottleneck which tends to lead to low parsing speeds. Another approach consists of creating a reversible conversion of discontinuous constituents to dependencies, and to parse those with an appropriate dependency parser. This very successful app"
W16-0906,P04-1042,0,0.054247,"rman TIGER and NeGra treebanks. This can be seen in Fig. 1, which shows the treebank annotation of (1). The connection between Dar¨uber and its reference participle nachgedacht is made by grouping both words under a single VP node. Parsing discontinuous constituents is a challenge, since approaches that produce context-free derivations cannot be used. For treebanks in which discontinuities are represented by traces, various approaches have been presented, mostly based on the extension of a CFG parser with a pre-, post- or in-processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings of DiscoNLP 2016, pages 47–57, c San Diego, Calif"
W16-0906,maier-etal-2014-discosuite,1,0.898482,"Missing"
W16-0906,P15-1116,1,0.145893,"16, pages 47–57, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics or Cohen and Gildea (2015). Even with advanced approaches such as the latter, the high parsing complexity with such approaches is a major bottleneck which tends to lead to low parsing speeds. Another approach consists of creating a reversible conversion of discontinuous constituents to dependencies, and to parse those with an appropriate dependency parser. This very successful approach is taken by Hall and Nivre (2008) and Fern´andezGonz´alez and Martins (2015). Recently, Versley (2014) and Maier (2015) have exploited a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): One can convert every non-projective dependency tree into a projective one by reordering its words. Non-projective dependency parsing can therefore be cast as projective dependency parsing with an additional online reordering operation which allows for the input to be processed out-of-order (“swap”). The same holds for the parsing of discontinuous constituency trees. While Versley (2014) adapts the “easy-first” strategy of Goldberg and Elhadad (2010) to work with a swap operation, Maier ("
W16-0906,N15-1108,0,0.0850197,"Missing"
W16-0906,C14-1130,0,0.454408,"processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings of DiscoNLP 2016, pages 47–57, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics or Cohen and Gildea (2015). Even with advanced approaches such as the latter, the high parsing complexity with such approaches is a major bottleneck which tends to lead to low parsing speeds. Another approach consists of creating a reversible conversion of discontinuous constituents to dependencies, and to parse those with an appropriate dependency parser. This very successful approach is taken by Hall and Nivre (2008) and Fern´andezGonz´alez and Martins (2015)"
W16-0906,W09-3811,0,0.244973,"stics or Cohen and Gildea (2015). Even with advanced approaches such as the latter, the high parsing complexity with such approaches is a major bottleneck which tends to lead to low parsing speeds. Another approach consists of creating a reversible conversion of discontinuous constituents to dependencies, and to parse those with an appropriate dependency parser. This very successful approach is taken by Hall and Nivre (2008) and Fern´andezGonz´alez and Martins (2015). Recently, Versley (2014) and Maier (2015) have exploited a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): One can convert every non-projective dependency tree into a projective one by reordering its words. Non-projective dependency parsing can therefore be cast as projective dependency parsing with an additional online reordering operation which allows for the input to be processed out-of-order (“swap”). The same holds for the parsing of discontinuous constituency trees. While Versley (2014) adapts the “easy-first” strategy of Goldberg and Elhadad (2010) to work with a swap operation, Maier (2015) extends the shift-reduce approach of Zhu et al. (2013) correspondingly. Note that the idea of proce"
W16-0906,P09-1040,0,0.626414,"tional Linguistics or Cohen and Gildea (2015). Even with advanced approaches such as the latter, the high parsing complexity with such approaches is a major bottleneck which tends to lead to low parsing speeds. Another approach consists of creating a reversible conversion of discontinuous constituents to dependencies, and to parse those with an appropriate dependency parser. This very successful approach is taken by Hall and Nivre (2008) and Fern´andezGonz´alez and Martins (2015). Recently, Versley (2014) and Maier (2015) have exploited a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): One can convert every non-projective dependency tree into a projective one by reordering its words. Non-projective dependency parsing can therefore be cast as projective dependency parsing with an additional online reordering operation which allows for the input to be processed out-of-order (“swap”). The same holds for the parsing of discontinuous constituency trees. While Versley (2014) adapts the “easy-first” strategy of Goldberg and Elhadad (2010) to work with a swap operation, Maier (2015) extends the shift-reduce approach of Zhu et al. (2013) correspondingly. Note t"
W16-0906,P06-1023,0,0.0312172,"banks. This can be seen in Fig. 1, which shows the treebank annotation of (1). The connection between Dar¨uber and its reference participle nachgedacht is made by grouping both words under a single VP node. Parsing discontinuous constituents is a challenge, since approaches that produce context-free derivations cannot be used. For treebanks in which discontinuities are represented by traces, various approaches have been presented, mostly based on the extension of a CFG parser with a pre-, post- or in-processing step. See, e.g., Johnson (2002), Dienes and Dubey (2003), Levy and Manning (2004), Schmid (2006), and Cai et al. (2011). For the direct parsing of discontinuous constituents, grammarbased techniques have been used, mostly on the basis of Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is an extension of Context-Free Grammar in which a single non-terminal can span k ≥ 1 continuous parts of the input string; i.e., CFG is a special case of LCFRS in which k = 1. See, e.g., Kallmeyer and Maier (2013), van Cranenburgh (2012), Angelov and Ljungl¨of (2014), Nederhof and Vogler (2014), 47 Proceedings of DiscoNLP 2016, pages 47–57, c San Diego, California, June 17,"
W16-0906,J10-1001,0,0.0329821,"m (compared to our result of 19.36/39.71/26.03) (unfortunately, Fern´andez-Gonz´alez and Martins (2015) have not reported results on discontinuous constituents alone). In future work, we will explore possibilities of integrating such a mechanism in a shiftreduce approach. We have seen that choosing the transition order well, i.e., picking the right oracle, is crucial for parsing success. We therefore want to explore how the order of transitions affects parsing results in the continuous case. A particular transition order could be forced via a tree transformation such as rightcorner transform (Schuler et al., 2010). Concretely, right-corner transform would give preference to easier transition orders in which we reduce as soon as possible, i.e., long sequences of S HIFTs would be avoided. Last, it should be noted that the application of S KIP S HIFT-i is not limited to discontinuous constituency parsing. We want to apply our method to non-projective dependency parsing, where S KIP S HIFT-i could be used instead of swapeager/lazy transitions (Nivre et al., 2009) in a parsing framework such as the one of Zhang and Nivre (2011). This would also allow for an ”intersection” between our work and then one of Fe"
W16-0906,P15-1148,0,0.0210078,"ance, in order to correctly recognize an NP with an extraposed modifier, the reduction of the full NP must be delayed until the complete modifier has been recognized. With the current parsing model, in some situations, there is just no way of knowing if a delay is necessary, since the modifier can be still out of reach for the feature function when the first part of the full NP has already been recognized. Due to beam search, once the NP is reduced, we cannot backtrack, i.e., the modifier cannot be attached later. One way of addressing this problem could be the use of exact search, such as in Thang et al. (2015). However, there is another perspective. An important finding of the experiments is that recognizing material in gaps before the discontinuous constituent itself (R IGHT D) leads to high precision and low recall on discontinuous constituents, while recognizing the discontinuous constituent first (L EFT) leads to more errors, but also catches more cases (lower precision, higher recall). The reason for this is that in the former case, one decides too late and in the latter case too early if a partially recognized constituent is part of a discontinuous structure. We conjecture that what makes the"
W16-0906,W13-5701,0,0.3816,"Missing"
W16-0906,E12-1047,0,0.0942598,"Missing"
W16-0906,W14-6104,0,0.62825,"ings of DiscoNLP 2016, pages 47–57, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics or Cohen and Gildea (2015). Even with advanced approaches such as the latter, the high parsing complexity with such approaches is a major bottleneck which tends to lead to low parsing speeds. Another approach consists of creating a reversible conversion of discontinuous constituents to dependencies, and to parse those with an appropriate dependency parser. This very successful approach is taken by Hall and Nivre (2008) and Fern´andezGonz´alez and Martins (2015). Recently, Versley (2014) and Maier (2015) have exploited a strategy known from non-projective dependency parsing (Nivre, 2009; Nivre et al., 2009): One can convert every non-projective dependency tree into a projective one by reordering its words. Non-projective dependency parsing can therefore be cast as projective dependency parsing with an additional online reordering operation which allows for the input to be processed out-of-order (“swap”). The same holds for the parsing of discontinuous constituency trees. While Versley (2014) adapts the “easy-first” strategy of Goldberg and Elhadad (2010) to work with a swap o"
W16-0906,W09-3825,0,0.558357,"ue, starting with the second stack element.3 • F INISH pops the last remaining element from the stack, given that it is labeled with the root label and the queue is empty. • I DLE can be applied any number of times after F INISH. This compensates for different lengths of analyses (Zhu et al., 2013). A parser state to which F INISH has been applied is a final state. Transitions can only be applied on states which fulfill certain conditions. For instance, S HIFT can only be applied if there are elements left on the queue. The full set of the corresponding conditions is listed in the appendix of Zhang and Clark (2009), and in Maier (2015) (for S WAP). 2.2 Oracles An oracle is used to obtain canonical transition sequences from gold treebank trees. These sequences can then be learned by the parser. Since we only use transitions that handle unary and binary nodes, incoming trees must be binarized. As in previous work, we use head-outward binarization with binary top and bottom productions, and a single binarization label @X for all X constituents. For details on the binarization, see Maier (2015) and references therein. For continuous parsing, i.e., when no discontinuous trees have to be handled such as in Zh"
W16-0906,P11-2033,0,0.0300728,"on order could be forced via a tree transformation such as rightcorner transform (Schuler et al., 2010). Concretely, right-corner transform would give preference to easier transition orders in which we reduce as soon as possible, i.e., long sequences of S HIFTs would be avoided. Last, it should be noted that the application of S KIP S HIFT-i is not limited to discontinuous constituency parsing. We want to apply our method to non-projective dependency parsing, where S KIP S HIFT-i could be used instead of swapeager/lazy transitions (Nivre et al., 2009) in a parsing framework such as the one of Zhang and Nivre (2011). This would also allow for an ”intersection” between our work and then one of Fern´andezGonz´alez and Martins (2015). 5 Conclusion We have presented a new tree reordering method which makes discontinuous constituency trees continuous. The reordering method can be used to obtain oracles for discontinuous shift-reduce parsing. In conjunction with a new parser transition, we have achieved state-of-the-art results for discontinuous shift-reduce constituency parsing. Appendix A. Feature Templates Our parser uses feature templates from previous work. For the sake of completeness, we list them here."
W16-0906,P13-1043,0,0.341535,"rojective dependency parsing (Nivre, 2009; Nivre et al., 2009): One can convert every non-projective dependency tree into a projective one by reordering its words. Non-projective dependency parsing can therefore be cast as projective dependency parsing with an additional online reordering operation which allows for the input to be processed out-of-order (“swap”). The same holds for the parsing of discontinuous constituency trees. While Versley (2014) adapts the “easy-first” strategy of Goldberg and Elhadad (2010) to work with a swap operation, Maier (2015) extends the shift-reduce approach of Zhu et al. (2013) correspondingly. Note that the idea of processing linear precedence and immediate dominance for discontinuous parsing separately has also been explored in a grammar-based context by Nederhof and Vogler (2014). In this paper, we build on the work of Maier (2015) and make two contributions. Firstly, we introduce a new parser transition S KIP S HIFT-i which in comparison to the swap operation reduces the amount of decisions required to be taken in order to produce a discontinuous constituent and therefore leads to fewer errors. Secondly, we address the problem that when processing the input term"
W16-5808,W15-3206,0,0.105,"ut requiring a local installation of software; • On server side, there should be safe storage; furthermore, the administration overhead should be kept minimal and there should only be minimal software requirements for the server side. 1 http://getbootstrap.com 65 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 65–70, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics Even though several annotation interfaces for similar tasks have been presented, such as COLANN (Benajiba and Diab, 2010), COLABA (Diab et al., 2010), and DIWAN (Al-Shargi and Rambow, 2015), they were either not available or did not match our needs. We therefore built SAWT. SAWT has been successfully used to create a code-switched corpus of 223k tokens with three annotators (Samih and Maier, 2016). It is currently used for Part-of-Speech annotation of Moroccan Arabic dialect data. The remainder of the article is structured as follows. In section 2 we present the different aspects of SWAT, namely, its data storage model, its server side structure and its client side structure. In section 3, we review related work, and in section 4, we conclude the article. 2 SAWT SAWT is a web ap"
W16-5808,P02-1022,0,0.101638,"ws for a very high speed, since only one click per token is required. 4 Figure 1: Screenshot of SAWT: Annotation on Android device shows a screenshot of code-switching annotation done in the context of our earlier work (Samih and Maier, 2016). Finally, figure 1 shows a screenshot of the POS annotation interface used on the Asus Nexus 7 2013 tablet running Google Chrome on Android 6. 3 Related Work As mentioned above, we are not aware of a software which would have fulfilled our needs exactly. Previously released annotation software can be grouped into several categories. Systems such as GATE (Cunningham et al., 2002), CLaRK (Simov et al., 2003) and MMAX2 (M¨uller and Strube, 2006) are desktop-based software. They offer a large range of functions, and are in general oriented towards more complex annotation tasks, such as syntactic treebank annotation. In the context of Arabic dialect annotation, several systems have been created. COLANN GUI (Benajiba and Diab, 2010), which unfortunately was not available to us, is a web application that specialized on dialect annotation. DIWAN (Al-Shargi and Rambow, 2015) is a desktop application for dialect annotation which can be used online. The systems that came closes"
W16-5808,pasha-etal-2014-madamira,0,0.045052,"Missing"
W16-5808,petrov-etal-2012-universal,0,0.149276,"model runs on the web server. If suggestions are desired, then in the configuration file, the corresponding path and parameters for the software must be given. If the parameter is left blank, no suggestions are shown. • Search box activation: A boolean parameter indicating if a search box is desired. In the search box, the annotator can look up his previous annotations for a certain token. • Utility links: The top border of the user interface consists of a link bar, the links for which can be freely configured. In our project, e.g., they are used for linking to the list of Universal POS tags (Petrov et al., 2012), to a list of Arabic function words, to an Arabic Morphological Analyzer (MADAMIRA) (Pasha et al., 2014), and to an Arabic screen keyboard, as can be seen in figures 2 and 3. Once the configuration script has been run, the web application code must be copied to a suitable place within a web server installation. In order to upload a text which is to be annotated by a certain user, the second script must be used. It takes the following command line parameters. • Input data: The name of the file containing the data to be annotated. The text must be pretokenized (one space between each token), an"
W16-5808,L16-1658,1,0.930223,"it is a common phenomenon. Both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) variants co-exist, MSA and DA being used for formal and informal communication, respectively (Ferguson, 1959). Particularly recently, the computational treatment of code-switching has received attention (Solorio et al., 2014). Within a project concerned with the processing of code-switched data of an under-resourced Arabic dialect, Moroccan Darija, a large code-switched corpus had to be annotated token-wise with the extended label set from the EMNLP 2014 Shared Task on Code-Switching (Solorio et al., 2014; Samih and Maier, 2016). The label set contains three labels that mark MSA and DA tokens, as well as tokens in another language (English, French, Spanish, Berber). Furthermore, it contains labels for tokens which mix two languages (e.g., for French words to • It should not be bound to a particular label set, since within the project, not only codeswitching annotation, but also the annotation of Part-of-Speech was envisaged; • It should allow for post-editing of tokenization during the annotation; • It should be web-based, due to the annotators being at different physical locations; • On client side, it should be pla"
W16-5808,W14-3907,0,0.0135575,"ided, furthermore it should be as simple as possible to use for the annotators, allowing for a high annotation speed; Introduction Code-switching (Bullock and Toribio, 2009) occurs when speakers switch between different languages or language variants within the same context. In the Arab world, for instance, it is a common phenomenon. Both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) variants co-exist, MSA and DA being used for formal and informal communication, respectively (Ferguson, 1959). Particularly recently, the computational treatment of code-switching has received attention (Solorio et al., 2014). Within a project concerned with the processing of code-switched data of an under-resourced Arabic dialect, Moroccan Darija, a large code-switched corpus had to be annotated token-wise with the extended label set from the EMNLP 2014 Shared Task on Code-Switching (Solorio et al., 2014; Samih and Maier, 2016). The label set contains three labels that mark MSA and DA tokens, as well as tokens in another language (English, French, Spanish, Berber). Furthermore, it contains labels for tokens which mix two languages (e.g., for French words to • It should not be bound to a particular label set, sinc"
W16-5808,E12-2021,0,0.124532,"Missing"
W16-5808,P13-4001,0,0.27822,"Missing"
W17-5517,P13-1025,0,0.100358,"Missing"
W17-5517,W01-1607,0,0.107108,"ranigan and Pearson, 2006; Branigan et al., 2010; Bell, 2003) and researchers have many intuitions about the differences between humanmachine and human-human communication, interpersonal speaking style is often taken as a baseline for naturalness as can be seen from the discussed literature and it has not been examined to which extent the criteria mentioned for naturalness characterize naturally spoken utterances towards stateof-the-art SDS. There exist only a few empirical studies which investigate the differences. These research works focus either on dialog issues such as turn-taking, e.g. (Doran et al., 2001), or on lexical alignment, e.g. (Branigan et al., 2011), but not on natural language input towards SDS in a car environment. The way people address the system is not only influenced by their beliefs about the system but also by individual properties such as age or gender. Work in this area of research has been done by Bell (2003) who found that individual differences in speaker behavior are significant and by M¨oller (2008) who found that younger users differ from older users in the way they speak with a smart-home system. The observations show that different user groups may have a different u"
W17-5517,L16-1119,1,0.336118,"s expected, that systems answer naturally to the user. However, a discussion of system output is beyond the scope of this paper. 137 Proceedings of the SIGDIAL 2017 Conference, pages 137–146, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics a question indicator, such as what and do, and ending with a question mark. our study design. Section 4 presents the evaluation of the study, in section 5 the results are discussed and section 6 concludes the article. 2 The second group consists of literature which (linguistically) analyzes spoken user input style. Braunger et al. (2016), e.g., compare crowdsourced natural language user input in terms of sentence constructions. They conclude that if people speak freely to an SDS, they mostly use an imperative style. Winter et al. (2010) collect naturally spoken utterances and quantify their complexity and variety. They use context information as a qualitative measurement for classification, classifying the utterance content into three categories: information data, context relevant words and non-context relevant words. They find that users tend to repeat similar utterance patterns composed from a limited set of different words"
W17-5517,moller-etal-2008-corpus,0,0.579151,"Missing"
W17-5517,pak-paroubek-2010-twitter,0,0.0104685,"Missing"
W17-5517,P11-2024,0,0.0275251,"e builds two natural language models, one based on a corpus representing classic formal language and one based on a corpus representing a more colloquial web language. For measuring the similarity to a natural language model he used perplexity. He concludes that voice queries are still far from natural language questions. The authors of (Hayakawa et al., 2016) compared direct human-human dialogs to dialogs that are mediated by a speech-to-speech machine translation system. They found that in machine mediated conversation speakers use less words than in direct human-to-human communication. In (Pang and Kumar, 2011) written natural language questions posed as web search queries are compared to a natural language sample of questions posted by web users on a communitybased question-answering site. Since written text tends to be structurally complete Pang et al. (2011) measure naturalness by means of the probability mass of function words. A more intuitive and natural interaction with SDSs presupposes understanding naturally spoken user utterances. In order to choose an appropriate system design which allows the system to deal with naturally spoken utterances, a definition of what exactly constitutes natura"
W17-5517,N16-1081,0,0.105853,"Missing"
W17-5517,W16-4317,0,0.0334665,"Missing"
W17-5517,petrov-etal-2012-universal,0,\N,Missing
W18-3212,W18-3219,0,0.0485394,"Missing"
W18-3212,attia-etal-2010-automatically,1,0.861285,"Missing"
W18-3212,W03-0420,0,0.159291,"Missing"
W18-3212,J92-4003,0,0.601548,"le representation of the input words by using word embeddings and a character-based representation (with CNNs). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. The model uses the following. Word-level embeddings allow the learning algorithms to use large unlabeled data to generalize beyond the seen training data. We explore randomly initialized embeddings based on the seen training data and pre-trained embedding. Brown clusters (BC) Brown clustering is an unsupervised learning method where words are grouped based on the contexts in which they appear (Brown et al., 1992). The assumption is that words that behave in similar ways tend to appear in similar contexts and hence belong to the same cluster. BCs can be learned from large unlabeled texts and have been shown to improve POS tagging (Owoputi et al., 2013; Stratos and Collins, 2015). We test the effectiveness of using Brown clusters in the context of named entity recognition in a DNN model. We train BCs on our crawled code-switched corpus of 380 million words (mentioned above) with 100 Brown Clusters. 1 Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). Named Entity Gazetteer"
W18-3212,N13-1039,0,0.0608435,"Missing"
W18-3212,Q16-1026,0,0.0379796,".com Abstract den Markov Models (HMM) (Bikel et al., 1999), Maximum-Entropy Model (ME) (Bender et al., 2003; Curran and Clark, 2003; Finkel et al., 2005), Support Vector Machines (SVM) (Takeuchi and Collier, 2002), and Conditional Random Fields (CRF) (McCallum and Li, 2003). Standard data sets came from the English MUC-6 (Sundheim, 1995) and the multilingual CoNLL-02 (Tjong Kim Sang, 2002) and 03 (Tjong Kim Sang and De Meulder, 2003) shared tasks. More recent work relies on neural networks. A number of architecture variants have proven to be effective (Huang et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Reimers and Gurevych, 2017). What they have in common is that they use a bidirectional LSTM (bi-LSTM) over vector representations of the input words in order model their left and right contexts. On top of the bi-LSTM, they use a CRF layer to take the final tagging decisions. Other than a softmax layer which would treat tagging decisions independently, the CRF is able to model the linear dependencies between labels. This is essential for NER, where for instance, B-L OCATION cannot be followed by I-P ERSON. The architectures differ in their way of obtaining a vector represen"
W18-3212,D17-1035,0,0.0411367,"Missing"
W18-3212,W03-0424,0,0.250237,"Missing"
W18-3212,W15-3904,0,0.0437138,"Missing"
W18-3212,P05-1045,0,0.196077,"Missing"
W18-3212,W15-1511,0,0.0243402,"e learning algorithms to use large unlabeled data to generalize beyond the seen training data. We explore randomly initialized embeddings based on the seen training data and pre-trained embedding. Brown clusters (BC) Brown clustering is an unsupervised learning method where words are grouped based on the contexts in which they appear (Brown et al., 1992). The assumption is that words that behave in similar ways tend to appear in similar contexts and hence belong to the same cluster. BCs can be learned from large unlabeled texts and have been shown to improve POS tagging (Owoputi et al., 2013; Stratos and Collins, 2015). We test the effectiveness of using Brown clusters in the context of named entity recognition in a DNN model. We train BCs on our crawled code-switched corpus of 380 million words (mentioned above) with 100 Brown Clusters. 1 Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). Named Entity Gazetteers We use a large collec99 Layer Characters CNN Bi-LSTM Dropout Word Emb. Characters Emb. Clustering Emb. Gazetteer Emb. Hyper-Parameters window size number of filters state size dropout rate dimension dimension dimension dimension batch size Value 4 40 100 0.5 300 100 1"
W18-3212,M95-1002,0,0.333938,"Missing"
W18-3212,W02-2029,0,0.263726,"Missing"
W18-3212,N16-1030,0,0.0812034,"odeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). Bi-LSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. CRF is used jointly with bi-LSTMs to avoid the output label independence assumptions of bi-LSTMs and to impose sequence labeling constraints as in Lample et al. (2016). System Description We used a DNN model which is mainly suited for sequence tagging. It is a variant of the bi-LSTM-CRF architecture proposed by Ma and Hovy (2016); Lample et al. (2016); Huang et al. (2015).1 It combines a double representation of the input words by using word embeddings and a character-based representation (with CNNs). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. The model uses the following. Word-level embeddings allow the learning algorithms to use large unlabeled data to generalize beyond the seen training data. We explore ran"
W18-3212,W02-2024,0,0.35867,"Missing"
W18-3212,P16-1101,0,0.0653533,"iwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. CRF is used jointly with bi-LSTMs to avoid the output label independence assumptions of bi-LSTMs and to impose sequence labeling constraints as in Lample et al. (2016). System Description We used a DNN model which is mainly suited for sequence tagging. It is a variant of the bi-LSTM-CRF architecture proposed by Ma and Hovy (2016); Lample et al. (2016); Huang et al. (2015).1 It combines a double representation of the input words by using word embeddings and a character-based representation (with CNNs). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. The model uses the following. Word-level embeddings allow the learning algorithms to use large unlabeled data to generalize beyond the seen training data. We explore randomly initialized embeddings based on the seen training data and pre-trained embedding. Brown clusters (BC) Brown clustering is an unsupervised learning method wher"
W18-3212,W03-0430,0,0.336463,"Missing"
