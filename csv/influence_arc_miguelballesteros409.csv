2020.emnlp-main.375,N19-1334,1,0.531877,"component of human linguistic capabilities that has been so far untested in the neural setting. 1.1 Related Work Bayesian models of word learning have shown successes in acquiring proper syntactic generalizations from minimal exposure (Tenenbaum and Xu, 2000; Wang et al., 2017), however it is not clear how well neural network models would exhibit these rapid generalizations. Comparing between neural network architectures, recent work has shown that models enhanced with explicit structural supervision during training produce more humanlike syntactic generalizations (Kuncoro et al., 2017, 2018; Wilcox et al., 2019), but it remains untested whether such supervision helps learn properties of tokens that occur rarely during training. Previous studies have found that Artificial Neural Networks (ANNs) are capable of learning some argument structure paradigms and make correct predictions across multiple frames (Kann et al., 2018), however these capabilities remain untested for incremental language models. Much has been written about the ability of ANNs to learn number agreement (Linzen et al., 2016; Gulordava et al., 2018; Giulianelli et al., 2018), including their ability to maintain the dependency across di"
2020.emnlp-main.436,N19-1423,0,0.0239802,"6–20, 2020. 2020 Association for Computational Linguistics (or subword units for BERT-like models) i.e. {x0 , x1 , ..., xn−1 }, representing the input text. A subsequence spani is defined by starti , endi ∈ [0, n). Subsequences span1 and span2 represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2. First, the model embeds the input sequence into a vector representation using either static wang2vec representations (Ling et al., 2015), or contextualized representations from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or RoBERTa (Liu et al., 2019b). These embedded sequences are then optionally encoded with either LSTMs or Transformers. When BERT or RoBERTa is used to embed the input, we do not use any sequence encoders. The final sequence representation H[0,n) comprises of individual token representations i.e. {h0 , h1 , ..., hn−1 }. While the goal is to predict the temporal relation between span1 and span2 , the context around these two spans also has linguistic signals that connect the two arguments. To use this contextual information, we extract five constituent subsequences from the sequence represent"
2020.emnlp-main.436,P19-1433,0,0.38539,"facto standard for temporal ordering of events.1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Du"
2020.emnlp-main.436,P17-1085,0,0.0396269,"Missing"
2020.emnlp-main.436,Q18-1017,1,0.930198,"Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Durrett (2019). (3) A self-training method that incorporates the predictions of our model and learns from them; we test it jointly with the SMTL method. Our baseline model that uses RoBERTa (Liu et al., 2019b) already surpasses the state-of-the-art by 2 F1 points. Applying SMTL techniques affords further improvements with at least one of our auxiliary tasks. Finally, our self-training experiments, explore"
2020.emnlp-main.436,P14-1038,0,0.0658311,"Missing"
2020.emnlp-main.436,N15-1142,0,0.0314709,"Conference on Empirical Methods in Natural Language Processing, pages 5412–5417, c November 16–20, 2020. 2020 Association for Computational Linguistics (or subword units for BERT-like models) i.e. {x0 , x1 , ..., xn−1 }, representing the input text. A subsequence spani is defined by starti , endi ∈ [0, n). Subsequences span1 and span2 represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2. First, the model embeds the input sequence into a vector representation using either static wang2vec representations (Ling et al., 2015), or contextualized representations from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or RoBERTa (Liu et al., 2019b). These embedded sequences are then optionally encoded with either LSTMs or Transformers. When BERT or RoBERTa is used to embed the input, we do not use any sequence encoders. The final sequence representation H[0,n) comprises of individual token representations i.e. {h0 , h1 , ..., hn−1 }. While the goal is to predict the temporal relation between span1 and span2 , the context around these two spans also has linguistic signals that connect the two arguments. To use th"
2020.emnlp-main.436,W19-1917,0,0.108986,"al ordering of events.1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Durrett (2019). (3)"
2020.emnlp-main.436,2021.ccl-1.108,0,0.121946,"Missing"
2020.emnlp-main.436,P16-1105,0,0.0733547,"Missing"
2020.emnlp-main.436,W15-1506,0,0.0914746,"Missing"
2020.emnlp-main.436,D19-1642,0,0.395417,"It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Durrett (2019). (3) A self-training method th"
2020.emnlp-main.436,P18-1122,0,0.51359,"ts establish a new state-of-the-art on this task. 1 Introduction The task of temporal ordering of events involves predicting the temporal relation between a pair of input events in a span of text (Figure 1). This task is challenging as it requires deep understanding of temporal aspects of language and the amount of annotated data is scarce. Albright (e1, came) to the State Department to (e2, offer) condolences. Figure 1: Example from the MATRES dataset. The relation between (e1, came) and (e2, offer) is Before. Note that for the same span there may be other relation pairs. The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal ordering of events.1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering mod"
2020.emnlp-main.436,N18-1202,0,0.0143787,"ages 5412–5417, c November 16–20, 2020. 2020 Association for Computational Linguistics (or subword units for BERT-like models) i.e. {x0 , x1 , ..., xn−1 }, representing the input text. A subsequence spani is defined by starti , endi ∈ [0, n). Subsequences span1 and span2 represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2. First, the model embeds the input sequence into a vector representation using either static wang2vec representations (Ling et al., 2015), or contextualized representations from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or RoBERTa (Liu et al., 2019b). These embedded sequences are then optionally encoded with either LSTMs or Transformers. When BERT or RoBERTa is used to embed the input, we do not use any sequence encoders. The final sequence representation H[0,n) comprises of individual token representations i.e. {h0 , h1 , ..., hn−1 }. While the goal is to predict the temporal relation between span1 and span2 , the context around these two spans also has linguistic signals that connect the two arguments. To use this contextual information, we extract five constituent subsequences"
2020.emnlp-main.436,D19-1030,0,0.0433235,"Missing"
2020.emnlp-main.436,S13-2001,0,0.220663,"Missing"
2020.emnlp-main.436,N16-1174,0,0.0606923,"sequence before span1 i.e., H[0,start1 ) , (2) S2 , the subsequence corresponding to span1 i.e., H[start1 ,end1 ) , (3) S3 , the subsequence between span1 and span2 i.e, H[end1 ,start2 ) , (4) S4 , the subsequence corresponding to span2 i.e., H[start2 ,end2 ) and (5) S5 , the subsequence after span2 , i.e. H[end2 ,n) . Each of these subsequences Si has a variable number of tokens which are pooled to yield a fixed size representation si : si = pool(Si ) ∀i ∈ {1, ..., 5} (1) where pool is the result of concatenating the output of an attention mechanism (we use the word attention pooling method (Yang et al., 2016) for all tokens in a given span) and mean pooling. The final contextual pair representation c is formed by concatenating4 the five span representations si with a sequence representation r. For models with BERT and RoBERTa, r is the CLS and &lt;s&gt; token representation respectively while for other models r = pool(H[0,n) ). c = s1 s2 s3 s4 s5 r (2) This final contextual pair representation c is then projected with a fully connected layer followed by 4 is used to denote concatenation a softmax function to get a distribution over the output classes. The entire model is trained end-toend using the cros"
2020.emnlp-main.636,C18-1139,0,0.0449853,"Missing"
2020.emnlp-main.636,D19-1539,0,0.0210273,"(∼146M) CNN-DM (∼138M) Wiki+Books (∼192M) Mean F1 92.26±0.11 91.22±0.21 85.54±0.19 88.02±0.18 93.5 Results on CONLL-2012 dataset Model CVT BERTBase Pre-BERTBase APBERTBase BERT-MRC+DSC Unlabeled Data CNN-DM (∼18M) Wiki+Books (∼192M) CNN-DM (∼146M) CNN-DM (∼138M) Wiki+Books (∼192M) Mean F1 89.26±0.1 89.0±0.23 84.20±0.19 85.88±0.17 92.07 Table 3: Model performance for NER. The same unlabeled dataset is used for training CVT, Pre-BERTBase and APBERTBase , and Unlabeled Data indicates the approximate number of sentences seen by each model during training, until convergence criteria is met. Cloze (Baevski et al., 2019) and BERT-MRC+DSC (Li et al., 2019) are SOTA baselines for CONLL-2003 and CONLL-2012, respectively, for this task. Baevski et al. (2019) also use subsampled Common Crawl and News Crawl datasets but do not provide exact splits for these. Performance Metrics We report mean F1 (with standard deviation) on the labeled test splits for each task over 5 randomized runs, and compare the models using statistical significance tests over these runs. Further, we report the approximate number of unlabeled sentences seen by each model. Table 2 shows the results for the OTE detection task. Here, out of the 3"
2020.emnlp-main.636,D18-1217,0,0.237422,"hese are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxiliary modules, with restricted views of the unlabeled data, attempt to replicate the p"
2020.emnlp-main.636,N19-1423,0,0.186313,"ing tasks, with lesser financial and environmental impact. 1 Introduction Exploiting unlabeled data to improve performance has become the foundation for many natural language processing tasks. The question we investigate in this paper is how to effectively use unlabeled data: in a task-agnostic or a task-specific way. An example of the former is training models on language model (LM) like objectives on a large unlabeled corpus to learn general representations, as in ELMo (Embeddings from Language Models) (Peters et al., 2018) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT"
2020.emnlp-main.636,2020.acl-main.740,0,0.0365544,"Missing"
2020.emnlp-main.636,N16-1030,1,0.615518,"Missing"
2020.emnlp-main.636,P19-1524,0,0.0474345,"anguage model (LM) like objectives on a large unlabeled corpus to learn general representations, as in ELMo (Embeddings from Language Models) (Peters et al., 2018) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the prima"
2020.emnlp-main.636,2021.ccl-1.108,0,0.102216,"Missing"
2020.emnlp-main.636,W18-5711,0,0.0765426,"tasks. We find that the CVT-based approach using less unlabeled data achieves similar performance with BERT-based models, while being superior in terms of financial and environmental cost as well. 2 Background, Tasks and Datasets Before presenting the models and their training setups, we discuss the relevant literature and introduce the tasks and datasets used for our experiments. We focus on three tasks: opinion target expression (OTE) detection; named entity recognition (NER), and slot-labeling, each of which can be modeled as a sequence tagging problem (Xu et al., 2018; Liu et al., 2019a; Louvan and Magnini, 2018). The IOB sequence tagging scheme (Ramshaw and Marcus, 1999) is used for each of these tasks. Related Work. The usefulness of continued training of large transformer-based models on domain/task-related unlabeled data has been shown recently (Gururangan et al., 2020; Rietzler et al., 2019; Xu et al., 2019), with a varied use of terminology for the process. Xu et al. (2019) and Rietzler et al. (2019) show gains of further tuning BERT using in-domain unlabeled data and refer to this as Post-training, and LM finetuning, respectively. More recently, Gururangan et al. (2020) use the term Domain-Adap"
2020.emnlp-main.636,P16-1101,0,0.107731,"Missing"
2020.emnlp-main.636,P11-1015,0,0.0628885,"Missing"
2020.emnlp-main.636,N06-1020,0,0.0187733,"a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxiliary modules, with restricted views of the unlabeled data, attempt to replicate the primary module predictions. This helps to learn better representations for the task. We present an experimental study that investigates different task-agnostic and task-specific approaches to use unsupervised data and evaluates"
2020.emnlp-main.636,D14-1162,0,0.0925881,"Missing"
2020.emnlp-main.636,N18-1202,0,0.0237559,"ecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact. 1 Introduction Exploiting unlabeled data to improve performance has become the foundation for many natural language processing tasks. The question we investigate in this paper is how to effectively use unlabeled data: in a task-agnostic or a task-specific way. An example of the former is training models on language model (LM) like objectives on a large unlabeled corpus to learn general representations, as in ELMo (Embeddings from Language Models) (Peters et al., 2018) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmenta"
2020.emnlp-main.636,S16-1002,0,0.0496953,"Missing"
2020.emnlp-main.636,S14-2004,0,0.0914221,"Missing"
2020.emnlp-main.636,W12-4501,0,0.0498685,"Missing"
2020.emnlp-main.636,P19-1355,0,0.0551668,"BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxil"
2020.emnlp-main.636,N19-1242,0,0.0170219,"rature and introduce the tasks and datasets used for our experiments. We focus on three tasks: opinion target expression (OTE) detection; named entity recognition (NER), and slot-labeling, each of which can be modeled as a sequence tagging problem (Xu et al., 2018; Liu et al., 2019a; Louvan and Magnini, 2018). The IOB sequence tagging scheme (Ramshaw and Marcus, 1999) is used for each of these tasks. Related Work. The usefulness of continued training of large transformer-based models on domain/task-related unlabeled data has been shown recently (Gururangan et al., 2020; Rietzler et al., 2019; Xu et al., 2019), with a varied use of terminology for the process. Xu et al. (2019) and Rietzler et al. (2019) show gains of further tuning BERT using in-domain unlabeled data and refer to this as Post-training, and LM finetuning, respectively. More recently, Gururangan et al. (2020) use the term Domain-Adaptive Pretraining and show benefits over RoBERTa (Liu et al., 2019b). There have also been efforts to reduce model sizes for BERT, such as DistilBERT (Sanh et al., 2019), although these come at significant losses in performance. Opinion Target Expression (OTE) Detection: An integral component of fine-grain"
2020.emnlp-main.636,P18-2094,0,0.107342,"he labeled datasets, for the various tasks. We find that the CVT-based approach using less unlabeled data achieves similar performance with BERT-based models, while being superior in terms of financial and environmental cost as well. 2 Background, Tasks and Datasets Before presenting the models and their training setups, we discuss the relevant literature and introduce the tasks and datasets used for our experiments. We focus on three tasks: opinion target expression (OTE) detection; named entity recognition (NER), and slot-labeling, each of which can be modeled as a sequence tagging problem (Xu et al., 2018; Liu et al., 2019a; Louvan and Magnini, 2018). The IOB sequence tagging scheme (Ramshaw and Marcus, 1999) is used for each of these tasks. Related Work. The usefulness of continued training of large transformer-based models on domain/task-related unlabeled data has been shown recently (Gururangan et al., 2020; Rietzler et al., 2019; Xu et al., 2019), with a varied use of terminology for the process. Xu et al. (2019) and Rietzler et al. (2019) show gains of further tuning BERT using in-domain unlabeled data and refer to this as Post-training, and LM finetuning, respectively. More recently, Gur"
2020.emnlp-main.636,P95-1026,0,0.25807,"t the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxiliary modules, with restricted views of the unlabeled data, attempt to replicate the primary module predictions. This helps to learn better representations for the task. We present an experimental study that investigates different task-agnostic and task-specific approaches to use unsupervised data and evaluates them in terms of"
2020.findings-emnlp.318,2020.acl-main.740,0,0.0615272,"Missing"
2020.findings-emnlp.318,P17-1044,0,0.0310303,"re dependent on triggers, i.e., the same argument span plays completely different roles toward different triggers. An example is shown in Fig. 1, where “two soldiers” plays the role Target for the event ATTACK and the role Victim for INJURY. Different from 2 EAE has similarities with semantic role labeling. Event triggers are comparable to predicates in SRL and the roles in most SRL datasets have a standard convention of interpreting who did what to whom. EAE has a custom taxonomy of roles by domain. We also use inspiration from the SRL body of work (Strubell et al., 2018; Wang et al., 2019b; He et al., 2017; Marcheggiani and Titov, 2017). 3554 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3554–3559 c November 16 - 20, 2020. 2020 Association for Computational Linguistics existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge"
2020.findings-emnlp.318,P11-1113,0,0.725461,"identify the entities that serve as arguments of an event and to classify the specific roles they play. As in Fig. 1, “two soldiers” and “yesterday” are arguments, where the event triggers are “attacked” (with event type being ATTACK1 ) and “injured” (event type INJURY). For the trigger “attacked”, “two soldiers” plays the argument role Target while “yesterday” plays the argument role Attack Time. For the event trigger “injured”, “two soldiers” and “yesterday” play the role Victim and INJURY Time, respectively. There has been significant work on event extraction (EE) (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), but the EAE ∗ Indicates Equal Contribution. Following ACE https://www.ldc.upenn.edu/ collaborations/past-projects/ace 1 task remains a challenge and has become the bottleneck for improving the overall performance of EE (Wang et al., 2019a).2 Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that, (1) We use BERT (Devlin et al., 2018) as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike Yang et al. (2019) who added a final/pr"
2020.findings-emnlp.318,P08-1030,0,0.251345,"ew model which provides the best results in the EAE task. The model can generate trigger-aware argument representations, incorporate syntactic information (via dependency parses), and handle the role overlapping problem with rolespecific argument decoder. We also experiment with some methods to address the data scarcity issue. Experimental results show the effectiveness of our proposed approaches. Acknowledgments Related Work Event Argument Exaction (EAE) is an important task in Event Extraction (EE). Early studies designed lexical, contextual or syntactical features to tackle the EE problem (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013). Later on, neural networks (Yubo et al., 2015; Sha et al., 2016; Nguyen et al., 2016; Sha et al., 2018; Liu et al., 2018; Yang et al., 2019; Wang et al., 2019a) demonstrated their effectiveness in representation learning without manual feature engineering. Our proposed model belongs to the latter category. Here we present and discuss the most related studies to our work. Yang et al. (2019) used a pre-trained model with a state-machine based span boundary detector. They used heuristics to resolve final span boundaries. Wang et al. ("
2020.findings-emnlp.318,P13-1008,0,0.730345,"es that serve as arguments of an event and to classify the specific roles they play. As in Fig. 1, “two soldiers” and “yesterday” are arguments, where the event triggers are “attacked” (with event type being ATTACK1 ) and “injured” (event type INJURY). For the trigger “attacked”, “two soldiers” plays the argument role Target while “yesterday” plays the argument role Attack Time. For the event trigger “injured”, “two soldiers” and “yesterday” play the role Victim and INJURY Time, respectively. There has been significant work on event extraction (EE) (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), but the EAE ∗ Indicates Equal Contribution. Following ACE https://www.ldc.upenn.edu/ collaborations/past-projects/ace 1 task remains a challenge and has become the bottleneck for improving the overall performance of EE (Wang et al., 2019a).2 Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that, (1) We use BERT (Devlin et al., 2018) as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike Yang et al. (2019) who added a final/prediction layer to"
2020.findings-emnlp.318,P10-1081,0,0.384909,"extraction (EAE) aims to identify the entities that serve as arguments of an event and to classify the specific roles they play. As in Fig. 1, “two soldiers” and “yesterday” are arguments, where the event triggers are “attacked” (with event type being ATTACK1 ) and “injured” (event type INJURY). For the trigger “attacked”, “two soldiers” plays the argument role Target while “yesterday” plays the argument role Attack Time. For the event trigger “injured”, “two soldiers” and “yesterday” play the role Victim and INJURY Time, respectively. There has been significant work on event extraction (EE) (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), but the EAE ∗ Indicates Equal Contribution. Following ACE https://www.ldc.upenn.edu/ collaborations/past-projects/ace 1 task remains a challenge and has become the bottleneck for improving the overall performance of EE (Wang et al., 2019a).2 Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that, (1) We use BERT (Devlin et al., 2018) as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike Yang et al. (2019) w"
2020.findings-emnlp.318,D18-1156,0,0.558163,"l Linguistics: EMNLP 2020, pages 3554–3559 c November 16 - 20, 2020. 2020 Association for Computational Linguistics existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge the gap from a word to another distant but highly related word (Sha et al., 2018; Liu et al., 2018; Strubell et al., 2018). We modify a Transformer (Devlin et al., 2018) by explicitly incorporating syntax via an attention layer driven by the dependency parse of the sequence. We design our role-specific argument decoder to seamlessly accommodate both settings (with and without the availability of entities). We also tackle the role overlap problem (Yang et al., 2019) using a set of classifiers or taggers in our decoder. Our model achieves the new state-of-the-art on ACE2005 Events data (Grishman et al., 2005). 2 2.1 Event Argument Extraction Task Setup Consider a sequence X = {x1 , ...xi , ."
2020.findings-emnlp.318,P19-1335,0,0.0198086,", where yg belongs to a fixed set of pre-defined trigger types. Given a sequencetrigger pair (X , g) as input, EAE has two goals: (1) Identify all argument spans from X and (2) Classify the role r for each argument. In some settings, a set of entities is given (each entity is a span in X) and such entities are used as a candidate pool for arguments. For example, “two soldiers” and “yesterday” are candidate entities in Fig. 1. 2.2 tokens, where the BERT embedding of token xt is denoted as bt . A segment (0/1) embedding segt for each token xt indicating whether xt belongs to the trigger or not (Logeswaran et al., 2019, inter-alia) is used, which is added up with token embedding and position embedding as input to BERT (Fig.2). The encoder then concatenates the following learned representations3 for each token: (1) A trigger representation hg by max pooling over BERT embeddings of the tokens in trigger g; (2) A trigger type embedding pyg for yg ; (3) A trigger indicator (0/1) embedding lt , indicating whether xt belongs to the trigger or not.4 (4) A token embedding bt . This results in a trigger-aware representation ct for each token where ct = Concat(bt ; pyg ; lt ; hg ) and C for the whole sequence with T"
2020.findings-emnlp.318,D17-1159,0,0.0309689,"riggers, i.e., the same argument span plays completely different roles toward different triggers. An example is shown in Fig. 1, where “two soldiers” plays the role Target for the event ATTACK and the role Victim for INJURY. Different from 2 EAE has similarities with semantic role labeling. Event triggers are comparable to predicates in SRL and the roles in most SRL datasets have a standard convention of interpreting who did what to whom. EAE has a custom taxonomy of roles by domain. We also use inspiration from the SRL body of work (Strubell et al., 2018; Wang et al., 2019b; He et al., 2017; Marcheggiani and Titov, 2017). 3554 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3554–3559 c November 16 - 20, 2020. 2020 Association for Computational Linguistics existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge the gap from a word to another"
2020.findings-emnlp.318,N16-1034,0,0.496229,"l on the silver dataset; the resulting model is later fine-tuned on the gold data. Auxiliary tasks: Although trigger detection is not the focus of this work, we model it as an auxiliary task to help EAE. We share the BERT encoder (Sec. 2.2) for both tasks. The trigger detection task uses the standard sequence tagging decoder for BERT (Devlin et al., 2018). The intuition here is to improve (1) the representation of the shared BERT component and (2) trigger representation, by performing trigger detection. 3 Argument Identification (AI) P R F1 Role Classification (RC) P R F1 (Yubo et al., 2015) (Nguyen et al., 2016) (Sha et al., 2016) (Sha et al., 2018) (Yang et al., 2019) (Wang et al., 2019a) (Liu et al., 2018) 68.8 61.4 63.2 71.3 71.4 71.4 51.9 64.2 59.4 64.5 60.1 65.6 59.1 62.8 61.2 67.7 65.3 68.4 62.2 54.2 54.1 66.2 62.3 62.2 66.8 46.9 56.7 53.5 52.8 54.2 56.6 54.9 53.5 55.4 53.8 58.7 58.0 59.3 60.3 Ours Ours + Pretraining Ours + Self Training 64.8 65.8 64.5 63.7 62.9 65.0 64.2 64.3 64.7 61.1 62.3 61.1 60.6 60.0 62.3 60.8 61.1 61.7 (Sha et al., 2018)† (Zhang et al., 2019)† (Nguyen and Nguyen, 2019)† (Wadden et al., 2019)† (Zhang et al., 2020)† 63.3 59.9 - 48.7 59.8 - 57.2 55.1 59.9 55.4 - 61.6 52.1 5"
2020.findings-emnlp.318,P16-1116,0,0.45819,"t; the resulting model is later fine-tuned on the gold data. Auxiliary tasks: Although trigger detection is not the focus of this work, we model it as an auxiliary task to help EAE. We share the BERT encoder (Sec. 2.2) for both tasks. The trigger detection task uses the standard sequence tagging decoder for BERT (Devlin et al., 2018). The intuition here is to improve (1) the representation of the shared BERT component and (2) trigger representation, by performing trigger detection. 3 Argument Identification (AI) P R F1 Role Classification (RC) P R F1 (Yubo et al., 2015) (Nguyen et al., 2016) (Sha et al., 2016) (Sha et al., 2018) (Yang et al., 2019) (Wang et al., 2019a) (Liu et al., 2018) 68.8 61.4 63.2 71.3 71.4 71.4 51.9 64.2 59.4 64.5 60.1 65.6 59.1 62.8 61.2 67.7 65.3 68.4 62.2 54.2 54.1 66.2 62.3 62.2 66.8 46.9 56.7 53.5 52.8 54.2 56.6 54.9 53.5 55.4 53.8 58.7 58.0 59.3 60.3 Ours Ours + Pretraining Ours + Self Training 64.8 65.8 64.5 63.7 62.9 65.0 64.2 64.3 64.7 61.1 62.3 61.1 60.6 60.0 62.3 60.8 61.1 61.7 (Sha et al., 2018)† (Zhang et al., 2019)† (Nguyen and Nguyen, 2019)† (Wadden et al., 2019)† (Zhang et al., 2020)† 63.3 59.9 - 48.7 59.8 - 57.2 55.1 59.9 55.4 - 61.6 52.1 54.5 45.7 52.1 52.4"
2020.findings-emnlp.318,D18-1548,0,0.0536388,"Missing"
2020.findings-emnlp.318,D19-1585,0,0.0348839,"task EAE and the auxiliary task of trigger detection; we alternate between batches of main task and auxiliary task with probabilities of 0.9 and 0.1, respectively. We early stop training if performance on the development set does not improve after 20 epochs. All model parameters are fine-tuned during training. For BERT pretraining, we use the same setting as in (Devlin et al., 2018) but with an initial learning rate of 1e-5. We stop pretraining after 10k steps. In order to obtain reliably predicted triggers as input for EAE, we trained a five-model ensemble trigger detection system following Wadden et al. (2019).12 9 Standard splits (Li et al., 2013): 529 documents (14,385 sentences) are used for training, 30 documents (813 sentences) for development, and 40 documents (632 sentences) for test. 10 https://catalog.ldc.upenn.edu/LDC2011T07 11 https://stanfordnlp.github.io/CoreNLP/ 12 Since trigger detection is not our main task and improving it is not the focus of this work, its results are not for comparison and thus excluded from the main result tables. As a Table 1: Experimental results. † indicates a model does not use gold entities. Ours show the mean of 5 random seeds. P refers to precision and R"
2020.findings-emnlp.318,D19-1584,0,0.500705,"type INJURY). For the trigger “attacked”, “two soldiers” plays the argument role Target while “yesterday” plays the argument role Attack Time. For the event trigger “injured”, “two soldiers” and “yesterday” play the role Victim and INJURY Time, respectively. There has been significant work on event extraction (EE) (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), but the EAE ∗ Indicates Equal Contribution. Following ACE https://www.ldc.upenn.edu/ collaborations/past-projects/ace 1 task remains a challenge and has become the bottleneck for improving the overall performance of EE (Wang et al., 2019a).2 Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that, (1) We use BERT (Devlin et al., 2018) as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike Yang et al. (2019) who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components (Sec. 2). (2) We use (unlabeled) in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in (Gur"
2020.findings-emnlp.318,P19-1529,0,0.243205,"type INJURY). For the trigger “attacked”, “two soldiers” plays the argument role Target while “yesterday” plays the argument role Attack Time. For the event trigger “injured”, “two soldiers” and “yesterday” play the role Victim and INJURY Time, respectively. There has been significant work on event extraction (EE) (Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), but the EAE ∗ Indicates Equal Contribution. Following ACE https://www.ldc.upenn.edu/ collaborations/past-projects/ace 1 task remains a challenge and has become the bottleneck for improving the overall performance of EE (Wang et al., 2019a).2 Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that, (1) We use BERT (Devlin et al., 2018) as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike Yang et al. (2019) who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components (Sec. 2). (2) We use (unlabeled) in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in (Gur"
2020.findings-emnlp.318,P19-1522,0,0.517565,"o and Grishman, 2010; Hong et al., 2011; Li et al., 2013), but the EAE ∗ Indicates Equal Contribution. Following ACE https://www.ldc.upenn.edu/ collaborations/past-projects/ace 1 task remains a challenge and has become the bottleneck for improving the overall performance of EE (Wang et al., 2019a).2 Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that, (1) We use BERT (Devlin et al., 2018) as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike Yang et al. (2019) who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components (Sec. 2). (2) We use (unlabeled) in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in (Gururangan et al., 2020). This makes the encoder domain-aware. (3) We perform self-training to construct auto-labeled data (silver data). A crucial aspect for EAE is to integrate event trigger information into the learned representations. This is important because arguments are dependent on triggers, i.e., the same argument"
2020.findings-emnlp.318,P15-1017,0,0.362531,"version of our model on the silver dataset; the resulting model is later fine-tuned on the gold data. Auxiliary tasks: Although trigger detection is not the focus of this work, we model it as an auxiliary task to help EAE. We share the BERT encoder (Sec. 2.2) for both tasks. The trigger detection task uses the standard sequence tagging decoder for BERT (Devlin et al., 2018). The intuition here is to improve (1) the representation of the shared BERT component and (2) trigger representation, by performing trigger detection. 3 Argument Identification (AI) P R F1 Role Classification (RC) P R F1 (Yubo et al., 2015) (Nguyen et al., 2016) (Sha et al., 2016) (Sha et al., 2018) (Yang et al., 2019) (Wang et al., 2019a) (Liu et al., 2018) 68.8 61.4 63.2 71.3 71.4 71.4 51.9 64.2 59.4 64.5 60.1 65.6 59.1 62.8 61.2 67.7 65.3 68.4 62.2 54.2 54.1 66.2 62.3 62.2 66.8 46.9 56.7 53.5 52.8 54.2 56.6 54.9 53.5 55.4 53.8 58.7 58.0 59.3 60.3 Ours Ours + Pretraining Ours + Self Training 64.8 65.8 64.5 63.7 62.9 65.0 64.2 64.3 64.7 61.1 62.3 61.1 60.6 60.0 62.3 60.8 61.1 61.7 (Sha et al., 2018)† (Zhang et al., 2019)† (Nguyen and Nguyen, 2019)† (Wadden et al., 2019)† (Zhang et al., 2020)† 63.3 59.9 - 48.7 59.8 - 57.2 55.1 5"
2020.findings-emnlp.89,N19-1253,0,0.111422,"tate such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Resu"
2020.findings-emnlp.89,D17-1130,1,0.92647,"Missing"
2020.findings-emnlp.89,D12-1133,0,0.0148225,"mti is a {−∞, 0} mask, pti are the position embeddings for elements in the stack, h = f (w) is the output of the Transformer encoder. The attention would be computed from the score function as αti = softmax(et )i Both mask and positions change for each word and time-step as the parser state changes, but they imply little computation overhead and can be precomputed for training. Henceforth this modification will be referred to as stack-Transformer. 3.2 Labeled SHIFT Multi-task It is common practice for transition-based systems to add an additional Part of Speech (POS) or word prediction task (Bohnet and Nivre, 2012). This is achieved by labeling the SHIFT action, that moves a word from the buffer to the stack, with the word’s tag. This decorated actions become part of the action history a&lt;t , which was expected to give better visibility into stack/buffer content and exploit Transformer’s attentional encoding of history. In initial experiments, POS tags produced a small improvement while word prediction led to performance decrease. It was observed, however, that prediction of only 100 − 300 most frequent words, leaving SHIFT undecorated otherwise, led to large performance increases. This is thus the metho"
2020.findings-emnlp.89,2020.acl-main.119,0,0.476243,"ck-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Results show that local"
2020.findings-emnlp.89,P13-2131,0,0.17666,"f entity sub-graphs, this increased oracle Smatch from 93.7 to 98.1 and notably improved model performance. We therefore provide results for the Naseem et al. (2019) oracle for comparison. Both previous works predict a node creation action and then a node label, or call a lemmatizer if no label is found. Instead, we directly predicted the label and added COPY actions to construct node names from lemmas1 or surface words, resulting in a maximum of 9K actions. Node label predictions were limited to those seen during training for the word on the top of the stack. Results were measured in Smatch (Cai and Knight, 2013) using the latest version 1.0.42 . Regarding model implementation, all models were implemented on the fairseq toolkit and trained with only minor modifications over the MT model hyper-parameters (Ott et al., 2018). This used crossentropy training with learning rate 5e−4 , inverse square root scheduling with min. 1e−9 , 4000 warmup updates with learning rate 1e−7 , and maximum 3584 tokens per batch. Adam parameters 0.9 and 0.98, label smoothing was reduced to 0.013 . All models used 6 layers of encoding and decoding with size 256 and 4 attention heads, except the normal Transformers in AMR, whi"
2020.findings-emnlp.89,N19-1423,0,0.0410949,"for global (full buffer, full stack) variants being more performant. Modeling of the buffer seems also more important than modeling of the stack. One possible explanation for this is that, since the total number of heads is kept fixed, it may be more useful to gain an additional free head than modeling the stack content. Furthermore without recursive representation building, as in stack-LSTMs, the role of the stack can be expected to be less important. Tables 2 and 3 compare with prior works. Pretrained embeddings used are indicated as XLnet-largeX (Yang et al., 2019), BERT baseβ and largeB (Devlin et al., 2019), Graph Recategorization, which utilizes an external entity recognizer (Lyu and Titov, 2018; Zhang et al., 2019) as (G.R.) and a∗ indicates the Naseem et al. (2019) oracle. Overall, the stack-Transformer is competitive against recent works particularly for AMR, likely due to the higher complexity of the task. Compared to prior AMR systems, it is worth noting the large performance increase against stack-LSTM (Naseem et al., 2019), while sharing a similar oracle and embeddings and not using reinforcement learning fine-tuning. The stack-Transformer also matches the best reported AMR system (Cai a"
2020.findings-emnlp.89,P81-1022,0,0.716276,"Missing"
2020.findings-emnlp.89,P15-1033,1,0.771904,"e attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this"
2020.findings-emnlp.89,N16-1024,1,0.865879,"Missing"
2020.findings-emnlp.89,N19-1076,0,0.0269193,"Missing"
2020.findings-emnlp.89,J13-4006,0,0.0304049,"tions of the sequence-to-sequence Transformer architecture to model either global or local parser states in transition-based parsing. We show that modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data. 1 Introduction Transition-based Parsing transforms the task of predicting a graph from a sentence into predicting an action sequence of a state machine that produces the graph (Nivre, 2003, 2004; Kubler et al., 2009; Henderson et al., 2013). These parsers are attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such a"
2020.findings-emnlp.89,W16-2316,0,0.111522,"Missing"
2020.findings-emnlp.89,Q16-1023,0,0.397406,"tionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross"
2020.findings-emnlp.89,D19-1279,0,0.02522,"espread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model si"
2020.findings-emnlp.89,D19-1277,0,0.0397653,"Missing"
2020.findings-emnlp.89,W17-6315,0,0.284317,"ch as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Results show that local and global state modeling of the parser state yield more than 2 percentage points absolute improvement over a strong Transformer baseline, both for dependency and Abstract Meaning Representation (AMR) parsing. Gains are also particularly large for smaller train sets"
2020.findings-emnlp.89,2021.ccl-1.108,0,0.0504955,"Missing"
2020.findings-emnlp.89,P18-1037,0,0.145188,"seems also more important than modeling of the stack. One possible explanation for this is that, since the total number of heads is kept fixed, it may be more useful to gain an additional free head than modeling the stack content. Furthermore without recursive representation building, as in stack-LSTMs, the role of the stack can be expected to be less important. Tables 2 and 3 compare with prior works. Pretrained embeddings used are indicated as XLnet-largeX (Yang et al., 2019), BERT baseβ and largeB (Devlin et al., 2019), Graph Recategorization, which utilizes an external entity recognizer (Lyu and Titov, 2018; Zhang et al., 2019) as (G.R.) and a∗ indicates the Naseem et al. (2019) oracle. Overall, the stack-Transformer is competitive against recent works particularly for AMR, likely due to the higher complexity of the task. Compared to prior AMR systems, it is worth noting the large performance increase against stack-LSTM (Naseem et al., 2019), while sharing a similar oracle and embeddings and not using reinforcement learning fine-tuning. The stack-Transformer also matches the best reported AMR system (Cai and Lam, 2020) on AMR1.0 without graph recategorization, but using RoBERTa instead of BERT e"
2020.findings-emnlp.89,P18-1130,0,0.0181131,"nd buffer attentions. While simple, this precludes the use of SWAP actions needed for AMR parsing and non-projective parsing. Zhang et al. (2017) mask out reduced words and add a bias to the attention weights for words in the stack. While being the closest to the proposed technique, this method does not separately model stack and buffer nor retains free attention heads, which we consider a fundamental advantage. We also provide evidence that modeling the parser state still produces gains when using pre-trained Transformer embeddings and provide a detailed analysis of components. Finally, RNN (Ma et al., 2018) and selfattention (Ahmad et al., 2019) Stack-Pointer networks sum encoder representations based on local graph structure, which can be interpreted as masked uniform attention over 3 words and is related to the previous methods. 7 Conclusions We have explored modifications of sequence-tosequence Transformers to encode the parser state for transition-based parsing, inspired by stackLSTM’s global modeling of the parser state. While simple, these modifications consistently provide improvements against a normal sequence to sequence Transformer in transition-based parsing, both for dependency parsi"
2020.findings-emnlp.89,J93-2004,0,0.0707626,"Missing"
2020.findings-emnlp.89,2021.tacl-1.8,0,0.0860829,"Missing"
2020.findings-emnlp.89,P19-1451,1,0.724394,"ks, also well resourced (36K sentences). AMR1.0 has around 10K sentences and can be considered as AMR with limited train data. The dependency parsing setup followed Dyer et al. (2015), in the setting with no POS tags. This has only SHIFT, LEFT-ARC(label), and RIGHTARC(label) base action with a total of 82 different actions. Results were measured in terms of (Un)labeled Attachment Scores (UAS/LAS). The AMR setup followed Ballesteros and AlOnaizan (2017a), which introduced new actions to segment text and derive nodes or entity sub-graphs. In addition, we use the alignments and wikification from Naseem et al. (2019). Unlike previous works, we force-aligned the unaligned nodes to neighbouring words and allowed attachment to the leaf nodes of entity sub-graphs, this increased oracle Smatch from 93.7 to 98.1 and notably improved model performance. We therefore provide results for the Naseem et al. (2019) oracle for comparison. Both previous works predict a node creation action and then a node label, or call a lemmatizer if no label is found. Instead, we directly predicted the label and added COPY actions to construct node names from lemmas1 or surface words, resulting in a maximum of 9K actions. Node label"
2020.findings-emnlp.89,W18-6301,0,0.0213418,"a node creation action and then a node label, or call a lemmatizer if no label is found. Instead, we directly predicted the label and added COPY actions to construct node names from lemmas1 or surface words, resulting in a maximum of 9K actions. Node label predictions were limited to those seen during training for the word on the top of the stack. Results were measured in Smatch (Cai and Knight, 2013) using the latest version 1.0.42 . Regarding model implementation, all models were implemented on the fairseq toolkit and trained with only minor modifications over the MT model hyper-parameters (Ott et al., 2018). This used crossentropy training with learning rate 5e−4 , inverse square root scheduling with min. 1e−9 , 4000 warmup updates with learning rate 1e−7 , and maximum 3584 tokens per batch. Adam parameters 0.9 and 0.98, label smoothing was reduced to 0.013 . All models used 6 layers of encoding and decoding with size 256 and 4 attention heads, except the normal Transformers in AMR, which performed better on a 3/8 layer configuration instead of 6/6. To study the effect of model size, small versions of all models using a 2/2 configuration were also tested. 1 We used https://spacy.io/ as lemmatize"
2020.findings-emnlp.89,D19-1392,0,0.204029,"Missing"
2020.findings-emnlp.89,P11-2033,0,0.23849,"ion sequence of a state machine that produces the graph (Nivre, 2003, 2004; Kubler et al., 2009; Henderson et al., 2013). These parsers are attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neural networks, global models of the parser state such as the stack-LSTM (Dyer et al., 2015) allowed encoding the entire buffer and ∗ Miguel’s and Austin’s contributions were carried out while at IBM Research. stack. It was later shown that local features of the stack and buffer extracted from contextual word representations, such as Bi-LSTMs, could outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this"
2020.findings-emnlp.89,D17-1175,0,0.232117,"outperform global modeling (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). With the rise of the Transformer model (Vaswani et al., 2017), various approaches have been proposed that leverage this architecture for parsing (Kondratyuk, 2019; Kulmizev et al., 2019; Mrini et al., 2019; Ahmad et al., 2019; Cai and Lam, 2020). In this work we revisit the local versus global paradigms of state modeling in the context of sequence-to-sequence Transformers applied to action prediction for transition-based parsing. Similarly to previous works for RNN sequence to sequence (Liu and Zhang, 2017; Zhang et al., 2017), we propose a modification of the cross-attention mechanism of the Transformer to provide global parser state modeling. We analyze the role of local versus global parser state modeling, stack and buffer modeling, effects model size as well as task complexity and amount of training data. Results show that local and global state modeling of the parser state yield more than 2 percentage points absolute improvement over a strong Transformer baseline, both for dependency and Abstract Meaning Representation (AMR) parsing. Gains are also particularly large for smaller train sets and smaller model si"
2020.findings-emnlp.89,W03-3017,0,0.140844,"ing systems, this work explores modifications of the sequence-to-sequence Transformer architecture to model either global or local parser states in transition-based parsing. We show that modifications of the cross attention mechanism of the Transformer considerably strengthen performance both on dependency and Abstract Meaning Representation (AMR) parsing tasks, particularly for smaller models or limited training data. 1 Introduction Transition-based Parsing transforms the task of predicting a graph from a sentence into predicting an action sequence of a state machine that produces the graph (Nivre, 2003, 2004; Kubler et al., 2009; Henderson et al., 2013). These parsers are attractive for their linear inference time and interpretability, however, their performance hinges on effective modeling of the parser state at every decision step. Parser states typically comprise two memories, a buffer and a stack, from which tokens can be pushed or popped (Kubler et al., 2009). Traditionally, parser states were modeled using hand selected local features pertaining only to the words on the top of the stack or buffer (Nivre et al., 2007; Zhang and Nivre, 2011, inter-alia). With the widespread use of neura"
2020.findings-emnlp.89,W04-0308,0,0.141912,"Missing"
2021.eacl-main.191,P17-1080,0,0.0313539,"dely spread practice in NLP, with models such as ELMo (Peters et al., 2018) and, most notably, BERT (Devlin et al., 2019), achieving state-of-the-art results in many well-known Natural Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)) BERT deep models’ vector geometry implicitly embeds entire syntax trees (Hewitt and Manning, 2019). However, rather little is understood about how these representations change when fine-tuned to solve downstream tasks (Peters et al., 2019). In this work, we aim to understand how syntax trees implicitly embedded in the geometry of deep models evolve along the fine-tuning process of BERT on different supervised tasks, and shed some light on the importance of the syntactic information for those tasks. Intuitivel"
2021.eacl-main.191,C10-3009,0,0.0660577,"Missing"
2021.eacl-main.191,W05-0620,0,0.23958,"Missing"
2021.eacl-main.191,2020.acl-main.561,0,0.0248025,"paraphrase identification. The first three inherently deal with (morpho-)syntactic information while the latter three, which traditionally draw upon the output of syntactic parsing (Carreras and M`arquez, 2005; Bj¨orkelund et al., 2010; Strubell et al., 2018; Wang et al., 2019, inter-alia), deal with higher level, semantic information. Almost all of our experiments are on English corpora; one is on multilingual dependency parsing. 2 Related work BERT has become the default baseline in NLP, and consequently, numerous studies analyze its linguistic capabilities in general (Rogers et al., 2020; Henderson, 2020), and its syntactic capabilities in particular (Linzen and Baroni, 2020). Even if syntactic information is distributed across all layers (Durrani et al., 2020), BERT captures most phrase-level information in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019"
2021.eacl-main.191,N19-1419,0,0.429062,"ral Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)) BERT deep models’ vector geometry implicitly embeds entire syntax trees (Hewitt and Manning, 2019). However, rather little is understood about how these representations change when fine-tuned to solve downstream tasks (Peters et al., 2019). In this work, we aim to understand how syntax trees implicitly embedded in the geometry of deep models evolve along the fine-tuning process of BERT on different supervised tasks, and shed some light on the importance of the syntactic information for those tasks. Intuitively, we expect morpho-syntactic tasks to clearly reinforce the encoded syntactic information, while tasks that are not explicitly syntactic in nature should maintain it in case they bene"
2021.eacl-main.191,P19-1356,0,0.0185575,"istic capabilities in general (Rogers et al., 2020; Henderson, 2020), and its syntactic capabilities in particular (Linzen and Baroni, 2020). Even if syntactic information is distributed across all layers (Durrani et al., 2020), BERT captures most phrase-level information in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019). The syntactic structure captured by BERT adheres to that of the Universal Dependencies (Kulmizev et al., 2020); different syntactic and semantic relations are captured by self-attention patterns (Kovaleva et al., 2019; Limisiewicz et al., 2020; Ravishankar et al., 2021), and it has been shown that full dependency trees can be decoded from single attention heads (Ravishankar et al., 2021). BERT performs remarkably well on subject-verb agreement (Goldberg, 2019), and is able to do full parsing relying only on pretraining architectures and no dec"
2021.eacl-main.191,D19-1445,0,0.0194995,"ation in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019). The syntactic structure captured by BERT adheres to that of the Universal Dependencies (Kulmizev et al., 2020); different syntactic and semantic relations are captured by self-attention patterns (Kovaleva et al., 2019; Limisiewicz et al., 2020; Ravishankar et al., 2021), and it has been shown that full dependency trees can be decoded from single attention heads (Ravishankar et al., 2021). BERT performs remarkably well on subject-verb agreement (Goldberg, 2019), and is able to do full parsing relying only on pretraining architectures and no decoding (Vilares et al., 2020), surpassing existing sequence labeling parsers on the Penn Treebank dataset (De Marneffe et al., 2006) and on the end-to-end Universal Dependencies Corpus for English (Silveira et al., 2014). It can generally also distinguish good from bad"
2021.eacl-main.191,2020.acl-main.375,0,0.0180508,"information is distributed across all layers (Durrani et al., 2020), BERT captures most phrase-level information in the lower layers, followed by surface 2243 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2243–2258 April 19 - 23, 2021. ©2021 Association for Computational Linguistics features, syntactic features and semantic features in the intermediate and top layers (Jawahar et al., 2019; Tenney et al., 2019a; Hewitt and Manning, 2019). The syntactic structure captured by BERT adheres to that of the Universal Dependencies (Kulmizev et al., 2020); different syntactic and semantic relations are captured by self-attention patterns (Kovaleva et al., 2019; Limisiewicz et al., 2020; Ravishankar et al., 2021), and it has been shown that full dependency trees can be decoded from single attention heads (Ravishankar et al., 2021). BERT performs remarkably well on subject-verb agreement (Goldberg, 2019), and is able to do full parsing relying only on pretraining architectures and no decoding (Vilares et al., 2020), surpassing existing sequence labeling parsers on the Penn Treebank dataset (De Marneffe et al., 2006) and on the end-to-end Univers"
2021.eacl-main.191,2020.tacl-1.50,0,0.201783,"little is understood about how these representations change when fine-tuned to solve downstream tasks (Peters et al., 2019). In this work, we aim to understand how syntax trees implicitly embedded in the geometry of deep models evolve along the fine-tuning process of BERT on different supervised tasks, and shed some light on the importance of the syntactic information for those tasks. Intuitively, we expect morpho-syntactic tasks to clearly reinforce the encoded syntactic information, while tasks that are not explicitly syntactic in nature should maintain it in case they benefit from syntax (Kuncoro et al., 2020) and lose it if they do not. In order to cover the three main levels of the linguistic description (morphology, syntax and semantics), we select six different tasks: PoS tagging, constituency parsing, syntactic dependency parsing, semantic role labeling (SRL), question answering (QA) and paraphrase identification. The first three inherently deal with (morpho-)syntactic information while the latter three, which traditionally draw upon the output of syntactic parsing (Carreras and M`arquez, 2005; Bj¨orkelund et al., 2010; Strubell et al., 2018; Wang et al., 2019, inter-alia), deal with higher le"
2021.eacl-main.191,N19-1112,0,0.0180436,"nly on pretraining architectures and no decoding (Vilares et al., 2020), surpassing existing sequence labeling parsers on the Penn Treebank dataset (De Marneffe et al., 2006) and on the end-to-end Universal Dependencies Corpus for English (Silveira et al., 2014). It can generally also distinguish good from bad completions and robustly retrieves noun hypernyms, but shows insensitivity to the contextual impacts of negation (Ettinger, 2020). Different supervised probing models have been used to test for the presence of a wide range of linguistic phenomena in the BERT model (Conneau et al., 2018; Liu et al., 2019; Tenney et al., 2019b; Voita and Titov, 2020; Elazar et al., 2020). Hewitt and Manning (2019)’s structural probe shows that entire syntax trees are embedded implicitly in BERT’s vector geometry. Extending their work, Chi et al. (2020) show that multilingual BERT recovers syntactic tree distances in languages other than English and learns representations of syntactic dependency labels. Regarding how fine-tuning affects the representations of BERT, Gauthier and Levy (2019) found a significant divergence between the final representations of models fine-tuned on different tasks when using the str"
2021.eacl-main.191,J93-2004,0,0.0740579,"and training data order (Dodge et al., 2020), we repeat this process 5 times per task with different random seeds and average results. PoS tagging. We fine-tune BERT with a linear layer on top of the hidden-states output for token classification.3 Dataset: Universal Dependencies Corpus for English (UD 2.5 EN EWT Silveira et al. (2014)). Constituency parsing. Following Vilares et al. (2020), we cast constituency parsing as a sequence labeling problem, and use a single feed-forward layer on top of BERT to directly map word vectors to labels that encode a linearized tree. Dataset: Penn Treebank (Marcus et al., 1993). Dependency parsing. We fine-tune a Deep Biaffine neural dependency parser (Dozat and Manning, 2016) on three different datasets: i) UD 2.5 English EWT (Silveira et al., 2014); ii) a multilingual benchmark generated by concatenating the UD 2.5 standard data splits for German, English, Spanish, French, Italian, Portuguese, and Swedish (Zeman et al., 2019), with gold PoS tags; iii) PTB SD 3.3.0 (De Marneffe et al., 2006). Semantic role labeling. Following Shi and Lin (2019), we decompose the task into i) predicate sense disambiguation and argument identification, and ii) classification. Both su"
2021.eacl-main.191,2020.blackboxnlp-1.4,0,0.0210519,"zar et al., 2020). Hewitt and Manning (2019)’s structural probe shows that entire syntax trees are embedded implicitly in BERT’s vector geometry. Extending their work, Chi et al. (2020) show that multilingual BERT recovers syntactic tree distances in languages other than English and learns representations of syntactic dependency labels. Regarding how fine-tuning affects the representations of BERT, Gauthier and Levy (2019) found a significant divergence between the final representations of models fine-tuned on different tasks when using the structural probe of Hewitt and Manning (2019), while Merchant et al. (2020) concluded that fine-tuning is conservative and does not lead to catastrophic forgetting of linguistic phenomena – which our experiments do not confirm. However, we find that the encoded syntactic information is forgotten, reinforced or preserved differently along the fine-tuning process depending on the task. 3 Experimental setup We study the evolution of the syntactic structures discovered during pretraining along the fine-tuning of BERT-base (cased) (Devlin et al., 2019)1 on six different tasks, drawing upon the structural probe of Hewitt and Manning (2019).2 We fine-tune the whole model on"
2021.eacl-main.191,N18-1202,0,0.0463525,"g. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semanticsrelated tasks) in different ways along the finetuning process depending on the task. 1 Introduction Adapting unsupervised pretrained language models (LMs) to solve supervised tasks has become a widely spread practice in NLP, with models such as ELMo (Peters et al., 2018) and, most notably, BERT (Devlin et al., 2019), achieving state-of-the-art results in many well-known Natural Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sen"
2021.eacl-main.191,W19-4302,0,0.0522777,"Missing"
2021.eacl-main.191,2020.acl-main.420,0,0.0133724,"aster/examples/ text-classification/run_glue.py. 6 Cf. also Supplementary Material. Figure 1: Tree distance evaluation. UUAS evolution. Figure 2: Tree distance evaluation. Dspr evolution. assumed. As many words have a clear preference towards a specific PoS, especially in English, and most of the ambiguous cases can be resolved using information in the close vicinity (e.g., a simple 3gram sequence tagger is able to achieve a very high accuracy (Manning, 2011)), syntactic structure information may not be necessary and, therefore, the model does not preserve it. This observation is aligned with Pimentel et al. (2020), who found that PoS-tagging is not an ideal task for contemplating the syntax contained in contextual word embeddings. The loss is less pronounced on depth-related metrics, maybe because the root of the sentence usually corresponds to the verb, which may also help in identifying the PoS of surrounding words. Constituency parsing and dependency parsing share a very similar tendency, with a big improvement in the first fine-tuning steps preserved along the rest of the process. As both tasks heavily rely on syntactic information, this improvement intuitively makes sense. Dependency parsing fine-"
2021.eacl-main.191,P18-2124,0,0.128433,"show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semanticsrelated tasks) in different ways along the finetuning process depending on the task. 1 Introduction Adapting unsupervised pretrained language models (LMs) to solve supervised tasks has become a widely spread practice in NLP, with models such as ELMo (Peters et al., 2018) and, most notably, BERT (Devlin et al., 2019), achieving state-of-the-art results in many well-known Natural Language Understanding benchmarks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Several studies investigate what the LMs learn, how and where the learned knowledge is represented and what the best methods to improve it are; cf., e.g., (Rogers et al., 2020). There is evidence that, among other information (such as, e.g., PoS, syntactic chunks and roles (Tenney et al., 2019b; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)) BERT deep models’ vector geometry implicitly embeds entire syntax trees (Hewitt and Manning, 2019). However, rather little is understood about how these representations change"
2021.eacl-main.198,W06-0901,0,0.0270483,"r time. This goal of identifying and tracking topics from a news stream was first introduced in the Topic Detection and Tracking (TDT) task (Allan et al., 1998). Topics in the news stream setting usually correspond to real-world events, while news articles may also be categorized thematically into ∗ sports, politics, etc. We focus on the task of clustering news on the basis of event-based story chains. We make a distinction between our definition of an event topic, which follows TDT and refers to large-scale real-world events, and the fine-grained events used in trigger-based event detection (Ahn, 2006). Given the non-parametric nature of our task (the number of events is not known beforehand and evolves over time), the two primary approaches have been topic modeling using Hierarchical Dirichlet Processes (HDPs) (Teh et al., 2005; Beykikhoshk et al., 2018) and Stream Clustering (MacQueen, 1967; Laban and Hearst, 2017; Miranda et al., 2018). While HDPs use word distributions within documents to infer topics, stream clustering models use representation strategies to encode and cluster documents. Contemporary models have adopted stream clustering using TF-IDF weighted bag of words representatio"
2021.eacl-main.198,N19-1423,0,0.17652,"n, 1967; Laban and Hearst, 2017; Miranda et al., 2018). While HDPs use word distributions within documents to infer topics, stream clustering models use representation strategies to encode and cluster documents. Contemporary models have adopted stream clustering using TF-IDF weighted bag of words representations to achieve state-of-the-art results (Staykovski et al., 2019). In this paper, we present a model for event topic detection and tracking from news streams that leverages a combination of dense and sparse document representations. Our dense representations are obtained from BERT models (Devlin et al., 2019) finetuned using the triplet network architecture (Hoffer and Ailon, 2015) on the event similarity task, which we describe in Section 3. We also use an adaptation of the triplet loss to learn a Support Vector Machine (SVM) (Boser et al., 1992) based document-cluster similarity model and handle the non-parametric cluster creation using a shallow neural network. We empirically show consistent improvement in clustering performance across many clustering metrics and significantly less cluster fragmentation. The main contributions of this paper are: Work done during internship at Amazon • We presen"
2021.eacl-main.198,N18-1098,0,0.0464114,"Missing"
2021.eacl-main.198,W17-2701,0,0.236159,"politics, etc. We focus on the task of clustering news on the basis of event-based story chains. We make a distinction between our definition of an event topic, which follows TDT and refers to large-scale real-world events, and the fine-grained events used in trigger-based event detection (Ahn, 2006). Given the non-parametric nature of our task (the number of events is not known beforehand and evolves over time), the two primary approaches have been topic modeling using Hierarchical Dirichlet Processes (HDPs) (Teh et al., 2005; Beykikhoshk et al., 2018) and Stream Clustering (MacQueen, 1967; Laban and Hearst, 2017; Miranda et al., 2018). While HDPs use word distributions within documents to infer topics, stream clustering models use representation strategies to encode and cluster documents. Contemporary models have adopted stream clustering using TF-IDF weighted bag of words representations to achieve state-of-the-art results (Staykovski et al., 2019). In this paper, we present a model for event topic detection and tracking from news streams that leverages a combination of dense and sparse document representations. Our dense representations are obtained from BERT models (Devlin et al., 2019) finetuned"
2021.eacl-main.198,P19-1335,0,0.019525,"similarity = 1), while those from different events are dissimilar (with similarity = 0). Given the embeddings of an anchor document da , a positive document dp (from the same event as the anchor) and a negative document dn (from a different event), triplet loss is computed as ltriplet = sim(da , dn ) − sim(da , dp ) + m (1) where sim is the cosine similarity function and m is the hyper-parameter margin. Providing External Entity Knowledge In line with TDT’s definition, entities are central to events and thus need to be highlighted in document representations for our clustering task. We follow Logeswaran et al. (2019) to introduce entity awareness to BERT by leveraging knowledge from an external NER system. Apart from token, position and token type embeddings, we also add an entity presence-absence embedding for each token depending on whether it corresponds to an entity or not. The entity aware BERT model architecture is shown in Figure 2. This enhanced entity-aware model can then be coupled with the event similarity (E-S-BERT) objective for fine-tuning. 3.1.3 Temporal Representation Documents are also represented with the timestamp of publication. Unlike TF-IDF and dense embeddings, which are vector valu"
2021.eacl-main.198,H05-1004,0,0.104726,"hile mistakes on smaller clusters can fall through without incurring much penalty. In our experiments, we observed that this property of the metric prevents it from capturing cluster fragmentation errors on smaller events. In the news stream clustering setting, small events may correspond to recent salient events and thus, we want our metric 6 The mean and standard deviation of the cluster count over five independent training and evaluations of our model are 312 ± 27. 2337 to be agnostic to the size of the clusters. We thus use an additional metric that weights every cluster equally - CEAF-e (Luo, 2005). The CEAF-e metric creates a one-to-one mapping between the clustering output and gold clusters using the Kuhn-Munkres algorithm. The similarity between a gold cluster G and an output cluster O is computed as the fraction of articles that are common to the clusters. Once the clusters are aligned, precision and recall are computed using the aligned pairs of clusters. This ensures that unaligned clusters contribute to a penalty in the score and cluster fragmentation and coalescing is captured by the metric. In order to ensure that our model’s better performance is metric-agnostic, we also empir"
2021.eacl-main.198,D18-1483,0,0.115479,"Missing"
2021.eacl-main.198,D15-1225,0,0.0160655,"and cluster creation. Similarity between a document and cluster is computed along multiple document representations and then aggregated using a Rank-SVM model (Joachims, 2002). The decision to merge a document with a cluster or create a new cluster is taken by an SVM classifier. Our model also follows this architecture, but critically adds dense document representations, an SVM trained on the adapted triplet loss for aggregating document-cluster similarities and a shallow neural network for cluster creation. News event tracking has also been framed as a non-parametric topic modeling problem (Zhou et al., 2015) and HDPs that share parameters across temporal batches have been used for this task (Beykikhoshk et al., 2018). Dense document representations have been shown to be useful in the parametric variant of our problem, with neural LDA (Dieng et al., 2019a; Keya et al., 2019; Dieng et al., 2019b; Bianchi et al., 2020), temporal topic evolution models (Zaheer et al., 2017; Gupta et al., 2018; Zaheer et al., 2019; Brochier et al., 2020) and embedding space clustering (Momeni et al., 2018; Sia et al., 2020) being some prominent approaches in the literature. 2331 Figure 1: The architecture of the news"
2021.eacl-main.198,D19-1410,0,0.0276536,"sify a document into one of the events in the output space. Fine-tuning on Event Similarity Fine-tuning on the task of event classification constrains the embedding of documents corresponding to different events to be non-linearly separable. Semantics about events can be better captured if the vector similarity between document embeddings encode whether they are from the same event or not. For this, we adapt the triplet network architecture (Hoffer and Ailon, 2015) and fine-tune on the task of event similarity. Triplet BERT networks were introduced for the semantic text similarity (STS) task (Reimers and Gurevych, 2019), where the vector similarity between sentence embeddings was tuned to reflect the semantic similarity between them. We formulate the event similarity task, where the term “similarity” refers to whether two documents are from the same event cluster or not. In our task, documents from the same event are similar (with similarity = 1), while those from different events are dissimilar (with similarity = 0). Given the embeddings of an anchor document da , a positive document dp (from the same event as the anchor) and a negative document dn (from a different event), triplet loss is computed as ltrip"
2021.emnlp-main.118,P17-1080,0,0.0303724,"with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-theoretical metrics such as perplexity, i.e., the probability of predicting a word in its context. The general wisdom is that the more pretraining data a model is fed, the lower its perplexity gets. However, large volumes of pretraining data are not always available and pretraining is costly, such that the following questions need to be answered: (i) Do we always ne"
2021.emnlp-main.118,2020.acl-main.493,0,0.0135195,"tion 6 summarizes the implications that our work has for the use of pretrained language models. 2 2.1 Background Syntactic assessment of language models suites, finding substantial differences in syntactic generalization performance by model architecture. Supervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language models keep growing in orders of magnitude, so do the resources necessary for their development and, consequently, also the inclusivity gap. The financial cost of the required hardware and electricity favors industry-powered resear"
2021.emnlp-main.118,P18-1198,0,0.0247188,"BERTas models and the syntactic tests as well as the downstream applications we explore. Section 4 presents the outcome of our experiments. Section 5 offers a cost-benefit analysis of the pretraining of the different models, and Section 6 summarizes the implications that our work has for the use of pretrained language models. 2 2.1 Background Syntactic assessment of language models suites, finding substantial differences in syntactic generalization performance by model architecture. Supervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language mo"
2021.emnlp-main.118,N19-1423,0,0.0353809,"and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 20"
2021.emnlp-main.118,I05-5002,0,0.115912,"Missing"
2021.emnlp-main.118,N19-1004,1,0.899159,"Missing"
2021.emnlp-main.118,W18-2501,0,0.0122756,"follow Wang and Cho (2019)’s sequential sampling procedure, which is not affected by the error that was reported in equations 1-3, related to the Non-sequential sampling procedure. To compute the probability distribution for a sentence with N tokens, we start with a sequence of begin_of_sentence token plus N mask tokens plus an extra mask token to account for the end_of_sentence token. For each masked position in [1, N ], we compute the probability dis2 The implementation relies in the Transformers library tribution over the vocabulary given the left context (Wolf et al., 2020) and AllenNLP (Gardner et al., 2018). For of the original sequence, and select the probability implementation details, pretrained weights and hyperparameassigned by the model to the original word. Note ter values, cf. the documentation of the libraries. 3 Source: https://github.com/Tarpelite/ that this setup allows the models to know how many UniNLP/blob/master/examples/run_pos.py tokens there are in the sentences, and therefore the 4 Source: https://github.com/huggingface/ results are not directly comparable with those of transformers/blob/master/examples/ unidirectional models, that do not have any infor- text-classification/r"
2021.emnlp-main.118,2020.acl-demos.10,0,0.0185617,"ic capabilities of RoBERTa by means of the MiniBERTas models, a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on quantities of data ranging from 1M to 1B words. In particular: ∗ Work partially done during internship at Amazon AI. • We use the syntactic structural probes from Hewitt and Manning (2019b) to determine whether those models pretrained on more data encode a higher amount of syntactic information than those trained on less data; • We perform a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582 c November 7–11, 2021. 2021 Association for Computational Linguistics • We compare the performance of the different models on two morpho-syntactic tasks (PoS tagging and dependency parsing), and a nonsyntactic task (paraphrase identification); • We conduct a cost-benefit trade-off analysis (Strubell et al., 2019; Bhattacharjee et al., 2020) of the models training. We observe that models pretrained on more data encode a higher amou"
2021.emnlp-main.118,N18-1108,0,0.0218954,"and propose actionable recommendations to reduce costs and improve equity, namely 1) reporting training time and sensitivity to hyperparameters; 2) a government-funded academic compute cloud to provide equitable access to all researchers; and 3) prioritizing computationally efficient hardware and algorithms. The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). Regarding the evalu- 2.3 Related work ation of modern language models, Warstadt et al. Several studies investigate the relation between pre(2020a) present a challenge set that isolates specific training data size and linguistic knowledge in lanphenomena in syntax, morphology, and semantics, guage models. van Schijndel et al. (2019); Hu finding that state-of-the-art models struggle with et al. (2020); Micheli et al. (2020) find out that, some subtle semantic and syntactic phenomena, given a relatively large data size (e.g., 10M words), such as n"
2021.emnlp-main.118,2020.acl-main.158,0,0.256772,"a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on quantities of data ranging from 1M to 1B words. In particular: ∗ Work partially done during internship at Amazon AI. • We use the syntactic structural probes from Hewitt and Manning (2019b) to determine whether those models pretrained on more data encode a higher amount of syntactic information than those trained on less data; • We perform a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582 c November 7–11, 2021. 2021 Association for Computational Linguistics • We compare the performance of the different models on two morpho-syntactic tasks (PoS tagging and dependency parsing), and a nonsyntactic task (paraphrase identification); • We conduct a cost-benefit trade-off analysis (Strubell et al., 2019; Bhattacharjee et al., 2020) of the models training. We observe that models pretrained on more data encode a higher amount of syntax according to Hewitt and Manning (2019b)’s"
2021.emnlp-main.118,W19-4825,0,0.0274722,"Missing"
2021.emnlp-main.118,Q16-1037,0,0.0336098,"cessful neural network models for NLP, and propose actionable recommendations to reduce costs and improve equity, namely 1) reporting training time and sensitivity to hyperparameters; 2) a government-funded academic compute cloud to provide equitable access to all researchers; and 3) prioritizing computationally efficient hardware and algorithms. The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). Regarding the evalu- 2.3 Related work ation of modern language models, Warstadt et al. Several studies investigate the relation between pre(2020a) present a challenge set that isolates specific training data size and linguistic knowledge in lanphenomena in syntax, morphology, and semantics, guage models. van Schijndel et al. (2019); Hu finding that state-of-the-art models struggle with et al. (2020); Micheli et al. (2020) find out that, some subtle semantic and syntactic phenomena, given a relatively la"
2021.emnlp-main.118,N19-1112,0,0.0823389,"complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi e"
2021.emnlp-main.118,2021.ccl-1.108,0,0.0255846,"Missing"
2021.emnlp-main.118,D18-1151,0,0.346375,"ecommendations to reduce costs and improve equity, namely 1) reporting training time and sensitivity to hyperparameters; 2) a government-funded academic compute cloud to provide equitable access to all researchers; and 3) prioritizing computationally efficient hardware and algorithms. The targeted syntactic evaluation incorporates methods from psycholinguistic experiments, focusing on highly specific measures of language modeling performance and allowing to distinguish models with human-like representations of syntactic structure (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). Regarding the evalu- 2.3 Related work ation of modern language models, Warstadt et al. Several studies investigate the relation between pre(2020a) present a challenge set that isolates specific training data size and linguistic knowledge in lanphenomena in syntax, morphology, and semantics, guage models. van Schijndel et al. (2019); Hu finding that state-of-the-art models struggle with et al. (2020); Micheli et al. (2020) find out that, some subtle semantic and syntactic phenomena, given a relatively large data size (e.g., 10M words), such as negative polarity items an"
2021.emnlp-main.118,2020.emnlp-main.632,0,0.0659525,"Missing"
2021.emnlp-main.118,N18-1202,0,0.02345,"lin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-theoretical metrics such as perplexity, i.e., the probability of predicting a word in its context. The general wisdom is that the more pretraining data a model is fed, the lower its perplexity gets. However, large volumes of pretraining data are not always available and pretraining is costly, such that the following questions need to be answered: (i) Do we always need models pretrained on internetscale corpora"
2021.emnlp-main.118,2020.acl-main.420,0,0.0145241,"ave also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language models keep growing in orders of magnitude, so do the resources necessary for their development and, consequently, also the inclusivity gap. The financial cost of the required hardware and electricity favors industry-powered research, and harms academics, students, and nonindustry researchers, particularly those from emerging economies. Moreover, the training of such models is not only financially expensive, but has also a large carbon footprint. Schwartz et al. (2019) propose to report the financial cost of deve"
2021.emnlp-main.118,P18-2124,0,0.0275291,"a encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-t"
2021.emnlp-main.118,D17-1035,0,0.0147493,"als, mask, mask], and compare the surprisal of the model predicting a dot ‘.’ for the first masked position in each case. 3.5 Downstream applications To compare the performance of the models on downstream applications, we analyze their learning curves along the fine-tuning process on two morpho-syntactic tasks (PoS tagging and dependency parsing) and a non-syntactic task (paraphrase identification). Each task is fine-tuned for 3 epochs, with the default learning rate of 5e−5 . To mitigate the variance in performance induced by weight initialization and training data order (Dodge et al., 2020; Reimers and Gurevych, 2017), we repeat this process 5 times per task with different random seeds and average results.2 For PoS tagging, we fine-tune RoBERTa with a linear layer on top of the hidden-states output for token classification.3 Dataset: Universal Dependencies Corpus for English (UD 2.5 English EWT (Silveira et al., 2014)). For Dependency parsing, we fine-tune a Deep Biaffine neural dependency parser (Dozat and Manning, 2016). Dataset: UD 2.5 English EWT (Silveira et al., 2014). For Paraphrase identification, we fine-tune RoBERTa with a linear layer on top of the pooled sentence representation.4 Dataset: Micro"
2021.emnlp-main.118,2020.tacl-1.54,0,0.0173918,"onmental cost. 1 Introduction The use of unsupervised pretrained language models in the context of supervised tasks has become a widely spread practice in NLP, with Transformer-based models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b) achieving outstanding results in many well-known Natural Language Understanding benchmarks such as GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2018). Consequently, several studies investigate the types of knowledge learned by BERT, how and where this knowledge is represented and what the best methods to improve it are; see, e.g., (Rogers et al., 2020). There is evidence that, among other information (e.g., part-of-speech, syntactic chunks and roles (Tenney et al., 2019; Lin et al., 2019; Belinkov et al., 2017), morphology in general (Peters et al., 2018), or sentence length (Adi et al., 2016)), BERT representations implicitly embed entire syntax trees (Hewitt and Manning, 2019b). Language models are traditionally assessed by information-theoretical metrics such as perplexity, i.e., the probability of predicting a word in its context. The general wisdom is that the more pretraining data a model is fed, the lower its perplexity gets. However"
2021.emnlp-main.118,silveira-etal-2014-gold,0,0.0537333,"Missing"
2021.emnlp-main.118,P19-1355,0,0.316243,"form a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1571–1582 c November 7–11, 2021. 2021 Association for Computational Linguistics • We compare the performance of the different models on two morpho-syntactic tasks (PoS tagging and dependency parsing), and a nonsyntactic task (paraphrase identification); • We conduct a cost-benefit trade-off analysis (Strubell et al., 2019; Bhattacharjee et al., 2020) of the models training. We observe that models pretrained on more data encode a higher amount of syntax according to Hewitt and Manning (2019b)’s metrics, but do not always lead to a better syntactic generalization. Indeed, we find that models pretrained on less data perform equally good or even better than those pretrained on more data on 3 out of 6 syntactic test suites. When applied to downstream tasks, the models pretrained on more data perform generally better. However, the analysis of the trade-off between the cost of training a model and its performance sho"
2021.emnlp-main.118,D19-1592,0,0.0323774,"Missing"
2021.emnlp-main.118,2020.emnlp-main.14,0,0.015276,"m applications we explore. Section 4 presents the outcome of our experiments. Section 5 offers a cost-benefit analysis of the pretraining of the different models, and Section 6 summarizes the implications that our work has for the use of pretrained language models. 2 2.1 Background Syntactic assessment of language models suites, finding substantial differences in syntactic generalization performance by model architecture. Supervised probing models have also been used to test for the presence of a wide range of linguistic phenomena (Conneau et al., 2018; Liu et al., 2019a; Tenney et al., 2019; Voita and Titov, 2020; Elazar et al., 2020), and it has been shown that entire syntax trees are embedded implicitly in BERT’s vector geometry (Hewitt and Manning, 2019b; Chi et al., 2020). However, other works have criticized some probing methods, claiming that classifier probes can learn the linguistic task from training data (Hewitt and Liang, 2019), and can fail to determine whether the detected features are actually used (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). 2.2 Costs of modern language models While modern language models keep growing in orders of magnitude, so do the resources n"
2021.emnlp-main.118,W19-2304,0,0.0289405,"(Silveira et al., 2014). For Paraphrase identification, we fine-tune RoBERTa with a linear layer on top of the pooled sentence representation.4 Dataset: Microsoft Research Paraphrase Corpus (MRPC) The tests in SyntaxGym evaluate whether models are able to assign a higher probability to grammatical and natural continuations of sentences. As RoBERTa is a bidirectional model, to be able to ask it to predict the probability of a token given the context of previous tokens we test it in a left-to-right generative setup, as done in (Rongali et al., 2020; Zhu et al., 2020). More precisely, we follow Wang and Cho (2019)’s sequential sampling procedure, which is not affected by the error that was reported in equations 1-3, related to the Non-sequential sampling procedure. To compute the probability distribution for a sentence with N tokens, we start with a sequence of begin_of_sentence token plus N mask tokens plus an extra mask token to account for the end_of_sentence token. For each masked position in [1, N ], we compute the probability dis2 The implementation relies in the Transformers library tribution over the vocabulary given the left context (Wolf et al., 2020) and AllenNLP (Gardner et al., 2018). For"
2021.emnlp-main.118,W18-5446,0,0.0592616,"Missing"
2021.emnlp-main.118,2020.emnlp-main.16,0,0.183139,"ch that the following questions need to be answered: (i) Do we always need models pretrained on internetscale corpora? (ii) As the models are pretrained on more data, and their perplexity improves, do they encode more syntactic information and offer a better syntactic generalization? (iii) Do the models with more pretraining perform better when applied in downstream tasks? To address these questions, we explore the relation between the size of the pretraining data and the syntactic capabilities of RoBERTa by means of the MiniBERTas models, a set of 12 RoBERTa models pretrained from scratch by Warstadt et al. (2020b) on quantities of data ranging from 1M to 1B words. In particular: ∗ Work partially done during internship at Amazon AI. • We use the syntactic structural probes from Hewitt and Manning (2019b) to determine whether those models pretrained on more data encode a higher amount of syntactic information than those trained on less data; • We perform a targeted syntactic evaluation to analyze the generalization performance of the different models using SyntaxGym (Gauthier et al., 2020) and the syntactic tests presented in (Hu et al., 2020); 1571 Proceedings of the 2021 Conference on Empirical Metho"
2021.emnlp-main.118,W19-4819,0,0.0271355,"Missing"
2021.emnlp-main.118,W18-5423,0,0.0203788,"ithout a second clause less probable, and should make a second clause more probable. The circuit is composed of 4 Subordination tests from Futrell et al. (2018). 5. Licensing: Measures when a particular token must exist within the scope of an upstream licensor token. The circuit is composed of 4 Negative Polarity Item Licensing (NPI) tests and 6 Reflexive Pronoun Licensing tests, all from Marvin and Linzen (2018). 6. Long-Distance Dependencies: Measures covariations between two tokens that span long distances in tree depth. The circuit is composed of 6 Filler-Gap Dependencies (FGD) tests from Wilcox et al. (2018) and Wilcox et al. (2019b), and 2 Cleft tests from (Hu et al., 2020). 3.4 Encoding unidirectional context with bidirectional models mation regarding the length of the sequence. For example, in a Subordination test with the examples ‘Because the students did not like the material.’ and ‘The students did not like the material.’, we expect the model to assign a higher surprisal (Wilcox et al., 2019c) to the first example, because the initial ""Because"" implies that the immediately following clause is not the main clause of the sentence, but instead is a subordinate that must be followed by the mai"
2021.emnlp-main.118,N19-1334,1,0.901988,"sh finite present tense verbs. It is composed of 3 Subject-Verb Number Agreement tests from Marvin and Linzen (2018), 2. Center Embedding: Tests the ability to embed a phrase in the middle of another phrase of the same type. Subject and verbs must match in 3.2 Structural probing a first-in-last-out order, meaning models must apHewitt and Manning (2019b)’s structural probes as- proximate a stack-like data-structure in order to sess how well syntax trees are embedded in a linear successfully process them. The circuit is composed transformation of the network representation space of 2 tests from Wilcox et al. (2019a). applying two different evaluations: Tree distance 3. Garden-Path Effects: Measures the syntacevaluation, in which squared L2 distance encodes tic phenomena that result from tree structural ambi1 https://huggingface.co/nyu-mll guities that give rise to locally coherent but globally 1573 implausible syntactic parses. The circuit is composed of 2 Main Verb / Reduced Relative Clause (MVRR) tests and 4 NP/Z Garden-paths (NPZ) tests, all from Futrell et al. (2018). 4. Gross Syntactic Expectation: Tests the ability of the models to distinguish between coordinate and subordinate clauses: introduci"
2021.emnlp-main.118,2020.conll-1.40,0,0.020115,"d Manning, 2016). Dataset: UD 2.5 English EWT (Silveira et al., 2014). For Paraphrase identification, we fine-tune RoBERTa with a linear layer on top of the pooled sentence representation.4 Dataset: Microsoft Research Paraphrase Corpus (MRPC) The tests in SyntaxGym evaluate whether models are able to assign a higher probability to grammatical and natural continuations of sentences. As RoBERTa is a bidirectional model, to be able to ask it to predict the probability of a token given the context of previous tokens we test it in a left-to-right generative setup, as done in (Rongali et al., 2020; Zhu et al., 2020). More precisely, we follow Wang and Cho (2019)’s sequential sampling procedure, which is not affected by the error that was reported in equations 1-3, related to the Non-sequential sampling procedure. To compute the probability distribution for a sentence with N tokens, we start with a sequence of begin_of_sentence token plus N mask tokens plus an extra mask token to account for the end_of_sentence token. For each masked position in [1, N ], we compute the probability dis2 The implementation relies in the Transformers library tribution over the vocabulary given the left context (Wolf et al.,"
2021.emnlp-main.382,P19-1409,0,0.410005,"ngle pass, taking into account all prior coreference decisions at each step. Our contributions are: (1) we present the first study of sequential modeling for cross-document entity and event coreference and achieve competitive performance with a large reduction in computation time, (2) we conduct extensive ablation studies both on input information and model features, providing new insights for future models. 2 Related Work Although a few previous works attempt to use information about existing clusters through incremental construction (Yang et al., 2015; Lee et al., 2012) or argument sharing (Barhom et al., 2019; Choubey and Huang, 2017), these either continue to rely on pairwise decisions or use shallow, noncontextualized features that have limited efficacy. For example, Xu and Choi (2020) explore a variant of cluster merging for WD entity conreference only that still relies on scores between individual mentions. Additional recent work on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clus"
2021.emnlp-main.382,P10-1143,0,0.0413513,"del can better handle the temporal-component of many usage settings and is better suited to life-long learning. # Topics # Subtopics # Documents # Event Mentions # Entity Mentions # Event Clusters # Entity Clusters Train 25 50 574 3808 4758 1527 1286 Dev 8 16 196 1245 1476 409 330 Test 10 20 206 1780 2055 805 608 Table 1: Data Statistics for ECB+ corpus. Topics: train {1, 3, 4, 6-11, 13, 14, 16, 19-20, 22, 24-33}, development {2, 5, 12, 18, 21, 23, 34, 35}. and test 36-45 cross-document event and entity coreference. The ECB+ dataset is an extension of the Event Coreference Bank dataset (ECB) (Bejan and Harabagiu, 2010), which consists of news articles clustered into topics by seminal events (e.g., “6.1 earthquake Indonesia 2009”). The extension of ECB adds an additional seminal event to each topic (e.g., “6.1 earthquake Indonesia 2013”). Documents on each of the two seminal events then form subtopic clusters within each topic in ECB+. Following the recommendations of Cybulska and Vossen (2015b), we use only the subset of annotations that have been validated for correctness in our experiments (see Table 1). As a result, our results are comparable to recent studies (e.g., Barhom et al. (2019); Kenyon-Dean et"
2021.emnlp-main.382,P14-1005,0,0.0160023,"u et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) a"
2021.emnlp-main.382,2021.findings-emnlp.225,0,0.659001,"size 128, for the argument coreference coreference, updating the argument features features we use two learned embeddings of dimenfor events (and event features for entities) after sion df = 50, and for the multi-perspective cosine each iteration, similarity we use k = 1 projection layers with dimension 50 for entity coreference and k = 3 pro• Cattan et al. (2020) – general sequential jection layers with the same dimension for event WDCR model from Lee et al. (2017) paired coreference. We do not tune our hyperparameters. with agglomerative clustering We follow Barhom et al. (2019) and use K• Caciularu et al. (2021) – graph-based Bh2019 means to compute document clusters for inference model using representations from a Longfrom their implementation with K = 20. Specif2 https://catalog.ldc.upenn.edu/LDC2011T07 ically, as features, we use the TF-IDF scores of 4664 4.4 Baselines and Models unigrams, bigrams, and trigrams in the unfiltered dataset, excluding stop words. We select K = 20 as this is the number of gold document clusters in the test data but this can be modified without affecting our algorithm. 5 5.1 Results and Analyses Coreference Resolution drop in performance (i.e., the performance is identi"
2021.emnlp-main.382,W10-4305,0,0.0478682,"Missing"
2021.emnlp-main.382,D17-1226,0,0.0877483,"o account all prior coreference decisions at each step. Our contributions are: (1) we present the first study of sequential modeling for cross-document entity and event coreference and achieve competitive performance with a large reduction in computation time, (2) we conduct extensive ablation studies both on input information and model features, providing new insights for future models. 2 Related Work Although a few previous works attempt to use information about existing clusters through incremental construction (Yang et al., 2015; Lee et al., 2012) or argument sharing (Barhom et al., 2019; Choubey and Huang, 2017), these either continue to rely on pairwise decisions or use shallow, noncontextualized features that have limited efficacy. For example, Xu and Choi (2020) explore a variant of cluster merging for WD entity conreference only that still relies on scores between individual mentions. Additional recent work on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple docum"
2021.emnlp-main.382,D16-1245,0,0.0160213,"ention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) and only constructed coreference clusters computed across within-document, our"
2021.emnlp-main.382,W15-0801,0,0.0166238,"-11, 13, 14, 16, 19-20, 22, 24-33}, development {2, 5, 12, 18, 21, 23, 34, 35}. and test 36-45 cross-document event and entity coreference. The ECB+ dataset is an extension of the Event Coreference Bank dataset (ECB) (Bejan and Harabagiu, 2010), which consists of news articles clustered into topics by seminal events (e.g., “6.1 earthquake Indonesia 2009”). The extension of ECB adds an additional seminal event to each topic (e.g., “6.1 earthquake Indonesia 2013”). Documents on each of the two seminal events then form subtopic clusters within each topic in ECB+. Following the recommendations of Cybulska and Vossen (2015b), we use only the subset of annotations that have been validated for correctness in our experiments (see Table 1). As a result, our results are comparable to recent studies (e.g., Barhom et al. (2019); Kenyon-Dean et al. (2018); Meged et al. (2020)) but not earlier methods (see Upadhyay et al. (2016) for a more complete overview of evaluation settings). We use the standard partitions of the dataset into train, development and test split by topic and use subtopics (gold or predicted) for document clustering. 4.2 Identifying Event Structures The ECB+ dataset does not include relations between"
2021.emnlp-main.382,D12-1045,0,0.0228691,"onal Linguistics ence decisions in a single pass, taking into account all prior coreference decisions at each step. Our contributions are: (1) we present the first study of sequential modeling for cross-document entity and event coreference and achieve competitive performance with a large reduction in computation time, (2) we conduct extensive ablation studies both on input information and model features, providing new insights for future models. 2 Related Work Although a few previous works attempt to use information about existing clusters through incremental construction (Yang et al., 2015; Lee et al., 2012) or argument sharing (Barhom et al., 2019; Choubey and Huang, 2017), these either continue to rely on pairwise decisions or use shallow, noncontextualized features that have limited efficacy. For example, Xu and Choi (2020) explore a variant of cluster merging for WD entity conreference only that still relies on scores between individual mentions. Additional recent work on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a"
2021.emnlp-main.382,N19-1159,1,0.902288,"Missing"
2021.emnlp-main.382,D17-1018,0,0.175016,"into the importance of various inputs and representation types in coreference. 1 Introduction Relating entities and events in text is a key component of natural language understanding. For example, whether two news articles describing hurricanes are referring to the same hurricane event. A crucial component of answering such questions is reasoning about groups entities and events across multiple documents. The goal of coreference resolution is to compute these clusterings of entities or events from extracted spans of text. While within-document coreference has been studied extensively (e.g., Lee et al. (2017, 2018)), there has been relatively less work on the cross-document task. However, growing interest in multi-document applications, such as summarization (e.g., Liu and Lapata (2019); Fabbri et al. (2019)) and reading comprehension (e.g., Yan et al. (2019); Welbl et al. (2018)), highlights the importance of developing efficient and accurate ∗ Work done during internship at Amazon AI cross-document coreference models to minimize error-propagation in complex reasoning tasks. In this work we focus on cross-document coreference (CDCR), which implicitly requires withindocument coreference (WDCR), a"
2021.emnlp-main.382,N19-1423,0,0.191676,"nts, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) and only constructed coreference clusters computed across within-document, our work extends the sequential multiple documents. In the following sections, we paradigm to cross-document coreference and also will first describe our model for entity coreferenc"
2021.emnlp-main.382,N18-2108,0,0.104023,"-document event coreference. Our method is also able to take advantage of the history of previously made coreference decisions, approximating a higher-order model (i.e., operating on mentions as well as structures with mentions). Specifically, for every mention, a coreference decision is made not over a set of individual mentions but rather over the current state of coreference clusters. In this way, the model is able to use knowledge about the mentions currently in a cluster when making its decisions. While higher-order models have achieved state-of-the-art performance on entity coreference (Lee et al., 2018), they have been used infrequently for event coreference. For example, Yang et al. (2015) use one Chinese restaurant process for WDCR and then a second for CDCR over the within-document clusters. In contrast, our models make within- and cross-document corefer1 Note that this is different from mention-pair models, which generally refer to computing scores between all pairs of mentions, regardless of ordering. 4659 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4659–4671 c November 7–11, 2021. 2021 Association for Computational Linguistics ence deci"
2021.emnlp-main.382,D13-1203,0,0.0265475,"20) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequen"
2021.emnlp-main.382,P15-1033,1,0.719274,"vi , followed by mean-pooling. Finally, we combine the entity representation and event representations using an affine transformation to obtain the full mention representation hx . 3.3 Incremental Candidate Composition Let Le = {P1 , . . . , Pn } be a set of coreference clusters over the antecedents of mention xi . We compute a candidate cluster representation hP for each set P of coreferring entity antecedents in Le . In a similar manner to composition functions in neural dependency parsing, which incrementally combine head-word and modifier information to construct a subtree representation (Dyer et al., 2015, 2016; de Lhoneux et al., 2019), we incrementally combine document- and mention-level information to form a complete candidate cluster representation hP . That is, for each xj ∈ P , we combine hxj and hCLSj , the CLS token embedding from the document containing xj , using a non-linear transformation hcj = tanh(Wx hxj + WCLS hCLSj + bc ), (1) 3.4 Coreference Link Prediction We predict coreference links between a query entity mention x and a set of candidates by passing a set of similarity features through a softmax layer. Let Cx = {hP1 , . . . , hPm } be the set of m candidate representations"
2021.emnlp-main.382,N16-1024,1,0.881805,"Missing"
2021.emnlp-main.382,P19-1102,0,0.0282485,"hether two news articles describing hurricanes are referring to the same hurricane event. A crucial component of answering such questions is reasoning about groups entities and events across multiple documents. The goal of coreference resolution is to compute these clusterings of entities or events from extracted spans of text. While within-document coreference has been studied extensively (e.g., Lee et al. (2017, 2018)), there has been relatively less work on the cross-document task. However, growing interest in multi-document applications, such as summarization (e.g., Liu and Lapata (2019); Fabbri et al. (2019)) and reading comprehension (e.g., Yan et al. (2019); Welbl et al. (2018)), highlights the importance of developing efficient and accurate ∗ Work done during internship at Amazon AI cross-document coreference models to minimize error-propagation in complex reasoning tasks. In this work we focus on cross-document coreference (CDCR), which implicitly requires withindocument coreference (WDCR), and propose a new model that improves both coreference performance and computational complexity. Recent advances in within-document entity coreference resolution have shown that sequential prediction (i.e."
2021.emnlp-main.382,W12-4502,0,0.0186014,"020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and increme"
2021.emnlp-main.382,2020.acl-main.740,0,0.0528613,"Missing"
2021.emnlp-main.382,P18-2058,0,0.093145,"events and entities. Although prior work used the Swirl (Surdeanu et al., 2007) semantic role labeling (SRL) parser to extract predicate-argument structures, this does not take advantage of recent advances in SRL. In fact, prior works on coreference using ECB+ have added a number of additional rules on top of the parser output to improve its coverage and linking. For example, Barhom et al. (2019) used a dependency parser to identify additional mentions. Therefore, in this work we use the 4 Experimental Setup current state-of-the-art SRL parser on the standard 4.1 Data CoNLL-2005 shared task (He et al., 2018), which We conduct experiments using the ECB+ has improved performance by ∼10 F1 points both in- and out-of-domain. dataset (Cybulska and Vossen, 2014), the largest available dataset for both within-document and Following prior work, we restrict the event struc4663 ture to the following four argument roles: ARG0, ARG1, TIME, and LOC. However, we additionally add a type constraint during pre-processing that requires entities of type TIME and LOC only fill matching roles (TIME and LOC respectively). 4.3 Domain Adaptive Pre-training Since BERT was trained on the BooksCorpus and Wikipedia (Devlin"
2021.emnlp-main.382,P19-1500,0,0.0275107,"tanding. For example, whether two news articles describing hurricanes are referring to the same hurricane event. A crucial component of answering such questions is reasoning about groups entities and events across multiple documents. The goal of coreference resolution is to compute these clusterings of entities or events from extracted spans of text. While within-document coreference has been studied extensively (e.g., Lee et al. (2017, 2018)), there has been relatively less work on the cross-document task. However, growing interest in multi-document applications, such as summarization (e.g., Liu and Lapata (2019); Fabbri et al. (2019)) and reading comprehension (e.g., Yan et al. (2019); Welbl et al. (2018)), highlights the importance of developing efficient and accurate ∗ Work done during internship at Amazon AI cross-document coreference models to minimize error-propagation in complex reasoning tasks. In this work we focus on cross-document coreference (CDCR), which implicitly requires withindocument coreference (WDCR), and propose a new model that improves both coreference performance and computational complexity. Recent advances in within-document entity coreference resolution have shown that seque"
2021.emnlp-main.382,H05-1004,0,0.318787,"nsistent with prior work on the efficacy of sequential models (cf. mention-ranking models for WD entity coreference) (Martschat and Strube, 2015) and the importance of higher-order inference mechanisms (e.g., incremental candidate clustering) in cross-document tasks (Zhou et al., 2020). In addition, our results demonstrate the importance of algorithmic improvements, in addition to improvements in the underlying language model, for strong coreference performance. We evaluate using the standard metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF-e (Luo, 2005), and CoNLL F1 (their average). For entity coreference, our model outperforms most prior methods (see Table 2) and for event coreference our model demonstrates strong performance (see Table 3). Although Caciularu et al. (2021) achieve the highest performance, we observe first that much of their gains come from the use of a Longformer (80.4 CoNLL F1 for entities, 84.6 for events) – a language-model specifically designed for long contexts. Additionally, they fine-tune with coreference specific data and special tokens, neither of which our models use. As observed by Xu and Choi (2020), improvemen"
2021.emnlp-main.382,2020.findings-emnlp.318,1,0.745129,"g prior work, we restrict the event struc4663 ture to the following four argument roles: ARG0, ARG1, TIME, and LOC. However, we additionally add a type constraint during pre-processing that requires entities of type TIME and LOC only fill matching roles (TIME and LOC respectively). 4.3 Domain Adaptive Pre-training Since BERT was trained on the BooksCorpus and Wikipedia (Devlin et al., 2019) and the ECB+ dataset contains news articles, there is a domain mismatch. In addition, the use of a domain corpus for pre-training helps address the data scarcity issue for events and entities, indicated by Ma et al. (2020). Therefore, before training our coreference models, we first fine-tune BERT using the English Gigaword Corpus2 with both BERT losses, as this has been shown to be effective for domain transfer (Gururangan et al., 2020). Following Ma et al. (2020), we randomly sample 50k documents (626k sentences) and pre-train for 10k steps, using the hyperparameter settings from Devlin et al. (2019). former (Beltagy et al., 2020) with crossdocument attention during pre-training • Lemma – a strong baseline that links mentions with the same head-word lemma in the same document topic cluster (Barhom et al., 201"
2021.emnlp-main.382,Q15-1029,0,0.330556,", our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) and only constructed coreferen"
2021.emnlp-main.382,D07-1013,0,0.107879,"e of G in lines 9 and 15 in Figure 2. Further- prediction) and graph-based models (i.e., finding more, we use predicted topic clusters Tˆ, computed optimal connected components from a graph of using K-means (§4.5), in place of T . pairwise similarity scores). This dichotomy is Our model is trained to minimize cross-entropy analogous to that in dependency parsing between loss computed in batches. Here, all M entity men- transition-based parsers (i.e., greedy left-to-right 4662 models) and graph-based parsers. While the differences between the paradigms have been studied for dependency parsing (McDonald and Nivre, 2007, 2011), comparisons for coreference have been limited to WD entity coreference only (Martschat and Strube, 2015). In part, this is due to the usages of the two paradigms; sequential models are primarily used for WDCR while graph-based models are used for CDCR. However, as in dependency parsing, the sequential models can be made much more computationally efficient than the graph-based models as we show with our model. Let D be a set of documents with m mentions that form c coreference clusters. In a graph-based model, scores are computed between all pairs of mentions in all documents and in a"
2021.emnlp-main.382,J11-1007,0,0.0528846,"Missing"
2021.emnlp-main.382,2020.findings-emnlp.440,0,0.376758,"nd their whether two entity mentions refer to the same syntactic diversity (e.g., both verb phrases and real-world entity, with an analogous definition noun-phrases). Prior work on event coreference typically involves pairwise scoring between men- for event coreference. Formally, define an entity mention x = he, V i where e is an entity and V tions followed by a standard clustering algorithm to predict coreference links (Pandian et al., 2018; is a set of events in which e participates. We Choubey and Huang, 2017; Cremisini and Fin- adopt the definition of an event as “a specific layson, 2020; Meged et al., 2020; Yu et al., 2020b; occurrence of something that happens” (CybulCattan et al., 2020), classification over a fixed num- ska and Vossen, 2014). More specifically V = [ht1 , r1 i, . . . , htn , rn i] where ti is an event trigger ber of clusters (Kenyon-Dean et al., 2018) and template-based methods (Cybulska and Vossen, and ri ∈ R is the role e takes in in the event with 2015b,a). While pairwise scoring (e.g., graph- trigger ti , from a fixed set of argument roles. based models, see §3.7) with clustering is effective, it requires tuned thresholds (for the clustering al- 3.2 Entity Mention Represen"
2021.emnlp-main.382,P10-1142,0,0.0340348,"k on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution"
2021.emnlp-main.382,D19-1588,0,0.0111564,"alized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) and only constructed coreference clusters computed across within-document, our work extends the sequential multiple documents. In the following sections, we pa"
2021.emnlp-main.382,P19-1066,0,0.0189419,"across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) and only constructed coreference clusters computed across within-document, our work extends the sequential multiple documents."
2021.emnlp-main.382,D14-1162,0,0.0968153,"Missing"
2021.emnlp-main.382,S18-2001,0,0.532156,"eference. Formally, define an entity mention x = he, V i where e is an entity and V tions followed by a standard clustering algorithm to predict coreference links (Pandian et al., 2018; is a set of events in which e participates. We Choubey and Huang, 2017; Cremisini and Fin- adopt the definition of an event as “a specific layson, 2020; Meged et al., 2020; Yu et al., 2020b; occurrence of something that happens” (CybulCattan et al., 2020), classification over a fixed num- ska and Vossen, 2014). More specifically V = [ht1 , r1 i, . . . , htn , rn i] where ti is an event trigger ber of clusters (Kenyon-Dean et al., 2018) and template-based methods (Cybulska and Vossen, and ri ∈ R is the role e takes in in the event with 2015b,a). While pairwise scoring (e.g., graph- trigger ti , from a fixed set of argument roles. based models, see §3.7) with clustering is effective, it requires tuned thresholds (for the clustering al- 3.2 Entity Mention Representation gorithm) and cannot use already predicted scores To construct a representation for entity mention x, to inform later ones, since all scores are predicted we first embed the entity e, along with its context, independently. To the best of our knowledge, our as he"
2021.emnlp-main.382,D10-1048,0,0.0595872,"r prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links bet"
2021.emnlp-main.382,2020.emnlp-main.407,0,0.35944,"t the amount of information that can be used during training. Additionally, candidate composition allows sequential models to make use of information not only about antecedents (in contrast to the graph-based models) but also about prior coreference decisions (in contrast to non-compositional sequential models). Our results are consistent with prior work on the efficacy of sequential models (cf. mention-ranking models for WD entity coreference) (Martschat and Strube, 2015) and the importance of higher-order inference mechanisms (e.g., incremental candidate clustering) in cross-document tasks (Zhou et al., 2020). In addition, our results demonstrate the importance of algorithmic improvements, in addition to improvements in the underlying language model, for strong coreference performance. We evaluate using the standard metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF-e (Luo, 2005), and CoNLL F1 (their average). For entity coreference, our model outperforms most prior methods (see Table 2) and for event coreference our model demonstrates strong performance (see Table 3). Although Caciularu et al. (2021) achieve the highest performance, we observe first"
2021.emnlp-main.382,2020.emnlp-main.685,0,0.0301839,"rks attempt to use information about existing clusters through incremental construction (Yang et al., 2015; Lee et al., 2012) or argument sharing (Barhom et al., 2019; Choubey and Huang, 2017), these either continue to rely on pairwise decisions or use shallow, noncontextualized features that have limited efficacy. For example, Xu and Choi (2020) explore a variant of cluster merging for WD entity conreference only that still relies on scores between individual mentions. Additional recent work on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Dur"
2021.emnlp-main.382,C16-1183,0,0.0183904,"inal events (e.g., “6.1 earthquake Indonesia 2009”). The extension of ECB adds an additional seminal event to each topic (e.g., “6.1 earthquake Indonesia 2013”). Documents on each of the two seminal events then form subtopic clusters within each topic in ECB+. Following the recommendations of Cybulska and Vossen (2015b), we use only the subset of annotations that have been validated for correctness in our experiments (see Table 1). As a result, our results are comparable to recent studies (e.g., Barhom et al. (2019); Kenyon-Dean et al. (2018); Meged et al. (2020)) but not earlier methods (see Upadhyay et al. (2016) for a more complete overview of evaluation settings). We use the standard partitions of the dataset into train, development and test split by topic and use subtopics (gold or predicted) for document clustering. 4.2 Identifying Event Structures The ECB+ dataset does not include relations between events and entities. Although prior work used the Swirl (Surdeanu et al., 2007) semantic role labeling (SRL) parser to extract predicate-argument structures, this does not take advantage of recent advances in SRL. In fact, prior works on coreference using ECB+ have added a number of additional rules on"
2021.emnlp-main.382,M95-1005,0,0.789961,"t to non-compositional sequential models). Our results are consistent with prior work on the efficacy of sequential models (cf. mention-ranking models for WD entity coreference) (Martschat and Strube, 2015) and the importance of higher-order inference mechanisms (e.g., incremental candidate clustering) in cross-document tasks (Zhou et al., 2020). In addition, our results demonstrate the importance of algorithmic improvements, in addition to improvements in the underlying language model, for strong coreference performance. We evaluate using the standard metrics for coreference resolution: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF-e (Luo, 2005), and CoNLL F1 (their average). For entity coreference, our model outperforms most prior methods (see Table 2) and for event coreference our model demonstrates strong performance (see Table 3). Although Caciularu et al. (2021) achieve the highest performance, we observe first that much of their gains come from the use of a Longformer (80.4 CoNLL F1 for entities, 84.6 for events) – a language-model specifically designed for long contexts. Additionally, they fine-tune with coreference specific data and special tokens, neither of which our models"
2021.emnlp-main.382,Q18-1021,0,0.0288488,"urricane event. A crucial component of answering such questions is reasoning about groups entities and events across multiple documents. The goal of coreference resolution is to compute these clusterings of entities or events from extracted spans of text. While within-document coreference has been studied extensively (e.g., Lee et al. (2017, 2018)), there has been relatively less work on the cross-document task. However, growing interest in multi-document applications, such as summarization (e.g., Liu and Lapata (2019); Fabbri et al. (2019)) and reading comprehension (e.g., Yan et al. (2019); Welbl et al. (2018)), highlights the importance of developing efficient and accurate ∗ Work done during internship at Amazon AI cross-document coreference models to minimize error-propagation in complex reasoning tasks. In this work we focus on cross-document coreference (CDCR), which implicitly requires withindocument coreference (WDCR), and propose a new model that improves both coreference performance and computational complexity. Recent advances in within-document entity coreference resolution have shown that sequential prediction (i.e., making coreference predictions from left to right in a text1 ) achieves"
2021.emnlp-main.382,N16-1114,0,0.0223335,"e decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and Kuhn (2014); Martschat and Strube (2015); Wiseman et al. (2016); Clark and Manning (2016); Lee et al. (2018); Kantor and Globerson (2019)). Recently, Joshi et al. (2019) showed that 3 Methods pre-trained language models, in particular BERTlarge (Devlin et al., 2019), achieve state-of-the- 3.1 Overview and Task Definition art performance on entity coreference. In con- We propose a new sequential model for crossdocument coreference resolution (see Figure 1) that trast to prior work on entity coreference, which predicts links between mentions and incrementally is primarily sequential (i.e., left to right) and only constructed coreference clusters computed ac"
2021.emnlp-main.382,2020.emnlp-main.695,0,0.0405547,"Missing"
2021.emnlp-main.382,2020.emnlp-main.686,0,0.141114,"event coreference and achieve competitive performance with a large reduction in computation time, (2) we conduct extensive ablation studies both on input information and model features, providing new insights for future models. 2 Related Work Although a few previous works attempt to use information about existing clusters through incremental construction (Yang et al., 2015; Lee et al., 2012) or argument sharing (Barhom et al., 2019; Choubey and Huang, 2017), these either continue to rely on pairwise decisions or use shallow, noncontextualized features that have limited efficacy. For example, Xu and Choi (2020) explore a variant of cluster merging for WD entity conreference only that still relies on scores between individual mentions. Additional recent work on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pa"
2021.emnlp-main.382,Q15-1037,0,0.0192364,"previously made coreference decisions, approximating a higher-order model (i.e., operating on mentions as well as structures with mentions). Specifically, for every mention, a coreference decision is made not over a set of individual mentions but rather over the current state of coreference clusters. In this way, the model is able to use knowledge about the mentions currently in a cluster when making its decisions. While higher-order models have achieved state-of-the-art performance on entity coreference (Lee et al., 2018), they have been used infrequently for event coreference. For example, Yang et al. (2015) use one Chinese restaurant process for WDCR and then a second for CDCR over the within-document clusters. In contrast, our models make within- and cross-document corefer1 Note that this is different from mention-pair models, which generally refer to computing scores between all pairs of mentions, regardless of ordering. 4659 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4659–4671 c November 7–11, 2021. 2021 Association for Computational Linguistics ence decisions in a single pass, taking into account all prior coreference decisions at each step."
2021.emnlp-main.382,2020.lrec-1.2,0,0.163295,"clusters through incremental construction (Yang et al., 2015; Lee et al., 2012) or argument sharing (Barhom et al., 2019; Choubey and Huang, 2017), these either continue to rely on pairwise decisions or use shallow, noncontextualized features that have limited efficacy. For example, Xu and Choi (2020) explore a variant of cluster merging for WD entity conreference only that still relies on scores between individual mentions. Additional recent work on WD coreference investigates incremental construction of clusters for prediction (Xia et al., 2020; Toshniwal et al., 2020) and cluster ranking (Yu et al., 2020a) In contrast, our method makes coreference decisions between a mention and all existing coreference clusters across multiple documents using contextualized features and so takes advantage of interdependencies between mentions, even across documents, while making all decisions in one pass. Prior work on coreference resolution is generally split into either entity or event coreference. Entity coreference is relatively well studied (Ng, 2010), with the largest focus on within-document coreference (e.g., Raghunathan et al. (2010); Fernandes et al. (2012); Durrett and Klein (2013); Björkelund and"
2021.emnlp-main.382,2020.coling-main.275,0,0.82834,"tagy et al., 2020) with crossdocument attention during pre-training • Lemma – a strong baseline that links mentions with the same head-word lemma in the same document topic cluster (Barhom et al., 2019). For event coreference we additionally compare to: • Meged et al. (2020) – graph-based Bh2019 model with an additional paraphrase-based feature, • Cremisini and Finlayson (2020) – graph-based prediction of coreference scores over four types of similarity features, • three graph-based representation learning models with agglomerative clustering – Kenyon-Dean et al. (2018), Yu et al. (2020b) and Zeng et al. (2020). 4.5 Implementation Details Our models are tuned for a maximum of 80 epochs with early-stopping on the development set (using We experiment with the following baseline vari- CoNLL F1) with a patience of 20 epochs. All models are optimized using Adam (Kingma and Ba, ations of our model: BERT-SeqWD – computes coreference scores using only the entity (or event) 2015) with a learning rate of 2e-5 and treat all menrepresentations, without any cross-document link- tions in a document as a batch. We clip gradients to 30 to prevent exploding gradients. Document ing, and BERT-SeqXdoc – computes corefe"
2021.emnlp-main.631,2020.semeval-1.45,0,0.0275106,"n did not seem to be a problem, and that BART was able to handle multiple negations very well. Therefore marking negation scopes could have introduced unneeded noise into the model, causing the observed performance drop. 3.5 Reasoning conduct such reasoning by relying only on the reference summaries (this difficulty is exacerbated by the fact that SAMSum is of a relatively small size). Multi-task learning (MTL) enables knowledge transfer across relevant tasks. For instance Li et al. (2019) improved their summarization performance by jointly learning summarization and topic segmentation. Also, Konar et al. (2020) improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table 7 in the Appendix A). We use the ROC stories dataset (Mostafazadeh et al., 2016). • Commonsense Generat"
2021.emnlp-main.631,2020.acl-main.703,0,0.0365465,"MSum, a benchmark for abstractive everyday dialogue summarization. Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 3 Challenges 3.1 Experimental Setup For all our experiments, we use BART large architecture (Lewis et al., 2020).1 All our experiments are run using fairseq (Ott et al., 2019). 3.2 Baselines • Vanilla BART: Fine-tuning the original BART large checkpoint model on SAMSum. • Multi-view Seq2Seq (Chen and Yang, 2020) : This is based on BART, as well, but during the summarization, the model considers multiple views, each of which defines a certain structure for the dialogue. We compare to their best model which combines topic and stage views. 3.3 Multiple Speakers We hypothesize that uncommon (less frequent in the original pretraining data) or new names could be an issue to a pretrained model, especially if s"
2021.emnlp-main.631,P19-1210,0,0.0213006,"investigate the negation challenge dialogues put together in (Chen and Yang, 2020). We found that in all examples, negation did not seem to be a problem, and that BART was able to handle multiple negations very well. Therefore marking negation scopes could have introduced unneeded noise into the model, causing the observed performance drop. 3.5 Reasoning conduct such reasoning by relying only on the reference summaries (this difficulty is exacerbated by the fact that SAMSum is of a relatively small size). Multi-task learning (MTL) enables knowledge transfer across relevant tasks. For instance Li et al. (2019) improved their summarization performance by jointly learning summarization and topic segmentation. Also, Konar et al. (2020) improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See exam"
2021.emnlp-main.631,2020.findings-emnlp.165,0,0.0186249,"multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table 7 in the Appendix A). We use the ROC stories dataset (Mostafazadeh et al., 2016). • Commonsense Generation: Generative commonsense reasoning (Lin et al., 2020) is a task involving generating an everyday scenario description given basic concepts. We assume such task could help the model reason more about conversations, which is certainly needed in many dialogues (see example 3 in Table 7 in Appendix A). • Commonsense Knowledge Base Construction: The task here is to generate relation triplets similar to (Bosselut et al., 2019). More specifically, we train our model to predict relation objects given both relation and subject. We use ConceptNet (Liu and Singh, 2004). Table 4 shows the summarization performance after multi-task fine-tuning of BART. We al"
2021.emnlp-main.631,S12-1035,0,0.0264525,"observe that the more participants in the summary, the more effect this technique has. Notably, the average number of speakers per dialogue in SAMSum is only ~2.4. and we expect name substitution to work even better with datasets that have many more speakers per dialogue. 3.4 Negation Understanding Chen and Yang (2020) argue that negations represent a challenge for dialogues. We experiment with marking negation scopes in the input dialogues before feeding them to BART. To do that, we fine-tune a RoBERTa base model on the CD-SCO dataset from SEM Shared Task 2012 for negation scope prediction (Morante and Blanco, 2012). Then, we mark negation scope using two designated special tokens to mark the start and the end of the negation scope. For example, the sentence “I don’t know what to do” becomes “I don’t <NEG> know what to do <NEG>” after negation scope highlighting. We initialize the embeddings of the special tokens <NEG> and <NEG> randomly. Results are shown in Table 3. While we expected to see a performance boost due to negation scope highlighting, we actually saw a performance drop except on ROUGE-L on the test set. To understand why, we investigate the negation challenge dialogues put together in (Che"
2021.emnlp-main.631,N16-1098,0,0.022276,"and topic segmentation. Also, Konar et al. (2020) improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table 7 in the Appendix A). We use the ROC stories dataset (Mostafazadeh et al., 2016). • Commonsense Generation: Generative commonsense reasoning (Lin et al., 2020) is a task involving generating an everyday scenario description given basic concepts. We assume such task could help the model reason more about conversations, which is certainly needed in many dialogues (see example 3 in Table 7 in Appendix A). • Commonsense Knowledge Base Construction: The task here is to generate relation triplets similar to (Bosselut et al., 2019). More specifically, we train our model to predict relation objects given both relation and subject. We use ConceptNet (Liu and Singh, 2004). Table 4"
2021.emnlp-main.631,N19-4009,0,0.0179545,". Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 3 Challenges 3.1 Experimental Setup For all our experiments, we use BART large architecture (Lewis et al., 2020).1 All our experiments are run using fairseq (Ott et al., 2019). 3.2 Baselines • Vanilla BART: Fine-tuning the original BART large checkpoint model on SAMSum. • Multi-view Seq2Seq (Chen and Yang, 2020) : This is based on BART, as well, but during the summarization, the model considers multiple views, each of which defines a certain structure for the dialogue. We compare to their best model which combines topic and stage views. 3.3 Multiple Speakers We hypothesize that uncommon (less frequent in the original pretraining data) or new names could be an issue to a pretrained model, especially if such names were seen very few times, or not at all, during pretr"
2021.emnlp-main.631,P18-1062,0,0.0197847,"and multi-tasked. The summary generated by the vanilla model indicates that the rat is the cheater, pointing to a lack of commonsense reasoning on the model side. The output of our multi-tasked model (section 3.5) clearly shows better understanding of the dialogue. We compare our techniques to two summarization baselines: 2 Related Work Early work on dialogue summarization focused more on extractive than abstractive techniques for summarization of meetings (Murray et al., 2005; Riedhammer et al., 2008) or random conversations (Murray and Renals, 2007). In the context of meeting summarization, Shang et al. (2018) proposed an unsupervised graph-based sentence compression approach for meeting summarization on the AMI (McCowan et al., 2005) and ICSI (Janin et al., 2003) benchmarks. Goo and Chen (2018) leveraged hidden representations from a dialogue act classifier through a gated attention mechanism to guide the summary decoder. More recently, Gliwa et al. (2019) proposed SAMSum, a benchmark for abstractive everyday dialogue summarization. Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang"
2021.emnlp-main.631,J02-4003,0,0.297889,"zer model must keep track of the different lines of thoughts of individual speakers, distinguish salient from non-salient utterances, and finally produce a coherent, monologue summary of the dialogue. Dialogues usually include unfinished sentences where speakers were interrupted or repetitions, where a speaker expresses their thoughts more than once and possibly in different styles. Moreover, a single dialogue could touch on many topics without a clear boundary between the different topics. All the aforementioned phenomena certainly add to the difficulty of the task (Zechner and Waibel, 2000; Zechner, 2002; Chen and Yang, 2020). Our work focuses on SAMSum (Gliwa et al., 2019), which is a dialogue summarization dataset comprised of ~16K everyday dialogues with their ∗ We propose a combination of techniques to tackle a set of dialogue summarization challenges. The first challenge is having multiple speakers (generally, more than 2), where it becomes harder for the model to keep track of different utterances and determine their saliency. The second challenge is multiple negations, which is thought by Chen and Yang (2020) to pose some difficulty to dialogue understanding. The third of these challen"
2021.emnlp-main.631,C00-2140,0,0.319141,"n many speakers, a summarizer model must keep track of the different lines of thoughts of individual speakers, distinguish salient from non-salient utterances, and finally produce a coherent, monologue summary of the dialogue. Dialogues usually include unfinished sentences where speakers were interrupted or repetitions, where a speaker expresses their thoughts more than once and possibly in different styles. Moreover, a single dialogue could touch on many topics without a clear boundary between the different topics. All the aforementioned phenomena certainly add to the difficulty of the task (Zechner and Waibel, 2000; Zechner, 2002; Chen and Yang, 2020). Our work focuses on SAMSum (Gliwa et al., 2019), which is a dialogue summarization dataset comprised of ~16K everyday dialogues with their ∗ We propose a combination of techniques to tackle a set of dialogue summarization challenges. The first challenge is having multiple speakers (generally, more than 2), where it becomes harder for the model to keep track of different utterances and determine their saliency. The second challenge is multiple negations, which is thought by Chen and Yang (2020) to pose some difficulty to dialogue understanding. The third o"
2021.emnlp-main.631,P18-1205,0,0.0265998,"1, ROUGE-2, and ROUGE-L on SAMSum with and without names substitution. Results are shown on the validation and test splits from (Gliwa et al., 2019). Figure 1: ROUGE values against the number of participants per dialogue on the development set of SAMSum. Performance boost is more clear in dialogues with more participants logue domain. Therefore, we adapt BART to dialogue inputs by further pretraining of BART on a dialogue corpus and with dialogue-specific objectives. 4 3.6.1 Pretraining Corpora We consider the following 2 corpora for further pretraining of BART: PersonaChat (140K utterances) (Zhang et al., 2018), and a collection of 12M Reddit comments. We experiment with both whole word masking and span masking (masking random contiguous tokens). Our experimental setup is described in the Appendix in section B.1. Table 5 shows the results of fine-tuning BART pretrained on dialogue corpora.5 The best model (PersonaChat, word masking) outperforms the vanilla BART on all metrics and the Multiview SS baseline on test set ROUGE-2 and ROUGEL. We can see that in general, BART pretrained on PersonaChat is better than pretraining on both PersonaChat and Reddit, which is surprising since more pretraining data"
2021.emnlp-main.631,2020.coling-main.39,0,0.0429498,"meetings (Murray et al., 2005; Riedhammer et al., 2008) or random conversations (Murray and Renals, 2007). In the context of meeting summarization, Shang et al. (2018) proposed an unsupervised graph-based sentence compression approach for meeting summarization on the AMI (McCowan et al., 2005) and ICSI (Janin et al., 2003) benchmarks. Goo and Chen (2018) leveraged hidden representations from a dialogue act classifier through a gated attention mechanism to guide the summary decoder. More recently, Gliwa et al. (2019) proposed SAMSum, a benchmark for abstractive everyday dialogue summarization. Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 3 Challenges 3.1 Experimental Setup For all our experiments, we use BART large architecture (Lewis et al., 2020).1 All our experiments are run using fairseq (Ott et al., 2019). 3"
2021.naacl-main.65,W13-2322,0,0.0139934,"ncy statistics, and rich metadata. that simply contains a list of names without any other information. Sil et al. (2012) use databaseagnostic features to link against arbitrary databases. However, their approach still requires training data from the target KB. In contrast, this work aims to train entity linking models that do not rely on training data from the target KB, and can be trained on arbitrary KBs, and applied to a different set of KBs. Pan et al. (2015) also do unsupervised entity linking by generating rich context representations for mentions using Abstract Meaning Representations (Banarescu et al., 2013), followed by unsupervised graph inference to compare contexts. They assume a rich target KB that can be converted to a connected graph. This works for Wikipedia and adjacent resources but not for arbitrary KBs. Logeswaran et al. (2019) introduce a novel zeroshot framework to “develop entity linking systems that can generalize to unseen specialized entities"". Table 1 summarizes differences between our framework and those from prior work. Contextualized Representations for Entity Linking Models in this work are based on BERT (Devlin et al., 2019). While many studies have tried to explain the ef"
2021.naacl-main.65,P15-2049,0,0.0736761,"Missing"
2021.naacl-main.65,P15-1033,1,0.764967,"zation Schemes for Improving Generalization Building models for entity linking against unseen KBs requires that such models do not overfit to the training data by memorizing characteristics of the training KB. This is done by using two regularization schemes that we apply on top of the candidate string generation techniques discussed in the previous section. The first scheme, which we call attribute-OOV, prevents models from overtly relying on individual [Ki ] tokens and generalize to attributes that are not seen during training. Analogous to how out-of-vocabulary tokens are commonly handled (Dyer et al., 2015, inter alia), every [Ki ] token is stochastically replaced with the [SEP] token during training with probability pdrop . This encourages the model to encode semantics of the attributes in not only the [Ki ] tokens, but also in the [SEP] token, which is used when unseen attributes are encountered during inference. The second regularization scheme discourages the model from memorizing the order in which particular attributes occur. Under attribute-shuffle, every time an entity is encountered during training, its attribute/values are randomly shuffled before it is converted to a string represent"
2021.naacl-main.65,D11-1072,0,0.0750474,"le, every time an entity is encountered during training, its attribute/values are randomly shuffled before it is converted to a string representation using the techniques from Section 4.1. 5 5.1 Experiments and Discussion Data Our held-out test bed is the TAC-KBP 2010 data (LDC2018T16) which consists of documents from English newswire, discussion forum and web data (Ji et al., 2010).4 The target KB (KB test ) is the TAC-KBP Reference KB and is built from English Wikipedia articles and their associated infoboxes (LDC2014T16).5 Our primary training and validation data is the CoNLL-YAGO dataset (Hoffart et al., 2011), which consists of documents from the CoNLL 2003 Named Entity Recognition task (Tjong Kim Sang and De Meulder, 2003) linked 4 https://catalog.ldc.upenn.edu/ LDC2018T16 5 https://catalog.ldc.upenn.edu/ LDC2014T16 Number of mentions Size of target KB CoNLL-YAGO (train) CoNLL-YAGO (val.) 18.5K 4.8K 5.7M Wikia (train) Wikia (val.) 49.3K 10.0K 0.5M 1.7K 0.8M TAC KBP 2010 (test) Table 2: Number of mentions in our training, validation, and test sets, along with the number of entities in their respective KBs. to multiple KBs.6 To ensure that our training and target KBs are different, we use Wikidata"
2021.naacl-main.65,P83-1022,0,0.249737,"Missing"
2021.naacl-main.65,P17-2085,0,0.0433025,"Missing"
2021.naacl-main.65,Q15-1023,0,0.0288401,"ple training datasets which link to different KBs with different schemas. We investigate the impact of training on multiple datasets in two sets of experiments involving additional training data that links to (a) a third KB that is different from our original training and testing KBs, and (b) the same KB as the test data. These experiments reveal that our models perform favorably under all conditions compared to baselines. 2 Background Conventional entity linking models are trained and evaluated on the same KB, which is typically Wikipedia, or derived from Wikipedia (Bunescu and Pa¸sca, 2006; Ling et al., 2015). This limited scope allows models to use other sources of information to improve linking, including alias tables, frequency statistics, and rich metadata. that simply contains a list of names without any other information. Sil et al. (2012) use databaseagnostic features to link against arbitrary databases. However, their approach still requires training data from the target KB. In contrast, this work aims to train entity linking models that do not rely on training data from the target KB, and can be trained on arbitrary KBs, and applied to a different set of KBs. Pan et al. (2015) also do uns"
2021.naacl-main.65,P19-1335,0,0.16151,"ll have training data of this nature, i.e. mentions of products linked to its database. Our focus is on linking entities to unseen KBs with arbitrary schemas. One solution is to annotate data that can be used to train specialized models for each target KB of interest, but this is not scalable. A more generic solution is to build entity linking models that work with arbitrary KBs. We follow this latter approach and build entity linking models that link to target KBs that have not been observed during training.1 Our solution builds on recent models for zero-shot entity linking (Wu et al., 2020; Logeswaran et al., 2019). However, these models assume the same, simple KB schema during training and inference. We generalize these models to handle different KBs during training and inference, containing entities represented with an arbitrary set of attribute-value pairs. This generalization relies on two key ideas. First, we convert KB entities into strings that are consumed by the models for zero-shot linking. Central to the string representation are special tokens called attribute separators, which represent frequently occurring attributes in the training KB(s), and carry over their knowledge to unseen KBs durin"
2021.naacl-main.65,L16-1528,0,0.0270862,"codes knowledge of entities. This has also been shown empirically by many works that use BERT and other contextualized models for entity linking and disambiguation (Broscheit, 2019; Shahbazi et al., 2019; Yamada et al., 2020; Févry et al., 2020; Poerner et al., 2020). Beyond Conventional Entity Linking There have been several attempts to go beyond such con- 3 Preliminaries ventional settings, e.g. by linking to KBs from 3.1 Entity Linking Setup diverse domains such as the biomedical sciences (Zheng et al., 2014; D’Souza and Ng, 2015) and Entity linking consists of disambiguating entity music (Oramas et al., 2016) or even being com- mentions M from one or more documents to a pletely domain and language independent (Wang target knowledge base, KB, containing unique et al., 2015; Onoe and Durrett, 2020). Lin et al. entities. We assume that each entity e ∈ KB (2017) discuss approaches to link entities to a KB is represented using a set of attribute-value pairs 835 {(ki , vi )}ni=1 . The attributes ki collectively form the schema of KB. The disambiguation of each m ∈ M is aided by the context c in which m appears. Models for entity linking typically consist of two stages that balance recall and precision."
2021.naacl-main.65,N15-1119,0,0.0248227,"sca, 2006; Ling et al., 2015). This limited scope allows models to use other sources of information to improve linking, including alias tables, frequency statistics, and rich metadata. that simply contains a list of names without any other information. Sil et al. (2012) use databaseagnostic features to link against arbitrary databases. However, their approach still requires training data from the target KB. In contrast, this work aims to train entity linking models that do not rely on training data from the target KB, and can be trained on arbitrary KBs, and applied to a different set of KBs. Pan et al. (2015) also do unsupervised entity linking by generating rich context representations for mentions using Abstract Meaning Representations (Banarescu et al., 2013), followed by unsupervised graph inference to compare contexts. They assume a rich target KB that can be converted to a connected graph. This works for Wikipedia and adjacent resources but not for arbitrary KBs. Logeswaran et al. (2019) introduce a novel zeroshot framework to “develop entity linking systems that can generalize to unseen specialized entities"". Table 1 summarizes differences between our framework and those from prior work. Co"
2021.naacl-main.65,2020.tacl-1.54,0,0.0140671,"ference to compare contexts. They assume a rich target KB that can be converted to a connected graph. This works for Wikipedia and adjacent resources but not for arbitrary KBs. Logeswaran et al. (2019) introduce a novel zeroshot framework to “develop entity linking systems that can generalize to unseen specialized entities"". Table 1 summarizes differences between our framework and those from prior work. Contextualized Representations for Entity Linking Models in this work are based on BERT (Devlin et al., 2019). While many studies have tried to explain the effectiveness of BERT for NLP tasks (Rogers et al., 2020), the work by Tenney et al. (2019) is most relevant as they use probing tasks to show that BERT encodes knowledge of entities. This has also been shown empirically by many works that use BERT and other contextualized models for entity linking and disambiguation (Broscheit, 2019; Shahbazi et al., 2019; Yamada et al., 2020; Févry et al., 2020; Poerner et al., 2020). Beyond Conventional Entity Linking There have been several attempts to go beyond such con- 3 Preliminaries ventional settings, e.g. by linking to KBs from 3.1 Entity Linking Setup diverse domains such as the biomedical sciences (Zhen"
2021.naacl-main.65,D12-1011,0,0.0485444,"Missing"
2021.naacl-main.65,P19-1139,0,0.0586146,"Missing"
2021.naacl-main.65,P19-1452,0,0.0203964,"assume a rich target KB that can be converted to a connected graph. This works for Wikipedia and adjacent resources but not for arbitrary KBs. Logeswaran et al. (2019) introduce a novel zeroshot framework to “develop entity linking systems that can generalize to unseen specialized entities"". Table 1 summarizes differences between our framework and those from prior work. Contextualized Representations for Entity Linking Models in this work are based on BERT (Devlin et al., 2019). While many studies have tried to explain the effectiveness of BERT for NLP tasks (Rogers et al., 2020), the work by Tenney et al. (2019) is most relevant as they use probing tasks to show that BERT encodes knowledge of entities. This has also been shown empirically by many works that use BERT and other contextualized models for entity linking and disambiguation (Broscheit, 2019; Shahbazi et al., 2019; Yamada et al., 2020; Févry et al., 2020; Poerner et al., 2020). Beyond Conventional Entity Linking There have been several attempts to go beyond such con- 3 Preliminaries ventional settings, e.g. by linking to KBs from 3.1 Entity Linking Setup diverse domains such as the biomedical sciences (Zheng et al., 2014; D’Souza and Ng, 20"
2021.naacl-main.65,D18-1270,0,0.0562127,"Missing"
2021.naacl-main.65,D15-1081,0,0.0575908,"Missing"
2021.naacl-main.65,2020.emnlp-main.519,0,0.506698,"at the company will have training data of this nature, i.e. mentions of products linked to its database. Our focus is on linking entities to unseen KBs with arbitrary schemas. One solution is to annotate data that can be used to train specialized models for each target KB of interest, but this is not scalable. A more generic solution is to build entity linking models that work with arbitrary KBs. We follow this latter approach and build entity linking models that link to target KBs that have not been observed during training.1 Our solution builds on recent models for zero-shot entity linking (Wu et al., 2020; Logeswaran et al., 2019). However, these models assume the same, simple KB schema during training and inference. We generalize these models to handle different KBs during training and inference, containing entities represented with an arbitrary set of attribute-value pairs. This generalization relies on two key ideas. First, we convert KB entities into strings that are consumed by the models for zero-shot linking. Central to the string representation are special tokens called attribute separators, which represent frequently occurring attributes in the training KB(s), and carry over their kno"
ballesteros-nivre-2012-maltoptimizer-system,N07-1050,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,D07-1013,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W04-2407,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,D07-1097,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,nivre-etal-2006-maltparser,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,E12-2012,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W06-2920,0,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W04-0308,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W09-3811,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W02-1002,0,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,J08-4003,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,P05-1012,0,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,C10-1093,0,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,P09-1040,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,P06-1055,0,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W06-2932,0,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,P05-1013,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,D07-1096,1,\N,Missing
ballesteros-nivre-2012-maltoptimizer-system,W03-3017,1,\N,Missing
C14-1076,P11-2123,0,0.0146111,"te of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some recent research on trying to manually find better feature models for dependency parsers, such as Nivre et al. (2006), Hall et al. (2007), Hall (2008), Zhang and Nivre (2011), and Agirre et al. (2011). There is also research on automatic feature selection in the case of transition-based dependency parsing, a good example is MaltOptimizer (Ballesteros and Nivre, 2014) which implements a search for the best feature model that it can find, following acquired previous experience and deep linguistic knowledge (Hall et al., 2007; Nivre and Hall, 2010); Nilsson and Nugues (2010) also tried to search for optimal feature sets in the case of transition-based parsing, starting from a reduced test set using the concept of topological neighbors. Finally, He He et al. (2013) also tried automatic feature"
C14-1076,W13-4907,1,0.80843,"Finding an optimal and accurate set of feature templates is crucial when training statistical parsers; in fact it is essential when building any machine learning system (Smith, 2011). In dependency parsing, the features are based on the linguistic information that is annotated within the words and the information that is being calculated during the parsing process. Researchers normally tend to include a large set of feature templates in their machine learning models, following the idea that more is always better; however some recent research on feature selection for transition-based parsing (Ballesteros, 2013; Ballesteros and Nivre, 2014) and graph-based parsing (He et al., 2013) have shown that more features are not always better, at least in the case of dependency parsing; models containing more features are always slower in parsing and training time and they do not always provide better results. This indicates that a smart feature template selection could be the key in the trade-off for finding an accurate and fast feature model for a given parsing model. On the one hand, we want a parser that should provide the best results possible, while on the other hand, we want a parser that should provid"
C14-1076,C00-2143,0,0.0333958,"used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative syntactic structures that are considered in the parsing process, and thus it requires more time and memory while it normally provides better res"
C14-1076,boguslavsky-etal-2002-development,0,0.025467,"bank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative syntactic structures that are considered in the parsing process, and thus it requires more time and memory while it normally provides better results. The parser uses two a"
C14-1076,E12-1009,1,0.868382,"β). 2 2.1 Related Work Mate Parser For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The drawback of such a large feature set is a huge impact on the speed. Important research questions include (1) whether the number of features could be reduced to speed up the parser and (2) whether languages dependent feature sets would be ben"
C14-1076,D12-1133,1,0.900663,"Missing"
C14-1076,Q13-1034,1,0.64751,"e fastest way possible. For practical applications, a fast model is crucial. In this paper, we report the results of feature selection experiments that we carried out with the intention of obtaining accurate and faster feature models, for the transition-based Mate parser with and without graph-based completion models. The Mate parser is a beam search parser that uses a hash kernel for training, joint part-of-speech tagging, morphological tagging and dependency parsing. As a result of this research, we provide a framework that allows to find an optimal feature template set for the Mate parser (Bohnet et al., 2013). Moreover, our models provide some of the highest results ever reported for a set of treebanks. The paper is organized as follows. Section 2 describes related work including the used agenda-based dependency parser. This section depicts the feature templates that can be used by a transition-based or a graph-based parser. Section 3 describes the feature selection algorithm that we implemented for our experiments. Section 4 shows the experimental set-up. Section 5 reports the main results of our experiments. Section 6 provides the parsing times and memory requirements. Finally, Section 7 conclud"
C14-1076,W08-2102,0,0.217725,"Missing"
C14-1076,D07-1101,0,0.0325551,"sentence. Frequently, dynamic programming techniques are used to find the optimal tree for each span, considering candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to decide among alternative spans. The typical feature models are based on combinations of edges (as known as, factors). A factor consists either of a single edge, two or three edges; which are called first order, second and third order factors, respectively. The later are employed in more advanced and recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the 795 vertexes involved in the factors. A feature template of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some recent research"
C14-1076,P04-1015,0,0.0618449,"σ|j], [i|β], Γ) i 6= 0 0&lt;i&lt;j Figure 1: Transition set for joint morphological and syntactic analysis. The stack Σ is represented as a list with its head to the right (and tail σ) and the buffer B as a list with its head to the left (and tail β). 2 2.1 Related Work Mate Parser For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The d"
C14-1076,E12-1007,0,0.0128836,"We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative syntactic structures that are considered in the parsing process, and thus it"
C14-1076,I11-1136,0,0.0137306,"ng model with the model with selected transition-based and graph-based features, we observe a parsing time of 0.066 seconds per sentence and a small accuracy difference of only 0.27. If we use six CPU cores then parsing time decreases drastically to 0.016 seconds per sentence for the selected transition-based feature model, 0.023 for the selected transition- and graph-based feature model and to 0.05 seconds per sentence for the model with all features (which is much slower). Our experiments 801 Parser UAS POS MSTParser1 75.56 93.51 MSTParser2 77.73 93.51 Li et al. (2011) 3rd-order 80.60 92.80 Hatori et al. (2011) HS 79.60 94.01 Hatori et al. (2011) ZN 81.20 93.94 this work (sel. trans.) 81.20 94.17 this work (sel. trans.+ sel. cmp.) 81.77 94.28 Parser UAS LAS POS McDonald et al. (2005a) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (2010) 93.26 Bohnet and Nivre (2012) 93.38 92.44 97.33 this work (sel. trans.& sel. cmpl.) 93.05 92.08 97.44 this work (P&M cf. Table 3) 93.49 92.53 – Koo et al. (2008) † 93.16 Carreras et al. (2008) † 93.5 Suzuki et al. (2009) † 93.79 (b) Accuracy scores for the Chinese treebank con"
C14-1076,D13-1152,0,0.0813251,"Missing"
C14-1076,P10-1110,0,0.143518,"Missing"
C14-1076,P10-1001,0,0.0281682,"ently, dynamic programming techniques are used to find the optimal tree for each span, considering candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to decide among alternative spans. The typical feature models are based on combinations of edges (as known as, factors). A factor consists either of a single edge, two or three edges; which are called first order, second and third order factors, respectively. The later are employed in more advanced and recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the 795 vertexes involved in the factors. A feature template of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some recent research on trying to manually fi"
C14-1076,P08-1068,0,0.196155,"Missing"
C14-1076,D11-1109,0,0.0898476,"iments for the feature selection algorithm, we carried out a series of tests based on the parser settings. From these experiments, we obtained the best parser settings, the threshold that provides the best results given a development set, and the best scoring method and some additional configurations, that gave us reliable results and a fast outcome. We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applyi"
C14-1076,D10-1004,0,0.172862,"Missing"
C14-1076,E06-1011,0,0.29438,"Missing"
C14-1076,P05-1012,0,0.170027,"trees of the words of a sentence. Frequently, dynamic programming techniques are used to find the optimal tree for each span, considering candidate spans by successively building larger spans in a bottom-up fashion. A classifier is used to decide among alternative spans. The typical feature models are based on combinations of edges (as known as, factors). A factor consists either of a single edge, two or three edges; which are called first order, second and third order factors, respectively. The later are employed in more advanced and recent parsers trading off accuracy with complexity, cf. (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). The features in a graph-based algorithm consist of sets of features drawn from the 795 vertexes involved in the factors. A feature template of a second order factor is composed of properties drawn from up to all three vertex, e.g., the part-of-speech of the head, the dependent and a child denoted as POS ( H )+ POS ( D )+ POS ( C ). In our experiments, we use in addition to the transition-based model, a completion model that uses graph-based feature templates with up to third order factors to re-score the beam. 2.4 Feature Selection There has been some"
C14-1076,H05-1066,0,0.191657,"Missing"
C14-1076,C10-1093,0,0.128242,"beam. 2.4 Feature Selection There has been some recent research on trying to manually find better feature models for dependency parsers, such as Nivre et al. (2006), Hall et al. (2007), Hall (2008), Zhang and Nivre (2011), and Agirre et al. (2011). There is also research on automatic feature selection in the case of transition-based dependency parsing, a good example is MaltOptimizer (Ballesteros and Nivre, 2014) which implements a search for the best feature model that it can find, following acquired previous experience and deep linguistic knowledge (Hall et al., 2007; Nivre and Hall, 2010); Nilsson and Nugues (2010) also tried to search for optimal feature sets in the case of transition-based parsing, starting from a reduced test set using the concept of topological neighbors. Finally, He He et al. (2013) also tried automatic feature selection but for a graph-based parsing algorithm, where they pruned the feature space, removing unused features, in a first-order graph-based dependency parser, providing models that are equally accurate and faster. Zhang and Nivre (2011) pointed out that two different parsers based on the same algorithm may need different feature templates since other design aspects of a p"
C14-1076,W06-2933,0,0.076751,"Missing"
C14-1076,seeker-kuhn-2012-making,0,0.0299622,"ditional configurations, that gave us reliable results and a fast outcome. We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training times while maintaining a realistic training and optimization scenario. A major factor for the time usage is the beam size. The beam contains the alternative synt"
C14-1076,D09-1058,0,0.0955016,"Missing"
C14-1076,W03-3023,0,0.286938,"se experiments, we obtained the best parser settings, the threshold that provides the best results given a development set, and the best scoring method and some additional configurations, that gave us reliable results and a fast outcome. We used the following corpora for our experiments. Chinese: We used the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), with the same split as in (Zhang and Clark, 2008) and (Li et al., 2011).3 English: We used the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).4 German: We used Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). Hungarian: We used the Szeged Dependency Treebank (Farkas et al., 2012). Russian: We used the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002). 4.1 Parser settings As outlined in Section 3, our feature selection experiments require the training of a large number of parsing models and applying these to the development set.5 Therefore, we aimed to find a training setup for the parser that provided fast training time"
C14-1076,D08-1059,0,0.651666,"to the left (and tail β). 2 2.1 Related Work Mate Parser For our experiments, we used the transition-based parser of Bohnet et al. (2013). This parser performs joint part-of-speech tagging, morphological tagging, and non-projective labeled dependency parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The drawback of such a large feature set is a huge impact on the speed. Important research questions include (1) whether the number of features could be reduced to speed up the parser and (2) whether languages dependent f"
C14-1076,P11-2033,0,0.678302,"ncy parsing. The parser employs a number of techniques that lead to very competitive accuracy such as beam-search with early update (Collins and Roark, 2004), a hash kernel that can quickly cope with a large feature set, a graph-based completion model that adds scores for tree parts which a transition-based parser would not be able to consider, cf. (Zhang and Clark, 2008; Bohnet and Kuhn, 2012). The graph-based model takes into account second and third order factors and obtains a score as soon as the tree parts are completed. The parser employs a rich feature set for a transition-based model (Zhang and Nivre, 2011; Bohnet et al., 2013) as well as for a graph-based model. In total, there are 326 different feature templates for the two models. The drawback of such a large feature set is a huge impact on the speed. Important research questions include (1) whether the number of features could be reduced to speed up the parser and (2) whether languages dependent feature sets would be beneficiary. 2.2 Features in transition-based dependency parsing Every transition-based parser uses two data structures: (1) a buffer that contains at the beginning of the parsing process all words of the sentence that have to"
C14-1076,D07-1097,0,\N,Missing
C14-1133,W13-2322,0,0.0603372,"Missing"
C14-1133,D12-1133,1,0.846199,"tically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set. POS LEMMA LAS UAS 96.14 91.10 78.64 86.49 Table 1: Results of Bohnet and Nivre’s surface-syntactic parser on the development set In what follows, we first present the realization of the SSyntS–DSyntS transducer and then the realization of the baseline. 3.1 SSyntS–DSyntS transducer As outlined in Section 2.2, the SSyntS–DSyntS transducer is composed of three s"
C14-1133,W06-2920,0,0.0316683,"c parsing pipeline 5 Related Work To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated synta"
C14-1133,J08-1003,0,0.0551117,"Missing"
C14-1133,W09-1207,0,0.0198429,"also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules i"
C14-1133,J07-4004,0,0.0802815,"Missing"
C14-1133,fillmore-etal-2002-framenet,0,0.0465585,"matical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attributive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank and Semantic Frame structures are not always connected, may contain either individual lexical items or phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or sentential). In other words, they"
C14-1133,W09-1205,0,0.0190178,"int parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997). However, this is not to say that the idea of the surface→surface syntax→deep syntax pipeline is"
C14-1133,P03-1046,0,0.0820135,"Missing"
C14-1133,W12-3602,0,0.0984375,", publish-COORD→or-II→perish, and so on. APPEND subsumes all parentheticals, interjections, direct addresses, etc., as, e.g., in Listen, John!: listen-APPEND→John. DSyntSs thus show a strong similarity with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated; (ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv) they are connected.4 A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al., 2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows: Definition 2 (DSyntS) An DSyntS of a language L is a quintuple TDS = hN, A, λls →n , ρrs →a , γn→g i defined over the full lexical items Ld of L, the set of semantic grammemes Gsem , and the set of deepsyntactic relations Rdsynt , where • the set N of nodes and the set A of directed arcs form a connected tree, • λls →n assigns to each n ∈ N an ls ∈ Ld , • ρrs →a assigns to each a ∈ A an r ∈ Rdsynt , and • γn→g assigns to each λls →n (n) a set of grammemes Gt ∈ Gsem . Consider in Figure 1 an example for an"
C14-1133,W07-2416,0,0.483157,"(forests of trees defined over individual lexemes or phrasal chunks and abstract semantic role labels which capture the argument structure of predicative elements, dropping all attributive and coordinative dependencies). We propose a parser that delivers deep syntactic structures as output. 1 Introduction Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simplification, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the source and the target structures. Therefore, the use of more abstract “syntactico-semantic” structures seems more appropriate. Following Mel’ˇcuk (1988), we call these structures deep-syntactic structures (DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to abstract from l"
C14-1133,W08-2123,0,0.0217204,"rsing in 2006 and 2007 8 We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too weak to be used as baseline. 1409 (Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Hajiˇc et al., 2009). The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued with predicate identification, argument identification, argument labeling, and word sense disambiguation; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly all arguments to select the best combination was applied. Some of the systems were based on integrated syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu´ıs et al., 2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform structural changes—as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS. Klimeˇs (2006)’s parser removes nodes (producing tectogrammatical structures as in the Prague Dependency Treebank), but"
C14-1133,P86-1038,0,0.257327,"proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt , i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for verbs. The value of lexssynt can be any (either full or functional) lexical item; in graphical representations of SSyntSs, usually only the value of lexssynt is shown. The edge labels of a SSyntS are grammatical functions ‘subj’, ‘dobj’, ‘det’, ‘modif’, etc. In other words, SSyntSs are syntactic structures of the kind as encountered"
C14-1133,C12-2082,1,0.889085,"ucer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641 tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank). To obtain the SSyntS, we use Bohnet and Nivre (2012)’s transition-based parser, which combines lemmatization, PoS tagging, and syntactic dependency parsing—tuned and trained on the respective sets of the SSyntS treebank. Cf."
C14-1133,W13-3724,1,0.686421,"less than a dozen grammemes, etc. 3 Experiments In order to validate the outlined SSyntS–DSyntS transduction and to assess its performance in combination with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of 1405 experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2. DSynt Treebank SSynt Treebank Joint PoS Tagger SSynt parser Plain Sentences SSyntS SSynt−DSynt Transducer DSynS Figure 2: Setup of a deep-syntactic parser For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS treebank the semantically and information structure influenced relation tags to obtain an annotation granularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)). Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and 86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentenc"
C14-1133,C69-0101,0,0.518574,"r outcome. Section 5 summarizes the related work, before in Section 6 some conclusions and plans for future work are presented. 2 Fundamentals of SSyntS–DSyntS transduction Before we set out to discuss the principles of the SSyntS–DSynt transduction, we must specify the DSyntSs and SSyntSs as used in our experiments. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 The term ‘tree transduction’ is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension of finite state transduction (Aho, 1972) to trees. 1402 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1402–1413, Dublin, Ireland, August 23-29 2014. 2.1 Defining SSyntS and DSyntS SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels. The features of the node labels in SSyntSs are lexssynt , and “syntactic grammemes” of the value of lexssynt ,"
C14-1133,W08-2121,0,0.228819,"Missing"
C14-1133,taule-etal-2008-ancora,0,0.0989872,"Missing"
C14-1133,P08-1101,0,0.0611179,"Missing"
C14-1133,W09-1201,0,\N,Missing
C14-1133,P01-1033,0,\N,Missing
C14-1133,J05-1004,0,\N,Missing
C14-1133,Q13-1018,0,\N,Missing
C14-1133,D07-1096,0,\N,Missing
C14-1133,P13-2017,0,\N,Missing
C14-1133,ballesteros-nivre-2012-maltoptimizer-system,1,\N,Missing
C18-1263,Q16-1031,1,0.928698,"d those which are not. Sequence to sequence models with attention are no exception. Each set of parameters provides different levels of generalization (Reimers and Gurevych, 2017), which is evidenced in the synergistic task of training multilingual translation models. For example, Dong et al. (2015) jointly train decoders while the rest of the parameters are task-specific; Zoph and Knight (2016) jointly train the encoders while the rest of the parameters are task-specific, and Johnson et al. (2017) train both encoders and decoders jointly with language-specific tokens to guide learning as in (Ammar et al., 2016). These latter approaches are the ones that we build on. We augment our decoder with a task-specific attention mechanism intended to better capture word order and language-specific nuances while continuing to share the rest of the model parameters (including token embeddings). 4 Task-Specific Attention Models Fully n-way multilingual NMT systems need to support multiple source and target languages in the encoder and decoder GRUs. Our work focuses on improving the use of attention in the decoder. At each time-step t, the decoder computes the attention model weights αt,i of Equation (5) in order"
C18-1263,W14-4012,0,0.101684,"Missing"
C18-1263,P15-1166,0,0.15643,"red parameters. Sharing parameters may be useful when attempting to solve different tasks, since we can minimize representation bias by learning a more regularized representation (Baxter, 2000). The flexibility nature of neural architectures allows for selection of the components of the model that are to be shared and those which are not. Sequence to sequence models with attention are no exception. Each set of parameters provides different levels of generalization (Reimers and Gurevych, 2017), which is evidenced in the synergistic task of training multilingual translation models. For example, Dong et al. (2015) jointly train decoders while the rest of the parameters are task-specific; Zoph and Knight (2016) jointly train the encoders while the rest of the parameters are task-specific, and Johnson et al. (2017) train both encoders and decoders jointly with language-specific tokens to guide learning as in (Ammar et al., 2016). These latter approaches are the ones that we build on. We augment our decoder with a task-specific attention mechanism intended to better capture word order and language-specific nuances while continuing to share the rest of the model parameters (including token embeddings). 4 T"
C18-1263,N18-1005,0,0.0544263,"Missing"
C18-1263,Q18-1017,1,0.892701,"Missing"
C18-1263,W17-3204,0,0.0457492,"Missing"
C18-1263,P07-2045,0,0.0106074,"Missing"
C18-1263,E06-1005,0,0.0362188,"tive importance of the vector representation of each source position hi when producing the next target word at time-step t. The weights are computed using a two-layer feed-forward network r: exp{r(st−1 , hi , yt−1 )} αt,i = P . k exp{r(st−1 , hk , yt−1 )} 3113 (5) 3 Related Work Early phrase-based approaches to multilingual MT focused on multi-source translation. Och and Ney (2002) used a simple product or max rule to select at the sentence-level the single best hypothesis with the highest translation score from multiple decoders. There is no sharing of parameters. Consensus network decoding (Matusov et al., 2006) can also be used to combine the word-level output of translations from multiple source languages. Such system combination techniques allow sharing of the words in the candidate translations, but still require training individual models for each language pair of interest. Training multilingual MT systems capable of translating between multiple languages can be considered an instance of multi-task learning (Caruana, 1997), which is the idea of solving synergistic tasks while maximizing the number of shared parameters. Sharing parameters may be useful when attempting to solve different tasks, si"
C18-1263,P02-1040,1,0.119587,"d sub-word vocabularies. Tokenized corpus statistics for the parallel training data are summarized in Table 3, while Table 4 compares the vocabulary sizes obtained from BPE processed data for the single-data baseline and fully n-way multilingual models. The Europarl evaluation data set dev2006 is used as our validation set, while devtest2006 and test2007 are our blind test sets. We also evaluate the out-of-domain News Commentary test sets ncdev2007 and nc-devtest2007 in order to demonstrate the robustness of our approach to different kinds of data. Case-sensitive single-reference BLEU scores (Papineni et al., 2002) are computed using the multi-bleu.perl script included with Moses. Reimers and Gurevych (2017) have shown that reporting a single metric score can sometimes be misleading for many neural architectures. For this reason, all BLEU scores reported in the tables are obtained by averaging the decoding results from five separate models initialized with distinct random seeds. 5.1 Implementation Details Our sequence-to-sequence NMT model with task-specific attention is implemented in C++ using DyNet3 (Neubig et al., 2017a). We use DyNet since the computation graph can be efficiently modified on a batc"
C18-1263,D17-1035,0,0.107023,"instance of multi-task learning (Caruana, 1997), which is the idea of solving synergistic tasks while maximizing the number of shared parameters. Sharing parameters may be useful when attempting to solve different tasks, since we can minimize representation bias by learning a more regularized representation (Baxter, 2000). The flexibility nature of neural architectures allows for selection of the components of the model that are to be shared and those which are not. Sequence to sequence models with attention are no exception. Each set of parameters provides different levels of generalization (Reimers and Gurevych, 2017), which is evidenced in the synergistic task of training multilingual translation models. For example, Dong et al. (2015) jointly train decoders while the rest of the parameters are task-specific; Zoph and Knight (2016) jointly train the encoders while the rest of the parameters are task-specific, and Johnson et al. (2017) train both encoders and decoders jointly with language-specific tokens to guide learning as in (Ammar et al., 2016). These latter approaches are the ones that we build on. We augment our decoder with a task-specific attention mechanism intended to better capture word order a"
C18-1263,E17-3017,0,0.0767575,"Missing"
C18-1263,P16-1157,0,0.0673311,"Missing"
C18-1263,P17-1179,0,0.0637453,"Missing"
C18-1263,N16-1004,0,0.0546979,"ce we can minimize representation bias by learning a more regularized representation (Baxter, 2000). The flexibility nature of neural architectures allows for selection of the components of the model that are to be shared and those which are not. Sequence to sequence models with attention are no exception. Each set of parameters provides different levels of generalization (Reimers and Gurevych, 2017), which is evidenced in the synergistic task of training multilingual translation models. For example, Dong et al. (2015) jointly train decoders while the rest of the parameters are task-specific; Zoph and Knight (2016) jointly train the encoders while the rest of the parameters are task-specific, and Johnson et al. (2017) train both encoders and decoders jointly with language-specific tokens to guide learning as in (Ammar et al., 2016). These latter approaches are the ones that we build on. We augment our decoder with a task-specific attention mechanism intended to better capture word order and language-specific nuances while continuing to share the rest of the model parameters (including token embeddings). 4 Task-Specific Attention Models Fully n-way multilingual NMT systems need to support multiple source"
D15-1041,Q13-1034,0,0.0297403,"Missing"
D15-1041,D14-1082,0,0.480661,"m incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portuga"
D15-1041,P14-2111,0,0.0181423,"hment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes (2015) learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. 6 Conclusion We have presented several interesting findings. First, we add new evidence t"
D15-1041,D07-1022,1,0.91229,"on practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words + POS: words and POS tags (§3"
D15-1041,W15-3904,0,0.0444823,"Missing"
D15-1041,P15-1033,1,0.075747,"ecially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees. We do this by augmenting its transition operations We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are s"
D15-1041,P11-2124,0,0.0240466,"orted (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zha"
D15-1041,W13-4907,1,0.945071,"on University, Pittsburgh, PA, USA ♣ Marianas Labs, Pittsburgh, PA, USA ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, chris@marianaslabs.com, nasmith@cs.washington.edu Abstract The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides strong evidence regarding its grammatical role in morphologically rich languages (Ballesteros, 2013, inter alia), this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled. In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in th"
D15-1041,Q13-1033,0,0.0347924,"sualization was produced using t-SNE; see http: //lvdmaaten.github.io/tsne/. word of the input sentence; however, at test time these results could be cached. On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons9 We are using a machine with 32 Intel Xeon CPU E52650 at 2.00GHz; the parser runs on a single core. 355 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chi"
D15-1041,W13-4916,0,0.0362755,"Missing"
D15-1041,P08-1043,0,0.0336093,"Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predict"
D15-1041,W14-6111,0,0.046747,"traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parse"
D15-1041,P13-1088,0,0.0104461,"te (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr (u, v) or gr (v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. Predicting Parser Decisions 3 The parser uses a probab"
D15-1041,P82-1020,0,0.877457,"Missing"
D15-1041,D10-1125,0,0.00949521,"t performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang"
D15-1041,seeker-kuhn-2012-making,0,0.013972,"se vectors and a (learned) representation of their tag to produce the representation w. As in §3.1, a linear map (V) is applied and passed through a component-wise ReLU. n o → ← x = max 0, V[w; w; t] + b sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean ´ (Choi, 2013), Polish (Swidzi´ nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in conte"
D15-1041,J93-2004,0,0.0644981,"98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set. No pretraining of a"
D15-1041,de-marneffe-etal-2006-generating,0,0.0548307,"Missing"
D15-1041,D13-1032,0,0.0302696,"Missing"
D15-1041,D13-1170,0,0.00309807,"s. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr (u, v) or gr (v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. Predicting Parser Decision"
D15-1041,W06-2933,0,0.0487182,"Missing"
D15-1041,nivre-etal-2006-talbanken05,0,0.0429632,"LU. n o → ← x = max 0, V[w; w; t] + b sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean ´ (Choi, 2013), Polish (Swidzi´ nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English This process is shown schematically in Figure 3"
D15-1041,W07-2218,0,0.0081179,"ges show that the parsing model benefits from incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro"
D15-1041,W04-0308,0,0.155705,"ng the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard parsing algorithm (Nivre, 2004). At each time step t, a transition action is applied that alters these data structures by pushing or popping words from the stack and the buffer; the operations are listed in Figure 1. Along with the discrete transitions above, the parser calculates a vector representation of the states of B, S, and A; at time step t these are denoted by bt , st , and at , respectively. The total parser state at t is given by pt = max {0, W[st ; bt ; at ] + d} it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = 1 − it ct = ft ct−1 + it tanh(Wcx xt + Wch ht−1 + bc ) ot = σ(Wox xt + Woh ht−1 + Woc ct + bo ) ht = ot"
D15-1041,N03-1033,0,0.216746,"Missing"
D15-1041,P09-1040,0,0.427849,"stm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it ) or forget (ft ). We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt , the previous hidden state ht−1 , and the memory cell ct−1 : with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard pars"
D15-1041,P06-3009,0,0.199759,"rs + POS. It is common practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words +"
D15-1041,P15-1032,0,0.60146,"oduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational"
D15-1041,P08-1101,0,0.0599117,"he SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped wh"
D15-1041,D08-1059,0,0.407672,"he SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped wh"
D15-1041,P13-1013,0,0.0124566,"2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a). To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper. tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), wh"
D15-1041,P15-1117,0,0.0362065,"gs of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Associat"
D15-1041,D15-1176,1,\N,Missing
D15-1041,W13-4917,0,\N,Missing
D15-1041,Q14-1017,0,\N,Missing
D15-1041,vincze-etal-2010-hungarian,0,\N,Missing
D16-1111,D15-1041,1,0.72849,"ed in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introduction (or restoration) of punctuation in speech transcripts. Most often, classifier models are used that are trained on n-gram models (Gr"
D16-1111,candito-etal-2010-statistical,0,0.0279997,"Missing"
D16-1111,P15-1033,1,0.696496,"al., 1972), which makes it reflect the syntactic structure of a sentence. The different functions of punctuation are also reflected in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introdu"
D16-1111,N16-1024,1,0.925026,"ntence generation, trained on a variety of syntactic features from LFG f-structures, preceding punctuation bigrams and cue words. Our proposal is most similar to Tilk and Alum¨ae (2015), but our task is more complex since we generate the full range of punctuation marks. Furthermore, we do not use any acoustic features. Compared to Guo et al. (2010), we do not use any syntactic features either since our input is just raw text material. 3 Model Our model is inspired by a number of recent works on neural architectures for structure prediction: Dyer et al. (2015)’s transition-based parsing model, Dyer et al. (2016)’s generative language model and phrase-structure parser, Ballesteros et al. (2015)’s character-based word representation for parsing, and Ling et al. (2015b)’s part-of-speech tagging . 3.1 Transition SHIFT GENERATE(“,”) SHIFT SHIFT SHIFT GENERATE(“.”) Output [] [No] [No ,] [No , it] [No , it was ] [No , it was not] [No, it was not .] Figure 1: Transition sequence for the input sequence No it was not – with the output No, it was not. put and input buffers, is encoded in terms of a vector st ; see Section 3.3 for different alternatives of state representation. As Dyer et al. (2015), we use st t"
D16-1111,N15-1142,0,0.298788,"ion are also reflected in different tasks in natural language processing (NLP): introduction of punctuation marks into a generated sentence that is to be read aloud, restoration of punctuation in speech transcripts, parsing under consideration of punctuation, or generation of punctuation in written discourse. Our work is centered in the last task. We present a novel punctuation generation algorithm that is based on the transitionbased algorithm with long short-term memories (LSTMs) by Dyer et al. (2015) and character-based continuous-space vector embeddings of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). The algorithm takes as input raw material without punctuation and effectively introduces the full range of punctuation symbols. Although intended, first of all, for use in sentence generation, the algorithm is function- and language-neutral, which makes it different, compared to most of the stateof-the-art approaches, which use function- and/or language-specific features. 2 Related Work The most prominent punctuation-related NLP task has been so far introduction (or restoration) of punctuation in speech transcripts. Most often, classifier models are used that are"
D16-1111,D15-1176,0,0.0277625,"Missing"
D16-1111,W14-1701,0,0.0373703,"Missing"
D16-1111,W04-0308,0,0.0236883,"s the actions (either SHIFT or GENERATE(p)).1 st encodes information about previous actions (since it may include the history with the actions taken and the generated punctuation symbols are introduced in the output buffer, see Section 3.3), thus the probability of a sequence of actions z given the input sequence is: Algorithm We define a transition-based algorithm that introduces punctuation marks into sentences that do not contain any punctuation. In the context of NLG, the input sentence would be the result of the surface realization task (Belz et al., 2011). As in transitionbased parsing (Nivre, 2004), we use two data structures: Nivre’s queue is in our case the input buffer and his stack is in our case the output buffer. The algorithm starts with an input buffer full of words and an empty output buffer. The two basic actions of the algorithm are SHIFT, which moves the first word from the input buffer to the output buffer, and GEN ERATE , which introduces a punctuation mark after the first word in the output buffer. Figure 1 shows an example of the application of the two actions. At each stage t of the application of the algorithm, the state, which is defined by the contents of the outInpu"
D16-1111,W08-1703,0,0.119176,"c features (Baron et al., 2002; Kol´aˇr and Lamel, 2012). Tilk and Alum¨ae (2015) use a lexical and acoustic (pause duration) feature-based LSTM model for the restoration of periods and commas in Estonian speech transcripts. The grammatical and syntactic functions of punctuation have been addressed in the context of written 1048 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1048–1053, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics language. Some of the proposals focus on the grammatical function (Doran, 1998; White and Rajkumar, 2008), while others bring the grammatical and syntactic functions together and design rule-based grammatical resources for parsing (Briscoe, 1994) and surface realization (White, 1995; Guo et al., 2010). Guo et al. (2010) is one of the few works that is based on a statistical model for the generation of punctuation in the context of Chinese sentence generation, trained on a variety of syntactic features from LFG f-structures, preceding punctuation bigrams and cue words. Our proposal is most similar to Tilk and Alum¨ae (2015), but our task is more complex since we generate the full range of punctuat"
D16-1180,P16-1231,0,0.391211,"ing standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding consensus. We address"
D16-1180,D15-1041,1,0.89892,"Missing"
D16-1180,D16-1211,1,0.803959,"Missing"
D16-1180,D12-1133,0,0.212269,"ensemble pipeline. Accuracy. All scores are shown in Table 5. First, consider the neural FOG parser trained with Hamming cost (CH in the second-to-last row). This is a very strong benchmark, outperforming many higherorder graph-based and neural network models on all three datasets. Nonetheless, training the same model with distillation cost gives consistent improvements for all languages. For English, we see that this model comes close to the slower ensemble it was trained to simulate. For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre (2012) for LAS. Effects of Pre-trained Word Embedding. As an ablation study, we ran experiments on English without pre-trained word embedding, both with the Hamming and distillation costs. The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS, compared to 93.6 UAS and 91.1 LAS for the model with distillation cost. This result further showcases the consistent improvements from using the distillation cost across different settings and languages. We conclude that “soft targets” derived from ensemble uncertainty offer useful guidance, through the distillation cost function and discriminativ"
D16-1180,D14-1082,0,0.22734,"ized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neu"
D16-1180,P15-1033,1,0.267775,"sentropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the"
D16-1180,C96-1058,0,0.199002,"ing function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the t"
D16-1180,N10-1112,1,0.815815,"POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second, 6 Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experiment"
D16-1180,D16-1139,0,0.0603475,"ensemble training to re-lexicalize a transfer model for a target language. We similarly use an ensemble of models as a supervision for a sin10 Our cost is zero when the correct arc is predicted, regardless of what the soft target thinks, something a compression model without gold supervision cannot do. gle model. By incorporating the ensemble uncertainty estimates in the cost function, our approach is cheaper, not requiring any marginalization during training. An additional difference is that we learn from the gold labels (“hard targets”) rather than only ensemble estimates on unlabeled data. Kim and Rush (2016) proposed a distillation model at the sequence level, with application in sequence-to-sequence neural machine translation. There are two primary differences with this work. First, we use a global model to distill the ensemble, instead of a sequential one. Second, Kim and Rush (2016) aim to distill a larger model into a smaller one, while we propose to distill an ensemble instead of a single model. 8 Conclusions We demonstrate that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state of the art accuracy on English dependency parsing. This approach corresponds to minimum Ba"
D16-1180,Q16-1023,0,0.116166,"w cost function, inspired by the notion of “soft targets” (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse"
D16-1180,P10-1001,0,0.0124705,"o derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h, m, `), where h is the index of a head, m the index of"
D16-1180,D14-1081,0,0.0194869,"he ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h, m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or rel"
D16-1180,N15-1142,1,0.446195,"oves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncert"
D16-1180,C12-2077,0,0.0564198,"Missing"
D16-1180,D08-1017,1,0.760634,"cost—and use it in discriminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parse"
D16-1180,P13-2109,1,0.895869,"Missing"
D16-1180,P05-1012,0,0.876112,"(h, m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or relation type). Most dependency parsers are constrained to return y that form a directed tree. A first-order graph-based (FOG; also known as “arc-factored”) dependency parser exactly solves y ˆ(x) = arg max y∈T (x) X s(h, m, x), (h,m)∈y | {z S(y,x) (1) } where T (x) is the set of directed trees over x, and s is a local scoring function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based par"
D16-1180,H05-1066,0,0.452646,"Missing"
D16-1180,P08-1108,0,0.0151135,"criminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike mod"
D16-1180,W03-3017,0,0.0218202,"016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the time of this writing employ neural networks (Andor et al., 2016). 1 https://github.com/adhigunasurya/ distillation_parser.git 1745 Let hy (m) denote the parent of xm in y (using a special null symbol when m is the root of the tree), and hy0 (m) denotes the parent of xm in the predicted tree y 0 . Given two dependency parses of"
D16-1180,N10-1003,0,0.026416,"denoted with an underline. The † sign indicates the use of predicted tags for Chinese in the original publication, although we report accuracy using gold Chinese tags based on private correspondence with the authors. ered a FOG parser, though future work might investigate any parser amenable to training to minimize a cost-aware loss like the structured hinge. 7 Related Work Our work on ensembling dependency parsers is based on Sagae and Lavie (2006) and Surdeanu and Manning (2010); an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing. Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals. The local optima in his base model’s training objective arise from latent variables instead of neural networks (in our case). Model distillation was proposed by Bucilˇa et al. (2006), who used a single neural network to simulate a large ensemble of classifiers. More recently, Ba and Caruana (2014) showed that a single shal1751 low neural network can closely replicate the performance of an ensemble of deep neural networks in phoneme recognition and object dete"
D16-1180,N06-2033,0,0.0848776,"ble, N = 10, MST ensemble, N = 15, MST ensemble, N = 20, MST Consensus and Minimum Bayes Risk Despite the recent success of neural network dependency parsers, most prior works exclusively report single-model performance. Ensembling neural network models trained from different random starting points is a standard technique in a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al., 2015). We aim to investigate the benefit of ensembling independently trained neural network dependency parsers by applying the parser ensembling method of Sagae and Lavie (2006) to a collection of N strong neural network base parsers. Here, each base parser is an instance of the greedy, transition-based parser of Dyer et al. (2015), known as the stack LSTM parser, trained from a different random initial estimate. Given a sentence x, the consensus FOG parser (Eq. 1) defines score s(h, m, x) as the number of base parsers that include the attachment (h, m), which we denote votes(h, m).2 An example of this scoring function with an ensemble of 20 models is shown in Figure 1 We assign to dependency (h, m) the label most frequently selected by the base parsers that attach m"
D16-1180,P06-2101,0,0.0302359,"nd compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second, 6 Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2"
D16-1180,N10-1091,0,0.261328,"ndencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015).3 Table 1 shows that ensembles, even with small N , strongly outperform a single stack LSTM parser. Our ensembles of greedy, locally normalized parsers perform comparably to the best previously reported, due to Andor et al. (2016), which uses a beam (width 32) for training and decoding. 4 What is Ensemble Uncertainty? While previous works have already demonstrated the merit of ensembling in dependency parsing (Sagae and Lavie, 2006; Surdeanu and Manning, 2010), usually with diverse base parsers, we consider whether the posterior marginals estimated by pˆ((h, m) ∈ Y |x) = votes(h, m)/N can be interpreted. We conjecture that disagreement among base parsers about where to attach xm (i.e., uncertainty in the posterior) is a sign of difficulty or am3 We use the standard data split (02–21 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.com/clab/lstm-parser, each with a different random initialization; this dif"
D16-1180,N13-1126,0,0.0166262,"Missing"
D16-1180,N03-1033,0,0.0210996,"ate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set,"
D16-1180,P16-1218,0,0.223292,"notion of “soft targets” (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a"
D16-1180,P15-1032,0,0.0231937,"ns required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding"
D16-1180,C02-1145,0,0.117563,"isk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data a"
D16-1180,K15-1015,0,0.0197952,"Missing"
D16-1180,D08-1059,0,0.0578633,"single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike model combination techniqu"
D16-1180,P11-2033,0,0.0443312,"Missing"
D16-1180,P15-1112,0,0.0208299,"Missing"
D16-1211,P16-1231,0,0.489171,".94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87 90.75 88.14 91.44 89.29 93.22 91.23 German UAS LAS 88.09 85.24 88.56 86.15 89.80 87.29 90.34 88.17 89.6 86.0 89.12 86.95 90.91 89.15 Japanese UAS LAS 93.10 92.28 – – 93.47 92.70 – – – – 93.71 92.85 93.65 92.84 Spanish UAS LAS 89.08 85.03 90.76 87.48 89.53 85.69 91.09 87.95 88.3 85.4 91.01 88.14 92.62 89.95 Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudoprojective parsing. Y’15 and A’16 are beam = 1 parsers from Yazdani and Henderson (2015) and Andor et al. (2016), respectively. A’16-beam is the parser with beam larger than 1 by Andor et al. (2016). Bold numbers indicate the best results among the greedy parsers. The error-exploring dynamic-oracle training always improves over static oracle training controlling for the transition system, but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Ando"
D16-1211,W15-2210,0,0.0477838,"Missing"
D16-1211,P05-1022,0,0.0279624,"English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87"
D16-1211,D14-1082,0,0.316369,"urns out to be very beneficial for the configurations that make use of external embeddings. Indeed, these configurations achieve high accuracies and sharp class distributions early on in the training process. The parser is trained to maximize the likelihood of a correct action zg at each parsing state pt according to Equation 1. When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is the marginal likelihood of all correct actions,3 X p(zg |pt ) = p(zgi |pt ). (3) zgi ∈zg 3 Experiments Following the same settings of Chen and Manning (2014) and Dyer et al (2015) we report results4 in the English PTB and Chinese CTB-5. Table 1 shows the results of the parser in its different configurations. The table also shows the best result obtained with the static oracle (obtained by rerunning Dyer et al. parser) for the sake of comparison between static and dynamic training strategies. Method Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) + pre-training: Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66"
D16-1211,P15-1033,1,0.660448,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are typically stored in a stack data structure, and the words remaining to be processed. to capture information from the entirety of the state, without resorting to locality assumptions that were common in most other transition-based par"
D16-1211,C12-1059,1,0.947246,"ces, given words. At test time, the parser makes greedy decisions according to the learned model. Although this setup obtains very good performance, the training and testing conditions are mismatched in the following way: at training time the historical context of an action is always derived from the gold standard (i.e., perfectly correct past actions), but at test time, it will be a model prediction. In this work, we adapt the training criterion so as to explore parser states drawn not only from the training data, but also from the model as it is being learned. To do so, we use the method of Goldberg and Nivre (2012; 2013) to dynamically chose an optimal (relative to the final attachment accuracy) action given an imperfect history. By interpolating between algorithm states sampled from the model and those sampled from the training data, more robust predictions at test time can be made. We show that the technique can be used to improve the strong parser of Dyer et al. 2 Parsing Model and Parameter Learning Our departure point is the parsing model described by Dyer et al. (2015). We do not describe the model in detail, and refer the reader to the original work. At each stage t of the parsing process, the p"
D16-1211,Q13-1033,1,0.93704,"ing with Exploration Improves a Greedy Stack LSTM Parser Miguel Ballesteros♦ Yoav Goldberg♣ Chris Dyer♠ Noah A. Smith♥ ♦ NLP Group, Pompeu Fabra University, Barcelona, Spain ♣ Computer Science Department, Bar-Ilan University, Ramat Gan, Israel ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, yoav.goldberg@gmail.com, cdyer@google.com, nasmith@cs.washington.edu Abstract We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Mats"
D16-1211,Q14-1010,1,0.946318,"if the best tree that can be reached after taking the action is no worse (in terms of accuracy with respect to the gold tree) than the best tree that could be reached prior to taking that action. Goldberg and Nivre (2013) define the arcdecomposition property of transition systems, and show how to derive efficient dynamic oracles for transition systems that are arc-decomposable.2 Unfortunately, the arc-standard transition system does 2 Specifically: for every parser configuration p and group of not have this property. While it is possible to compute dynamic oracles for the arc-standard system (Goldberg et al., 2014), the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack. As the dynamic oracle has to be queried for each parser state seen during training, the use of this dynamic oracle will make the training runtime several times longer. We chose instead to switch to the arc-hybrid transition system (Kuhlmann et al., 2011), which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime. We implemented the dynamic oracle to the arc-hybrid s"
D16-1211,W13-5709,1,0.953811,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The framework of training with exploration using dynamic oracles suggested by Goldberg and Nivre (2012; 2013) provides answers to these questions. While the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take for any valid parser state. In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path, the dynamic oracle is well defined for states that result from parsing mistakes, and they may produce more than a single gold action for a given state. Under the dynamic oracle framework, an action is said to be optimal for a state if the best tree that can be reached afte"
D16-1211,P15-2042,0,0.0776288,"Missing"
D16-1211,D14-1099,0,0.0889626,"Missing"
D16-1211,Q14-1011,0,0.0127702,"rained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; R"
D16-1211,W13-3518,1,0.840178,"clude results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012"
D16-1211,P11-1068,0,0.126938,"Missing"
D16-1211,P05-1013,0,0.0596932,"rc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Andor et al., 2016)5 we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task (Hajiˇc et al., 2009). Since some of the treebanks contain nonprojective sentences and archybrid does not allow nonprojective trees, we use the pseudo-projective approach (Nivre and Nilsson, 2005). We used predicted part-of-speech tags provided by the CoNLL 2009 shared task organizers. We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several resea"
D16-1211,W03-3017,0,0.353357,"ssuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous de"
D16-1211,W04-0308,0,0.52657,"ror-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (some"
D16-1211,J08-4003,0,0.0983095,"on history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called t"
D16-1211,P00-1061,0,0.278952,"brid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59"
D16-1211,P15-3004,0,0.0262076,"Missing"
D16-1211,Q16-1014,0,0.0220862,"nish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training"
D16-1211,W03-3023,0,0.18219,"nd Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection o"
D16-1211,K15-1015,0,0.032838,"ize 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training regimens that make the learned model more robust to test-time prediction errors. Solutions based on curriculum learning (Bengio et al., 2015), expected loss training (Shen et al., 2015), and reinforcement learning have been proposed (Ranzato et al., 2016). Finally, abandoning greedy search in favor of approximate global search offers an alternative solution to the problems with greedy search (Andor et al., 2016), and has been analyzed as well (Kulesza and Pereira"
D16-1254,P16-1231,0,0.0514973,"cts a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure. Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence. In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions. Recent models include the greedy StackLSTM model of Dyer et al. (2015) and the globally normalized feed-forward networks of Andor et al. (2016). These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of"
D16-1254,D12-1133,0,0.0602576,"Missing"
D16-1254,P13-1104,0,0.290773,"the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam. Another approach to solving this issue is given by Choi and McCallum (2013). They introduced selectional branching, which involves performing an initial greedy parse, and then using confidence estimates on each prediction to spawn additional beams. Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up. In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function (Pearl, 1984): a function which estimates 2313 Proceedings of the 2016 Conference on Empirical Methods in Natural Language"
D16-1254,P04-1015,0,0.742986,", we look at all explored nodes that still have at least one unexplored child, and choose the node with the lowest heuristic confidence (see Section 3.2). We rewind our stack, buffer, and action history to that state, and execute the highest-scoring transition from that node that has not yet been explored. At this point, we are again in a fully-unexplored node, and can greedily parse just as before until we reach another leaf. Once we have generated b leaves, we score them all and return the transition sequence leading up to the highest-scoring leaf as the answer. Just as in previous studies (Collins and Roark, 2004), we use the n11 n12 n13 n14 ... n1l n22 n23 n24 ... n32 n33 n34 n42 n43 n44 n11 n12 n13 n14 ... n1l n2l n22 n23 n24 ... n2l ... n3l n32 n34 ... n3l ... n4l n42 (a) Beam Search n11 n4l (b) Dynamic Beam Search n12 n13 n14 ... n1l n22 n23 n24 ... n2l n33 n34 ... n44 ... n11 n12 n13 n14 ... n1l n22 n23 n24 ... n2l n3l n34 ... n3l n4l n44 ... n4l (c) Selectional Branching (d) Heuristic Backtracking Figure 1: Visualization of various decoding algorithms sum of the log probabilities of all individual transitions as the overall score for the parse. 3.2 We do this in the following way: H(nji ) = (V (n"
D16-1254,P15-1033,1,0.933825,"arsing, one of the most prominent dependency parsing techniques, constructs a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure. Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence. In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions. Recent models include the greedy StackLSTM model of Dyer et al. (2015) and the globally normalized feed-forward networks of Andor et al. (2016). These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers. One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013). When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams. Common methods for pruning are removing all beams below some"
D16-1254,W04-0308,0,0.333279,"the search early if we have found an answer that we believe to be the gold parse, saving time proportional to the number of backtracks remaining. We compare the performance of these various decoding algorithms with the Stack-LSTM parser (Dyer et al., 2015), and achieve slightly higher accuracy than beam search, in significantly less time. the valid transition actions that may be taken in the current state. The objective function is: 2 3.1 Transition-Based Parsing With Stack-LSTM Our starting point is the model described by Dyer et al. (2015).1 The parser implements the arc-standard algorithm (Nivre, 2004) and it therefore makes use of a stack and a buffer. In (Dyer et al., 2015), the stack and the buffer are encoded with Stack-LSTMs, and a third sequence with the history of actions taken by the parser is encoded with another Stack-LSTM. The three encoded sequences form the parser state pt defined as follows, pt = max {0, W[st ; bt ; at ] + d} , (1) where W is a learned parameter matrix, bt , st and at are the stack LSTM encoding of buffer, stack and the history of actions, and d is a bias term. The output pt (after a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011"
D16-1254,P15-1032,0,0.0242667,"Missing"
D16-1254,K15-1015,0,0.0545119,"Missing"
D16-1254,D08-1059,0,0.0323662,"Missing"
D16-1254,P11-2033,0,0.173805,"Missing"
D16-1254,P15-1117,0,0.124944,"Missing"
D17-1130,D15-1198,0,0.18344,"al. is calculated using a pretrained LDC2015 model, available at https:// github.com/mdtux89/amr-eager, but evaluated on the LDC2014 dataset. This means that the score is not directly comparable with the rest. The second entry (0.64) for Damonte et al. is calculated by training their parser with the LDC2014 training set which makes it directly comparable with the rest of the parsers. 1272 Model Flanigan et al. (2014)* (POS, DEP) Flanigan et al. (2016)* (POS, DEP, NER) Werling et al. (2015)* (POS, DEP, NER) Damonte et al. (2016)8 (POS, DEP, NER, SRL) Damonte et al. (2016)8 (POS, DEP, NER, SRL) Artzi et al. (2015) (POS, CCG) Goodman et al. (2016)* (POS, DEP, NER) Zhou et al. (2016)* (POS, DEP, NER, SRL) Pust et al. (2015) (LM, NER) Pust et al. (2015) (Wordnet, LM, NER) Wang et al. (2015b)* (POS, DEP, NER) Wang et al. (2015a)* (POS, DEP, NER, SRL) O UR PARSER (N O PRETRAINED -N O CHARS ) O UR PARSER (N O PRETRAINED -W ITH CHARS ) O UR PARSER (W ITH PRETRAINED -N O CHARS ) O UR PARSER O UR PARSER (POS) O UR PARSER (POS, DEP) F1 (Newswire) 0.59 0.62 0.62 – – 0.66 0.70 0.71 – – 0.63 0.70 0.64 0.66 0.66 0.68 0.68 0.69 F1 (ALL) 0.58 0.59 – 0.61 0.64 – – 0.66 0.61 0.66 0.59 0.66 0.59 0.61 0.62 0.63 0.63 0.64"
D17-1130,D15-1041,1,0.860981,"qe is a bias term for the node e. It is important to mention that this implies finding a propbank sense or a lemma. For that, we rely entirely on the AMR training set instead of using additional resources. Given that the system runs two softmax operations, one to predict the action to take and the second one to predict the corresponding AMR node, and they both share LSTMs to make predictions, we include an additional layer with a tanh nonlinearity after st for each softmax. 3.3 Word Representations We use character-based representations of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). They learn representations for words that are orthographically similar. Note that they are updated with the updates to the model. Ballesteros et al. (2015) and Lample et al. (2016) demonstrated that it is possible to achieve high results in syntactic parsing and named entity recognition by just using characterbased word representations (not even POS tags, in fact, in some cases the results with just characterbased representations outperform those that used explicit POS tags since they provide similar vectors for words with similar/same morphosyntactic tag (Ballesteros et al., 2015)); in this"
D17-1130,J13-1002,1,0.907506,"Missing"
D17-1130,W13-2322,0,0.435229,"actic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shift-reduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based treeto-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more similar (with a different parsing algorithm) to our approach, but their parser relies on external tools, such as"
D17-1130,P13-2131,0,0.213131,"the sentences to nodes in the AMR graph we need to align them. We use the JAMR aligner provided by Flanigan et al. (2014).3 It is important to mention that even though the aligner is quite accurate, it is not perfect, producing a F1 score of around 0.90. This means that most sentences have at least one alignment error which implies that our oracle is not capable of perfectly reproducing all AMR graphs. This has a direct impact on the accuracy of the parser described in the next section since it is trained on sequences of actions that are not perfect. The oracle achieves 0.895 F1 Smatch score (Cai and Knight, 2013) when it is run on the development set of the LDC2014T12. The algorithm allows a set of different constraints that varies from the basic ones (not allowing impossible actions such as SHIFT when the buffer is empty or not generating arcs when the words have not yet been CONFIRMed and thus transformed to nodes) to more complicated ones based on the propbank candidates and number of arguments. We choose to constrain the parser to the basic ones and let it learn the more complicated ones. 3 Parsing Model In this section, we revisit Stack-LSTMs, our parsing model and our word representations. 3 We"
D17-1130,P15-1033,1,0.949787,"Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shift-reduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based treeto-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more similar (with a different parsing algorithm) to our approach, but their parser relies on external tools, such as dependency parsing, semantic role labeling or named entity recognition. The input of our parser is plain text sentences and, through rich word re"
D17-1130,N16-1024,1,0.847858,"itive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan e"
D17-1130,S16-1186,0,0.0747463,"ining/development/test split: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences heldout for testing. 8 The first entry for Damonte et al. is calculated using a pretrained LDC2015 model, available at https:// github.com/mdtux89/amr-eager, but evaluated on the LDC2014 dataset. This means that the score is not directly comparable with the rest. The second entry (0.64) for Damonte et al. is calculated by training their parser with the LDC2014 training set which makes it directly comparable with the rest of the parsers. 1272 Model Flanigan et al. (2014)* (POS, DEP) Flanigan et al. (2016)* (POS, DEP, NER) Werling et al. (2015)* (POS, DEP, NER) Damonte et al. (2016)8 (POS, DEP, NER, SRL) Damonte et al. (2016)8 (POS, DEP, NER, SRL) Artzi et al. (2015) (POS, CCG) Goodman et al. (2016)* (POS, DEP, NER) Zhou et al. (2016)* (POS, DEP, NER, SRL) Pust et al. (2015) (LM, NER) Pust et al. (2015) (Wordnet, LM, NER) Wang et al. (2015b)* (POS, DEP, NER) Wang et al. (2015a)* (POS, DEP, NER, SRL) O UR PARSER (N O PRETRAINED -N O CHARS ) O UR PARSER (N O PRETRAINED -W ITH CHARS ) O UR PARSER (W ITH PRETRAINED -N O CHARS ) O UR PARSER O UR PARSER (POS) O UR PARSER (POS, DEP) F1 (Newswire) 0.59"
D17-1130,P14-1134,0,0.644142,"al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shift-reduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based treeto-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more simil"
D17-1130,P16-1001,0,0.302861,"ng representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shift-reduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based treeto-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more similar (with a different parsing algorithm) to our approach, but their parser relies on external tools, such as dependency parsing, semantic role labeling or named entity recognition. The input of our parser is"
D17-1130,J13-4006,0,0.362333,"ts further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description"
D17-1130,P82-1020,0,0.833413,"Missing"
D17-1130,N16-1030,1,0.90375,"g data. Adding additional information, such as POS tags and dependency trees, improves the results further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al.,"
D17-1130,N15-1142,0,0.035175,"g of the node e, and qe is a bias term for the node e. It is important to mention that this implies finding a propbank sense or a lemma. For that, we rely entirely on the AMR training set instead of using additional resources. Given that the system runs two softmax operations, one to predict the action to take and the second one to predict the corresponding AMR node, and they both share LSTMs to make predictions, we include an additional layer with a tanh nonlinearity after st for each softmax. 3.3 Word Representations We use character-based representations of words using bidirectional LSTMs (Ling et al., 2015b; Ballesteros et al., 2015). They learn representations for words that are orthographically similar. Note that they are updated with the updates to the model. Ballesteros et al. (2015) and Lample et al. (2016) demonstrated that it is possible to achieve high results in syntactic parsing and named entity recognition by just using characterbased word representations (not even POS tags, in fact, in some cases the results with just characterbased representations outperform those that used explicit POS tags since they provide similar vectors for words with similar/same morphosyntactic tag (Ballest"
D17-1130,D15-1176,0,0.040119,"Missing"
D17-1130,D16-1183,0,0.0221165,"on, such as POS tags and dependency trees, improves the results further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia)"
D17-1130,W03-3017,0,0.231456,"hawan Road, Route 134 Yorktown Heights, NY 10598. U.S miguel.ballesteros@ibm.com, onaizan@us.ibm.com Abstract We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Da"
D17-1130,W04-0308,0,0.142613,"• SWAP: pops the two top items at the top of the STACK, pushes the second node to the front of the BUFFER, and pushes the first one back into the STACK. This action allows nonprojective arcs as in (Nivre, 2009) but it also helps to introduce reentrancies. At oracle time, SWAP is produced when the word at the top of the stack is blocking actions that may happen between the second element at the top of the stack and any of the words in the buffer. the semantic actions presented by Henderson et al. (2013), the transition-based NER algorithm by Lample et al. (2016) and the arc-standard algorithm (Nivre, 2004). As in (Ballesteros and Nivre, 2013) the buffer starts with the root symbol at the end of the sequence. Figure 2 shows a running example. The transition inventory is the following: • SHIFT: pops the front of the BUFFER and push it to the STACK. • CONFIRM: calls a subroutine that predicts the AMR node corresponding to the top of the STACK. It then pops the word from the STACK and pushes the AMR node to the STACK . An example is the prediction of a propbank sense: From occured to occur-01. • REDUCE: pops the top of the STACK. It occurs when the word/node at the top of the stack is complete (no"
D17-1130,J08-4003,0,0.388535,"Missing"
D17-1130,D16-1065,0,0.616574,"y recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016) and Dyer et al. (2015), we present a shift-reduce algorithm that produces AMR graphs directly from plain text. Wang et al. (2015b,a); Zhou et al. (2016); Goodman et al. (2016) presented transition-based treeto-graph transducers that traverse a dependency tree and transforms it to an AMR graph. Damonte et al. (2016)’s input is a sentence and it is therefore more similar (with a differen"
D17-1130,P09-1040,0,0.122386,"and/or words) and a BUFFER that contains the words that have yet to be processed. The parsing algorithm is inspired from 2 We use the dynamic framework of Neubig et al. (2017) to implement our parser. 1269 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1269–1275 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics • SWAP: pops the two top items at the top of the STACK, pushes the second node to the front of the BUFFER, and pushes the first one back into the STACK. This action allows nonprojective arcs as in (Nivre, 2009) but it also helps to introduce reentrancies. At oracle time, SWAP is produced when the word at the top of the stack is blocking actions that may happen between the second element at the top of the stack and any of the words in the buffer. the semantic actions presented by Henderson et al. (2013), the transition-based NER algorithm by Lample et al. (2016) and the arc-standard algorithm (Nivre, 2004). As in (Ballesteros and Nivre, 2013) the buffer starts with the root symbol at the end of the sequence. Figure 2 shows a running example. The transition inventory is the following: • SHIFT: pops th"
D17-1130,D15-1136,0,0.177855,"luated on the LDC2014 dataset. This means that the score is not directly comparable with the rest. The second entry (0.64) for Damonte et al. is calculated by training their parser with the LDC2014 training set which makes it directly comparable with the rest of the parsers. 1272 Model Flanigan et al. (2014)* (POS, DEP) Flanigan et al. (2016)* (POS, DEP, NER) Werling et al. (2015)* (POS, DEP, NER) Damonte et al. (2016)8 (POS, DEP, NER, SRL) Damonte et al. (2016)8 (POS, DEP, NER, SRL) Artzi et al. (2015) (POS, CCG) Goodman et al. (2016)* (POS, DEP, NER) Zhou et al. (2016)* (POS, DEP, NER, SRL) Pust et al. (2015) (LM, NER) Pust et al. (2015) (Wordnet, LM, NER) Wang et al. (2015b)* (POS, DEP, NER) Wang et al. (2015a)* (POS, DEP, NER, SRL) O UR PARSER (N O PRETRAINED -N O CHARS ) O UR PARSER (N O PRETRAINED -W ITH CHARS ) O UR PARSER (W ITH PRETRAINED -N O CHARS ) O UR PARSER O UR PARSER (POS) O UR PARSER (POS, DEP) F1 (Newswire) 0.59 0.62 0.62 – – 0.66 0.70 0.71 – – 0.63 0.70 0.64 0.66 0.66 0.68 0.68 0.69 F1 (ALL) 0.58 0.59 – 0.61 0.64 – – 0.66 0.61 0.66 0.59 0.66 0.59 0.61 0.62 0.63 0.63 0.64 Table 1: AMR results on the LDC2014T12 dataset; Newsire section (left) and full (right). Rows labeled with O U"
D17-1130,K16-1019,1,0.939202,"on Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by"
D17-1130,N03-1033,0,0.0567966,"cter˜ C ); and a fixed vector repbased representation (w ˜ LM ). resentation from a neural language model (w A linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, ˜ C; w ˜ LM ] + b} . x = max {0, V[w where V is a learned parameter matrix, b is a bias term and wC is the character-based learned rep˜ LM is the pretrained resentation for each word, w word representation. 3.4 POS Tagging and Dependency Parsing We may include preprocessed POS tags or dependency parses to incorporate more information into our model. For the POS tags we use the Stanford tagger (Toutanova et al., 2003) while we use the Dyer et al. (2015)’s Stack-LSTM parser trained on the English CoNLL 2009 dataset (Hajiˇc et al., 2009) to get the dependencies. POS tags: The POS tags are preprocessed and a learned representation tag is concatenated with the word representations. This is the same setting as (Dyer et al., 2015). Dependency Trees: We use them in the same way as POS tags by concatenating a learned representation dep of the dependency label to the parent with the word representation. Additionally, we enrich the state representation st , presented in Section 3.2. If the two words at the top of th"
D17-1130,P15-2141,0,0.497685,"umoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016)"
D17-1130,N15-1040,0,0.678344,"umoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated features (Flanigan et al., 2014; Zhou et al., 2016; Werling et al., 2015; Wang et al., 2015b, inter-alia). 1 Check (Banarescu et al., 2013) for a complete description of AMR graphs. Inspired by Wang et al. (2015b,a); Goodman et al. (2016); Damonte et al. (2016)"
D17-1130,P15-1095,0,0.0968759,"Missing"
D17-1130,W03-3023,0,0.120294,"n Research Center, 1101 Kitchawan Road, Route 134 Yorktown Heights, NY 10598. U.S miguel.ballesteros@ibm.com, onaizan@us.ibm.com Abstract We present a transition-based AMR parser that directly generates AMR parses from plain text. We use Stack-LSTMs to represent our parser state and make decisions greedily. In our experiments, we show that our parser achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al."
D17-1130,J11-1005,0,0.0206417,"er achieves very competitive scores on English using only AMR training data. Adding additional information, such as POS tags and dependency trees, improves the results further. 1 Introduction Transition-based algorithms for natural language parsing (Yamada and Matsumoto, 2003; Nivre, 2003, 2004, 2008) are formulated as a series of decisions that read words from a buffer and incrementally combine them to form syntactic structures in a stack. Apart from dependency parsing, these models, also known as shift-reduce algorithms, have been successfully applied to tasks like phrase-structure parsing (Zhang and Clark, 2011; Dyer et al., 2016), named entity recognition (Lample et al., 2016), CCG parsing (Misra and Artzi, 2016) joint syntactic and semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016) and even abstract-meaning representation parsing (Wang et al., 2015b,a; Damonte et al., 2016). AMR parsing requires solving several natural language processing tasks; mainly named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling.1 Given the difficulty of building an end-to-end system, most prior work is based on pipelines or heavily dependent on precalculated"
E12-2012,ballesteros-nivre-2012-maltoptimizer-system,1,0.567209,"ation tools for machine learning, such as Paramsearch (Van den Bosch, 2004). In addition, Nilsson and Nugues (2010) has explored automatic feature selection specifically for MaltParser, but MaltOptimizer is the first system that implements a complete customized optimization process for this system. In the rest of the paper, we describe the optimization process implemented in MaltOptimizer (Section 2), report experiments (Section 3), outline the demonstration (Section 4), and conclude (Section 5). A more detailed description of MaltOptimizer with additional experimental results can be found in Ballesteros and Nivre (2012). 2 The MaltOptimizer System MaltOptimizer is written in Java and implements an optimization procedure for MaltParser based on the heuristics described in Nivre and Hall (2010). The system takes as input a training set, consisting of sentences annotated with dependency trees in CoNLL data format,1 and outputs an optimized MaltParser configuration together with an estimate of the final parsing accuracy. The evaluation metric that is used for optimization by default is the labeled attachment score (LAS) excluding punctuation, that is, the percentage of non-punctuation tokens that are assigned th"
E12-2012,W06-2920,0,0.556884,"apted to new settings provided that we can find suitable training data. However, such components may require careful feature selection and parameter tuning in order to Joakim Nivre Uppsala University Sweden joakim.nivre@lingfil.uu.se give optimal performance, a task that can be difficult for application developers without specialized knowledge of each component. A typical example is MaltParser (Nivre et al., 2006), a widely used transition-based dependency parser with state-of-the-art performance for many languages, as demonstrated in the CoNLL shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). MaltParser is an open-source system that offers a wide range of parameters for optimization. It implements nine different transition-based parsing algorithms, each with its own specific parameters, and it has an expressive specification language that allows the user to define arbitrarily complex feature models. Finally, any combination of parsing algorithm and feature model can be combined with a number of different machine learning algorithms available in LIBSVM (Chang and Lin, 2001) and LIBLINEAR (Fan et al., 2008). Just running the system with default settings when tr"
E12-2012,C10-1093,0,0.35669,"er, who can use the tool for black box optimization, as well as expert users, who can use it interactively to speed up optimization. Experiments on a number of data sets show that using MaltOptimizer for completely automatic optimization gives consistent and often substantial improvements over the default settings for MaltParser. The importance of feature selection and parameter optimization has been demonstrated for many NLP tasks (Kool et al., 2000; Daelemans et al., 2003), and there are general optimization tools for machine learning, such as Paramsearch (Van den Bosch, 2004). In addition, Nilsson and Nugues (2010) has explored automatic feature selection specifically for MaltParser, but MaltOptimizer is the first system that implements a complete customized optimization process for this system. In the rest of the paper, we describe the optimization process implemented in MaltOptimizer (Section 2), report experiments (Section 3), outline the demonstration (Section 4), and conclude (Section 5). A more detailed description of MaltOptimizer with additional experimental results can be found in Ballesteros and Nivre (2012). 2 The MaltOptimizer System MaltOptimizer is written in Java and implements an optimiz"
E12-2012,P05-1013,1,0.72883,"performing algorithm and creates a new option file for the best configuration so far. The user is again given the opportunity to edit the option file (or stop the process) before optimization continues. 2.3 Figure 2: Decision tree for best non-projective algorithm (+PP for pseudo-projective parsing). Stack algorithms (Nivre, 2009; Nivre et al., 2009) Both the Covington group and the Stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (Nivre and Nilsson, 2005). In phase 2, MaltOptimizer explores the parsing algorithms implemented in MaltParser, based on the data characteristics inferred in the first phase. In particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc-standard versions of Nivre’s algorithm, the projective version of Covington’s projective parsing algorithm and the projective Stack algorithm. The system follows a decision tree considering the characteristics of each algorithm, which is shown in Figure 1. On the other hand, if the training"
E12-2012,P81-1022,0,0.82109,"Missing"
E12-2012,W09-3811,1,0.567952,"st projective algorithm. in Figure 1 and Figure 2 and picking the algorithm that gives the best results after traversing both. Once the system has finished testing each of the algorithms with default settings, MaltOptimizer tunes some specific parameters of the best performing algorithm and creates a new option file for the best configuration so far. The user is again given the opportunity to edit the option file (or stop the process) before optimization continues. 2.3 Figure 2: Decision tree for best non-projective algorithm (+PP for pseudo-projective parsing). Stack algorithms (Nivre, 2009; Nivre et al., 2009) Both the Covington group and the Stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (Nivre and Nilsson, 2005). In phase 2, MaltOptimizer explores the parsing algorithms implemented in MaltParser, based on the data characteristics inferred in the first phase. In particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc-standard versions of"
E12-2012,W03-3017,1,0.554527,"Missing"
E12-2012,J08-4003,1,0.472223,"Missing"
E12-2012,P09-1040,1,0.686176,"n tree for best projective algorithm. in Figure 1 and Figure 2 and picking the algorithm that gives the best results after traversing both. Once the system has finished testing each of the algorithms with default settings, MaltOptimizer tunes some specific parameters of the best performing algorithm and creates a new option file for the best configuration so far. The user is again given the opportunity to edit the option file (or stop the process) before optimization continues. 2.3 Figure 2: Decision tree for best non-projective algorithm (+PP for pseudo-projective parsing). Stack algorithms (Nivre, 2009; Nivre et al., 2009) Both the Covington group and the Stack group contain algorithms that can handle non-projective dependency trees, and any projective algorithm can be combined with pseudo-projective parsing to recover non-projective dependencies in postprocessing (Nivre and Nilsson, 2005). In phase 2, MaltOptimizer explores the parsing algorithms implemented in MaltParser, based on the data characteristics inferred in the first phase. In particular, if there are no non-projective dependencies in the training set, then only projective algorithms are explored, including the arc-eager and arc"
E12-2012,nivre-etal-2006-maltparser,1,\N,Missing
E12-2012,D07-1096,1,\N,Missing
E17-1117,P16-1231,0,0.223878,"Missing"
E17-1117,C02-1126,0,0.0882932,"Missing"
E17-1117,D16-1257,0,0.541811,"n all past actions. The joint probability estimate p(x, y) can be used for both phrase-structure parsing (finding arg maxy p(y |x)) and language modeling (finding p(x) by marginalizing over the set of possible parses for x). Both inference problems can be solved using an importance sampling procedure.4 We report all RNNG performance based on the corrigendum to Dyer et al. (2016). 3 Composition is Key Given the same data, under both the discriminative and generative settings RNNGs were found to parse with significantly higher accuracy than (respectively) the models of Vinyals et al. (2015) and Choe and Charniak (2016) that represent y as a “linearized” sequence of symbols and parentheses without explicitly capturing the tree structure, or even constraining the y to be a well-formed tree (see Table 1). Vinyals et al. (2015) directly predict the sequence of nonterminals, “shifts” (which consume a terminal symbol), and parentheses from left to right, conditional on the input terminal sequence x, while Choe and Charniak (2016) used a sequential LSTM language model on the same linearized trees to create a generative variant of the Vinyals et al. (2015) model. The generative model is used to re-rank parse candid"
E17-1117,P97-1003,0,0.860051,"guistics: Volume 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequen"
E17-1117,de-marneffe-etal-2006-generating,0,0.0559403,"Missing"
E17-1117,P81-1022,0,0.79275,"Missing"
E17-1117,N16-1024,1,0.631277,"ial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model’s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis. 1 Introduction In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016), designed to model syntactic derivations of sentences. We focus on RNNGs as generative probabilistic models over trees, as summarized in §2. Fitting a probabilistic model to data has often been understood as a way to test or confirm some aspect of a theory. We talk about a model’s assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it “discovers” from the data. In some sense, such models can be thought of as mini-scientists. Neural networks, including RNNGs, are capable of representing larger classes of hypotheses tha"
E17-1117,Q16-1023,0,0.0670931,"Missing"
E17-1117,P02-1017,0,0.525775,"(that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.01) IBM (0.25) both (0.02) stocks (0.03) and ("
E17-1117,P03-1054,0,0.278815,"e 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequence of actions to construct"
E17-1117,D15-1278,0,0.0303018,"f understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on"
E17-1117,N16-1082,0,0.0221556,"put the verb as the head of the verb phrase. Another interesting finding is that the model pays attention to polarity information, where negations are almost always assigned nontrivial attention weights.7 Furthermore, we find that the model attends to the conjunction terminal in conjunctions of verb phrases (e.g., “VP → VP and VP”, 10), reinforcing the similar finding for conjunction of noun phrases. PPs. In almost all cases, the model attends to the preposition terminal instead of the noun phrases or complete clauses under it, regardless of the type of preposition. Even when the preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3"
E17-1117,P92-1017,0,0.673069,"ecessary. If the endocentric hypothesis is true (that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.0"
E17-1117,P06-1055,0,0.0272857,"information, even when trained on a fairly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The"
E17-1117,E09-1080,0,0.0214366,"ly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models"
E17-1117,D16-1159,0,0.0208633,"ads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sen"
E17-1117,D16-1137,0,0.0306962,", essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG"
E17-1117,W07-2416,0,0.0109927,"preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3 2.5 2 1.5 1 ADJP VP NP PP QP SBAR Figure 3: Average perplexity of the learned attention vectors on the test set (blue), as opposed to the average perplexity of the uniform distribution (red), computed for each major phrase type. 5.2 Comparison to Existing Head Rules To better measure the overlap between the attention vectors and existing head rules, we converted the trees in PTB §23 into a dependency representation using the attention weights. In this case, the attention weight functions as a “dynamic” head rule, where all other constituents within the same composed phrase are considered t"
E17-1117,J98-4004,0,0.453425,"and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and nonterminal (Johnson, 1998; Klein and Manning, 2003) augmentations. The conjecture that fine-grained nonterminal rules and labels can be discovered given weaker bracketing structures was based on several studies (Chiang Conclusion We probe what recurrent neural network grammars learn about syntax, through ablation scenarios and a novel variant with a gated attention mechanism on the composition function. The composition function, a key differentiator between the RNNG and other neural models of syntax, is crucial for good performance. Using the attention vectors we discover that the model is learning something similar t"
E17-2017,D15-1041,1,0.823519,"esentations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is th"
E17-2017,W16-6208,0,0.0726224,"Missing"
E17-2017,P82-1020,0,0.842587,"Missing"
E17-2017,N16-1030,1,0.449927,"s Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is that the RNNs take into account sequences of words and thus, the entire context. 3.1 Bi-Directional LSTMs Given the proven effectiveness and the impact of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional 5 The output embeddings of the emojis have 100 dimensions. 6 100 dimensions. 3 https://dev.twitter.com 4 Available at http://sempub.taln.upf.edu/tw/eacl17 106 3.2 Baselines In this Section we describe the two baselines. Unlike the previous model, the baselines do not take into account the word order. However, in the second baseline (Section 3.2.2) we abstract on the plain word representation using semantic vectors, previously trained on Twitter"
E17-2017,L16-1626,1,0.335627,"advent of social media has brought along a novel way of communication where meaning is composed by combining short text messages and visual enhancements, the so-called emojis. This visual language is as of now a de-facto standard for online communication, available not only in Twitter, but also in other large online platforms such as Facebook, Whatsapp, or Instagram. Despite its status as language form, emojis have been so far scarcely studied from a Natural Language Processing (NLP) standpoint. Notable exceptions include studies focused on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a; Barbieri et al., 2016b; Barbieri et al., 2016c; Eisner et al., 2016; Ljubeˇsic and Fiˇser, 2016), or sentiment (Novak et al., 2015). However, the interplay between text-based messages 1 https://www.washingtonpost.com/news/theintersect/wp/2016/02/19/the-secret-meanings-of-emoji/ 2 http://www.dailydot.com/debug/emojimiscommunicate/ 105 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 105–111, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Long Short-term Memory Network"
E17-2017,N15-1142,0,0.0599508,"his paper. Word Representations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the R"
E17-2017,W10-2914,0,0.0154471,"h token of the tweet. Formally, each message m is represented with the vector Vm : P St V m = t∈Tm |Tm | 4 5 R .60 .60 .59 .61 .61 .63 Table 2: Results of 5, 10 and 20 emojis. Precision, Recall, F-measure. BOW is bag of words, AVG is the Skipgram Average model, C refers to charBLSTM and W refers to word-BLSTM. +P refers to pretrained embeddings. Bag of Words We applied a bag of words classifier as baseline, since it has been successfully employed in several classification tasks, like sentiment analysis and topic modeling (Wallach, 2006; Blei, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010). We represent each message with a vector of the most informative tokens (punctuation marks included) selected using term frequency−inverse document frequency (TFIDF). We employ a L2-regularized logistic regression classifier to make the predictions. 3.2.2 P .59 .60 .59 .61 .61 .63 First Experiment This experiment is a classification task, where in each tweet the unique emoji is removed and 7 107 https://catalog.ldc.upenn.edu/LDC2003T05 vised pre-trained semantic knowledge (Ling et al., 2015a), which helps to achieve better results. Emoji P R F1 Rank Num 0.48 0.32 0.35 0.31 0.24 0.46 1 0.44 0."
E17-2017,D15-1176,0,0.0383434,"his paper. Word Representations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the R"
E17-2017,P16-2044,0,0.0123791,"The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is that the RNNs take into account sequences of words and thus, the entire context. 3.1 Bi-Directional LSTMs Given the proven effectiveness and the impact of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional 5 The output embeddings of the emojis have 100 dimensions. 6 100 dimensions. 3 https://dev.twitter.com 4 Available at http://sempub.taln.upf.edu/tw/eacl17 106 3.2 Baselines In this Section we describe the two baselines. Unlike the previous model, the baselines do not take into account the word order. However, in the second baseline (Section 3.2.2) we abstract on the plain word representation using semantic vectors, previously trained on Twitter data. 3.2.1 BOW AVG W C W+P C+P Skip-Gram Vector Average Where Tm are the set of tokens include"
E17-2017,W16-2610,0,0.116772,"Missing"
E17-2017,P15-1033,1,0.46148,"f the LSTMs are word embeddings6 . Following, we present two alternatives explored in the experiments presented in this paper. Word Representations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1"
E17-2017,N16-1064,0,0.0376523,"and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is that the RNNs take into account sequences of words and thus, the entire context. 3.1 Bi-Directional LSTMs Given the proven effectiveness and the impact of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional 5 The output embeddings of the emojis have 100 dimensions. 6 100 dimensions. 3 https://dev.twitter.com 4 Available at http://sempub.taln.upf.edu/tw/eacl17 106 3.2 Baselines In this Section we describe the two baselines. Unlike the previous model, the baselines do not take into account the word order. However, in the second baseline (Section 3.2.2) we abstract on the plain word representation using semantic vectors, previously trained on Twitter data. 3.2.1 BOW AVG"
E17-2017,P11-1015,0,\N,Missing
I13-1123,H05-1066,0,0.0382647,"Missing"
I13-1123,C12-1007,0,0.114532,"of state-of-theart parsers. We trained different systems over a large Spanish treebank and tested them over a treebank from a different domain. Our experiments show that though the obtained LAS is high, the performance over some concrete labels is very low in all cases, limiting the usability of the parsed data for tasks than rely on label-specific accuracy. One important future line is to look for parser modifications that allow the system to perform better in the labels we are interested in. To do so, one idea would be to use semantic features to give more information to the parser like in (Agirre et al., 2012). We did some preliminary experiments in that line, using information about the semantic classes for common nouns (specifically for the classes human and location), but the results showed that this information did not lead to a better performance of the parser. This is probably due to the sparsity of this information, but it is still an interesting line to study, since it lead to good results in other cases (Agirre et al., 2012). 4.1 Specific Label Performance The results obtained in terms of LAS are very satisfactory (the best parsing results reported for Spanish so far), specially for Mate p"
I13-1123,ballesteros-nivre-2012-maltoptimizer-system,1,0.843917,"3 tokens 471,624 114,610 41,620 Table 1: Sizes of the used corpora Experiments 2 sentences 33,679 8,125 3,376 MaltParser (Nivre and Hall, 2005; Nivre et al., 2007b) is a transition-based dependency parser generator. It was one of the best parsers in the CoNLL Shared Tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) and it contains four different families of transition-based parsers. A transition-based parser is based on an automaton that performs shift-reduce operations, whose transitions manage the input words in order to assign dependencies between them. MaltOptimizer (Ballesteros and Nivre, 2012) is a system designed to optimize MaltParser models by implementing a search of parameters and feature specifications. MaltOptimizer takes a training set in CoNLL data format,4 and provides an optimal configuration that includes the best parsing algorithm, parsing parameters and a complex feature model over the data structures involved in the parsing process. MaltOptimizer searches the optimal model maximizing the score of a single evaluation measure, either LAS, LCM (Labeled complete match) or unlabeled evaluation measures. As we mentioned in section 2, our intention is to enhance the perform"
I13-1123,W06-2932,0,0.0123438,"zer modifications that we explained in section 3.2.1. This therefore confirms our expectations that optimizing over very frequent labels may improve the overall accuracy6 . The algorithm selected by MaltOptimizer was Covington non-projective, which is described by Covington (2001) and included in MaltParser by Nivre (2008). For the Mate parsers, we used the default training settings for the graph-based parser. For the transition-based parser, we used a beam size of 40 and 25 training iterations. 4 Results and Discussion MSTParser is an arc-factored spanning tree parser (McDonald et al., 2005; McDonald et al., 2006). It implements a graph-based second-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint"
I13-1123,E12-1009,1,0.806962,"-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint PoS tagging and dependency parsing (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012). We refer to the graphbased parser as Mate-G, to the transition-based parser without graph-based re-scoring as MateT, and to the transition-based parser with enabled graph-based completion model to Mate-C. These parsers benefit from a passive-aggressive perceptron algorithm implemented as a Hash Kernel, which makes the parser fast to train and improves accuracy. Mate-T provides joint PoS tagging and dependency parsing to give account for the interaction between morphology and syntax. It uses a beam search over the space of possible transitions and Parser Malt MST Mate"
I13-1123,P08-3010,0,0.0604335,"Missing"
I13-1123,D12-1133,1,0.715122,"ich scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint PoS tagging and dependency parsing (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012). We refer to the graphbased parser as Mate-G, to the transition-based parser without graph-based re-scoring as MateT, and to the transition-based parser with enabled graph-based completion model to Mate-C. These parsers benefit from a passive-aggressive perceptron algorithm implemented as a Hash Kernel, which makes the parser fast to train and improves accuracy. Mate-T provides joint PoS tagging and dependency parsing to give account for the interaction between morphology and syntax. It uses a beam search over the space of possible transitions and Parser Malt MST Mate-T Mate-C Mate-G IULA Tes"
I13-1123,C10-1011,1,0.806568,"4 Results and Discussion MSTParser is an arc-factored spanning tree parser (McDonald et al., 2005; McDonald et al., 2006). It implements a graph-based second-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the whole tree. 3.2.3 Joint tagger Transition-based parser and Graph-based parser with Hash Kernel and Beam Search- Mate tools The Mate-Tools provide two parser types: a graphbased (Bohnet, 2010) and a transition-based parser with graph-based re-scoring (completion model) that is able to perform joint PoS tagging and dependency parsing (Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012). We refer to the graphbased parser as Mate-G, to the transition-based parser without graph-based re-scoring as MateT, and to the transition-based parser with enabled graph-based completion model to Mate-C. These parsers benefit from a passive-aggressive perceptron algorithm implemented as a Hash Kernel, which makes the parser fast to train and improves accuracy. Mate-T provides joint PoS tagging and depend"
I13-1123,A97-1052,0,0.0979151,"Missing"
I13-1123,W06-2920,0,0.143429,"periments using IULA Spanish LSP Treebank2 (Marimon et al., 2012). This corpus (henceforth IULA) contains the syntactic annotation of 42,000 sentences (around 590,000 tokens) taken from domain-specific (technical literature) texts. We used the train and test partitions provided by the Treebank developers which are publicly available for replicability.3 Furthermore, we used the Tibidabo Treebank (Marimon, 2010) as an alternative test set. Tibidabo contains a set of sentences extracted from the Ancora corpus (Taul´e et al., 2008), which was used in the CoNLL-X Shared Task of dependency parsing (Buchholz and Marsi, 2006). The Tibidabo Treebank was annotated using the same guidelines as IULA Treebank. Therefore, it has the same functions and tag-set as IULA, but since the sentences come from a completely different corpus, it represents a good evaluation frame with regards to the influence of domain change. In summary, we used a training set to train the different models and two different test sets to evaluate each model. See table 1 for details about the size of the different partitions. The treebanks used in this work contain up to 25 different dependency relations. In this work we will pay special attention"
I13-1123,J08-4003,0,0.0133764,"are those obtained with the configuration that leads to better LAS results over IULA Test. The same configuration is used to annotate Tibidabo. For MaltParser, the best LAS was obtained when optimizing the parser for the DO dependency label, which was obtained by applying the MaltOptimizer modifications that we explained in section 3.2.1. This therefore confirms our expectations that optimizing over very frequent labels may improve the overall accuracy6 . The algorithm selected by MaltOptimizer was Covington non-projective, which is described by Covington (2001) and included in MaltParser by Nivre (2008). For the Mate parsers, we used the default training settings for the graph-based parser. For the transition-based parser, we used a beam size of 40 and 25 training iterations. 4 Results and Discussion MSTParser is an arc-factored spanning tree parser (McDonald et al., 2005; McDonald et al., 2006). It implements a graph-based second-order parsing model which scores all possible dependency arcs in the sentence and then extracts the dependency tree with the highest score. The score of the trees is calculated basically adding the score of every arc, having at the end a sum with the score of the w"
I13-1123,W09-3828,0,0.0462243,"Missing"
I13-1123,taule-etal-2008-ancora,0,0.0494356,"Missing"
I13-1123,marimon-etal-2012-iula,0,0.175262,"Missing"
I13-1123,D07-1013,0,0.0432006,"Missing"
I13-1123,D07-1096,0,\N,Missing
I13-2007,nilsson-nivre-2008-malteval,0,0.189776,"he system, we can then infer which arcs were moved due to pseudo-projective parsing. In fact, this is something that an user could do manually in the current version of MaltDiver. A great idea would be to integrate MaltDiver with MaltOptimizer (Ballesteros and Nivre, 2012) in order to understand how the parser changes its behavior by updating the features selected. and the one to the right is the buffer. 4 Related Work The importance of visualization systems has been evidenced during the last years in the NLP community. In the parsing and generation area we can find systems, such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet et al., 2000), XLDD (Culy et al., 2011) or more recently, TreeExplorer (Thiele et al., 2013), which are, among other things, systems that visualize parse trees for evaluation and to provide the option of exploring dependency structures. We also consider relevant and motivated in a similar way the work developed by Christopher Collins et al. about visualization of linguistic data in the Computer Graphics area (Collins et al., 2009a; Collins et al., 2009b), in which they present interactive visualization systems for NLP in discourse analysis, document content and even ma"
I13-2007,N09-2066,0,0.0299373,"Missing"
I13-2007,ballesteros-nivre-2012-maltoptimizer-system,1,0.770921,"rojective transformation of Nivre and Nilsson (2005) in the system process. We believe that the implementation of this step is rather straightforward, because we would only have to trace the projective parsing process –as we have already done– resulting in the pseudo-projective tree before post-processing. By comparing this to the final tree output by the system, we can then infer which arcs were moved due to pseudo-projective parsing. In fact, this is something that an user could do manually in the current version of MaltDiver. A great idea would be to integrate MaltDiver with MaltOptimizer (Ballesteros and Nivre, 2012) in order to understand how the parser changes its behavior by updating the features selected. and the one to the right is the buffer. 4 Related Work The importance of visualization systems has been evidenced during the last years in the NLP community. In the parsing and generation area we can find systems, such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet et al., 2000), XLDD (Culy et al., 2011) or more recently, TreeExplorer (Thiele et al., 2013), which are, among other things, systems that visualize parse trees for evaluation and to provide the option of exploring dependency"
I13-2007,P05-1013,0,0.0418872,"can be achieved by using the following setting in MaltParser: -di true -dif filename.log 3 See www.maltparser.org/userguide.html 4 TikZ-dependency tool is available for downloading through https://sourceforge.net/projects/ tikz-dependency/ 26 Figure 2: A print-screen of the system for the following sentence written in Spanish: Este, a sus cuarenta a˜ nos de edad, sufre una terrible e imparable degeneraci´ on nerviosa. The structure to the left of the picture is the stack, and the one to the right is the buffer. We could also provide an implementation of the pseudo-projective transformation of Nivre and Nilsson (2005) in the system process. We believe that the implementation of this step is rather straightforward, because we would only have to trace the projective parsing process –as we have already done– resulting in the pseudo-projective tree before post-processing. By comparing this to the final tree output by the system, we can then infer which arcs were moved due to pseudo-projective parsing. In fact, this is something that an user could do manually in the current version of MaltDiver. A great idea would be to integrate MaltDiver with MaltOptimizer (Ballesteros and Nivre, 2012) in order to understand"
I13-2007,J13-1002,1,0.822148,"⇒ hΣ, B, H, Di Left-Arc (r): hΣ|i, j|B, H, Di ⇒ hΣ, j|B, H[i → j], D[i →r]}i if h(i) 6= 0. Right-Arc (r): hΣ|i, j|B, H, Di ⇒ hΣ|i|j, B, H[j → i], D[j →r]}i if h(j) = 0. Figure 1: Transition System for arc-eager algorithm. and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a), it was one of the best parsers. MaltParser contains four different families of transition-based parsers, the current version of MaltDiver only handles arc-eager parsing algorithm. These parsers mainly differ in the attachment of right-dependents, being the arceager greedier when right attachments have to be generated (Ballesteros and Nivre, 2013). Figure 1 shows the parsing transitions for Nivre arc-eager with reduce transition: (i) Shift, (ii) Reduce, (iii) Left-Arc and (iv) Right-Arc. Nivre’s arc-eager parsing algorithm makes use of two data structures in order to handle the input words: a buffer, which keeps the words that have to be read, and a stack, containing words that have already been processed but they are still available to producing a dependency arc. The Shift transition removes the first word in the buffer, and puts it on the top of the stack. The Reduce transition removes the word that is on the top of the stack because"
I13-2007,W00-1436,0,0.0648466,"re moved due to pseudo-projective parsing. In fact, this is something that an user could do manually in the current version of MaltDiver. A great idea would be to integrate MaltDiver with MaltOptimizer (Ballesteros and Nivre, 2012) in order to understand how the parser changes its behavior by updating the features selected. and the one to the right is the buffer. 4 Related Work The importance of visualization systems has been evidenced during the last years in the NLP community. In the parsing and generation area we can find systems, such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet et al., 2000), XLDD (Culy et al., 2011) or more recently, TreeExplorer (Thiele et al., 2013), which are, among other things, systems that visualize parse trees for evaluation and to provide the option of exploring dependency structures. We also consider relevant and motivated in a similar way the work developed by Christopher Collins et al. about visualization of linguistic data in the Computer Graphics area (Collins et al., 2009a; Collins et al., 2009b), in which they present interactive visualization systems for NLP in discourse analysis, document content and even machine translation parse trees. 5 6 Con"
I13-2007,W06-2920,0,0.124929,"Missing"
I13-2007,W03-3017,0,0.0592396,"show how they traverse the transition-system. We believe that there are mainly two different target researchers, that belong to different knowledge levels: (i) expert users who are willing to see how the parser behaves with a new set of features or with a different parsing constraint, 1 The system is available for download at http: //taln.upf.edu/pages/MaltDiver/. It includes examples and a complete readme file that explains how to use the tool. 25 The Companion Volume of the Proceedings of IJCNLP 2013: System Demonstrations, pages 25–28, Nagoya, Japan, 14-18 October 2013. parsing algorithm (Nivre, 2003) – but it would not be difficult to include new transition systems (Nivre, 2008) as we also mention in Section 5. MaltDiver processes the outcome of the diagnostic feature of MaltParser,2 which prints the transition sequence for each sentence of the test corpus. It basically shows the list of transitions and the dependency label selected (if available). Besides that, MaltDiver also makes use of the dependency tree produced in order to ensure the reliability of the transition sequences inferred in the MaltDiver processes. We included an extra option in MaltDiver which is the one that correspond"
I13-2007,J08-4003,0,0.167345,"two different target researchers, that belong to different knowledge levels: (i) expert users who are willing to see how the parser behaves with a new set of features or with a different parsing constraint, 1 The system is available for download at http: //taln.upf.edu/pages/MaltDiver/. It includes examples and a complete readme file that explains how to use the tool. 25 The Companion Volume of the Proceedings of IJCNLP 2013: System Demonstrations, pages 25–28, Nagoya, Japan, 14-18 October 2013. parsing algorithm (Nivre, 2003) – but it would not be difficult to include new transition systems (Nivre, 2008) as we also mention in Section 5. MaltDiver processes the outcome of the diagnostic feature of MaltParser,2 which prints the transition sequence for each sentence of the test corpus. It basically shows the list of transitions and the dependency label selected (if available). Besides that, MaltDiver also makes use of the dependency tree produced in order to ensure the reliability of the transition sequences inferred in the MaltDiver processes. We included an extra option in MaltDiver which is the one that corresponds with the allow root option in MaltParser.3 This option decides whether there i"
I13-2007,P13-4010,0,0.094496,"Missing"
I13-2007,D07-1013,0,\N,Missing
I13-2007,D07-1097,0,\N,Missing
I13-2007,D07-1096,0,\N,Missing
J13-1002,N09-2066,0,0.025541,"Missing"
J13-1002,W06-2920,0,0.118821,"de does not correspond to an input token, it has no welldeﬁned position in the node sequence deﬁned by the word order of a sentence and could in principle be inserted anywhere (or nowhere at all). One option that can be found in the literature is to insert it at the end of this sequence, but the more common convention in contemporary research on dependency parsing is to insert it at the beginning, hence treating it as a dummy word preﬁxed to the sentence. This is also the choice implicitly assumed in the CoNLL data format, used in the CoNLL shared tasks on dependency parsing in 2006 and 2007 (Buchholz and Marsi 2006; Nivre et al. 2007) and the current de facto standard for exchange of dependency annotated data. The question that concerns us here is whether the use of a dummy root node is just a harmless technicality permitting us to treat different dependency theories uniformly, and whether its placement in the input sequence is purely arbitrary, or whether both of these choices may in fact have an impact on the parsing accuracy that can be achieved with a given parsing model. In order to investigate this question empirically, we deﬁne three different types of dependency graphs that differ only with resp"
J13-1002,de-marneffe-etal-2006-generating,0,0.0131845,"type None is not a tree, but a forest, because it consists of two disjoint trees. 3. Experiments In order to test the hypothesis that the existence and placement of the dummy root node can have an impact on parsing accuracy, we performed an experiment using two widely used data-driven dependency parsers, MaltParser (Nivre, Hall, and Nilsson 2006) and MSTParser (McDonald 2006), and all the 13 data sets from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi 2006) as well as the English Penn Treebank converted to Stanford dependencies (de Marneffe, MacCartney, and Manning 2006). We created three different versions of each data set, corresponding to the representation types None, First, and Last, and used them to evaluate MaltParser with two different transition systems—arc-eager (Nivre 2003) and arc-standard (Nivre 2004)—and MSTParser with the arc-factored non-projective algorithm (McDonald et al. 2005). The results are shown in Table 1. When creating the data sets, we took the original version from the CoNLL-X shared task as None, because it does not include the dummy root node as an explicit input token. In this representation, the tokens of a sentence are indexed"
J13-1002,D07-1097,1,0.473771,"rties is clearly outside the scope of this article and has to be left for future research. Another limitation of the current study is that it only examines three different parsers, and although this is clearly sufﬁcient to prove the existence of the phenomenon it will be interesting to see whether the same patterns can be found if we examine more recent state-of-the-art methods, going from deterministic parsing to beam search for transition-based parsing and from arc-factored to higher-order models for graph-based parsing. In this context, it is also relevant to mention previous work, such as Hall et al. (2007) and Attardi and Dell’Orletta (2009), which have tried to improve parsing accuracy by switching or 11 Computational Linguistics Volume 39, Number 1 combining parsing directions, which implicitly has the effect of changing the position of the root node (if present). In conclusion, we believe there may be two methodological lessons to learn from our experiments. The ﬁrst is that, for certain parsing models, the existence and placement of the dummy root node is in fact a parameter worth tuning for best performance. Thus, for the deterministic arc-eager parser, it seems that we can obtain higher p"
J13-1002,J93-2004,0,0.0462133,"Missing"
J13-1002,H05-1066,0,0.0254842,"Missing"
J13-1002,W03-3017,1,0.603484,"acy, we performed an experiment using two widely used data-driven dependency parsers, MaltParser (Nivre, Hall, and Nilsson 2006) and MSTParser (McDonald 2006), and all the 13 data sets from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi 2006) as well as the English Penn Treebank converted to Stanford dependencies (de Marneffe, MacCartney, and Manning 2006). We created three different versions of each data set, corresponding to the representation types None, First, and Last, and used them to evaluate MaltParser with two different transition systems—arc-eager (Nivre 2003) and arc-standard (Nivre 2004)—and MSTParser with the arc-factored non-projective algorithm (McDonald et al. 2005). The results are shown in Table 1. When creating the data sets, we took the original version from the CoNLL-X shared task as None, because it does not include the dummy root node as an explicit input token. In this representation, the tokens of a sentence are indexed from 1 to n and the dependency graph is speciﬁed by giving each word a head index ranging from 0 to n, where 0 signiﬁes that the token is not a dependent on any other token in the sentence. The First version was creat"
J13-1002,W04-0308,1,0.783459,"t using two widely used data-driven dependency parsers, MaltParser (Nivre, Hall, and Nilsson 2006) and MSTParser (McDonald 2006), and all the 13 data sets from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi 2006) as well as the English Penn Treebank converted to Stanford dependencies (de Marneffe, MacCartney, and Manning 2006). We created three different versions of each data set, corresponding to the representation types None, First, and Last, and used them to evaluate MaltParser with two different transition systems—arc-eager (Nivre 2003) and arc-standard (Nivre 2004)—and MSTParser with the arc-factored non-projective algorithm (McDonald et al. 2005). The results are shown in Table 1. When creating the data sets, we took the original version from the CoNLL-X shared task as None, because it does not include the dummy root node as an explicit input token. In this representation, the tokens of a sentence are indexed from 1 to n and the dependency graph is speciﬁed by giving each word a head index ranging from 0 to n, where 0 signiﬁes that the token is not a dependent on any other token in the sentence. The First version was created by adding an extra token at"
J13-1002,nivre-etal-2006-maltparser,1,0.0564195,"Missing"
J13-1002,P05-1013,1,0.0505385,"makes the parser start with an empty stack instead of a stack containing an extra dummy root node.2 For MSTParser, we modiﬁed the parser implementation so that it extracts a maximum spanning tree that is still rooted in an extra dummy root node but where the score of a tree is based only on the scores of arcs connecting real token nodes. Finally, because MaltParser with the arc-eager and arc-standard transition systems can only construct projective dependency graphs, we projectivized all training sets before training the MaltParser models using the baseline pseudo-projective transformation of Nivre and Nilsson (2005).3 Except for these modiﬁcations, all parsers were run with out-of-the-box settings. 3.1 Deterministic Arc-Eager Parsing The arc-eager transition-based parser ﬁrst described in Nivre (2003) parses a sentence in a single pass from left to right, using a stack to store partially processed tokens and greedily choosing the highest-scoring parsing action at each point. The arc-eager property entails that every arc in the output graph is added at the earliest possible opportunity, which means that right-dependents are attached to their head before they have found their own right-dependents. This can"
J13-1002,J14-2001,1,\N,Missing
J13-1002,D07-1096,1,\N,Missing
J17-2002,Q16-1031,1,0.843163,"information in a joint model. Similar improvements may be achieved in an outof-domain scenario. Even though the parser is greedy, it provides very consistent results comparable with the best parsers of the state-of-the-art. We even obtained further improvement as demonstrated with the experiments with the dynamic oracles, that provide a push over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their"
J17-2002,P16-1231,0,0.190862,"w view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a"
J17-2002,P81-1022,0,0.756303,"Missing"
J17-2002,D12-1133,0,0.163485,"Missing"
J17-2002,Q13-1034,0,0.205338,"Missing"
J17-2002,W06-2920,0,0.0983953,"ddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section 6.2.1. 6.2.3 Data for the Dynamic Oracle. Because the arc-hybrid transition-based parsing algorithm is limited to fully projective trees, we use the same data as in Section 6.2.1, which makes it comparable with the basic model that uses standard word representations and a static oracle arc-standard algorithm. 6.2.4 CoNLL-2009 Data. We also report results with all the CoNLL 2009 data sets (Hajiˇc et al. 2009) to make a complete c"
J17-2002,P05-1022,0,0.0472943,"al embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, finding that they are quite benefic"
J17-2002,D14-1082,0,0.112043,"ork in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack"
J17-2002,C14-1078,0,0.146438,"Missing"
J17-2002,P13-1104,0,0.246028,"Missing"
J17-2002,P14-2111,0,0.126115,"Missing"
J17-2002,D07-1022,1,0.889561,"Missing"
J17-2002,W15-3904,0,0.0920647,"Missing"
J17-2002,P15-1033,1,0.882509,"therefore also investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,W13-5709,1,0.94387,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The dynamic oracles framework for training with exploration, suggested by Goldberg and Nivre (2012, 2013), provides answers to these questions. Although the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). 5.2.1 Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take under a given parser state. In contrast to static oracles that derive a canonical sequence for each gold parse tree and say nothing about parsing states that do not stem from this canonical path, the dynamic oracle is well-defined for states that result from parsing mistakes, and may produce more than a single gold action for a given state. Under the dynamic oracle framework, a parsing action is said to be optimal in a given state if the best tree that"
J17-2002,P11-2124,1,0.874063,"nt neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio, and Satta 2014; Honnibal, Goldb"
J17-2002,C12-1059,1,0.936735,"Missing"
J17-2002,Q13-1033,1,0.949811,"eaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled with the stack LSTM. Figure 4 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Because a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. 3.3.3 Arc-Hybrid. For the dynamic oracle training scenario, described in Section 5.2, we switch to the arc-hybrid transition system, which is amenable to an efficient dynamic oracle (Goldberg and Nivre 2013). The arc-hybrid system is summarized in Figure 5. The SHIFT and REDUCE - RIGHT transitions are the same as in arc-standard. However, the REDUCE - LEFT transition pops the top of the stack and attaches it as a child of the first item in the buffer. Although it is possible to extend the arc-hybrid system to support nonprojectivity by adding a SWAP transition, this extension would invalidate an important guarantee enabling efficient dynamic oracles.10 We therefore restrict the dynamic-oracle experiments to the fully projective English and Chinese treebanks. In order to parse nonprojective trees,"
J17-2002,Q14-1010,1,0.935088,"Missing"
J17-2002,P08-1043,1,0.846251,"014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio"
J17-2002,P13-2111,1,0.891593,"work uses continuous-valued relaxations of these. 3. Dependency Parser We now turn to the problem of learning representations of dependency parser states. We preserve the standard data structures of a transition-based dependency parser, namely, a buffer of words to be processed (B) and a stack (S) of partially constructed syntactic elements. Each stack element is augmented with a continuous-space vector embedding representing a word and, in the case of S, any of its syntactic dependents. Additionally, we introduce a third stack (A) to represent the history of transition actions taken by the 5 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. 316 } ) T L( IF DSH RE od am … pt | {z TO P {z S Greedy Transition-Based Dependency Parsing with Stack LSTMs B P TO | Ballesteros et al. } amod ; was made TO |{z } an decision overhasty P root ; REDUCE-LEFT(amod) A SHIFT … Figure 2 Parser state computation encountered while parsing the sentence an overhasty decision was made. Here S designates the stack of partially constructed dependency subtre"
J17-2002,D14-1099,0,0.164827,"Missing"
J17-2002,W09-1201,0,0.137744,"Missing"
J17-2002,N03-1014,0,0.736917,"ounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LS"
J17-2002,P04-1013,0,0.567345,"Missing"
J17-2002,P13-1088,0,0.0439757,"and qz is a bias term for action z. The set A(S, B) represents the valid transition actions that may be taken given the current contents of the stack and buffer.9 Because pt = f (st , bt , at ) encodes information about all previous decisions made by the parser, the chain rule may be invoked to write the probability of any valid sequence of parse transitions z conditional on the input as: p(z |w) = |z| Y t=1 p(zt |pt ) (2) 3.2 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Hermann and Blunsom 2013; Socher et al. 2011, 2013a, 2013b). We follow previous work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed in Section 4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S,"
J17-2002,W13-3518,1,0.937487,"Missing"
J17-2002,Q14-1011,0,0.100404,"Missing"
J17-2002,P11-1068,0,0.272942,"Missing"
J17-2002,N16-1030,1,0.677399,"Missing"
J17-2002,D14-1081,0,0.0383284,"crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by the stack, buffer, an"
J17-2002,N15-1142,1,0.846911,"and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with probability 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained Word Embeddings. There are several options for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches that discard order perform less well (Bansal, Gimpel, and Livescu 2014); therefore, we used a variant of the skip n-gram model introduced by Ling et al. (2015a), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. 4.2 Modeling Characters Instead of Words Following Ling et al. (2015b), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber 2005). When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character se"
J17-2002,D15-1176,1,0.903245,"Missing"
J17-2002,de-marneffe-etal-2006-generating,0,0.292516,"Missing"
J17-2002,D13-1032,0,0.0288931,"Experiments with Static Oracle and Standard Word Representations We report results on five experimental configurations per language, as well as the Chen and Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (−POS), the stack LSTM parsing model without pretrained language model embeddings (−pretraining), the 13 Training: 02-21. Development: 22. Test: 23. 14 Training: 001–815, 1001–1136. Development: 886–931, 1148–1151. Test: 816–885, 1137–1147. ¨ 15 The POS tags were calculated with MarMot tagger (Mueller, Schmid, and Schutze 2013) by the best ¨ performing system of the SPMRL Shared Task (Bjorkelund et al. 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 16 Because the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4,996 sentences of the training set as a development set. 328 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 1 Unlabeled attachment scores and labeled attachment scores on the development sets (top) and the final test"
J17-2002,W03-3017,0,0.216793,"ril 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—which is unboun"
J17-2002,W04-0308,0,0.395191,"embeddings of the head, dependent, and relation and applying a linear operator and a component-wise nonlinearity as follows: c = tanh (U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. Th"
J17-2002,J08-4003,0,0.37078,"4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S, B) is the complete set of parser actions discussed in Section 3.3, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT transition is obligatory (Nivre 2008). 318 Ballesteros et al. det Greedy Transition-Based Dependency Parsing with Stack LSTMs amod an overhasty mod an decision cc2 rel det head mod overhasty c1 head rel amod decision Figure 3 The representation of a dependency subtree (top) is computed by recursively applying composition functions to hhead, modifier, relationi triples. In the case of multiple dependents of a single head, the recursive branching order is imposed by the order of the parser’s reduce transition (bottom). in the order they are “reduced” in the parser, as illustrated in Figure 3. Each node in this expanded syntactic tr"
J17-2002,P09-1040,0,0.801651,"(U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. The arc-standard transition inventory (Nivre 2004) is given in Figure 4. The SHIFT transition moves a word from the buffer to the stack,"
J17-2002,W06-2933,0,0.104845,"Missing"
J17-2002,P05-1013,0,0.662475,"B B B (v, v), B Dependency — r u→v r u←v — Figure 4 Parser transitions of the arc-standard system (with swap, Section 3.3.2) indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. (gr (x, y), x) refers to the composition function presented in 3.2. the stack. The arc-standard system allows building all and only projective trees. In order to parse nonprojective trees, this can be combined with the pseudo-projective approach (Nivre and Nilsson 2005) or follow what is presented in Section 3.3.2. 3.3.2 Arc-Standard with Swap. In order to deal with nonprojectivity, the arc-standard system can be augmented with a SWAP transition (Nivre 2009). The SWAP transition removes the second-to-top item from the stack and pushes it back to the buffer, allowing for the creation of nonprojective trees. We only use this transition when the training data set contains nonprojective trees. The inclusion of the SWAP transition requires breaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled"
J17-2002,nivre-etal-2006-talbanken05,0,0.111357,"82.15 79.04 92.57 90.21 Words + POS UAS LAS 86.85 85.36 93.04 90.87 Words + Chars UAS LAS 81.90 78.81 92.56 90.38 Words + Chars + POS UAS LAS 86.92 85.49 92.75 90.62 Test: Language Chinese English 6.4.3 Comparison with State-of-the-Art Parsers. Table 5 shows a comparison with state-ofthe-art parsers. We include greedy transition-based parsers that, like ours, do not apply beam search. For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006), who also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. For English and Chinese, we report our results of the best parser on the develpment set in Section 6.3—which is Words + POS but with pretrained word embeddings. We also show the best reported results on these data sets. For the SPMRL data ¨ sets, the best performing system of the shared task is either Bjoreklund et al. (2013) or ¨ Bjorkelund et al. (2014), which are better than our system. Note that the comparison is harsh to our system, which does not use unlabeled data nor any"
J17-2002,J17-2002,1,0.0512826,"Missing"
J17-2002,P00-1061,0,0.0437166,"that make use of external embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, findin"
J17-2002,W14-6111,0,0.0979038,"Missing"
J17-2002,W13-4917,1,0.910835,"Missing"
J17-2002,seeker-kuhn-2012-making,0,0.0639458,"uage model word embeddings were generated from the AFE portion of the English Gigaword corpus (version 5), and from the complete Chinese Gigaword corpus (version 2), as segmented by the Stanford Chinese Segmenter (Tseng et al. 2005). 6.2.2 Data to Test the Character-Based Representations and Static Oracle for Training. For the character-based representations we applied our model to the treebanks of the SPMRL ¨ Shared Task (Seddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section"
J17-2002,P13-1045,0,0.0704272,"3, Number 2 manually crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by"
J17-2002,D13-1170,0,0.00710017,"Missing"
J17-2002,K16-1019,1,0.850904,"sh over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their help with the parsing algorithms. This work was sponsored in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319. Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENS"
J17-2002,P15-1150,0,0.158426,"Missing"
J17-2002,P07-1080,0,0.609122,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,W07-2218,0,0.744034,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,P15-3004,0,0.0618368,"so investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,N03-1033,0,0.365748,"Missing"
J17-2002,P06-3009,0,0.158184,"xisting models. For instance, Chrupala (2014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different w"
J17-2002,I05-3027,0,0.0858346,"Missing"
J17-2002,Q16-1014,0,0.139104,"Missing"
J17-2002,P15-1113,0,0.105221,"Missing"
J17-2002,P15-1032,0,0.317311,"parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can"
J17-2002,W03-3023,0,0.341093,"ised version received: 6 April 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—whi"
J17-2002,K15-1015,0,0.465378,"finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack pointer that is manipulated by push and pop o"
J17-2002,D15-1251,1,0.901456,"Missing"
J17-2002,P16-1147,0,0.0322281,"rd embeddings are useful for other languages we also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training set-up as in Section 4; for English and Chinese we used the same pretrained word embeddings as in previous experiments, for German we pretrained embeddings using the monolingual training data from the WMT 2015 data set22 , and for Spanish we used the Spanish Gigaword version 3. The results for the parser with character-based representations on these data sets (last line of the table) were published by Andor et al. (2016). In Zhang and Weiss (2016), it is also possible to find results of the same version of the parser on the Universal Dependency treebanks (Nivre et al. 2015). 21 We report the performance of these parsers in the most comparable set-up, that is, with beam = 1 or greedy. 22 http://www.statmt.org/wmt15/translation-task.html. 336 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 7 Results of the parser in its different versions including comparison with other systems. St. refers to static oracle with the arc-standard parser and Dyn. refers to dynamic oracle with the arc-hybrid parser with α"
J17-2002,D08-1059,0,0.177172,"Missing"
J17-2002,P11-2033,0,0.0417267,"tions on this sequence, and r the complete contents of the stack of partially constructed syntactic structures. This global sensitivity of the state representation contrasts with most previous work in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which"
J17-2002,P15-1117,0,0.148064,"ders only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be unders"
J17-2002,N07-1050,0,\N,Missing
J17-2002,D15-1041,1,\N,Missing
J17-2002,W13-4916,0,\N,Missing
J17-2002,J13-1002,1,\N,Missing
J17-2002,P14-6005,0,\N,Missing
J17-2002,W14-6110,0,\N,Missing
J17-2002,Q14-1017,0,\N,Missing
J17-2002,W13-4907,1,\N,Missing
J17-2002,P14-2131,0,\N,Missing
J17-2002,C14-1076,1,\N,Missing
J17-2002,W15-2210,0,\N,Missing
J17-2002,P15-2042,0,\N,Missing
K15-1029,D14-1082,0,0.0607156,"Missing"
K15-1029,P96-1025,0,0.561927,"from a treebank of constituent trees with head-child annotations in each constituent (Carreras et al., 2008): starting from a token, its spine consists of the non-terminal labels of the constituents whose head is the token; the parent node of the top of the spine gives information about the lexical head (by following the head children of the parent) and the position where the spine attaches to. Given a spinal tree it is trivial to recover the constituent and dependency trees. the following: • We define an arc-eager statistical model for spinal parsing that is based on the triplet relations by Collins (1996). Such relations, in conjunction with the partial spinal structure available in the stack of the parser, provide a very rich set of features. • We describe a set of conditions that an arceager strategy must guarantee in order to produce valid spinal structures. • In experiments using beam search we show that our method obtains a good tradeoff between speed and accuracy for both dependency-based attachment scores and constituent measures. 2 2.1 Background Spinal Trees A spinal tree is a generalization of a dependency tree that adds constituent structure to the dependencies in the form of spines"
K15-1029,P97-1003,0,0.593379,"Missing"
K15-1029,P15-1033,1,0.872346,"Missing"
K15-1029,P15-1147,0,0.13411,"Missing"
K15-1029,W08-1007,0,0.011848,"edy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical depend"
K15-1029,C14-1076,1,0.875508,"Missing"
K15-1029,W07-2444,0,0.0216766,"of that, with a greedy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do"
K15-1029,D12-1133,0,0.0201753,"and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others. Results marked with * are shiftreduce parsers. Carreras et al. (2008) was the first to use spinal representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses"
K15-1029,P10-1110,0,0.0875648,"Missing"
K15-1029,W08-2102,1,0.851515,"ier Carreras3 1 NLP Group, Pompeu Fabra University 2 Carnegie Mellon University 3 Xerox Research Centre Europe miguel.ballesteros@upf.edu xavier.carreras@xrce.xerox.com Abstract constituent-based PCFG that make a central use of lexical dependencies. An alternative approach is to view the combined representation as a dependency structure augmented with constituent information. This approach was first explored by Collins (1996), who defined a dependency-based probabilistic model that associates a triple of constituents with each dependency. In our case, we follow the representations proposed by Carreras et al. (2008), which we call spinal trees. In a spinal tree (see Figure 1 for an example), each token is associated with a spine of constituents, and head-modifier dependencies are attached to nodes in the spine, thus combining the two sources of information in a tight manner. Since spinal trees are inherently dependencybased, it is possible to extend dependency models for such representations, as shown by Carreras et al. (2008) using a so-called graph-based model. The main advantage of such models is that they allow a large family of rich features that include dependency features, constituent features and"
K15-1029,P08-1067,0,0.0676573,"Missing"
K15-1029,J14-2001,0,0.0196901,"configuration. The SHIFT transition removes the first node from the buffer and puts it on the stack. The REDUCE transition removes the top node from the stack. The LEFT- ARCt transition introduces a labeled dependency edge between the first element of the buffer and the top element of the stack with the label t. The top element is removed from the stack (reduce transition). The RIGHT- ARCt transition introduces a labeled dependency edge between the top element of the stack and the first element in the buffer with a label d, and it performs a shift transition. Each action can have constraints (Nivre et al., 2014), Figure 2 and Section 3.2 describe the constraints of the spinal parser. 290 (a) — Constituent Tree with head-children annotations S NP . VP DT NN This market VBN . VP has VBN VP been ADVP RB ADV very badly VBN damaged (b) — Spinal Tree S ? This NP VP ? market ? has VP VP ADVP ? been ? very ? badly ? damaged ? . Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the underlined child annotates the head child of the constituent. (b) The corresponding spinal tree. In this paper, we took the already existent implementation of arc-eager from ZPar1 ("
K15-1029,N15-1080,0,0.0315935,"Missing"
K15-1029,W03-3017,0,0.465803,"Carreras et al. (2008) using a so-called graph-based model. The main advantage of such models is that they allow a large family of rich features that include dependency features, constituent features and conjunctions of the two. However, the consequence is that the additional spinal structure greatly increases the number of dependency relations. Even though a graph-based model remains parseable in cubic time, it is impractical unless some pruning strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are We present a transition-based arc-eager model to parse spinal trees, a dependencybased represe"
K15-1029,W04-0308,0,0.103844,"uctured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also recently, Fern´andezGonz´alez and Martins (2015) proposed an arcfactored dependency model for constituent parsing. In that work dependency labels encode the constituent node where the dependency arises as well as the position index of that node in the head spine. In contrast, we use constituent triplets as dependency labels. Our method is based on constraining a shiftreduce parser using the arc-eager strategy. Nivre (2003) and Nivre (2004) establish the basis for arc-eager algorithm and arc-standard parsing algorithms, which are central to most recent transitionbased parsers (Zhang and Clark, 2011b; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). These parsers are very fast, because the number of parsing actions is linear in the length of the sentence, and they obtain state-of-the-art-performance, as shown in Section 4.3. Table 3: State-of-the-art comparison in the test set for phrase structure parsing. Results marked with † use additional information, such as semisupervised models, and are not directly comparable to the others"
K15-1029,P10-1001,0,0.0888556,"Missing"
K15-1029,N07-1051,0,0.0384221,"Missing"
K15-1029,P08-1068,1,0.863908,"Missing"
K15-1029,D10-1004,0,0.11651,"Missing"
K15-1029,D10-1001,0,0.0280099,"l representations to define an arc-factored dependency parsing model based on the Eisner algorithm, that parses in cubic time. Our work can be seen as the transition-based counterpart of that, with a greedy parsing strategy that runs in linear time. Because of the extra complexity of spinal structures, they used three probabilistic non-spinal dependency models to prune the search space of the spinal model. In our work, we show that a single arc-eager model can obtain very competitive results, even though the accuracies of our model are lower than theirs. In terms of parsing spinal structures, Rush et al. (2010) introduced a dual decomposition method that uses constituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to"
K15-1029,N06-1020,0,0.100666,"Missing"
K15-1029,W05-1513,0,0.474993,"stituent and dependency parsing routines to parse a combined spinal structure. In a similar style to our method Hall et al. (2007), Hall and Nivre (2008) and Hall (2008) introduced an approach for parsing Swedish and German, in which MaltParser (Nivre et al., 2007) is used to predict dependency trees, whose dependency labels are enriched with constituency labels. They used tuples that encode dependency labels, constituent labels, head relations and the attachment. The last step is to make the inverse transformation from a dependency graph to a constituent For shift-reduce constituent parsing, Sagae and Lavie (2005; 2006) presented a shift-reduce phrase structure parser. The main difference to ours is that their models do not use lexical dependencies. Zhang and Clark (2011a) presented a shift-reduce parser based on CCG, and as such is lexicalized. Both spinal and CCG representations are very expressive. One difference is that spinal trees can be directly obtained from constituent treebanks with head-child information, while CCG derivations are harder to obtain. More recently, Zhang and Clark (2009) and the subsequent work of Zhu et al. (2013) described a beam-search shift-reduce parsers obtaining very h"
K15-1029,E06-1011,0,0.229238,"Missing"
K15-1029,N06-2033,0,0.0979139,"Missing"
K15-1029,P05-1012,0,0.0689642,"uracy, and yields state of the art performance for both dependency and constituent parsing measures. 1 Introduction There are two main representations of the syntactic structure of sentences, namely constituent and dependency-based structures. In terms of statistical modeling, an advantage of dependency representations is that they are naturally lexicalized, and this allows the statistical model to capture a rich set of lexico-syntactic features. The recent literature has shown that such lexical features greatly favor the accuracy of statistical models for parsing (Collins, 1999; Nivre, 2003; McDonald et al., 2005). Constituent structure, on the other hand, might still provide valuable syntactic information that is not captured by standard dependencies. In this work we investigate transition-based statistical models that produce spinal trees, a representation that combines dependency and constituent structures. Statistical models that use both representations jointly were pioneered by Collins (1999), who used constituent trees annotated with head-child information in order to define lexicalized PCFG models, i.e. extensions of classic 289 Proceedings of the 19th Conference on Computational Language Learn"
K15-1029,D09-1058,1,0.894619,"Missing"
K15-1029,J14-2002,0,0.0473466,"Missing"
K15-1029,I11-1140,0,0.0324855,"Missing"
K15-1029,P15-1032,0,0.031042,"Missing"
K15-1029,W03-3023,0,0.297738,"The assumption is that identical contiguous edges correspond to sibling dependencies that attach to the same node in the spine.2 4.1 2. Merge the left L and right R sequences of edges overlapping them as much as possible, i.e. looking for the shortest spine. We do this in O(nm), where n and m are the lengths of the two sequences. Whenever multiple shortest spines are compatible with the left and right edge sequences, we give preference to the spine that places left edges to the bottom. We use the WSJ portion of the Penn Treebank4 , augmented with head-dependant information using the rules of Yamada and Matsumoto (2003). This results in a total of 974 different constituent triplets, which we use as dependency labels in the spinal arc-eager model. We use predicted part-ofspeech tags5 . 4.2 The result of this process is a spine σi with left and right dependents attached to positions of the spine. Note that this strategy has some limitations: (a) it can not recover non-terminal spinal nodes that do not participate in any triplet; and (b) it flattens spinal structures that involve contiguous identical spinal edges. 3 Results in the Development Set In Table 1 we show the results of our parser for the dependency t"
K15-1029,W09-3825,0,0.344063,", Figure 2 and Section 3.2 describe the constraints of the spinal parser. 290 (a) — Constituent Tree with head-children annotations S NP . VP DT NN This market VBN . VP has VBN VP been ADVP RB ADV very badly VBN damaged (b) — Spinal Tree S ? This NP VP ? market ? has VP VP ADVP ? been ? very ? badly ? damaged ? . Figure 1: (a) A constituent tree for This market has been very badly damaged. For each constituent, the underlined child annotates the head child of the constituent. (b) The corresponding spinal tree. In this paper, we took the already existent implementation of arc-eager from ZPar1 (Zhang and Clark, 2009) which is a beam-search parser implemented in C++ focused on efficiency. ZPar gives competitive accuracies, yielding state-of-the-art results, and very fast parsing speeds for dependency parsing. In the case of ZPar, the parsing process starts with a root node at the top of the stack (see Figure 3) and the buffer contains the words/tokens to be parsed. 3 tees that the arc-eager derivations we produce correspond to spinal trees. Finally we discuss how to map arc-eager derivations to spinal trees. 3.1 We follow Collins (1996) and define a labeling for dependencies based on constituent triplets."
K15-1029,P11-1069,0,0.244694,"strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and"
K15-1029,J11-1005,0,0.18843,"strategy is used (Carreras et al., 2008). In this paper we propose a transition-based parser for spinal parsing, based on the arc-eager strategy by Nivre (2003). Since transition-based parsers run in linear time, our aim is to speed up spinal parsing while taking advantage of the rich representation it provides. Thus, the research question underlying this paper is whether we can accurately learn to take greedy parsing decisions for rich but complex structures such as spinal trees. To control the trade-off, we use beam search for transition-based parsing, which has been shown to be successful (Zhang and Clark, 2011b). The main contributions of this paper are We present a transition-based arc-eager model to parse spinal trees, a dependencybased representation that includes phrasestructure information in the form of constituent spines assigned to tokens. As a main advantage, the arc-eager model can use a rich set of features combining dependency and constituent information, while parsing in linear time. We describe a set of conditions for the arc-eager system to produce valid spinal structures. In experiments using beam search we show that the model obtains a good trade-off between speed and accuracy, and"
K15-1029,P11-2033,0,0.213215,"at the bottom of the spine is well formed. We enforce no further constraints looking at edges in the middle of the spine. This means that left and right arc operations can add spinal edges in a free manner, without explicitly encoding how these edges relate to each other. In other words, we rely on the statistical model to correctly build a spine by adding left and • Top t: S • Left edges L: VP − S • Right edges R: ?−VP, VP − S 293 with the state-of-the-art. We used the ZPar implementation modified to incorporate the constraints for spinal arc-eager parsing. We used the exact same features as Zhang and Nivre (2011), which extract a rich set of features that encode higherorder interactions betwen the current action and elements of the stack. Since our dependency labels are constituent triplets, these features encode a mix of constituent and dependency structure. In this case the shortest spine that is consistent with the edges and the top is ?−V P − S. Our method runs in two steps: 1. Collapse. Traverse each sequence of edges and replace any contiguous subsequence of identical edges by a single occurrence. The assumption is that identical contiguous edges correspond to sibling dependencies that attach to"
K15-1029,P15-1117,0,0.0207295,"Missing"
K15-1029,P13-1043,0,0.248264,"arts-of-speech of the candidate dependency. Incorporating a filtering strategy of this kind would result in a speedup factor constant to all beam sizes. Related Work Collins (1996) defined a statistical model for dependency parsing based on using constituent triplets in the labels, which forms the basis of our arc-eager model. In that work, a chart-based algorithm was used for parsing, while here we use greedy transition-based parsing. 295 Beam-size Sagae and Lavie (2005)* Ratnaparkhi (1999) Sagae and Lavie (2006)* Collins (1999) Charniak (2000) Zhang and Clark (2009)* Petrov and Klein (2007) Zhu et al. (2013)-1* Carreras et al. (2008) Zhu et al. (2013)-2†* Huang (2008) Charniak (2000) Huang et al. (2010) McClosky et al. (2006) this work (beam 64)* LR 86.1 86.3 87.8 88.1 89.5 90.0 90.1 90.2 90.7 91.1 91.2 91.2 91.2 91.2 88.7 LP 86.0 87.5 88.1 88.3 89.9 89.9 90.2 90.7 91.4 91.5 91.8 91.8 91.8 91.8 89.2 F1 86.0 86.9 87.9 88.2 89.5 89.9 90.1 90.4 91.1 91.3 91.5 91.5 91.5 91.5 89.0 structure. Recently Kong et al. (2015) proposed a structured prediction model for mapping dependency trees to constituent trees, using the CKY algorithm. They assume a fixed dependency tree used as a hard constraint. Also re"
K15-1029,A00-2018,0,\N,Missing
K15-1029,J03-4003,0,\N,Missing
K15-1029,C12-1052,0,\N,Missing
K15-1029,D10-1002,0,\N,Missing
K16-1019,J13-1002,1,0.83594,"data structures: a syntactic stack St , a semantic stack Mt —each containing partially built structures—and a buffer of input words Bt . Our algorithm also places partial syntactic and semantic parse structures onto the front of the buffer, so it is also implemented as a stack. Each arc in the output corresponds to a transition (or “action”) chosen based on the current state; every transition modifies the state by updating St , Mt , and Bt to St+1 , Mt+1 , and Bt+1 , respectively. While each state may license several valid actions, each action 2 This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front. 3 Note that in the original arc-eager algorithm (Nivre, 2008), S HIFT and R IGHT-A RC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it). 1 https://github.com/clab/ joint-lstm-parser 188 and Congress has other fied and the algorithm returns to syntactic transitions. This implies that, for each word, its leftside syntactic dependencies are resolved before its left-side semantic dependencies. An example run of the algorithm"
K16-1019,D15-1041,1,0.865442,"le 3: Comparison on the CoNLL 2009 English test set. The first block presents results of other models evaluated for both syntax and semantics on the CoNLL 2009 task. The second block presents our models. The third block presents the best published models, each using its own syntactic preprocessing. corporate morphological features where available; this could potentially improve performance, especially in highly inflective languages like Czech. An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015). Language Catalan Chinese Czech English German Japanese Spanish Average #1 C+’09 81.84 76.38 83.27 87.00 82.44 85.65 81.90 82.64 #2 Z+ ’09a 83.01 76.23 80.87 87.69 81.22 85.28 83.31 82.52 #3 G+ ’09 82.66 76.15 83.21 86.03 79.59 84.91 82.43 82.14 7 Conclusion We presented an incremental, greedy parser for joint syntactic and semantic dependency parsing. Our model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks, without using expert-crafted, expensive features of the full syntactic parse. Joint 82.40 79.27 79.53 87.45 81.05 80.91 83.11 81.96 Acknowled"
K16-1019,W09-1206,0,0.209308,"Missing"
K16-1019,C10-3009,0,0.177351,"Missing"
K16-1019,D13-1152,0,0.0460906,"Missing"
K16-1019,W08-2122,0,0.59599,"al setWe present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008–9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics. 1 Introduction We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2"
K16-1019,W05-0620,0,0.265775,"Missing"
K16-1019,W08-2134,0,0.144763,"updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the development performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB. 5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system. State-of"
K16-1019,J13-4006,0,0.565554,"and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, 187 Proceedings of the 20th SIGN"
K16-1019,W09-1207,0,0.0159494,"performs the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system. State-of-the-art SRL systems (shown in the last block of Table 3) which use advances orthogonal to the contributions in this paper, perform better than our models. Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj¨orkelund et al., 2009; Bj¨orkelund et al., 2010; Roth and W"
K16-1019,P82-1020,0,0.887895,"Missing"
K16-1019,W08-2138,0,0.0752159,"Missing"
K16-1019,W08-2123,0,0.016286,"L 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.9±42 seconds) on the same machine.8 Table 2: Joint parsers: comparison on the CoNLL 2008 test (WSJ+Brown) set. sitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Llu´ıs and M`arquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system’s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013), demonstrating the benefit of our entire-parser-state repre"
K16-1019,D09-1059,0,0.230367,"Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We beli"
K16-1019,P14-1112,0,0.0304826,"of the U.S. Army Research Office or the U.S. Government. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Table 4: Comparison of macro F1 scores on the multilingual CoNLL 2009 test set. 6 a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset. There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M`arquez, 2005), their approach inspired Zhou a"
K16-1019,P15-1033,1,0.571352,"features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design. Our system’s performance does not ma"
K16-1019,D15-1112,0,0.418532,"Missing"
K16-1019,D15-1169,0,0.122887,"Missing"
K16-1019,S15-1033,0,0.183514,"Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for"
K16-1019,P10-1113,0,0.0151724,"oject MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Table 4: Comparison of macro F1 scores on the multilingual CoNLL 2009 test set. 6 a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset. There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M`arquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs. Our approach for dependency-based SRL is not directly compara"
K16-1019,W09-1205,0,0.35133,"oduction We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic"
K16-1019,N15-1142,1,0.750894,"in the English CoNLL 2009 training data. where v and u are vectors corresponding to atomic words or composed parse fragments; l and r are learned vector representations for syntactic and semantic labels respectively. Syntactic and semantic parameters are separated (Zs , es and Zm , em , respectively). Finally, for predicates, we use another recursive function to compose the word representation, v with a learned representation for the dismabiguated sense of the predicate, p: gd (v, p) = tanh(Zd [v; p] + ed ) Pretrained Embeddings Following Dyer et al. (2015), “structured skipgram” embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs. For out-of-vocabulary words, a randomly initialized vector of the same dimension was used. Figure 5: Example of a joint parse tree fragment with vector representations shown at each node. The vectors are obtained by recursive composition of representations of head, dependent, and label vectors. Syntactic dependencies and labels are in green, semantic in blue. gs (v, u, l) = tanh(Zs [v; u; l] + es ) (5) Training Training the classifier req"
K16-1019,J02-3001,0,0.506702,"Joint Syntactic-Semantic Parsing with Stack LSTMs Swabha Swayamdipta♣ Miguel Ballesteros♦ Chris Dyer♠ Noah A. Smith♥ ♣ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA ♦ Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data st"
K16-1019,W08-2124,0,0.0851174,"Missing"
K16-1019,P15-1032,0,0.050578,"Missing"
K16-1019,Q13-1018,0,0.0957373,"Missing"
K16-1019,J93-2004,0,0.0604819,"Missing"
K16-1019,W08-2127,0,0.227225,"nt descent was used and updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the development performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB. 5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expe"
K16-1019,W04-2705,0,0.160463,"Missing"
K16-1019,W09-1209,0,0.713032,"ts of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design. Our system’s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Bj¨orkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (T¨ackstr¨om et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015). Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them. Because our system is very fast— with an end-to-end runtime of 177.6±18 seconds to parse the CoNLL 2009 English test data on a single core—we believe it will be useful in practical setWe present a transition-based parser that jointl"
K16-1019,P15-1109,0,0.238586,"Missing"
K16-1019,J08-4003,0,0.451374,"are expected to reopen soon expect.01 reopen.01 C-A1 A1 AM-TMP A1 Figure 1: Example of a joint parse. Syntactic dependencies are shown by arcs above the sentence and semantic dependencies below; predicates are marked in boldface. C- denotes continuation of argument A1. Correspondences between dependencies might be close (between expected and to) or not (between reopen and all). 2.2 There are separate sets of syntactic and semantic transitions; the former manipulate S and B, the latter M and B. All are formally defined in Table 1. The syntactic transitions are from the “arceager” algorithm of Nivre (2008). They include: • S-S HIFT, which copies3 an item from the front of B and pushes it on S. • S-R EDUCE pops an item from S. • S-R IGHT(`) creates a syntactic dependency. Let u be the element at the top of S and v be the element at the front of B. The new dependency has u as head, v as dependent, and label `. u is popped off S, and the resulting structure, rooted at u, is pushed on S. Finally, v is copied to the top of S. • S-L EFT(`) creates a syntactic dependency with label ` in the reverse direction as S-R IGHT. The top of S, u, is popped. The front of B, v, is replaced by the new structure,"
K16-1019,P09-1040,0,0.412162,"pty. Actions that pop from a stack (S-R EDUCE and M-R EDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-S WAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-S HIFT. Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, following the analysis by Nivre (2009).5 Because SRL graphs allow a node to be a semantic argument of two parents—like all in the example in Figure 1—M-L EFT and M-R IGHT do not remove the dependent from the semantic stack and buffer respectively, unlike their syntactic equivalents, S-L EFT and S-R IGHT. We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues: • M-S WAP swaps the top two items on M , to allow for crossing semantic arcs. • M-P RED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate. The CoNLL 2009"
K16-1019,J05-1004,0,0.264767,"Missing"
K16-1019,D14-1045,0,0.175162,"er Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA ♦ Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used i"
K16-1019,W08-2121,0,0.31456,"Missing"
K16-1019,W05-0636,0,0.0398908,"ures at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, 187 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 187–197, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics has a deterministic effect on the state of the algorithm. Initially, S0 and M0 are empty, and B0 contains the input sentence with the first word at the front of B and a special root symbol at the end.2"
K16-1019,Q15-1003,0,0.171472,"Missing"
K16-1019,J08-2002,0,0.328138,"Missing"
K18-2009,D15-1041,1,0.917237,"elli Miguel Ballesteros IBM Research AI 1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA hwan,tnaseem,ysuklee,vittorio@us.ibm.com miguel.ballesteros@ibm.com Abstract Arabic). For further description of the task, data, framework and evaluation please refer to (Nivre et al., 2018, 2017b; Zeman et al., 2018; Potthast et al., 2014; Nivre and Fang, 2017). In this paper we describe the IBM Research AI submission to the Shared Task on Parsing Universal Dependencies. Our starting point is the Stack-LSTM3 parser (Dyer et al., 2015; Ballesteros et al., 2017) with character-based word representations (Ballesteros et al., 2015), which we extend to handle tokenization, POS tagging and morphological tagging. Additionally, we apply the ideas presented by Ammar et al. (2016) to all low resource languages since they benefited from high-resource languages in the same family. Finally, we also present two different ensemble algorithms that boosted our results (see Section 2.4). Participants are requested to obtain parses from raw texts. This means that, sentence segmentation, tokenization, POS tagging and morphological tagging need to be done besides parsing. Participants can choose to use the baseline pipeline (UDPipe 1.2"
K18-2009,D16-1111,1,0.934079,"ticipants can choose to use the baseline pipeline (UDPipe 1.2 (Straka et al., 2016)) for those steps besides parsing, or create their own implementation. We choose to use our own implementation for most of the languages. However, in a few treebanks with very hard tokenization, like Chinese and Japanese, we rely on UDPipe 1.2 and a run of our base parser (section 2.1), since this produces better results. For the rest of languages, we produce parses from raw text that may be in documents (and thus we need to find the sentence markers within those documents); for some of the treebanks we adapted Ballesteros and Wanner (2016) punctuation prediction system (which is also based in the StackLSTM framework) to predict sentence markers. Given that the text to be segmented into sentences This paper presents the IBM Research AI submission to the CoNLL 2018 Shared Task on Parsing Universal Dependencies. Our system implements a new joint transition-based parser, based on the Stack-LSTM framework and the ArcStandard algorithm, that handles tokenization, part-of-speech tagging, morphological tagging and dependency parsing in one single model. By leveraging a combination of character-based modeling of words and recursive comp"
K18-2009,Q13-1034,0,0.0662082,"Missing"
K18-2009,Q17-1010,0,0.0809011,"proiel, da_ddt, el_gdt, en_ewt, fr_sequoia, fro_srcmf, gl_ctg, hi_hdtb, ko_gsd, ko_kaist, pl_sz, pt_bosque) while in the rest we ran a 10-model ensemble. In multi-lingual setting, we ran 5-model ensemble in most cases except grc_proiel, grc_perseus and sv_lines where 10-models ensembles were used for decoding and no_nynorsklia where a single model was used for decoding. Multilingual word embeddings: The crosslingual parser of Ammar et al. (2016) requires word vectors for each language to be in the same universal space. To this end, we use alignment matrices provided by Smith et al. (2017) for Bojanowski et al. (2017) word embeddings. However, for several low-resource languages, pre-computed alignment matrices were not available. These include Naija, Faroese, Kurmanji, Northern Sami, Uyghur, Buryat and Irish. For these languages, to map monolingual embeddings to multilingual space, we seed the mapping algorithm of Smith et al. (2017) with freely available dictionaries5 combined with shared vocabulary with one of the already mapped languages. 2.4 2.4.1 Model Rescoring: Sentence Level Ensemble Sentence-based Ensemble and MST Ensemble Graph-based ensemble We adapt Sagae and Lavie (2006) ensemble method to our"
K18-2009,D15-1159,0,0.118854,"the resulting stack and buffer states. Figure 2 gives an example of transition sequence in our joint system. Modules: Our joint system extends the transition-based parser in Section 2.1 with extra modules to handle tokenization, UPOS and morphological tagging. The final loss function is the sum of the loss functions from the parser itself and these extra modules. Due to time limitation we did not introduce weights in the sum. Joint tokenization, tagging and dependency parsing Inspired by joint models like the ones by Bohnet et al. (2013), Zhang and Clark (2008), Rasooli and Tetreault (2013); Alberti et al. (2015); Swayamdipta et al. (2016), among others, we extend the transition-based parser presented in 2.1 with extra actions that handle tokenization, UPOS tagging and morphological tagging. 1. Tokenization module. When a string appears at buffer top, for each offset inside the string, predict whether to tokenize here. If tokenization happens at some offset i, apply TOKENIZE (i) and transit to next state accordingly. If no tokenization happens, predict an Actions: Actions RIGHT-ARC(r) , LEFT-ARC(r) and SWAP remain unchanged, where r represents 93 Stackt S, (u, u), (v, v) S, (u, u), (v, v) S, (u, u), ("
K18-2009,Q16-1031,1,0.570948,"bm.com Abstract Arabic). For further description of the task, data, framework and evaluation please refer to (Nivre et al., 2018, 2017b; Zeman et al., 2018; Potthast et al., 2014; Nivre and Fang, 2017). In this paper we describe the IBM Research AI submission to the Shared Task on Parsing Universal Dependencies. Our starting point is the Stack-LSTM3 parser (Dyer et al., 2015; Ballesteros et al., 2017) with character-based word representations (Ballesteros et al., 2015), which we extend to handle tokenization, POS tagging and morphological tagging. Additionally, we apply the ideas presented by Ammar et al. (2016) to all low resource languages since they benefited from high-resource languages in the same family. Finally, we also present two different ensemble algorithms that boosted our results (see Section 2.4). Participants are requested to obtain parses from raw texts. This means that, sentence segmentation, tokenization, POS tagging and morphological tagging need to be done besides parsing. Participants can choose to use the baseline pipeline (UDPipe 1.2 (Straka et al., 2016)) for those steps besides parsing, or create their own implementation. We choose to use our own implementation for most of th"
K18-2009,J17-2002,1,0.901744,"ultilingual Parsing Hui Wan Tahira Naseem Young-Suk Lee Vittorio Castelli Miguel Ballesteros IBM Research AI 1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA hwan,tnaseem,ysuklee,vittorio@us.ibm.com miguel.ballesteros@ibm.com Abstract Arabic). For further description of the task, data, framework and evaluation please refer to (Nivre et al., 2018, 2017b; Zeman et al., 2018; Potthast et al., 2014; Nivre and Fang, 2017). In this paper we describe the IBM Research AI submission to the Shared Task on Parsing Universal Dependencies. Our starting point is the Stack-LSTM3 parser (Dyer et al., 2015; Ballesteros et al., 2017) with character-based word representations (Ballesteros et al., 2015), which we extend to handle tokenization, POS tagging and morphological tagging. Additionally, we apply the ideas presented by Ammar et al. (2016) to all low resource languages since they benefited from high-resource languages in the same family. Finally, we also present two different ensemble algorithms that boosted our results (see Section 2.4). Participants are requested to obtain parses from raw texts. This means that, sentence segmentation, tokenization, POS tagging and morphological tagging need to be done besides parsi"
K18-2009,P09-1040,0,0.154189,"Missing"
K18-2009,E17-2102,0,0.0599012,"uages (these are: Breton, Naija, Faroese and Thai), most low resource languages (these are: Buryat, Kurmanji, Kazakh, Sorbian Upper, Armenian, Irish, Vietnamese, Northern Sami and Uyghur ), and some other languages in which we observed strong improvements on development data when parsing with a cross-lingual model trained in the same language family (these are: Ancient Greek – grc_proiel and grc_perseus, Swedish – sv_pud, Norwegian Language embeddings: Ammar et al. (2016) architecture utilizes language embeddings that capture language nuances and allow generaliza¨ tion. We adapt the method of Ostling and Tiedemann (2017) to pretrain language embeddings. This method is essentially a character-level language model, where a 2-layered LSTM predicts next character at each time step given previous character inputs. A language vector is concatenated to each input as well as the hidden layer before final softmax. The model is trained on a raw corpus containing texts from different languages. Language vectors are shared within the same language. ¨ The model of Ostling and Tiedemann (2017) operates at the level of characters; They restrict their experiments to the languages that are written in Latin, Cyrillic or Greek"
K18-2009,D13-1013,0,0.0317313,"to the stack and buffer and 2) the resulting stack and buffer states. Figure 2 gives an example of transition sequence in our joint system. Modules: Our joint system extends the transition-based parser in Section 2.1 with extra modules to handle tokenization, UPOS and morphological tagging. The final loss function is the sum of the loss functions from the parser itself and these extra modules. Due to time limitation we did not introduce weights in the sum. Joint tokenization, tagging and dependency parsing Inspired by joint models like the ones by Bohnet et al. (2013), Zhang and Clark (2008), Rasooli and Tetreault (2013); Alberti et al. (2015); Swayamdipta et al. (2016), among others, we extend the transition-based parser presented in 2.1 with extra actions that handle tokenization, UPOS tagging and morphological tagging. 1. Tokenization module. When a string appears at buffer top, for each offset inside the string, predict whether to tokenize here. If tokenization happens at some offset i, apply TOKENIZE (i) and transit to next state accordingly. If no tokenization happens, predict an Actions: Actions RIGHT-ARC(r) , LEFT-ARC(r) and SWAP remain unchanged, where r represents 93 Stackt S, (u, u), (v, v) S, (u,"
K18-2009,N06-2033,0,0.0417538,"th et al. (2017) for Bojanowski et al. (2017) word embeddings. However, for several low-resource languages, pre-computed alignment matrices were not available. These include Naija, Faroese, Kurmanji, Northern Sami, Uyghur, Buryat and Irish. For these languages, to map monolingual embeddings to multilingual space, we seed the mapping algorithm of Smith et al. (2017) with freely available dictionaries5 combined with shared vocabulary with one of the already mapped languages. 2.4 2.4.1 Model Rescoring: Sentence Level Ensemble Sentence-based Ensemble and MST Ensemble Graph-based ensemble We adapt Sagae and Lavie (2006) ensemble method to our Stack-LSTM only models (see Section 2.1) to obtain the final parses of Chinese, Japanese, Hebrew, Hungarian, Turkish and Czech. Kuncoro et al. (2016) already tried an ensemble of several Stack-LSTM parser models achieving state-of-the-art in English, German and Chinese, which motivated us to improve the results of our greedy decoding method.6 3 Sentence Segmentation For sentence segmentation we adapted the punctuation prediction system by Ballesteros and Wanner (2016). This model is derived from the StackLSTM parser introduced in Section 2.1 and it uses the same archite"
K18-2009,L16-1680,0,0.0546565,"Missing"
K18-2009,K17-3009,0,0.0147929,"ensure that the parser is still getting most of its signal from the language of interest. In all cross-lingual experiments, sentence level ensemble (see Section 2.4.2) is used. 4.4 Segmentation Sentence segmentation models have hidden layer dimension equal to 100. It relies on the fastText embeddings (Bojanowski et al., 2017), which have dimension equal to 300. The sliding window width is of 100 words, and the overlap between adjacent windows is 30 words. 5 Results Table 4 presents the average F1 LAS results grouped by treebank size and type of our system compared to the baseline UDPipe 1.2 (Straka and Straková, 2017). Table 3 presents the F1 LAS results for all languages compared to the baseline UDPipe 1.2. Our system substantially surpassed the baseline but it is far from the best system of the task in most cases. Some exceptions are the 98 Treebank af_afribooms bg_btb bxr_bdt cs_cac cs_pdt cu_proiel de_gsd en_ewt en_lines es_ancora eu_bdt fi_ftb fi_tdt fr_gsd fr_sequoia ga_idt gl_treegal grc_perseus he_htb hr_set hu_szeged id_gsd it_postwita ja_modern kmr_mg ko_kaist la_perseus lv_lvtb nl_lassysmall no_nynorsk pcm_nsc pl_sz ro_rrt ru_taiga sl_ssj sme_giella sv_lines sv_talbanken tr_imst uk_iu vi_vtb Bas"
K18-2009,K16-1019,1,0.858038,"d buffer states. Figure 2 gives an example of transition sequence in our joint system. Modules: Our joint system extends the transition-based parser in Section 2.1 with extra modules to handle tokenization, UPOS and morphological tagging. The final loss function is the sum of the loss functions from the parser itself and these extra modules. Due to time limitation we did not introduce weights in the sum. Joint tokenization, tagging and dependency parsing Inspired by joint models like the ones by Bohnet et al. (2013), Zhang and Clark (2008), Rasooli and Tetreault (2013); Alberti et al. (2015); Swayamdipta et al. (2016), among others, we extend the transition-based parser presented in 2.1 with extra actions that handle tokenization, UPOS tagging and morphological tagging. 1. Tokenization module. When a string appears at buffer top, for each offset inside the string, predict whether to tokenize here. If tokenization happens at some offset i, apply TOKENIZE (i) and transit to next state accordingly. If no tokenization happens, predict an Actions: Actions RIGHT-ARC(r) , LEFT-ARC(r) and SWAP remain unchanged, where r represents 93 Stackt S, (u, u), (v, v) S, (u, u), (v, v) S, (u, u), (v, v) S S S S, (u, u) Buffe"
K18-2009,K17-3001,0,0.0267677,"Missing"
K18-2009,K18-2001,0,0.0838911,"Missing"
N15-1042,P13-2009,0,0.00851502,"ic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and"
N15-1042,2004.tmi-1.14,0,0.0251845,"y avoided. We present a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filipp"
N15-1042,C14-1133,1,0.883628,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,W14-4416,1,0.917886,"ructures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates the generation of the summaries, and in extractive summarization a better sentence fusion.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier i"
N15-1042,C00-1007,0,0.43582,"eep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a t"
N15-1042,W11-2832,0,0.444251,"SyntSs). While SSyntSs and linearized structures are isomorphic, the difference in the linguistic abstraction of the DSyntSs and SSyntSs leads to divergences that impede the isomorphy between the two and make the first mapping a challenge for statistical generation. Therefore, we focus in this section on 388 the presentation of the DSyntSs and SSyntSs and the mapping between them. 2.1 2.1.1 DSyntSs and SSyntSs Input DSyntSs DSyntSs are very similar to the PropBank (Babko-Malaya, 2005) structures and the structures as used for the deep track of the First Surface Realization Shared Task (SRST, (Belz et al., 2011)) annotations. DSyntSs are connected trees that contain only meaning-bearing lexical items and both predicate-argument (indicated by Roman numbers: I, II, III, IV, . . . ) and lexico-structural, or deepsyntactic, (ATTR(ibutive), APPEND(itive) and COORD(inative)) relations. In other words, they do not contain any punctuation and functional nodes, i.e., governed elements, auxiliaries and determiners. Governed elements such governed prepositions and subordinating conjunctions are dropped because they are imposed by sub-categorization restrictions of the predicative head and void of own meaning— a"
N15-1042,W05-1601,0,0.0234085,"focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach."
N15-1042,C10-1012,1,0.761097,"neration still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et al., 2012) (where the transfer is done at a deep level). In abstractive summarization, it facilitates t"
N15-1042,W11-2835,1,0.945822,"projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine translation (Jones et"
N15-1042,de-marneffe-etal-2006-generating,0,0.0170839,"Missing"
N15-1042,P07-1041,0,0.125277,"sentations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portabi"
N15-1042,D08-1019,0,0.28662,", 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module rai"
N15-1042,W11-2833,0,0.0446441,"Missing"
N15-1042,W13-2131,0,0.0280973,"th rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use it for generation of spoken discourse in Arabic, Polish and Turkish. We believe"
N15-1042,P09-1091,0,0.108813,"their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this pap"
N15-1042,W07-2416,0,0.0989154,"starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summarize related work, before in Section 6 some conclusions are drawn and future work is outlined. 2 The Fu"
N15-1042,C12-1083,0,0.0809232,"ent a fully stochastic generator that is able to cope with projection between non-isomorphic structures. The generator, which starts from PropBank-like structures, consists of a cascade of SVM-classifier based submodules that map in a series of transitions the input structures onto sentences. The generator has been evaluated for English on the Penn-Treebank and for Spanish on the multi-layered AncoraUPF corpus. 1 Introduction Applications such as machine translation that inherently draw upon sentence generation increasingly deal with deep meaning representations; see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008"
N15-1042,P95-1034,0,0.355379,"ans that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov e"
N15-1042,P98-1116,0,0.439226,"to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a fully stochastic generator that is able to cope with the projection between non-isomorphic structures.1 Such a generator can be used as a stand-alone application and also, e.g., in text simplification (Klebanov et al., 2004) or deep machine"
N15-1042,W02-2103,0,0.194135,"see, e.g., (Aue et al., 2004; Jones et al., 2012; Andreas et al., 2013). Deep representations tend to differ in their topology and number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and"
N15-1042,P10-1157,0,0.0362809,"Missing"
N15-1042,C12-2082,1,0.839143,"he experiments Spanish Treebank For the validation of the performance of our generator on Spanish, we use the AnCora-UPF treebank, which contains only about 100,000 tokens, but which has been manually annotated and validated on the SSyntS- and DSyntS-layers, such that its quality is rather high. The deep annotation does not contain any functional prepositions since they have been removed for all predicates of the corpus, and the DSyntS-relations have been edited following annotation guidelines. AnCora-UPF SSyntSs are annotated with fine-grained dependencies organized in a hierarchical scheme (Mille et al., 2012), in a similar fashion as the dependencies of the Stanford Scheme (de Marneffe et al., 2006).7 Thus, it is possible to use the full set of labels or to reduce it according to our needs. We performed preliminary experiments in order to assess which tag granularity is better suited for generation and came up with the 31-label tagset. 7 The main difference with the Stanford scheme is that in AnCora-UPF no distinction is explicitly made between argumental and non-argumental dependencies. 2.3.2 English Treebank For the validation of the generator on English, we use the dependency Penn TreeBank (abo"
N15-1042,W13-3724,1,0.879189,"o, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline the fundamentals of sentence generation as we view it in our work, focusing in particular on the most challenging part of it: the transition between the non-isomorphic predicateargument lexico-structural structures and surfacesyntactic structures. Section 3 outlines the setup of our system. Section 4 discusses the experiments we carried out and the results we obtained. In Section 5, we briefly summari"
N15-1042,W11-2836,0,0.029485,"ybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learning approach. This generator has been successfully tested on an English and a Spanish corpus, as a stand-alone DSyntS–SSyntS generator and as a part of the generation pipeline. We are currently about to apply it to other languages—including Chinese, French and German. Furthermore, resources are compiled to use i"
N15-1042,C04-1097,0,0.0373783,"state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between meaning representations and syntactic structures in terms of a principled machine learni"
N15-1042,P04-1011,0,0.0319245,"on.2 1 The data-driven sentence generator is available for public downloading at https://github.com/ talnsoftware/deepgenerator/wiki. 2 For all of these applications, the deep representation can be obtained by a deep parser, such as, e.g., (Ballesteros et al., 2014a). 387 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 387–397, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The generator, which starts from elementary predicate-argument lexico-structural structures as used in sentence planning by Stent et al. (2004), consists of a cascade of Support Vector Machines (SVM)-classifier based submodules that map the input structures onto sentences in a series of transitions. Following the idea presented in (Ballesteros et al., 2014b), a separate SVM-classifier is defined for the mapping of each linguistic category. The generator has been tested on Spanish with the multi-layered Ancora-UPF corpus (Mille et al., 2013) and on English with an extended version of the dependency Penn TreeBank (Johansson and Nugues, 2007). The remainder of the paper is structured as follows. In the next section, we briefly outline t"
N15-1042,E09-1097,0,0.0213842,"nd number of nodes from the corresponding surface structures since they do not contain, e.g., any functional nodes, while syntactic structures or chains of tokens in linearized trees do. This means that sentence generation needs to be able to cope with the projection between non-isomorphic structures. However, most of the recent work in datadriven sentence generation still avoids this challenge. Some systems focus on syntactic generation (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008) or linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 2009; Guo et al., 2011a), and avoid thus the need to cope with this projection all together; some use a rule-based module to handle the projection between non-isomorphic structures (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Bohnet et al., 2011); and some adapt the meaning structures to be isomorphic with syntactic structures (Bohnet et al., 2010). However, it is obvious that a “syntacticization” of meaning structures can be only a temporary workaround and that a rule-based module raises the usual questions of coverage, maintenance and portability. In this paper, we present a f"
N15-1042,N07-1022,0,0.00899511,"lassifiers for data-driven generators. As already mentioned in Section 1, most of the state-of-the-art work focuses on syntactic generation; see, among others (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008), or only on linearization and inflection (Filippova and Strube, 2007; He et al., 2009; Wan et al., 395 2009; Guo et al., 2011a). A number of proposals are hybrid in that they combine statistical machine learning-based generation with rule-based generation. Thus, some combine machine learning with pre-generated elements, as, e.g., (Marciniak and Strube, 2004; Wong and Mooney, 2007; Mairesse et al., 2010), or with handcrafted rules, as, e.g., (Ringger et al., 2004; Belz, 2005). Others derive automatically grammars for rule-based generation modules from annotated data, which can be used for surface generation, as, e.g., (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998; Oh and Rudnicky, 2002; Zhong and Stent, 2005; Bohnet et al., 2011; Rajkumar et al., 2011) or for generation from ontology triples, as, e.g., (Gyawali and Gardent, 2013). 6 Conclusions We presented a statistical deep sentence generator that successfully handles the non-isomorphism between mean"
N15-1042,C08-1038,0,\N,Missing
N15-1042,C98-1112,0,\N,Missing
N15-1042,W09-1201,0,\N,Missing
N15-3012,I13-2007,1,0.859674,"d simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it"
N15-3012,C14-1133,1,0.913287,"structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As s"
N15-3012,N15-1042,1,0.837746,"million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joi"
N15-3012,D12-1133,1,0.928036,"are available as yet to control the output of deep-syntactic parsers. In this paper, we propose such a visualization interface. The interface can be used for both a pipeline consisting of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguis"
N15-3012,bohnet-wanner-2010-open,1,0.822011,"3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of"
N15-3012,E14-2003,0,0.0630282,"Missing"
N15-3012,S10-1059,0,0.0176587,"best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an operational interface for the visualization of the output of a deep-syntactic parser and of surface-syntactic structures that serve it as input. The interface is flexible in that it allows for the display of any additional structural information provided by an extended parsing pipeline. For instance, if the obtained deep-syntactic structure is projected onto a frame-like structure (Chen et al., 2010) with semantic roles as arc labels, this frame structure can be displayed as well. We are currently working on such an extension. Furthermore, we aim to expand our visualization interface to facilitate active exploration of linguistic structures with Brat and thus add to the static display of structures the dimension of Visual Analytics (Keim et al., 2008). Acknowledgments This work has been partially funded by the European Union’s Seventh Framework and Horizon 2020 Research and Innovation Programmes under the Grant Agreement numbers FP7-ICT-610411, FP7-SME606163, and H2020-RIA-645012. Referen"
N15-3012,P08-5006,0,0.0225905,"plicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that"
N15-3012,N10-1011,0,0.0251457,"ructure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntacti"
N15-3012,D08-1019,0,0.0297568,"ions between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-syntactic parsing has been introduced as a new parsing paradigm; see, e.g., (Ballesteros et al., 2014).1 No visualization interfaces are available as yet to control the output"
N15-3012,W07-2416,0,0.13618,"ing of a syntactic parser and a deep parser and a joint syntactic+deep parser. In the first configuration, it facilitates the visualization of the output of the syntactic parser and of the output of the deep parser. In the second configuration, it visualizes directly the output of the joint parser. In what follows, we present its use for the first configuration applied to English. As surfacesyntactic parser, we use Bohnet and Nivre (2012)’s joint tagger+lemmatizer+parser. As deep parser, we use Ballesteros et al. (2014)’s implementation. Both have been trained on the dependency Penn Treebank (Johansson and Nugues, 2007), which has been extended by the DSyntS-annotation. The interface can be inspected online; cf. http://dparse. 1 The source code of Ballesteros et al.’s deep parser and a short manual on how to use it can be downloaded from https://github.com/talnsoftware/ deepsyntacticparsing/wiki. 56 Proceedings of NAACL-HLT 2015, pages 56–60, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics adv adv quant quant subj analyt perf (a) almost 1.2 million jobs have analyt pass prepos det prepos det been created by the state in that time ATTR ATTR agent ATTR ATTR II I II ATT"
N15-3012,C12-1083,0,0.016108,"ucture of a sentence. More precisely, a deep-syntactic structure (DSyntS) is a dependency tree that captures the argumentative, attributive and coordinative relations between full (i.e., meaningful) words of a sentence. For illustration, Figure 1 shows a surface-syntactic structure (above) and deep-syntactic structure (below) for the sentence: almost 1.2 million jobs have been created by the state in that time. DSyntSs have a great potential for such downstream applications as deep machine translation, summarization or information extraction. In deep machine translation as discussed, e.g., by Jones et al. (2012), DSyntSs simplify the alignment between the source and target language structures considerably. In extractive summarization, sentence fusion (Filippova and Strube, 2008) becomes much more straightforward at the level of DSyntSs. A stochastic sentence realizer that takes as input DSyntSs can then be used to generate surface sentences (Ballesteros et al., 2015). In information extraction (Attardi and Simi, 2014) the procedures for the distillation of the information to fill the slots of the corresponding patterns are also simpler at the DSyntS level. However, it is only recently that deep-synta"
N15-3012,nilsson-nivre-2008-malteval,0,0.0344817,"structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-syntactic structures with Brat 4 5 Related Work Visualization interfaces normally offer a universal and simple way to access the output of NLP tools, among them parsers. This leads to better comprehension of their outputs and a better usability for downstream applications. Therefore, it is not surprising that visualization interfaces have been a relevant topic during the last years in the NLP community; see, e.g., (Collins et al., 2008; Collins et al., 2009; Feng and Lapata, 2010). In the parsing area, tools such as MaltEval (Nilsson and Nivre, 2008), the Mate Tools (Bohnet and Wanner, 2010), XLDD (Culy et al., 2011), TreeExplorer (Thiele et al., 2013), ViZPar (Ortiz et al., 2014), MaltDiver (Ballesteros and Carlini, 2013), or XLike Services (Carreras et al., 2014) have been proposed for the visualization of parse trees and their subsequent evaluation. The interface described in this paper serves a similar purpose. To the best of our knowledge, it is the first interface that uses the flexible off-theshelf tool Brat and that serves for the visualization of deep-syntactic structures. 59 Conclusions and Future Work We have presented an opera"
N15-3012,J05-1004,0,0.0170667,", 2012). Brat takes an annotation file, which is produced by transforming the CoNLL files that the parsers output into Brat’s native format, and generates the graphical interface for the dependency trees. Figure 2 shows three sample surface syntactic structures in Brat. In Figure 3, their equivalent deepsyntactic structures are displayed. As already Figure 1, the figures illustrate the difference of both types of structures with respect to the abstraction of linguistic phenomena. The DSyntSs are clearly much closer to semantics. As a matter of fact, they are equivalent to PropBank structures (Palmer et al., 2005). However, this does not mean that they must per se be “simpler” than their corresponding surface-syntactic structures—compare, for instance, the structures (3a) and (3b) in Figures 2 and 3, where both SSyntS and DSyntS contain the same number of nodes, i.e., are isomorphic. The structures (2a) and (2b) illustrate the capacity of the deep parser to correctly identify the arguments of a lexical item without that explicit hints are available in the surface structure. (1a) (2a) (3a) Figure 2: Visualization of surface syntactic structures with Brat (1b) (2b) (3b) Figure 3: Visualization of deep-sy"
N15-3012,E12-2021,0,0.106271,"Missing"
N15-3012,P13-4010,0,0.0552154,"Missing"
N16-1024,P99-1070,0,0.0566848,"imilar constraints have been proposed to deal with the analogous problem in bottom-up shift-reduce parsers (Sagae and Lavie, 2005). 201 Constraints on generator transitions. The generation algorithm also requires slightly modified constraints. These are: • The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n ≥ 1. To designate the set of valid generator transitions, we write AG (T, S, n). This transition set generates trees using nearly the same structure building actions and stack configurations as the “top-down PDA” construction proposed by Abney et al. (1999), albeit without the restriction that the trees be in Chomsky normal form. 3.3 Transition Sequences from Trees Any parse tree can be converted to a sequence of transitions via a depth-first, left-to-right traversal of a parse tree. Since there is a unique depth-first, leftro-right traversal of a tree, there is exactly one transition sequence of each tree. For a tree y and a sequence of symbols x, we write a(x, y) to indicate the corresponding sequence of generation transitions, and b(x, y) to indicate the parser transitions. 3.4 Runtime Analysis A detailed analysis of the algorithmic propertie"
N16-1024,N15-1083,0,0.0397053,"Missing"
N16-1024,E03-1005,0,0.0656796,"Missing"
N16-1024,J92-4003,0,0.647152,"the size of AG (S, T, n), word generation is broken into two parts. First, the decision to generate is made (by predicting GEN as an action), and then choosing the word, conditional on the current parser state. To further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu pand Blunsom, 2015; Goodman, 2001). By using |Σ| classes for a vocabulary p of size |Σ|, this prediction step runs in time O( |Σ|) rather than the O(|Σ|) of the full-vocabulary softmax. To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992). 4.3 Discriminative Parsing Model 5 Inference via Importance Sampling Our generative model p(x, y) defines a joint distribution on trees (y) and sequences of words (x). To evaluate this as a language model, it is necessary to compute the marginal probability p(x) = P 0 y 0 ∈Y(x) p(x, y ). And, to evaluate the model as a parser, we need to be able to find the MAP parse tree, i.e., the tree y ∈ Y(x) that maximizes p(x, y). However, because of the unbounded dependencies across the sequence of parsing actions in our model, exactly solving either of these inference problems is intractable. To obta"
N16-1024,W15-2108,0,0.16968,"c process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, whe"
N16-1024,P15-2142,0,0.211515,"c process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, whe"
N16-1024,A00-2018,0,0.925699,"le 4: Language model perplexity results. Parsing results on PTB §23 (D=discriminative, G=generative, S=semisupervised). Model Henderson (2004) Socher et al. (2013a) Zhu et al. (2013) Vinyals et al. (2015) – WSJ only Petrov and Klein (2007) Bod (2003) Shindo et al. (2012) – single Shindo et al. (2012) – ensemble Zhu et al. (2013) McClosky et al. (2006) Vinyals et al. (2015) – single Vinyals et al. (2015) – ensemble Discriminative, q(y |x) Generative, pˆ(y |x) type D D D D G G G G S S S S D G Table 3: Parsing results on CTB 5.1. Model Zhu et al. (2013) Wang et al. (2015) Huang and Harper (2009) Charniak (2000) Bikel (2004) Petrov and Klein (2007) Zhu et al. (2013) Wang and Xue (2014) Wang et al. (2015) Discriminative, q(y |x) Generative, pˆ(y |x) type D D D G G G S S S D G F1 89.4 90.4 90.4 90.5 90.1 90.7 91.1 92.4 91.3 92.1 92.5 92.8 89.8 92.4 F1 82.6 83.2 84.2 80.8 80.6 83.3 85.6 86.3 86.6 80.7 82.7 Discussion It is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model. This is the result of (i) relaxing conventional independence assumptions (e.g., context-freeness) and (ii) inferring continuous representations of symbols alongsi"
N16-1024,D10-1066,0,0.578391,"dling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Furthermore, standard parsing evaluation scores do not depend on preterminal prediction accuracy. operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the “linearized” parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010). A further connection is to LL(∗ ) parsing which uses an unbounded lookahead (compactly represented by a DFA) to distinguish between parse alternatives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA. unprocessed words, rather there is an output buffer (T ), and (ii) instead of a SHIFT operation there are GEN (x) operations which generate terminal symbol x ∈ Σ and add it to the top of the stack and the output buffer. At each timestep an action is stochastically selected according to a conditional distribution that depen"
N16-1024,P15-1033,1,0.788281,"contents (Cho et al., 2014). Since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet, it is straightforward to apply a standard RNN encoding architecture. The stack (S) is more complicated for two reasons. First, the elements of the stack are more complicated objects than symbols from a discrete alphabet: open nonterminals, terminals, and full trees, are all present on the stack. Second, it is manipulated using both push and pop operations. To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015). 4.1 Syntactic Composition Function When a REDUCE operation is executed, the parser pops a sequence of completed subtrees and/or tokens (together with their vector embeddings) from the stack and makes them children of the most recent open nonterminal on the stack, “completing” the constituent. To compute an embedding of this new subtree, we use a composition function based on bidirectional LSTMs, which is illustrated in Fig. 6. The first vector read by the LSTM in both the forward and reverse directions is an embedding of the label on the constituent being constructed (in the figure, NP). Thi"
N16-1024,P81-1022,0,0.662754,"lated to the operations used in Earley’s algorithm which likewise introduces nonterminals symbols with its PREDICT 2 Preterminal symbols are, from the parsing algorithm’s point of view, just another kind of nonterminal symbol that requires no special handling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Furthermore, standard parsing evaluation scores do not depend on preterminal prediction accuracy. operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the “linearized” parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010). A further connection is to LL(∗ ) parsing which uses an unbounded lookahead (compactly represented by a DFA) to distinguish between parse alternatives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA. unprocessed words, rather there is an output buffer (T ), and (ii) i"
N16-1024,P06-1121,0,0.0200588,"ther than the importance sampling method based on a discriminative parser, §5) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014). A third consideration regarding how RNNGs, human sentence processing takes place in a left-toright, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is 207 left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010). Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization. 10 Conclusion We intr"
N16-1024,J14-2005,1,0.778993,"nce sampling method based on a discriminative parser, §5) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014). A third consideration regarding how RNNGs, human sentence processing takes place in a left-toright, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is 207 left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010). Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization. 10 Conclusion We introduced recurrent neural n"
N16-1024,N03-1014,0,0.528102,"lationships among words and phrases. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transitio"
N16-1024,P04-1013,0,0.146212,"latively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down i"
N16-1024,D09-1087,0,0.056023,"Missing"
N16-1024,J91-3004,0,0.723725,"ta. 8 symbol), inverted, and exponentiated to yield the perplexity. Results are summarized in Table 4. 7 Model IKN 5-gram LSTM LM RNNG Related Work Our language model combines work from two modeling traditions: (i) recurrent neural network language models and (ii) syntactic language modeling. Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990). Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neural-network–based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize generative parsing based on a left-corner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a,b). Modeling generation top-down as"
N16-1024,P01-1042,0,0.112989,"generative model. We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model (§5). Experiments show that RNNGs are effective for both language modeling and parsing (§6). Our generative model obtains (i) the best-known parsing results using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models. Surprisingly—although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)— parsing with the generative model obtains significantly better results than parsing with the discriminative model. 2 RNN Grammars Formally, an RNNG is a triple (N, Σ, Θ) consisting of a finite set of nonterminal symbols (N ), a finite set of terminal symbols (Σ) such that N ∩ Σ = ∅, and a collection of neural network parameters Θ. It does not explicitly define rules since these are implicitly characterized by Θ. The algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition-based algorithm, which is outlined in the next section. In"
N16-1024,Q16-1032,0,0.0218427,"top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take. The neural networks we use to model sentences are structured according to the syntax of the sentence being generated. Syntactically structured neural architectures have been explored in a number of applications, including discriminative parsing (Socher et al., 2013a; Kiperwasser and Goldberg, 2016), sentiment analysis (Tai et al., 2015; Socher et al., 2013b), and sentence representation (Socher et al., 2011; Bowman et al., 2006). However, these models have been, without exception, discriminative; this is the first work to use syntactically structured neural models to generate language. Earlier work has demonstrated that sequential RNNs have the capacity to recognize contextfree (and beyond) languages (Sun et al., 1998; Siegelmann and Sontag, 1995). In contrast, our work may be understood as a way of incorporating a context-free inductive bias into the model structure. 9 Outlook RNNGs ca"
N16-1024,N06-1020,0,0.0266107,"Missing"
N16-1024,D13-1032,0,0.02466,"Missing"
N16-1024,N07-1051,0,0.0217429,"Missing"
N16-1024,J01-2004,0,0.40637,"for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a line"
N16-1024,W05-1513,0,0.163106,"forward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down information, but a compl"
N16-1024,P12-1046,0,0.149805,"Missing"
N16-1024,D13-1170,0,0.0582441,"what sort of head it should be looking for as it processes the child node embeddings. The final state of the forward and reverse LSTMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.4 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an 4 We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task. RE GEDUC NTN E ( NT NP) (VP ) … St z }| (S p(at ) { NP z ut (VP a&lt;t cat Tt }| hungry { The The hungry cat Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St ), output buffer (Tt ) and history of actions (a&lt;t ). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the stack are not shown. This architecture corresponds to the generator"
N16-1024,P15-1150,0,0.365082,"TMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.4 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an 4 We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task. RE GEDUC NTN E ( NT NP) (VP ) … St z }| (S p(at ) { NP z ut (VP a&lt;t cat Tt }| hungry { The The hungry cat Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St ), output buffer (Tt ) and history of actions (a&lt;t ). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the stack are not shown. This architecture corresponds to the generator state at line 7 of Figure 4. 4.4 x x A discriminative parsing model can be obtained by replacing the embedding of Tt at each tim"
N16-1024,W07-2218,0,0.0387922,"te via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminati"
N16-1024,N03-1033,0,0.190016,"en, 2011). Our importance sampling algorithm uses a conditional proposal distribution q(y |x) with the following properties: (i) p(x, y) > 0 =⇒ q(y | x) > 0; (ii) samples y ∼ q(y |x) can be obtained efficiently; and (iii) the conditional probabilities q(y |x) of these samples are known. While many such distributions are available, the discrim5 Training The parameters in the model are learned to maximize the likelihood of a corpus of trees. 204 For the discriminative parser, the POS tags are processed similarly as in (Dyer et al., 2015); they are predicted for English with the Stanford Tagger (Toutanova et al., 2003) and Chinese with Marmot (Mueller et al., 2013). inatively trained variant of our parser (§4.4) fulfills these requirements: sequences of actions can be sampled using a simple ancestral sampling approach, and, since parse trees and action sequences exist in a one-to-one relationship, the product of the action probabilities is the conditional probability of the parse tree under q. We therefore use our discriminative parser as our proposal distribution. Importance sampling uses importance weights, which we define as w(x, y) = p(x, y)/q(y |x), to compute this estimate. Under this definition, we c"
N16-1024,P15-1110,0,0.219171,"Missing"
N16-1024,P14-1069,0,0.0783975,"Missing"
N16-1024,D15-1199,0,0.0152728,"Missing"
N16-1024,J11-1005,0,0.213134,"use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down information, but a complete path from the root"
N16-1024,P13-1043,0,0.278571,"her assumptions. Assuming our fixed constraint on maximum depth, it is linear. 3.5 Comparison to Other Models Our generation algorithm algorithm differs from previous stack-based parsing/generation algorithms in two ways. First, it constructs rooted tree structures top down (rather than bottom up), and second, the transition operators are capable of directly generating arbitrary tree structures rather than, e.g., assuming binarized trees, as is the case in much prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013). 4 Generative Model RNNGs use the generator transition set just presented to define a joint distribution on syntax trees (y) and words (x). This distribution is defined as a sequence model over generator transitions that is parameterized using a continuous space embedding of the algorithm state at each time step (ut ); i.e., |a(x,y)| p(x, y) = Y p(at |a&lt;t ) t=1 |a(x,y)| = Y t=1 exp r> at ut + bat , > a0 ∈AG (Tt ,St ,nt ) exp ra0 ut + ba0 P and where action-specific embeddings ra and bias vector b are parameters in Θ. The representation of the algorithm state at time t, ut , is computed by com"
N16-1030,D16-1211,1,0.107089,"Missing"
N16-1030,W10-1703,0,0.0202334,"layer just before the input to the bidirectional LSTM in Figure 1. We observe a significant improvement in our model’s performance after using dropout (see table 5). 5 Experiments This section presents the methods we use to train our models, the results we obtained on various tasks and the impact of our networks’ configuration on model performance. 5.1 Training For both models presented, we train our networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with 2 (Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009) a learning rate of 0.01 and a gradient clipping of 5.0. Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014). Although we observe faster convergence using these methods, none of them perform as well as SGD with gradient clipping. Our LSTM-CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100. Tuning this dimension did not significantly impact model performance. We set the dropout rate to 0.5. Using higher rates negatively impacted our results, while s"
N16-1030,W02-2004,0,0.0600038,"Missing"
N16-1030,W99-0612,0,0.105164,"ddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers. Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information. Finally, there is currently a lot of interest in models for NER that use letter-based representations. Gillick et"
N16-1030,W02-2007,0,0.09417,"Missing"
N16-1030,P15-1033,1,0.569042,"heme. 3 Transition-Based Chunking Model As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation). This model relies on a stack data structure to incrementally construct chunks of the input. To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a “stack pointer.” While sequential LSTMs model sequences from left to right, stack LSTMs permit embedding of a stack of objects that are both added to (using a push operation) and removed from (using a pop operation). This allows the Stack-LSTM to work like a stack that maintains a “summary embedding” of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity. 263 Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the StackLSTM model since in this paper we merely use the same architec"
N16-1030,W11-2202,0,0.0426334,"(2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers. Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information. Finally, there is currently a lot of interest in models for NER that use letter-based representations. Gillick et al. (2015) model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character-based representations into their encoder model. Chiu and Nichols (2015) employ"
N16-1030,W03-0425,0,0.259283,"ng Chiu and Nichols (2015) Chiu and Nichols (2015)* LSTM-CRF (no char) LSTM-CRF S-LSTM (no char) S-LSTM F1 89.59 83.78 90.90 90.10 90.05 90.90 89.9 91.2 90.69 90.77 90.20 90.94 87.96 90.33 ing character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. Model LSTM LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF S-LSTM S-LSTM S-LSTM S-LSTM S-LSTM Table 1: English NER results (CoNLL-2003 test set). * indicates models trained with the use of external labeled data Model Florian et al. (2003)* Ando and Zhang (2005a) Qi et al. (2009) Gillick et al. (2015) Gillick et al. (2015)* LSTM-CRF – no char LSTM-CRF S-LSTM – no char S-LSTM F1 72.41 75.27 75.72 72.08 76.22 75.06 78.76 65.87 75.66 ent configurations. “pretrain” refers to models that include pretrained word embeddings, “char” refers to models that include character-based modeling of words, “dropout” refers to models that include dropout rate. cates models trained with the use of external labeled data F1 77.05 78.6 78.08 82.84 73.14 81.74 69.90 79.88 Table 3: Dutch NER (CoNLL-2002 test set). * indicates models trained with the us"
N16-1030,D11-1072,0,0.0309592,"Missing"
N16-1030,P09-1116,0,0.033325,"r-based representations to achieve competitive performance; we hypothesize that the LSTM-CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs; however, the Stack-LSTM model consumes the words one by one and it just relies on the word representations when it chunks words. 5.4 Network architectures Our models had several components that we could tweak to understand their impact on the overall performance. We explored the impact that the CRF, the character-level representations, pretraining of our Model Collobert et al. (2011)* Lin and Wu (2009) Lin and Wu (2009)* Huang et al. (2015)* Passos et al. (2014) Passos et al. (2014)* Luo et al. (2015)* + gaz Luo et al. (2015)* + gaz + linking Chiu and Nichols (2015) Chiu and Nichols (2015)* LSTM-CRF (no char) LSTM-CRF S-LSTM (no char) S-LSTM F1 89.59 83.78 90.90 90.10 90.05 90.90 89.9 91.2 90.69 90.77 90.20 90.94 87.96 90.33 ing character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. Model LSTM LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF S-LSTM S-LSTM S-L"
N16-1030,D15-1161,1,0.175082,"here, (i) a bidirectional LSTM with a sequential conditional random layer above it (LSTM-CRF; §2), and (ii) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM; §3). Second, token-level evidence for “being a name” includes both orthographic evidence (what does the word being tagged as a name look like?) and distributional evidence (where does the word being tagged tend to occur in a corpus?). To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b). Our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence (§4). Experiments in English, Dutch, German, and Spanish show that we are able to obtain state260 Proceedings of NAACL-HLT 2016, pages 260–270, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics of-the-art NER performance with the LSTM-CRF model in Dutch, German, and Spanish, and v"
N16-1030,D15-1176,1,0.106945,"Missing"
N16-1030,D15-1104,0,0.870265,"ude them in our models. We did not perform any dataset preprocessing, apart from replacing every digit with a zero in the English NER dataset. 3 English (D=0.2), German, Spanish and Dutch (D=0.3) 266 5.3 Results Table 1 presents our comparisons with other models for named entity recognition in English. To make the comparison between our model and others fair, we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases. Our models do not use gazetteers or any external labeled resources. The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91.2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011). Their model uses a lot of hand-engineered features including spelling features, WordNet clusters, Brown clusters, POS tags, chunks tags, as well as stemming and external knowledge bases like Freebase and Wikipedia. Our LSTM-CRF model outperforms all other systems, including the ones using external labeled data like gazetteers. Our StackLSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015). Tables 2"
N16-1030,W04-0308,0,0.0378214,"). This allows the Stack-LSTM to work like a stack that maintains a “summary embedding” of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity. 263 Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the StackLSTM model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following Section. 3.1 Chunking Algorithm We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed. The transition inventory contains the following transitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack while the REDUCE(y) transition pops all items from the top of the stack creating a “chunk,” labels this with label y, and pushes a representation of this chunk onto the output stack. The a"
N16-1030,W14-1609,0,0.0526639,"; we hypothesize that the LSTM-CRF model requires less orthographic information since it gets more contextual information out of the bidirectional LSTMs; however, the Stack-LSTM model consumes the words one by one and it just relies on the word representations when it chunks words. 5.4 Network architectures Our models had several components that we could tweak to understand their impact on the overall performance. We explored the impact that the CRF, the character-level representations, pretraining of our Model Collobert et al. (2011)* Lin and Wu (2009) Lin and Wu (2009)* Huang et al. (2015)* Passos et al. (2014) Passos et al. (2014)* Luo et al. (2015)* + gaz Luo et al. (2015)* + gaz + linking Chiu and Nichols (2015) Chiu and Nichols (2015)* LSTM-CRF (no char) LSTM-CRF S-LSTM (no char) S-LSTM F1 89.59 83.78 90.90 90.10 90.05 90.90 89.9 91.2 90.69 90.77 90.20 90.94 87.96 90.33 ing character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. Model LSTM LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF LSTM-CRF S-LSTM S-LSTM S-LSTM S-LSTM S-LSTM Table 1: English NER results (CoNLL-2003 te"
N16-1030,W09-1119,0,0.489621,"a named entity, I-label if it is inside a named entity but not the first token within the named entity, or O otherwise. However, we decided to use the IOBES tagging scheme, a variant of IOB commonly used for named entity recognition, which encodes information about singleton entities (S) and explicitly marks the end of named entities (E). Using this scheme, tagging a word as I-label with high-confidence narrows down the choices for the subsequent word to I-label or E-label, however, the IOB scheme is only capable of determining that the subsequent word cannot be the interior of another label. Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance marginally. However, we did not observe a significant improvement over the IOB tagging scheme. 3 Transition-Based Chunking Model As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation). This m"
N16-1030,W15-3904,0,0.31074,"Missing"
N16-1030,W02-2024,0,0.52193,"g is of dimension 20. We experimented with different dropout rates and reported the scores using the best dropout rate for each language.3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016). 5.2 Data Sets We test our model on different datasets for named entity recognition. To demonstrate our model’s ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain independent named entity labels for English, Spanish, German and Dutch. All datasets contain four different types of named entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categories. Although POS tags were made available for all datasets, we did not include them in our models. We did not perform any dataset preprocessing, apart from replacing every digit with a zero in the English NER dataset. 3 English (D=0.2), German, Spanish and Dutch (D=0.3) 266 5.3 Results Table 1 pr"
N16-1030,P10-1040,0,0.0470759,"Missing"
N16-1030,J11-1005,0,0.0353525,"ropout rate to 0.5. Using higher rates negatively impacted our results, while smaller rates led to longer training time. The stack-LSTM model uses two layers each of dimension 100 for each stack. The embeddings of the actions used in the composition functions have 16 dimensions each, and the output embedding is of dimension 20. We experimented with different dropout rates and reported the scores using the best dropout rate for each language.3 It is a greedy model that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016). 5.2 Data Sets We test our model on different datasets for named entity recognition. To demonstrate our model’s ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain independent named entity labels for English, Spanish, German and Dutch. All datasets contain four different types of named entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categorie"
N16-1030,P15-1109,0,0.122812,"he output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers. Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisens"
N18-2107,D15-1041,1,0.848507,"101), initialized with pretrained parameters learned on ImageNet (Deng et al., 2009). We use this model as a starting point to later finetune it on our emoji classification task. Learning rate was set to 0.0001 and we early stopped the training when there was not improving in the validation set. 3.3 B-LSTM Baseline Barbieri et al. (2017) propose a recurrent neural network approach for the emoji prediction task. We use this model as baseline, to verify whether FastText achieves comparable performance. They used a Bidirectional LSTM with character representation of the words (Ling et al., 2015; Ballesteros et al., 2015) to handle orthographic variants (or even spelling errors) of the same word that occur in social media (e.g. cooooool vs cool). 4 Experiments and Evaluation In order to study the relation between Instagram posts and emojis, we performed two different experiments. In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017). Our second experiment (Section 4.3) evaluates the visual (ResNet) and textual (FastText) models on the emoji prediction task. Moreover, we evaluate a multimodal combination of both m"
N18-2107,E17-2017,1,0.94921,"is that incorporating the two synergistic modalities, in a combined model, improves accuracy in an emoji prediction task. This result demonstrates that these two modalities (text and images) encode different information on the use of emojis and therefore can complement each other. 1 Introduction In the past few years the use of emojis in social media has increased exponentially, changing the way we communicate. The combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images. Recent work (Barbieri et al., 2017) has shown that textual information can be used to predict emojis associated to text. In this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models. We explore the use of emojis in the social media platform Instagram. We put forward a multimodal approach to predict the emojis associated to an In2 Dataset and Task Dataset: We gathered Instagram posts published between July 2016 and October 2016, and geolocalized in the U"
N18-2107,W17-5216,0,0.261348,"orrectly predicts both of them: the blue color in the picture associated to (2) helps to change the color of the heart, and the sunny/bright picture of the garden in (1) helps to correctly predict . 5 Related Work Modeling the semantics of emojis, and their applications, is a relatively novel research problem with direct applications in any social media task. Since emojis do not have a clear grammar, it is not clear their role in text messages. Emojis are considered function words or even affective markers (Na’aman et al., 2017), that can potentially affect the overall semantics of a message (Donato and Paggio, 2017). Emojis can encode different meanings, and they can be interpreted differently. Emoji interpretation has been explored user-wise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), and gender-wise (Chen et al., 2017) and time-wise (Barbieri et al., 2018). Emoji sematics and usage have been studied with distributional semantics, with models trained on Twitter data (Barbieri et al., 2016c), Twitter data together with the official unicode description (Eisner et al., 2016), or using text from a popular keyboard app Ai et al"
N18-2107,S18-2011,1,0.823962,"ncy Maps In order to show the parts of the image most relevant for each class we analyze the global average pooling (Lin et al., 2013) on the convolutional 682 context, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. In order to further study emoji semantics, two datasets with pairwise emoji similarity, with human annotations, have been proposed: EmoTwi50 (Barbieri et al., 2016c) and EmoSim508 (Wijeratne et al., 2017b). Emoji similarity has been also used for proposing efficient keyboard emoji organization (Pohl et al., 2017). Recently, Barbieri and Camacho-Collados (2018) show that emoji modifiers (skin tones and gender) can affect the semantics vector representation of emojis. Emoji play an important role in the emotional content of a message. Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ruder, 2016; Hu et al., 2017). During the last decade several studies have shown how sentiment analysis improves when we jointly leverage information coming from different modalities (e.g. text, images"
N18-2107,W16-6208,0,0.104099,"Missing"
N18-2107,D17-1169,0,0.0624218,"edict the emoji to be associated to a piece of content may help to improve natural language processing tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). Emojis are small images that are commonly included in social media text messages. The combination of visual and textual content in the same message builds up a modern way of communication, that automatic systems are not used to deal with. In this paper we extend recent advances in emoji prediction by putting forward a multimodal approach that is able to predict emojis in Instagram posts. Instagram posts are composed of pictures together with texts which sometimes include emojis. We show that these emojis can be predicted by using the text, but also using the picture. Our main finding is that"
N18-2107,E17-2068,0,0.0438927,"emojis (the 1 In this paper we only utilize the first comment issued by the user who posted the picture. 679 Proceedings of NAACL-HLT 2018, pages 679–686 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 3.2 most frequent emojis are shown in Table 3). Our dataset is composed of 299,809 posts, each containing a picture, the text associated to it and only one emoji. In the experiments we also considered the subsets of the 10 (238,646 posts) and 5 most frequent emojis (184,044 posts) (similarly to the approach followed by Barbieri et al. (2017)). Fastext (Joulin et al., 2017) is a linear model for text classification. We decided to employ FastText as it has been shown that on specific classification tasks, it can achieve competitive results, comparable to complex neural classifiers (RNNs and CNNs), while being much faster. FastText represents a valid approach when dealing with social media content classification, where huge amounts of data needs to be processed and new and relevant information is continuously generated. The FastText algorithm is similar to the CBOW algorithm (Mikolov et al., 2013), where the middle word is replaced by the label, in our case the em"
N18-2107,L16-1626,1,0.900084,"an give context to textual messages like in the following Instagram posts: (1)“Love my new home ” (associated to a picture of a bright garden, outside) and (2) “I can’t believe it’s the first day of school!!! Saliency Maps In order to show the parts of the image most relevant for each class we analyze the global average pooling (Lin et al., 2013) on the convolutional 682 context, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. In order to further study emoji semantics, two datasets with pairwise emoji similarity, with human annotations, have been proposed: EmoTwi50 (Barbieri et al., 2016c) and EmoSim508 (Wijeratne et al., 2017b). Emoji similarity has been also used for proposing efficient keyboard emoji organization (Pohl et al., 2017). Recently, Barbieri and Camacho-Collados (2018) show that emoji modifiers (skin tones and gender) can affect the semantics vector representation of emojis. Emoji play an important role in the emotional content of a message. Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ru"
N18-2107,D15-1293,0,0.0722539,"Missing"
N18-2107,D15-1176,0,0.0230806,"101 layers (ResNet-101), initialized with pretrained parameters learned on ImageNet (Deng et al., 2009). We use this model as a starting point to later finetune it on our emoji classification task. Learning rate was set to 0.0001 and we early stopped the training when there was not improving in the validation set. 3.3 B-LSTM Baseline Barbieri et al. (2017) propose a recurrent neural network approach for the emoji prediction task. We use this model as baseline, to verify whether FastText achieves comparable performance. They used a Bidirectional LSTM with character representation of the words (Ling et al., 2015; Ballesteros et al., 2015) to handle orthographic variants (or even spelling errors) of the same word that occur in social media (e.g. cooooool vs cool). 4 Experiments and Evaluation In order to study the relation between Instagram posts and emojis, we performed two different experiments. In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017). Our second experiment (Section 4.3) evaluates the visual (ResNet) and textual (FastText) models on the emoji prediction task. Moreover, we evaluate a multi"
N18-2107,P17-3022,0,0.316941,"Missing"
N18-2107,D15-1303,0,0.0352395,"and gender) can affect the semantics vector representation of emojis. Emoji play an important role in the emotional content of a message. Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ruder, 2016; Hu et al., 2017). During the last decade several studies have shown how sentiment analysis improves when we jointly leverage information coming from different modalities (e.g. text, images, audio, video) (Morency et al., 2011; Poria et al., 2015; Tran and Cambria, 2018). In particular, when we deal with Social Media posts, the presence of both textual and visual content has promoted a number of investigations on sentiment or emotions (Baecchi et al., 2016; You et al., 2016b,a; Yu et al., 2016; Chen et al., 2015) or emojis (Cappallo et al., 2015, 2018). Figure 2: Three test pictures. From left to right, we show the four most likely predicted emojis and their correspondent class activation mapping heatmap. I love being these boys’ mommy!!!! #myboys #mommy ” (associated to picture of two boys wearing two blue shirts). In both examples t"
N18-3014,W14-4012,0,0.155196,"Missing"
N18-3014,D17-1300,0,0.0584202,"responds seamlessly. Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications (Abdelfattah et al., 2016). In addition, the batch size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work (as evidenced by increasingly difficult batching scenarios in dynamic frameworks (Neubig et al., 2017b)). Others have successfully used low precision approximations to neural net models. Vanhoucke et al. (2011) explored 8-bit quantization for feedforward neural nets for speech recognition. Devlin (2017) explored 16-bit quantization for machine translation. In this paper we show the effectiveness of 8-bit decoding for models that have been trained using 32-bit floating point values. Results show that 8-bit decoding does not hurt the fluency or adequacy of the output, while producing results up to 4-6x times faster. In addition, implementation is straightforward and we can use the models as is without altering training. The paper is organized as follows: Section 2 reviews the attentional model of translation to be sped up, Section 3 presents our 8-bit quantization in our implementation, Sectio"
N18-3014,W17-3204,0,0.02254,"tency-sensitive applications and to control cloud hosting costs. In this paper we show the effectiveness of translating with 8-bit quantization for models that have been trained using 32-bit floating point values. Results show that 8-bit translation makes a non-negligible impact in terms of speed with no degradation in accuracy and adequacy. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sutskever et al., 2014) has recently achieved remarkable performance improving fluency and adequacy over phrase-based machine translation and is being deployed in commercial settings (Koehn and Knowles, 2017). However, this comes at a cost of slow decoding speeds compared to phrase-based and syntax-based SMT (see section 3). NMT models are generally trained using 32-bit floating point values. At training time, multiple sentences can be processed in parallel leveraging graphical processing units (GPUs) to good advantage since the data is processed in batches. This is also true for decoding for non-interactive applications such as bulk document translation. Why is fast execution on CPUs important? First, CPUs are cheaper than GPUs. Fast CPU computation will reduce commercial deployment costs. Second"
N18-3014,E17-1099,0,0.0313246,"wn Heights, NY 10598. U.S jlquinn@us.ibm.com, miguel.ballesteros@ibm.com Abstract 2017a), it is important to translate individual sentences quickly enough so that users can have an application experience that responds seamlessly. Translating individual sentences with NMT requires many memory bandwidth intensive matrixvector or matrix-narrow matrix multiplications (Abdelfattah et al., 2016). In addition, the batch size is 1 and GPUs do not have a speed advantage over CPUs due to the lack of adequate parallel work (as evidenced by increasingly difficult batching scenarios in dynamic frameworks (Neubig et al., 2017b)). Others have successfully used low precision approximations to neural net models. Vanhoucke et al. (2011) explored 8-bit quantization for feedforward neural nets for speech recognition. Devlin (2017) explored 16-bit quantization for machine translation. In this paper we show the effectiveness of 8-bit decoding for models that have been trained using 32-bit floating point values. Results show that 8-bit decoding does not hurt the fluency or adequacy of the output, while producing results up to 4-6x times faster. In addition, implementation is straightforward and we can use the models as is"
N18-3014,D08-1060,0,0.0848281,"Missing"
N18-3014,E17-3017,0,0.0869626,"Missing"
N18-3014,P16-1162,0,0.106873,"Missing"
N18-3014,W06-3602,0,0.0758728,"Missing"
N19-1004,2017.lilt-15.3,0,0.0190167,"Department of Linguistics and Philosophy, MIT 5 Department of Brain and Cognitive Sciences, MIT, {pqian,rplevy}@mit.edu 6 IBM Research, MIT-IBM Watson AI Lab, miguel.ballesteros@ibm.com 2 Department Abstract language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind. The basic idea is to examine language models’ behavior on targeted sentences chosen to probe particular aspects of the learned representations. This approach was introduced by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphor"
N19-1004,J92-4003,0,0.532968,"mbeddings as input. The second large LSTM is the model described in the supplementary materials of Gulordava et al. (2018), which we call “GRNN”, trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), and the surprisal of the i-th word in the sentence min (w1...i ) is estimated as log PPmin (w1...i−1 ) . Our action beam is size 100, and our word beam is size 10. Finally, to disentangle effects of training set from model architecture, we use an LSTM trained on string data from the Penn T"
N19-1004,C18-1012,0,0.10732,"by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a represen"
N19-1004,P18-1132,0,0.0960351,"Missing"
N19-1004,N16-1024,1,0.930831,"es which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signalling changes in syntactic state. 1 In this work, we consider syntactic representations of a different kind. Previous studies have focused on relationships of dependency: one word licenses another word, which is tested by asking whether a language model favors one (grammatically licensed) form over another in a particular context. Here we fo"
N19-1004,K17-1003,0,0.0839398,"Missing"
N19-1004,E17-1065,1,0.849761,"gs of NAACL-HLT 2019, pages 32–42 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (1) In psycholinguistics, the common practice is to study reaction times per word (for example, reading time as measured by an eyetracker), as a measure of the word-by-word difficulty of online language processing. These reading times are often taken to reflect the extent to which humans expect certain words in context, and may be generally proportional to surprisal given the comprehender’s probabilistic language model (Hale, 2001; Levy, 2008; Smith and Levy, 2013; Futrell and Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used as models of human online language processing (Hale et al.,"
N19-1004,Q16-1037,0,0.203867,"arch Institute, Kyoto University, tmorita@alum.mit.edu 4 Department of Linguistics and Philosophy, MIT 5 Department of Brain and Cognitive Sciences, MIT, {pqian,rplevy}@mit.edu 6 IBM Research, MIT-IBM Watson AI Lab, miguel.ballesteros@ibm.com 2 Department Abstract language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind. The basic idea is to examine language models’ behavior on targeted sentences chosen to probe particular aspects of the learned representations. This approach was introduced by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli"
N19-1004,W18-0102,0,0.157489,"comprehender’s probabilistic language model (Hale, 2001; Levy, 2008; Smith and Levy, 2013; Futrell and Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used as models of human online language processing (Hale et al., 2018). As the doctor studied the textbook, the nurse walked into the office. In this work, we use a targeted evaluation approach (Marvin and Linzen, 2018) to elicit evidence for syntactic state representations from language models. That is, we examine language model behavior on artificially constructed sentences designed to expose behavior that is crucially dependent on syntactic state representations. In particular, we study complex subordinate clauses and garden path effects (based on mainverb/reduced-rel"
N19-1004,N18-1108,0,0.360138,"partment of Brain and Cognitive Sciences, MIT, {pqian,rplevy}@mit.edu 6 IBM Research, MIT-IBM Watson AI Lab, miguel.ballesteros@ibm.com 2 Department Abstract language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind. The basic idea is to examine language models’ behavior on targeted sentences chosen to probe particular aspects of the learned representations. This approach was introduced by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We inv"
N19-1004,J93-2004,0,0.0644458,"ated that humans maintain representations of this kind in syntactic processing (Staub and Clifton, 2006; Lau et al., 2006; Levy et al., 2012). Here we ask whether the string completion probabilities produced by neural language models show evidence of the same knowledge. and CNN character embeddings as input. The second large LSTM is the model described in the supplementary materials of Gulordava et al. (2018), which we call “GRNN”, trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), a"
N19-1004,P18-1254,0,0.26142,"Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used as models of human online language processing (Hale et al., 2018). As the doctor studied the textbook, the nurse walked into the office. In this work, we use a targeted evaluation approach (Marvin and Linzen, 2018) to elicit evidence for syntactic state representations from language models. That is, we examine language model behavior on artificially constructed sentences designed to expose behavior that is crucially dependent on syntactic state representations. In particular, we study complex subordinate clauses and garden path effects (based on mainverb/reduced-relative ambiguities and NP/Z ambiguities). We ask three general questions: (1) Is there basic e"
N19-1004,D18-1151,0,0.404611,"., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to updat"
N19-1004,N01-1021,0,0.780657,"upon recent work studying neural 32 Proceedings of NAACL-HLT 2019, pages 32–42 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (1) In psycholinguistics, the common practice is to study reaction times per word (for example, reading time as measured by an eyetracker), as a measure of the word-by-word difficulty of online language processing. These reading times are often taken to reflect the extent to which humans expect certain words in context, and may be generally proportional to surprisal given the comprehender’s probabilistic language model (Hale, 2001; Levy, 2008; Smith and Levy, 2013; Futrell and Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used"
N19-1004,N18-1202,0,0.0711524,"r of neural language models reflects the kind of generalizations that would be captured using a stack-based incremental parse state in a symbolic grammar-based model. For example, during the underlined portion of Example (1), an incremental language model should represent and maintain the knowledge that it is currently inside a subordinate clause, implying (among other things) that a full main clause must follow. Introduction It is now standard practice in NLP to derive sentence representations using neural sequence models of various kinds (Elman, 1990; Sutskever et al., 2014; Goldberg, 2017; Peters et al., 2018; Devlin et al., 2018). However, we do not yet have a firm understanding of the precise content of these representations, which poses problems for interpretability, accountability, and controllability of NLP systems. More specifically, the success of neural sequence models has raised the question of whether and how these networks learn robust syntactic generalizations about natural language, which would enable robust performance even on data that differs from the peculiarities of the training set. Here we build upon recent work studying neural 32 Proceedings of NAACL-HLT 2019, pages 32–42 c Mi"
N19-1004,D18-1499,0,0.0904612,"Missing"
N19-1004,D17-1178,0,0.0835211,"materials of Gulordava et al. (2018), which we call “GRNN”, trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), and the surprisal of the i-th word in the sentence min (w1...i ) is estimated as log PPmin (w1...i−1 ) . Our action beam is size 100, and our word beam is size 10. Finally, to disentangle effects of training set from model architecture, we use an LSTM trained on string data from the Penn Treebank training set, which we call TinyLSTM. For TinyLSTM we use 256dimensional wor"
N19-1004,J95-2002,0,0.114217,"ers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), and the surprisal of the i-th word in the sentence min (w1...i ) is estimated as log PPmin (w1...i−1 ) . Our action beam is size 100, and our word beam is size 10. Finally, to disentangle effects of training set from model architecture, we use an LSTM trained on string data from the Penn Treebank training set, which we call TinyLSTM. For TinyLSTM we use 256dimensional word-embedding inputs and hidden layers and dropout probability 0.3, just as with the RNNG. 3 We can detect the knowledge of synt"
N19-1004,W18-5423,1,0.824536,"ed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic st"
N19-1159,D15-1041,1,0.825182,"haracters ch1:m of wi (BiLSTM(ch1:m )). xi = e(wi ) ◦ p(wi ) ◦ BiLSTM(ch1:m ) Without a POS tag embedding, the word vector is a representation of the word type. With POS information, we have some information about the word in the context of the sentence and the tagger has had access to the full sentence. The representation of the word at the input of the BiLSTM is therefore more contextualised and it can be expected that a recursive composition function will be less helpful than when POS information is not used. Character information has been shown to be useful for dependency parsing first by Ballesteros et al. (2015). Ballesteros et al. (2015) and Smith et al. (2018b) among others have shown that POS and character information are somewhat complementary. Ballesteros et al. (2015) used similar character vectors in the S-LSTM parser but did not look at the impact of composition when using these vectors. Here, we experiment with ablating either or both of the character and POS vectors. We look at the impact of using composition on the full model as well as these ablated models. We hypothesise that composition is most helpful when those vectors are not used, since they give information about the functional use"
N19-1159,P15-1033,1,0.906149,"crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved lookahead of a backward LSTM is especially important for head-final languages. 1 Introduction Recursive neural networks allow us to construct vector representations of trees or subtrees. They have been used for constituency parsing by Socher et al. (2013) and Dyer et al. (2016) and for dependency parsing by Stenetorp (2013) and Dyer et al. (2015), among others. In particular, Dyer et al. (2015) showed that composing representations of subtrees using recursive neural networks can be beneficial for transition-based dependency parsing. These results were further strengthened in Kuncoro et al. (2017) who showed, using ablation experiments, that composition is key in the Recurrent Neural Network Grammar (RNNG) generative parser by Dyer et al. (2016). In a parallel development, Kiperwasser and Goldberg (2016b) showed that using BiLSTMs for feature extraction can lead to high parsing accuracy even with fairly simple parsing architectures, an"
N19-1159,N16-1024,1,0.870152,"features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved lookahead of a backward LSTM is especially important for head-final languages. 1 Introduction Recursive neural networks allow us to construct vector representations of trees or subtrees. They have been used for constituency parsing by Socher et al. (2013) and Dyer et al. (2016) and for dependency parsing by Stenetorp (2013) and Dyer et al. (2015), among others. In particular, Dyer et al. (2015) showed that composing representations of subtrees using recursive neural networks can be beneficial for transition-based dependency parsing. These results were further strengthened in Kuncoro et al. (2017) who showed, using ablation experiments, that composition is key in the Recurrent Neural Network Grammar (RNNG) generative parser by Dyer et al. (2016). In a parallel development, Kiperwasser and Goldberg (2016b) showed that using BiLSTMs for feature extraction can lead to h"
N19-1159,N18-1108,0,0.0222468,"shared task (Zeman et al., 2017) and 10 out of the 10 highest performing systems of the 2018 CoNLL shared task (Zeman et al., 2018). This raises the question of whether features extracted with BiLSTMs in themselves capture information about subtrees, thus making recursive composition superfluous. Some support for this hypothesis comes from the results of Linzen et al. (2016) which indicate that LSTMs can capture hierarchical information: they can be trained to predict long-distance number agreement in English. Those results were extended to more constructions and three additional languages by Gulordava et al. (2018). However, Kuncoro et al. (2018) have also shown that although sequential LSTMs can learn syntactic information, a recursive neural network which explicitly models hierarchy (the RNNG model from Dyer et al. (2015)) is better at this: it performs better on the number agreement task from Linzen et al. (2016). To further explore this question in the context of dependency parsing, we investigate the use of recursive composition (henceforth referred to as composition) in a parser with an architecture like the one in Kiperwasser and Goldberg (2016b). This allows us to explore variations of features"
N19-1159,Q16-1032,0,0.292801,"es. They have been used for constituency parsing by Socher et al. (2013) and Dyer et al. (2016) and for dependency parsing by Stenetorp (2013) and Dyer et al. (2015), among others. In particular, Dyer et al. (2015) showed that composing representations of subtrees using recursive neural networks can be beneficial for transition-based dependency parsing. These results were further strengthened in Kuncoro et al. (2017) who showed, using ablation experiments, that composition is key in the Recurrent Neural Network Grammar (RNNG) generative parser by Dyer et al. (2016). In a parallel development, Kiperwasser and Goldberg (2016b) showed that using BiLSTMs for feature extraction can lead to high parsing accuracy even with fairly simple parsing architectures, and using BiLSTMs for feature extraction has therefore become very popular in dependency parsing. It is used in the state-of-the-art parser of Dozat and Manning (2017), was used in 8 of the 10 highest performing systems of the 2017 CoNLL shared task (Zeman et al., 2017) and 10 out of the 10 highest performing systems of the 2018 CoNLL shared task (Zeman et al., 2018). This raises the question of whether features extracted with BiLSTMs in themselves capture inform"
N19-1159,Q16-1023,0,0.38479,"es. They have been used for constituency parsing by Socher et al. (2013) and Dyer et al. (2016) and for dependency parsing by Stenetorp (2013) and Dyer et al. (2015), among others. In particular, Dyer et al. (2015) showed that composing representations of subtrees using recursive neural networks can be beneficial for transition-based dependency parsing. These results were further strengthened in Kuncoro et al. (2017) who showed, using ablation experiments, that composition is key in the Recurrent Neural Network Grammar (RNNG) generative parser by Dyer et al. (2016). In a parallel development, Kiperwasser and Goldberg (2016b) showed that using BiLSTMs for feature extraction can lead to high parsing accuracy even with fairly simple parsing architectures, and using BiLSTMs for feature extraction has therefore become very popular in dependency parsing. It is used in the state-of-the-art parser of Dozat and Manning (2017), was used in 8 of the 10 highest performing systems of the 2017 CoNLL shared task (Zeman et al., 2017) and 10 out of the 10 highest performing systems of the 2018 CoNLL shared task (Zeman et al., 2018). This raises the question of whether features extracted with BiLSTMs in themselves capture inform"
N19-1159,P11-1068,0,0.0687526,"steros et al. (2015) used similar character vectors in the S-LSTM parser but did not look at the impact of composition when using these vectors. Here, we experiment with ablating either or both of the character and POS vectors. We look at the impact of using composition on the full model as well as these ablated models. We hypothesise that composition is most helpful when those vectors are not used, since they give information about the functional use of the word in context. Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a SWAP transition and a StaticDynamic oracle, as described in de Lhoneux et al. (2017b)4 . The SWAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarsegrained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below th"
N19-1159,E17-1117,1,0.904856,"Missing"
N19-1159,P18-1132,0,0.0947468,"Missing"
N19-1159,K17-3022,1,0.911906,"Missing"
N19-1159,W17-6314,1,0.904094,"Missing"
N19-1159,Q16-1037,0,0.200219,"res, and using BiLSTMs for feature extraction has therefore become very popular in dependency parsing. It is used in the state-of-the-art parser of Dozat and Manning (2017), was used in 8 of the 10 highest performing systems of the 2017 CoNLL shared task (Zeman et al., 2017) and 10 out of the 10 highest performing systems of the 2018 CoNLL shared task (Zeman et al., 2018). This raises the question of whether features extracted with BiLSTMs in themselves capture information about subtrees, thus making recursive composition superfluous. Some support for this hypothesis comes from the results of Linzen et al. (2016) which indicate that LSTMs can capture hierarchical information: they can be trained to predict long-distance number agreement in English. Those results were extended to more constructions and three additional languages by Gulordava et al. (2018). However, Kuncoro et al. (2018) have also shown that although sequential LSTMs can learn syntactic information, a recursive neural network which explicitly models hierarchy (the RNNG model from Dyer et al. (2015)) is better at this: it performs better on the number agreement task from Linzen et al. (2016). To further explore this question in the conte"
N19-1159,de-marneffe-etal-2014-universal,1,0.845216,"Missing"
N19-1159,D07-1013,1,0.833013,"the recurrent cell does not help the forward LSTM case but the LSTM cell does to some extent. It is interesting to note that using composition, especially using an LSTM cell, bridges a substantial part of the gap between the bw and the bi models. These results can be related to the literature on transition-based dependency parsing. Transitionbased parsers generally rely on two types of features: history-based features over the emerging dependency tree and lookahead features over the buffer of remaining input. The former are based on a hierarchical structure, the latter are purely sequential. McDonald and Nivre (2007) and McDonald and Nivre (2011) have shown that historybased features enhance transition-based parsers as long as they do not suffer from error propagation. However, Nivre (2006) has also shown that lookahead features are absolutely crucial given the greedy left-to-right parsing strategy. backward LSTM provides an improved lookahead. Similarly to the lookahead in statistical parsing, it is sequential. The difference is that it gives information about upcoming words with unbounded length. The forward LSTM in this model architecture provides history-based information but unlike in statistical par"
N19-1159,J08-4003,1,0.569151,"e the same for all occurrences of a word type. Type vectors are then passed through a feature function which learns representations of words in the context of the sentence. xi = e(wi ) vi = f (x1:n , i) We refer to the vector vi as a token vector, as it is different for different tokens of the same word type. In Kiperwasser and Goldberg (2016b), the feature function used is a BiLSTM. As is usual in transition-based parsing, parsing involves taking transitions from an initial configuration to a terminal one. Parser configurations are represented by a stack, a buffer and set of dependency arcs (Nivre, 2008). For each configuration c, the feature extractor concatenates the token representations of core elements from the stack and 1 Kiperwasser and Goldberg (2016b) also define a graphbased parser with similar feature extraction, but we focus on transition-based parsing. buffer. These token vectors are passed to a classifier, typically a Multilayer Perceptron (MLP). The MLP scores transitions together with the arc labels for transitions that involve adding an arc. Both the word type vectors and the BiLSTMs are trained together with the model. 3 Composing Subtree Representations Dyer et al. (2015) l"
N19-1159,P09-1040,1,0.805646,"vectors. We look at the impact of using composition on the full model as well as these ablated models. We hypothesise that composition is most helpful when those vectors are not used, since they give information about the functional use of the word in context. Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a SWAP transition and a StaticDynamic oracle, as described in de Lhoneux et al. (2017b)4 . The SWAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarsegrained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embeddings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished version of the model used for the shared task"
N19-1159,P17-2018,0,0.0183754,"POS and/or character information for Czech and English, it does it to a much smaller extent for Basque and Finnish. We hypothesise that arc depth might impact the usefulness of composition, since more depth means more matrix multiplications with the composition function. However, we find no correlation between average arc depth of the treebanks and usefulness of composition. It is an open question why composition helps some languages more than others. Note that we are not the first to use composition over vectors obtained from a BiLSTM in the context of dependency parsing, as this was done by Qi and Manning (2017). The difference is that they compose vectors before scoring transitions. It was also done by Kiperwasser and Goldberg (2016a) who showed that using BiLSTM vectors for words in their Tree LSTM parser is helpful but they did not compare this to using BiLSTM vectors without the Tree LSTM. Recurrent and recursive LSTMs in the way they have been considered in this paper are two ways of constructing contextual information and making it available for local decisions in a greedy parser. The strength of recursive LSTMs is that they can build this contextual information using hierarchical context rathe"
N19-1159,N06-2033,0,0.0398274,"4.6 74.4 92.7 66.7 83.1 79.2 59.5 60.2 85.3 67.3 77.2 93.1 68.1 79.9 70.5 48.7 52.8 83.1 62.6 74.2 79.5 53.4 79.8 71.5 51.2 54.7 83.3 63.4 74.7 80.2 55.0 80.8 73.8 74.5 75.9 76.0 72.8 74.8 67.2 68.2 Table 1: LAS for bi, bw and f w, without and with composition (+lc) with an LSTM. Difference > 0.5 with +lc in bold. 5.3 Ensemble To investigate further the information captured by BiLSTMs, we ensemble the 6 versions of the models with POS and character information with the different feature extractors (bi, bw, f w) with (+lc) and without composition. We use the (unweighted) reparsing technique of Sagae and Lavie (2006)6 and ignoring labels. As can be seen from the UAS scores in Table 2, the ensemble (full) largely outperforms the parser using only a BiLSTM, indicating that the information obtained from the different models is complementary. To investigate the contribution of each of the 6 models, we ablate each one by one. As can be seen from Table 2, ablating either of the BiLSTM models or the backward LSTM using composition, results in the least effective of the ablated models, further strengthening the conclusion that BiLSTMs are powerful feature extractors. 6 Conclusion We investigated the impact of com"
N19-1159,K18-2011,1,0.908244,"Missing"
N19-1159,D18-1291,1,0.894709,"Missing"
N19-1159,P13-1045,0,0.0455775,"o be related to lookahead features and the forward LSTM to the rich history-based features both crucial for transition-based parsers. To capture history-based information, composition is better than a forward LSTM on its own, but it is even better to have a forward LSTM as part of a BiLSTM. We correlate results with language properties, showing that the improved lookahead of a backward LSTM is especially important for head-final languages. 1 Introduction Recursive neural networks allow us to construct vector representations of trees or subtrees. They have been used for constituency parsing by Socher et al. (2013) and Dyer et al. (2016) and for dependency parsing by Stenetorp (2013) and Dyer et al. (2015), among others. In particular, Dyer et al. (2015) showed that composing representations of subtrees using recursive neural networks can be beneficial for transition-based dependency parsing. These results were further strengthened in Kuncoro et al. (2017) who showed, using ablation experiments, that composition is key in the Recurrent Neural Network Grammar (RNNG) generative parser by Dyer et al. (2016). In a parallel development, Kiperwasser and Goldberg (2016b) showed that using BiLSTMs for feature e"
N19-1159,L16-1680,0,0.0471752,"Missing"
N19-1159,K18-2001,1,0.888735,"Missing"
N19-1159,J11-1007,1,\N,Missing
N19-1334,D16-1257,0,0.0739912,"Missing"
N19-1334,C18-1012,0,0.0860248,"gap through certain types of syntactic nodes, illustrated in Figure 4 (Ross, 1967). Contemporary theories variously attribute island effects to grammatical rules, incremental processing considerations, or discourse-structural factors (Ambridge and Goldberg, 2008; Hofmeister and Sag, 2010; Sprouse and Hornstein, 2013). In our setting, a language model is sensitive to an island constraint if it fails to show a wh-licensing interaction between a filler and a gap that cross an island. Wilcox et al. (2018) found evidence that large-data LSTMs are sensitive to some island constraints (although see Chowdhury and Zamparelli (2018) for a contrasting view), but not to others. Here we investigate whether LSTMs would learn these from smaller training datasets, and if an RNNG’s syntactic supervision provides a learning advantage for island constraints. In this section we measure the wh-licensing interaction in the material immediately following the potential gap site, which is guaranteed to implicate the model’s (lack of) expectation for a gap inside the island, rather than throughout the entire embedded clause, which also implicates filler-driven expectations after the end of the island. 5.1 Adjunct Islands Adjunct clauses"
N19-1334,N16-1024,1,0.948293,"ntial contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success. Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement. Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus two models that represent hierarchical structure and deploy it in left-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et al., 2016) and a incrementalized version of the Parsing-as-Language-Modeling configuration from Charniak et al. (2016). Models are tested on a diverse range of configurations for two classes of non-local grammatical dependencies in English—Negative Polarity licensing and Filler–Gap Dependencies. Using the same training data across models, we find that structurally-supervised models outperform the LSTM, with the RNNG demonstrating best results on both types of grammatical dependencies and even learning many of the Island Constraints on the filler–gap dependency. Structural supervision thus provides data"
N19-1334,N18-1108,0,0.058097,". (3) a. *The senator that supported the measure has ever found any support from her constituents. b. No senator that supported the measure has ever found any support from her constituents. c. *The senator that supported no measure has ever found any support from her constituents. d. No senator that supported no measure has ever found any support from her constituents. Learning of NPI licensing conditions by LSTM language models trained on large corpora has previously been investigated by Marvin and Linzen (2018) and Futrell et al. (2018). Futrell et al. found that the language models of both Gulordava et al. (2018) and Jozefowicz et al. (2016) (hereafter called ‘Large Data LSTMs’) learned a contingency between licensors and NPIs: the NPIs in examples like (3) were lower-surprisal when linearly preceded by negative licensors. However, both papers reported that these models failed to constrain the contingency along the correct structural lines: negative NPI surprisal was decreased at least as much by a preceding negative distractor as by a negative licensor. Syntactic supervision might plausibly facilitate learning of NPI licensing conditions. We tested this following the method of Futrell et al. (2018),"
N19-1334,N01-1021,0,0.377516,"m syntactic annotation–crucially, only constituent boundaries and major syntactic categories, with functional tags and empty categories stripped away—whereas the LSTM language model only uses the sequences of terminal words. We train the models until performance converges on the held-out PTB development-set data. 2.2 Psycholinguistic Assessment Paradigm 2.2.1 Surprisal The surprisal, or negative log-conditional probability, S(xi ) of a sentence’s ith word xi , tells us how strongly xi is expected in context and is also known to correlate with human processing difficulty (Smith and Levy, 2013; Hale, 2001; Levy, 2008). For sentences out of context, surprisal is: S(xi ) = − log p(xi |x1 . . . xi−1 ) We investigate a model’s knowledge of a grammatical dependency, which is the co-variance between an upstream licensor and a downstream licensee, by measuring the effect that an upstream licensor has on the surprisal of a downstream licensee. The idea is that grammatical licensors 3303 should set up an expectation for the licensee thus reducing its surprisal compared to minimal pairs in which the licensor is absent. We derive the word surprisal from the LSTM language model by directly computing the n"
N19-1334,P18-1254,0,0.0358147,"e of a grammatical dependency, which is the co-variance between an upstream licensor and a downstream licensee, by measuring the effect that an upstream licensor has on the surprisal of a downstream licensee. The idea is that grammatical licensors 3303 should set up an expectation for the licensee thus reducing its surprisal compared to minimal pairs in which the licensor is absent. We derive the word surprisal from the LSTM language model by directly computing the negative log value of the predicted conditional probability p(xi |x1 . . . xi−1 ) from the softmax layer. Following the method in Hale et al. (2018) for estimating word surprisals from RNNG, we use word-synchronous beam search (Stern et al., 2017) to find a set of most likely incremental parses and sum their forward probabilities to approximate P(x1 , . . . xi ) and P(x1 , . . . xi−1 ) for computing the surprisal. We set the action beam size to 100 and word beam size to 10. We ensured that the correct incremental RNNG parses were present on the beam immediately before and throughout the material over which surprisal was calculated through manual spot inspection; the correct parse was almost always at the top of the beam. 2.2.2 Wh-Licensin"
N19-1334,P18-1132,0,0.153611,"Missing"
N19-1334,Q16-1037,0,0.151024,"Island Constraints on the filler–gap dependency. Structural supervision thus provides data efficiency advantages over purely stringbased training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies. 1 Introduction Long Short-Term Memory Recurrent Neural Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) have achieved state of the art language modeling performance (Jozefowicz et al., 2016) and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (Linzen et al., 2016; Wilcox et al., 2018), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing (Marvin and Linzen, 2018; Futrell et al., 2018). LSTMs, however, require large amounts of training data and remain relatively uninterpretable. One model that attempts to address both these issues is the Recurrent Neural Network Grammar (Dyer et al., 2016). RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing. They can achieve state-of-the-art broad-coverage scores on language modeling a"
N19-1334,J93-2004,0,0.0674976,"ic phrasal boundary marker. The model was trained using embedding size 256, dropout 0.3, and was able to achieve a parsing F1 score of 92.81 on the PTB, which is only marginally better than the performance of the original architecture on the same test set, as reported in Kuncoro et al. (2016). We will refer to this model as the “ActionLSTM” model in the following sections. All three models are trained on the trainingset portion of the English Penn Treebank standardly used in the parsing literature (PTB; sections 2-21), which consists of about 950,000 tokens of English language news-wire text (Marcus et al., 1993). The RNNG and Action models get supervision from syntactic annotation–crucially, only constituent boundaries and major syntactic categories, with functional tags and empty categories stripped away—whereas the LSTM language model only uses the sequences of terminal words. We train the models until performance converges on the held-out PTB development-set data. 2.2 Psycholinguistic Assessment Paradigm 2.2.1 Surprisal The surprisal, or negative log-conditional probability, S(xi ) of a sentence’s ith word xi , tells us how strongly xi is expected in context and is also known to correlate with hum"
N19-1334,D18-1151,0,0.566222,"ng of neural language models in acquiring human-like generalizations about non-local grammatical dependencies. 1 Introduction Long Short-Term Memory Recurrent Neural Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) have achieved state of the art language modeling performance (Jozefowicz et al., 2016) and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (Linzen et al., 2016; Wilcox et al., 2018), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing (Marvin and Linzen, 2018; Futrell et al., 2018). LSTMs, however, require large amounts of training data and remain relatively uninterpretable. One model that attempts to address both these issues is the Recurrent Neural Network Grammar (Dyer et al., 2016). RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing. They can achieve state-of-the-art broad-coverage scores on language modeling and phrase structure parsing tasks, learn Noun Phrase headedness (Kuncoro et al., 2016), and outperform linear models at learning subject-verb numbe"
N19-1334,D17-1178,0,0.298463,"eam licensee, by measuring the effect that an upstream licensor has on the surprisal of a downstream licensee. The idea is that grammatical licensors 3303 should set up an expectation for the licensee thus reducing its surprisal compared to minimal pairs in which the licensor is absent. We derive the word surprisal from the LSTM language model by directly computing the negative log value of the predicted conditional probability p(xi |x1 . . . xi−1 ) from the softmax layer. Following the method in Hale et al. (2018) for estimating word surprisals from RNNG, we use word-synchronous beam search (Stern et al., 2017) to find a set of most likely incremental parses and sum their forward probabilities to approximate P(x1 , . . . xi ) and P(x1 , . . . xi−1 ) for computing the surprisal. We set the action beam size to 100 and word beam size to 10. We ensured that the correct incremental RNNG parses were present on the beam immediately before and throughout the material over which surprisal was calculated through manual spot inspection; the correct parse was almost always at the top of the beam. 2.2.2 Wh-Licensing Interaction Unlike NPI, licensing, the filler—gap dependency is the covariance between a piece of"
N19-1334,W18-5423,1,0.195244,"the filler–gap dependency. Structural supervision thus provides data efficiency advantages over purely stringbased training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies. 1 Introduction Long Short-Term Memory Recurrent Neural Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) have achieved state of the art language modeling performance (Jozefowicz et al., 2016) and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (Linzen et al., 2016; Wilcox et al., 2018), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing (Marvin and Linzen, 2018; Futrell et al., 2018). LSTMs, however, require large amounts of training data and remain relatively uninterpretable. One model that attempts to address both these issues is the Recurrent Neural Network Grammar (Dyer et al., 2016). RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing. They can achieve state-of-the-art broad-coverage scores on language modeling and phrase structure pa"
P15-1033,C14-1076,1,0.893251,"Missing"
P15-1033,P14-2131,0,0.0270437,"ds—both those that are OOV in both the very limited parsing data but present in the pretraining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our par"
P15-1033,P82-1020,0,0.861444,"Missing"
P15-1033,D12-1133,0,0.0254088,"Missing"
P15-1033,P08-1067,0,0.025115,"Missing"
P15-1033,D14-1081,0,0.0362549,"Missing"
P15-1033,D14-1082,0,0.815907,"Transition-Based Dependency Parsing with Stack Long Short-Term Memory Chris Dyer♣♠ Miguel Ballesteros♦♠ Wang Ling♠ Austin Matthews♠ Noah A. Smith♠ ♣ Marianas Labs ♦ NLP Group, Pompeu Fabra University ♠ Carnegie Mellon University chris@marianaslabs.com, miguel.ballesteros@upf.edu, {lingwang,austinma,nasmith}@cs.cmu.edu Abstract decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al., 2014; Ballesteros and Bohnet, 2014) and more recently using neural networks (Chen and Manning, 2014; Stenetorp, 2013). We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser’s state: (i) unbounded look-ah"
P15-1033,N15-1142,1,0.133656,"aining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our parser to maximize the conditional log-likelihood (Eq. 1) of treebank parses given sentenc"
P15-1033,C14-1078,0,0.0487974,"Missing"
P15-1033,D10-1004,1,0.291438,"Missing"
P15-1033,P13-1104,0,0.0353977,"Missing"
P15-1033,de-marneffe-etal-2006-generating,0,0.0341911,"Missing"
P15-1033,W03-3017,0,0.0269603,"step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses thre"
P15-1033,W04-0308,0,0.20895,"ructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses three stack LSTMs:"
P15-1033,P13-2111,0,0.0261337,"ter points (i.e., the hTOP ), a continuous-space “summary” of the contents of the current stack configuration is available. We refer to this value as the “stack summary.” it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = σ(Wf x xt + Wf h ht−1 + Wf c ct−1 + bf ) ct = ft ct−1 + What does the stack summary look like? Intuitively, elements near the top of the stack will it tanh(Wcx xt + Wch ht−1 + bc ), 1 Ours is not the first deviation from a strict left-toright order: previous variations include bidirectional LSTMs (Graves and Schmidhuber, 2005) and multidimensional LSTMs (Graves et al., 2007). 2 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. where σ is the component-wise logistic sigmoid function, and is the component-wise (Hadamard) product. The value ht of the LSTM at each time step is controlled by a third gate (ot ) that is applied to the result of the application of a nonlinearity to the 335 P TO y0 P P TO TO y1 y0 y1 pop ; x1 y0 y1 y2 ; x1 x2 push ; x1 Figure 1: A stack LSTM extends a conventional left-to-right LSTM with the a"
P15-1033,N07-1050,0,0.0449813,"Missing"
P15-1033,J08-4003,0,0.120327,"Missing"
P15-1033,P09-1040,0,0.0683306,"Missing"
P15-1033,P13-1014,0,0.0122661,"represent each input token, we concatenate three vectors: a learned vector representation for each word type (w); a fixed vector representa˜ LM ), and a tion from a neural language model (w learned representation (t) of the POS tag of the token, provided as auxiliary input to the parser. A Our parser is based on the arc-standard transition inventory (Nivre, 2004), given in Figure 3. 5 In general, A(S, B) is the complete set of parser actions discussed in §3.2, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT operation is obligatory (Sartorio et al., 2013). 337 Stackt (u, u), (v, v), S (u, u), (v, v), S S Buffert B B (u, u), B Action Stackt+1 (gr (u, v), u), S (gr (v, u), v), S (u, u), S REDUCE - RIGHT (r) REDUCE - LEFT (r) SHIFT Buffert+1 B B B Dependency r u→v r u←v — Figure 3: Parser transitions indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the win"
P15-1033,P14-6005,0,0.0326662,"Missing"
P15-1033,P04-1013,0,0.0390693,"Missing"
P15-1033,P13-1088,0,0.0213712,"ons. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree has a value computed as a f"
P15-1033,P13-1045,0,0.0166315,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,D13-1170,0,0.0024939,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,P07-1080,0,0.0306643,"Missing"
P15-1033,N03-1033,0,0.0859622,"Missing"
P15-1033,I05-3027,0,0.0128763,"Missing"
P15-1033,P15-1032,0,0.257466,"Missing"
P15-1033,W03-3023,0,0.0327577,"for prediction at each time step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing mo"
P15-1033,D08-1059,0,0.0189076,"Missing"
P15-1033,P11-2033,0,0.0327321,"Missing"
P15-1033,D14-1109,0,0.0354567,"Missing"
P15-1033,Q14-1017,0,\N,Missing
P19-1451,D17-1130,1,0.927908,"beled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and"
P19-1451,D15-1041,1,0.831875,"nd concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1451,D16-1211,1,0.880434,"oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (Daumé III and Marcu, 2005; Daumé III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others. To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees. We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algorithm of Williams (1992) with a ba"
P19-1451,W13-2322,0,0.430589,"ck-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-"
P19-1451,P13-2131,0,0.656993,"tural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also c"
P19-1451,P15-3007,0,0.0187689,"d Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros a"
P19-1451,E17-1053,0,0.0949736,"TMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it"
P19-1451,P15-1033,1,0.866449,"sentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017;"
P19-1451,S16-1186,0,0.0728509,"ents AMR annotations do not provide alignments between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et"
P19-1451,P14-1134,0,0.723706,"ild upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM trans"
P19-1451,P17-1043,0,0.0327329,"of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based algorithm (JAMR) of Flanigan"
P19-1451,C12-1059,0,0.0596808,"graph nodes introduce noise into oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (Daumé III and Marcu, 2005; Daumé III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others. To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees. We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algor"
P19-1451,Q13-1033,0,0.0548568,"Missing"
P19-1451,P16-1001,0,0.127283,"lts. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contr"
P19-1451,D18-1198,0,0.620886,"h study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1)"
P19-1451,N15-1142,0,0.0320197,"tities from the AMR dataset (there are more than 100 entity types in the AMR language) and we trained a neural network NER model (Ni et al., 2017) to predict NER labels for the AMR parser. In the NER model, the target word and its surrounding words and tags are used as features. We jackknifed (90/10) the training data, to train the AMR parser. The ten jackknifed models got an average NER F1 score of 79.48 on the NER dev set. 2.4 Contextualized Vectors Recent work has shown that the use of pre-trained networks improves the performance of downstream tasks. BO uses pre-trained word embeddings by Ling et al. (2015) along with learned character embeddings. In this work, we explore the effect of using contextualized word vectors as pre-trained word embeddings. We experiment with recent context based embedding obtained with BERT (Devlin et al., 2018). We use average of last 4 layers of BERT Large model with hidden representations of size 1024. We produce the word representation by mean pooling the representations of word piece tokens obtained using BERT. We only use the contextualized word vectors as input to our model, we do not back-propagate through the BERT layers. 2.5 Wikification Given that BO does n"
P19-1451,D18-1264,0,0.347593,"with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM depend"
P19-1451,D15-1166,0,0.0127201,"ed and fill in intermediate nodes again. Soft Alignments via Attention: The parser state is represented by the STACK, BUFFER and a list with the history of actions (which are encoded as LSTMs, the first two being Stack-LSTMs (Dyer et al., 2015)). This forms the vector st that represents the state: st = max {0, W[stt ; bt ; at ] + d} . (1) This vector st is used to predict the best action (and concept to add, if applicable) to take, given the state with a softmax. We complement the state with an attention over the input sentence (Bahdanau et al., 2014). In particular, we use general attention (Luong et al., 2015). In order to do so, we add a bidirectional LSTM encoder to the BO parsing model and we run attention over it in each time step. More formally, the attention weights αi (for position i) are calculated based on the actions predicted so far (represented as aj ), the encoder representation of the sentence (hi ) and a projection weight matrix Wa : ei = a> j Wa hi (2) exp(ei ) . αi = P k exp(ek ) (3) A vector representation (cj ) is computed by a weighted sum of the encoded sentence word representations and the α values. X cj = αi · hi . (4) i 2 https://isi.edu/~damghani/papers/ Aligner.zip 3 When"
P19-1451,P18-1037,0,0.310823,"ach of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of diffe"
P19-1451,S16-1166,0,0.0434133,"trics represent the structure and semantic parsing task. For all the remaining metrics, our parser consistently achieves the second best results. Also, our best single model (16) achieves more than 9 Smatch points on top of BO (0). Guo and Lu (2018)’s parser is a reimplementation of BO with a refined search space (which we did not attempt) and we beat their performance by 5 points. 5 BO reported results on the 2014 dataset. LDC2016E25 and LDC2017T10 contain the same AMR annotations as of March 2016. LDC2017T10 is the general release while LDC2016E25 was released for Semeval 2016 participants (May, 2016). 4589 6 Id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Experiment BO (JAMR) BO + Label (JAMR) BO + Label 2 + POS 3 + DEP 4 + NER 5 + Concepts 6 + BERT 1 + Attention 8 + POS 9 + DEP 10 + NER 11 + Concepts 12 + BERT11 13 + Smatch 8 + BERT 14 + RL Zhang et al. (2019) Lyu and Titov (2018) van Noord and Bos (2017) Guo and Lu (2018) Smatch 65.9 67.0 68.3 69.0 69.4 69.8 70.9 72.9 69.8 70.4 70.7 70.8 71.8 73.1 73.6 73.4 75.5 76.3 74.4 71.0 69.8 Unlabeled 71 72 73 74 75 75 76 78 75 75 75 76 77 78 78 78 80 79 77 74 74 No WSD 66 68 69 70 70 70 71 73 70 71 71 71 72 74 74 74 76 77 76 72 72 Named Entities 80"
P19-1451,P17-1135,1,0.902876,"Missing"
P19-1451,W03-3017,0,0.289676,"Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics any preprocessing required.1 We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that improved the resul"
P19-1451,J08-4003,0,0.114297,"Missing"
P19-1451,N18-1202,0,0.0509141,"Missing"
P19-1451,D14-1048,0,0.535358,"ased parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of B"
P19-1451,D17-1129,0,0.197202,"s between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based al"
P19-1451,P15-2141,0,0.142384,"e performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition,"
P19-1451,N15-1040,0,0.231316,"e performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition,"
P19-1451,W03-3023,0,0.169656,"as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics any preprocessing required.1 We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that impro"
P19-1451,P19-1009,0,0.337492,"Missing"
P19-1451,D16-1065,0,0.058624,"s comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several"
P19-1451,N18-1106,0,0.0653911,"Missing"
P19-1451,N18-2023,0,0.0336166,"ents of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has b"
Q16-1031,W14-3902,0,0.0352708,"Missing"
Q16-1031,W06-2920,0,0.165241,"l., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a) has created an opportunity to develop a parser that is capable of parsing sentences in multiple languages, addressing these theoretical and practical concerns.3 A multilingual parser can potentially replace an array of language-specific monolingually-trained parsers 2 While our parser can be used to parse input with codeswitching, we have not evaluated this capability due to the lack of appropriate data. 3 Although multilingual dependency treebanks have been available for a decade via the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the treebank of each language was annotated independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxili"
Q16-1031,D14-1082,0,0.821854,"languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a dependency parser for a set of target languages Lt , given universal dependency annotations in a set of source languages Ls . Ideally, we would like to have training data in all target"
Q16-1031,D11-1005,1,0.955701,"language was annotated independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically"
Q16-1031,P15-2139,0,0.546315,"entions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing:"
Q16-1031,D15-1040,0,0.568546,"entions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing:"
Q16-1031,D12-1001,0,0.0806483,"ework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in"
Q16-1031,P15-1033,1,0.28604,"predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a dependency parser for a set of target languages Lt , given universal dependency annotations in a set of source languages Ls . Ideally, we would like to have training data in all target languages (i.e., Lt"
Q16-1031,P15-1119,0,0.573596,"parsing with M A LOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in M A LOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings. 3.3 Lexical Embeddings Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; Täckström et al., 2012; Tiedemann, 2015; Guo et al., 2015). Therefore, we extend the token representation in M A LOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types. Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the ‘projected clusters’ method in Täckström et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster. Multilingual word embeddings. We"
Q16-1031,E89-1018,0,0.221127,"Missing"
Q16-1031,N03-1014,0,0.100262,"ers between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated se"
Q16-1031,P15-1162,0,0.195853,"efines a categorical distribution over possible POS tags. For parsing, we construct the token representation by further concatenating the embeddings of predicted POS tags. This token representation feeds into the stack-LSTM modules of the buffer and stack components of the transition-based parser. This multi-task learning setup enables us to predict both POS tags and dependency trees in the same model. We note that pretrained word embeddings, cluster embeddings and language embeddings are shared for tagging and parsing. Block dropout. We use an independently developed variant of word dropout (Iyyer et al., 2015), which we call block dropout. The token representation used for parsing includes the embedding of predicted POS tags, which may be incorrect. We introduce another modification which makes the parser more robust to incorrect POS tag predictions, by stochastically zeroing out the entire embedding of the POS tag. While training the parser, we replace the POS embedding vector e with another vector (of the same dimensionality) stochastically computed as: e0 = (1 − b)/µ × e, where b ∈ {0, 1} is a Bernoulli-distributed random variable with parameter µ which is initialized to 1.0 (i.e., always dropou"
Q16-1031,P12-3005,0,0.0572083,"Missing"
Q16-1031,P14-1126,0,0.0336688,"erson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embed"
Q16-1031,D11-1006,0,0.536368,"ed independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recen"
Q16-1031,P12-1066,0,0.677145,"eebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a"
Q16-1031,P05-1013,0,0.0543947,"nsition-based Parsing with S-LSTMs This section briefly reviews Dyer et al.’s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures: • a buffer (from which we read the token sequence), The parser uses the arc-standard transition system (Nivre, 2004).11 At each timestep t, a transition action is applied that alters these data structures according to Table 2. 11 In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the “baseline” scheme in (Nivre and Nilsson, 2005). We evaluate against the original nonprojective test set. 433 Token Representations The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in Dyer et al. (2015). 13 The total number of actions is 1+2× the number of unique dependency labels in the treebank used for training, but we only consider actions which meet the arc-standard preconditions in Fig. 2. Stackt u, v, S u, v, S S Buffert B B u, B Action REDUCE - RIGHT (r) REDUCE - LEFT (r) S"
Q16-1031,W04-0308,0,0.0841575,"action in every time step until a complete parse tree is produced. The following sections describe our extensions of the core parser. More details about the core parser can be found in Dyer et al. (2015). • a list of actions previously taken by the parser. 3.2 3.1 Transition-based Parsing with S-LSTMs This section briefly reviews Dyer et al.’s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures: • a buffer (from which we read the token sequence), The parser uses the arc-standard transition system (Nivre, 2004).11 At each timestep t, a transition action is applied that alters these data structures according to Table 2. 11 In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the “baseline” scheme in (Nivre and Nilsson, 2005). We evaluate against the original nonprojective test set. 433 Token Representations The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in Dyer et al. (2015). 1"
Q16-1031,P15-2034,0,0.047877,"word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotat"
Q16-1031,petrov-etal-2012-universal,0,0.079337,"Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7),"
Q16-1031,D15-1039,0,0.0550973,"ntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embeddings and fine-grained POS embeddings, on average outperforms monolingually-trained parsers for target languages with a treebank. This pattern of results is qui"
Q16-1031,P15-1165,0,0.11785,"Missing"
Q16-1031,N12-1052,0,0.735129,"accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly avai"
Q16-1031,Q13-1001,0,0.0298107,"OPA de 54.1 55.9 57.1 es 68.3 73.0 74.6 target language fr it pt 68.8 69.4 72.5 71.0 71.2 78.6 73.9 72.5 77.0 average sv 62.5 69.5 68.1 65.9 69.3 70.5 Table 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then applied it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. Naseem et al. (2012), Täckström et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer. To add lexical information, Täckström et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015), Søgaard et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Duong et al. (2015b) used a neural network based model, sharing most of the parameters between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding la"
Q16-1031,W14-1614,0,0.110791,"nd Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped"
Q16-1031,W15-2137,0,0.0388388,"For multilingual parsing with M A LOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in M A LOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings. 3.3 Lexical Embeddings Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; Täckström et al., 2012; Tiedemann, 2015; Guo et al., 2015). Therefore, we extend the token representation in M A LOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types. Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the ‘projected clusters’ method in Täckström et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster. Multilingual"
Q16-1031,P07-1080,0,0.0567686,"nguages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2"
Q16-1031,P16-2069,0,0.0633887,"Missing"
Q16-1031,W14-1613,0,0.0268074,"nt scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then applied it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. Naseem et al. (2012), Täckström et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer. To add lexical information, Täckström et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015), Søgaard et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Duong et al. (2015b) used a neural network based model, sharing most of the parameters between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers"
Q16-1031,H01-1035,0,0.0721877,"k parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single pa"
Q16-1031,I08-3008,0,0.044074,"train the parser on six other languages in the Google universal dependency treebanks version 2.029 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags. Our parser uses the same word embeddings and word clusters used in Guo et al. (2016), and does not use any typology information.30 The results in Table 8 show that, on average, our parser outperforms both baselines by more than 1 point in LAS, and gives the best LAS results in four (out of six) languages. 5 Related Work Our work builds on the model transfer approach, which was pioneered by Zeman and Resnik (2008) 29 https://github.com/ryanmcd/uni-dep-tb/ In preliminary experiments, we found language embeddings to hurt the performance of the parser for target languages without a treebank. 30 LAS Zhang and Barzilay (2015) Guo et al. (2016) M A LOPA de 54.1 55.9 57.1 es 68.3 73.0 74.6 target language fr it pt 68.8 69.4 72.5 71.0 71.2 78.6 73.9 72.5 77.0 average sv 62.5 69.5 68.1 65.9 69.3 70.5 Table 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then appli"
Q16-1031,D15-1213,0,0.342662,"th its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to"
Q16-1031,D15-1127,0,\N,Missing
Q18-1017,P17-2021,0,0.01881,"information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks like multiword expression detection and part-of-speech tagging have been found very useful for others"
Q18-1017,Q16-1031,1,0.803409,"en used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al., 2017; Luong et al., 2015a; Zoph and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to sequence model inspired by Ammar et al. (2016). All tasks share a common output vocabulary and generate terms according to (3). We learn multiple tasks simultaneously by prepending a special task embedding vector to the target. The task vector symbolizes the task we are focusing on. The model can root det pobj amod nsubj prep det The brown fox jumped over the fence 2 1 1 -4 -1 1 -2 Figure 1: Illustration of the encoding of an unlabeled parsing tree into a sequence of distances. The first row contains the sentence (source) and its parse tree, and the second row contains the matching distances sequence (target). solve each of the tasks it w"
Q18-1017,P16-1231,0,0.0440711,"itrarily output a shorter/longer sequence.4 Although no structural constraints were imposed, our sequence to sequence model is able to obtain a decent parsing result.5 The model achieves 86.99 UAS for English Penn tree-bank with Stanford Dependencies,6 and 80.28 UAS for the German TIGER treebank when the model is only trained to predict the sequence of distances to head as described in Section 3. This is below the best results achieved by state-of-the-art parsers, that are already around 95 for English (Dozat and Manning, 2016; Kuncoro et al., 2017), and around 90 for the same German dataset (Andor et al., 2016; Kuncoro et al., 2016; Bohnet and Nivre, 2012). As a side product of our research, we show that dependency parsing can be approached via a sequence to sequence with an attention mode commonly used for neural machine translation with linearized (using sequences of head distances) dependency trees. Note that, in this case, the models are solely trained on predicting the sequence of distances to the head and are not trained to predict the sequence of dependency labels. For part-of-speech tagging, we use the same sequence to sequence with attention architecture presented in Section 2. Our model u"
Q18-1017,D17-1209,0,0.031321,"Missing"
Q18-1017,P17-1080,0,0.0302687,"man) and a lowresource (WIT German to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results"
Q18-1017,E17-2026,0,0.438703,"beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks like multiword expression detection and part-of-speech tagging have been found very useful for others like combinatory categorical grammar (CCG) parsing, chunking and super-sense tagging (Bingel and Søgaard, 2017). In order to perform accurate translations, we proceed by analogy to humans. It is desirable to acquire a deep understanding of the languages; and, once this is acquired it is possible to learn how to translate gradually and with experience (including revisiting and re-learning some aspects of the languages). We propose a similar strategy by introducing the concept of Scheduled Multi-Task Learning (Section 4) in which we propose to interleave the different tasks. In this paper, we propose to learn the structure of language (through syntactic parsing and part-ofspeech tagging) with a multi-tas"
Q18-1017,D12-1133,0,0.245372,"4 Although no structural constraints were imposed, our sequence to sequence model is able to obtain a decent parsing result.5 The model achieves 86.99 UAS for English Penn tree-bank with Stanford Dependencies,6 and 80.28 UAS for the German TIGER treebank when the model is only trained to predict the sequence of distances to head as described in Section 3. This is below the best results achieved by state-of-the-art parsers, that are already around 95 for English (Dozat and Manning, 2016; Kuncoro et al., 2017), and around 90 for the same German dataset (Andor et al., 2016; Kuncoro et al., 2016; Bohnet and Nivre, 2012). As a side product of our research, we show that dependency parsing can be approached via a sequence to sequence with an attention mode commonly used for neural machine translation with linearized (using sequences of head distances) dependency trees. Note that, in this case, the models are solely trained on predicting the sequence of distances to the head and are not trained to predict the sequence of dependency labels. For part-of-speech tagging, we use the same sequence to sequence with attention architecture presented in Section 2. Our model uses BiLSTM encodings, in a similar way as propo"
Q18-1017,W16-2301,0,0.408832,"on (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) cons"
Q18-1017,buck-etal-2014-n,0,0.0510001,"Missing"
Q18-1017,2012.eamt-1.60,0,0.0357979,"enized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl. 2 Training: 02-21. Development: 22. Test: 23. 3 German CoNLL 2009 dataset (Hajiˇc et al., 2009). gold annotations for dependency parsing and partof-speech tags. Given the language, we extract three datasets from the relevant tree-bank. We extract parallel corpus of sentences and their gold part-of-speech annotations. The same is done in order to extract a dataset of the unlabeled distances and the dependency labels. Translation In order to simulate low-resource translation tasks, we used 4M tokens of the WIT corpus (Cettolo et al., 2012) for German to English as training data. We used tst2012 for validation and tst2013 for testing, provided by the International Workshop on Spoken Language Translation (IWSLT). Byte-pair encoding is applied, resulting in a vocabulary of 29937 tokens in the source side and 21938 tokens in the target side. For standard translation setting, we use WMT parallel corpus (Buck et al., 2014) with 4.5M sentence pairs (we translate from English to German). We use newstest2013 (3000 sentences) as the development set to select our hyper-parameters, and newstest2014 for testing. Note that we use the same (M"
Q18-1017,N16-1024,1,0.824982,"ne Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) cons"
Q18-1017,K17-1001,0,0.149282,"alchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being ab"
Q18-1017,P17-2012,0,0.035793,"than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks like multiword expression detection and part-of-speech tagging have been"
Q18-1017,Q17-1024,0,0.0929076,"Missing"
Q18-1017,D13-1176,0,0.0295956,"kip@gmail.com Abstract Neural encoder-decoder models of machine translation have achieved impressive results, while learning linguistic knowledge of both the source and target languages in an implicit end-to-end manner. We propose a framework in which our model begins learning syntax and translation interleaved, gradually putting more focus on translation. Using this approach, we achieve considerable improvements in terms of BLEU score on relatively large parallel corpus (WMT14 English to German) and a lowresource (WIT German to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ign"
Q18-1017,Q16-1023,1,0.85956,"eaved, gradually putting more focus on translation. Using this approach, we achieve considerable improvements in terms of BLEU score on relatively large parallel corpus (WMT14 English to German) and a lowresource (WIT German to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddo"
Q18-1017,D16-1180,1,0.837645,"orter/longer sequence.4 Although no structural constraints were imposed, our sequence to sequence model is able to obtain a decent parsing result.5 The model achieves 86.99 UAS for English Penn tree-bank with Stanford Dependencies,6 and 80.28 UAS for the German TIGER treebank when the model is only trained to predict the sequence of distances to head as described in Section 3. This is below the best results achieved by state-of-the-art parsers, that are already around 95 for English (Dozat and Manning, 2016; Kuncoro et al., 2017), and around 90 for the same German dataset (Andor et al., 2016; Kuncoro et al., 2016; Bohnet and Nivre, 2012). As a side product of our research, we show that dependency parsing can be approached via a sequence to sequence with an attention mode commonly used for neural machine translation with linearized (using sequences of head distances) dependency trees. Note that, in this case, the models are solely trained on predicting the sequence of distances to the head and are not trained to predict the sequence of dependency labels. For part-of-speech tagging, we use the same sequence to sequence with attention architecture presented in Section 2. Our model uses BiLSTM encodings,"
Q18-1017,E17-1117,1,0.927436,"tractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks"
Q18-1017,W07-0734,0,0.0336184,"n order to show how each of the systems performed. We include Google web8 system to see a comparison with a state-of-the-art system that is probably trained with more data. Note that in the low-resource data we only have 300k sentence pairs. We selected the output of the systems with highest score in each category (NMT Only, NMT+POS with Constant Scheduler, NMT+Parsing and NMT+POS+Parsing with Exponent Scheduler). Given that the examples in Table 2 suggest that the SMTL models may be doing a better job at avoiding dropping words we complement our BLEU scores with the METEOR evaluation metric (Lavie and Agarwal, 2007) which is more sensitive to recall. We report METEOR (and fragmentation penalty that captures how well the system produces the correct order of the words) for the models with highest BLEU scores in each category (NMT Only, 8 https://translate.google.com 232 NMT+POS with Constant Scheduler, NMT+Parsing and NMT+POS+Parsing with Exponent Scheduler). Table 3 shows the results. Models with the higher BLEU scores also produce higher METEOR scores. In addition it is interesting to see that the fragmentation penalty is higher for the NMT Only model; the NMT Only model only produces 19,768 test words ("
Q18-1017,Q16-1037,0,0.0282173,"to English) setup. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently become the stateof-the-art approach to machine translation (Bojar et al., 2016). One of the main advantages of neural approaches is the impressive ability of RNNs to act as feature extractors over the entire input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machin"
Q18-1017,D15-1166,0,0.60299,"t word per timestep, hence, we can decompose the conditional probability as X log p(y|x) = p(yj |y&lt;j , x). (3) j The decoding procedure consists of two main processes: attention and LSTM based decoding. The attention mechanism calculates the weights (αi ) for each source word based on the words translated/decoded so far. The model gives higher weight to words that are more relevant to decode the next word in the sequence. This is based on the words 226 decoded so far represented by the decoder state (dj ), and the encoder representation of the sentence (hi ). Concretely, we use dot attention (Luong et al., 2015b) to calculate the attention weights. More formally, αi is calculated as follows: ei = d&gt; j hi exp(ei ) αi = P . k exp(ek ) (4) (5) A vector representation (cj ) capturing the information relevant to this time-step is computed by a weighted sum of the encoded source vector representations using α values as weights. X cj = αi · hi . (6) i Given the sentence representation produced by the attention mechanism (cj ) and the decoder state capturing the translated words so far (dj ), the model decodes the next word in the output sequence. The decoding is done using a multi-layer perceptron which re"
Q18-1017,J93-2004,0,0.0637975,"m to determine whether other syntactically oriented tasks can improve translation and vice versa. Each task is presented in a sequence to sequence manner (as described in Section 3). A single sequence to sequence with attention model is used to solve all tasks (all the parameters are shared between the different tasks). 5.1 Data We train the byte-pair encoding model (Sennrich et al., 2016) for the translation parallel corpus and apply it to all the data (including non-translation data). Syntax For English, we extract part-of-speech tagging, dependency heads and labels from the Penn tree-bank (Marcus et al., 1993) with Stanford Dependencies2 . For German, we extract them from TIGER tree-bank (Brants et al., 2002).3 Both treebanks are annotated by experts and contain the 1 All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl. 2 Training: 02-21. Development: 22. Test: 23. 3 German CoNLL 2009 dataset (Hajiˇc et al., 2009). gold annotations for dependency parsing and partof-speech tags. Given the language, we extract three datasets from the relevant tree-bank. We extract parallel corpus of sentences and their gold part-of-speech annotations. The same is done in orde"
Q18-1017,E17-1005,0,0.032519,"continue to represent the syntax objective since the auxiliary tasks are less visited but still in use during training. Having embeddings that share the same space enables the model to share information between the tasks, and functions as regularization (Caruana, 1997). The effectiveness of this scheduler is supported by the results (Table 1) showing superior results (on average) on the WIT German to English translation task. Many approaches have been employing multitask learning in order to inject linguistic knowledge with great success (Luong et al., 2015b; Niehues and Cho, 2017; Mart´ınez Alonso and Plank, 2017, 236 inter-alia). The final representation is then adapted to solve multiple tasks, however continuing to finetune on solely the main task might result in better accuracy. The latter resembles the Sigmoid Scheduler which starts with multi-task learning and gradually shifts to fine-tuning. The results (Table 4) support that this approach can further benefit multi-task learning systems since it shows superior results (on average) in the WMT14 English to German translation task, although it is still not more superior than the baseline that does not use MTL. 8 Conclusions and Future Work This pap"
Q18-1017,N18-1130,0,0.0273365,"es from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks like multiword expression detection and part-of-speech tagging have been found very useful for others like combinatory categorical grammar (CCG) parsing, chunking and sup"
Q18-1017,P05-1012,0,0.253793,"Missing"
Q18-1017,W17-3208,0,0.0157023,"o select our hyper-parameters, and newstest2014 for testing. Note that we use the same (MT) development sets to select the hyper-parameters of the syntactically oriented tasks. After byte-pair encoding is applied, it results in a vocabulary of 59937 tokens in the source side and 63680 tokens in the target side. We only used training examples shorter than 60 words per sentence. We also filter out pairs where the target length is more than 1.5x times the source length. 5.2 Training Details We use mini-batching that limits the number of words in the mini-batch instead of the number of sentences (Morishita et al., 2017). We limit the mini-batch size to 5000 words. Based on the scheduler we sample, the dataset to draw training examples from, and add it to the mini-batch until the word limit is reached. In contrast to other approaches (Luong et al., 2015a; Zoph and Knight, 2016), our minibatch is not separated by tasks and often contains examples from multiple tasks. We shuffle each dataset at the beginning of the training, and after the model has been trained on all the source and target pairs belonging to the dataset(s). We use a two layer stacking B I LSTM for the de230 coder, and a single layer B I LSTM fo"
Q18-1017,W17-4708,0,0.264061,"re input (Kiperwasser and Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks like multiword expressi"
Q18-1017,P02-1040,0,0.101742,"tunes the parameters based solely on the main task (resembling pre-training and fine-tuning). Sigmoid Scheduler We assign probability to the queue we focus on using a sigmoid and divide the rest of the probability uniformly between remaining queues (pq (t) = 1+e1−αt ). This approach starts by looking at all tasks (resembling MTL), and it later tunes the parameters based solely on the main task we wish to focus on. 229 5 Experimental Setup We evaluate the effectiveness of our models for a low-resource setting and a standard setting. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002). We report translation quality using tokenized1 BLEU comparable with existing Neural Machine Translation papers. Our experiments are centered around the translation task. We aim to determine whether other syntactically oriented tasks can improve translation and vice versa. Each task is presented in a sequence to sequence manner (as described in Section 3). A single sequence to sequence with attention model is used to solve all tasks (all the parameters are shared between the different tasks). 5.1 Data We train the byte-pair encoding model (Sennrich et al., 2016) for the translation parallel c"
Q18-1017,D17-1035,0,0.025255,"ge. 4 Scheduled Multi-Task Learning In order to produce accurate translations, neural machine translation systems have to learn syntax in order to generate grammatically correct sentences. Furthermore, translation systems have to disambiguate different parts-of-speech on the source side sentence, since a different part-of-speech can result in different translations. There are many sets of parameters able to capture the training data when employing LSTM (RNN) models. This applies to sequence to sequence models with attention. Each set of parameters provides a different level of generalization (Reimers and Gurevych, 2017). As suggested by Dyer (2017), representations learned by the network do not capture the linguistic properties, and they are biased to make overly strong linguistic generalizations. Providing “guidance” to the sequence to sequence network at the beginning focusing it on a representation enriched with linguistic knowledge, such as syntax or part-of-speech tagging, helps it obtain information necessary for converging to a more general solution. We suggest interleaving the learning of the syntax and translation tasks, and gradually decrease the weight of the syntactically oriented tasks (auxiliar"
Q18-1017,D15-1044,0,0.0507356,"ntion mechanism (cj ) and the decoder state capturing the translated words so far (dj ), the model decodes the next word in the output sequence. The decoding is done using a multi-layer perceptron which receives cj and dj and outputs a score for each word in the target vocabulary: 3 1 1 gj = tanh(WDec dj + WAtt cj ) (7) 2 2 uj = tanh(gj + WDec dj + WAtt cj ) (8) p(yj |y&lt;j , x) ≈ sof tmax(Wout uj + bout ). (9) Many Tasks One Sequence to Sequence Sequence to sequence models have been used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al., 2017; Luong et al., 2015a; Zoph and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to sequence model inspired by Ammar et al. (2016). All tasks share a common output vocabulary and generate terms according to (3). We learn multiple tasks simultane"
Q18-1017,W16-2209,0,0.0266909,"nd Goldberg, 2016), rather than focusing on local information. Neural architectures are able to extract linguistic properties from the input sentence in the form of morphology (Belinkov et al., 2017) or syntax (Linzen et al., 2016). Nonetheless, as shown in Dyer et al. (2016) and Dyer (2017), systems that ignore explicit linguistic structures are incorrectly biased and they tend to make overly strong linguistic generalizations. Providing explicit linguistic information (Dyer et al., ∗ Work carried out during summer internship at IBM Research. 2016; Kuncoro et al., 2017; Niehues and Cho, 2017; Sennrich and Haddow, 2016; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Nadejde et al., 2017; Bastings et al., 2017; Matthews et al., 2018) has proven to be beneficial, achieving higher results in language modeling and machine translation. Multi-task learning (MTL) consists of being able to solve synergistic tasks with a single model by jointly training multiple tasks that look alike. The final dense representations of the neural architectures encode the different objectives, and they leverage the information from each task to help the others. For example, tasks like multiword expression detection and part-of-sp"
Q18-1017,P16-1162,0,0.047722,"ported in case-sensitive BLEU (Papineni et al., 2002). We report translation quality using tokenized1 BLEU comparable with existing Neural Machine Translation papers. Our experiments are centered around the translation task. We aim to determine whether other syntactically oriented tasks can improve translation and vice versa. Each task is presented in a sequence to sequence manner (as described in Section 3). A single sequence to sequence with attention model is used to solve all tasks (all the parameters are shared between the different tasks). 5.1 Data We train the byte-pair encoding model (Sennrich et al., 2016) for the translation parallel corpus and apply it to all the data (including non-translation data). Syntax For English, we extract part-of-speech tagging, dependency heads and labels from the Penn tree-bank (Marcus et al., 1993) with Stanford Dependencies2 . For German, we extract them from TIGER tree-bank (Brants et al., 2002).3 Both treebanks are annotated by experts and contain the 1 All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl. 2 Training: 02-21. Development: 22. Test: 23. 3 German CoNLL 2009 dataset (Hajiˇc et al., 2009). gold annotations f"
Q18-1017,P16-2038,0,0.180356,"Missing"
Q18-1017,D17-1175,0,0.0309937,"and the trained models still remember how to perform the auxiliary tasks (part-of-speech tagging and dependency parsing). This means that a key aspect of our models is that they are able to improve the translation accuracy by incorporating syntactically based objectives into the model. Our models report modest dependency parsing and part-ofspeech tagging numbers but they clearly learn to perform the tasks; it is worth noting that there is a lack of constraints related to sequence length and correspondence between input tokens and tags/distances which is needed to achieve good parsing scores (Zhang et al., 2017). We also want to explore another family of schedulers which treats the layers of the neural network differently. For instance, the scheduler can gradually freeze the top LSTM layer of the decoder (by lowering the learning rate), allowing fine-tuning only of the bottom LSTM layer when training for auxiliary tasks. Søgaard and Goldberg (2016) demonstrated the potential of such an approach. Our experiments show that scheduled multi-task learning is very sensitive to the type of scheduler chosen, and many types of schedulers can be explored. We plan to carry out these experiments in the future. A"
Q18-1017,N16-1004,0,0.215656,": 3 1 1 gj = tanh(WDec dj + WAtt cj ) (7) 2 2 uj = tanh(gj + WDec dj + WAtt cj ) (8) p(yj |y&lt;j , x) ≈ sof tmax(Wout uj + bout ). (9) Many Tasks One Sequence to Sequence Sequence to sequence models have been used for many tasks such as: machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), summarization (Rush et al., 2015) and syntax (Vinyals et al., 2015). Several recent works have shown that parameter sharing between multiple sequence to sequence models that aim to solve different tasks may improve the accuracy of the individual tasks (Kaiser et al., 2017; Luong et al., 2015a; Zoph and Knight, 2016; Niehues and Cho, 2017; Bingel and Søgaard, 2017, inter-alia). We apply a simple yet effective approach to learn multiple tasks using a single sequence to sequence model inspired by Ammar et al. (2016). All tasks share a common output vocabulary and generate terms according to (3). We learn multiple tasks simultaneously by prepending a special task embedding vector to the target. The task vector symbolizes the task we are focusing on. The model can root det pobj amod nsubj prep det The brown fox jumped over the fence 2 1 1 -4 -1 1 -2 Figure 1: Illustration of the encoding of an unlabeled pars"
S12-1037,W10-2919,1,0.905449,"Missing"
S12-1037,W10-3110,0,0.179179,"non nobody never nowhere Words with implicit negation cues unpleasant unnatural dislike fearless hopeless illegal nor ... impatient ... have into account these negation cues when analyzing opinionated texts because these words themselves usually appear in affective lexicons with their corresponding polarity values (i.e., impatient, for instance, appears in SentiWordNet with a negative polarity value). In order to detect negation cues, we use a list of predefined negation signals, along with an automatic method for detecting new ones. The list has been extracted from different previous works (Councill et al., 2010; Morante, 2010). This list also includes the most frequent contracted forms (e.g., don’t, didn’t, etc.). The automated method, in turn, is intended for discovering in text new affixal negation cues. To this end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech. Since not all words with such affixes are negation cues, we use semantic information from WordNet concepts and relations to decide. In this way, we retrieve from WordNet the synset that correspond to each word, using WordNet::Sense"
S12-1037,morante-daelemans-2012-conandoyle,0,0.0379688,"gated concepts. The negated event is the property that is 1 2 http://www.clips.ua.ac.be/NeSpNLP2010/ www.inf.u-szeged.hu/rgai/conll2010st/ First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282–287, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics negated by the cue. For instance, in the sentence: [Holmes] did not [say anything], the scope is enclosed in square brackets, the negation cue is underlined and the negated event is shown in bold. More details about the annotation of negation cues, scopes and negated events may be found in (Morante and Daelemans, 2012). The system presented to the shared task is an adaptation of the one published in (Carrillo de Albornoz et al., 2010), whose aim was to detect and process negation in opinionated text in order to improve polarity and intensity classification. When classifying sentiments and opinions it is important to deal with the presence of negations and their effect on the emotional meaning of the text affected by them. Consider the sentence (1) and (2). Sentence (1) expresses a positive opinion, whereas that in sentence (2) the negation word not reverses the polarity of such opinion. (1) I liked this hot"
S12-1037,D08-1075,0,0.150246,"aning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of 282 detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several initiatives such as the Negation and Speculation in Natural Language Processing workshop (NeSp-NLP 2010)1 or the CoNLL-2010 Shared Task2 , which aimed at identifying hedges and their scope in natural language texts. In spite of this, most of the approaches proposed so far deal with negation in a superficial manner. This paper describes our contribution to the *SEM Shared Task 2012 on Resolving the Scope and Focus of Negation. As its name su"
S12-1037,morante-2010-descriptive,0,0.0274564,"re Words with implicit negation cues unpleasant unnatural dislike fearless hopeless illegal nor ... impatient ... have into account these negation cues when analyzing opinionated texts because these words themselves usually appear in affective lexicons with their corresponding polarity values (i.e., impatient, for instance, appears in SentiWordNet with a negative polarity value). In order to detect negation cues, we use a list of predefined negation signals, along with an automatic method for detecting new ones. The list has been extracted from different previous works (Councill et al., 2010; Morante, 2010). This list also includes the most frequent contracted forms (e.g., don’t, didn’t, etc.). The automated method, in turn, is intended for discovering in text new affixal negation cues. To this end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech. Since not all words with such affixes are negation cues, we use semantic information from WordNet concepts and relations to decide. In this way, we retrieve from WordNet the synset that correspond to each word, using WordNet::SenseRelate (Patwardh"
S12-1037,P05-3019,0,0.0279987,"e, 2010). This list also includes the most frequent contracted forms (e.g., don’t, didn’t, etc.). The automated method, in turn, is intended for discovering in text new affixal negation cues. To this end, we first find in the text all words with prefixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suffix -less that present the appropriate part of speech. Since not all words with such affixes are negation cues, we use semantic information from WordNet concepts and relations to decide. In this way, we retrieve from WordNet the synset that correspond to each word, using WordNet::SenseRelate (Patwardhan et al., 2005) to correctly disambiguate the meaning of the word according to its context, along with all its antonym synsets. We next check if, after removing the affix, the word exists in WordNet and belongs to any of the antonym synsets. If so, we consider the original word to be a negation cue (i.e., the word without the affix has the opposite meaning than the lexical item with the affix). Table 1 presents some examples of explicit negation cues and words with implicit negation cues. For space reasons, not all cues are shown. We also consider common spelling errors such as the omission of apostrophes (e"
S12-1037,W10-3111,0,0.0961605,"n arises to get a first approximation to the negation scope, which is later refined using a set of post-processing rules that bound or expand such scope. 1 Introduction Detecting negation is important for many NLP tasks, as it may reverse the meaning of the text affected by it. In information extraction, for instance, it is obviously important to distinguish negated information from affirmative one (Kim and Park, 2006). It may also improve automatic indexing (Mutalik et al., 2001). In sentiment analysis, detecting and dealing with negation is critical, as it may change the polarity of a text (Wiegand et al., 2010). However, research on negation has mainly focused on the biomedical domain, and addressed the problem of 282 detecting if a medical term is negated or not (Chapman et al., 2001), or the scope of different negation signals (Morante et al., 2008). During the last years, the importance of processing negation is gaining recognition by the NLP research community, as evidenced by the success of several initiatives such as the Negation and Speculation in Natural Language Processing workshop (NeSp-NLP 2010)1 or the CoNLL-2010 Shared Task2 , which aimed at identifying hedges and their scope in natural"
S12-1037,S12-1035,0,\N,Missing
S12-1038,W09-1105,0,0.182193,"Missing"
S12-1038,morante-daelemans-2012-conandoyle,0,0.0390163,"irst algorithm and simple linguistic clause boundaries. An initial version of the system was developed to handle the annotations of the Bioscope corpus. For the present version, we have changed, omitted or extended the rules and the lexicon of cues (allowing prefix and suffix negation cues, such as impossible or meaningless), to make it suitable for the present task. 1 Introduction One of the challenges of the *SEM Shared Task (Morante and Blanco, 2012) is to infer and classify the scope and event associated to negations, given a training and a development corpus based on Conan Doyle stories (Morante and Daelemans, 2012). Negation, simple in concept, is a complex but essential phenomenon in any language. It turns an affirmative statement into a negative one, changing the meaning completely. We believe therefore that being able to handle and classify negations we would be able to improve several text mining applications. Previous to this Shared Task, we can find several systems that handle the scope of negation in the state of the art. This is a complex problem, because it requires, first, to find and capture the negation cues, and second, based on either syntactic or semantic representations, to identify the"
S12-1038,D08-1075,0,0.286551,"Missing"
S12-1038,W08-2121,0,0.0329737,"Missing"
S12-1038,W08-0606,0,0.162026,"008; 2009), in which they presented a machine learning approach for the biomedical domain evaluating it on the Bioscope corpus. In 2010, a Workshop on Negation and Speculation in Natural Language Processing (Morante and Sporleder, 2010) was held in Uppsala, Sweden. Most of the approaches presented worked in the biomedical domain, which is the most studied in negation detection. The system presented in this paper is a modification of the one published in Ballesteros et al. (2012). This system was developed in order to replicate (as far as possible) the annotations given in the Bioscope corpus (Vincze et al., 2008). Therefore, for the one presented in the task we needed to modify most of the rules to make it able to handle the more complex negation structures in the Conan Doyle corpus and the new challenges that it represents. The present paper has the intention of exemplifying the problems of such a system when the task is changed. Our system presented to the Shared Task is based on the following properties: it makes use of an algorithm that traverses dependency structures, it classifies the scope of the negations by using a rule-based approach that studies linguistic clause boundaries and the outcomes"
S12-1038,S12-1035,0,\N,Missing
S18-1003,N18-2107,1,0.891609,", the goal is to alleviate the lack of an associated grammar. This context, which makes it difficult to encode a clear and univocous single meaning for each emoji, has given rise to work considering emojis as function words or even affective markers (Na’aman et al., 2017), potentially affecting the overall semantics of longer utterances like sentences (Monti et al., 2016; Donato and Paggio, 2017). The polysemy of emoji has been explored userwise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), gender-wise, time-wise (Barbieri et al., 2018b; Chen et al., 2017), and even device-wise, due to the fact that emojis may have different pictorial characteristics (and therefore, different interpretations), depending on the device (e.g., Iphone, Android, Samsung, etc.) or app (Whatsapp, Twitter, Facebook, and so forth)3 (Tigwell and Flatla, 2016; Miller et al., 2016). An aspect related with emoji semantic modeling in which awareness is increasing dramatically is the inherent bias existing in these representations. For example, Barbieri and CamachoCollados (2018) show that emoji modifiers can affect the semantics of emojis (they looked sp"
S18-1003,E17-2017,1,0.505123,"iently rich that oversimplifying them to sentiment carriers or boosters would be to neglect the semantic richness of these ideograms, which in addition to mood ( ) include in their vocabulary references to food ( ), sports ( ), scenery ( ), etc2 . In general, however, effectively predicting the emoji associated with a piece of content may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Barbieri et al., 2017; Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). In this context, Barbieri et al. (2017) introduced the task of emoji prediction in Twitter by training several models based on bidirectional Long ShortTerm Memory networks (LSTMs) (Graves, 2012), and showing they can outperform humans in solvThis paper describes the results of the first shared task on Multil"
S18-1003,S18-2011,1,0.899882,"Missing"
S18-1003,L16-1626,1,0.837949,"elements of this shared task (Section 2 and 3). Then, we cover the dataset compilation, curation and release process (Section 4). In Section 5 we detail the evaluation metrics and describe the overall results obtained by participating systems. Finally, we wrap this task description paper up with the main conclusions drawn from the organization of this challenge, as well as outlining potential avenues for future work, in Section 6. 2 Today, modeling emoji semantics via vector representations is a well defined avenue of work. Contributions in this respect include models trained on Twitter data (Barbieri et al., 2016c), Twitter data together with the official unicode description (Eisner et al., 2016), or using text from a popular keyboard app Ai et al. (2017). In the latter contribution it is argued that emojis used in an affective context are more likely to become popular, and in general, the most important factor for an emoji to become popular is to have a clear meaning. In fact, the area of emoji vector evaluation has also experienced a significant growth as of recent. For instance, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. Further studies on evaluating emoji semantics"
S18-1003,S18-1075,0,0.0205523,"do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble ap"
S18-1003,S18-1062,0,0.0222283,"arch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine"
S18-1003,D17-1169,0,0.303851,"the emoji associated with a piece of content may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Barbieri et al., 2017; Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). In this context, Barbieri et al. (2017) introduced the task of emoji prediction in Twitter by training several models based on bidirectional Long ShortTerm Memory networks (LSTMs) (Graves, 2012), and showing they can outperform humans in solvThis paper describes the results of the first shared task on Multilingual Emoji Prediction, organized as part of SemEval 2018. Given the text of a tweet, the task consists of predicting the most likely emoji to be used along such tweet. Two subtasks were proposed, one for English and one for Spanish, and participants were allowed to submit a system run t"
S18-1003,S18-1079,0,0.0244507,", but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competiti"
S18-1003,S18-1067,0,0.0177432,"rse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperfor"
S18-1003,S18-1072,0,0.0393464,"Missing"
S18-1003,S18-1081,0,0.0367034,"Missing"
S18-1003,S18-1004,0,0.0800282,"Missing"
S18-1003,S18-1078,0,0.0516896,"Missing"
S18-1003,S18-1070,0,0.037717,"Missing"
S18-1003,W17-5216,0,0.0205954,"d Work Modeling the semantics of emojis, and their applications thereof, is a relatively novel research problem with direct applications in any social media task. By explicitly modeling emojis as selfcontaining semantic units, the goal is to alleviate the lack of an associated grammar. This context, which makes it difficult to encode a clear and univocous single meaning for each emoji, has given rise to work considering emojis as function words or even affective markers (Na’aman et al., 2017), potentially affecting the overall semantics of longer utterances like sentences (Monti et al., 2016; Donato and Paggio, 2017). The polysemy of emoji has been explored userwise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), gender-wise, time-wise (Barbieri et al., 2018b; Chen et al., 2017), and even device-wise, due to the fact that emojis may have different pictorial characteristics (and therefore, different interpretations), depending on the device (e.g., Iphone, Android, Samsung, etc.) or app (Whatsapp, Twitter, Facebook, and so forth)3 (Tigwell and Flatla, 2016; Miller et al., 2016). An aspect related with emoji semantic modeling in wh"
S18-1003,S18-1077,0,0.021307,"018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infrequent classes are oversampled using the SMOTE algorithm. As for features, they use both unigrams and bigrams. English Emo F1 87.8 37.8 47.1 26.9 55.5 16.2 22.6 36.2 24 22.2 40 64.7 63.7 17.1 13 29.2 14.3 73.6 38.4 9 particularities of each individual language should be taken into consideration for best performance. The most precise systems were EmoNLP and T¨ubingen-Oslo, whereas the highest Recall was obtained by NTUA-SLP a"
S18-1003,E17-2068,0,0.0346041,"is was a single label classification problem, the classic precision (Prec.), recall (Recall), fscore (F1) and accuracy (Acc.) were used as official evaluation metrics. Note that because of the skewed distribution of the label set we opted for macro average over all labels. 5.2 • EmoNLP (Liu, 2018). This system is based on a Gradient Boosting Regression Tree Approach combined with a Bi-LSTM on character and word ngrams. It is complemented with several lexicons as well as learning sentiment specific word embeddings. Baseline The baseline system for this task was a classifier based on FastText6 (Joulin et al., 2017). Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − • UMDuluth-CS8761 (Beaulieu and Asamoah Owusu, 2018) This supervised system combines an SVM with a bag-of-words approach for extracting salient features. This is one of the most competitive systems with the highest precision in English and the third best result in Spanish. n=1 1 X en log(softmax (BAxn )) N N where en is the emoji included in the n-th Twitter post, represented as hot vector, and used as label. Hyperparameters were set as d"
S18-1003,W16-6208,0,0.0910628,"Missing"
S18-1003,S18-1064,0,0.0208509,"default7 . 5.3 • Hatching Chick (Coster et al., 2018). This system builds an SVM classifier (with gradient descent optimization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but"
S18-1003,S18-1059,0,0.0252405,"on, and pretrained word2vec vectors. They used external resources for associating each tweet with information on emotions, concreteness, familiarity, and others. They only participated in the English subtask but they classified second (according to the F1 score) with the highest recall. Evaluation Metrics As this was a single label classification problem, the classic precision (Prec.), recall (Recall), fscore (F1) and accuracy (Acc.) were used as official evaluation metrics. Note that because of the skewed distribution of the label set we opted for macro average over all labels. 5.2 • EmoNLP (Liu, 2018). This system is based on a Gradient Boosting Regression Tree Approach combined with a Bi-LSTM on character and word ngrams. It is complemented with several lexicons as well as learning sentiment specific word embeddings. Baseline The baseline system for this task was a classifier based on FastText6 (Joulin et al., 2017). Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − • UMDuluth-CS8761 (Beaulieu and Asamoah Owusu, 2018) This supervised system combines an SVM with a bag-of-words approach"
S18-1003,S18-1068,0,0.0230149,"with gradient descent optimization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This sys"
S18-1003,S18-1066,0,0.053806,"Missing"
S18-1003,S18-1073,0,0.0252344,"below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infrequent classes are oversampled usi"
S18-1003,S18-1060,0,0.0230182,"is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infreque"
S18-1003,P17-3022,0,0.0972214,"Missing"
S18-1003,S18-1063,0,0.0211944,"om/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve"
S18-1003,S18-1065,0,0.0231871,"timization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM clas"
S18-1003,S18-1076,0,0.0529383,"Missing"
S18-1003,S18-1071,0,0.0142065,"cter ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of f"
W11-2831,W10-4237,0,0.0666307,"Missing"
W11-2831,W06-1513,0,0.0148891,"data will be derived from the CoNLL 09 AnCora corpus. We will process and adapt the treebank to make it useful for the generation task. It is expected that the actual format taken by this data wil depend largely on the insights obtained from the Surface Realization Pilot Task for English currently taking place during 2011. It is worth to emphasize that AnCora contains a wide range of sentence lengths, though most of them are between 20 and 50 wordforms. This provides a good benchmark for a surface realization task, with realizations over a broad range of lengths. Moreover, as it is shown in (Gardent and Kow, 2006) surface realization is exponential in the length of the input. This makes the AnCora corpus very suitable for this proposal. Figure 1 shows the distribution of sentences in the AnCora corpus according to their length. Figure 1: Distribution of sentences in the AnCora corpus according to their length. The x axis represents length and the y axis the approximate number of sentences. We hope to produce two types of input representations, following the guidelines presented in (Belz 1 http://ufal.mff.cuni.cz/conll2009-st/task-description.html 214 et al., 2010), one shallow and one deep. For both sh"
W11-2831,mille-wanner-2010-syntactic,0,0.0216209,"tic labels derived from the AnCora annotation for the CoNLL’09 Shared Task. For the development of the deep representation, we have contacted Simon Mille and Leo Wanner who are trying to refine AnCora’s tagset at Figure 2: Shallow transformation of the following AnCora sentence: Y, en la mesa, se acab´o eso de usar los palillos una sola vez y tirarlos [And, at the table, no more using the toothpicks once and throw them out] the syntactic level (around 60 syntactic tags), and introduce temporary semantic tags in order to facilitate the mapping to the deeper levels (shallow and deep semantics) (Mille and Wanner, 2010). In this way, with their work and the forthcoming version of AnCora (Mariona’s work) we should have a robust corpus that will be suitable for the generation task. 2.2 Evaluation Evaluating surface realization is intrinsically difficult, due to the fact that there is usually no a single correct answer, but rather a range of possible correct answers, some of them better than others. To address this problem, based on the data resources described above we intend to develop evaluation techniques based on Fluency, Clarity and Appropriateness that take this difficulty into account. To this end, outp"
W11-2831,taule-etal-2008-ancora,0,0.089745,"Missing"
W11-2831,W09-1201,0,\N,Missing
W13-3703,C12-1007,0,0.145344,"Missing"
W13-3703,ballesteros-nivre-2012-maltoptimizer-system,1,0.935927,"igurations of morphosyntactic features which would allow for optimizing the parsing of Spanish texts, and to evaluate the impact that each feature has, independently and in combination with others. 1 Introduction As shown in natural language processing (NLP) research, a careful selection of the linguistic information is relevant in order to produce an impact on the results. In this paper, we want to look into different sets of morphosyntactic features in order to test their effect on the quality of parsing for Spanish. To this end, we apply MaltParser (Nivre et al., 2007b), and MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a), which is a system capable of exploring and exploiting the different feature sets that can be extracted from the data and used over the models generated for MaltParser. Starting from a corpus annotated with finegrained language-specific information, we can use all, or a part of the morphosyntactic features to build different models and see the impact of each feature set on the Labeled Attachment Score (henceforth LAS) of the parser. 13 We decided to use MaltOptimizer in order to answer the following questions: (i) is the inclusion of all morphological features"
W13-3703,E12-2012,1,0.938995,"igurations of morphosyntactic features which would allow for optimizing the parsing of Spanish texts, and to evaluate the impact that each feature has, independently and in combination with others. 1 Introduction As shown in natural language processing (NLP) research, a careful selection of the linguistic information is relevant in order to produce an impact on the results. In this paper, we want to look into different sets of morphosyntactic features in order to test their effect on the quality of parsing for Spanish. To this end, we apply MaltParser (Nivre et al., 2007b), and MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a), which is a system capable of exploring and exploiting the different feature sets that can be extracted from the data and used over the models generated for MaltParser. Starting from a corpus annotated with finegrained language-specific information, we can use all, or a part of the morphosyntactic features to build different models and see the impact of each feature set on the Labeled Attachment Score (henceforth LAS) of the parser. 13 We decided to use MaltOptimizer in order to answer the following questions: (i) is the inclusion of all morphological features"
W13-3703,W09-3822,0,0.0248559,"yze in depth whether it is useful to incorporate morphological information as independent features. Eryigit et al. (2008) have already contributed to this topic by testing different morphosyntactic combinations and their effect on MaltParser when applied to Turkish: they point out that some features do not make the dependency parser improve (in their case, number and person), and that Labeled and Unlabeled Attachment Scores (LAS/UAS) are unequally impacted by the feature variation (inflectional features affect more the labeled than the unlabeled accuracy). We also find interesting the work of Bengoetxea and Gojenola (2009) and Atutxa et al. (2012), which have respectively tried to include semantic classes and feature propagation between different parsing models, with the intention of improving the parsing results for Basque. However, none of these works made use of MaltOptimizer in their experiments, for the simple reason that it was not available at the time. Spanish may not be as morphologically rich as other languages such as Hebrew, Turkish or Basque, but it involves enough morphological interactions to allow our research to contribute to 14 such important discussion (Tsarfaty et al., 2010). For instance, d"
W13-3703,E12-1009,0,0.0228251,"Missing"
W13-3703,D12-1133,0,0.0332467,"Missing"
W13-3703,W06-2920,0,0.115763,". ] ... (some hidden transitions) L EFT-A RC subj [ ROOT ] { Eso } [ es lo que hicieron . ] R IGHT-A RC 3.1 MaltParser, MalOptimizer and the CoNLL Data Format ROOT subj MaltParser (Nivre et al., 2007b) is a transitionbased dependency parser generator that requires as an input a training set annotated in CoNLL-X data format,2 and provides models capable of producing the dependency parsing of new sentences. MaltParser implements four different transitionbased parsers families and provides high and stable performance (see, e.g., (Mille et al., 2012)). In the CoNLL Shared Tasks in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a), it was one of the best parsers, achieving either the first or the second place for most of the languages. A transition-based parser is based on a state machine over mainly two data structures: (i) a buffer that stores the words to be processed and (ii) a stack that stores the ones that are being processed (see Figure 1 for details). The different transitions are shown in Figure 2; as can be observed, the state machine transitions manage the input words in order to assign dependencies between them. The transition-based parsers implemented in MaltParser use a model learne"
W13-3703,H05-1100,0,0.0239908,"iii) involving gender, number and sometimes person sharing; furthermore, some features are required on some verbs by their syntactic governor, such as a certain type of finiteness (gerund, participle, infinitive, finite) or mood. All those properties are encoded in the tagset used for the annotation of the AnCora-UPF corpus (see (Burga et al., 2011; Mille et al., 2013) for details about how the tagset was designed), so we expect that the presence or absence of one or more of these features in the training corpus will have a clear impact on the quality of the parsing. In this way, the work of (Cowan and Collins, 2005) makes a step exploring how specific morphologic features (encoded as different PoS) affect the parsing results in Spanish. Even though the authors use a constituent-based treebank and not a dependency-based one, they find that number and mood (verbal feature that overlaps our mood and finiteness) are the features that most affect the parser’s behaviour. 3 Experimental Setup Here are the five steps we followed: 1. The corpus was divided into a training set (3263 sentences, 93803 tokens, 28.7 tokens/sentence) and a test set (250 sentences, 7089 tokens, 28.4 tokens/sentence); 2. 82 different ver"
W13-3703,C12-2082,1,0.866977,"Missing"
W13-3703,W13-3724,1,0.934811,"n use all, or a part of the morphosyntactic features to build different models and see the impact of each feature set on the Labeled Attachment Score (henceforth LAS) of the parser. 13 We decided to use MaltOptimizer in order to answer the following questions: (i) is the inclusion of all morphological features found in an annotation useful for Spanish parsing?; (ii) what are the optimal configurations of morphological features?; (iii) can we explain why different features are more or less important for the parser? For this purpose, we used the UPF version of a subsection of the AnCora corpus (Mille et al., 2013) (see also Section 3.2), which includes features such as number, gender, person, mood, tense, finiteness, and coarse- and fine-grained part-ofspeech (PoS). The impact of each feature or combination of features on subsets of dependency relations is also analyzed; for this, a fine-grained annotation of the syntactic layer is preferred since it allows for a more detailed analysis. The version of the AnCora-UPF corpus that we use contains 41 language-specific syntactic tags and thus is perfectly suitable for our task. In the rest of the paper, we situate our goals within the state-of-the-art (Sect"
W13-3703,W03-3017,0,0.0587315,"ure model –the ones included in the default feature model- and the ones selected or rejected by MaltOptimizer. However, our intention is to study the effect of the features included in the FEATS column, and the interaction with the other features is actually the real case scenario. By performing an automatic search of the linguistic annotation with MaltOptimizer, we are sure that all the morphosyntactic annotation included in the FEATS column is studied and tested by MaltOptimizer. After running MaltOptimizer for Phase 1 and Phase 2, the best parser for (all) our data sets is Nivre arc-eager (Nivre, 2003), which behaves as shown in Figure 1; we were therefore ready to run the feature selection implemented in the Phase 3 POSTAG CC CD DT of MaltOptimizer. Furthermore, the experiments performed by MaltOptimizer ensure that our features are tested in the last steps of the optimization process (Ballesteros and Nivre, 2012a). IN JJ NN NP PP RB 3.2 The AnCora-UPF dependency corpus This corpus, presented by Mille et al. (2013), consists of a small section (3513 sentences, 100892 tokens) of the Spanish dependency corpus AnCora (Taul´e et al., 2008). Mille et al. (2009) explain the partially automatic m"
W13-3703,C12-1147,0,0.0275872,"Missing"
W13-3703,W12-5205,0,0.158172,"ecific syntactic tags and thus is perfectly suitable for our task. In the rest of the paper, we situate our goals within the state-of-the-art (Section 2), we describe the experimental setup, i.e. MaltParser, MaltOptimizer, the corpus used and the experiments that we carried out (Section 3), we report and discuss the results of the experiments (Section 4), and finally present the conclusions and some suggestions for further work (Section 5). 2 Motivation and Related Work Other researchers have already applied MaltOptimizer to their datasets, with different objectives in mind. Thus, the work of Seraji et al. (2012) shows that, for Persian, the parser results improve when following the model suggested by the optimizer. Tsarfaty et al. (2012a) work with Hebrew –a morphologically rich language- and incorporate the optimization offered by MaltOptimizer for presenting novel metrics that allow for jointly evaluating syntactic parsing and morphological segmentation. Mambrini and Passarotti (2012) use the opProceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 13–22, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013."
W13-3703,taule-etal-2008-ancora,0,0.153347,"Missing"
W13-3703,J08-3003,0,0.0224605,"12b) present three different parsing challenges, broadly described as: (i) the architectural challenge, which focuses on how and when to introduce morphological segmentation; (ii) the modeling challenge, focused on how and where the morphological information should be encoded; and (iii) the lexical challenge, which faces the question of how to deal with morphological variants of a word that are not included in the corpus. Our work is directly related to the modeling challenge, given that we analyze in depth whether it is useful to incorporate morphological information as independent features. Eryigit et al. (2008) have already contributed to this topic by testing different morphosyntactic combinations and their effect on MaltParser when applied to Turkish: they point out that some features do not make the dependency parser improve (in their case, number and person), and that Labeled and Unlabeled Attachment Scores (LAS/UAS) are unequally impacted by the feature variation (inflectional features affect more the labeled than the unlabeled accuracy). We also find interesting the work of Bengoetxea and Gojenola (2009) and Atutxa et al. (2012), which have respectively tried to include semantic classes and fe"
W13-3703,W10-1401,0,0.0484842,"Missing"
W13-3703,P03-1054,0,0.0111841,"specifies the POSTAG column and can be used in order to improve the parsing; however, it does not work the other way around: the Tree Tagger PoS tags in the FEATS column do not bring any new information to that one already introduced in the POSTAG column, and thus are ignored by MaltOptimizer. Also, MaltOptimizer follows a stepwise procedure, under this scenario it starts with a higher baseline and it is therefore difficult to get improvements during the optimization steps by testing new features, and thus the features are not selected. There is therefore less room for improvement. Klein and Manning (2003) present similar improvements when splitting the IN tag during their experiments on constituency parsing with a PCFG; we can see now that it is probably the case for dependency parsing too. The best configuration for MaltParser and AnCora-UPF corpus is [finiteness gender number spos]. For parsing purposes, then, it seems enough to enrich the morphosyntactic annotation just with these features, at least in the case of Spanish. These features not only work well together, but also very often improve the results when are individually added to any combination of features. On the one hand, there is"
W13-3703,E12-1006,0,0.0750063,"tate-of-the-art (Section 2), we describe the experimental setup, i.e. MaltParser, MaltOptimizer, the corpus used and the experiments that we carried out (Section 3), we report and discuss the results of the experiments (Section 4), and finally present the conclusions and some suggestions for further work (Section 5). 2 Motivation and Related Work Other researchers have already applied MaltOptimizer to their datasets, with different objectives in mind. Thus, the work of Seraji et al. (2012) shows that, for Persian, the parser results improve when following the model suggested by the optimizer. Tsarfaty et al. (2012a) work with Hebrew –a morphologically rich language- and incorporate the optimization offered by MaltOptimizer for presenting novel metrics that allow for jointly evaluating syntactic parsing and morphological segmentation. Mambrini and Passarotti (2012) use the opProceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 13–22, c 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic Prague, August 27–30, 2013. timizer not only to capture the feature model that fits best Ancient Greek, but also to evaluate how the genre used in the trai"
W13-3703,H05-1066,0,0.170482,"Missing"
W13-3703,D08-1059,0,0.0777577,"Missing"
W13-3703,P11-2033,0,0.0625344,"Missing"
W13-3703,J13-1003,0,\N,Missing
W13-3703,J08-4010,0,\N,Missing
W13-3703,D07-1096,0,\N,Missing
W13-4907,C12-1007,0,0.0396962,"Missing"
W13-4907,ballesteros-nivre-2012-maltoptimizer-system,1,0.711014,"Introduction Since the CoNLL Shared Tasks on Syntactic Dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), the number of treebanks and new parsing methods have considerably increased. Thanks to that, it has been observed that parsing morphologically rich languages (henceforth, MRLs) is a challenge because these languages include multiple levels of information that are difficult to classify and, therefore, to parse. This is why there has been recent research in this direction, with for instance a Special Issue in Computational Linguistics (Tsarfaty et al., 2012b). MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a) is a system that is capable of providing optimal settings for training models with MaltParser (Nivre et al., 2006a), a freely available transition-based parser generator. MaltOptimizer, among other things, performs an in-depth feature selection, selecting the attributes that help to achieve better parsing results. In this paper – and in this participation in the Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) – we present an extension of MaltOptimizer that performs a deeper search over the morphological features that are somewhat one"
W13-4907,E12-2012,1,0.678808,"Introduction Since the CoNLL Shared Tasks on Syntactic Dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), the number of treebanks and new parsing methods have considerably increased. Thanks to that, it has been observed that parsing morphologically rich languages (henceforth, MRLs) is a challenge because these languages include multiple levels of information that are difficult to classify and, therefore, to parse. This is why there has been recent research in this direction, with for instance a Special Issue in Computational Linguistics (Tsarfaty et al., 2012b). MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a) is a system that is capable of providing optimal settings for training models with MaltParser (Nivre et al., 2006a), a freely available transition-based parser generator. MaltOptimizer, among other things, performs an in-depth feature selection, selecting the attributes that help to achieve better parsing results. In this paper – and in this participation in the Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) – we present an extension of MaltOptimizer that performs a deeper search over the morphological features that are somewhat one"
W13-4907,W13-3703,1,0.827733,"ltOptimizer shows improvements by adding DEGREE (0.09), GENDER (0.08) and ABBRV (0.06). However, as we can see the improvements for Swedish are actually lower compared to the rest of languages. 6 Related Work There has been some recent research making use of MaltOptimizer. For instance, Seraji et al. (2012) used MaltOptimizer to get optimal models for parsing Persian. Tsarfaty et al. (2012a) worked with MaltOptimizer and Hebrew by including the optimization for presenting new ways of evaluating statistical parsers. Mambrini and Passarotti (2012), Agirre et al. (2012), Padr´o et al. (2013) and Ballesteros et al. (2013) applied MaltOptimizer to test different features of Ancient Greek, Basque and Spanish (the last 2) respectively; however at that time MaltOptimizer did not allow the FEATS column to be divided. Finally, Ballesteros et al. (2012) applied MaltOptimizer for different parsing algorithms that are not included in the downloadable version showing that it is also possible to optimize different parsing algorithms. 7 Conclusions This new MaltOptimizer implementation helps the developers to adapt MaltParser models to new languages in which there is a rich set of features. It shows which features are abl"
W13-4907,W06-2920,0,0.167151,"hm and optimal feature set achieving the best results that it can find for parsers trained with MaltParser. In this paper, we present an extension of MaltOptimizer that explores, one by one and in combination, the features that are geared towards morphology. From our experiments in the context of the Shared Task on Parsing Morphologically Rich Languages, we extract an in-depth study that shows which features are actually useful for transition-based parsing and we provide competitive results, in a fast and simple way. 1 Introduction Since the CoNLL Shared Tasks on Syntactic Dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), the number of treebanks and new parsing methods have considerably increased. Thanks to that, it has been observed that parsing morphologically rich languages (henceforth, MRLs) is a challenge because these languages include multiple levels of information that are difficult to classify and, therefore, to parse. This is why there has been recent research in this direction, with for instance a Special Issue in Computational Linguistics (Tsarfaty et al., 2012b). MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a) is a system that is capable of providin"
W13-4907,W10-1412,0,0.107161,"ON (0.10) provide improvements, but as we can observe a little bit less than the improvements provided in the gold scenario. 5.5 Hebrew For the Hebrew (Sima’an et al., 2001; Tsarfaty, 2013) treebank, unfortunately we did not see a lot of improvements by adding the morphological features. For the gold input, only CPOSTAG (0.08) shows some improvements, while the predicted scenario shows improvements for NUM (0.08) and PER (0.08). It is worth noting that the TED accuracy (Tsarfaty et al., 2011) for the lattices is 0.8305 which is ranked second. This outcome is different from the one obtained by Goldberg and Elhadad (2010), but it is also true that perhaps by selecting a different parsing algorithm it may turn out different, because two parsers may need different features, as shown by Zhang and Nivre (2012). This is why, it would be very interesting to perform new experiments with MaltOptimizer by testing different parsing algorithms included in MaltParser with the Hebrew treebank. 5.6 Hungarian The Hungarian (Vincze et al., 2010) results are also very consistent. During the feature selection phase, MaltOptimizer achieves an improvement of 10 points by the inclusion of morphological features. This also happens"
W13-4907,P09-2056,0,0.0263317,"8 73.40 73.40 69.97 69.97 73.01 73.01 10.45 76.82 77.62 69.08 70.15 79.00 79.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65 3.98 80.61 80.83 75.29 75.63 79.50 80.49 3.13 72.90 72.90 73.21 73.21 75.82 75.82 Table 2: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task on PMRLs in the predicted scenario on the held-out test set for optimization. The columns of this table report the results in the same way as Table 1 but using predicted inputs. al., 2004), specifically its SPMRL 2013 dependency instance, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009), extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For the gold input, the most useful feature is, by far, DASHTAG3 with an improvement of 2 points. CASE is also very useful, as it is for most of the languages, with 0.67 points. Moreover, SUBCAT (0.159) and CAT (0.129) provide improvements as well. In the pred scenario, there is no DASHTAG, and this allows other features to rise, for instance, CASE (0.66), CPOSTAG (0.12), GENDER (0.08), SUBCAT (0.07) and CAT (0.06) provide improvements. Finally it is worth noting that the TED accuracy 3 DASHTAG"
W13-4907,D07-1097,0,0.190109,"Missing"
W13-4907,P81-1022,0,0.671471,"Missing"
W13-4907,nivre-etal-2006-talbanken05,0,0.575554,"arsing methods have considerably increased. Thanks to that, it has been observed that parsing morphologically rich languages (henceforth, MRLs) is a challenge because these languages include multiple levels of information that are difficult to classify and, therefore, to parse. This is why there has been recent research in this direction, with for instance a Special Issue in Computational Linguistics (Tsarfaty et al., 2012b). MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a) is a system that is capable of providing optimal settings for training models with MaltParser (Nivre et al., 2006a), a freely available transition-based parser generator. MaltOptimizer, among other things, performs an in-depth feature selection, selecting the attributes that help to achieve better parsing results. In this paper – and in this participation in the Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) – we present an extension of MaltOptimizer that performs a deeper search over the morphological features that are somewhat one of the keys to parsing MRLs. Instead of lumping all morphosyntactic features together, we define a different field for each individual feature (c"
W13-4907,I13-1123,1,0.860118,"Missing"
W13-4907,W12-5205,0,0.0855631,"tely we did not see a lot of improvements by adding the morphological features. For the gold input, only CPOSTAG (0.08) shows some improvements, while the predicted scenario shows improvements for NUM (0.08) and PER (0.08). It is worth noting that the TED accuracy (Tsarfaty et al., 2011) for the lattices is 0.8305 which is ranked second. This outcome is different from the one obtained by Goldberg and Elhadad (2010), but it is also true that perhaps by selecting a different parsing algorithm it may turn out different, because two parsers may need different features, as shown by Zhang and Nivre (2012). This is why, it would be very interesting to perform new experiments with MaltOptimizer by testing different parsing algorithms included in MaltParser with the Hebrew treebank. 5.6 Hungarian The Hungarian (Vincze et al., 2010) results are also very consistent. During the feature selection phase, MaltOptimizer achieves an improvement of 10 points by the inclusion of morphological features. This also happens in the initial experiments performed with MaltOptimizer (Ballesteros and Nivre, 2012a), by using the Hungarian treebank of the CoNLL 2007 Shared Task. The current Hungarian treebank presen"
W13-4907,D11-1036,0,0.213682,"ature is, by far, DASHTAG3 with an improvement of 2 points. CASE is also very useful, as it is for most of the languages, with 0.67 points. Moreover, SUBCAT (0.159) and CAT (0.129) provide improvements as well. In the pred scenario, there is no DASHTAG, and this allows other features to rise, for instance, CASE (0.66), CPOSTAG (0.12), GENDER (0.08), SUBCAT (0.07) and CAT (0.06) provide improvements. Finally it is worth noting that the TED accuracy 3 DASHTAG comes from the original constituent data, when a DASHTAG was present in a head node label, this feature was kept in the Catib corpus. 66 (Tsarfaty et al., 2011) for the lattices is 0.8674 with the full treebanks and 0.8563 with 5k treebanks, which overcomes the baseline in more than 0.06 points, this shows that MaltOptimizer is also useful under TED evaluation constraints. 5.2 Basque The improvement provided by the feature selection for Basque (Aduriz et al., 2003) is really high. It achieves almost 13 points improvement with the gold input and around 8 points with the predicted input. The results in the gold scenario are actually a record if we also consider the experiments performed over the treebanks of the CoNLL Shared Tasks (Ballesteros and Nivr"
W13-4907,E12-1006,0,0.131494,"ve results, in a fast and simple way. 1 Introduction Since the CoNLL Shared Tasks on Syntactic Dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), the number of treebanks and new parsing methods have considerably increased. Thanks to that, it has been observed that parsing morphologically rich languages (henceforth, MRLs) is a challenge because these languages include multiple levels of information that are difficult to classify and, therefore, to parse. This is why there has been recent research in this direction, with for instance a Special Issue in Computational Linguistics (Tsarfaty et al., 2012b). MaltOptimizer (Ballesteros and Nivre, 2012b; Ballesteros and Nivre, 2012a) is a system that is capable of providing optimal settings for training models with MaltParser (Nivre et al., 2006a), a freely available transition-based parser generator. MaltOptimizer, among other things, performs an in-depth feature selection, selecting the attributes that help to achieve better parsing results. In this paper – and in this participation in the Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013) – we present an extension of MaltOptimizer that performs a deeper search over th"
W13-4907,P13-2103,0,0.0344317,"nder] (0.3). 5.4 German For German (Brants et al., 2002) the results are more or less in the average. For the gold input, LEMMA is the best feature providing around 0.8 points; from the morphological features the most useful one is, as expected, CASE with 0.58 points. GENDER (0.16) and NUMBER (0.16) are also useful. In the predicted scenario, CASE is again very useful (0.67). Other features, such as, NUMBER (0.10) and PERSON (0.10) provide improvements, but as we can observe a little bit less than the improvements provided in the gold scenario. 5.5 Hebrew For the Hebrew (Sima’an et al., 2001; Tsarfaty, 2013) treebank, unfortunately we did not see a lot of improvements by adding the morphological features. For the gold input, only CPOSTAG (0.08) shows some improvements, while the predicted scenario shows improvements for NUM (0.08) and PER (0.08). It is worth noting that the TED accuracy (Tsarfaty et al., 2011) for the lattices is 0.8305 which is ranked second. This outcome is different from the one obtained by Goldberg and Elhadad (2010), but it is also true that perhaps by selecting a different parsing algorithm it may turn out different, because two parsers may need different features, as shown"
W13-4907,C12-2136,0,0.017362,"ebank, unfortunately we did not see a lot of improvements by adding the morphological features. For the gold input, only CPOSTAG (0.08) shows some improvements, while the predicted scenario shows improvements for NUM (0.08) and PER (0.08). It is worth noting that the TED accuracy (Tsarfaty et al., 2011) for the lattices is 0.8305 which is ranked second. This outcome is different from the one obtained by Goldberg and Elhadad (2010), but it is also true that perhaps by selecting a different parsing algorithm it may turn out different, because two parsers may need different features, as shown by Zhang and Nivre (2012). This is why, it would be very interesting to perform new experiments with MaltOptimizer by testing different parsing algorithms included in MaltParser with the Hebrew treebank. 5.6 Hungarian The Hungarian (Vincze et al., 2010) results are also very consistent. During the feature selection phase, MaltOptimizer achieves an improvement of 10 points by the inclusion of morphological features. This also happens in the initial experiments performed with MaltOptimizer (Ballesteros and Nivre, 2012a), by using the Hungarian treebank of the CoNLL 2007 Shared Task. The current Hungarian treebank presen"
W13-4907,nivre-etal-2006-maltparser,0,\N,Missing
W13-4907,J13-1003,0,\N,Missing
W13-4907,W13-4917,0,\N,Missing
W13-4907,vincze-etal-2010-hungarian,0,\N,Missing
W13-4907,D07-1096,0,\N,Missing
W13-4907,L12-1000,0,\N,Missing
W14-4416,W02-2103,0,\N,Missing
W14-4416,C10-1012,1,\N,Missing
W14-4416,N01-1001,0,\N,Missing
W14-4416,D08-1019,0,\N,Missing
W14-4416,W08-2102,0,\N,Missing
W14-4416,C00-1007,0,\N,Missing
W14-4416,C10-3009,0,\N,Missing
W14-4416,W11-2832,0,\N,Missing
W14-4416,D08-1008,0,\N,Missing
W14-4416,W13-3724,1,\N,Missing
W14-4416,W02-2105,0,\N,Missing
W17-4402,K16-1017,0,0.0322971,"Missing"
W17-4402,L16-1626,1,0.847409,"’, and thus we can conclude that both emotes are used interchangeably. 7 Related Work The most similar communicative phenomena to emotes are emojis. Emojis are used by the vast majority of Social Media services and instant messaging platforms (Jibril and Abdullah, 2013; Park et al., 2013, 2014). Emojis (like the older emoticons) give the possibility to express a variety of ideas and feelings in a visual, concise and appealing way that is perfectly suited for the informal style of Social Media. Several recent works studied Emojis, focusing on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Eisner et al., 2016; Ljubesic and Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique"
W17-4402,E14-3007,1,0.858781,"t on the game that is being played, make an out-of-context joke, or discuss an unrelated event like a football game. 2.1 The kappa emote as a trolling indicator 3.2 Predicting Twitch Emotes Trolling Detection The availability and general usage of the ‘kappa’ emote enables a potential test bed for performing experiments on detecting troll messages in Twitch chatrooms. We approach this task under the assumption that adding ‘kappa’ at the end of a message has a similar effect as it would be to add #irony or #sarcasm at the end of a Twitter message (see (Davidov et al., 2010; Reyes et al., 2013b; Barbieri and Saggion, 2014) for extensive research on irony and sarcasm detection in Twitter under this assumption). Thus, for the trolling prediction experiments, we benefit from this particularity and construct an evaluation dataset where messages are split by considering presence or absence of this emote. In an additional experiment, we further investigate the properties of derivations Twitch Emotes Twitch messages can be enhanced with Twitch emotes, “small pictorial glyphs that fans pepper into text”1 . These emotes range from the more regular smiley faces, to others such as game-specific, channel-specific, or even"
W17-4402,P11-2102,0,0.0201227,"of the B-LSTM model. We chose two common algorithms for text classification, which unlike 4 These games are: Counter Strike: Global Offensive, Dota 2, Hearthstone: Heroes of Warcraft, League of Legends and World of Warcraft. 5 LSTM hidden states are of size 100, and each LSTM has two layers. 13 Model Majority BOW Vec-AVG B-LSTM LSTMs, do not take into account the entire sequence of words. 5.2.1 Bag of Words We designed a Bag-of-Words (Bow) classifier as such model has been successfully employed in several classification tasks, like sentiment analysis and irony detection (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Reyes et al., 2013a). We represent each message with a vector of the most informative tokens (punctuation marks are included as well). Words are selected using term frequency-inverse document frequency (TF-IDF), which is intended to reflect how important a word is to a document (message) in the corpus. After obtaining a vector for each message we classify with a L2-regularized logistic regression classifier to make the predictions6 with ε equal to 0.001. F1 0.10 0.29 0.32 0.39 each emote, along with their Ranking and occurrences in the test set. The Ranking is the average number of emotes wi"
W17-4402,W10-2914,0,0.0640622,"In a very short time span, users may comment on the game that is being played, make an out-of-context joke, or discuss an unrelated event like a football game. 2.1 The kappa emote as a trolling indicator 3.2 Predicting Twitch Emotes Trolling Detection The availability and general usage of the ‘kappa’ emote enables a potential test bed for performing experiments on detecting troll messages in Twitch chatrooms. We approach this task under the assumption that adding ‘kappa’ at the end of a message has a similar effect as it would be to add #irony or #sarcasm at the end of a Twitter message (see (Davidov et al., 2010; Reyes et al., 2013b; Barbieri and Saggion, 2014) for extensive research on irony and sarcasm detection in Twitter under this assumption). Thus, for the trolling prediction experiments, we benefit from this particularity and construct an evaluation dataset where messages are split by considering presence or absence of this emote. In an additional experiment, we further investigate the properties of derivations Twitch Emotes Twitch messages can be enhanced with Twitch emotes, “small pictorial glyphs that fans pepper into text”1 . These emotes range from the more regular smiley faces, to others"
W17-4402,W16-5001,0,0.0303422,"Missing"
W17-4402,P16-2044,0,0.0262326,"ges in total. Messages were randomly selected to avoid topic bias. The second dataset (Multi Kappa dataset) is composed of 100,000 messages per game that contain ‘kappa’ emotes, hence a total of 500,000 messages. Due to the similarity of some emotes to ‘kappa’ we considered five different emotes as ‘kappa’, namely ‘kappa’, ‘kappapride’, ‘keepo’, ‘kappaross’ and ‘kappaclaus’. 5.1 Bi-Directional LSTMs Given the proven effectiveness of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Bahdanau et al., 2014, interalia), which also includes modeling of tweets (Dhingra et al., 2016; Barbieri et al., 2017), our Emote prediction model is based on RNNs, which are modeled to learn sequential data. We use the word based B-LSTM architecture by Barbieri et al. (2017), designed to model emojis in Twitter. The forward LSTM reads the message from left to right and the backward one reads the message in the reverse direction.5 The learned vector of each LSTM, is passed through a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011); finally, an affine transformation of these learned vectors is passed to a softmax layer to give a distribution over the list o"
W17-4402,W16-6208,0,0.0222945,"that both emotes are used interchangeably. 7 Related Work The most similar communicative phenomena to emotes are emojis. Emojis are used by the vast majority of Social Media services and instant messaging platforms (Jibril and Abdullah, 2013; Park et al., 2013, 2014). Emojis (like the older emoticons) give the possibility to express a variety of ideas and feelings in a visual, concise and appealing way that is perfectly suited for the informal style of Social Media. Several recent works studied Emojis, focusing on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Eisner et al., 2016; Ljubesic and Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique was later validated by var"
W17-4402,P15-2124,0,0.0466985,"Missing"
W17-4402,W16-0425,0,0.0252767,"Missing"
W17-4402,P11-2008,0,0.11067,"Missing"
W17-4402,N15-1142,0,0.0359483,"Missing"
W17-4402,W16-2610,0,0.0311034,"used interchangeably. 7 Related Work The most similar communicative phenomena to emotes are emojis. Emojis are used by the vast majority of Social Media services and instant messaging platforms (Jibril and Abdullah, 2013; Park et al., 2013, 2014). Emojis (like the older emoticons) give the possibility to express a variety of ideas and feelings in a visual, concise and appealing way that is perfectly suited for the informal style of Social Media. Several recent works studied Emojis, focusing on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Eisner et al., 2016; Ljubesic and Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique was later validated by various studies (Wang, 2013;"
W17-4402,S16-1003,0,0.0570629,"Missing"
W17-4402,Y13-1035,0,0.0338972,"Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique was later validated by various studies (Wang, 2013; Sulis et al., 2016), which analyze the language associated to the use of irony-related hashtags (such as #irony, and #not). Recent years have seen an increase in models for detecting #irony and #sarcasm. Many of these models adopted hand crafted features (amoung others (Reyes et al., 2013a; Barbieri and Saggion, 2014; Liu et al., 2014; Joshi et al., 2015)), and others employed pretrained word embeddings or deep learning systems such as CNN or LSTMs (Joshi et al., 2016; Ghosh and Veale, 2016; Poria et al., 2016; Amir et al., 2016). 8 We thank the three anonymous reviewers for their time and t"
W17-4402,C16-1151,0,0.0341361,"Missing"
W17-6316,W08-2102,1,0.853407,"Missing"
W17-6316,P16-1231,0,0.0177774,"nal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tr"
W17-6316,P16-2006,0,0.231347,"Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inherent in head-driven model"
W17-6316,K15-1029,1,0.924659,"d parameter matrix, s represents the token in the stack (and its partial spine, if non-terminals have been added to it) and n represents the non-terminal symbol that we are adding to s; b is a bias term. As shown by Kuncoro et al. (2017) composition is an essential component in this kind of parsing models. 5 Related Work Collins (1997) first proposed head-driven derivations for constituent parsing, which is the key idea for spinal parsing, and later Carreras et al. (2008) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to co"
W17-6316,D16-1001,0,0.112913,"Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inherent in head-driven model"
W17-6316,J17-2002,1,0.819626,"ed on the same architecture, with the addition of the node(n) action. The state of our algorithm presented in Section 3 is represented by the contents of the STACK, the BUFFER and a list with the history of actions with Stack-LSTMs. This state representation is then used to predict the next action to take. Composition: when the parser predicts a left-arc() or right-arc(), we compose the vector representation of the head and dependent elements; this is equivalent to what it is presented by Dyer et al. (2015). The 1 Set to 10 in our experiments We refer interested readers to (Dyer et al., 2015; Ballesteros et al., 2017). 2 117 Leftmost heads Leftmost h., no n-comp Rightmost heads Rightmost h., no n-comp SD heads SD heads, no n-comp SD heads, dummy spines YM heads LR 91.18 90.20 91.03 90.64 90.75 90.38 90.82 LP 90.93 90.76 91.20 91.24 91.11 90.58 90.84 F1 UAS (SD) 91.05 90.48 91.11 90.04 90.93 93.49 90.48 93.16 93.30 90.83 - identities (right or left) work better than those using linguistic ones. This suggests that the StackLSTM model already finds useful head-child relations in a constituent by parsing from the left (or right) even if there are non-local interactions. In this case, head rules are not useful."
W17-6316,de-marneffe-etal-2006-generating,0,0.114196,"Missing"
W17-6316,D16-1211,1,0.853748,"3 present results on the test, for constituent and dependency parsing respectively. As shown in Table 2 our model is competitive compared to the best parsers; the generative parsers by Choe and Charniak (2016b), Dyer et al. (2016) and Kuncoro et al. (2017) are better than the rest, but compared to the rest our parser is at the same level or better. The most similar system is by Ballesteros and Carreras (2015) and our parser significantly improves the performance. Considering dependency parsing, our model is worse than the ones that train with exploration as Kiperwasser and Goldberg (2016) and Ballesteros et al. (2016), but it slightly improves the parser by Dyer et al. (2015) with static training. The systems that calculate dependencies by transforming phrase-structures with conversion rules and that use generative training are ahead compared to the rest. Table 1: Development results for spinal models, in terms of labeled precision (LP), recall (LR) and F1 for constituents, and unlabeled attachment score (UAS) against Stanford dependencies. Spinal models are trained using different head annotations (see text). Models labeled with “no ncomp” do not use node compositions. The model labeled with “dummy spines"
W17-6316,P04-1013,0,0.050495,"eads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together"
W17-6316,N16-1024,1,0.927541,"relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This representation is inhere"
W17-6316,E17-1117,1,0.889306,"Missing"
W17-6316,P15-1147,0,0.0424669,"Missing"
W17-6316,D16-1180,1,0.888238,"Missing"
W17-6316,Q17-1004,0,0.0362201,"Missing"
W17-6316,J93-2004,0,0.0607348,"Missing"
W17-6316,P16-2016,0,0.0155036,"; b is a bias term. As shown by Kuncoro et al. (2017) composition is an essential component in this kind of parsing models. 5 Related Work Collins (1997) first proposed head-driven derivations for constituent parsing, which is the key idea for spinal parsing, and later Carreras et al. (2008) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LS"
W17-6316,W04-0308,0,0.0385643,"built. Spinal nodes are noted nji , where n is the non-terminal, i is the position of the head token, and j is the node level in the spine. s PP NP s Buffer β [And, their, . . . ] [their, suspicions, . . . ] [suspicions, of, . . . ] [of, each, . . . ] [of, each, . . . ] [of, each, . . . ] [of, each, . . . ] [each, other, . . . ] [each, other, . . . ] [other, run, . . . ] [run, deep, . . . ] [run, deep, . . . ] [run, deep, . . . ] [run, deep, . . . ] [run, deep, . . . ] Arc-Standard Spinal Parsing We use the transition system by Cross and Huang (2016a), which extends the arc-standard system by Nivre (2004) for constituency parsing in a headdriven way, i.e. spinal parsing. We describe it here for completeness. The parsing state is a tuple hβ, σ, δi, where β is a buffer of input tokens to be processed; σ is a stack of tokens with partial spines; and δ is a set of spinal dependencies. The operations are the following: • right-arc : hβ, σ:s+n:t, δi → hβ, σ:s+n, δ ∪(n, t)i This operation is symmetric to left-arc, it adds a spinal dependency from the top node n of the second spine in the stack to the top element t, which is reduced from the stack and becomes the rightmost child of n. At a high level,"
W17-6316,J08-4003,0,0.233926,"g a node above, or by reducing the spine with an arc operation with this spine as dependent). By this 116 Figure 2 shows an example of a derivation. The process is initialized with all sentence tokens in the buffer, an empty stack, and an empty set of dependencies. Termination is always attainable and occurs when the buffer is empty and there is a single element in the stack, namely the spine of the full sentence head. This transition system is correct and sound with respect to the class of projective spinal trees, in the same way as the arc-standard system is for projective dependency trees (Nivre, 2008). A derivation has 2n + m steps, where n is the sentence length and m is the number of constituents in the derivation. We note that the system naturally handles constituents of arbitrary arity. In particular, unary productions add one node in the spine without any children. In practice we put a hard bound on the number of consecutive unary productions in a spine1 , to ensure that in the early training steps the model does not generate unreasonably long spines. We also note there is a certain degree of non-determinism: left and right arcs (steps 3 and 4) can be mixed as long as the children of"
W17-6316,N03-1014,0,0.0397213,"reras et al. (2008) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions. Finally, dependency parsers have been extended to Spinal Stack-LSTMs Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems th"
W17-6316,W05-1513,0,0.0473611,"8) came up with a higher-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions. Finally, dependency parsers have been extended to Spinal Stack-LSTMs Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems that maintains an embeddi"
W17-6316,W07-2218,0,0.0303993,"thod is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system"
W17-6316,P15-1113,0,0.0785455,"using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operations to parse in linear time, and rely on the ability of neural networks to infer and propagate hidden structure through the derivation. This contrasts with state-of-the-art factored linear models, which explicitly use of higher-order information to capture non-local phenomena in a derivation. In this paper, we present a transition system for parsing sentences into spinal trees, a type of syntactic tree that explicitly represents together dependency and constituency structure. This repre"
W17-6316,W03-3023,0,0.456891,"tituent nodes with dependency-based derivations. In experiments, we show that this model adapts to different styles of dependency relations, but this choice has little effect for predicting constituent structure, suggesting that LSTMs induce useful states by themselves. 1 In experiments on the Penn Treebank, we look at how sensitive our method is to different styles of dependency relations, and show that spinal models based on leftmost or rightmost heads are as good or better than models using linguistic dependency relations such as Stanford Dependencies (De Marneffe et al., 2006) or those by Yamada and Matsumoto (2003). This suggests that Stack-LSTMs figure out effective ways of modeling non-local phenomena within constituents. We also show that turning a dependency Stack-LSTM into spinal results in some improvements. Introduction There is a clear trend in neural transition systems for parsing sentences into dependency trees (Titov and Henderson, 2007; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016) and constituent trees (Henderson, 2004; Vinyals et al., 2014; Watanabe and Sumita, 2015; Dyer et al., 2016; Cross and Huang, 2016b). These transition systems use a relatively simple set of operati"
W17-6316,P13-1043,0,0.0217284,"r-order graph-based parser for this representation. Transition systems for spinal parsing are not new. Ballesteros and Carreras (2015) presented an arc-eager system that labels dependencies with constituent nodes, and builds the spinal tree in post-processing. Hayashi et al. (2016) and Hayashi and Nagata (2016) presented a bottom-up arc-standard system that assigns a full spine with the shift operation, while ours builds spines incrementally and does not depend on a fixed set of full spines. Our method is different from shift-reduce constituent parsers (Henderson, 2003; Sagae and Lavie, 2005; Zhu et al., 2013; Watanabe and Sumita, 2015) in that it is head-driven. Cross and Huang (2016a) extended the arc-standard system to constituency parsing, which in fact corresponds to spinal parsing. The main difference from that work relies on the neural model: they use sequential BiLSTMs, while we use Stack-LSTMs and composition functions. Finally, dependency parsers have been extended to Spinal Stack-LSTMs Dyer et al. (2015) presented an arc-standard parser that uses Stack-LSTMs, an extension of LSTMs (Hochreiter and Schmidhuber, 1997) for transition-based systems that maintains an embedding for each elemen"
