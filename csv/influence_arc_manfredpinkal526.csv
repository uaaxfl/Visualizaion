burchardt-etal-2006-salsa,erk-pado-2006-shalmaneser,1,\N,Missing
burchardt-etal-2006-salsa,burchardt-etal-2006-salto,1,\N,Missing
burchardt-etal-2006-salsa,fliedner-2006-towards,0,\N,Missing
burchardt-etal-2006-salsa,W04-2703,0,\N,Missing
burchardt-etal-2006-salsa,H05-1047,0,\N,Missing
burchardt-etal-2006-salsa,P98-1013,0,\N,Missing
burchardt-etal-2006-salsa,C98-1013,0,\N,Missing
burchardt-etal-2006-salsa,J02-3001,0,\N,Missing
burchardt-etal-2006-salsa,J05-1004,0,\N,Missing
burchardt-etal-2006-salsa,erk-pado-2004-powerful,1,\N,Missing
burchardt-etal-2006-salsa,W04-1906,1,\N,Missing
C12-2033,E09-1005,0,0.108167,"Missing"
C12-2033,D09-1046,0,0.406595,"s on the level of single instances. Erk et al. (2009) give the example of “paper” occurring in a sentence which clearly identifies a scientific context. All three annotators agree that the WordNet sense scholarly article fully applies and consistently assign a score of 5. However, the senses essay and medium for written communication are also assigned high scores by some of the annotators. This reflects these annotators’ intuitions that several senses apply simultaneously, and induces an ordering of the senses’ applicabilities. A first, supervised, computational model for GWSA is presented by Erk and McCarthy (2009). In this paper, we explore models that are unsupervised in the sense that they do not depend on annotated training material; in the WSD terminology, they belong to the class of knowledgebased WSD systems. More specifically, we address the task of ranking the WordNet senses of a lemma for each of its instances, according to the degree of applicability of the respective senses in context. We evaluate our models against the data sets provided by Erk et al. (2009, 2012), and use the ranking induced by the average scores for each word sense as a gold standard. We carry out the evaluation for three"
C12-2033,P09-1002,0,0.349553,"cular in connection with fine-grained sense inventories, like the one provided by WordNet (Fellbaum, 1998). The single-sense restriction typically leads to a somewhat arbitrary overspecification of word meaning, which may be detrimental to the use of WSD systems in practical applications. Moreover, both agreement between human annotators and accuracy of WSD systems tend to be rather low, which stands in contrast to the strong intuition that words in context generally have a well-understood meaning. Recently, the notion of graded word sense assignment (GWSA) has been brought into discussion by Erk et al. (2009, 2012), and two closely related GWSA data sets are now available. The underlying assumption of GWSA is that a word in context may in fact evoke more than one sense, and the different senses may participate in the meaning of the word to different degrees. To produce the aforementioned data sets, annotators were presented target instances, i.e., lemmas in the context of a sentence, and asked to assign a value, which indicates the applicability of the sense in the context, on a scale from 1 to 5 to each WordNet sense of the lemma independently. The annotation method allows more than one word sen"
C12-2033,D08-1094,0,0.0300232,"pus, only a very small amount of data is left for evaluation. 3 Modeling This section reviews the three knowledge-based WSD algorithms that we use in our study, and which we chose for the following reasons: (1) They are knowledge-lean, i.e., the only resource required is a semantic lexicon (such as WordNet), and they can be implemented quickly. (2) They exhibit state-of-the-art performance on the SemEval-2007 coarse-grained WSD task. 3.1 Vector Space-based WSD System We use the vector-space model (VSM) of Thater et al. (2011), which is closely related to the models of Thater et al. (2010) and Erk and Padó (2008). The general idea behind VSMs of word meaning is to represent words by vectors in a high-dimensional space. These vectors record co-occurrence statistics with context words in a large unlabeled text corpus, and their relative directions are taken to indicate semantic similarity. The particular model used in our experiments is the one of Thater et al. (2011), which provides context-specific (contextualized) vectors for words in their syntactic context. It can be applied to WSD and GWSA in a straightforward way: given a target word in a sentential context, we extract a set of sense paraphrases"
C12-2033,S12-1027,0,0.0205915,"n judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater et al. (2010) describe an approach to unsupervised GWSA on the basis of a syntactically informed distributional similarity 330 model. The evaluation was carried out for three selected verb lemmas, and therefore has the character of a case study only. The study of Jurgens (2012), which explores the application of word sense induction techniques to GWSA, has a similar status: Since he needs a large part of the GWSA data set as a sense mapping corpus, only a very small amount of data is left for evaluation. 3 Modeling This section reviews the three knowledge-based WSD algorithms that we use in our study, and which we chose for the following reasons: (1) They are knowledge-lean, i.e., the only resource required is a semantic lexicon (such as WordNet), and they can be implemented quickly. (2) They exhibit state-of-the-art performance on the SemEval-2007 coarse-grained WS"
C12-2033,P10-1116,0,0.0308291,"e WordNet senses of a lemma for each of its instances, according to the degree of applicability of the respective senses in context. We evaluate our models against the data sets provided by Erk et al. (2009, 2012), and use the ranking induced by the average scores for each word sense as a gold standard. We carry out the evaluation for three different systems: two related models, which are based on the individual similarity scores between the contextualized vector representation of a target word in context and vector representations computed for the respective word senses (Thater et al., 2011; Li et al., 2010), plus a reimplementation of the approach of Sinha and Mihalcea (2007), a representative of the larger class of graph-based approaches to WSD. Our major findings are first, that the knowledge-based systems show positive correlation with the human judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater et al. (2010) descr"
C12-2033,N04-3012,0,0.0667701,"valuation of GWSA. A total of 430 sentences for 11 different lemmas were extracted from SemCor and Senseval-3. Three untrained annotators provided judgments of the applicability of word senses of the lemmas in the context of the sentence on a scale from 1 to 5, where 1 means that the sense is not present at all in the sentence and 5 means that the sense totally matches the meaning of the word in the context. We refer to the task of ranking the senses of a word (lemma) in the context of a particular sentence as the lemma-sentence ranking task. 1 We are using the WordNet::Similarity toolkit of (Pedersen et al., 2004). We also experimented with other sense similarity measures, but the method suggested by Sinha and Mihalcea (2007) worked best with PageRank. 332 WSsim-2: In this round of data collection, eight annotators judged the applicability of the senses of 26 lemmas in 10 sentences each, resulting in a set of 260 sentences (Erk et al., 2012). Otherwise, the annotation procedure was identical to WSsim-1. 4.2 Correlation Analysis of Sense Ranking Erk and McCarthy (2009) propose Spearman’s rank correlation coefficient (ρ) as a measure of a system’s performance on the GWSA task. ρ compares two rankings whi"
C12-2033,P10-1154,0,0.0314661,"ce, the Topic Models approach might yield better performance using different parameter settings. We noticed that due to the sampling step inside the algorithm, the results varied by small, but non-negligible, amounts. We thus sum up the scores produced by the system across multiple (ten) runs in order to predict a more reliable ranking. This results in a slight increase of performance. 331 3.3 Graph-based WSD System To date, many graph-based WSD algorithms have been proposed, (among others by Sinha and Mihalcea, 2007; Agirre and Soroa, 2009; Navigli and Lapata, 2010; Tsatsaronis et al., 2010; Ponzetto and Navigli, 2010). We chose to reimplement the approach of Sinha and Mihalcea (2007) for several reasons. First, it is based on the PageRank algorithm, which is easy to understand and implement; second, a reference implementation was made available by the authors, which allowed for clarification in several issues; and third, its performance is robust. The algorithm consists of the following steps, which we illustrate using Figure 1. (1) Construction of the graph. When disambiguating a word (e.g. “order”), a graph is built using a context of N (2 in the example) content words on either side of the word. For eac"
C12-2033,P10-1097,1,0.848537,"l., 2011; Li et al., 2010), plus a reimplementation of the approach of Sinha and Mihalcea (2007), a representative of the larger class of graph-based approaches to WSD. Our major findings are first, that the knowledge-based systems show positive correlation with the human judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater et al. (2010) describe an approach to unsupervised GWSA on the basis of a syntactically informed distributional similarity 330 model. The evaluation was carried out for three selected verb lemmas, and therefore has the character of a case study only. The study of Jurgens (2012), which explores the application of word sense induction techniques to GWSA, has a similar status: Since he needs a large part of the GWSA data set as a sense mapping corpus, only a very small amount of data is left for evaluation. 3 Modeling This section reviews the three knowledge-based WSD algorithms that we use in our study, and"
C12-2033,I11-1127,1,0.911785,"he task of ranking the WordNet senses of a lemma for each of its instances, according to the degree of applicability of the respective senses in context. We evaluate our models against the data sets provided by Erk et al. (2009, 2012), and use the ranking induced by the average scores for each word sense as a gold standard. We carry out the evaluation for three different systems: two related models, which are based on the individual similarity scores between the contextualized vector representation of a target word in context and vector representations computed for the respective word senses (Thater et al., 2011; Li et al., 2010), plus a reimplementation of the approach of Sinha and Mihalcea (2007), a representative of the larger class of graph-based approaches to WSD. Our major findings are first, that the knowledge-based systems show positive correlation with the human judgments, and second, that there are interesting differences in performance between the different types of systems according to our metric of Adjusted Accuracy. 2 Related Work The only WSD system that has been evaluated on the full GWSA data set of Erk et al. (2009) so far is the supervised model of Erk and McCarthy (2009). Thater e"
C12-2033,J13-3003,0,\N,Missing
C86-1088,P83-1007,0,\N,Missing
C96-1024,P91-1021,1,0.751083,"Missing"
C96-1024,C92-1017,0,0.0762618,"onotonic representation language for compositional semantics as discussed in (Alshawi and Crouch, 1992). The QLF formalism incorporates a Davidsonian approach to semantics, containing underspecified quantifiers and operators, as well as 'anaphoric terms' which stand for entities and relations to be determined by reference resolution. In these respects, the basic ideas of the QLF formalism are quite similar to LUD. 5 5.1 Syntax-Semantics Implementation Interface and Grammar The LUD semantic construction component has been implemented in the grammar formalism TUG, Trace and Unification Grammar (Block and Schachtl, 1992), in a system called TrUG (in cooperation with Siemens AG, Munich, who provided the German syntax and the TrUG system). TUG is a formalism that combines ideas from Government and Binding theory, namely the use of traces, with unification in order to account for, for example, the free word order phenomena found in German. 5.1.1 S y n t a x and S e m a n t i c s A TUG grammar basically consists of PATR-II style context free rules with feature annotations. Each syntactic rule gets annotated with a semantic counterpart. In this way, syntactic derivation and semantic construction are fully interlea"
C96-1024,1995.tmi-1.2,0,0.334937,"Missing"
C96-1024,1993.mtsummit-1.11,0,0.0813897,"he interpretation of a category on the right side of a rule subsumes the interpretation of the left side of the nile. lar tasks. The actual implementation is described in Section 5, which also discusses coverage and points to some areas of further research. Finally, Section 6 sums up the previous discussion. 2 The Verbmobil Project The project Verbmobil funded by the German Federal Ministry of Research and Technology (BMBF) combines speech technology with machine translation techniques in order to develop a system for translation in face-to-face dialogues. The overall project is described in (Wahlster, 1993); in this section we will give a short overview of the key aspects. The ambitious overall objective of the Verbmobil project is to produce a device which will provide English translations of dialogues between German and Japanese businessmen who only have a restricted active, but larger passive knowledge of English. The domain is the scheduling of business appointments. The major requirement is to provide translations as and when users need them, and do so robustly and in (near) real-time. In order to achieve this, the system is composed of time-limited processing components which on the source"
C96-1024,P92-1005,0,\N,Missing
D11-1072,P10-1097,1,0.193595,"Missing"
D11-1072,E06-1002,0,\N,Missing
D11-1072,W03-0419,0,\N,Missing
D11-1072,D07-1074,0,\N,Missing
D11-1072,P05-1045,0,\N,Missing
D15-1294,J93-2004,0,0.0559869,"ormations (such as use of the perfect or modals) have been applied. Siegel and McKeown (2000) propose the use of linguistic indicators (explained in Section 5.2); Friedrich and Palmer (2014) show the importance of using context-based features in addition. Zarcone and Lenci (2008) classify occurrences of 28 Italian verbs according to Vendlers classes state, process, accomplishment and achievement. Mathew and Katz (2009) address the problem of ‘supervised categorization for habitual versus 2473 episodic sentences’ . The authors randomly select 1052 sentences for 57 verbs from the Penn TreeBank (Marcus et al., 1993) and manually mark them with regard to whether they are habitual or episodic. They focus on verbs that are lexically dynamic and discuss a variety of syntactic features, which they extract from gold standard parse trees. Their aim is to study the ability of syntactic features alone to identify habitual sentences. Xue and Zhang (2014) annotate verbs with the four event types habitual event, state, on-going event and episodic event with the aim of improving tense prediction for Chinese. Recent related work (Williams, 2012; Williams and Katz, 2012) extracts typical durations (in term of actual ti"
D15-1294,E12-1027,0,0.348378,"als, we apply a three-way distinction for clausal aspect in this work. We classify clauses as one of the three categories habitual, episodic and static.1 Through its impact on entailment properties and temporal discourse structure, the determination of clausal aspect is relevant to various natural language processing applications requiring text understanding, such as novelty detection (Soboroff and Harman, 2005), knowledge extraction from text (Van Durme, 2010) or question answering (Llorens et al., 2015). Using aspectual information has been shown to improve temporal relation identification (Costa and Branco, 2012). Some languages (e.g., Czech or Swahili) have systematic morphological markers of habituality 1 For clarity, we use the label static for the clausal aspect of non-episodic and non-habitual sentences. We reserve stative, which is more common in the literature, for the lexical aspectual class. 2471 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2471–2481, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. (Dahl, 1985). In other languages, there are cues for habituality, such as the simple present in English, a"
D15-1294,P14-2085,1,0.540061,"s well as negated or modalized clauses containing a dynamic main verb. A computational model for identifying episodic and habitual clauses clearly needs to address this third class as well if it is to be applied in a realistic setting. Linguistically, the determination of clausal aspect depends on the recognition of the verb’s lexical aspectual class (stative or dynamic), and on the recognition of any aspectual markers or transformations, such as use of the perfect tense, negations or modals. Our work builds on results for the related subtasks (Mathew and Katz, 2009; Siegel and McKeown, 2000; Friedrich and Palmer, 2014), using both context-based and verbtype based information. Our major contributions are: (i) We create a corpus of 102 Wikipedia texts whose about 10,000 clauses are annotated as episodic, static or habitual with substantial agreement. This corpus allows for studying the range of linguistic phenomena related to the clause types as defined above (compared to previous work which uses only a small set of verbs and sentences), and provides a basis for future research. (ii) We provide the first fully automatic approach for this classification task, combining two classification tasks (lexical aspectu"
D15-1294,S15-2134,0,0.0204978,"to be aspectually stative. Since there are clear differences between ordinary statives such as (1b) and habituals, we apply a three-way distinction for clausal aspect in this work. We classify clauses as one of the three categories habitual, episodic and static.1 Through its impact on entailment properties and temporal discourse structure, the determination of clausal aspect is relevant to various natural language processing applications requiring text understanding, such as novelty detection (Soboroff and Harman, 2005), knowledge extraction from text (Van Durme, 2010) or question answering (Llorens et al., 2015). Using aspectual information has been shown to improve temporal relation identification (Costa and Branco, 2012). Some languages (e.g., Czech or Swahili) have systematic morphological markers of habituality 1 For clarity, we use the label static for the clausal aspect of non-episodic and non-habitual sentences. We reserve stative, which is more common in the literature, for the lexical aspectual class. 2471 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2471–2481, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Lingui"
D15-1294,ruppenhofer-rehbein-2012-yes,0,0.0307157,"hn rarely ate fruit. He just ate oranges. (habitual) In some preliminary experiments, we tried to leverage the discourse context of a clause for its classification by means of incorporating the gold standard label of the previous clause as a feature. This did not result in significant performance improvements. However, further experiments trying to incorporate discourse information are due, and, due to our new corpus of fully annotated texts, now possible. Another related research direction is the classification of the different types of static clauses, e.g., the different senses of modality (Ruppenhofer and Rehbein, 2012). As mentioned before, a finer classification of the temporal structure of clauses is needed, among others identifying the lexical aspectual class as well as viewpoint aspect as perfective vs. imperfective (Smith, 1997). Finally, the next steps in this line of research are to integrate the aspectual information attributed to clauses by our model into models of temporal discourse structure, which in turn are useful for information extraction and text understanding tasks in general. Costa and Branco (2012) are the first to show that aspectual information is relevant here; we hope to show in the"
D15-1294,J00-4004,0,0.34986,"exically stative clauses as well as negated or modalized clauses containing a dynamic main verb. A computational model for identifying episodic and habitual clauses clearly needs to address this third class as well if it is to be applied in a realistic setting. Linguistically, the determination of clausal aspect depends on the recognition of the verb’s lexical aspectual class (stative or dynamic), and on the recognition of any aspectual markers or transformations, such as use of the perfect tense, negations or modals. Our work builds on results for the related subtasks (Mathew and Katz, 2009; Siegel and McKeown, 2000; Friedrich and Palmer, 2014), using both context-based and verbtype based information. Our major contributions are: (i) We create a corpus of 102 Wikipedia texts whose about 10,000 clauses are annotated as episodic, static or habitual with substantial agreement. This corpus allows for studying the range of linguistic phenomena related to the clause types as defined above (compared to previous work which uses only a small set of verbs and sentences), and provides a basis for future research. (ii) We provide the first fully automatic approach for this classification task, combining two classifi"
D15-1294,H05-1014,0,0.0210993,"not move narrative time, similar to stative clauses such as (1b). Carlson (2005) considers habituals to be aspectually stative. Since there are clear differences between ordinary statives such as (1b) and habituals, we apply a three-way distinction for clausal aspect in this work. We classify clauses as one of the three categories habitual, episodic and static.1 Through its impact on entailment properties and temporal discourse structure, the determination of clausal aspect is relevant to various natural language processing applications requiring text understanding, such as novelty detection (Soboroff and Harman, 2005), knowledge extraction from text (Van Durme, 2010) or question answering (Llorens et al., 2015). Using aspectual information has been shown to improve temporal relation identification (Costa and Branco, 2012). Some languages (e.g., Czech or Swahili) have systematic morphological markers of habituality 1 For clarity, we use the label static for the clausal aspect of non-episodic and non-habitual sentences. We reserve stative, which is more common in the literature, for the lexical aspectual class. 2471 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages"
D15-1294,N03-1030,0,0.0606114,"Missing"
D15-1294,J86-2003,0,0.0259354,"Missing"
D15-1294,P12-2044,0,0.0260747,"1052 sentences for 57 verbs from the Penn TreeBank (Marcus et al., 1993) and manually mark them with regard to whether they are habitual or episodic. They focus on verbs that are lexically dynamic and discuss a variety of syntactic features, which they extract from gold standard parse trees. Their aim is to study the ability of syntactic features alone to identify habitual sentences. Xue and Zhang (2014) annotate verbs with the four event types habitual event, state, on-going event and episodic event with the aim of improving tense prediction for Chinese. Recent related work (Williams, 2012; Williams and Katz, 2012) extracts typical durations (in term of actual time measures) for verb lemmas from Twitter. They distinguish episodic and habitual uses of the verbs, using the method of Mathew and Katz (2009). 4 Data In this section, we describe the data sets used in our experiments.3 4.1 Penn TreeBank (M&K) data set Mathew and Katz (2009) randomly select sentences for several verbs from the WSJ and Brown corpus sections of the Penn Treebank. They require the verb to be lexically dynamic. Sentences are marked as habitual or episodic, further details on the annotation guidelines are not specified. Their data s"
D15-1294,W12-3309,0,0.0188226,"randomly select 1052 sentences for 57 verbs from the Penn TreeBank (Marcus et al., 1993) and manually mark them with regard to whether they are habitual or episodic. They focus on verbs that are lexically dynamic and discuss a variety of syntactic features, which they extract from gold standard parse trees. Their aim is to study the ability of syntactic features alone to identify habitual sentences. Xue and Zhang (2014) annotate verbs with the four event types habitual event, state, on-going event and episodic event with the aim of improving tense prediction for Chinese. Recent related work (Williams, 2012; Williams and Katz, 2012) extracts typical durations (in term of actual time measures) for verb lemmas from Twitter. They distinguish episodic and habitual uses of the verbs, using the method of Mathew and Katz (2009). 4 Data In this section, we describe the data sets used in our experiments.3 4.1 Penn TreeBank (M&K) data set Mathew and Katz (2009) randomly select sentences for several verbs from the WSJ and Brown corpus sections of the Penn Treebank. They require the verb to be lexically dynamic. Sentences are marked as habitual or episodic, further details on the annotation guidelines are n"
D15-1294,xue-zhang-2014-buy,0,0.0158217,"Vendlers classes state, process, accomplishment and achievement. Mathew and Katz (2009) address the problem of ‘supervised categorization for habitual versus 2473 episodic sentences’ . The authors randomly select 1052 sentences for 57 verbs from the Penn TreeBank (Marcus et al., 1993) and manually mark them with regard to whether they are habitual or episodic. They focus on verbs that are lexically dynamic and discuss a variety of syntactic features, which they extract from gold standard parse trees. Their aim is to study the ability of syntactic features alone to identify habitual sentences. Xue and Zhang (2014) annotate verbs with the four event types habitual event, state, on-going event and episodic event with the aim of improving tense prediction for Chinese. Recent related work (Williams, 2012; Williams and Katz, 2012) extracts typical durations (in term of actual time measures) for verb lemmas from Twitter. They distinguish episodic and habitual uses of the verbs, using the method of Mathew and Katz (2009). 4 Data In this section, we describe the data sets used in our experiments.3 4.1 Penn TreeBank (M&K) data set Mathew and Katz (2009) randomly select sentences for several verbs from the WSJ a"
D15-1294,zarcone-lenci-2008-computational,0,0.339216,"ion of lexical aspectual class of verbs is generally skewed towards dynamic (Friedrich and Palmer, 2014). 3 Related work The task of predicting fundamental aspectual class aims to determine whether the verb is used in a stative or dynamic sense. This task predicts the aspectual class of a verb in context before any aspectual markers or transformations (such as use of the perfect or modals) have been applied. Siegel and McKeown (2000) propose the use of linguistic indicators (explained in Section 5.2); Friedrich and Palmer (2014) show the importance of using context-based features in addition. Zarcone and Lenci (2008) classify occurrences of 28 Italian verbs according to Vendlers classes state, process, accomplishment and achievement. Mathew and Katz (2009) address the problem of ‘supervised categorization for habitual versus 2473 episodic sentences’ . The authors randomly select 1052 sentences for 57 verbs from the Penn TreeBank (Marcus et al., 1993) and manually mark them with regard to whether they are habitual or episodic. They focus on verbs that are lexically dynamic and discuss a variety of syntactic features, which they extract from gold standard parse trees. Their aim is to study the ability of sy"
D15-1294,loaiciga-etal-2014-english,0,\N,Missing
D18-1282,P16-1113,0,0.301124,"and how), and this may require a joint processing or reasoning with possibly multiple (extra-)linguistic information sources (e.g., text, images). In NLP, the well-established and studied task of semantic role labeling (SRL) aims to extract such knowledge in the form of shallow semantic structures from natural language texts (e.g., questioning(Agent:man, Theme:women) ); see, e.g., Gildea and Jurafsky (2002); Palmer et al. (2010), for an overview). It is considered an essential task towards text understanding, and was shown to be beneficial for applications such as information extraction (see Roth and Lapata (2016) and the references therein) and question answering (Shen and Lapata, 2007). In computer vision research, recent efforts have been made on visual SRL or situation recognition, a task coined by transferring the use of semantic roles to produce similar structured meaning descriptions for visual scenes (e.g., Yang et al. (2016); Yatskar et al. (2016)). To facilitate the endeavor of joint processing over multiple sources, it is desirable to induce representations of texts and visual scenes which do encode this kind of information, and in, essentially, a congruent and generic way. The latter would"
D18-1282,D07-1002,0,0.121186,"ly multiple (extra-)linguistic information sources (e.g., text, images). In NLP, the well-established and studied task of semantic role labeling (SRL) aims to extract such knowledge in the form of shallow semantic structures from natural language texts (e.g., questioning(Agent:man, Theme:women) ); see, e.g., Gildea and Jurafsky (2002); Palmer et al. (2010), for an overview). It is considered an essential task towards text understanding, and was shown to be beneficial for applications such as information extraction (see Roth and Lapata (2016) and the references therein) and question answering (Shen and Lapata, 2007). In computer vision research, recent efforts have been made on visual SRL or situation recognition, a task coined by transferring the use of semantic roles to produce similar structured meaning descriptions for visual scenes (e.g., Yang et al. (2016); Yatskar et al. (2016)). To facilitate the endeavor of joint processing over multiple sources, it is desirable to induce representations of texts and visual scenes which do encode this kind of information, and in, essentially, a congruent and generic way. The latter would furthermore support the induction of a desired level of abstraction as need"
D18-1282,J02-3001,0,0.617907,"t. As the examples illustrate, the interpretation of a (visual) scene is related to the determination of its events, their participants and the roles they play therein (i.e., distill who did what to whom, where, why and how), and this may require a joint processing or reasoning with possibly multiple (extra-)linguistic information sources (e.g., text, images). In NLP, the well-established and studied task of semantic role labeling (SRL) aims to extract such knowledge in the form of shallow semantic structures from natural language texts (e.g., questioning(Agent:man, Theme:women) ); see, e.g., Gildea and Jurafsky (2002); Palmer et al. (2010), for an overview). It is considered an essential task towards text understanding, and was shown to be beneficial for applications such as information extraction (see Roth and Lapata (2016) and the references therein) and question answering (Shen and Lapata, 2007). In computer vision research, recent efforts have been made on visual SRL or situation recognition, a task coined by transferring the use of semantic roles to produce similar structured meaning descriptions for visual scenes (e.g., Yang et al. (2016); Yatskar et al. (2016)). To facilitate the endeavor of joint p"
D18-1282,J05-1004,0,0.287945,"(2c) ARREST ( Suspect:r5/A young guy ) (1b) [r1,r2 The police] arresting [r5 someone] on [r3 a busy city street] . (1c) [r5 A young guy] is getting arrested. PLACING Figure 4: Flickr30k captions for the image in Fig. 2. Left: Flickr30k Entities annotations of the mentioned objects with unique entity ids. Right: Frame-semantic annotations of the sentences, output by PathLSTM (Roth, 2016; Roth and Lapata, 2016). 3.3 Using Linguistic Knowledge for Data Creation SRL systems in NLP research use training data which have been carefully created by linguistic experts (e.g., Ruppenhofer et al. (2006); Palmer et al. (2005)) for many years. To train our model on the visual SRL task, we build upon the annotation efforts made in NLP. The exploitation of existing resources which were developed for the analogous goal means to get around the time-consuming and costly annotation effort involved in the creation of training data. Moreover, adopting an established framework in NLP for shallow semantic representations (FrameNet, Ruppenhofer et al. (2006), in our case), including the therein defined frame and role labels, could facilitate cross-modal interactions— advances in vSRL can help to improve SRL and vice versa, or"
D18-1282,N16-1019,0,0.143179,"Missing"
E14-1006,P98-1013,0,0.23749,"Missing"
E14-1006,W12-1901,1,0.846802,"rior knowledge as defined above enables the application of our model to small data sets, we emphasize that the model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts that MSA cannot capt"
E14-1006,P08-1090,0,0.840503,"Work In the 1970s, scripts were introduced as a way to equip AI systems with world knowledge (Schank and Abelson, 1975; Barr and Feigenbaum, 1986). Task-specific script databases were developed manually. FrameNet (Baker et al., 1998) follows a similar idea, in defining verb frames together with argument types that can fill the verbs’ argument slots. Frames can then be combined into “scenario frames”. Manual composition of such databases, is arguably expensive and does not scale well. This paper follows a series of more recent work which aims to infer script knowledge automatically from data. Chambers and Jurafsky (2008) The GMM has been successfully applied to modeling ordering constraints in NLP tasks. Chen et al. (2009) augment classical topic models with a GMM, under the assumption that topics in structured domains (e.g., biographies in Wikipedia) tend to follow an underlying canonical ordering, an assumption which matches well our data (the annotators were asked to follow the temporal or50 tion in an ESD with one event type e. We specify the number of possible event types E a priori as a number exceeding the number of event types in all the scripts considered. The model will select an effective subset of"
E14-1006,P09-1068,0,0.567259,"v models. We are not aware of previous work on modeling events with GMMs. Conversely, MMs were considered in the very recent work of Cheung et al. (2013) in the context of script induction from news corpora where the Markovian assumption is much more natural. There exists a body of work for learning participant types involved in scripts. Regneri et al. (2011) extend their work by inducing participant types on the basis of the TSG, using structural information about participant mentions in the TSG as well as WordNet similarity, which they then combine into an Integer Linear Program. Similarly, Chambers and Jurafsky (2009) extend their work on narrative chains, presenting a system with which they jointly learn event types and semantic roles of the participants involved, but do not consider event orderings. We include participant types as a latent feature in our model, assuming that participant mentions in an event description are a predictive feature for the corresponding event type. One way of alleviating the problem of small data sets is incorporating informed prior knowledge. Raina et al. (2006) encode word correlations in a variance-covariance matrix of a multivariate normal distribution (MVN), and sample p"
E14-1006,P10-1100,1,0.849007,"iversity of Edinburgh, United Kingdom 2 ILLC, University of Amsterdam, Netherlands 3 Department of Computational Linguistics, Saarland University, Germany Abstract natural text (cf. (Chambers and Jurafsky, 2008)), as not all types of scripts are elaborated in natural text – being left implicit because of assumed readers’ world knowledge. Our model, operating on data obtained in a cheap way by crowdsourcing, is applicable to any kind of script and can fill this gap. We follow work in inducing script knowledge from explicit instantiations of scripts, socalled event sequence descriptions (ESDs) (Regneri et al., 2010). Our data consists of sets of ESDs, each set describing a well-known situation we will call scenario (e.g., “washing laundry”). An ESD consists of a sequence of events, each describing an action defining part of the scenario (e.g., “place the laundry in the washing machine”). We refer to descriptions of the same event across ESDs as event types. We refer to entities involved in a scenario as participants (e.g., a “washing machine” or a “detergent”), and to sets of participant descriptions describing the same entity as participant types. For each type of scenario, our model clusters descriptio"
E14-1006,R11-1064,1,0.951101,"umber of possible event types E a priori as a number exceeding the number of event types in all the scripts considered. The model will select an effective subset of those types. Assume a scenario-specific corpus c, consisting of D ESDs, c = {d1 , ..., dD }. Each ESD di consists of Nd event descriptions di = {di,1 , ..., di,Ni }. Boundaries between descriptions of single events are marked in the data. For each event description di,n a bag of participant descriptions is extracted. Each participant description corresponds to one noun phrase as identified automatically by a dependency parser (cf. Regneri et al. (2011)). We also associate participant types with participant descriptions, these types are latent and induced at the inference stage. Given such a corpus of ESDs, our model assigns each event description di,n in an ESD di one event type zdi,n = e, where e ∈ {1, ..., E}. Assuming that all ESDs are generated from the same underlying set of event types, our objective is to assign the same event type to equivalent event descriptions across all ESDs in the corpus. We furthermore assume that there exists a canonical temporal ordering of event types for each scenario type, and that events in observed scen"
E14-1006,P11-1145,1,0.710389,"t. While we will show that prior knowledge as defined above enables the application of our model to small data sets, we emphasize that the model is generally widely applicable for two reasons. First, the data, collected using crowdsourcing, is comparatively easy and cheap to extend. Secondly, our model is domain independent and can be applied to scenario descriptions from any domain without any modification. Note that parameters were tuned on held-out scenarios, and no scenario-specific tuning was performed. 2 A related task, unsupervised frame induction, has also been considered in the past (Titov and Klementiev, 2011; Modi et al., 2012; O’Connor, 2012); the frame representations encode events and participants but ignore the temporal aspect of script knowledge. We model temporal constraints on event type orderings with the Generalized Mallows Model (GMM; Mallows (1957); Fligner and Verducci (1986); Klementiev et al. (2008)), a statistical model over permutations. The GMM is a flexible model which can specify item-specific sensitivity to perturbation from the item’s position in the canonical permutation. With the GMM we are thus able to model event type-specific temporal flexibility – a feature of scripts t"
E14-1006,C98-1013,0,\N,Missing
E14-1006,J82-1004,0,\N,Missing
E91-1009,J87-1005,0,0.0378711,"Missing"
E91-1009,P88-1010,0,0.0668791,"Missing"
E91-1009,E91-1010,0,\N,Missing
I11-1127,J93-3001,0,0.113073,"Missing"
I11-1127,J90-1003,0,0.261605,"ector in the vector space V spanned by the set of basis vectors {e(r,w0 ) |r ∈ R, w0 ∈ W }. Such a vector records the association strength between w and any context word w0 occurring in relation r. Specifically, we associate a word w ∈ W with a vector v(w) ∈ V by setting ∑ v(w) := r∈R,w0 ∈W f (w, r, w0 ) · e(r,w0 ) where f is a function that assigns a weight to the dependency triple (w, r, w0 ). In the simplest case, this could be the frequency of w occurring together with w0 in relation r in a corpus of dependency trees. In the experiments reported below, we use pointwise mutual information (Church and Hanks, 1990) instead, as it proved superior to raw frequency counts: PMI(w, r, w0 ) = log p(w, w0 |r) p(w, · |r)p(·, w0 |r) Here the dots stand for marginalization over the relevant variables. Given an occurrence of a word w in the context of another word wc , related by the syntactic relation rc , we now define a contextualized version of v(w) by reweighting the vector components. We set vrc ,wc (w) := ∑ r∈R,w0 ∈W αrc ,wc ,r,w0 · f (w, r, w0 ) · e(r,w0 ) Here, the weights αrc ,wc ,r,w0 quantify the degree to which a vector dimension (r, w0 ) is compatible with the observed context (rc , wc ). We consider"
I11-1127,de-marneffe-etal-2006-generating,0,0.00454815,"Missing"
I11-1127,D10-1113,0,0.323566,"ing syntactic relations r1 , . . . , rn , we obtain a contextualized vector of w by superimposing the vectors vri ,wi (1 ≤ i ≤ n) through vector addition: n vr1 ,w1 ,...,rn ,wn (w) := ∑ vri ,wi (w) i=1 The resulting vector vr1 ,w1 ,...,rn ,wn (w) is our completely contextualized representation for the word w that contains information about all context words. 4 Ranking Paraphrases In this section, we evaluate to what extent our model supports the choice of contextually appropriate paraphrases for different uses of a target word. We follow previous work (Thater et al., 2010; Erk and Padó, 2010; Dinu and Lapata, 2010) and consider the following task: We are given a target word w in a sentential context and a set of reference words w1 , . . . , wk , where each wi is a lexical paraphrase of w in one of w’s senses. The task is to rank the candidate words wi according to their appropriateness as paraphrases of w in the given context. Ideally, the model will rank, for instance, levy higher than recharge as a paraphrase of charge in charge a fee, and lower in charge the battery. Experimental Set-up Gold standard. We derive our gold standard from the SemEval 2007 lexical substitution task dataset (McCarthy and Na"
I11-1127,D08-1094,0,0.471674,"be properly reflected in their respective meaning vectors, since the former, but not the latter, includes (context words reflecting) the “supply electricity” sense of charge. The problem of modeling context-sensitivity in a distributional framework has first been addressed in the seminal paper of Schütze (1998), who uses second-order bag-of-words vectors for the task of word sense discrimination. Recently, the issue has been taken up by several approaches that include some kind of syntactic information, in part under the heading of “distributional compositionality” (Mitchell and Lapata, 2008; Erk and Padó, 2008), in part as “syntax-sensitive contextualization” (Thater et al., 2010). These approaches have in common that the contextual influence on the meaning of a target word w is modeled through vector composition: The meaning of w in context c is represented by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition. The results published during the last couple of years show a considerable increase of performance, but at the price of an increasing complexity and lack of intuitive transparency of the models. In this paper, we will d"
I11-1127,P04-1036,0,0.0400925,"nal information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance. 1 Introduction Distributional vector-space models of word meaning have proven helpful for a number of basic natural language processing tasks, such as word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), or modeling of selectional preferences (Erk, 2007), and have been successfully used in a variety of applications like information retrieval (Manning et al., 2008) or question answering (Tellex et al., 2003). Standard distributional models of meaning are attractive because they are simple, have wide coverage, and, in particular, can be acquired using unsupervised methods at virtually no cost. Vector-space models of meaning lend themselves as a basis for determining a soft and gradual concept of semantic similarity (e.g., through the cosine measure), which does not rely on a fixed set of dicti"
I11-1127,P08-1028,0,0.580079,"arge/impose a fee will not be properly reflected in their respective meaning vectors, since the former, but not the latter, includes (context words reflecting) the “supply electricity” sense of charge. The problem of modeling context-sensitivity in a distributional framework has first been addressed in the seminal paper of Schütze (1998), who uses second-order bag-of-words vectors for the task of word sense discrimination. Recently, the issue has been taken up by several approaches that include some kind of syntactic information, in part under the heading of “distributional compositionality” (Mitchell and Lapata, 2008; Erk and Padó, 2008), in part as “syntax-sensitive contextualization” (Thater et al., 2010). These approaches have in common that the contextual influence on the meaning of a target word w is modeled through vector composition: The meaning of w in context c is represented by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition. The results published during the last couple of years show a considerable increase of performance, but at the price of an increasing complexity and lack of intuitive transparency of the models. In"
I11-1127,S07-1006,0,0.0342037,"Missing"
I11-1127,P10-2017,0,0.270154,", wn and corresponding syntactic relations r1 , . . . , rn , we obtain a contextualized vector of w by superimposing the vectors vri ,wi (1 ≤ i ≤ n) through vector addition: n vr1 ,w1 ,...,rn ,wn (w) := ∑ vri ,wi (w) i=1 The resulting vector vr1 ,w1 ,...,rn ,wn (w) is our completely contextualized representation for the word w that contains information about all context words. 4 Ranking Paraphrases In this section, we evaluate to what extent our model supports the choice of contextually appropriate paraphrases for different uses of a target word. We follow previous work (Thater et al., 2010; Erk and Padó, 2010; Dinu and Lapata, 2010) and consider the following task: We are given a target word w in a sentential context and a set of reference words w1 , . . . , wk , where each wi is a lexical paraphrase of w in one of w’s senses. The task is to rank the candidate words wi according to their appropriateness as paraphrases of w in the given context. Ideally, the model will rank, for instance, levy higher than recharge as a paraphrase of charge in charge a fee, and lower in charge the battery. Experimental Set-up Gold standard. We derive our gold standard from the SemEval 2007 lexical substitution task"
I11-1127,J07-2002,0,0.120831,"Missing"
I11-1127,P07-1028,0,0.0456217,"ase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance. 1 Introduction Distributional vector-space models of word meaning have proven helpful for a number of basic natural language processing tasks, such as word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), or modeling of selectional preferences (Erk, 2007), and have been successfully used in a variety of applications like information retrieval (Manning et al., 2008) or question answering (Tellex et al., 2003). Standard distributional models of meaning are attractive because they are simple, have wide coverage, and, in particular, can be acquired using unsupervised methods at virtually no cost. Vector-space models of meaning lend themselves as a basis for determining a soft and gradual concept of semantic similarity (e.g., through the cosine measure), which does not rely on a fixed set of dictionary senses with their well-known problems (Kilgarr"
I11-1127,P10-1154,0,0.0132838,"ext, without relying on any manually annotated training data. Our system is knowledge-based, according to the classification of WSD approaches proposed in McCarthy 1139 (2009) and Navigli (2009). It is a knowledge-lean system, in contrast to many other systems that exploit external resources, since it uses only a small subset of the structural information provided by WordNet – just as much as is required to adapt our contextualization model to the WSD task. The state of the art in knowledge-based WSD systems not trained on annotated data is defined by the models of Navigli and Velardi (2005), Ponzetto and Navigli (2010) and Li et al. (2010). The former two rely on a rich inventory of additional knowledge resources. Li et al. (2010) restricts itself to WordNet information in a similar way as our approach, and therefore is our natural benchmark. 5.1 Method We frame the task of choosing the right WordNet sense as a paraphrase ranking task like the one considered in Section 4, with all possible synonyms of the target word constituting the set of (lexical) paraphrase candidates. The basic idea for predicting a sense of the target word is to choose the synset that contains the most similar paraphrase. As the WordN"
I11-1127,P10-1116,0,0.155754,"syntactic arguments. Contextualization is modeled as above in terms of vector composition. Among the aforementioned approaches, their proposal performs best, but at the cost of a rather complex and unintuitive concept of second-order co-occurrence vectors. Other approaches achieve good results without using vector composition. Dinu and Lapata (2010) represent word meaning in context by using a latent variable model, where context-dependence is modeled by conditioning the latent variable on the context in which a word occurs. Similar proposals have been made by Reisinger and Mooney (2010a) and Li et al. (2010). A different approach has been taken by Erk and Padó (2010) and Reisinger and Mooney (2010b). Instead of “refining” vector representations ranging over all words in a corpus by means of vector composition, they start out from “token” vectors for individual instances of words in context, and then group these token vectors into different sensespecific clusters. 2 3 Related work Inspired by earlier work of Kintsch (2001), who proposes a network algorithm to extract contextspecific vector representations for words in context, Mitchell and Lapata (2008) investigate the systematic combination of di"
I11-1127,S07-1009,0,0.0594949,"d Lapata, 2010) and consider the following task: We are given a target word w in a sentential context and a set of reference words w1 , . . . , wk , where each wi is a lexical paraphrase of w in one of w’s senses. The task is to rank the candidate words wi according to their appropriateness as paraphrases of w in the given context. Ideally, the model will rank, for instance, levy higher than recharge as a paraphrase of charge in charge a fee, and lower in charge the battery. Experimental Set-up Gold standard. We derive our gold standard from the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). The original dataset contains 10 instances for each of 201 target words (nouns, verbs, adjectives and adverbs) in different sentential contexts. For each instance, five subjects were asked to name appropriate paraphrases. Table 1 shows an example of three instances of charge together with their gold standard paraphrases. Each paraphrase comes with a weight, which corresponds to the number of times it was chosen by the different subjects. The original task addresses two subtasks: identifying paraphrase candidates and ranking them according to the context. Here, we restrict ourselves to the se"
I11-1127,D10-1114,0,0.0830357,"Missing"
I11-1127,N10-1013,0,0.334544,"on the task of word sense disambiguation in Section 5. Section 6 concludes. tionality, but it can also be taken to be a method to contextualize a target word through its dependents. Erk and Padó (2008) propose structured vector representations, where each word is characterized by a standard co-occurrence vector, plus separate vector representations for the (inverse) selectional preferences for subject, object, and other syntactic relations. Contextualization is modeled by combining, e.g., the basic vector of the target verb with the selectional preferences of subject and object. Thater et al. (2010) propose a similar approach, where word meaning is modeled as a second-order vector obtained by summing over first-order vectors representing the inverse selectional preferences of a word’s syntactic arguments. Contextualization is modeled as above in terms of vector composition. Among the aforementioned approaches, their proposal performs best, but at the cost of a rather complex and unintuitive concept of second-order co-occurrence vectors. Other approaches achieve good results without using vector composition. Dinu and Lapata (2010) represent word meaning in context by using a latent variab"
I11-1127,J98-1004,0,0.826718,"ts components, based on distributional information about the context words. Evaluation on a paraphrase ranking task derived from the SemEval 2007 Lexical Substitution Task shows that our model outperforms all previous models on this task. We show that our model supports a wider range of applications by evaluating it on a word sense disambiguation task. Results show that our model achieves state-of-the-art performance. 1 Introduction Distributional vector-space models of word meaning have proven helpful for a number of basic natural language processing tasks, such as word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), or modeling of selectional preferences (Erk, 2007), and have been successfully used in a variety of applications like information retrieval (Manning et al., 2008) or question answering (Tellex et al., 2003). Standard distributional models of meaning are attractive because they are simple, have wide coverage, and, in particular, can be acquired using unsupervised methods at virtually no cost. Vector-space models of meaning lend themselves as a basis for determining a soft and gradual concept of semantic similarity (e.g., through the cosine measure),"
I11-1127,P10-1097,1,0.789295,"ormer, but not the latter, includes (context words reflecting) the “supply electricity” sense of charge. The problem of modeling context-sensitivity in a distributional framework has first been addressed in the seminal paper of Schütze (1998), who uses second-order bag-of-words vectors for the task of word sense discrimination. Recently, the issue has been taken up by several approaches that include some kind of syntactic information, in part under the heading of “distributional compositionality” (Mitchell and Lapata, 2008; Erk and Padó, 2008), in part as “syntax-sensitive contextualization” (Thater et al., 2010). These approaches have in common that the contextual influence on the meaning of a target word w is modeled through vector composition: The meaning of w in context c is represented by a vector obtained by combining the vectors of w and c using some operation such as component-wise multiplication or addition. The results published during the last couple of years show a considerable increase of performance, but at the price of an increasing complexity and lack of intuitive transparency of the models. In this paper, we will demonstrate that one can keep the model simple and at the same time outp"
I17-2007,P16-1004,0,0.0136662,"r1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@mq.edu.au 3 Max Planck Institute for Informatics, Germany cxchu@mpi-inf.mpg.de Abstract ploy sequence-to-sequence learning (S EQ 2S EQ) for this task. S EQ 2S EQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, S EQ 2S EQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of S EQ 2S EQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of S EQ 2S EQ allow us to approach the event prediction task. S EQ 2S EQ has been applied to text prediction by Kiros et al. (2015)"
I17-2007,D15-1166,0,0.0288107,"Missing"
I17-2007,K16-1028,0,0.0186085,"tional Linguistics, Saarland University, Germany {daiquocn, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@mq.edu.au 3 Max Planck Institute for Informatics, Germany cxchu@mpi-inf.mpg.de Abstract ploy sequence-to-sequence learning (S EQ 2S EQ) for this task. S EQ 2S EQ have received significant research attention, especially in machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and in other NLP tasks such as parsing (Vinyals et al., 2015; Dong and Lapata, 2016), text summarization (Nallapati et al., 2016) and multi-task learning (Luong et al., 2016). In general, S EQ 2S EQ uses an encoder which typically is a recurrent neural network (RNN) (Elman, 1990) to encode a source sequence, and then uses another RNN which we call decoder to decode a target sequence. The goal of S EQ 2S EQ is to estimate the conditional probability of generating the target sequence given the encoding of the source sequence. These characteristics of S EQ 2S EQ allow us to approach the event prediction task. S EQ 2S EQ has been applied to text prediction by Kiros et al. (2015) and Pichotta and Mooney (2016). We also use S"
I17-2007,P02-1040,0,0.100146,"the timestep i. We use two advanced variants of RNNs that replace the cells of RNNs with the Long Sort Term Memory (LSTM) cells (Hochreiter and Schmidhuber, 1997) and the Gated Recurrent Unit (GRU) cells (Cho et al., 2014). We also use a deeper architecture of multi-layers, to model complex interactions in the context. This is different from Kiros et al. (2015) and Pichotta and Mooney (2016) where they only use a single layer. So we in fact experiment with Bidirectional-LSTM multi-layer RNN (BiLSTM) and Bidirectional-GRU multilayer RNN (BiGRU). • Pichotta and Mooney (2016) use the BLEU score (Papineni et al., 2002) for evaluation (i.e., the standard evaluation metric used in machine translation), which measures surface similarity between predicted and actual sentences. We complement this evaluation by measuring prediction accuracy on the semantic level. To this purpose, we use the gold paraphrase sets of event descriptions in the D E S CRIPT corpus, e.g., “Remove cake”, “Remove from oven” and “Take the cake out of oven” belong to the same gold paraphrase set of taking out oven. The gold paraphrase sets allow us to access the correctness of the prediction which could not be attained by using the BLEU mea"
I17-2007,D14-1179,0,0.010377,"Missing"
I17-2007,W15-4915,0,0.0483245,"Missing"
I17-2007,2015.eamt-1.16,0,\N,Missing
I17-2007,P16-1027,0,\N,Missing
L16-1135,E06-1042,0,0.0348704,"l¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was needed. Inspired by standard approaches to supervised word sense disambiguation, we use the (lemmatized) words which occur within the same"
L16-1135,W07-1106,0,0.129658,"nderspecification: the Dem Hochzeitsunterhalter Robbie kommt der Frohsinn abhanden, als ihn die eigene Braut vor dem Traualtar stehenl¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was"
L16-1135,J09-1005,0,0.0733409,"the Dem Hochzeitsunterhalter Robbie kommt der Frohsinn abhanden, als ihn die eigene Braut vor dem Traualtar stehenl¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was needed. Inspired by"
L16-1135,W02-1006,0,0.06052,"skip n-grams, 1 to 6-grams in a window that spans 3 positions to the left and the right of the compound to be classified, skipping the compound itself. Additionally, we collect as features POS information of context words in the same window (pos) and the POS tag of the compound itself (pos0 ) or the second part if it is written as two individual tokens. To cover syntactic information of an item, we use the subject and accusative object of the compound as assigned by the Zurich parser (Sennrich et al., 2009), as well as their part-of-speech tags (syn). All these features have been proposed by (Lee and Ng, 2002) for a word sense disambiguation task. We also use selectional preference information (sel), counting how frequently the subject and accusative object head noun in a specific occurrence of the verb group occur as the subject or object, respectively, of the base verb (first component). For example, Bild (picture) occurs often as a subject of h¨angen, which we take as an indicator that an instance of h¨angen bleiben with Bild as a subject is used literally. Finally, we add topical information (topic) for the news article in which an item occurs using manually annotated topic categories in the Wa"
L16-1135,E09-1086,0,0.0798742,"der Frohsinn abhanden, als ihn die eigene Braut vor dem Traualtar stehenl¨asst. (The wedding entertainer Robbie loses his own chance at happiness when his own bride leaves him standing before the altar.) 3. Experiments This section presents results in automatically distinguishing literal and idiomatic uses of German infinitive-verb compounds. Previous approaches to detecting idiomatic expressions on the level of individual instances often relied on properties which are not applicable in our case, like the concepts of canonical form (Cook et al., 2007; Fazly et al., 2009) or lexical cohesion (Sporleder and Li, 2009). Conceptually, our work is more in the spirit of Birke and Sarkar (2006), who treated “literal” and “idiomatic” as two different senses of the target word and applied (unsupervised) word-sense disambiguation techniques. In contrast to this work, however, we use a simple supervised approach to distinguish literal from idiomatic uses. 3.1. Feature sets and classifier The Wahrig corpus already comes with automatically annotated sentence and token boundaries, as well as POS information, so no additional linguistic preprocessing was needed. Inspired by standard approaches to supervised word sense"
L16-1555,P08-1090,0,0.630155,"typical human activity such as going to a restaurant or visiting a doctor” (Barr and Feigenbaum, 1981). Script events describe an action/activity along with the involved participants. For example, in the script describing A VISIT TO A RESTAURANT , typical events are ENTERING THE RESTAURANT, ORDERING FOOD or EATING . Participants in this scenario can include animate objects like the WAITER and the CUSTOMER , as well as inanimate objects such as CUTLERY or FOOD. Script knowledge has been shown to play an important role in text understanding (Cullingford, 1978; Miikkulainen, 1995; Mueller, 2004; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Modi and Titov, 2014; Rudinger et al., 2015). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario TAKING A BATH . Once the TAKING A BATH scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the TAKING A BATH script. Although in"
L16-1555,P09-1068,0,0.316802,"s going to a restaurant or visiting a doctor” (Barr and Feigenbaum, 1981). Script events describe an action/activity along with the involved participants. For example, in the script describing A VISIT TO A RESTAURANT , typical events are ENTERING THE RESTAURANT, ORDERING FOOD or EATING . Participants in this scenario can include animate objects like the WAITER and the CUSTOMER , as well as inanimate objects such as CUTLERY or FOOD. Script knowledge has been shown to play an important role in text understanding (Cullingford, 1978; Miikkulainen, 1995; Mueller, 2004; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Modi and Titov, 2014; Rudinger et al., 2015). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario TAKING A BATH . Once the TAKING A BATH scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the TAKING A BATH script. Although in this story, “entering the ba"
L16-1555,W14-1606,1,0.851867,"siting a doctor” (Barr and Feigenbaum, 1981). Script events describe an action/activity along with the involved participants. For example, in the script describing A VISIT TO A RESTAURANT , typical events are ENTERING THE RESTAURANT, ORDERING FOOD or EATING . Participants in this scenario can include animate objects like the WAITER and the CUSTOMER , as well as inanimate objects such as CUTLERY or FOOD. Script knowledge has been shown to play an important role in text understanding (Cullingford, 1978; Miikkulainen, 1995; Mueller, 2004; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Modi and Titov, 2014; Rudinger et al., 2015). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario TAKING A BATH . Once the TAKING A BATH scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the TAKING A BATH script. Although in this story, “entering the bath room”, “turning on"
L16-1555,P10-1100,1,0.763942,"“turning on the water” event, even if it was not explicitly mentioned in the text. Table 1 gives an example of typical events and participants for the script describing the scenario TAKING A BATH . A systematic study of the influence of script knowledge in texts is far from trivial. Typically, text documents (e.g. narrative texts) describing various scenarios evoke many different scripts, making it difficult to study the effect of a single script. Efforts have been made to collect scenariospecific script knowledge via crowdsourcing, for example the OMICS and SMILE corpora (Singh et al., 2002; Regneri et al., 2010; Regneri, 2013), but these corpora describe script events in a pointwise telegram style rather than in full texts. I was sitting on my couch when I decided that I hadn’t taken a bath in a while so I stood up and walked to the bathroom where I turned on the faucet in the sink and began filling the bath with hot water. While the tub was filling with hot water I put some bubble bath into the stream of hot water coming out of the faucet so that the tubbed filled with not only hot water[...] Figure 1: An excerpt from a story on the TAKING A BATH script. This paper presents the InScript 1 corpus (N"
L16-1555,S15-1024,1,0.825923,"Missing"
L16-1555,L16-1556,1,0.846082,"ith coreference information in order to facilitate the study of the interdependence between script structure and coreference. The InScript corpus is a unique resource that provides a basis for studying various aspects of the role of script knowledge in language processing by humans. The acquisition of this corpus is part of a larger research effort that aims at using script knowledge to model the surprisal and information density in written text. Besides InScript, this project also released a corpus of generic descriptions of script activities called DeScript (for Describing Script Structure, Wanzare et al. (2016)). DeScript contains a range of short and textually simple phrases that describe script events in the style of OMICS or SMILE (Singh et al., 2002; Regneri et al., 2010). These generic telegram-style descriptions are 1 The corpus can be downloaded at: http://www. sfb1102.uni-saarland.de/?page_id=2582 2 https://www.mturk.com 3485 2. Get Ingredients – gather all ingredients – get ingredients –… get ingred. Add Ingredients – pour ingredients in bowl – add ingredients to bowl –… add ingred. prepare ingred. Prepare Ingredients – mix ingredients together in bowl – stir ingredient –… […] 2.1. I gotget"
L16-1555,P13-4001,0,0.0727339,"Missing"
L16-1556,P09-1068,0,0.282407,"nt role for the computational modeling of cognitive abilities (in particular for natural language processing), but making this kind of knowledge available for use in modeling is not easy. On the one hand, the manual creation of widecoverage knowledge bases is infeasible, due to the size and complexity of relevant script knowledge. On the other hand, texts typically refer only to certain steps in a script and leave a large part of this knowledge implicit, relying on the reader’s ability to infer the full script in detail. Thus, extraction of script knowledge from large text corpora (as done by Chambers and Jurafsky (2009)) is difficult and the outcome can be noisy. In this work, we present a large-scale crowdsourced collection and annotation of explicit linguistic descriptions of event patterns, to be used for the automatic acquisition of high-quality script knowledge. This work is part of a larger research effort where we seek to provide a solid empirical basis for high-quality script modeling by inducing script structure from crowdsourced descriptions of typical events, and to investigate methods of text-toscript mapping, using naturalistic texts from crowdsourced stories, which describe real-life experience"
L16-1556,E14-1006,1,0.867864,"ts, the clustering algorithm would need information on script-specific semantic similarity that goes beyond pure semantic similarity. For instance, in the FLYING IN AN AIRPLANE scenario, it is not trivial for any semantical similarity measure to predict that walk up the ramp and board plane are functionally similar with respect to the given scenario. To address this issue, we collect partial alignment information that we will use as seed data in future work on semi-supervised clustering. The alignment annotations are also suitable for a semi-supervised extension of the event-ordering model of Frermann et al. (2014). In this work, we have taken measures to provide a sound empirical basis for better-quality script models, by extending existing corpora in two different ways. First, we crowdsourced a corpus of 40 scenarios with 100 ESDs each, thus going beyond the size of previous script collections. Second, we enriched the corpus with partial alignments of ESDs, done by human annotators. The result is a corpus of partially-aligned generic activity descriptions, the DeScript corpus (Describing Script Structure). More generally, DeScript is a valuable resource for any task involving alignment and paraphrase"
L16-1556,L16-1555,1,0.846009,"e noisy. In this work, we present a large-scale crowdsourced collection and annotation of explicit linguistic descriptions of event patterns, to be used for the automatic acquisition of high-quality script knowledge. This work is part of a larger research effort where we seek to provide a solid empirical basis for high-quality script modeling by inducing script structure from crowdsourced descriptions of typical events, and to investigate methods of text-toscript mapping, using naturalistic texts from crowdsourced stories, which describe real-life experiences and instantiate the same scripts (Modi et al., 2016). Predecessors of our work are the OMICS and SMILE corpora (Singh et al., 2002; Regneri et al., 2010), containing multiple eventsequence descriptions (ESDs) for specific activity types or scenarios. Figure 1 shows some example ESDs for the BAKING A CAKE scenario. As can be seen from the examples, the linguistic descriptions of the same event are different, but 1. Take out box of cake mix from shelf 2. Gather together cake ingredients 3. Get mixing bowl 4. Get mixing tool or spoon or fork 5. Add ingredients to bowl 6. Stir together and mix 7. Use fork to breakup clumps 8. Preheat oven 9. Spray"
L16-1556,P10-1100,1,0.839263,"linguistic descriptions of event patterns, to be used for the automatic acquisition of high-quality script knowledge. This work is part of a larger research effort where we seek to provide a solid empirical basis for high-quality script modeling by inducing script structure from crowdsourced descriptions of typical events, and to investigate methods of text-toscript mapping, using naturalistic texts from crowdsourced stories, which describe real-life experiences and instantiate the same scripts (Modi et al., 2016). Predecessors of our work are the OMICS and SMILE corpora (Singh et al., 2002; Regneri et al., 2010), containing multiple eventsequence descriptions (ESDs) for specific activity types or scenarios. Figure 1 shows some example ESDs for the BAKING A CAKE scenario. As can be seen from the examples, the linguistic descriptions of the same event are different, but 1. Take out box of cake mix from shelf 2. Gather together cake ingredients 3. Get mixing bowl 4. Get mixing tool or spoon or fork 5. Add ingredients to bowl 6. Stir together and mix 7. Use fork to breakup clumps 8. Preheat oven 9. Spray pan with non stick or grease 10. Pour cake mix into pan 11. Put pan into oven 12. Set timer on oven 1"
L18-1011,A00-1031,0,0.529327,"van Nooten and Holland (1994). This tagger produces all possible tokenizations of the input text that consist of morphologically and lexically valid word forms. Tokenization of Sanskrit is a challenging task, because individual words are merged by a set of phonetic rules called Sandhi (“connection”), whose resolution is non-deterministic and, therefore, guided by the morphological, lexical, and semantic composition of a sentence.1 This tokenization step results in a trellis of possible readings for each line of text. A dynamic programming approach that operates with a trigram language model (Brants, 2000) is used to find the most probable lexical path through this trellis. Final fine-grained morphological decisions are made by applying a Conditional Random Field (Lafferty et al., 2001) model to the most probable lexical path. The solutions are ordered by decreasing linguistic probability, given the data from the language model. The first author of this paper finally validated all proposed system analyses in a manual correction step, resulting in a morphological and lexical gold annotation of the complete R.V. Figure 1 shows a schematic overview of annotation levels for a part of hymn R.V, 1.13"
L18-1011,J08-2001,0,0.107844,"Missing"
L18-1011,Q16-1003,0,0.0308804,"nguistic domain. 2. Due to the chronological distance of approximately 1,000 years and fundamental differences in genres and topics, Vedic and Classical Sanskrit use rather different vocabularies (Hellwig, 2017). Vedic texts in general and especially the R.V contain many words that have disappeared in Classical Sanskrit. In addition, lexical semantics differ strongly between Vedic and Classical Sanskrit. The noun vadha, for example, can denote a tool for killing in the R.V (e.g., R.V 10.102.3), while it only denotes the act of killing in Classical Sanskrit. Bayesian models of semantic change (Frermann and Lapata, 2016) or diachronically motivated word embeddings (Hamilton et al., 2016) are not easily applicable, because the Vedic subcorpus is small,3 and the text historical research in older Sanskrit literature is full of uncertainties (Fosse, 1997). The lexical database of the tagger was adapted to the Vedic vocabulary using the specialized dictionary of Grassmann (1873), and Geldner’s German translation of the text (GeldSystem Adaptation Although Classical Sanskrit developed out of a late form of Vedic Sanskrit, which was described by the grammarian P¯an.ini, they represent two separate layers of Old Indo"
L18-1011,J02-3001,0,0.315927,"the remaining ones constitute sentences with missing copulae. Each verb has an average of 2.2 arguments (verbs without arguments: 6,399; with one arg.: 7,350; with 2-4 args.: 7,222; with more than 4 args.: 247).6 6. us.asah. An Algorithm for Argument Identification As mentioned in Sec. 4. and 5., the verb-argument annotation is selective. Therefore, we designed a basic argument identification algorithm that supports the re-annotation of non-oblique cases. Semantic role labeling is an active field of research in CL, and distinguishes between argument identification and argument classification (Gildea and Jurafsky, 2002). A wide range of learning algorithms such as probabilistic frameworks (Gildea and Jurafsky, 2002), 4 The verbal roots are referenced by strings in the VA annotation and by unique numeric IDs on the morpho-lexical level. The 67 mapping rules need to disambiguate homonymous verbal roots such as vas, which can mean “to dwell” (vasati), “to wear” (vaste), or “to shine” (ucchati). 5 The VA annotation indicates that a line of text contains a copula construction, but does not disambiguate the involved nominatives. – Use of copulae is optional in Sanskrit, with a strong tendency of not using it. So,"
L18-1011,P16-1141,0,0.0162552,",000 years and fundamental differences in genres and topics, Vedic and Classical Sanskrit use rather different vocabularies (Hellwig, 2017). Vedic texts in general and especially the R.V contain many words that have disappeared in Classical Sanskrit. In addition, lexical semantics differ strongly between Vedic and Classical Sanskrit. The noun vadha, for example, can denote a tool for killing in the R.V (e.g., R.V 10.102.3), while it only denotes the act of killing in Classical Sanskrit. Bayesian models of semantic change (Frermann and Lapata, 2016) or diachronically motivated word embeddings (Hamilton et al., 2016) are not easily applicable, because the Vedic subcorpus is small,3 and the text historical research in older Sanskrit literature is full of uncertainties (Fosse, 1997). The lexical database of the tagger was adapted to the Vedic vocabulary using the specialized dictionary of Grassmann (1873), and Geldner’s German translation of the text (GeldSystem Adaptation Although Classical Sanskrit developed out of a late form of Vedic Sanskrit, which was described by the grammarian P¯an.ini, they represent two separate layers of Old IndoAryan. Therefore, we needed to perform domain adaptation of the tagg"
L18-1011,W17-6811,1,0.854185,"e extended the morphological rule base and the full form dictionary of the tagger on per case basis, using Macdonell (1916). Figure 2 sets the number of newly added verbal forms (y-axis) in relation to the progress of annotation (x-axis). The plot shows that the number of cases in which we had to extend the full form database manually decreases over time, indicating improving adaptation to the new linguistic domain. 2. Due to the chronological distance of approximately 1,000 years and fundamental differences in genres and topics, Vedic and Classical Sanskrit use rather different vocabularies (Hellwig, 2017). Vedic texts in general and especially the R.V contain many words that have disappeared in Classical Sanskrit. In addition, lexical semantics differ strongly between Vedic and Classical Sanskrit. The noun vadha, for example, can denote a tool for killing in the R.V (e.g., R.V 10.102.3), while it only denotes the act of killing in Classical Sanskrit. Bayesian models of semantic change (Frermann and Lapata, 2016) or diachronically motivated word embeddings (Hamilton et al., 2016) are not easily applicable, because the Vedic subcorpus is small,3 and the text historical research in older Sanskrit"
L18-1011,J08-2006,0,0.0208715,"Missing"
L18-1011,P16-1113,0,0.0605806,"Missing"
L18-1512,N16-1067,0,0.0237228,"Missing"
L18-1512,P08-1090,0,0.23743,"t is substantially above the random baseline. A possible reason is that the compositional computation of the clause-level entailment type amounts to a generalization to the worst case. Thus a number of pairs end up in the Non-Entailment class although there is only a minor local incompatibility. 7. Background and Related Work Our work is based on script representations in which events are encoded as paraphrase sets. There also exist other research directions on modeling script knowledge. The most prominent alternative representation of script knowledge is that of narrative chains, proposed by Chambers and Jurafsky (2008) and subsequently extended by Chambers and Jurafsky (2009), Pichotta and Mooney (2014) and (Ahrendt and Demberg, 2016), to name but a few. Narrative chains 3245 have been used for event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi et al., 2016) or the related story cloze task (Mostafazadeh et al., 2016; Pichotta and Mooney, 2016), in which complete sentences are predicted. Narrative chains (and their aforementioned extensions) differ in two relevant aspects from the script representations used in our study. Instead of using paraphrase sets, ev"
L18-1512,P09-1068,0,0.56057,"at the cake was put into the oven, because it is obvious that this event took place. In contrast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase set"
L18-1512,W17-0905,0,0.0271532,"ed in our study. Instead of using paraphrase sets, events are represented as typed dependency relations between a verb and one of its dependents (the protagonist). Another difference is that narrative chains are intended to be learned automatically from large collections of unannotated text. By contrast, the script representations used in our study are learned from crowdsourced sequences of event descriptions, which are more focused and more detailed compared to narrative chains: They also contain events which are often not mentioned in text, since they are assumed to be background knowledge (Chambers, 2017). These two differences imply that the results of our annotation cannot easily be transferred to script representations along the lines of Chambers and Jurafsky (2008). Text-to-script mapping is similar to the task of recognizing textual entailment (RTE, Dagan et al. (2006)), in which systems have to decide whether a text entails a hypothesis. The text entails the hypothesis if a human reader would infer from the text that the hypothesis is most likely true. In our case, event mentions and event descriptions correspond to texts and hypotheses, respectively. The lexical entailment annotation in"
L18-1512,H92-1045,0,0.120866,"shows the fully labeled instance with participant and event annotations. To simplify the annotation, we make the assumption that each noun has only one sense per scenario: In the PLANTING A TREE scenario, the polysemous word stem e.g. always describes a part of a tree. In order to reduce the annotation effort, we presented all different noun types per participant type only once, rather than every single mentioned token in its sentential context. This on sense per scenario assumption is similar to the one sense per discourse hypothesis, which is often used in word sense disambiguation models (Gale et al., 1992). 4. Clause-Level Entailment: Composition In the previous section, we described the lexical entailment annotation on verbs and nouns, i.e. on a sub-event level. In this section, we now explain a method for an automatic, quasi-compositional computation of clausal-level entailment types. We compose the types from the lexical-level entailment labels of the verb and all its annotated noun dependents. Inspired by the textual inference method used in MacCartney and Manning (2007) and MacCartney and Manning (2009), we compute the type of clause-level entailment between InScript event mentions and DeS"
L18-1512,W07-1431,0,0.0933549,"r scenario assumption is similar to the one sense per discourse hypothesis, which is often used in word sense disambiguation models (Gale et al., 1992). 4. Clause-Level Entailment: Composition In the previous section, we described the lexical entailment annotation on verbs and nouns, i.e. on a sub-event level. In this section, we now explain a method for an automatic, quasi-compositional computation of clausal-level entailment types. We compose the types from the lexical-level entailment labels of the verb and all its annotated noun dependents. Inspired by the textual inference method used in MacCartney and Manning (2007) and MacCartney and Manning (2009), we compute the type of clause-level entailment between InScript event mentions and DeScript patterns from the manually annotated word-level entailment labels. Following MacCartney, we group these labels according to their truthconditional effects, and associate each group with one of six entailment types, shown in Figure 6. We adopt four entailment types from the schema of MacCartney and Manning (2009) and add two new types: We extend the schema with Identity, which is logically speaking a sub-case of Equality. Also, we use Partial Entailment to cover all ca"
L18-1512,W09-3714,0,0.198242,"in the DIG event into patterns (upper part) and the actual verb annotation (lower part). all script-relevant verb instances and all relevant participant instances in every story. 3. Lexical Entailment: Annotation Study To identify the type of semantic relation that needs to be modeled in order to align an event-denoting clause in the text with a paraphrase set representing the same event, the most straightforward way would be to conduct a clauselevel entailment annotation between the clause and EDs in the paraphrase set, e.g. with a set of clausal entailment types similar to the ones used in (MacCartney and Manning, 2009). We found, however, that the assessment of entailment types is time-consuming and unreliable, when based on a direct comparison of complex text clauses and paraphrase sets. We therefore simplify the task, breaking it down into two steps: First, annotators were asked to assign semantic relations between the event descriptions in the paraphrase sets and their instantiations in narrative texts on the lexical level only, labeling the event-denoting verbs and the participant-denoting noun phrases, as is described in this section. Second, we automatically derive an approximate clause-level entailme"
L18-1512,W14-1606,0,0.0151203,"rast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdso"
L18-1512,L16-1555,1,0.957132,"that this event took place. In contrast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of"
L18-1512,N16-1098,0,0.0927158,"Missing"
L18-1512,S17-1016,1,0.903696,"s acquired by first crowdsourcing alternative descriptions of an activity type in terms of sequences of short, telegram-style natural-language event descriptions (ED). Then, paraphrase sets are induced automatically as clusters of EDs, using multiple sequence alignment (Regneri et al., 2010) or semi-supervised clustering (Wanzare et al., 2016). In order to tap the potential of script knowledge in text understanding, systems must be able to link event mentions in texts to the corresponding event types of a script, as indicated by the dotted lines in Figure 2. To our knowledge, Ostermann et al. (2017) is the only existing work on this text-to-script mapping task. Their approach is based on RKP-style script representations. Using this representation, the identification of the correct event type of an event mention is in many cases reduced to a simple identity check 3240 GET_ TREE GET_ TOOLS - buy a tree - get a tree from the store -… DIG - get a shovel - take the shovel -… After finding a shovel, PLANT - dig hole - use your shovel - use shovel for a hole -… I - plant the tree - put the tree into the hole -… made a very deep ditch and put the tree inside HOLE GET_TOOLS SHOVEL GARDENER DIG De"
L18-1512,E14-1024,0,0.128676,"oven, because it is obvious that this event took place. In contrast, a text understanding system that does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1."
L18-1512,P16-1027,0,0.0132769,"nts are encoded as paraphrase sets. There also exist other research directions on modeling script knowledge. The most prominent alternative representation of script knowledge is that of narrative chains, proposed by Chambers and Jurafsky (2008) and subsequently extended by Chambers and Jurafsky (2009), Pichotta and Mooney (2014) and (Ahrendt and Demberg, 2016), to name but a few. Narrative chains 3245 have been used for event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi et al., 2016) or the related story cloze task (Mostafazadeh et al., 2016; Pichotta and Mooney, 2016), in which complete sentences are predicted. Narrative chains (and their aforementioned extensions) differ in two relevant aspects from the script representations used in our study. Instead of using paraphrase sets, events are represented as typed dependency relations between a verb and one of its dependents (the protagonist). Another difference is that narrative chains are intended to be learned automatically from large collections of unannotated text. By contrast, the script representations used in our study are learned from crowdsourced sequences of event descriptions, which are more focuse"
L18-1512,P10-1100,1,0.904319,"ariety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdsourcing alternative descriptions of an activity type in terms of sequences of short, telegram-style natural-language event descriptions (ED). Then, paraphrase sets are induced automatically as clusters of EDs, using multiple sequence alignment (Regneri et al., 2010) or semi-supervised clustering (Wanzare et al., 2016). In order to tap the potential of script knowledge in text understanding, systems must be able to link ev"
L18-1512,D15-1195,0,0.231744,"compatibility. 7. Background and Related Work Our work is based on script representations in which events are encoded as paraphrase sets. There also exist other research directions on modeling script knowledge. The most prominent alternative representation of script knowledge is that of narrative chains, proposed by Chambers and Jurafsky (2008) and subsequently extended by Chambers and Jurafsky (2009), Pichotta and Mooney (2014) and (Ahrendt and Demberg, 2016), to name but a few. Narrative chains 3245 have been used for event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi et al., 2016) or the related story cloze task (Mostafazadeh et al., 2016; Pichotta and Mooney, 2016), in which complete sentences are predicted. Narrative chains (and their aforementioned extensions) differ in two relevant aspects from the script representations used in our study. Instead of using paraphrase sets, events are represented as typed dependency relations between a verb and one of its dependents (the protagonist). Another difference is that narrative chains are intended to be learned automatically from large collections of unannotated text. By contrast, the script representat"
L18-1512,L16-1556,1,0.91254,"events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdsourcing alternative descriptions of an activity type in terms of sequences of short, telegram-style natural-language event descriptions (ED). Then, paraphrase sets are induced automatically as clusters of EDs, using multiple sequence alignment (Regneri et al., 2010) or semi-supervised clustering (Wanzare et al., 2016). In order to tap the potential of script knowledge in text understanding, systems must be able to link event mentions in texts to the corresponding event types of a script, as indicated by the dotted lines in Figure 2. To our knowledge, Ostermann et al. (2017) is the only existing work on this text-to-script mapping task. Their approach is based on RKP-style script representations. Using this representation, the identification of the correct event type of an event mention is in many cases reduced to a simple identity check 3240 GET_ TREE GET_ TOOLS - buy a tree - get a tree from the store -…"
L18-1512,W17-0901,1,0.787181,"t does not have access to script knowledge will probably not be able to draw this inference. BUY INGREDIENTS CHOOSE RECIPE GET INGREDIENTS MIX INGREDIENTS BAKE WAIT I stirred all ingredients well and baked the batter in the oven. I then waited for a while. Figure 2: An example for text-to-script mapping in the BAKING A CAKE scenario Script knowledge has been shown to be useful for a variety of tasks that are required for text understanding, such as event prediction (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Modi et al., 2016), event ordering (Modi and Titov, 2014), paraphrasing (Wanzare et al., 2017) or discourse referent prediction (Modi et al., 2017). One important aspect in which models that make use of script knowledge differ is the representation of events. Chambers and Jurafsky (2009), for instance, use a “shallow” surface-oriented representation of events which is based on the verb of an event-denoting clause. A different approach has been proposed by Regneri et al. (2010) (henceforth, RKP), who adopt a richer representation of events in terms of paraphrase sets, as depicted in Figure 1. Script knowledge of this kind is acquired by first crowdsourcing alternative descriptions of an"
L18-1564,P09-1068,0,0.0612194,"spects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each ques"
L18-1564,P16-1223,0,0.0634014,"computed in the same way. We use different weight matrices for a, t and q, respectively. A combined representation p for the text–question pair is then constructed using a bilinear transformation matrix W: p = t> Wq (1) We compute a score for each answer by using the dot product and pass the scores for both answers through a softmax layer for prediction. The probability p for an answer a to be correct is thus defined as: Attentive Reader The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus (Hermann et al., 2015; Chen et al., 2016). We use the model formulation by Chen et al. (2016) and Lai et al. (2017), who employ bilinear weight functions to compute both attention and answer-text fit. Bidirectional GRUs are used to encode questions, texts and answers into hidden representations. For a question q and an answer a, the last state of the GRUs, q and a, are used as representations, while the text is encoded as a sequence of hidden states t1 ...tn . We then compute an attention score sj for each hidden state tj using the question representation q, a weight matrix Wa , and an attention bias b. Last, a text representation t"
L18-1564,P17-1147,0,0.0871477,"Missing"
L18-1564,D17-1082,0,0.139289,"Missing"
L18-1564,W14-1606,1,0.892066,"to tell who ate the food: Rachel or the waitress. In contrast, if we utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney,"
L18-1564,L16-1555,1,0.938737,"scribe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section 2.1.). In Section 2.2., we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk1 (henceforth MTurk). Section 2.3. gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section 2.4. gives statistics about the final dataset. 2.1. Pilot Study As a starting point for our pilots, we made use of texts from the InScript corpus (Modi et al., 2016), which provides stories centered around everyday situations (see Section 2.2.2.). We conducted three different pilot studies to determine the best way of collecting questions that require inference over commonsense knowledge: The most intuitive way of collecting reading comprehension questions is to show texts to workers and let them formulate questions and answers on the texts, which is what we tried internally in a first pilot. Since our focus is to provide an evaluation framework for inference over commonsense knowledge, we manually assessed the number of questions that indeed require comm"
L18-1564,K16-1008,1,0.87344,"i and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of approx. 14,00"
L18-1564,D14-1162,0,0.0796453,"Missing"
L18-1564,E14-1024,0,0.0215786,"ly, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx."
L18-1564,P16-1027,0,0.0137069,"(Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of"
L18-1564,D16-1264,0,0.0964981,"Missing"
L18-1564,P10-1100,1,0.940718,"e utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connectio"
L18-1564,D13-1020,0,0.338565,"Missing"
L18-1564,W17-2623,0,0.0482171,"rdson et al., 2013), BAbI (Weston et al., 2015), the Children’s Book Test (CBT, Hill et al. (2015)), CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)), and RACE (Lai et al., 2017). These datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understand3572 ing, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text’s title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge re"
L18-1564,L16-1556,1,0.888375,"rio and that can be answered from different texts (about that scenario), but for which a text does not need to provide the answer explicitly. The next section will describe the mode of collection chosen for the final dataset, based on the third pilot, in more detail. 2.2. Data Collection 2.2.1. Scenario Selection As mentioned in the previous section, we decided to base the question collection on script scenarios rather than specific texts. As a starting point for our data collection, we use 1 www.mturk.com scenarios from three script data collections (Regneri et al., 2010; Singh et al., 2002; Wanzare et al., 2016). Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios. 2.2.2. Texts For the collection of texts, we followed Modi et al. (2016), where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequently, the texts are syntactically simple, facilitating ma"
L18-1564,W17-0901,1,0.858709,"knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks."
L18-1564,P08-1090,0,\N,Missing
L18-1641,Q13-1032,0,0.746971,"with some scored answers. Automatic scoring of short answer exercises is a challenge: in contrast to multiple choice or simple gap-filling exercises, SAS has to assess the semantic correctness of answers, and it is thus related to fields such as natural language understanding, paraphrase detection or textual entailment. In addition, it has to deal with noisy user input containing spelling and grammar errors. Most approaches to SAS consider automatic scoring as a classification task, relying on supervised machine learning (ML) techniques which require manually labeledtraining data In contrast Basu et al. (2013),Brooks et al. (2014) and Horbach et al. (2014) have focused on the use of clustering techniques for SAS. The rationale behind this procedure is that answers that are similar to each other – and therefore end up in the same cluster – are also likely to receive the same label for scoring and can thus be scored in one grading step by a teacher. Ideally, a teacher has to label only a single answer representative for the whole cluster which is then propagated to all members of the cluster. An added value of clustering is that it provides valuable structural information, while ML classifiers just a"
L18-1641,W15-0610,0,0.0180655,"estion, but generalizes to different prompts (Mohler et al., 2011; Meurers et al., 2011). Approaches that do not compare to a target answer build one classifier per prompt using features based on the content of individual answers, such as lemma or character n-grams or dependency triples occurring in an answer. With our work, we follow the feature extraction of the second approach by creating feature vectors for clustering representing the content of each answer and clustering answers per prompt. Our work is related to studies that address the number of answers needed as training data for SAS. Heilman and Madnani (2015) show that – as in many ML scenarios – the SAS task profits from larger training set sizes. Active learning (AL) is an ML technique that aims at selecting training instances in such a way that the classifier can learn most from them (Settles, 2010); AL methods have been used successfully in a variety of NLP tasks. In the SAS context, Horbach and Palmer (2016) explore active learning methods to select ML training instances and find that uncertainty sampling methods have an advantage over random sampling of training instances. Within the field of clustering for SAS, the most prominent contributi"
L18-1641,W16-0535,1,0.475596,"the second approach by creating feature vectors for clustering representing the content of each answer and clustering answers per prompt. Our work is related to studies that address the number of answers needed as training data for SAS. Heilman and Madnani (2015) show that – as in many ML scenarios – the SAS task profits from larger training set sizes. Active learning (AL) is an ML technique that aims at selecting training instances in such a way that the classifier can learn most from them (Settles, 2010); AL methods have been used successfully in a variety of NLP tasks. In the SAS context, Horbach and Palmer (2016) explore active learning methods to select ML training instances and find that uncertainty sampling methods have an advantage over random sampling of training instances. Within the field of clustering for SAS, the most prominent contribution is the Powergrading (PG) study by Basu et al. (2013) who use k-medoids and LDA clustering for answers to US citizenship exam questions. They learn a similarity metric between answers on a part of the data which contains gold standard information about semantic equivalence between answers. As features for this decision, they use various kinds of similaritie"
L18-1641,horbach-etal-2014-finding,1,0.942028,"of short answer exercises is a challenge: in contrast to multiple choice or simple gap-filling exercises, SAS has to assess the semantic correctness of answers, and it is thus related to fields such as natural language understanding, paraphrase detection or textual entailment. In addition, it has to deal with noisy user input containing spelling and grammar errors. Most approaches to SAS consider automatic scoring as a classification task, relying on supervised machine learning (ML) techniques which require manually labeledtraining data In contrast Basu et al. (2013),Brooks et al. (2014) and Horbach et al. (2014) have focused on the use of clustering techniques for SAS. The rationale behind this procedure is that answers that are similar to each other – and therefore end up in the same cluster – are also likely to receive the same label for scoring and can thus be scored in one grading step by a teacher. Ideally, a teacher has to label only a single answer representative for the whole cluster which is then propagated to all members of the cluster. An added value of clustering is that it provides valuable structural information, while ML classifiers just assign a score (Brooks et al., 2014). As an exam"
L18-1641,W11-2401,0,0.17907,"systems using various kinds of automatically extracted features. ML-based approaches can be generally classified in two ways: those that assess an answer based on the similarity with some sort of teacher-specified target answer and those where a target answer is not used or not even available. Approaches of the first type create feature vectors for each answer that express their similarity to the target answer based on semantic similarity or lexical overlap. The resulting classifier is not restricted to answers for just one question, but generalizes to different prompts (Mohler et al., 2011; Meurers et al., 2011). Approaches that do not compare to a target answer build one classifier per prompt using features based on the content of individual answers, such as lemma or character n-grams or dependency triples occurring in an answer. With our work, we follow the feature extraction of the second approach by creating feature vectors for clustering representing the content of each answer and clustering answers per prompt. Our work is related to studies that address the number of answers needed as training data for SAS. Heilman and Madnani (2015) show that – as in many ML scenarios – the SAS task profits fr"
L18-1641,P11-1076,0,0.128522,"Pulman, 2005), to ML systems using various kinds of automatically extracted features. ML-based approaches can be generally classified in two ways: those that assess an answer based on the similarity with some sort of teacher-specified target answer and those where a target answer is not used or not even available. Approaches of the first type create feature vectors for each answer that express their similarity to the target answer based on semantic similarity or lexical overlap. The resulting classifier is not restricted to answers for just one question, but generalizes to different prompts (Mohler et al., 2011; Meurers et al., 2011). Approaches that do not compare to a target answer build one classifier per prompt using features based on the content of individual answers, such as lemma or character n-grams or dependency triples occurring in an answer. With our work, we follow the feature extraction of the second approach by creating feature vectors for clustering representing the content of each answer and clustering answers per prompt. Our work is related to studies that address the number of answers needed as training data for SAS. Heilman and Madnani (2015) show that – as in many ML scenarios –"
L18-1641,W15-0615,0,0.766004,"practise religion, the freedom of religion, . . . }, {to bear arms, the right to bare arms, right to arms, . . . }. The first two clusters contain correct answers referring to different facts, the last one contains answers making the same error. Teachers may use the output clusters to identify common misconceptions among students and assign feedback to whole groups of answers. Some amount of human scoring is required for both supervised ML and clustering: annotation of training data in the one case and annotation of representative cluster members as a basis for propagation in the other case. Zesch et al. (2015) compared the performance of clustering with that of ML methods, keeping the number of manually labeled items constant. They carried out their study on the PG dataset and in addition on the ASAP dataset (see below, Section 3.). They reported that clustering proved beneficial only on the short and simple answers (a few words) of the PG dataset. On the ASAP dataset with more complex, longer answers, clustering falls far behind ML methods in their experiments. In this paper, we show that semi-supervised clustering can substantially improve clustering results. While existing clustering approaches"
P00-1066,J91-4003,0,0.0176864,"-calculus to model the semantics of words like book and library, in particular their behavior in the context of quanti cation and cardinality statements. 1 Introduction The treatment of lexical ambiguity is one of the main problems in lexical semantics and in the modeling of natural language understanding. Pustejovsky's framework of the Generative Lexicon"" made a contribution to the discussion by employing the concept of type coercion, thus replacing the enumeration of readings by the systematic context-dependent generation of suitable interpretations, in the case of systematic polysemies (Pustejovsky, 1991; Pustejovsky, 1995). Also, Pustejovsky pointed to a frequent and important phenomenon in lexical semantics, which at rst sight looks as another case of polysemy, but is signi cantly di erent in nature. (1) The book is blue/on the shelf. (2) Mary burned the book. (3) The book is amusing. (4) Mary understands the book. (5) The book is beautiful. (6) Mary likes the book. (7) Mary read the book. Examples (1)-(4) suggest an inherent ambiguity of the common noun book : blue, on the shelf, and burn subcategorize for a physical object, while amusing and understand require an informational object as a"
P03-1068,P98-1013,0,0.0248009,"e most serious bottlenecks for language technology. To train tools for the acquisition of semantic information for such lexica, large, extensively annotated resources are necessary. In this paper, we present current work of the SALSA (SAarbr¨ucken Lexical Semantics Annotation and analysis) project, whose aim is to provide such a resource and to investigate efficient methods for its utilisation. In the current project phase, the focus of our research and the backbone of the annotation are semantic role relations. More specifically, our role annotation is based on the Berkeley FrameNet project (Baker et al., 1998; Johnson et al., 2002). In addition, we selectively annotate word senses and anaphoric links. The TIGER corpus (Brants et al., 2002), a 1.5M word German newspaper corpus, serves as sound syntactic basis. Besides the sparse data problem, the most serious problem for corpus-based lexical semantics is the lack of specificity of the data: Word meaning is notoriously ambiguous, vague, and subject to contextual variance. The problem has been recognised and discussed in connection with the SENSEVAL task (Kilgarriff and Rosenzweig, 2000). Annotation of frame semantic roles compounds the problem as it"
P03-1068,H94-1020,0,0.0480689,"quisition of word-semantic information, e.g. the construction of domainindependent lexica. The backbone of the annotation are semantic roles in the frame semantics paradigm. We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation. 1 Introduction Corpus-based methods for syntactic learning and processing are well-established in computational linguistics. There are comprehensive and carefully worked-out corpus resources available for a number of languages, e.g. the Penn Treebank (Marcus et al., 1994) for English or the NEGRA corpus (Skut et al., 1998) for German. In semantics, the situation is different: Semantic corpus annotation is only in its initial stages, and currently only a few, mostly small, corpora are available. Semantic annotation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative (Kilgarriff, 2001), a notable exception being the Prague Treebank (Hajiˇcov´a, 1998) . As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical methods to extract semantic regularities from raw corpora, often u"
P03-1068,C98-1013,0,\N,Missing
P10-1097,P93-1016,0,0.0283255,"Missing"
P10-1097,P98-2127,0,0.530918,"distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapat"
P10-1097,J90-1003,0,0.0611287,"se dimensions correspond to pairs of a relation and a word. Recall that any vector of V1 can be represented as a finite sum of the form ∑ ai~er,w0 with appropriate scalar factors ai . In this vector space we define the first-order vector [w] of a word w as follows: [w] = ∑ ω(w, r, w0 ) ·~er,w0 r∈R w0 ∈W where ω is a function that assigns the dependency triple (w, r, w0 ) a corresponding weight. In the simplest case, ω would denote the frequency in a corpus of dependency trees of w occurring together with w0 in relation r. In the experiments reported below, we use pointwise mutual information (Church and Hanks, 1990) instead as it proved superior to raw frequency counts: pmi(w, r, w0 ) = log p(w, w0 |r) p(w |r)p(w0 |r) We further consider a similarly defined vector space V2 , spanned by an orthonormal basis {~er,r0 ,w0 |r, r0 ∈ R, w0 ∈ W }. Its dimensions therefore correspond to triples of two relations and a word. Evidently this is a higher dimensional space than V1 , which therefore can be embedded into V2 by the “lifting maps” Lr : V1 ,→ V2 defined by Lr (~er0 ,w0 ) := ~er,r0 ,w0 (and by linear extension therefore on all vectors of V1 ). Using these lifting maps we define the second-order vector [[w]]"
P10-1097,de-marneffe-etal-2006-generating,0,0.00414648,"Missing"
P10-1097,P08-2008,0,0.0103235,"of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by Schütze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms oth"
P10-1097,D09-1046,0,0.130457,"le. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct two experiments to assess the suitability of our method. Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). It will show that our method significantly outperforms other unsupervised methods that have been proposed in the literature to rank words with respect to their semantic similarity in a given linguistic context. In a second experiment, we apply our model to the “word sense similarity task” recently proposed by Erk and McCarthy (2009), which is a refined variant of a word-sense disambiguation task. The results show a substantial positive effect. Plan of the paper. We will first review related work in Section 2, before presenting our model in Section 3. In Sections 4 and 5 we evaluate our model on the two different tasks. Section 6 concludes. 2 Related Work Several approaches to contextualize vector representations of word meaning have been proposed. One common approach is to represent the meaning of a word a in context b simply as the sum, or centroid of a and b (Landauer and Dumais, 1997). Kintsch (2001) considers a varia"
P10-1097,D08-1094,0,0.824911,"the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares. Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Assoc"
P10-1097,W09-0208,0,0.292215,"Missing"
P10-1097,J03-4004,0,0.0589275,"expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of context words for a given target word provide invariant representations averaging over all di"
P10-1097,P08-1028,0,0.910717,"ng over all different usages of the target word. There is no obvious way to distinguish the different senses of e.g. acquire in different contexts, such as acquire knowledge or acquire shares. Several approaches for word-sense disambiguation in the framework of distributional semantics have been proposed in the literature (Schütze, 1998; McCarthy and Carroll, 2003). In contrast to these approaches, we present a method to model the mutual contextualization of words in a phrase in a compositional way, guided by syntactic structure. To some extent, our method resembles the approaches proposed by Mitchell and Lapata (2008) and Erk and Padó (2008). We go one step further, however, in that we employ syntactically enriched vector models as the basic meaning representations, assuming a vector space spanned by combinations of dependency relations and words (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Ann"
P10-1097,J07-2002,0,0.0413469,"ds (Lin, 1998). This allows us to model the semantic interaction between the meaning of a head word and its dependent at the micro-level of relation-specific cooccurrence frequencies. It turns out that the benefit to precision is considerable. Using syntactically enriched vector models raises problems of different kinds: First, the use 948 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by Schütze (1998); in a syntactic setting,"
P10-1097,D08-1048,0,0.015881,"Missing"
P10-1097,J98-1004,0,0.922668,"rocessing is highly inefficient and expensive. Co-occurrence-based semantic vector models offer an attractive alternative. In the standard approach, word meaning is represented by feature vectors, with large sets of context words as dimensions, and their co-occurrence frequencies as values. Semantic similarity information can be acquired using unsupervised methods at virtually no cost, and the information gained is soft and gradual. Many NLP tasks have been modelled successfully using vector-based models. Examples include information retrieval (Manning et al., 2008), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. Standard vector-space models have serious limitations, however: While semantic information is typically encoded in phrases and sentences, distributional semantics, in sharp contrast to logic-based semantics, does not offer any natural concept of compositionality that would allow the semantics of a complex expression to be computed from the meaning of its parts. A different, but related problem is caused by word-sense ambiguity and contextual variation of usage. Frequency counts of context words for a given target word provide"
P10-1097,W09-2506,1,0.904909,"l Linguistics, pages 948–957, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics of syntax increases dimensionality and thus may cause data sparseness (Padó and Lapata, 2007). Second, the vectors of two syntactically related words, e.g., a target verb acquire and its direct object knowledge, typically have different syntactic environments, which implies that their vector representations encode complementary information and there is no direct way of combining the information encoded in the respective vectors. To solve these problems, we build upon previous work (Thater et al., 2009) and propose to use syntactic second-order vector representations. Second-order vector representations in a bag-ofwords setting were first used by Schütze (1998); in a syntactic setting, they also feature in Dligach and Palmer (2008). For the problem at hand, the use of second-order vectors alleviates the sparseness problem, and enables the definition of vector space transformations that make the distributional information attached to words in different syntactic positions compatible. Thus, it allows vectors for a predicate and its arguments to be combined in a compositional way. We conduct tw"
P10-1097,S07-1009,0,\N,Missing
P10-1097,C98-2122,0,\N,Missing
P10-1100,P98-1013,0,0.0249075,"Missing"
P10-1100,N03-1003,0,0.0131605,"sually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together with the temporal ordering constraints. Jones and Thompson (2003) describe an approach to identifying different natural language realizations for the same event considering the temporal structure of a scenario. However, they don’t aim to acquire or represent the temporal structure of the whole script in the end. In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). Unlike Barzilay and Lee, we do not tackle the general paraphrase problem, but only consider whether two phrases describe the same event in the context of the same 3 Scripts Before we delve into the technical details, let us establish some terminology. In this paper, we distinguish scenarios, as classes of human activities, from scripts, which are stereotypical models of the internal structure of these activities. Where EATING IN A RESTAURANT is a scenario, the script describes a number of events, such as ordering and leaving, that must occur in a certain order in order to constitute an EATIN"
P10-1100,D08-1073,0,0.016894,"al script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we b"
P10-1100,P08-1090,0,0.655159,"al script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we b"
P10-1100,P09-1068,0,0.571795,"ur system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information a"
P10-1100,P07-2044,0,0.0085379,"n early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns differ"
P10-1100,W03-0418,0,0.0610793,"Missing"
P10-1100,P06-1095,0,0.0099398,"nd Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together"
P10-1100,D08-1027,0,0.0700099,"Missing"
P10-1100,C98-1013,0,\N,Missing
P10-1100,J82-1004,0,\N,Missing
P15-1123,W02-1503,0,0.0183354,"Missing"
P15-1123,W14-5201,0,0.0403221,"Missing"
P15-1123,W14-4921,1,0.543693,"s are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic"
P15-1123,W15-1603,1,0.850578,"neric in ACE-2. The class also contains mentions of an entity whose identity would be ‘difficult to locate’ (Officials reported ...). Moreover, annotators are asked to mark truly ambiguous cases that have both a generic and a non-generic reading as USP. Finally, NEG (negated) marks negatively quantified entities that refer to the empty set of the kind mentioned. While we agree that in general there are underspecified cases, the guidelines for ACE-2005 mix other phenomena into the USP class, resulting in a high confusion between USP and both of the labels SPC and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagge"
P15-1123,W13-2313,0,0.0668184,"notators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NP"
P15-1123,P07-1113,0,0.504399,"infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a rule-based approach, which extracts only bare plurals and singular NPs quantified with every or any as generic. Reiter and Frank (2010) use a wide range of syntactic and semantic features to train a supervised classifier for identifying generic NPs. We compare to their method (described in detail in Section 5.2) as a highlycompetitive baseline. Palmer et al. (2007) classify clauses into several types of situation entities including states, events, generalizing sentences (habitual utterances referring to specific individuals) and generic sentences. They find that using context by using the labels of preceding clauses as features improves the classification of clause types, but generic sentences are extremely sparse in their data set. Our present approach uses a sequence labeling model that computes the best labeling for an entire sequence. 3 WikiGenerics: Data and Annotations In order to study generics in a genre other than news (as in ACE), we turn to a"
P15-1123,P14-5010,0,0.00367574,"adcast news subsections.4 Due to low frequency, we omit instances of NEG in our experiments, and apply a three-way classification task (GEN, SPC, USP). We present results for all remaining 40106 mentions and for the subset of 18029 subject mentions, each time using 10-fold CV. 5.2 Baseline: Local Classifier The system for identifying generic NPs of Reiter and Frank (2010), henceforth R&F, makes use of the English ParGram LFG grammar for the XLE parser (Butt et al., 2002). As this grammar is not publicly available, we implement a similar system using exclusively the Stanford CoreNLP toolsuite (Manning et al., 2014), the Celex database of English nouns (Baayen et al., 1996) and WordNet (Fellbaum, 1999). Our system is based on dkpro (de Castilho and Gurevych, 2014). We extract the features listed in Table 1 based on the POS tags and syntactic dependencies assigned by the Stanford parser (Klein and Manning, 2002). We could not reimplement several tense- and aspect-related ParGram-specific features. In order to compensate for this, we add an additional feature (tense) with finer-grained tense and voice information, using the rules described by Loaiciga et al. (2014). Other additional features did not improv"
P15-1123,poesio-artstein-2008-anaphoric,0,0.0263288,"and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which are generic. Automatic Identification of Genericity. Suh et al. (2006) propose a r"
P15-1123,W04-0210,0,0.192349,"sulting in a high confusion between USP and both of the labels SPC and GEN in the manual annotations (Friedrich et al., 2015). Data from two annotators is available, and we compute an agreement of Cohen’s κ = 0.53 over the four labels. The ACE corpora consist only of news data, and the distributions of labels are highly skewed towards specific mentions. For some criticism of the ACE annotation scheme, see also Suh (2006). Several linguistically motivated annotation studies targeting genericity of noun phrases bear similarity to our annotation scheme (Section 3), but comprise very little data (Poesio, 2004; Herbelot and Copestake, 2009). In the ARRAU corpus (Poesio and Artstein, 2008), about 24321 markables are tagged for genericity. Nedoluzhko (2013) survey the treatment of genericity phenomena within coreference resolution research; they find a consistent definition of genericity to be lacking. Friedrich and Palmer (2014b) present an annotation scheme for situation types including generic sentences, which they find to be infrequent in their corpus consisting of news, jokes and (fund-raising) letters. Our new WikiGenerics corpus contains more than 10,000 clauses, approximately half of which ar"
P15-1123,D10-1048,0,0.0642741,"Missing"
P15-1123,P10-1005,0,0.258565,"hrough the improvement of textual entailment methods, and for systems acquiring machine-readable knowledge from text. Machinereadable knowledge bases have different representations for statements corresponding to generic knowledge about kinds and knowledge about specific individuals. The non-generic sentence (1b) roughly speaking provides ABox content for a machine-readable knowledge base, i.e., knowledge about particular instances, e.g, “A is an instance of B / has property X”. In contrast, the generic sentence (1a) feeds the TBox, i.e., knowledge of the form “All B are C / have property X”. Reiter and Frank (2010) provide a detailed discussion of the relevance of the distinction between classes and instances for automatic ontology construction. In this paper, we present a new corpus annotated in a linguistically motivated way for genericity, and a context-sensitive computational model for labeling sequences of clauses or noun phrases (NPs) with their genericity status. Both manual annotation and automatic recognition of generic expressions are challenging tasks: virtually all NP types – definites, indefinites and quantified NPs, full NPs, pronouns, and even proper names (e.g. species names such as Elep"
P15-1123,N03-1030,0,0.181362,"ally occur in subject position in English. Since generics in non-subject positions are less frequent and hard to interpret (see the discussion of “dependent generics” by Link (1995)), we decided to annotate subject NPs only. We are aware that we are missing relevant cases (e.g. the less preferred reading 1 The WikiGenerics corpus is freely available at: www.coli.uni-saarland.de/projects/sitent 1274 of Cats chase mice, which attributes to mice the property of being chased by cats), but in this work, we want to study the “easier” subject cases as a first step. We use the discourse parser SPADE (Soricut and Marcu, 2003) to automatically segment the first 70 sentences of each article into clauses. Each clause is manually annotated with the following information (for more details on the annotation scheme, see (Friedrich et al., 2015)): • Task NP: whether or not the subject NP of the clause refers to a class or kind (generic vs. nongeneric); • Task Cl: whether the clause is generic, defined as a clause that makes a characterizing statement about a class or kind, or non-generic. • Task Cl+NP: using the information from Task NP and Cl above, we automatically derive the following classification for each clause (co"
P15-1123,loaiciga-etal-2014-english,0,\N,Missing
P16-1166,J92-4003,0,0.549144,"including tokenization, POS tagging (Toutanova et al., 2003) and dependency parsing (Klein and Manning, 2002) using the UIMA-based DKPro framework (Ferrucci and Lally, 2004; Eckart de Castilho and Gurevych, 2014). A-pos: part-of-speech tags. These features count how often each POS tag occurs in a clause. A-bc: Brown cluster features. UT07 relies mostly on words and word/POS tag pairs. These simple features work well on the small Brown data set, but the approach quickly becomes impractical with increasing corpus size. We instead turn to distributional information in the form of Brown clusters (Brown et al., 1992), which can be learned from raw text and represent word classes in a hierarchical way. Originally developed in the context of n-gram language modeling, they aim to assign words to classes such that the average mutual information of the words in the clusters is maximized. We use existing, freely-available clusters trained on news data by Turian et al. (2010) using the implementation by Liang (2005).2 Clusterings with 320 and 1000 Brown clusters work best for our task. We use one feature per cluster, counting how often a word in the clause was assigned to this cluster (0 for most clusters). B-mv"
P16-1166,E12-1027,0,0.432546,"Missing"
P16-1166,W14-5201,0,0.0746748,"Missing"
P16-1166,W14-4921,1,0.935726,"s expanded by Palmer et al. (2007) to include three additional types: R EPORT, Q UES TION and I MPERATIVE. The latter two categories were added to accommodate exhaustive annota1 Corpora, annotation manual and code available at www.coli.uni-saarland.de/projects/sitent FACT: Objects of knowledge. I know that Mary refused the offer. P ROPOSITION: Objects of belief. I believe that Mary refused the offer. Figure 2: Abstract Entity SE types. tion of text; R EPORT is a subtype of event for attributions of quoted speech. Two parts of a clause provide important information for determining the SE type (Friedrich and Palmer, 2014b): a clause’s main verb and its main referent. The latter is loosely defined as the main entity that the segment is about; in English this is usually the subject. For example, main referents of G ENERIC S ENTENCEs are kinds or classes as in “Elephants are huge”, while the main referents of Eventualities and G ENERALIZ ING S ENTENCEs are particular individuals (“John is short”). For English, the main verb is the non-auxiliary verb ranked highest in the dependency parse (e.g. “kiss” in “John has kissed Joe”). S TATEs and E VENTs differ in the fundamental lexical aspectual class (Siegel and McKe"
P16-1166,W15-1603,1,0.903298,"Missing"
P16-1166,D15-1294,1,0.85804,"ical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation classification. The inventory of event classes, described in detail in the TimeML annotation guidelines (Saur´ı et al., 2006), combines semantic (REPORTING, PERCEPTION), aspectual (ASPECTUAL, STATE, OCCURRENCE), and intensional (I ACTION, I STATE) properties of events. Finally, there are close connections to systems which predict genericity of noun phrases (Reiter and Frank, 2010; Friedrich and Pinkal, 2015a), and habituality of clauses (Mathew and Katz, 2009; Friedrich and Pinkal, 2015b). 4 Data sets The experiments presented in this paper make use of two data sets labeled with SE types. Brown data. This data set consists of 20 texts from the popular lore section of the Brown corpus (Francis and Kuˇcera, 1979), manually segmented into 4391 clauses and marked by two annotators in corpus MASC Wikipedia tokens 357078 148040 SEs 30333 10607 Fleiss’ κ 0.69 0.66 Table 1: SE-labeled corpora: size and agreement. SE type S TATE E VENT R EPORT G ENERIC G ENERALIZING Q UESTION I MPERATIVE undecided MASC 4"
P16-1166,P15-1123,1,0.920896,"ical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation classification. The inventory of event classes, described in detail in the TimeML annotation guidelines (Saur´ı et al., 2006), combines semantic (REPORTING, PERCEPTION), aspectual (ASPECTUAL, STATE, OCCURRENCE), and intensional (I ACTION, I STATE) properties of events. Finally, there are close connections to systems which predict genericity of noun phrases (Reiter and Frank, 2010; Friedrich and Pinkal, 2015a), and habituality of clauses (Mathew and Katz, 2009; Friedrich and Pinkal, 2015b). 4 Data sets The experiments presented in this paper make use of two data sets labeled with SE types. Brown data. This data set consists of 20 texts from the popular lore section of the Brown corpus (Francis and Kuˇcera, 1979), manually segmented into 4391 clauses and marked by two annotators in corpus MASC Wikipedia tokens 357078 148040 SEs 30333 10607 Fleiss’ κ 0.69 0.66 Table 1: SE-labeled corpora: size and agreement. SE type S TATE E VENT R EPORT G ENERIC G ENERALIZING Q UESTION I MPERATIVE undecided MASC 4"
P16-1166,ide-etal-2008-masc,0,0.230728,"Missing"
P16-1166,P10-2013,0,0.34781,"Missing"
P16-1166,P14-2085,1,0.775096,"s expanded by Palmer et al. (2007) to include three additional types: R EPORT, Q UES TION and I MPERATIVE. The latter two categories were added to accommodate exhaustive annota1 Corpora, annotation manual and code available at www.coli.uni-saarland.de/projects/sitent FACT: Objects of knowledge. I know that Mary refused the offer. P ROPOSITION: Objects of belief. I believe that Mary refused the offer. Figure 2: Abstract Entity SE types. tion of text; R EPORT is a subtype of event for attributions of quoted speech. Two parts of a clause provide important information for determining the SE type (Friedrich and Palmer, 2014b): a clause’s main verb and its main referent. The latter is loosely defined as the main entity that the segment is about; in English this is usually the subject. For example, main referents of G ENERIC S ENTENCEs are kinds or classes as in “Elephants are huge”, while the main referents of Eventualities and G ENERALIZ ING S ENTENCEs are particular individuals (“John is short”). For English, the main verb is the non-auxiliary verb ranked highest in the dependency parse (e.g. “kiss” in “John has kissed Joe”). S TATEs and E VENTs differ in the fundamental lexical aspectual class (Siegel and McKe"
P16-1166,R09-1035,0,0.0283688,"Missing"
P16-1166,loaiciga-etal-2014-english,0,0.0236599,"2010) using the implementation by Liang (2005).2 Clusterings with 320 and 1000 Brown clusters work best for our task. We use one feature per cluster, counting how often a word in the clause was assigned to this cluster (0 for most clusters). B-mv: main verb. Using dependency parses, we extract the verb ranked highest in the clause’s parse as the main verb, and extract the set of features listed in Table 3 for that token. Features based on WordNet (Fellbaum, 1998) use the most frequent sense of the lemma. Tense and voice information is extracted from sequences of POS tags using a set of rules (Loaiciga et al., 2014). Linguistic indicators (Siegel and McKeown, 2000) are features collected per verb type over a large parsed background corpus, encoding how often a verb type occurred with each linguistic marker, e.g., in past tense or with an in-PP. We use values collected from Gigaword (Graff et al., 2003); these are freely available at our project web site (Friedrich and Palmer, 2014a). B-mr: main referent. We extract the grammatical subject of the main verb (i.e., nsubj or nsubjpass) as the clause’s main referent. While the main verb must occur within the clause, the 2 http://metaoptimize.com/projects/ wor"
P16-1166,P10-1005,0,0.207583,"well as a number of lexical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation classification. The inventory of event classes, described in detail in the TimeML annotation guidelines (Saur´ı et al., 2006), combines semantic (REPORTING, PERCEPTION), aspectual (ASPECTUAL, STATE, OCCURRENCE), and intensional (I ACTION, I STATE) properties of events. Finally, there are close connections to systems which predict genericity of noun phrases (Reiter and Frank, 2010; Friedrich and Pinkal, 2015a), and habituality of clauses (Mathew and Katz, 2009; Friedrich and Pinkal, 2015b). 4 Data sets The experiments presented in this paper make use of two data sets labeled with SE types. Brown data. This data set consists of 20 texts from the popular lore section of the Brown corpus (Francis and Kuˇcera, 1979), manually segmented into 4391 clauses and marked by two annotators in corpus MASC Wikipedia tokens 357078 148040 SEs 30333 10607 Fleiss’ κ 0.69 0.66 Table 1: SE-labeled corpora: size and agreement. SE type S TATE E VENT R EPORT G ENERIC G ENERALIZING Q UESTION"
P16-1166,J00-4004,0,0.849584,"nd Palmer, 2014b): a clause’s main verb and its main referent. The latter is loosely defined as the main entity that the segment is about; in English this is usually the subject. For example, main referents of G ENERIC S ENTENCEs are kinds or classes as in “Elephants are huge”, while the main referents of Eventualities and G ENERALIZ ING S ENTENCEs are particular individuals (“John is short”). For English, the main verb is the non-auxiliary verb ranked highest in the dependency parse (e.g. “kiss” in “John has kissed Joe”). S TATEs and E VENTs differ in the fundamental lexical aspectual class (Siegel and McKeown, 2000) of their main verbs (e.g. dynamic in “She filled the glass with water” vs. stative in “Water fills the glass”). While fundamental lexical aspectual class is a word-sense level attribute of the clause’s main verb, habituality is a property of the entire clause which is helpful to determine the clause’s SE type. For example, E VENT and G ENERALIZ ING S ENTENCE differ in habituality (e.g. episodic in “John cycled to work yesterday” vs. habitual in “John cycles to work”). Like habituality, SE types are a categorization at the clause level. Properties of the clause such as modals, negation, or the"
P16-1166,N13-1091,0,0.0709463,"Missing"
P16-1166,N03-1030,0,0.0706951,"Missing"
P16-1166,N03-1033,0,0.0222387,"thods used in our approach, which models SE type labeling as a supervised sequence labeling task. 5.1 Feature sets Our feature sets are designed to work well on large data sets, across genres and domains. Features are grouped into two sets: A consists of standard NLP features including POS tags and Brown clusters. Set B targets SE labeling, focusing on syntacticsemantic properties of the main verb and main referent, as well as properties of the clause which indicate its aspectual nature. Texts are pre-processed with Stanford CoreNLP (Manning et al., 2014), including tokenization, POS tagging (Toutanova et al., 2003) and dependency parsing (Klein and Manning, 2002) using the UIMA-based DKPro framework (Ferrucci and Lally, 2004; Eckart de Castilho and Gurevych, 2014). A-pos: part-of-speech tags. These features count how often each POS tag occurs in a clause. A-bc: Brown cluster features. UT07 relies mostly on words and word/POS tag pairs. These simple features work well on the small Brown data set, but the approach quickly becomes impractical with increasing corpus size. We instead turn to distributional information in the form of Brown clusters (Brown et al., 1992), which can be learned from raw text and"
P16-1166,P07-1113,1,0.717502,"subtypes. Eventualities comprise E VENT and S TATE, categories for clauses representing actual happenings, states of the world, or attributes of entities or situations. General Statives include G ENERIC S ENTENCE and G ENERALIZING S EN TENCE and reflect regularities in the world or general information predicated over classes or kinds. Finally, Abstract Entities (Figure 2) have the subtypes FACT and P ROPOSITION. Although Abstract Entities are part of the label inventory for UT07, we treat them in a separate identification step, for reasons discussed in Section 7. The inventory was expanded by Palmer et al. (2007) to include three additional types: R EPORT, Q UES TION and I MPERATIVE. The latter two categories were added to accommodate exhaustive annota1 Corpora, annotation manual and code available at www.coli.uni-saarland.de/projects/sitent FACT: Objects of knowledge. I know that Mary refused the offer. P ROPOSITION: Objects of belief. I believe that Mary refused the offer. Figure 2: Abstract Entity SE types. tion of text; R EPORT is a subtype of event for attributions of quoted speech. Two parts of a clause provide important information for determining the SE type (Friedrich and Palmer, 2014b): a cl"
P16-1166,P10-1040,0,0.0274098,"words and word/POS tag pairs. These simple features work well on the small Brown data set, but the approach quickly becomes impractical with increasing corpus size. We instead turn to distributional information in the form of Brown clusters (Brown et al., 1992), which can be learned from raw text and represent word classes in a hierarchical way. Originally developed in the context of n-gram language modeling, they aim to assign words to classes such that the average mutual information of the words in the clusters is maximized. We use existing, freely-available clusters trained on news data by Turian et al. (2010) using the implementation by Liang (2005).2 Clusterings with 320 and 1000 Brown clusters work best for our task. We use one feature per cluster, counting how often a word in the clause was assigned to this cluster (0 for most clusters). B-mv: main verb. Using dependency parses, we extract the verb ranked highest in the clause’s parse as the main verb, and extract the set of features listed in Table 3 for that token. Features based on WordNet (Fellbaum, 1998) use the most frequent sense of the lemma. Tense and voice information is extracted from sequences of POS tags using a set of rules (Loaic"
P16-1166,P15-2092,0,0.0665933,"Missing"
P16-1166,zarcone-lenci-2008-computational,0,0.337346,"cribe four categories as lexical properties of verbs, distinguishing states from three types of events (accomplishment, achievement, and activity), differing according to temporal and aspectual properties (e.g. telicity and punctuality). The work of Siegel and McKeown (2000) is a major inspiration in computational work on modeling these linguistic phenomena, introducing the use of linguistic indicators (see Section 5.1). Hermes et al. (2015) model Vendler classes computationally on a verbtype level for 95 different German verbs, combining distributional vectors with supervised classification. Zarcone and Lenci (2008) investigate both supervised and unsupervised classification frameworks for occurrences of 28 Italian verbs, and Friedrich and Palmer (2014a) predict lexical aspectual class for English verbs in context. The only previous approach to automatic classification of SE types comes from Palmer et al. (2007). This system (UT07) uses word and POS tag features as well as a number of lexical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation clas"
P97-1053,P92-1005,0,0.0302641,"ly specified form. During the last few years, the phenomenon of semantic underspecification, i.e. the incomplete availability of semantic information in processing, has received increasing attention. Several aspects of underspecification have been focussed upon, motivated mainly by computational considerations: the ambiguity and openness of lexical meaning (Pustejovsky, 1995; Copestake and Briscoe, 1995), referential underspecification (Asher, 1993), structural semantic underspecification caused by syntactic ambiguities (Egg and Lebeth, 1995), and by the underdetermination of scope relations (Alshawi and Crouch, 1992; Reyte, 1993). In addition, external factors such as insufficient coverage The challenge is to integrate a treatment of parallelism with underspecification, such as in cases of the interaction of scope and ellipsis. Problematic examples like (4) have been brought to attention by (Hirschbuehler, 1982). The example demonstrated that earlier treatments of ellipsis based on copying of the content of constituents are insufficient for such kinds of parallelism. 1The research reported in this paper has been supported by the SFB 378 at the UniversitS.t des Saarlandes and the Esprit Working Group CCL"
P97-1053,C96-1024,1,0.864931,"Missing"
P97-1053,C96-1073,0,0.0942414,"constraints alone do not provide us with a treatment of parallelism. An idea that seems to come close to our notion of equality up-to constraints is the co-indexing technique in (Reyle, 1995), where non-local forms of parallelism are treated by dependency marking on labels. We believe that our use of a separate constraint language is more transparent. A treatment for ellipsis interpretation which uses a form of higher-order unification has been proposed in (Dalrymple, Shieber, and Pereira, 1991) and extended to other kinds of parallel constructions by (Gardent, Kohlhase, and van Leusen, 1996; Gardent and Kohlhase, 1996). Though related in some respects, there are formal differences and differences in coverage between this approach and the one we propose. They use an algorithm for higher-order matching rather than context unification and they do not distinguish an object and m e t a language level. As a consequence they need to resort to additional machinery for the treatment of scope relations, such as Pereira's scoping calculus, described in (Shieber, Pereira, and Dalrymple, 1996). An implementation of a semi-decision procedure for context unification has been carried out by Jordi L6vy, and we applied it su"
perera-etal-2008-clios,W05-0803,0,\N,Missing
perera-etal-2008-clios,W07-1803,1,\N,Missing
perera-etal-2008-clios,C00-2139,0,\N,Missing
perera-etal-2008-clios,W07-1806,0,\N,Missing
perera-etal-2008-clios,P04-1060,0,\N,Missing
perera-etal-2008-clios,P01-1067,0,\N,Missing
perera-etal-2008-clios,P03-1011,0,\N,Missing
perera-etal-2008-clios,J03-1002,0,\N,Missing
perera-etal-2008-clios,A00-2033,0,\N,Missing
Q13-1003,W11-2503,0,0.0687814,"bach  , Dominikus Wetzel ∗ , Stefan Thater ∗ , Bernt Schiele  and Manfred Pinkal ∗ ∗ Department of Computational Linguistics, Saarland University, Saarbr¨ucken, Germany (regneri|dwetzel|stth|pinkal)@coli.uni-saarland.de  Max Planck Institute for Informatics, Saarbr¨ucken, Germany (rohrbach|schiele)@mpi-inf.mpg.de Abstract grounding meaning in visual information, in particular by extending the distributional approach to jointly cover texts and images (Feng and Lapata, 2010; Bruni et al., 2011). As a clear result, visual information improves the quality of distributional models. Bruni et al. (2011) show that visual information drawn from images is particularly relevant for concrete common nouns and adjectives. A natural next step is to integrate visual information from videos into a semantic model of event and action verbs. Psychological studies have shown the connection between action semantics and videos (Glenberg, 2002; Howell et al., 2005), but to our knowledge, we are the first to provide a suitable data source and to implement such a model. The contribution of this paper is three-fold: Recent work has shown that the integration of visual information into text-based models can subs"
Q13-1003,P11-1020,0,0.0111386,"n the landscape of related work (Sec. 2), then we introduce our corpus (Sec. 3). Sec. 4 reports our action similarity annotation experiment and Sec. 5 introduces the similarity measures we apply to the annotated data. We outline the results of our evaluation in Sec. 6, and conclude the paper with a summary and directions for future work (Sec. 7). 2 Related Work A large multimodal resource combining language and visual information resulted from the ESP game (von Ahn and Dabbish, 2004). The dataset contains many images tagged with several one-word labels. The Microsoft Video Description Corpus (Chen and Dolan, 2011, MSVD) is a resource providing textual descriptions of videos. It consists of multiple crowd-sourced textual descriptions of short video snippets. The MSVD corpus is much larger than our corpus, but most of the videos are of relatively low quality and therefore too challenging for state-ofthe-art video processing to extract relevant information. The videos are typically short and summarized with a single sentence. Our corpus contains coherent textual descriptions of longer video sequences, where each sentence is associated with a timeframe. Gupta et al. (2009) present another useful resource:"
Q13-1003,N12-1094,0,0.00938379,"th visual information, by incorporating features from article illustrations. They achieve better results when incorporating the visual information, providing an enriched model that pairs a single text with a picture. Bruni et al. (2011) used the ESP game data to create a visually grounded semantic model. Their results outperform purely text-based models using visual information from pictures for the task of modeling noun similarities. They model single words, and mostly visual features lead only to moderate improvements, which might be due to the mixed quality and random choice of the images. Dodge et al. (2012) recently investigated which words can actually be grounded in images at all, producing an automatic classifier for visual words. An interesting in-depth study by Mathe et al. (2008) automatically learnt the semantics of motion verbs as abstract features from videos. The study captures 4 actions with 8-10 videos for each of the actions, and would need a perfect object recognition from a visual classifier to scale up. Steyvers (2010) and later Silberer and Lapata (2012) present an alternative approach to incorporating visual information directly: they use so-called feature norms, which consist"
Q13-1003,D09-1046,0,0.0381859,"hich vegetable was prepared). We asked the annotators explicitly to ignore the actor of the action (e.g. whether it is a man or a woman) and score the similarities of the underlying actions rather than their verbalizations. Each subject rated all 900 pairs, which were shown to them in completely random order, with a different order for each subject. We compute inter-annotator agreement (and the forthcoming evaluation scores) using Spearman’s rank correlation coefficient (ρ), a non-parametric test which is widely used for similar evaluation tasks (Mitchell and Lapata, 2008; Bruni et al., 2011; Erk and McCarthy, 2009). Spearman’s ρ evaluates how the samples are ranked relative to each other rather than the numerical distance between the rankings. Fig. 5 shows the average similarity ratings in the different settings and the inter-annotator agreement. The average inter-rater agreement was ρ = 0.73 (averaged over pairwise rater agreements), with pairwise results of ρ = 0.77, 0.72, and 0.69, respectively, which are all highly significant at p < 0.001. As expected, pairs with the same activity and object are rated very similar (4.19) on average, while the similarity of different activities on the same object is"
Q13-1003,N10-1011,0,0.00751241,"tion in videos. However, the corpus contains no natural language texts. The connection between natural language sentences and videos has so far been mostly explored 26 by the computer vision community, where different methods for improving action recognition by exploiting linguistic data have been proposed (Gupta and Mooney, 2010; Motwani and Mooney, 2012; Cour et al., 2008; Tzoukermann et al., 2011; Rohrbach et al., 2012b, among others). Our resource is intended to be used for action recognition as well, but in this paper, we focus on the inverse effect of visual data on language processing. Feng and Lapata (2010) were the first to enrich topic models for newspaper articles with visual information, by incorporating features from article illustrations. They achieve better results when incorporating the visual information, providing an enriched model that pairs a single text with a picture. Bruni et al. (2011) used the ESP game data to create a visually grounded semantic model. Their results outperform purely text-based models using visual information from pictures for the task of modeling noun similarities. They model single words, and mostly visual features lead only to moderate improvements, which mig"
Q13-1003,P08-1028,0,0.00670692,"but we noted the relevant kitchen task (i.e. which vegetable was prepared). We asked the annotators explicitly to ignore the actor of the action (e.g. whether it is a man or a woman) and score the similarities of the underlying actions rather than their verbalizations. Each subject rated all 900 pairs, which were shown to them in completely random order, with a different order for each subject. We compute inter-annotator agreement (and the forthcoming evaluation scores) using Spearman’s rank correlation coefficient (ρ), a non-parametric test which is widely used for similar evaluation tasks (Mitchell and Lapata, 2008; Bruni et al., 2011; Erk and McCarthy, 2009). Spearman’s ρ evaluates how the samples are ranked relative to each other rather than the numerical distance between the rankings. Fig. 5 shows the average similarity ratings in the different settings and the inter-annotator agreement. The average inter-rater agreement was ρ = 0.73 (averaged over pairwise rater agreements), with pairwise results of ρ = 0.77, 0.72, and 0.69, respectively, which are all highly significant at p < 0.001. As expected, pairs with the same activity and object are rated very similar (4.19) on average, while the similarity"
Q13-1003,W11-0126,0,0.00903921,"ey use so-called feature norms, which consist of human associations for many given words, as a proxy for general perceptual information. Because this model is trained and evaluated on those feature norms, it is not directly comparable to our approach. The Restaurant Game by Orkin and Roy (2009) grounds written chat dialogues in actions carried out in a computer game. While this work is outstanding from the social learning perspective, the actions that ground the dialogues are clicks on a screen rather than real-world actions. The dataset has successfully been used to model determiner meaning (Reckman et al., 2011) in the context of the Restaurant Game, but it is unclear how this approach could scale up to content words and other domains. 3 The TACOS Corpus We build our corpus on top of the “MPII Cooking Composite Activities” video corpus (Rohrbach et al., 2012b, MPII Composites), which contains videos of different activities in the cooking domain, e.g., preparing carrots or separating eggs. We extend the existing corpus with multiple textual descriptions collected by crowd-sourcing via Amazon Mechanical Turk1 (MTurk). To facilitate the alignment of sentences describing activities with their proper vide"
Q13-1003,D12-1130,0,0.0415458,"d mostly visual features lead only to moderate improvements, which might be due to the mixed quality and random choice of the images. Dodge et al. (2012) recently investigated which words can actually be grounded in images at all, producing an automatic classifier for visual words. An interesting in-depth study by Mathe et al. (2008) automatically learnt the semantics of motion verbs as abstract features from videos. The study captures 4 actions with 8-10 videos for each of the actions, and would need a perfect object recognition from a visual classifier to scale up. Steyvers (2010) and later Silberer and Lapata (2012) present an alternative approach to incorporating visual information directly: they use so-called feature norms, which consist of human associations for many given words, as a proxy for general perceptual information. Because this model is trained and evaluated on those feature norms, it is not directly comparable to our approach. The Restaurant Game by Orkin and Roy (2009) grounds written chat dialogues in actions carried out in a computer game. While this work is outstanding from the social learning perspective, the actions that ground the dialogues are clicks on a screen rather than real-wo"
Q13-1003,I11-1127,1,0.381911,"Missing"
Q13-1003,J13-3003,0,\N,Missing
Q13-1003,P09-1002,0,\N,Missing
Q17-1003,N16-1067,1,0.720394,"depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution duri"
Q17-1003,J10-4006,0,0.0235405,"ree-valued feature indicates whether the previous mention of the candidate DR d is a pronoun, a non-pronominal noun phrase, or has never been observed before. 4.2.2 Selectional Preferences Feature The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as"
Q17-1003,P14-1023,0,0.0594907,"ture The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of larg"
Q17-1003,P08-1090,0,0.722488,"uman expectations. • testing the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz"
Q17-1003,P09-1068,0,0.121207,"the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Sc"
Q17-1003,E14-1006,1,0.888764,"chank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, en"
Q17-1003,E12-1034,0,0.242357,"Missing"
Q17-1003,W14-1606,1,0.882589,"), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure i"
Q17-1003,L16-1555,1,0.929396,"have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative texts. Participants were asked to write about a specific activity (e.g., a restaurant visit, a bus ride, or a grocery shopping event) which they personally experienced, and they were instructed to tell the story as if explaining the activity to a child. This resulted in stories that are centered around a specific scenario and that explicitly mention mundane details. Thus, they generally realize longer event chains associated with a single script, which makes them particularly appropriate"
Q17-1003,K16-1008,1,0.856447,"or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications f"
Q17-1003,N16-1098,0,0.0419051,"that were used; specifically, we would expect that larger predictability effects might be observable at script boundaries, rather than within a script, as is the case in our stories. A next step in moving our participant prediction model towards NLP applications would be to replicate our modelling results on automatic textto-script mapping instead of gold-standard data as done here (in order to approximate human level of processing). Furthermore, we aim to move to more complex text types that include reference to several scripts. We plan to consider the recently published ROC Stories corpus (Mostafazadeh et al., 2016), a large crowdsourced collection of topically unrestricted short and simple narratives, as a basis for these next steps in our research. Acknowledgments We thank the editors and the anonymous reviewers for their insightful suggestions. We would like to thank Florian Pusse for helping with the Amazon Mechanical Turk experiment. We would also like to thank Simon Ostermann and Tatjana Anikina for helping with the InScript corpus. This research was partially supported by the German Research Foundation (DFG) as part of SFB 1102 ‘Information Density and Linguistic Encoding’, European Research Counc"
Q17-1003,N15-1082,0,0.014094,"anical Turk experiment (Figure 2), our referent prediction model is asked to guess the upcoming DR. relation r, we collect all the predicates in the training set which have the participant type p in the position r. The embedding of the DR xp,r is given by the average embedding of these predicates. The feature is computed as the dot product of xp,r and the word embedding of the predicate v. Predicate schemas The following feature captures a specific aspect of knowledge about prototypical sequences of events. This knowledge is called predicate schemas in the recent co-reference modeling work of Peng et al. (2015). In predicate schemas, the goal is to model pairs of events such that if a DR d participated in the first event (in a specific role), it is likely to participate in the second event (again, in a specific role). For example, in the restaurant scenario, if one observes a phrase John ordered, one is likely to see John waited somewhere later in the document. Specific arguments are not that important (where it is John or some other DR), what is important is that the argument is reused across the predicates. This would correspond to the rule X-subject-of-order → X-subject-of-eat.4 Unlike the previo"
Q17-1003,E14-1024,0,0.0438992,"hoice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown"
Q17-1003,D12-1071,0,0.0196634,"nowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative"
Q17-1003,P10-1100,1,0.908959,"cal event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, i"
Q17-1003,S15-1024,1,0.925336,"Missing"
Q17-1003,W16-2518,1,0.85711,"preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of large corpora (ukWaC, BNC,"
R11-1064,P08-2012,0,0.119345,"p the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), which share roles (frame ele"
R11-1064,P04-1051,1,0.796122,"dered temporally. This guided way of learning script data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scrip"
R11-1064,H92-1045,0,0.0355937,"Missing"
R11-1064,P98-1013,0,0.12353,"variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), which share roles (frame elements) specifying people and objects involved in the events. The supervised approach of Mani et al. (2006) learns temporal event relations from TimeBank (Pustejovsky et al., 2006). All of these approaches rely on elaborate manual annotation efforts, and so it is unclear how they would scale to wide-coverage resources. Chambers and Jurafsky (2008; 2009) exploit coreference chains and co-occurrence frequency of verbs in te"
R11-1064,P03-1054,0,0.00700172,"Missing"
R11-1064,N06-1046,0,0.0281613,"guided way of learning script data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants."
R11-1064,P06-1095,0,0.085404,"Missing"
R11-1064,P10-1124,0,0.0200376,"ipt data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a st"
R11-1064,J93-2004,0,0.0366212,"Missing"
R11-1064,W10-4305,0,0.0136123,"ral information by itself is worthless: high precision loss makes align even worse than the na¨ıve baseline. (6) (7) Overall precision and recall is averaged over all tokens in the annotation. Overall F1 score is then computed as follows: F1 = 2 ∗ precision ∗ recall precision + recall (8) Unlike in coreference resolution, we have the problem that we compare gold-standard annotations against tokens extracted from automatic parses. However, the b3 -metric is only applicable if the gold standard and the test data contain the same set of tokens. Thus we apply b3sys , a variant of b3 introduced by Cai and Strube (2010). b3sys extends the gold standard and the test set such that both contain the same set of tokens. Roughly speaking, every token that appears in the gold standard but not in the test set is copied to the latter and treated as singleton set, and vice versa. See Cai and Strube for details. With the inaccurate parser, noun phrases are often parsed incompletely, missing modifiers or relative clauses. We therefore consider a participant description as equivalent with a gold standard phrase if they have the same head. This relaxed scoring metric evaluates the system realistically by punishing parsing"
R11-1064,P08-1090,0,0.602925,"and computational linguistics, including commonsense reasoning for text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989) and automated storytelling (Swanson and Gordon, 2008). But there is hardly an area where the discrepancy between the felt importance of a type of knowledge and the inability to provide any substantial amount of this knowledge for serious applications is greater. Recently, several groups have tackled the problem using unsupervised methods for learning script-like knowledge from text corpora or data obtained through web experiments (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010). For the first time, they open up a perspective to wide-coverage resources of script knowledge. However, each of these approaches handles only specific aspects of script 463 Proceedings of Recent Advances in Natural Language Processing, pages 463–470, Hissar, Bulgaria, 12-14 September 2011. 1 2 3 4 5 6 7 ESD 1 ESD 2 ESD3 put food on plate open microwave put plate in close microwave press start put food in bowl open door put food inside close door enter time push button ... put food on dish open oven place dish in oven close select desired le"
R11-1064,P09-1068,0,0.111637,"s, including commonsense reasoning for text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989) and automated storytelling (Swanson and Gordon, 2008). But there is hardly an area where the discrepancy between the felt importance of a type of knowledge and the inability to provide any substantial amount of this knowledge for serious applications is greater. Recently, several groups have tackled the problem using unsupervised methods for learning script-like knowledge from text corpora or data obtained through web experiments (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010). For the first time, they open up a perspective to wide-coverage resources of script knowledge. However, each of these approaches handles only specific aspects of script 463 Proceedings of Recent Advances in Natural Language Processing, pages 463–470, Hissar, Bulgaria, 12-14 September 2011. 1 2 3 4 5 6 7 ESD 1 ESD 2 ESD3 put food on plate open microwave put plate in close microwave press start put food in bowl open door put food inside close door enter time push button ... put food on dish open oven place dish in oven close select desired length strengths and weaknesses"
R11-1064,P10-1100,1,0.934831,"Department of Computational Linguistics, Saarland University {regneri,pinkal}@coli.uni-saarland.de † Department of Linguistics, University of Potsdam koller@ling.uni-potsdam.de ‡ Department of Information Science and Language Technology, University of Hildesheim Josef.Ruppenhofer@uni-hildesheim.de Abstract information: Chambers and Jurafsky (2009) learn narrative schemas and their participants; they group verbs into schemas by virtue of shared participants assuming that this is an indicator for being part of the same stereotypical activity, without knowing the actual scenarios. The system of Regneri et al. (2010) learns the temporal order of events occurring in specific stereotypical scenarios, but does not determine participants. In this paper, we present a system that automatically learns sets of participants associated with specific scenarios. We take the approach of Regneri et al. as our starting point. In this earlier work, several experimental subjects described what happens in a given scenario in a web experiment; the system then learns what event descriptions from different subjects refer to the same event, and how they are temporally ordered, using Multiple Sequence Alignment (Durbin et al.,"
R11-1064,H94-1010,0,0.343914,"Missing"
R11-1064,N07-1030,0,0.0286859,"scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), w"
R11-1064,C98-1013,0,\N,Missing
regneri-etal-2014-aligning,I05-5002,0,\N,Missing
regneri-etal-2014-aligning,wang-sporleder-2010-constructing,1,\N,Missing
regneri-etal-2014-aligning,shima-mitamura-2012-diversifiable,0,\N,Missing
regneri-etal-2014-aligning,W11-1208,1,\N,Missing
regneri-etal-2014-aligning,W04-3219,0,\N,Missing
regneri-etal-2014-aligning,E12-1002,0,\N,Missing
regneri-etal-2014-aligning,D12-1058,0,\N,Missing
regneri-etal-2014-aligning,W03-1609,0,\N,Missing
regneri-etal-2014-aligning,W08-2121,0,\N,Missing
regneri-etal-2014-aligning,C04-1051,0,\N,Missing
regneri-etal-2014-aligning,W09-2503,0,\N,Missing
regneri-etal-2014-aligning,W08-2126,1,\N,Missing
regneri-etal-2014-aligning,D09-1040,0,\N,Missing
regneri-etal-2014-aligning,P03-1054,0,\N,Missing
regneri-etal-2014-aligning,P01-1008,0,\N,Missing
regneri-etal-2014-aligning,D08-1021,0,\N,Missing
regneri-etal-2014-aligning,Q13-1003,1,\N,Missing
regneri-etal-2014-aligning,P99-1071,0,\N,Missing
regneri-etal-2014-aligning,C10-1149,0,\N,Missing
regneri-etal-2014-aligning,H05-1066,0,\N,Missing
regneri-etal-2014-aligning,N03-1003,0,\N,Missing
regneri-etal-2014-aligning,D11-1108,0,\N,Missing
regneri-etal-2014-aligning,P08-1089,0,\N,Missing
regneri-etal-2014-aligning,P05-1074,0,\N,Missing
regneri-etal-2014-aligning,W10-4217,0,\N,Missing
regneri-etal-2014-aligning,J03-1002,0,\N,Missing
regneri-etal-2014-aligning,P11-1020,0,\N,Missing
regneri-etal-2014-aligning,P10-1100,1,\N,Missing
regneri-etal-2014-aligning,P08-1116,0,\N,Missing
regneri-etal-2014-aligning,2005.mtsummit-papers.11,0,\N,Missing
regneri-etal-2014-aligning,E12-1073,0,\N,Missing
regneri-etal-2014-aligning,D12-1066,0,\N,Missing
regneri-etal-2014-aligning,D12-1084,1,\N,Missing
regneri-etal-2014-aligning,W04-3206,0,\N,Missing
ruppenhofer-etal-2010-generating,D07-1002,0,\N,Missing
ruppenhofer-etal-2010-generating,burchardt-pennacchiotti-2008-fate,0,\N,Missing
ruppenhofer-etal-2010-generating,W07-1206,0,\N,Missing
ruppenhofer-etal-2010-generating,E09-1026,0,\N,Missing
ruppenhofer-etal-2010-generating,D09-1002,0,\N,Missing
ruppenhofer-etal-2010-generating,P98-1013,0,\N,Missing
ruppenhofer-etal-2010-generating,C98-1013,0,\N,Missing
ruppenhofer-etal-2010-generating,P09-1003,0,\N,Missing
ruppenhofer-etal-2010-generating,J05-1004,0,\N,Missing
ruppenhofer-etal-2010-generating,W09-3003,1,\N,Missing
S13-1041,W08-0913,0,0.329927,"f foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target"
S13-1041,W12-2002,0,0.0562051,"Missing"
S13-1041,W12-2039,0,0.805582,"Missing"
S13-1041,W97-0802,0,0.250549,"for short answer scoring in the context of foreign language learning. 3 Answer-based models sions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target answers, questions and reading texts) using standard NLP tools for sentence splitting and tokenization (both OpenNLP2 ), POS tagging and stemming (both Treetagger (Schmid, 1994)), NP chunking (OpenNLP), and dependency parsing (Zurich Parser (Sennrich et al., 2009)). We use an NE Tagger (Faruqui and Pad´o, 2010) to annotate named entities. Synonyms and semantic types are extracted from GermaNet (Hamp and Feldweg, 1997). For keywords, which serve to give more emphasis to content words in the target answer, we extract all nouns from the target answer. Given that we are dealing with learner language, but do not want to penalize answers for typical learner errors, spellchecking (and subsequent correction of spelling errors) is especially important for this task. Our approach is as follows: we first identify all words from the learner answers that are not accepted by a German spellchecker (aspell3 ). We then check for each word whether the word nevertheless occurs in the target answer, question or reading text."
S13-1041,W11-2401,0,0.706813,"g. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are"
S13-1041,P10-1123,0,0.0696294,"Missing"
S13-1041,E09-1065,0,0.0253111,"be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). S"
S13-1041,P11-1076,0,0.33602,". Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al"
S13-1041,W05-0202,0,0.337334,"proaches to short answer scoring In short answer scoring (SAS) the task is to automatically assign labels to individual learner answers. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is s"
S13-1041,W12-2022,0,0.376269,"r and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are then assessed by determining whether the rel"
S15-1024,P08-1090,0,0.577726,"m the website “Dinners from Hell.” Our results suggest that applying these techniques to a domain-specific dataset may be reasonable way to learn domain-specific scripts. The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et"
S15-1024,P09-1068,0,0.325579,"” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attemp"
S15-1024,P11-1098,0,0.0291101,"lar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for"
S15-1024,J90-1003,0,0.34642,"y exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ord"
S15-1024,de-marneffe-etal-2006-generating,0,0.0309946,"Missing"
S15-1024,E12-1034,0,0.444975,"Missing"
S15-1024,P14-5010,0,0.00819738,"arrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit stories about their terrible restaurant experiences. For an example story, see Figure 1. To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014). Of the 237 stories obtained, we manually filtered out 94 stories that were “off-topic” (e.g., letters to the webmaster, dinners not at restaurants), leaving a total of 143 stories. The average story length is 352 words. 4.1 Annotation For the purposes of evaluation only, we hired four undergraduates to annotate every non-copular verb in each story as either corresponding to an event “related to the experience of eating in a restaurant” (e.g., ordered a steak), “unrelated to the experience of eating in a restaurant” (e.g., answered the phone), or uncertain. We used the WebAnno platform for an"
S15-1024,N04-1041,0,0.0726893,"al models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, eˆ = arg max e∈V k X pmi(ei , e) + i=1 n X pmi(e, ei ) i=k+1 (4) where C(e1 , e2 ) is asymmetric, i.e., C(e1 , e2 ) counts only cases in which e1 occurs before e2 . Second, the bigram probability model: eˆ = arg max e∈V where p(e2 |e1 ) = metric. k Y p(e|ei ) i=1 C(e1 ,e2 ) C(e1 ,∗) n Y p(ei |e) (5) i=k+1 and C(e1 , e2 ) is asymDiscounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by Chambers and Jurafsky (2008). For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting. 207 Document Threshold We include a document threshold parameter, D, that ensures that, in any narrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit"
S15-1024,E14-1024,0,0.534804,"uction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by"
S15-1024,P10-1100,1,0.740772,"Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learnin"
S15-1024,H91-1059,0,0.0679119,"at we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers"
S15-1024,P13-4001,0,0.0436435,"Missing"
S17-1015,D15-1177,0,0.0439185,"Missing"
S17-1015,P16-1191,0,0.00990687,"Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that they assign the same weight to ever"
S17-1015,P16-1101,0,0.00456847,"ord. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word correspond"
S17-1015,J15-4004,0,0.0198179,"Missing"
S17-1015,P12-1092,0,0.0244217,"ptures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most suitable topic of a word"
S17-1015,P15-1010,0,0.0522639,"Nguyen1 , Dat Quoc Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,K16-1008,1,0.408695,"enses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense o"
S17-1015,W14-1606,1,0.616494,"is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense"
S17-1015,N15-1070,0,0.0357356,"Missing"
S17-1015,Q17-1003,1,0.820726,"model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinge"
S17-1015,D14-1113,0,0.0546105,"Missing"
S17-1015,Q15-1016,0,0.0359882,"he word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,t . 1 We use an unigram distribution raised"
S17-1015,Q15-1022,1,0.436169,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D15-1200,0,0.0397679,"t size. In addition, v tor representation of the word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,"
S17-1015,K17-3014,1,0.253101,"we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific"
S17-1015,U15-1014,1,0.855226,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D14-1162,0,0.126941,"ntal results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Co"
S17-1015,W13-3512,0,0.17894,"Missing"
S17-1015,C14-1016,0,0.133176,"Missing"
S17-1015,D16-1174,0,0.425139,"Missing"
S17-1015,C14-1015,0,0.0135853,"Missing"
S17-1015,N15-1058,0,0.0460639,"Missing"
S17-1015,N10-1013,0,0.0136891,"dding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most sui"
S17-1015,P15-1173,0,0.0914243,"Missing"
S17-1015,D15-1036,0,0.0294844,"Missing"
S17-1015,K15-1026,0,0.0326275,"c Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,D13-1170,0,0.00363313,"r words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, appr"
S17-1015,D14-1110,0,\N,Missing
S17-1015,P15-2003,0,\N,Missing
S17-1015,P14-1023,0,\N,Missing
S17-1015,L16-1046,0,\N,Missing
S17-1016,P10-1143,0,0.026874,"narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities"
S17-1016,P09-1068,0,0.160534,"the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et"
S17-1016,L16-1555,1,0.896046,"veryday activities, so called event sequence descriptions (ESDs). ESDs consist of short telegram-style descriptions of single events (event descriptions, ED). The textual order of EDs corresponds to the temporal order of respective events, i.e. temporal information is explicitly encoded. DeScript contains 50 ESDs for each of 40 different scenarios. Alongside the ESDs, it also provides gold event paraphrase sets, i.e. clusters of all event descriptions denoting the same event type, labeled with the respective type. While DeScript is a source of structured script knowledge, the InScript corpus (Modi et al., 2016) provides us with the appropriate kind of narrative texts. InScript is a collection of 910 stories centered around some specific scenario, for 10 of the 40 scenarios in DeScript, e.g. BAKING A CAKE, RIDING A BUS , TAKING A SHOWER . All verbs occurring in the texts are annotated with an event type if they are relevant to the script instantiated by the story; as non-script event otherwise. In the upper part of Fig. 1, you see the initial fragment of a story about baking a cake; together with a script excerpt in the lower part, depicted by labeled event paraphrase sets. I looked up the recipe and"
S17-1016,W14-1606,0,0.175518,"75); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any h"
S17-1016,E14-1006,1,0.837053,"fsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regne"
S17-1016,Q17-1003,1,0.848171,"n that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect expli"
S17-1016,J02-3001,0,0.0278061,"derstanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long"
S17-1016,E12-1034,0,0.415153,"Missing"
S17-1016,D12-1045,0,0.0269025,"ipt event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible. 1 Introduction Event structure is a prominent topic in NLP. While semantic role labelers (Gildea and Jurafsky, 2002; Palmer et al., 2010) are well-established tools for the analysis of the internal structure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to"
S17-1016,P10-1100,1,0.84377,"tructure of event descriptions, modeling relations between events has gained increasing attention in recent years. Research on event coreference (Bejan and Harabagiu, 2010; Lee et al., 2012), temporal event ordering in newswire texts (Ling and Weld, 2010), as well as shared tasks on cross-document event ordering (Minard et al., 2015, inter alia) have in common that they model cross-document relations. The focus of this paper is on the task of analyzing text-internal event structure. We share the view of a long tradition in NLP (see e.g. Schank and Abelson (1975); Chambers and Jurafsky (2009); Regneri et al. (2010)) that script knowledge is of central importance to this task, i.e. common-sense knowledge about events and their typical order in everyday activities (also referred to as scenarios, Barr and Feigenbaum (1981)). Script knowledge guides expectation by predicting which type of event or discourse referent might be addressed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address"
S17-1016,P16-1113,1,0.812832,"h the current script scenario, we employ two features: a binary feature indicating whether the verb is used in the ESDs for the given scenario; and a scenariospecific tf–idf score that is computed by treating all ESDs from a scenario as one document, summed over the verb and its dependents. In Section 4.2, we evaluate models with and without script features, to test the impact of scenario-specific information. Frame Feature. We further employ framesemantic information because we expect script events to typically evoke certain frames.We use a state-of-the-art semantic role labeler (Roth, 2016; Roth and Lapata, 2016) based on FrameNet (Rup2 For EDs, we use all mentioned head nouns. To emphasize the importance of the verb, we double its weight when averaging. 4 Because our CRF model only supports nominal features, we discretize embeddings from code.google.com/ archive/p/word2vec/ by binning the component values into three intervals [−∞, −], [−, ], [, ∞]. The hyperparameter  is determined on a held-out development set. 3 P Lemma Our model Our model (scen. indep.) R F1 0.365 0.949 0.526 0.628 0.817 0.709 0.513 0.877 0.645 Table 1: Identification of script-relevant verbs within a scenario and independent"
S17-1016,S15-1024,1,0.873865,"Missing"
S17-1016,L16-1556,1,0.905957,"ssed next in a story (Modi et al., 2017), allows to infer missing events from events explicitly mentioned (Chambers and Jurafsky, 2009; Jans et al., 2012; Rudinger et al., 2015), and to determine text-internal temporal order (Modi and Titov, 2014; Frermann et al., 2014). We address the task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regneri et al., 2010; Wanzare et al., 2017). The events of the resulting structure are defined as sets of alternative realizations, which cover lexical variation and provide paraphrase information. To the best of our knowledge, these advantages have not been explicitly used elsewhere. Aligning script structures with texts is a complex task. In a first at"
S17-1016,W17-0901,1,0.781511,"task of automatically mapping narrative texts to scripts, which will leverage explicit script knowledge for the afore-mentioned aspects of text understanding, as well as for downstream tasks such as textual entailment, question answering or paraphrase detection. We build on the work of Regneri et al. (2010) and Wanzare et al. (2016), who collect explicit script knowledge via crowdsourcing, by asking people to describe everyday activities. These crowdsourced descriptions form a basis for high-quality automatic extraction of script structure without any human intervention (Regneri et al., 2010; Wanzare et al., 2017). The events of the resulting structure are defined as sets of alternative realizations, which cover lexical variation and provide paraphrase information. To the best of our knowledge, these advantages have not been explicitly used elsewhere. Aligning script structures with texts is a complex task. In a first attempt, we assume that three steps are necessary to solve it, although in the long run, an integrated approach will be preferable: First, the script which is addressed by the event mention must be identified. Second, it has to be decided whether a verb denotes a script event at all. Fina"
S17-1016,S15-2132,0,\N,Missing
S17-1016,J82-1004,0,\N,Missing
S18-1119,D15-1075,0,0.0806933,"Missing"
S18-1119,P08-1090,0,0.586286,"corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine compreh"
S18-1119,P09-1068,0,0.104067,"Missing"
S18-1119,P16-1223,0,0.0969874,"Missing"
S18-1119,P17-1168,0,0.024165,"are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) employs a BiLSTM with attention to find similarities between texts, questions, and answers. Each word is represented based on Numberbatch embeddings, which encode information from ConceptNet. 752 YNU AI1799 (Liu et al., 2018) submitted an ensemble of neural network models based on LSTMs, RNNs, and BiLSTM/CNN combinations, with attention mechanisms. In addition to word2vec embeddings, positional embeddings are used that are generated based on word embeddings. Rank Team name y/n what why who where when 1 2 3 4 5 6 7 8 9 10 11 Yuanfudao MITRE J"
S18-1119,S18-1174,0,0.0156272,"2: The accuracy of participating systems and the two baselines in total, on commonsense-based questions (CS), text-based questions (TXT) and on out-of-domain questions (from the 5 held-out testing scenarios). The best performance for each column is marked in bold print. Significant differences in results between two adjacent lines are marked by an asterisk (* p&lt;0.05) in the upper line. The last line shows the human upper bound (Ostermann et al., 2018) as comparison. (based on ConceptNet). The model is pretrained on another large machine comprehension dataset, namely the RACE corpus. YNU Deep (Ding and Zhou, 2018) test different LSTMs and BiLSTMs variants to encode questions, answers and texts. A simple attention mechanism is applied between question–answer and text–answer pairs. The final submission is an ensemble of five model instances. MITRE (Merkhofer et al., 2018) use a combination of 3 systems - two LSTMs with attention mechanisms, and one logistic regression model using patterns based on the vocabulary of the training set. The two neural models use different word embeddings - one trained on GoogleNews, another one trained on Twitter, which were enriched with word overlap features. Interestingly"
S18-1119,S18-1176,0,0.0287908,"Missing"
S18-1119,S18-1172,0,0.0613083,"Missing"
S18-1119,P17-1147,0,0.0637147,"Missing"
S18-1119,S18-1180,0,0.0405537,"other one trained on Twitter, which were enriched with word overlap features. Interestingly, the simple logistic regression model achieves competitive performance and would have ranked 4th as an individual system. Jiangnan (Xia, 2018) applies a BiLSTM over GloVe and CoVe embeddings (McCann et al., 2017) with an additional attention mechanism. The attention mechanism computes soft word alignment between words in the question and the text or answer. Manual features, including part-of-speech tags, named entitity types, and term frequencies, are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) e"
S18-1119,S18-1120,0,0.229149,"Missing"
S18-1119,S18-1173,0,0.0325421,"Missing"
S18-1119,S18-1181,0,0.0662618,"Missing"
S18-1119,K16-1008,1,0.821428,"ate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine comprehension datasets. In Section 3, we describe"
S18-1119,L16-1555,1,0.854976,"riety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proce"
S18-1119,W14-1606,1,0.907601,"the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the durat"
S18-1119,Q17-1003,1,0.839109,"she thus refers to Rachel. This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems. In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc. (Schank and Abelson, 1975). The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system (Bower et al., 1979; Schank, 1982; Modi et al., 2017). From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative"
S18-1119,N16-1098,0,0.0711233,"mind think about nothing but peaceful, happy thoughts. I stayed in there for only about ten minutes because it was so hot and steamy. When I got out, I turned the sauna off to save energy and took a cool shower. I got out of the shower and dried off. After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home. 2 Q1 Where did they sit inside the sauna? a. on the floor b. on a bench Q2 How long did they stay in the sauna? a. about ten min- b. over thirty utes minutes Figure 1: An example for a text from MCScript with 2 reading comprehension questions. (Mostafazadeh et al., 2016)). These tasks test a system’s ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks. Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension. The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text. In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions r"
S18-1119,L18-1564,1,0.88915,"ms with regard to specific question types and based on whether a question is directly answerable, or only inferable from the text. 4.2 Baselines We provide results of two baseline systems as lower bounds for comparison: a rule-based baseline (Sliding Window) and a neural end-to-end system (Attentive Reader). Both baselines are described in 2 IUCM cluster MCScript texts and try to find answers also in other texts, that are topically similar. In that sense, MCScript itself is used to represent commonsense knowledge. more detail below. For details about the tuning of hyperparameters, we refer to Ostermann et al. (2018). Sliding Window The sliding window baseline is a simple rule-based method that answers a question on a text by predicting the answer option with the highest similarity to the text. The intuition underlying this method is that answers similar to a text should be more plausible than answer options that are different from the text (independent of the question). In our baseline implementation, we compute similarity using a sliding window that compares each answer option to any possible “window” of w tokens of the text. For comparison, each window and each answer is represented by an average vecto"
S18-1119,D14-1162,0,0.0806018,"Missing"
S18-1119,D16-1264,0,0.12393,"atasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of 100,000 questions on Wikipedia articles collected via crowdsourcing. In that dataset, the answer to a question corresponds to a segment/span from the reading passage. Since Wikipedia articles mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reaso"
S18-1119,P10-1100,1,0.886473,"plicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally i"
S18-1119,S18-1179,0,0.0225614,"ion of the softmax function over both answer options for the question: p(a|t, q) = sof tmax(t> Ws a) 5 embeddings (Speer et al., 2017). One participating system made use of script knowledge in the form of event sequence descriptions. Resources commonly used by participants include pretrained word embeddings such as GloVe (Pennington et al., 2014) or word2vec (Mikolov et al., 2013), and preprocessing pipelines such as NLTK4 . In the following, we provide short summaries of the participants’ systems and we give an overview of models and resources used by them (Table 1). Non-neural methods IUCM (Reznikova and Derczynski, 2018) applied an unsupervised approach that assigns the correct answer to a question based on text overlap. Text overlap is computed based on the given passage and text sources of the same topic. Different clustering and topic modeling techniques are used to identify such text sources in MCScript and DeScript. (2) Participants We ran our shared task through the CodaLab platform3 . 24 teams submitted results during the evaluation period, out of which 11 teams provided system descriptions: 8 teams from China, and one team each from Spain, Russia and the US. The full leader board containing all 24 sub"
S18-1119,D13-1020,0,0.150799,"ned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts. For example, consider the short narrative in (1). For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text: Usually, people sit on benches inside a sauna, an Related Work Recently, a number of datasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of"
S18-1119,S15-1024,1,0.874914,"Missing"
S18-1119,D15-1195,0,0.0608887,"Missing"
S18-1119,S18-1175,0,0.0341503,"Missing"
S18-1119,W17-2623,0,0.0302808,"s mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reasoning to solve different tasks. In contrast to our dataset, the artificial texts in BAbI are not reflective of a typically occurring narrative text. Two recently published datasets that also have a larger focus on commonsense reasoning are NewsQA and TriviaQA. NewsQA (Trischler et al., 2017) contains newswire texts from CNN with crowdsourced questions and answers. During the question collection, workers were only presented with the title of the text, and a short summary. This 748 method ensures that literal repetitions of the text are avoided and the generation of non-trivial questions requiring background knowledge is supported. The NewsQA text collection differs from MCScript in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Tr"
S18-1119,W17-0901,1,0.841966,"cript knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper i"
S18-1119,C00-2137,0,0.237567,"Missing"
S18-1119,S18-1177,0,0.0482581,"Missing"
S19-1012,P17-1147,0,0.0906084,"Missing"
S19-1012,Q18-1023,0,0.0576173,"Missing"
S19-1012,D17-1082,0,0.122087,"Missing"
S19-1012,P14-5010,0,0.00332435,"lot studies, we then showed the texts with highlighted target sentences to workers and asked them to write questions about these sentences. We however found, that in many cases, the written questions were too general or nonsensical. We concluded that an even more structured task was required and decided to concentrate on questions of two types: (1) questions that ask about participants, and (2) questions about the temporal event structure of a scenario. Participants are usually instantiated by noun phrases (NPs), while events are described by verb phrases (VPs). We thus used Stanford CoreNLP (Manning et al., 2014) to extract both NPs and VPs in the target sentences and split up the experiment into two parts: In the first part, workers were required to write questions that ask about the given noun phrase. Figure 2 shows a screenshot of an item from the first part. The first column shows the reading text with the target sentence highlighted. The second columns shows all extracted phrases with a field for one question per phrase.3 Full details of the experiment instructions are given in the Supplemental Material. In the second part, we then asked workers to write a temporal question (when, how long, etc.)"
S19-1012,S18-1181,0,0.109565,"Missing"
S19-1012,L16-1555,1,0.896473,"ton et al., 2009), a large corpus of narrative blog stories and identified additional scenarios in these stories. Third, we added new scenarios that are related to existing ones or that extend them. We collected 20 texts per new scenario, using the same text collection method as Ostermann et al. (2018a): We asked workers to tell a story about a certain everyday scenario “as if talking to a child”. This instruction ensures that the resulting stories are simple in language and clearly structured. Texts collected this way have been found to explicitly mention many script events and participants (Modi et al., 2016; Ostermann et al., 2018a). They are thus ideal to evaluate script-based inference. 3.2 Question Collection For the question collection, we followed Ostermann et al. (2018a) in telling workers that the data are collected for a reading comprehension task for children, in order to get linguistically simple and explicit questions. However, as mentioned above, we guide workers towards asking questions about target sentences rather than a complete text. As target sentences, we selected every fourth sentence in a text. In order to avoid selecting target sentences with too much or too little content,"
S19-1012,W14-1606,0,0.0173416,"ts. In the example, script knowledge subsumes the fact that the paying event is a part of the eating in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated"
S19-1012,L18-1564,1,0.343517,"learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding. Our work closes this gap: We present MCScript2.0, a reading comprehension corpus focused on script events and participants. It contains more than 3,400 tex"
S19-1012,S17-1016,1,0.849017,"rio. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding. Our work closes this gap: We present MCScript2.0, a reading comprehension corpus focused on script events"
S19-1012,E14-1024,0,0.0350256,"cts that typically play a role in the situation, referred to as participants. In the example, script knowledge subsumes the fact that the paying event is a part of the eating in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the"
S19-1012,P10-1100,1,0.928525,"ke place during such situations, and their typical temporal order, referred to as events, as well as the persons and objects that typically play a role in the situation, referred to as participants. In the example, script knowledge subsumes the fact that the paying event is a part of the eating in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data"
S19-1012,D15-1195,0,0.133014,"Missing"
S19-1012,W17-2623,0,0.0200773,"evaluate a very broad notion of commonsense, including e.g. physical knowledge (for trivia texts) and knowledge about political facts (for newswire texts). However, none of them explicitly tackle script knowledge. 7 6 Related Work Recent years have seen a number of datasets that evaluate commonsense inference. Like our corpus, most of these data sets choose a machine comprehension setting. The data sets can be roughly classified along their text domain: News Texts. Two recently published machine comprehension data sets that require commonsense inference are based on news texts. First, NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. During data collection, full texts were not shown to workers as a basis for question formulation, but only the text’s title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. Second, ReCoRD (Zhang et al., 2018) contains cloze-style questions on newswire texts that were not crowdsourced, but automatically extracted by pruning a named entity in a larger passage from the text. Web Texts. Other corpora use web docum"
S19-1012,S18-1120,0,0.0123581,"text, question and answers are represented with word embeddings. Bidirectional gated recurrent units (GRUs, Cho et al. (2014)) process the text, question and answers and transform them into sequences of contextualized hidden states. The text is represented as a weighted average of the hidden states with a bilinear attention formulation, and another bilinear weight matrix is used to compute a scalar as score for each answer. For a formalization, we refer to Ostermann et al. (2018a) and Chen et al. (2016). Three-way Attentive Network (TriAN) As third model, we use a three-way attentive network (Wang et al., 2018), the best-scoring model of the shared task on MCScript6 . Various types of in6 Code available at https://github.com/ intfloat/commonsense-rc formation are employed to represent tokens: Word embeddings, part of speech tags, named entity embeddings, and word count/overlap features, similar to the logistic classifier. Three bidirectional LSTM (Hochreiter and Schmidhuber, 1997) modules are used to encode text, question and answers. The resulting hidden representations are reweighted with three attention matrices and then summed into vectors using three self-attention layers. Additionally, token r"
S19-1012,W17-0901,1,0.871616,"in a restaurant scenario. Recent years have seen different approaches to learning script knowledge, centered around two strands: Work around narrative chains that are learned from large text collections (Chambers and Jurafsky, 2008, 2009), and the manual induction of script knowledge via crowdsourcing (Regneri et al., 2010; Wanzare et al., 2016). Script knowledge has been represented both symbolically (Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) and with neural models (Modi and Titov, 2014; Pichotta and Mooney, 2016). Scripts have been evaluated mostly intrinsically (Wanzare et al., 2017; Ostermann et al., 2017). An exception is MCScript (Ostermann et al., 2018a), a reading comprehension corpus with a focus on script knowledge, and a predecessor to the data set presented in this work. Previous work has shown, however, that script knowledge is not required for performing well on the data set (Ostermann et al., 2018b). Hence, to date, there exists no evaluation method that allows one to systematically assess the contribution of models of script knowledge to the task of automated text understanding. Our work closes this gap: We present MCScript2.0, a reading comprehension corpus"
S19-1012,D18-1009,0,0.0729801,"didates per question for the data set. To choose the most plausible correct answer candidate, we adapt the procedure from Ostermann et al. (2018a): We normalize all correct answers (lowercasing, normalizing numbers4 , deleting stopwords5 ) and then merge candidates that are contained in another candidate, and candidate pairs with a Levenshtein (1966) distance of less than 3. The most frequent candidate is then selected as correct answer. If there was no clear majority, we selected a candidate at random. To select an incorrect answer candidate, we adapt the adversarial filtering algorithm from Zellers et al. (2018). Our implementation uses a simple classifier that utilizes shallow surface features. The algorithm selects the incorrect answer candidate from the set of possible candidates that is most difficult for the classifier, i.e. an incorrect answer that is hard to tell apart from the correct 4 answer (e.g. the incorrect answers in Figure 1: eating and utensils are also mentioned in the text). By picking incorrect answers with the adversarial filtering method, the dataset becomes robust against surface-oriented methods. Practically, the algorithm starts with a random assignment, i.e. a random incorre"
S19-1012,E12-1034,0,\N,Missing
S19-1012,P08-1090,0,\N,Missing
S19-1012,P09-1068,0,\N,Missing
S19-1012,S18-1119,1,\N,Missing
W06-0203,burchardt-etal-2006-salto,0,\N,Missing
W09-2506,D08-1094,0,0.742797,"king Paraphrases in Context Stefan Thater Universität des Saarlandes stth@coli.uni-sb.de Georgiana Dinu Universität des Saarlandes dinu@coli.uni-sb.de Abstract argument slots bear selectional preference information (Pantel et al., 2007; Basili et al., 2007). A different line of accounting for contextual variation has been taken by Mitchell and Lapata (2008), who propose a compositional approach, “contextualizing” the vector-space meaning representation of predicates by combining the distributional properties of the predicate with those of its arguments. A related approach has been proposed by Erk and Padó (2008), who integrate selectional preferences into the compositional picture. In this paper, we propose a context-sensitive vector-space approach which draws some important ideas from Erk and Pado’s paper (“E&P” in the following), but implements them in a different, more effective way: An evaluation on the SemEval 2007 lexical substitution task data shows that our model significantly outperforms E&P in terms of average precision. We present a vector space model that supports the computation of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An eval"
W09-2506,W09-0208,0,0.361254,"them to the argument meanings of the argument head nouns in the input sentence. The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restricted vector of the input verb using cosine similarity. In order to compare our model with state of the art, we reimplement E&P’s structured vector space model. We filter stop words, and compute lexical vectors in a “syntactic” space using the most frequent 2000 words from the BNC as basis. We also consider a variant in which the basis corresponds to words indexed by their grammatical roles. We choose parameters that Erk and Padó (2009) report to perform best, and use the method described in Erk and Padó (2009) to compute vectors in context. Generalized average precision (GAP) is a more precise measure than Poot : Applied to a ranking task with about 20 candidates, Poot just gives the percentage of good candidates found in the upper half of the proposed ranking. Average precision is sensitive to the relative position of correct and incorrect candidates in the ranking, GAP moreover rewards the correct order of positive cases w.r.t. their gold standard weight. We define average precision first: AP = Σni=1 xi pi R pi = Σik=1 xk"
W09-2506,P05-1014,0,0.0179912,"finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y ⇔ X throw Y ” is appropriate in a sentence like “a mouse study sheds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of context sensitivity by deriving inference rules whose Plan of the paper. Section 2 presents our model and briefly relates it to previous work. Section 3 describes the evaluation of our model on the lexical substitution task data. Section 4 concludes. 2 A model for meaning in context We propose a dependency-based model whose dimensions reflect dependency relations, and distinguish two kinds or layers of lexical meaning: argument meaning and predicate meaning. The argument meaning of a word w is a vector representing frequencies of all pairs (w0 , r0 ) o"
W09-2506,P93-1016,0,0.0133603,"tances from the dataset for which the target verb or one of its arguments is not in the BNC. We obtain a set of 162 instances for 34 different verbs. We also remove paraphrases that are not in the BNC. On average, target verbs have 20.5 paraphrase candidates, 3.9 of which are correct in specific contexts. Poot GAP Random baseline E&P (target only) E&P (add, object only) E&P (min, both) TDP TDP (target only) 54.25 64.61 (63.31) 66.20 (62.90) 64.86 (59.62) 63.32 62.60 26.03 29.95 (32.02) 29.93 (31.54) 32.22 (31.28) 36.54 33.04 Table 2: Results Experimental setup. We parse the BNC using MiniPar (Lin, 1993) and extract co-occurrence frequencies, considering only dependency relations for the most frequent 2000 verbs. We don’t use raw frequency counts directly but reweight the vectors by pointwise mutual information. To rank paraphrases in context, we compute contextually constrained vectors for the verb in the input sentence and all its paraphrase candidates by taking the corresponding predicate vectors and restricting them to the argument meanings of the argument head nouns in the input sentence. The restricted vectors for the paraphrase candidates are then ranked by comparing them to the restri"
W09-2506,P08-1028,0,0.40197,"Missing"
W09-2506,N07-1071,0,0.0556872,"Missing"
W09-2506,W04-3206,0,0.0299526,"on of appropriate vector representations for words in context, and apply it to a paraphrase ranking task. An evaluation on the SemEval 2007 lexical substitution task data shows promising results: the model significantly outperforms a current state of the art model, and our treatment of context is effective. 1 Manfred Pinkal Universität des Saarlandes pinkal@coli.uni-sb.de Introduction Knowledge about paraphrases is of central importance to textual inference modeling. Systems which support automatic extraction of large repositories of paraphrase or inference rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y ⇔"
W09-2506,P07-1058,0,0.0150323,"nce rules like Lin and Pantel (2001) or Szpektor et al. (2004) thus form first-class candidate resources to be leveraged for NLP tasks like question answering, information extraction, or summarization, and the meta-task of recognizing textual entailment. Existing knowledge bases still suffer a number of limitations, making their use in applications challenging. One of the most serious problems is insensitivity to context. Natural-language inference is highly context-sensitive, the applicability of inference rules depending on word sense and even finer grained contextual distinctions in usage (Szpektor et al., 2007). Application of a rule like “X shed Y ⇔ X throw Y ” is appropriate in a sentence like “a mouse study sheds light on the mixed results,” but not in sentences like “the economy seems to be shedding fewer jobs” or “cats do not shed the virus to other cats.” Systems like the above-mentioned ones base the extraction of inference rules on distributional similarity of words rather than word senses, and apply unconditionally whenever one side of the rule matches on the word level, which may lead to considerable precision problems (Geffet and Dagan, 2005) . Some approaches address the problem of conte"
W09-2506,C00-2137,0,0.109924,"Missing"
W09-2506,S07-1009,0,\N,Missing
W09-2506,W07-1401,0,\N,Missing
W14-3505,P05-1074,0,0.147038,"Missing"
W14-3505,W03-1004,0,0.120097,"Missing"
W14-3505,P01-1008,0,0.216901,"Missing"
W14-3505,D08-1021,0,0.0607471,"Missing"
W14-3505,S13-2100,0,0.0650705,"Missing"
W14-3505,W12-2039,0,0.14856,"Missing"
W14-3505,W97-0802,0,0.290394,"nce, correct LA – text sentence, incorrect LA – text sentence. We increase the training material available by boosting the corpus in several ways. First, to emphasize the importance of lexical identity for learning word alignments, we add trivially63 identical pairs: each reading text sentence paired with itself, and each word in the CREG corpus vocabulary, also paired with itself. Additionally, we repeat non-identical sentence pairs, with the number of repetitions linked to the nature of the sub-corpus in which the pair appears. We have also begun experiments adding word pairs from GermaNet (Hamp and Feldweg, 1997), in order to learn lexical paraphrases, but the results reported here do not include GermaNet-based boosting. For intrinsic evaluation of the detected paraphrase fragments (Section 3.3), we aim to reduce noise in the data and emphasize reliable sentence pairs. Accordingly, each pair involving correct LAs, as well as those with TAs and text sentences, is copied 10 times. Pairs involving incorrect LAs appear just one time. The trivially-identical pairs are entered 10 times for sentences and 20 times for word pairs. Preprocessing. To prepare the data for word alignment, we apply a standard lingu"
W14-3505,S13-1041,1,0.891129,"ur use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our reimplementation of the alignment-based approach by Meurers et al. (2011b). This model uses alignments on different linguistic levels (like words, lemmas, chunks and dependency triples) to align elements in the learner answer to elements in the 61 target answer. Features (e.g. percentage of aligned tokens/c"
W14-3505,W11-2401,0,0.366851,"describe and evaluate paraphrase fragment detection on the CREG corpus in section 3. Section 4 describes and evaluates our use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our reimplementation of the alignment-based approach by Meurers et al. (2011b). This model uses alignments on different linguistic levels (like words, lemmas, chunks and dependency triples) t"
W14-3505,P11-1076,0,0.0279202,"es in section 2, and describe and evaluate paraphrase fragment detection on the CREG corpus in section 3. Section 4 describes and evaluates our use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our reimplementation of the alignment-based approach by Meurers et al. (2011b). This model uses alignments on different linguistic levels (like words, lemmas, chunks and"
W14-3505,P06-1011,0,0.0229672,"s. Following previous work (Wang and Callison-Burch, 2011; Regneri and Wang, 2012), we pass our input corpus to GIZA++ (Och and Ney, 2003) in order to: (a) estimate word alignments for input sentence pairs, and (b) obtain a lexical correspondence table with scores for individual word pairs. Links between aligned words in the sentence pairs are then classified as positive or negative based on their scores, a technique which has previously been applied to extract paraphrase fragments from non-parallel bilingual corpora and has been shown to improve a state of the art machine translation system (Munteanu and Marcu, 2006). Word pairs containing punctuation or stop words are excluded from the alignment prior to scoring.5 Afterwards, the alignment is refined by removing all negatively-scored word pairs, such that only very strong alignments survive. We then smooth the alignment by recomputing scores for each word, averaging over a window of five words. In this way we often capture context words 1 http://opennlp.apache.org/ http://nlp.stanford.edu/software/tokenizer.shtml 3 http://nlp.stanford.edu/software/CRF-NER.shtml 4 http://aspell.net/ 5 http://www.ranks.nl/stopwords/german.html 2 64 that are left out of the"
W14-3505,J03-1002,0,0.0107569,"emantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm originally developed for aligning bilingual 60 parallel texts in Machine Translation (Och and Ney, 2003). The alignment algorithm learns semantic information from the corpus in an unsupervised way, without any labeled training material. Once this semantic information is given, paraphrase fragments are predicted in a robust manner, using no or (in the chunk-based version of the algorithm) only very shallow additional linguistic information. An example for the fragments that are extracted from a learner answer and the corresponding target answer are the bold-print parts of the example in figure 1. We create a parallel corpus using the Corpus of Reading Comprehension Exercises in German (CREG) (Ott"
W14-3505,W05-0202,0,0.0416844,"hich surpasses the state of the art and seems to be appropriate for practical application. The remainder of this paper is structured as follows: we discuss related approaches in section 2, and describe and evaluate paraphrase fragment detection on the CREG corpus in section 3. Section 4 describes and evaluates our use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our r"
W14-3505,W04-3219,0,0.0813885,"Missing"
W14-3505,D12-1084,0,0.0915974,"xical and shallow syntactic information, plus possibly lexical-semantic resources such as WordNet (Fellbaum, 1998), in part with impressively good results (for an overview, see Ziai et al. (2012)). In the present paper, we describe an approach to short answer scoring that uses semantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm originally developed for aligning bilingual 60 parallel texts in Machine Translation (Och and Ney, 2003). The alignment algorithm learns semantic information from the corpus in an unsupervised way, without any labeled training material. Once this semantic information is given, paraphrase fragments are predicted in a robust manner, using no or (in the chunk-based version of the algorithm) only very shallow additional linguistic"
W14-3505,W11-1208,0,0.0935423,"and target answer, mostly using lexical and shallow syntactic information, plus possibly lexical-semantic resources such as WordNet (Fellbaum, 1998), in part with impressively good results (for an overview, see Ziai et al. (2012)). In the present paper, we describe an approach to short answer scoring that uses semantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm originally developed for aligning bilingual 60 parallel texts in Machine Translation (Och and Ney, 2003). The alignment algorithm learns semantic information from the corpus in an unsupervised way, without any labeled training material. Once this semantic information is given, paraphrase fragments are predicted in a robust manner, using no or (in the chunk-based version of the algorithm) only very s"
W14-3505,P08-1089,0,0.0503632,"Missing"
W14-3505,W12-2022,0,0.40292,"ven completely impossible, as can be seen in the learner answer of figure 1. Also, both the target answers and, in particular, the student answers, have a tendency to keep close to the text surface. Therefore, shallow approaches considering only surface information form a strong baseline. Existing approaches to automatic short answer scoring typically rely on alignments between learner and target answer, mostly using lexical and shallow syntactic information, plus possibly lexical-semantic resources such as WordNet (Fellbaum, 1998), in part with impressively good results (for an overview, see Ziai et al. (2012)). In the present paper, we describe an approach to short answer scoring that uses semantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm or"
W15-1603,P10-1143,0,0.0161259,"to locate’ (Officials reported ...). In our opinion, the latter interferes with the definition of SPC as marking cases where the entity referred to is a particular object in the real world, even if the author does not know its identity (At least four people were injured). The breadth of the USP category causes problems with consistency of application (see Section 3). The ACE annotation scheme has also been applied in the Newsreader project.3 The ECB+ corpus (Cybulska and Vossen, 2014) is an extension of EventCorefBank (ECB), a corpus of news articles marked with event coreference information (Bejan and Harabagiu, 2010). ECB+ annotates entity mentions according to ACE-2005, but collapses the three non-GEN labels into a single category. Roughly 12500 event participant mentions are annotated, some doubly and some singly. Agreement statistics for genericity are not reported. 3 www.newsreader-project.eu 23 2.1.2 Other corpora annotated at the NP-level The resources surveyed here apply carefullydefined notions of genericity but are too small to be feasible machine learning training data. The question of whether an NP is generic or not arises in the research context of coreference resolution. Some approaches mark"
W15-1603,bhatia-etal-2014-unified,0,0.0634569,"Missing"
W15-1603,bjorkelund-etal-2014-extended,0,0.0649647,"Missing"
W15-1603,cybulska-vossen-2014-using,0,0.0226472,"is providing new opportunities for women in New Delhi), and cases where the author mentions an entity whose identity would be ‘difficult to locate’ (Officials reported ...). In our opinion, the latter interferes with the definition of SPC as marking cases where the entity referred to is a particular object in the real world, even if the author does not know its identity (At least four people were injured). The breadth of the USP category causes problems with consistency of application (see Section 3). The ACE annotation scheme has also been applied in the Newsreader project.3 The ECB+ corpus (Cybulska and Vossen, 2014) is an extension of EventCorefBank (ECB), a corpus of news articles marked with event coreference information (Bejan and Harabagiu, 2010). ECB+ annotates entity mentions according to ACE-2005, but collapses the three non-GEN labels into a single category. Roughly 12500 event participant mentions are annotated, some doubly and some singly. Agreement statistics for genericity are not reported. 3 www.newsreader-project.eu 23 2.1.2 Other corpora annotated at the NP-level The resources surveyed here apply carefullydefined notions of genericity but are too small to be feasible machine learning train"
W15-1603,doddington-etal-2004-automatic,0,0.0944194,"Missing"
W15-1603,W14-4921,1,0.641791,"vial cases, one of which is the case of nominal modifiers described above. In summary, the ACE scheme problematically fails to treat subject NPs differently from NPs in other syntactic positions, and ‘fuzzy’ points in the guidelines, particularly concerning the USP label, contribute to disagreements between annotators. 4 Annotating genericity as reference to kinds on NP- and clause-level We next present an annotation scheme for marking both clauses and their subject NPs with regard to whether they are generic. Our scheme is primarily motivated by the contributions of clauses to the discourse (Friedrich and Palmer, 2014): do they report on a particular event or state, or do they report on some regularity? These different types of clauses have different entailment properties, and differ in how they contribute to the temporal structure of the discourse. In this work, we focus on separating generic clauses from other types of clauses. We approach the problem from a linguistic perspective rather than focusing on any particular content extraction task, arguing that any generally applicable annotation scheme must be based on solid theoretical foundations. We believe our annotation scheme is a step toward solving th"
W15-1603,W10-1809,0,0.058422,"Missing"
W15-1603,W11-0118,0,0.0551305,"Missing"
W15-1603,N06-2015,0,0.170767,"Missing"
W15-1603,P10-2013,0,0.545796,"ric expressions, in turn facilitating knowledge extraction from natural language text. In this paper we provide the next steps for such an annotation endeavor. Our contributions are: (1) we survey the most important previous projects annotating genericity, focusing on resources for English; (2) with a new agreement study we identify problems in the annotation scheme of the largest currentlyavailable resource (ACE-2005); and (3) we introduce a linguistically-motivated annotation scheme for marking both clauses and their subjects with regard to their genericity. (4) We present a corpus of MASC (Ide et al., 2010) and Wikipedia texts annotated according to our scheme, achieving substantial agreement. 1 Manfred Pinkal1 Making this distinction is important for NLP tasks that aim to disentangle information about particular events or entities from general information about classes, kinds, or particular individuals, such as question answering or knowledge base population. Our present work targets the current lack of a large and satisfactorily-annotated data set for genericity, which is a prerequisite for research aiming to automatically identify these linguistic phenomena. Krifka et al. (1995) report the ce"
W15-1603,I11-1068,0,0.0632525,"phenomena related to genericity on clauses of text. Annotating habituality. Mathew and Katz (2009) conduct a study on automatically distinguishing habitual from episodic sentences. Habitual sentences are taken to be sentences whose main verb is lexically dynamic, but which do not refer to particular events (see for example (3)), and may have generic or non-generic subjects. Their singlyannotated data set, from which they excluded verb types with skewed class distributions, comprises 1052 examples covering 57 verb stems. Their data set is not publicly available. General vs. specific sentences. Louis and Nenkova (2011) describe a method for automatic classification of sentences as general or specific. General sentences are loosely defined as those which make “broad statements about a topic,” while specific sentences convey more detailed information. This distinction is not immediately related to the phenomena treated as generics in the literature. Kind-referring subjects can occur in both general (4a) and specific (4b) sentences; general sentences can also have non-kind-referring subjects (4c). (4) (a) Climatologists and policy makers, ..., need to ponder such complexities... (general) (b) Solid silicon com"
W15-1603,W13-2313,0,0.132507,"Missing"
W15-1603,W04-0210,0,0.626043,"Missing"
W15-1603,P10-1005,0,0.115996,"1 describes corpora from the Automatic Content Extraction (ACE) program (Doddington et 22 al., 2004); other NP-level approaches are described in Section 2.1.2. 2.1.1 ACE entity class annotations The research objective of the ACE program (1999-2008) was the detection and characterization of entities, relations and events in natural text (Linguistic Data Consortium, 2000). All entity mentions receive an entity class label indicating their genericity status. Of the corpora described here, the ACE corpora have been the most widely used for recent research on automatically identifying generic NPs (Reiter and Frank, 2010). The annotation guidelines developed over time; we describe both the initial guidelines of ACE-2 and those from ACE-2005. The ACE-2 corpus (Mitchell et al., 2003) includes 40106 annotated entity mentions in 520 newswire and broadcast documents. The annotation guidelines give no formal definition of genericity; annotators are asked to determine whether each entity refers to “any member of the set in question” (generic) or rather “some particular, identifiable member of that set” (specific/non-generic).1 This leads to a mix of constructions being marked as generic: types of entities (Good stude"
W15-1603,N03-1030,0,0.447667,"Missing"
W15-1603,C14-1100,0,\N,Missing
W15-2702,W14-4921,1,0.740293,"SE types differentiate between clauses describing events, those describing states, and those conveying generic information (for more detail, see Section 4). While these semantic types are language-independent, they differ in their linguistic realizations. Here we perform the first detailed cross-linguistic study of SE types, aiming to understand both the differences in their linguistic characteristics across languages (Section 4.1) and how closely SE types correspond to each other cross-linguistically (Section 5.2). This requires adaptation of an existing annotation scheme for SEs in English (Friedrich and Palmer, 2014) to German. We discuss this adaptation (Section 4.1), including an experiment on the interpretation of The main contribution of this paper is a cross-linguistic empirical analysis of two interacting levels of linguistic analysis of written text: situation entity (SE) types, the semantic types of situations evoked by clauses of text, and discourse modes (DMs), a characterization of passages at the sub-document level. We adapt an existing annotation scheme for SEs in English to be used for German data, with a detailed discussion of the most important differences. We create the first parallel cor"
W15-2702,N03-1030,0,0.0952293,"mized for translation studies (Islam and Mehler, 2012), two documents from the news commentary corpus (WMT 2013 shared task training data2 ), sections from the novels Alice in Wonderland and Anna Karenina from the OPUS collection (Tiedemann, 2012),3 and two texts from a multilingual news website.4 These texts were segmented into clauses manually by one of the authors. English and German segments were also aligned manually. In addition, we use two documents (Sophie’s world and economy) from the Smultron corpus (Volk et al., 2010). We split the English part of Smultron into clauses using SPADE (Soricut and Marcu, 2003), and the German part using a syntax-based discourse segmenter for German.5 The Smultron corpus provides alignments on a token-/phrase-level, but these phrases do not necessarily match the clause segmentation. To align clauses, we first identify the main verb of each English segment using dependency parses (Klein and Manning, 2002). We then align each segment to the German segment containing the verb to which the identified (English) main verb is aligned. For all texts, paragraph segmentation follows the paragraph breaks in the original source texts. Related Work. Unlike genre, a notion of tex"
W15-2702,W15-1603,1,0.878854,"Missing"
W15-2702,islam-mehler-2012-customization,0,0.0165149,"Data This study requires aligned parallel data with different text types. We collect 11 parallel EnglishGerman texts from a variety of sources and produce clause- and paragraph-level alignments for the texts. Table 1 gives statistics on the number of segments, tokens, and paragraphs in each document, as well as aggregate statistics for the corpus. The translation direction differs across documents, and part of the data consists of translations from a third language into both English and German.1 The corpus includes three documents from a version of Europarl customized for translation studies (Islam and Mehler, 2012), two documents from the news commentary corpus (WMT 2013 shared task training data2 ), sections from the novels Alice in Wonderland and Anna Karenina from the OPUS collection (Tiedemann, 2012),3 and two texts from a multilingual news website.4 These texts were segmented into clauses manually by one of the authors. English and German segments were also aligned manually. In addition, we use two documents (Sophie’s world and economy) from the Smultron corpus (Volk et al., 2010). We split the English part of Smultron into clauses using SPADE (Soricut and Marcu, 2003), and the German part using a"
W15-2702,tiedemann-2012-parallel,0,0.0365078,"e texts. Table 1 gives statistics on the number of segments, tokens, and paragraphs in each document, as well as aggregate statistics for the corpus. The translation direction differs across documents, and part of the data consists of translations from a third language into both English and German.1 The corpus includes three documents from a version of Europarl customized for translation studies (Islam and Mehler, 2012), two documents from the news commentary corpus (WMT 2013 shared task training data2 ), sections from the novels Alice in Wonderland and Anna Karenina from the OPUS collection (Tiedemann, 2012),3 and two texts from a multilingual news website.4 These texts were segmented into clauses manually by one of the authors. English and German segments were also aligned manually. In addition, we use two documents (Sophie’s world and economy) from the Smultron corpus (Volk et al., 2010). We split the English part of Smultron into clauses using SPADE (Soricut and Marcu, 2003), and the German part using a syntax-based discourse segmenter for German.5 The Smultron corpus provides alignments on a token-/phrase-level, but these phrases do not necessarily match the clause segmentation. To align clau"
W15-2702,P09-1076,0,0.0132507,"tific background or Contrastive/comparative statements. The key difference is that AZ is a genre-specific approach, and DMs are relevant for most written text genres. Liakata et al. (2013) use AZ to improve summarization of scientific articles, showing that subdocument structure can indeed be useful in downstream applications. Santini (2006) also employ types over passages of text (called simply “text types”), with labels that are partially similar to Smith’s DMs. These text types are then used as building blocks for automatic web genre classification. Palmer and Friedrich (2014), inspired by Webber (2009), investigate the distribution of SE types for various genres of text. In contrast, here we study the distribution of SE types per DM. Re3 Annotating discourse modes This exploratory study takes the first steps toward computational treatment of DMs, resulting in the first corpus of texts labeled with DMs. 1 This metadata is available for each document pair. http://statmt.org/wmt13 3 http://opus.lingfil.uu.se 4 http://globalvoicesonline.org 5 Publication in preparation. 2 13 source text/excerpt OPUS: novels OPUS: novels Europarl Europarl Europarl GlobalVoices GlobalVoices NewsCommentary NewsCom"
W15-2702,D13-1070,0,0.0196569,"follows the paragraph breaks in the original source texts. Related Work. Unlike genre, a notion of text type for entire documents, DMs are an aspect of sub-document structure, and thus are similar to approaches such as Argumentative Zoning (AZ) (Teufel, 2010). AZ analyzes scientific research articles according to the rhetorical functions of their text passages, identifying and labeling passages with categories like General scientific background or Contrastive/comparative statements. The key difference is that AZ is a genre-specific approach, and DMs are relevant for most written text genres. Liakata et al. (2013) use AZ to improve summarization of scientific articles, showing that subdocument structure can indeed be useful in downstream applications. Santini (2006) also employ types over passages of text (called simply “text types”), with labels that are partially similar to Smith’s DMs. These text types are then used as building blocks for automatic web genre classification. Palmer and Friedrich (2014), inspired by Webber (2009), investigate the distribution of SE types for various genres of text. In contrast, here we study the distribution of SE types per DM. Re3 Annotating discourse modes This expl"
W15-2702,C14-1054,0,0.02692,"from a large-scale experimental study; (b) SEs mainly correspond to each other in parallel text, and a large part of the mismatches are systematic; (c) the DM annotation task can be performed intuitively with reasonable agreement; and (d) the annotated DMs show the predicted differences in the distributions of SE types. 1 Introduction There are complex and interwoven relationships between the nature of a text – whether construed as genre, register, text type, discourse mode, or something else – and the linguistic characteristics of the text (Werlich, 1975; Smith, 2003; Biber and Conrad, 2009; Passonneau et al., 2014, among others). Furthermore, these relationships involve phenomena at different levels, from lexical to structural, and from semantic to functional/pragmatic. In this paper we investigate correspondences across two levels of linguistic analysis, for phenomena spanning semantics and discourse, for two languages (English and German). 12 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 12–21, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. lated work for the other subparts of the study is d"
W15-4408,W08-0913,0,0.128746,"with a large number of approaches, cf. (Dagan et al., 2013). MacCartney and Manning (2009a) proposed an extension of the classification schema to a much more fine-grained inventory of 7 semantic relations that expresses additional concepts such as equivalence and reverse entailment and also inspired our label set. In SAS, the task is to assign a student answer a score that specifies the correctness of the answer. Many approaches to SAS compare learner answers given by a student to target answers specified by a teacher and rely on some measure of surface or semantic overlap between them (e.g. Bailey and Meurers (2008); Meurers et al. (2011); Mohler et al. (2011)) or measure whether teacher-specified aspects of a correct answer (so-called facets) are addressed in the learner answer (Nielsen et al., 2008). In SAS corpora, the label for an answer is a binary score, stating whether the LA is correct or incorrect. Some data sets also provide annotations with points from an integer scale (e.g. Mohler and Mihalcea (2009) or the kaggle SAS competition 2 ). Other data use more meaningful diagnostic labels such as Ott et al. (2012) and Bailey and Meurers (2008) that provide feedback to the learner. In our study, we"
W15-4408,P10-4003,0,0.019232,"r to understand the challenges of automatic scoring better, we evaluate which instances in terms of our entailment annotation labels are most problematic for automatic scoring with a binary label. • We will further explore the relation between textual entailment and SAS by comparing, how well features from shortanswer scoring tasks can be used to learn our classification. 2 Related Work Recognizing textual entailment and automatic shortanswer scoring are two related tasks in which 2 50 https://www.kaggle.com/c/asap-sas/data tations. et al., 2008) and physics questions from tutorial dialogues (Dzikovska et al., 2010), i.e. in contrast to our scenario they deal with native speakers – thus avoiding problems in processing learner language – and the questions do not refer to a specific reading text. Most importantly, our perspective on the relation between SAS and RTE also differs from the SemEval definition: The SemEval task uses RTE labels that are constructed from labels assigned by teachers as meaningful feedback to students. They assume that there is a direct mapping from RTE labels to binary teacher scores and construct their binary data set from collapsing those labels. Their approach is backed up by a"
W15-4408,N12-1021,0,0.0150587,"ores for our comparisons. For the RTE task, we see LA and TA as text and hypothesis and expect that entailment will correlate with correctness: While a LA paraphrasing the TA should definitely count as correct, making the LA more specific should not make it incorrect either. However, omitting crucial information from the TA will potentially make the LA incorrect. SemEval-2013 task 7 (Dzikovska et al., 2013b) took a first step in bringing together the RTE and the SAS community in a task to label student answers to explanation and definition questions with 5 RTE-labels. The data set used there (Dzikovska et al., 2012) focuses on science questions (Nielsen mapping. We expect, for example, that, if a LA entails a TA and vice versa at the same time, i.e. if they are paraphrases, then the LA will probably be scored as correct by a teacher. On the other hand, the fact that there is only some partial conceptual overlap between a LA and a TA does not constitute entailment, but is in some instances enough for an answer to be scored as correct by a teacher. We present in this paper the first part of an annotation project that aims at investigating the relationship between SAS and RTE and that compares existing bina"
W15-4408,S13-2045,0,0.18741,"the language they are learning and answer questions about it. With the advent of computerbased language learning courses, the automatic scoring of such shortanswer questions has become an important research topic (for an overview see Burrows et al. (2015); Ziai et al. (2012)), not only for reading and listening comprehension in the context of foreign language learning, but also e.g. in science questions for native speaker students. It has been often noted that the SAS task is related to the task of recognizing textual entailment (RTE, e.g. Mohler et al. (2011), Sukkarieh and Blackmore (2009), Dzikovska et al. (2013b)). RTE is the task to decide whether there is an inference relation between two texts; in the case of SAS, these texts are the learner answer (LA), given by a student, and a teacher-specified target answer (TA, i.e. a sample solution). An entailment 1 All examples are taken from the CREG corpus and translated by the authors preserving linguistic errors whenever possible. 49 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 49–58, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of"
W15-4408,W07-1401,0,0.0644446,"nguistic errors whenever possible. 49 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 49–58, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing text pairs are labeled with the relation between them: The RTE task in its original formulation (Dagan and Glickman, 2004) is a binary classification task deciding whether a text t entails a hypothesis h. The two-way task has been extended to a 3-way task involving the labels Entailed, Contradicted and Unknown (Giampiccolo et al., 2007). Annual RTE shared tasks led to a growing community with a large number of approaches, cf. (Dagan et al., 2013). MacCartney and Manning (2009a) proposed an extension of the classification schema to a much more fine-grained inventory of 7 semantic relations that expresses additional concepts such as equivalence and reverse entailment and also inspired our label set. In SAS, the task is to assign a student answer a score that specifies the correctness of the answer. Many approaches to SAS compare learner answers given by a student to target answers specified by a teacher and rely on some measur"
W15-4408,H05-1049,0,0.0311796,"Missing"
W15-4408,P11-1076,0,0.34339,"n foreign language education: Students read a text in the language they are learning and answer questions about it. With the advent of computerbased language learning courses, the automatic scoring of such shortanswer questions has become an important research topic (for an overview see Burrows et al. (2015); Ziai et al. (2012)), not only for reading and listening comprehension in the context of foreign language learning, but also e.g. in science questions for native speaker students. It has been often noted that the SAS task is related to the task of recognizing textual entailment (RTE, e.g. Mohler et al. (2011), Sukkarieh and Blackmore (2009), Dzikovska et al. (2013b)). RTE is the task to decide whether there is an inference relation between two texts; in the case of SAS, these texts are the learner answer (LA), given by a student, and a teacher-specified target answer (TA, i.e. a sample solution). An entailment 1 All examples are taken from the CREG corpus and translated by the authors preserving linguistic errors whenever possible. 49 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 49–58, c Beijing, China, July 31, 2015. 2015 Associatio"
W15-4408,W12-2039,0,0.0332057,"Missing"
W15-4408,nielsen-etal-2008-annotating,0,0.0558096,"ic relations that expresses additional concepts such as equivalence and reverse entailment and also inspired our label set. In SAS, the task is to assign a student answer a score that specifies the correctness of the answer. Many approaches to SAS compare learner answers given by a student to target answers specified by a teacher and rely on some measure of surface or semantic overlap between them (e.g. Bailey and Meurers (2008); Meurers et al. (2011); Mohler et al. (2011)) or measure whether teacher-specified aspects of a correct answer (so-called facets) are addressed in the learner answer (Nielsen et al., 2008). In SAS corpora, the label for an answer is a binary score, stating whether the LA is correct or incorrect. Some data sets also provide annotations with points from an integer scale (e.g. Mohler and Mihalcea (2009) or the kaggle SAS competition 2 ). Other data use more meaningful diagnostic labels such as Ott et al. (2012) and Bailey and Meurers (2008) that provide feedback to the learner. In our study, we primarily rely on binary correctness scores for our comparisons. For the RTE task, we see LA and TA as text and hypothesis and expect that entailment will correlate with correctness: While"
W15-4408,P13-2080,0,0.0850167,"feedback to students. They assume that there is a direct mapping from RTE labels to binary teacher scores and construct their binary data set from collapsing those labels. Their approach is backed up by a small feasibility study that shows the correspondence of the RTE and SAS label sets in their setting. In our study, we consider RTE and SAS as different tasks and want to explore their relation. We therefore compare labeling from a RTE perspective and scoring from a teacher’s point of view. Both within the context of the SemEval task and already before, RTE approaches have been used for SAS. Levy et al. (2013) try to recognize partial entailment based on the facet approach by Nielsen et al. (2008) and aim at exploring its possible impact on recognizing full entailment relations on learner data as part of the SemEval-2013 task 7. Consequently, they also see the tasks of RTE and SAS as equivalent. In contrast to this, Mohler et al. (2011) present a SAS approach that uses techniques from RTE (e.g. a dependency graph matching approach, cf. Haghighi et al. (2005)), but clearly point out that although their system uses those methods, it cannot be seen as RTE system. 3 3.1 3.2 LA-TA Annotation Scheme The"
W15-4408,W09-3714,0,0.29026,"s, pages 49–58, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing text pairs are labeled with the relation between them: The RTE task in its original formulation (Dagan and Glickman, 2004) is a binary classification task deciding whether a text t entails a hypothesis h. The two-way task has been extended to a 3-way task involving the labels Entailed, Contradicted and Unknown (Giampiccolo et al., 2007). Annual RTE shared tasks led to a growing community with a large number of approaches, cf. (Dagan et al., 2013). MacCartney and Manning (2009a) proposed an extension of the classification schema to a much more fine-grained inventory of 7 semantic relations that expresses additional concepts such as equivalence and reverse entailment and also inspired our label set. In SAS, the task is to assign a student answer a score that specifies the correctness of the answer. Many approaches to SAS compare learner answers given by a student to target answers specified by a teacher and rely on some measure of surface or semantic overlap between them (e.g. Bailey and Meurers (2008); Meurers et al. (2011); Mohler et al. (2011)) or measure whether"
W15-4408,P13-4001,0,0.0421171,"Missing"
W15-4408,W12-2022,0,0.0142661,"target hypothesis). In this study, we want to explicitly assess the relation between RTE labels and correctness scores assigned by teachers. We assume that they are related, but we expect that the relation is not a direct Introduction Reading comprehension exercises are a standard task in foreign language education: Students read a text in the language they are learning and answer questions about it. With the advent of computerbased language learning courses, the automatic scoring of such shortanswer questions has become an important research topic (for an overview see Burrows et al. (2015); Ziai et al. (2012)), not only for reading and listening comprehension in the context of foreign language learning, but also e.g. in science questions for native speaker students. It has been often noted that the SAS task is related to the task of recognizing textual entailment (RTE, e.g. Mohler et al. (2011), Sukkarieh and Blackmore (2009), Dzikovska et al. (2013b)). RTE is the task to decide whether there is an inference relation between two texts; in the case of SAS, these texts are the learner answer (LA), given by a student, and a teacher-specified target answer (TA, i.e. a sample solution). An entailment 1"
W15-4408,E09-1065,0,\N,Missing
W17-0901,N15-1122,0,0.0123315,"t learning. Gordon (2010) mined commonsense knowledge from stories describing events in day-to-day life. Jans et al. (2012) studied different ways of selecting event chains and used skipgrams for computing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event structure in an unsupervised way from a large collection of photo albums with time-stamped images and captions. This method is however limited by the availability of albums for “special” events such as WEDDING or BARBECUE, in contrast to everyday, trivial activites such as MAKING COFFEE or in recall. However, they had the option to label stories where they felt a scenari"
W17-0901,P16-1167,0,0.0133192,"omputing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event structure in an unsupervised way from a large collection of photo albums with time-stamped images and captions. This method is however limited by the availability of albums for “special” events such as WEDDING or BARBECUE, in contrast to everyday, trivial activites such as MAKING COFFEE or in recall. However, they had the option to label stories where they felt a scenario was only partially addressed in a different way, thus setting these cases apart from those where the scenario was centrally addressed. 5 While we take the judgment about the “C” class to be quite re"
W17-0901,P08-1090,0,0.743518,"n Thater Manfred Pinkal Universit¨at des Saarlandes Saarland, 66123, Germany {wanzare,zarcone,stth,pinkal}coli.uni-saarland.de Abstract tive abilities and has the potential to support NLP tasks such as anaphora resolution (Rahman and Ng, 2011), discourse relation detection, semantic role labeling, temporal order analysis, and applications such as text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989), question answering (Hajishirzi and Mueller, 2012). Several methods for the automatic acquisition of script knowledge have been proposed. Seminal work by Chambers and Jurafsky (2008; 2009) provided methods for the unsupervised widecoverage extraction of script knowledge from large text corpora. However, texts typically only mention small parts of a script, banking on the reader’s ability to infer missing script-related events. The task is therefore challenging, and the results are quite noisy. The work presented in this paper follows the approach proposed in Regneri et al. (2010) (henceforth “RKP”) who crowdsourced scenario descriptions by asking people how they typically carry out a particular activity. The collected event sequence descriptions provide generic descripti"
W17-0901,P09-1068,0,0.758462,"Missing"
W17-0901,E14-1006,1,0.933762,"efer to the same event, as the broader discourse context would suggest. To address this issue, we propose a semi-supervised approach, capitalizing on previous work by Klein et Datasets and gold standards. Three large crowdsourced collections of activity descriptions in terms of ESDs are available: the OMICS corpus (Gupta and Kochenderfer, 2004), the SMILE corpus (Regneri et al., 2010) and DeScript corpus (Wanzare et al., 2016). Sections 3-4 of this paper focus on a subset of ESDs for 14 scenarios from SMILE and OMICS, with on average 29.9 ESDs per scenario. In RKP, in the follow-up studies by Frermann et al. (2014) and Modi and Titov (2014) as well as in the present study, 4 of these scenarios were used as development set and 10 as test set. RKP provided two gold standard datasets for this subset: the RKP paraphrase dataset contains judgments for 60 event description pairs per scenario, the RKP temporal order dataset contains 60 event description pairs that are separately annotated in both directions, for a total of 120 datapoints per scenario. In order to directly evaluate our models for clustering quality, we also created a clustering gold standard for the RKP test set, adopting the experimental setup"
W17-0901,W16-2505,0,0.0137758,"n substantially contribute to the task of text understanding. P Figure 5: Example ROC-story with scenario annotation. ficient coverage for the analysis of script knowledge in natural-language texts? Answering this question is not trivial, as scenarios vary considerably in granularity and it is not trivial that the type of script knowledge we model can capture all kinds of event structures, even in narrative texts. In order to provide a rough estimate of coverage for the currently existing script material, we carried out a simple annotation study on the recently published ROC-stories database (Mostafazadeh et al., 2016a). The database consists of 50,000 short narrative texts, collected via Mechanical Turk. Workers were asked to write a 5-sentence length story about an everyday commonsense event, and they were encouraged to write about “anything they have in mind” to guarantee wide distribution across topics. For our annotation study, we merged the available datasets containing crowdsourced ESD collections (i.e. OMICS, SMILE, and DeScript), excluding two extremely general scenarios (GO OUTSIDE , CHILDHOOD ), which gives us a total of 226 different scenarios. We randomly selected 500 of the ROC-stories and as"
W17-0901,E14-1024,0,0.178579,"to consider the full scenario list4 . The three annotations were merged us6 Related Work Following the seminal work of Chambers and Jurafsky (2008) and (2009) on the induction of script-like narrative schemas from large, unlabeled corpora of news articles, a series of models have been presented for improving the induction method or explore alternative data sources for script learning. Gordon (2010) mined commonsense knowledge from stories describing events in day-to-day life. Jans et al. (2012) studied different ways of selecting event chains and used skipgrams for computing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event stru"
W17-0901,E12-1034,0,0.378855,"Missing"
W17-0901,W16-3644,0,0.0119913,"uction of script-like narrative schemas from large, unlabeled corpora of news articles, a series of models have been presented for improving the induction method or explore alternative data sources for script learning. Gordon (2010) mined commonsense knowledge from stories describing events in day-to-day life. Jans et al. (2012) studied different ways of selecting event chains and used skipgrams for computing event statistics. Pichotta and Mooney (2014) employed richer event representations, exploiting the interactions between multiple arguments to extract event sequences from a large corpus. Rahimtoroghi et al. (2016) learned contingency relations between events from a corpus of blog posts. All these approaches aim at high recall, resulting in a large amount of wide-coverage, but noisy schemas. Abend et al. (2015) proposed an edge-factored model to determine the temporal order of events in cooking recipes, but their model is limited to scenarios with an underlying linear order of events. Bosselut et al. (2016) induce prototypical event structure in an unsupervised way from a large collection of photo albums with time-stamped images and captions. This method is however limited by the availability of albums"
W17-0901,P10-1100,1,0.925342,"tion extraction (Rau et al., 1989), question answering (Hajishirzi and Mueller, 2012). Several methods for the automatic acquisition of script knowledge have been proposed. Seminal work by Chambers and Jurafsky (2008; 2009) provided methods for the unsupervised widecoverage extraction of script knowledge from large text corpora. However, texts typically only mention small parts of a script, banking on the reader’s ability to infer missing script-related events. The task is therefore challenging, and the results are quite noisy. The work presented in this paper follows the approach proposed in Regneri et al. (2010) (henceforth “RKP”) who crowdsourced scenario descriptions by asking people how they typically carry out a particular activity. The collected event sequence descriptions provide generic descriptions of a given scenario (e.g. BAKING A CAKE ) in concise telegram style (Fig. 1a). Based on these crowdsourced event sequence descriptions or ESDs, RKP extracted high-quality script knowledge for a variety of different scenarios, in the form of temporal script graphs (Fig. 1b). Temporal script graphs are partially ordered structures whose nodes are sets of alternative descriptions denoting the same eve"
W17-0901,W14-1606,0,0.656732,"the broader discourse context would suggest. To address this issue, we propose a semi-supervised approach, capitalizing on previous work by Klein et Datasets and gold standards. Three large crowdsourced collections of activity descriptions in terms of ESDs are available: the OMICS corpus (Gupta and Kochenderfer, 2004), the SMILE corpus (Regneri et al., 2010) and DeScript corpus (Wanzare et al., 2016). Sections 3-4 of this paper focus on a subset of ESDs for 14 scenarios from SMILE and OMICS, with on average 29.9 ESDs per scenario. In RKP, in the follow-up studies by Frermann et al. (2014) and Modi and Titov (2014) as well as in the present study, 4 of these scenarios were used as development set and 10 as test set. RKP provided two gold standard datasets for this subset: the RKP paraphrase dataset contains judgments for 60 event description pairs per scenario, the RKP temporal order dataset contains 60 event description pairs that are separately annotated in both directions, for a total of 120 datapoints per scenario. In order to directly evaluate our models for clustering quality, we also created a clustering gold standard for the RKP test set, adopting the experimental setup in Wanzare et al. (2016):"
W17-0901,N16-1098,0,0.0751556,"n substantially contribute to the task of text understanding. P Figure 5: Example ROC-story with scenario annotation. ficient coverage for the analysis of script knowledge in natural-language texts? Answering this question is not trivial, as scenarios vary considerably in granularity and it is not trivial that the type of script knowledge we model can capture all kinds of event structures, even in narrative texts. In order to provide a rough estimate of coverage for the currently existing script material, we carried out a simple annotation study on the recently published ROC-stories database (Mostafazadeh et al., 2016a). The database consists of 50,000 short narrative texts, collected via Mechanical Turk. Workers were asked to write a 5-sentence length story about an everyday commonsense event, and they were encouraged to write about “anything they have in mind” to guarantee wide distribution across topics. For our annotation study, we merged the available datasets containing crowdsourced ESD collections (i.e. OMICS, SMILE, and DeScript), excluding two extremely general scenarios (GO OUTSIDE , CHILDHOOD ), which gives us a total of 226 different scenarios. We randomly selected 500 of the ROC-stories and as"
W17-0901,L16-1556,1,0.899884,"ake (a) – take out box of ingredients from shelf – gather all cake ingredients – get cake mix get ingred. choose recipe – look up recipe – find cake recipe – get your recipe – stir to combine – mix well – mix ingredients together in bowls – stir cake ingredients – pour cake mix in bowl – add ingredients to bowl – add cake ingredients add ingred. buy ingred. prepare ingred. – purchase ingredients – buy cake mix – buy proper ingredients put cake into oven – place cake into oven – put cake in oven (b) Figure 1: Example ESDs (a) and induced script structure (b) for the BAKING A CAKE scenario from Wanzare et al. (2016) 2 Data We will now introduce the resources used in our study, namely the datasets of ESDs, the gold standards and the crowdsourced alignments between event descriptions. scriptions occurring in similar positions in ESDs tend to denote the same event type. However, MSA makes far too strong an assumption about the temporal ordering information in the ESDs. It does not allow for crossing edges and thus must assume a fixed and invariable order, while the ordering of events in a script is to some degree flexible (e.g., one can preheat the oven before or after mixing ingredients). We propose cluste"
W17-0901,D16-1017,1,\N,Missing
W19-3410,N15-1122,0,0.0183463,"esent a two-stage model that combines established methods from topic segmentation and text classification (Section 4). • Finally, we show that the proposed model achieves promising results but also reveals some of the difficulties underlying the task of scenario detection (Section 5). 2 Motivation and Background about the topics represented in these corpora. For instance, Chambers and Jurafsky (2008, 2009); Pichotta and Mooney (2014) leverage newswire texts, Manshadi et al. (2008); Gordon (2010); Rudinger et al. (2015); Tandon et al. (2014, 2017) leverage web articles while Ryu et al. (2010); Abend et al. (2015); Chu et al. (2017) leverage organized procedural knowledge (e.g. from eHow.com, wikiHow.com). The top part of Table 1 summarizes various script knowledge-bases. Our work lies in between both lines of research and may help to connect them: we take an extended set of specific scenarios as a starting point and attempt to identify instances of those scenarios in a large-scale collection of narrative texts. Textual resources. Previous work created scriptrelated resources by crowdsourcing stories that instantiate script knowledge of specific scenarios. For example, Modi et al. (2016) and Ostermann"
W19-3410,P10-1100,1,0.868215,"ant previous work in these areas in more detail. Script knowledge. Scripts are descriptions of prototypical everyday activities such as eating in a restaurant or riding a bus (Schank and Abelson, 1977). Different lines of research attempt to acquire script knowledge. Early researchers attempted to handcraft script knowledge (Mueller, 1999; Gordon, 2001). Another line of research focuses on the collection of scenario-specific script knowledge in form of event sequence descriptions (ESDs) via crowdsourcing, (Singh et al., 2002; Gupta and Kochenderfer, 2004; Li et al., 2012; Raisig et al., 2009; Regneri et al., 2010; Wanzare et al., 2016)). ESDs are sequences of short sentences, in bullet style, describing how a given scenario is typically realized. The top part of Table 1 summarizes various script knowledgebases (ESDs). While datasets like OMICS seem large, they focus only on mundane indoor scenarios (e.g. open door, switch off lights). A third line of research tries to leverage existing large text corpora to induce script-like knowledge Script-related tasks. Several tasks have been proposed that require or test computational models of script knowledge. For example, Kasch and Oates (2010) and Rahimtorog"
W19-3410,P17-4020,0,0.0586831,"Missing"
W19-3410,W10-0905,0,\N,Missing
W19-3410,P94-1002,0,\N,Missing
W19-3410,E14-1024,0,\N,Missing
W19-3410,P08-1090,0,\N,Missing
W19-3410,P09-1068,0,\N,Missing
W19-3410,D15-1195,0,\N,Missing
W19-3410,D14-1082,0,\N,Missing
W19-3410,N16-1098,0,\N,Missing
W19-3410,W16-3644,0,\N,Missing
W19-3410,W16-4011,0,\N,Missing
W19-3410,S18-1119,1,\N,Missing
W19-3410,L18-1512,1,\N,Missing
W19-3410,L16-1555,1,\N,Missing
W19-3410,W12-3307,0,\N,Missing
