1988.tmi-1.17,J85-1002,0,0.310088,"(Harris, 1982) that we have adopted, which incorporates operator trees as an intermediate representation, further explicates the underlying relationships among sublanguage word classes and substantially reduces the amount of structural transfer needed in the system. The sublanguage approach has found several computational applications, in North America, particularly in the work of the Linguistic String Project at New York University (e.g. Sager, 1981) and the TAUM group at the University of Montreal, where sublanguage grammars have been used in machine translation projects (Lehrberger, 1982; Isabelle & Bourbeau, 1985; Kittredge, 1987). To date, however, these techniques have not been tested on languages as dissimilar as Japanese and English, and the correctness of the premises outlined above is far from assured. The close correspondence between French and English sublanguage patterns found by the TAUM group is not guaranteed to carry over to Japanese and English. The relationships could just as easily be one-to-many or many-to-one. We have investigated this question with the goal of using sublanguage categories and patterns to facilitate the computer analysis of source texts in Japanese in the sublanguage"
1992.tmi-1.23,J90-2002,0,0.0687568,"Missing"
1992.tmi-1.23,J86-3002,1,0.846277,"Missing"
1992.tmi-1.23,C92-2099,1,0.890934,"Missing"
1992.tmi-1.23,C90-3044,0,0.0469973,"Missing"
1992.tmi-1.23,P91-1024,0,0.118024,"Missing"
1992.tmi-1.23,1988.tmi-1.17,1,0.895071,"Missing"
1992.tmi-1.23,C86-1155,0,0.0713764,"Missing"
1995.iwpt-1.26,H91-1060,0,0.0257967,"Missing"
1995.iwpt-1.26,H92-1026,0,0.0660896,"Missing"
1995.iwpt-1.26,E93-1006,0,0.0676845,"coverage grammars . Although it is inevitable that a stru ctured corpus will contains errors , statistical methods and t he size of the corpus may be able to ameliorate the effect of i ndividual errors. Aiso , because a large corpus will i nclude examples of many rare constructs, we have the potenitial of obtain ing broader coverage than we might with a hand-constructed grammar . Furthermore, experi ments over the past few years have shown the benefits of usi ng probabilistic i nformation in pars i ng, and the large corpus allows us to t rain the probabilities of a grammar (8) {7] [ l l] :(2) [4] ,(1 2] . A number of recent parsing experiments have also indicated that grammars whose production prob,:i bilities are dependent on the context can be more effective than context-free grammars in selecti ng a correct parse. This context sensitivity can be acqu i red ea5ily using a large corpus, whereas human abi l i ty to com pute such information is obviously limited. . There have been 216 several attempts to build context-dependent grammars based on large corpora. ( 1 4] ( 1 1] ( 1 3] (2] (4] ( 1 2] . As i s evident from the two lists of citations, there has been considerable research invo"
1995.iwpt-1.26,P93-1035,0,0.0953284,"Missing"
1995.iwpt-1.26,J93-1002,0,0.0706276,"Missing"
1995.iwpt-1.26,H90-1053,1,0.900788,"Missing"
1995.iwpt-1.26,E85-1024,0,0.0599362,"Missing"
1995.iwpt-1.26,C94-2119,1,0.869156,"Missing"
2021.eacl-main.172,W14-2907,0,0.0284318,"Missing"
2021.eacl-main.172,W18-6126,1,0.896303,"el embedding. In addition to ranking the positive classes higher than the negative ones, it ranks positive classes against each other to learn the connections between the positives classes. These methods all require annotated examples to learn the connections. In the case of relation types across different datasets, such annotation does not exist. We attempt to learn the similarity nevertheless using prototypes from each type. Multi-task learning: Training multiple relation datasets at the same time could improve the robustness of the model and reduce annotation cost for relation extraction. (Fu et al., 2018) proposed to use a shared encoder to learn more general feature representation. We use a similar multitask learning base model and incorporate the similarity between the relation schemas to further improve the performance. 3 rameters except the label embeddings. Suppose we obtain the sentence representation φ(x) with a neural architecture. We define the label embedding as Wl ∈ RD , a D-dimension vector for each type. We compute the L1 distance between them and learn a scoring function to estimate the scores Sθ (x) for every type: Sθ (x)l = Wo · |φ(x) − Wl |+ bo , (1) where Wo ∈ RD and bo ∈ R a"
2021.eacl-main.172,I17-2072,1,0.850943,"e a better choice at this setting. We take a step further and try to learn the similarity between the types at the same time. There are 6 main semantic types in ACE and 5 in ERE. Manual review of the relatedness (related or not) is trivial in this case because the relation names are almost identical for related relation types. In practice, it may take a few minutes to review more complicated relation schemas, but it would cost significantly less than annotating on the instance-level in text. For preprocessing the data, we follow previous work (Gormley et al., 2015; Nguyen and Grishman, 2015a; Fu et al., 2017, 2018) on ACE05. It contains 6 domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). We use newswire as training set (bn & nw), half of bc as the development set, and the other half of bc, cts and wl as the test sets. We followed their split of documents and their split of the relation types for asymmetric relations (directionality taken into account expect for physcial and personsocial types). We perform the same preprocessing for the ERE dataset, which contains documents from newswire and discussion forums. We f"
2021.eacl-main.172,D15-1205,0,0.0439323,"Missing"
2021.eacl-main.172,P16-1200,0,0.120513,"ncy from annotated examples. Surdeanu et al. (2012) used a two-layer hierarchical model. The object-level classifier is able to capture the label dependency, while the mention-level classifier is focused on multi-label classification. Riedel et al. (2013) used a neighborhood model to explicitly model the dependency between the labels in a matrix factorization framework. Both of models are designed to work on multi-label examples, which require annotation to capture the dependency between labels. In the recent work of neural methods for relation extraction, most of the work (Zeng et al., 2015; Lin et al., 2016; Liu et al., 2017) ignores the multi-label setting and does not explicitly model the label dependency. Ye et al. (2017), on the other hand, ranks the similarity between feature representation of the instance and the label embedding. In addition to ranking the positive classes higher than the negative ones, it ranks positive classes against each other to learn the connections between the positives classes. These methods all require annotated examples to learn the connections. In the case of relation types across different datasets, such annotation does not exist. We attempt to learn the simila"
2021.eacl-main.172,D17-1189,0,0.0158928,"examples. Surdeanu et al. (2012) used a two-layer hierarchical model. The object-level classifier is able to capture the label dependency, while the mention-level classifier is focused on multi-label classification. Riedel et al. (2013) used a neighborhood model to explicitly model the dependency between the labels in a matrix factorization framework. Both of models are designed to work on multi-label examples, which require annotation to capture the dependency between labels. In the recent work of neural methods for relation extraction, most of the work (Zeng et al., 2015; Lin et al., 2016; Liu et al., 2017) ignores the multi-label setting and does not explicitly model the label dependency. Ye et al. (2017), on the other hand, ranks the similarity between feature representation of the instance and the label embedding. In addition to ranking the positive classes higher than the negative ones, it ranks positive classes against each other to learn the connections between the positives classes. These methods all require annotated examples to learn the connections. In the case of relation types across different datasets, such annotation does not exist. We attempt to learn the similarity nevertheless u"
2021.eacl-main.172,W15-1506,1,0.924729,"2 are the cross-entropy losses for the two relation tasks. λ is the hyperparameter to control the learning speed between the two tasks. This would give a strong baseline of utilizing the two datasets together. 3.1 Prototypes of Relation Types for Learning Similarity For each relation type, we randomly select k examples (Sk ) from the training set as supporting examples. We use the mean of the representations of these examples as the prototype for the relation type: 1 X x ¯c = φ(xi ) (3) k xi ∈Sk Relation Model with Multi-task Learning The majority of neural relation models (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016) encode a sentence using a deep architecture to a vector representation followed by a softmax classifier, while the others (dos Santos et al., 2015; Ye et al., 2017) use a function to compute the score between label embedding and sentence representation. Inspired by Fu et al. (2018) where the shared encoder helps in the case of the multi-task learning, we choose the latter so that all relation types (including from different datasets) will share the whole model paThese prototypes are inspired by the Prototypical Networks (Snell et al., 2017). However, in"
2021.eacl-main.172,N13-1008,0,0.0427803,"on type dependency: There have been a few ways to model the relationships between types in a multi-label relation dataset where we can learn 2011 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2011–2016 April 19 - 23, 2021. ©2021 Association for Computational Linguistics the similarity or dependency from annotated examples. Surdeanu et al. (2012) used a two-layer hierarchical model. The object-level classifier is able to capture the label dependency, while the mention-level classifier is focused on multi-label classification. Riedel et al. (2013) used a neighborhood model to explicitly model the dependency between the labels in a matrix factorization framework. Both of models are designed to work on multi-label examples, which require annotation to capture the dependency between labels. In the recent work of neural methods for relation extraction, most of the work (Zeng et al., 2015; Lin et al., 2016; Liu et al., 2017) ignores the multi-label setting and does not explicitly model the label dependency. Ye et al. (2017), on the other hand, ranks the similarity between feature representation of the instance and the label embedding. In ad"
2021.eacl-main.172,P15-1061,0,0.413403,"ure representation. We use a similar multitask learning base model and incorporate the similarity between the relation schemas to further improve the performance. 3 rameters except the label embeddings. Suppose we obtain the sentence representation φ(x) with a neural architecture. We define the label embedding as Wl ∈ RD , a D-dimension vector for each type. We compute the L1 distance between them and learn a scoring function to estimate the scores Sθ (x) for every type: Sθ (x)l = Wo · |φ(x) − Wl |+ bo , (1) where Wo ∈ RD and bo ∈ R are shared for all types. We do not use the dot product (dos Santos et al., 2015; Ye et al., 2017) as the scoring function because the L1 distance works slightly better in the multi-task learning experiments. The probability of every class is computed as the softmax output of the scores. Similar to (Fu et al., 2018), we jointly train two relation tasks at the same time with cross-entropy losses. L = λLr1 + (1 − λ)Lr2 , (2) where Lr1 and Lr2 are the cross-entropy losses for the two relation tasks. λ is the hyperparameter to control the learning speed between the two tasks. This would give a strong baseline of utilizing the two datasets together. 3.1 Prototypes of Relation"
2021.eacl-main.172,D12-1042,0,0.0580987,"r to explore utilizing the relatedness between the relation types. Experiments on ACE05 and ERE show that it can further boost the performance, especially in the low-resource settings. 2 Related Work Relation type dependency: There have been a few ways to model the relationships between types in a multi-label relation dataset where we can learn 2011 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2011–2016 April 19 - 23, 2021. ©2021 Association for Computational Linguistics the similarity or dependency from annotated examples. Surdeanu et al. (2012) used a two-layer hierarchical model. The object-level classifier is able to capture the label dependency, while the mention-level classifier is focused on multi-label classification. Riedel et al. (2013) used a neighborhood model to explicitly model the dependency between the labels in a matrix factorization framework. Both of models are designed to work on multi-label examples, which require annotation to capture the dependency between labels. In the recent work of neural methods for relation extraction, most of the work (Zeng et al., 2015; Lin et al., 2016; Liu et al., 2017) ignores the mul"
2021.eacl-main.172,P17-1166,0,0.074587,"able to capture the label dependency, while the mention-level classifier is focused on multi-label classification. Riedel et al. (2013) used a neighborhood model to explicitly model the dependency between the labels in a matrix factorization framework. Both of models are designed to work on multi-label examples, which require annotation to capture the dependency between labels. In the recent work of neural methods for relation extraction, most of the work (Zeng et al., 2015; Lin et al., 2016; Liu et al., 2017) ignores the multi-label setting and does not explicitly model the label dependency. Ye et al. (2017), on the other hand, ranks the similarity between feature representation of the instance and the label embedding. In addition to ranking the positive classes higher than the negative ones, it ranks positive classes against each other to learn the connections between the positives classes. These methods all require annotated examples to learn the connections. In the case of relation types across different datasets, such annotation does not exist. We attempt to learn the similarity nevertheless using prototypes from each type. Multi-task learning: Training multiple relation datasets at the same"
2021.eacl-main.172,D15-1203,0,0.220061,"milarity or dependency from annotated examples. Surdeanu et al. (2012) used a two-layer hierarchical model. The object-level classifier is able to capture the label dependency, while the mention-level classifier is focused on multi-label classification. Riedel et al. (2013) used a neighborhood model to explicitly model the dependency between the labels in a matrix factorization framework. Both of models are designed to work on multi-label examples, which require annotation to capture the dependency between labels. In the recent work of neural methods for relation extraction, most of the work (Zeng et al., 2015; Lin et al., 2016; Liu et al., 2017) ignores the multi-label setting and does not explicitly model the label dependency. Ye et al. (2017), on the other hand, ranks the similarity between feature representation of the instance and the label embedding. In addition to ranking the positive classes higher than the negative ones, it ranks positive classes against each other to learn the connections between the positives classes. These methods all require annotated examples to learn the connections. In the case of relation types across different datasets, such annotation does not exist. We attempt t"
2021.eacl-main.172,C14-1220,0,0.0491074,"2) where Lr1 and Lr2 are the cross-entropy losses for the two relation tasks. λ is the hyperparameter to control the learning speed between the two tasks. This would give a strong baseline of utilizing the two datasets together. 3.1 Prototypes of Relation Types for Learning Similarity For each relation type, we randomly select k examples (Sk ) from the training set as supporting examples. We use the mean of the representations of these examples as the prototype for the relation type: 1 X x ¯c = φ(xi ) (3) k xi ∈Sk Relation Model with Multi-task Learning The majority of neural relation models (Zeng et al., 2014; Nguyen and Grishman, 2015b; Zeng et al., 2015; Lin et al., 2016) encode a sentence using a deep architecture to a vector representation followed by a softmax classifier, while the others (dos Santos et al., 2015; Ye et al., 2017) use a function to compute the score between label embedding and sentence representation. Inspired by Fu et al. (2018) where the shared encoder helps in the case of the multi-task learning, we choose the latter so that all relation types (including from different datasets) will share the whole model paThese prototypes are inspired by the Prototypical Networks (Snell"
A00-1039,P93-1022,0,0.00943648,"of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use t h e m to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &quot;company (hire/fire/expel...} person&quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in .our case the contexts are limited to selected patterns, relevant to the scenario. SE.g., &quot; J o h n sleeps&quot;, &quot;John is appointed by C o m p a n y &quot; , &quot;I saw a d o g which sleeps&quot;, &quot;She asked J o h n to buy a car&quot;. 6E.g., &quot; J o h n is appointed by Company&quot;, &quot;John is the p r e s i d e n t of Company&quot;, &quot;I saw a d o g which sleeps&quot;, The d o g which I saw sleeps. 7For example, &quot;She gave us our coffee b l a c k &quot; , &quot;Company appointed John Smith as p r e s i d e n t &quot; . 284 3.4 P a t t e r n Discovery Here we present the results from experiments we conducted on t"
A00-1039,M95-1011,0,0.0299138,"ed here are outside for clarity. of the central 1 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of MUC's highest-level"
A00-1039,J86-3002,1,0.637138,"ould s/he fail to provide an example of a particular class of syntactic/semantic construction, the system has no hope of recovering the corresponding events. Our experience has shown that (1) the process of discovering candidates is highly expensive, and (2) gaps in patterns directly translate into gaps in coverage. How can the system help automate the process of discovering new good candidates? The system should find examples of all common linguistic constructs relevant to a scenario. While there has been prior research on identifying the primary lexical patterns of a sub-language or corpus (Grishman et al., 1986; Riloff, 1996), the task here is more complex, since we are typically not provided in advance with a sub-corpus of relevant passages; these passages must themselves be found as part of the discovery process. The difficulty is that one of the best indications of the relevance of the passages is precisely the presence of these constructs. Because of this circularity, we propose to acquire the constructs and passages in tandem. 2 Solution We outline our procedure for automatic acquisition of patterns; details are elaborated in later sections. The procedure is unsupervised in that it does not req"
A00-1039,J93-1006,0,0.0233874,"acquire the constructs and passages in tandem. 2 Solution We outline our procedure for automatic acquisition of patterns; details are elaborated in later sections. The procedure is unsupervised in that it does not require the training corpus to be manually annotated with events of interest, nor a pro-classified corpus with relevance judgements, nor any feedback or intervention from the user 2. The idea is to combine IR-style document selection with an iterative relaxation process; this is similar to techniques used elsewhere in NLP, and is inspired in large part, if remotely, by the work of (Kay and RSscheisen, 1993) on automatic alignment of sentences and words in a bilingual corpus. There, the reasoning was: sentences that are translations of each 2however, it may be supervised after each iteration, where the user can answer yes/no questions to improve the quality of the results 283 other are good indicators that words they contain are translation pairs; conversely, words that are translation pairs indicate that the sentences which contain them correspond to one another. In our context, we observe that documents that are relevant to the scenario will necessarily contain good patterns; conversely, good p"
A00-1039,M92-1021,0,0.0805225,"t, constituents included here are outside for clarity. of the central 1 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of"
A00-1039,P93-1024,0,0.0163407,"e is reduced to a set of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use t h e m to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &quot;company (hire/fire/expel...} person&quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in .our case the contexts are limited to selected patterns, relevant to the scenario. SE.g., &quot; J o h n sleeps&quot;, &quot;John is appointed by C o m p a n y &quot; , &quot;I saw a d o g which sleeps&quot;, &quot;She asked J o h n to buy a car&quot;. 6E.g., &quot; J o h n is appointed by Company&quot;, &quot;John is the p r e s i d e n t of Company&quot;, &quot;I saw a d o g which sleeps&quot;, The d o g which I saw sleeps. 7For example, &quot;She gave us our coffee b l a c k &quot; , &quot;Company appointed John Smith as p r e s i d e n t &quot; . 284 3.4 P a t t e r n Discovery Here we present the results from experimen"
A00-1039,M95-1006,0,\N,Missing
A00-1039,A97-1011,1,\N,Missing
A88-1009,J83-3003,0,0.529937,"Missing"
A88-1009,J83-3001,0,0.740414,"Missing"
A88-1009,J83-3005,0,0.0422211,"Missing"
A88-1009,J86-2001,0,0.0613978,"Missing"
A88-1009,J80-2003,0,0.0825709,"Missing"
A88-1009,J81-2002,0,\N,Missing
A88-1010,J83-3003,0,0.0377867,"Missing"
A88-1010,P81-1036,0,\N,Missing
A92-1022,H91-1060,0,0.0748866,"Missing"
A92-1022,H90-1053,1,0.876033,"Missing"
A92-1022,P84-1005,0,0.0692762,"Missing"
A92-1022,C90-3071,1,0.835364,"Missing"
A92-1022,J83-3002,0,0.0817074,"Missing"
A92-1022,H91-1059,0,0.0825983,"Missing"
atkins-etal-2002-resources,2001.mtsummit-papers.39,1,\N,Missing
atkins-etal-2002-resources,bel-etal-2000-simple,1,\N,Missing
atkins-etal-2002-resources,calzolari-etal-2002-towards,1,\N,Missing
C00-1078,W98-1115,0,0.0730048,"edure is trivial for a system if, given a context, one transfer rule can be selected unambiguously. Otherwise, choosing the best set of transfer rules may involve the evaluation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in whic"
C00-1078,H90-1053,1,0.636374,"anslation from a given source language sentence. This procedure is trivial for a system if, given a context, one transfer rule can be selected unambiguously. Otherwise, choosing the best set of transfer rules may involve the evaluation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such syst"
C00-1078,P93-1004,0,0.684418,"es may involve the evaluation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in ("
C00-1078,C96-1078,1,0.903804,"ation of numerous competing sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in (Way et al., 1997) req"
C00-1078,meyers-etal-1998-multilingual,1,0.907283,"peting sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in (Way et al., 1997) require components for d"
C00-1078,P98-2139,1,0.938603,"peting sets. In fact, the number of possible transfer rule combinations increases exponentially with the length of the source language sentence. This situation mirrors the problem of choosing productions in a nondeterministic parser. In this paper, we describe a system for choosing transfer rules, based on statistical chart parsing (Bobrow, 1990; Chitrao and Grishman, 1990; Caraballo and Charniak, 1997; Charniak et al., 1998). In our Machine Translation system, transfer rules are generated automatically from parsed parallel text along the lines of (Matsumoto et al., 1993; Meyers et al., 1996; Meyers et al., 1998b). Our system tends to acquire a large number of transfer rules, due mainly to alternative ways of translating the same sequences of words, non-literal translations in parallel text and parsing errors. It is therefore crucial that our system choose the best set of rules eÆciently. While the technique discussed here obviously applies to similar such systems, it could also apply to hand-coded systems in which each word or group of words is related to more than one transfer rule. For example, both Multra (Hein, 1996) and the Eurotra system described in (Way et al., 1997) require components for d"
C00-1078,J98-2004,0,\N,Missing
C00-1078,H91-1042,0,\N,Missing
C00-1078,J93-2003,0,\N,Missing
C00-1078,C98-2134,1,\N,Missing
C00-2136,M95-1011,0,0.0047632,"portability and performance. Preparing good patterns for these systems requires considerable skill, and achieving good coverage requires the analysis of a large amount of text. These problems have been impediments to the wider use of extraction systems. These diÆculties have stimulated research on pattern acquisition. Some of this work has emphasized interactive tools to convert examples to extraction patterns (Yangarber and Grishman, 1997); much of the research has focused on methods for automatically converting a corpus annotated with extraction examples into patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These techniques may reduce the level of system expertise required to develop a new extraction application, but they do not lessen the burden of studying a large corpus in order to nd relevant candidates. The prior work most closely related to our own is that of (Rilo , 1996), who also seeks to build patterns automatically without the need to annotate a corpus with the information to be extracted. However, her work di ers from our own in several important respects. First, her patterns identify phrases that ll individual slots in the template, without specifying how thes"
C00-2136,M95-1014,1,0.40712,"rmally consists of an analysis of the text in terms of general linguistic structures and domain-speci c constructs, followed by a search for the scenariospeci c patterns. It is possible to build these constituent structures through a full syntactic analysis of the text, and the discovery procedure we describe below would be applicable to such an architecture. However, for reasons of speed, coverage, and system robustness, the more common approach at present is to perform a partial syntactic analysis using a cascade of nite-state transducers. This is the approach used by our extraction system (Grishman, 1995; Yangarber and Grishman, 1998). At the heart of our system is a regular expression pattern matcher which is capable of matching a set of regular expressions against a partially-analyzed text and producing additional annotations on the text. This core draws on a set of knowledge bases of varying degrees of domain- and task-speci city. The lexicon includes both a general English dictionary and de nitions of domain and scenario terms. The concept base arranges the domain terms into a semantic hierarchy. The predicate base describes the logical structure of the events to be extracted. The pattern"
C00-2136,M92-1021,0,0.0485974,"limitations on their portability and performance. Preparing good patterns for these systems requires considerable skill, and achieving good coverage requires the analysis of a large amount of text. These problems have been impediments to the wider use of extraction systems. These diÆculties have stimulated research on pattern acquisition. Some of this work has emphasized interactive tools to convert examples to extraction patterns (Yangarber and Grishman, 1997); much of the research has focused on methods for automatically converting a corpus annotated with extraction examples into patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These techniques may reduce the level of system expertise required to develop a new extraction application, but they do not lessen the burden of studying a large corpus in order to nd relevant candidates. The prior work most closely related to our own is that of (Rilo , 1996), who also seeks to build patterns automatically without the need to annotate a corpus with the information to be extracted. However, her work di ers from our own in several important respects. First, her patterns identify phrases that ll individual slots in the template, withou"
C00-2136,A97-1011,1,0.650996,"Missing"
C00-2136,A00-1039,1,0.782333,"e we are typically not provided in advance with a sub-corpus of relevant passages; these passages must themselves be found as part of the discovery procedure. The diÆculty is that one of the best indications of the relevance of the passages is precisely the presence of these constructs. Because of this circularity, we propose to acquire the constructs and passages in tandem. 2 ExDisco: the Discovery Procedure We rst outline ExDisco, our procedure for discovery of extraction patterns; details of some of the steps are presented in the section which follows, and an earlier paper on our approach (Yangarber et al., 2000). ExDisco is an unsupervised procedure: the training corpus does not need to be annotated with the speci c event information to be extracted, or even with information as to which documents in the corpus are relevant to the scenario. The only information the user must provide, as described below, is a small set of seed patterns regarding the scenario. Starting with this seed, the system automatically performs a repeated, automatic expansion of the pattern set. This is analogous to the process of automatic term expansion used in some information retrieval systems, where the terms from the most r"
C00-2136,M98-1011,1,\N,Missing
C00-2136,M95-1006,0,\N,Missing
C02-1154,A97-1029,0,0.0278225,"ies. This particular scenario requires a comprehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Director Dr. Gideon Br"
C02-1154,W98-1118,1,0.895488,"scenario requires a comprehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Director Dr. Gideon Bruckner said no cases of"
C02-1154,W99-0613,0,0.375558,"prehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Director Dr. Gideon Bruckner said no cases of mad cow disease have been f"
C02-1154,W99-0612,0,0.0484906,"Missing"
C02-1154,C96-2157,0,0.0991849,"the left and the right side of an instance in text. Separating the two sides allows the learner to accept weaker rules, and several correction phases compensate in cases of insuÆcient evidence by removing uncertain items, and preventing them from polluting the set of good seeds. Research in automatic terminology acquisition initially focused more on the problem of identi cation and statistical methods for this task, e.g., (Justeson and Katz, 1995), the CValue/NC-Value method, (Frantzi et al., 2000). Separately, the problem of classi cation or clustering is addressed in, e.g., (Ushioda, 1996) (Strzalkowski and Wang, 1996) presents an algorithm for learning universal concepts,&quot; which in principle includes both PNs and generic NPs|a step toward our notion of generalized names. The spotter&quot; proceeds iteratively from a handful of seeds and learns names in a single category. DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus. This allows the rules to use deeper, longer-range dependencies, which are diÆcult to express with surface-level information alone. However, a potential problem with using this approach for our task is that the Penn-Treebank-based p"
C02-1154,C96-2212,0,0.0182785,"separately for the left and the right side of an instance in text. Separating the two sides allows the learner to accept weaker rules, and several correction phases compensate in cases of insuÆcient evidence by removing uncertain items, and preventing them from polluting the set of good seeds. Research in automatic terminology acquisition initially focused more on the problem of identi cation and statistical methods for this task, e.g., (Justeson and Katz, 1995), the CValue/NC-Value method, (Frantzi et al., 2000). Separately, the problem of classi cation or clustering is addressed in, e.g., (Ushioda, 1996) (Strzalkowski and Wang, 1996) presents an algorithm for learning universal concepts,&quot; which in principle includes both PNs and generic NPs|a step toward our notion of generalized names. The spotter&quot; proceeds iteratively from a handful of seeds and learns names in a single category. DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus. This allows the rules to use deeper, longer-range dependencies, which are diÆcult to express with surface-level information alone. However, a potential problem with using this approach for our task is"
C02-1154,C96-1071,0,0.0321827,"al-purpose dictionaries. This particular scenario requires a comprehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Di"
C02-1154,C00-2136,1,0.441888,"ns, diseases) periodically enter into existence and literature. 3. A typical text contains all the information that is necessary for a human to infer the category. This makes discovering names in text an interesting research problem in its own right. The following section introduces the learning algorithm; Section 3 compares our approach to related prior work; Section 4 presents an evaluation of results; we conclude with a discussion of evaluation and current work, in Section 5. 2 Nomen: The Learning Algorithm Nomen is based on a bootstrapping approach, similar in essence to that employed in (Yangarber et al., 2000).1 The algorithm is trained on a large corpus of medical text, as described in Section 4. 2.1 Pre-processing 2.2 Unsupervised Learning A large text corpus is passed through a zoner, a tokenizer/lemmatizer, and a part-of-speech (POS) tagger. The zoner is a rule-based program to extract textual content from the mailing-list messages, i.e., stripping headers and footers. The tokenizer produces lemmas for the in ected surface forms. The statistical POS tagger is trained on the Wall Street Journal (possibly sub-optimal for texts about infectious disease). Unknown or foreign words are not lemmatized"
C02-1165,O97-1012,0,0.0620196,"ing a quanti ed NP, as in table 2 in paragraph (2). The trigger include (as a nite verb) functions similarly, but can also occur between sentences: [...] the Ugandan Ministry of Health has reported [...] 370 cases and 140 deaths. This gure includes 16 new con rmed cases in Gulu [...] In our training corpus, when these cue words occurred in this context, they consistently indicated an event inclusion relation. 5 Discussion Complexity of a scenario seems to depend of multiple factors. The notion of complexity, however, has not been investigated in great depth. Some research on this was done by (Bagga and Biermann, 1997; Bagga, 1997), classifying scenarios according to diÆculty by counting distances between components&quot; of an event in the text. In this way it attempts to account for variation in performance across the MUC scenarios. Our analysis suggests that the type and amount of inclusion relationships depend on the nature of the topic. In such scenarios as Management Succession and Corporate Acquisitions, an event usually occurs at one speci c point in time. By contrast, the Nature events typically take place across a span of time and space. As the event 	ravels&quot; and evolves, its manifestations are repo"
C02-1165,M95-1014,1,0.789406,"enting template structure. In section 4 we present examples of the linguistic cues to Disaster Date Location VictimDead Damage tornado Sunday night Georgia one person motel Disease Date Location VictimDead VictimSick Ebola since September Uganda 156 people - Table 1: Disaster Event and Disease Event recover the complex event structure, followed by discussion in section 5. 2 Background 2.1 Information Extraction Our IE system has been previously customized for several news topics, as part of the MUC program, such as Terrorist Attacks (MUC, 1991; MUC, 1992) and Management Succession (MUC, 1995; Grishman, 1995). Subsequently to the MUCs, we customized Proteus to extract, among other scenarios, Corporate Mergers and Acquisitions, Natural Disasters and Infectious Disease Outbreaks. We contrasted the Nature scenarios with the earlier MUC scenarios (Huttunen et al., 2002). The 	raditional&quot; template structure is such that all the information about the main event can be presented within a single template. The main events form separate instances, and there are no links between them. Management Succession scenario presents a slightly more complicated template structure, but it is still possible to present"
C02-1165,huttunen-etal-2002-diversity,1,0.641922,"e Nature scenarios, we encountered problems that did not arise in the traditional scenarios of the Message Understanding Conferences (MUCs). This included, in particular, delimiting the scope of a single event and organizing the events into templates. We identify two structural factors that contribute to the complexity of a scenario: rst, the scattering of events in text, and second, inclusion relationships between events. These factors cause diÆculty in representing the facts in an unambiguous way. We proposed that such event relationships can be described with a modular, hierarchical model (Huttunen et al., 2002). The phenomenon of inclusion is widespread in the Nature scenarios, and the types of inclusions are numerous. In this paper we present preliminary results obtained from our corpus analysis, with a classi cation and distribution of inclusion relationships. We discuss the potential for recovery of these inclusions from text with the help of the linguistic cues, of which we show some examples. This paper will argue that a thorough linguistic analysis of the corpus is needed to help recovery of the complex event structure in the text. In the next section we give a brief description of the scenari"
C02-1165,O98-3001,0,\N,Missing
C04-1109,P03-1028,0,0.0296085,"Missing"
C04-1109,W98-1105,0,0.379526,"Missing"
C04-1109,W01-1511,1,0.784239,"parser, but are not limited to these. Other general tools can also be included, which are not shown in the diagram. The triangles in the diagram are kernels that encode the corresponding syntactic processing result. In the training phase, the target slot fillers are labeled in the text so that SVM slot detectors can be trained through the kernels to find fillers for the key slots of events. In the testing phase, the SVM classifier will predict the slot fillers from unlabeled text and a merging procedure will merge slots into events if necessary. The main kernel we propose to use is on GLARF (Meyers et al., 2001) dependency graphs. SGML Parser Documents Texts Glarf Parser Sent Parser Slot Detector Name Tagger These kernels can be used alone or combined with each other using the properties of kernels. They can also be combined with high-order kernels like polynomial or RBF kernels, either individually or on the resulting kernel. As the depth of analysis of the preprocessing increases, the accuracy of the result decreases. Combining the results of deeper processing with those of shallower processing (such as n-grams) can also give us a back-off ability to recover from errors in deep processing. In pract"
C04-1109,M98-1009,0,0.0326447,"data for an IE system is often sparse since the target domain changes quickly. Traditional IE approaches try to generate patterns for events by various means using training data. For example, the FASTUS (Appelt et al., 1996) and Proteus (Grishman, 1996) systems, which performed well for MUC-6, used hand-written rules for event patterns. The symbolic learning systems, like AutoSlog (Riloff, 1993) and CRYSTAL (Fisher et al., 1996), generated patterns automatically from specific examples (text segments) using generalization and predefined pattern templates. There are also statistical approaches (Miller et al., 1998) (Collins et al., 1998) trying to encode event patterns in statistical CFG grammars. All of these approaches assume 2 2.1 Background Information Extraction The major task of IE is to find the elements of an event from text and combine them to form templates or populate databases. Most of these elements are named entities (NEs) involved in the event. To determine which entities in text are involved, we need to find reliable clues around each entity. The extraction procedure starts with 1 1. If K 1 ( x, y ) and K 2 ( x, y ) are kernels on X × Y , α , β > 0 , then αK 1 ( x, y ) + βK 2 ( x, y ) is"
C04-1109,M95-1014,1,\N,Missing
C04-1109,M95-1006,0,\N,Missing
C04-1109,M95-1011,0,\N,Missing
C04-1127,N03-4013,1,\N,Missing
C04-1127,P03-1029,1,\N,Missing
C04-1127,J03-1002,0,\N,Missing
C04-1127,P03-1044,0,\N,Missing
C04-1127,C02-1070,0,\N,Missing
C10-1077,J02-3001,0,0.0265898,"levance score and word similarity score, and found the results to be quite similar. Due to space limitations, we do not report these results here. 682 function is changed to incorporate lexical similarity information. For our experiments bootstrapping was terminated after a fixed number of iterations; in practice, we would monitor performance on a held-out (dev-test) sample and stop when it declines for k iterations. 4.1 Pre-processing Instead of limiting ourselves to surface syntactic relations, we want to get more general and meaningful patterns. To this end, we used semantic role labeling (Gildea and Jurafsky, 2002) to generate the logical grammatical and predicate-argument representation automatically from a parse tree (Meyers et al. 2009). The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse tree of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and dele"
C10-1077,W06-0204,0,0.423102,"ideally to a small set of seeds. Most semi-supervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments. The semi-supervised learning was based primarily on one of two assumptions: the document-centric approach, which assumes that relevant patterns should appear more frequently in relevant documents (Riloff 1996; Yangarber et al. 2000; Yangarber 2003; Surdeanu et al 2006); and the similarity-centric approach, which assumes that relevant patterns should have lexically related terms (Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consider the effect of different corp"
C10-1077,O97-1002,0,0.016884,"1 • ∑ Re l i (d) |H( p) |d ∈ H ( p ) where H(p) is the set of documents matching pattern p. Patterns are then ranked by € RankFunYangarber ( p) = Sup( p) * log Sup( p) H( p) where € (a generalization of Yangarber’s metric), and the top-ranked candidates are added to the set of accepted patterns. 4.3 Pattern Similarity For two words, there are several ways to measure their similarity using WordNet, which can be roughly divided into two categories: distance-based, including Leacock and Chodorow (1998), Wu and Palmer (1994); and information content based, including Resnik (1995), Lin (1998), and Jiang and Conrath (1997). We follow S&G (2005)’s method and use the semantic similarity of concepts based on Information Content (IC). Every pattern consists of a predicate and a constraint (“argument”) on its local syntactic context, and so the similarity of two patterns depends on the similarity of the predicates and the similarity of the arguments. We modified S&G’s structural similarity measure to reflect some differences in pattern structure: first, S&G only focus on patterns headed by verbs, while we include verbs, nouns and adjectives; second, they only record the subject and object to a verb, while we record"
C10-1077,W09-2423,1,0.75353,"esults here. 682 function is changed to incorporate lexical similarity information. For our experiments bootstrapping was terminated after a fixed number of iterations; in practice, we would monitor performance on a held-out (dev-test) sample and stop when it declines for k iterations. 4.1 Pre-processing Instead of limiting ourselves to surface syntactic relations, we want to get more general and meaningful patterns. To this end, we used semantic role labeling (Gildea and Jurafsky, 2002) to generate the logical grammatical and predicate-argument representation automatically from a parse tree (Meyers et al. 2009). The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse tree of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate-argument) relation corresponding to relations in PropBank (Palmer et al. 2005) and No"
C10-1077,J05-1004,0,0.0340273,"tree (Meyers et al. 2009). The output of the semantic labeling is the dependency representation of the text, where each sentence is a graph consisting of nodes (corresponding to words) and arcs. Each arc captures up to three relations between two words: (1) a SURFACE relation, the relation between a predicate and an argument in the parse tree of a sentence; (2) a LOGIC1 (grammatical logical) relation which regularizes for lexical and syntactic phenomena like passive, relative clauses, and deleted subjects; and (3) a LOGIC2 (predicate-argument) relation corresponding to relations in PropBank (Palmer et al. 2005) and NomBank In constructing extraction patterns from this graph, we take each dependency link along with its predicate-argument role; if that role is null, we use its logical grammatical role, and finally, its surface role. For example, for the sentence: John is hit by Tom’s brother. we generate the patterns: &lt;Arg1 hit John&gt; &lt;Arg0 hit brother&gt; &lt;T-pos brother Tom&gt; where the first two represent LOGIC2 relations and the third a SURFACE relation. To reduce data sparseness, all inflected words are changed to their root form (e.g. “attackers”→“attacker”), and all names are replaced by their ACE typ"
C10-1077,D07-1075,0,0.0498532,"manual document classification or corpus annotation. The seed patterns were used to identify some relevant documents, and the top-ranked patterns (based on their distribution in relevant and irrelevant documents) were added to the seed set. This process was repeated, assigning a relevance score to each document based on the relevance of the patterns it contains and gradually growing the set of relevant patterns. This approach was further refined by Surdeanu et al. (2006), who used a co-training strategy in which two classifiers seek to classify documents as relevant to a particular scenario. Patwardhan and Riloff (2007) presented an information extraction system that find relevant regions of text and applies extraction patterns within those regions. They created a self-trained relevant sentence classifier to identify relevant regions, and use a semantic affinity measure to automatically learn domain-relevant extraction patterns. They also distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. Stevenson and Greenwood (2005) (henceforth ‘S&G’) suggested an alternative method for ranking the candidate patterns. Their approach relied on the assumption tha"
C10-1077,N04-3012,0,0.122156,"Missing"
C10-1077,rose-etal-2002-reuters,0,0.115669,"e also did experiments on ACE 2005 data, because it provides many distinct event types; we conducted experiments on three disparate event types: attack, die, and start-position. Note that MUC-6 identifies a scenario while ACE identifies specific event types, and types which are in the same MUC scenario might represent different ACE events. For example, the executive succession scenario (MUC-6) includes the start-position and end-position events in ACE. 5.1 Data Description There are four corpora used in the experiments: MUC-6 corpora • Bootstrapping: pre-selected data from the Reuters corpus (Rose et al. 2002) from 1996 and 1997, including 3000 related documents and 3000 randomly chosen unrelated documents • Evaluation: MUC-6 annotated data, including 200 documents (official training and test). We were guided by the MUC-6 key file in annotating every document and sentence as relevant or irrelevant. ACE corpora • Bootstrapping: untagged data from the Gigaword corpus from January 2006, including 14,171 English newswire articles from Agence France-Presse (AFP). • 5.2 5.3 MUC-6 Experiments Our overall goal was to demonstrate that filtered ranking was in all cases competitive with and in at least some c"
C10-1077,P05-1047,0,0.733832,"e the annotated data required, ideally to a small set of seeds. Most semi-supervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments. The semi-supervised learning was based primarily on one of two assumptions: the document-centric approach, which assumes that relevant patterns should appear more frequently in relevant documents (Riloff 1996; Yangarber et al. 2000; Yangarber 2003; Surdeanu et al 2006); and the similarity-centric approach, which assumes that relevant patterns should have lexically related terms (Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consid"
C10-1077,W06-2207,0,0.514719,"uite well with enough training data, but annotating sufficient data may require months of labor. Semi-supervised methods aim to reduce the annotated data required, ideally to a small set of seeds. Most semi-supervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments. The semi-supervised learning was based primarily on one of two assumptions: the document-centric approach, which assumes that relevant patterns should appear more frequently in relevant documents (Riloff 1996; Yangarber et al. 2000; Yangarber 2003; Surdeanu et al 2006); and the similarity-centric approach, which assumes that relevant patterns should have lexically related terms (Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extract"
C10-1077,C00-2136,1,0.896183,"Missing"
C10-1077,P03-1044,0,0.122683,"ds can perform quite well with enough training data, but annotating sufficient data may require months of labor. Semi-supervised methods aim to reduce the annotated data required, ideally to a small set of seeds. Most semi-supervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments. The semi-supervised learning was based primarily on one of two assumptions: the document-centric approach, which assumes that relevant patterns should appear more frequently in relevant documents (Riloff 1996; Yangarber et al. 2000; Yangarber 2003; Surdeanu et al 2006); and the similarity-centric approach, which assumes that relevant patterns should have lexically related terms (Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Aut"
C10-1077,P94-1019,0,\N,Missing
C10-2137,H05-1091,0,0.15403,"is that if two patterns tend to occur in similar contexts then the meanings of the patterns tend to be similar. For example, in “X solves Y” and “X finds a solution to Y”, “solves” and “finds a solution to” share many common Xs and Ys and hence are similar to each other. This extended distributional hypothesis serves as the basis on which we compute similarities for each pair of patterns. 2.2 Pattern Representation — Shortest Dependency Path We adopt a shortest dependency path (SDP) representation of relation patterns. SDP has demonstrated its power in kernel methods for relation extraction (Bunescu and Mooney, 2005). Its capability in capturing most of the information of interest is also evidenced by a systematic comparison of effectiveness of different information extraction (IE) patterns in (Stevenson and Greenwood, 2006) 2 . For example, “nsubj Å met Æ prep_in” is able to represent LocatedIn between “Gates” and “Seattle” while a token-based pattern would be much less general because it would have to specify all the intervening tokens. Figure 1. Stanford dependency tree for sentence “Gates, Microsoft’s chairman, met with President Clinton in Seattle”. 2 SDP is equivalent to the linked chains described"
C10-2137,P07-1036,0,0.0653471,"Missing"
C10-2137,W09-2209,0,0.0203296,"wn as seeds. SSL starts with these seeds to train an initial model; it then applies this model to a large volume of unlabeled data to get more labeled examples and adds the most confident ones as new seeds to re-train the model. This iterative procedure has been successfully applied to a variety of NLP tasks, such as hypernym/hyponym extraction (Hearst, 1992), word sense disambiguation (Yarowsky, 1995), question answering (Ravichandran and Hovy, 2002), and information extraction (Brin, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Yangarber et al., 2000; Chen and Ji, 2009). While SSL can give good performance for many tasks, it is a procedure born with two defects. One is semantic drift. When SSL is under-constrained, the semantics of newly promoted examples might stray away from the original meaning of seed examples as discussed in (Brin, 1998; Curran et al., 2007; Carlson et al., 2010). For example, a SSL procedure to learn semantic patterns for the LocatedIn relation (PERSON in LOCATION/GPE1) might accept patterns for the Employment relation (employee of GPE / ORGANIZATION) because many unlabeled pairs of names are connected by patterns belonging to multiple"
C10-2137,P04-1053,1,0.908925,"Missing"
C10-2137,C92-2082,0,0.0279739,"d new domains all the time. Without enough labeled data of a new task or a new domain to conduct supervised learning, semi-supervised learning (SSL) is particularly attractive to NLP researchers since it only requires a handful of labeled examples, known as seeds. SSL starts with these seeds to train an initial model; it then applies this model to a large volume of unlabeled data to get more labeled examples and adds the most confident ones as new seeds to re-train the model. This iterative procedure has been successfully applied to a variety of NLP tasks, such as hypernym/hyponym extraction (Hearst, 1992), word sense disambiguation (Yarowsky, 1995), question answering (Ravichandran and Hovy, 2002), and information extraction (Brin, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Yangarber et al., 2000; Chen and Ji, 2009). While SSL can give good performance for many tasks, it is a procedure born with two defects. One is semantic drift. When SSL is under-constrained, the semantics of newly promoted examples might stray away from the original meaning of seed examples as discussed in (Brin, 1998; Curran et al., 2007; Carlson et al., 2010). For example, a SSL p"
C10-2137,P08-1068,0,0.102902,"Missing"
C10-2137,P09-1116,0,0.0374889,"lp from clusters. There are also clear connections to work on unsupervised relation discovery (Hasegawa et al., 2004; Zhang et al., 2005; Rosenfeld and Feldman, 2007). They group pairs of names into relation clusters based on the contexts between names while we group the contexts/patterns into clusters based on features extracted from names. 6 5 Related Work Recent research starts exploring unlabeled data for discriminative learning. Miller et al., (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. Other similar work includes (Wong and Ng, 2007) for name tagging, and (Koo et. al., 2008) for dependency parsing. While similar in spirit, our supervision is minimal, i.e., we only use a few seeds while the above approaches rely on a large amount of labeled data. To the best of our knowledge, the theme explored in this paper is the first study of using pattern clusters for preventing semantic drift in"
C10-2137,W08-1301,0,0.0826229,"Missing"
C10-2137,N04-1043,0,0.0675912,"(Yangarber et al., 2000), and many others. While they conduct bootstrapping on unlabeled data directly, we first cluster unlabeled data and then bootstrap with help from clusters. There are also clear connections to work on unsupervised relation discovery (Hasegawa et al., 2004; Zhang et al., 2005; Rosenfeld and Feldman, 2007). They group pairs of names into relation clusters based on the contexts between names while we group the contexts/patterns into clusters based on features extracted from names. 6 5 Related Work Recent research starts exploring unlabeled data for discriminative learning. Miller et al., (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. Other similar work includes (Wong and Ng, 2007) for name tagging, and (Koo et. al., 2008) for dependency parsing. While similar in spirit, our supervision is minimal, i.e., we only use a few seeds while the above approaches rely on a large amount"
C10-2137,P06-1015,0,0.070661,"graph. 1195 2.3 Pre-processing We tag and parse each sentence in our corpus with the NYU named entity tagger 3 and the Stanford dependency parser. Then for each pair of names in the dependency tree, we extract the SDP connecting them. Names in the path are replaced by their types. We require SDP to contain at least one verb or noun. We use the base form of words in SDP. We also require the length of the path (defined as the number of dependency relations and words in it) to be between 3 and 7. Short paths are more likely to be generic patterns such as “of” and can be handled separately as in (Pantel and Pennacchiotti, 2006). Very long paths are more likely to be non-relation patterns and too sparse to be useful even if they are relation patterns. 2.4 Clustering Algorithm The basic idea of our clustering algorithm is to group all the paths (including the seed paths used later for SSL) in our corpus into different clusters based on distributional similarities. We first extract a variety of features from the named entities X and Y connected by a path P as shown in Table 1. We then compute an analogue of tfidf for each feature f of P as follows: tf as the number of corpus instances of P having feature f divided by t"
C10-2137,P02-1006,0,0.180548,"Missing"
C10-2137,W06-0202,0,0.0150086,"share many common Xs and Ys and hence are similar to each other. This extended distributional hypothesis serves as the basis on which we compute similarities for each pair of patterns. 2.2 Pattern Representation — Shortest Dependency Path We adopt a shortest dependency path (SDP) representation of relation patterns. SDP has demonstrated its power in kernel methods for relation extraction (Bunescu and Mooney, 2005). Its capability in capturing most of the information of interest is also evidenced by a systematic comparison of effectiveness of different information extraction (IE) patterns in (Stevenson and Greenwood, 2006) 2 . For example, “nsubj Å met Æ prep_in” is able to represent LocatedIn between “Gates” and “Seattle” while a token-based pattern would be much less general because it would have to specify all the intervening tokens. Figure 1. Stanford dependency tree for sentence “Gates, Microsoft’s chairman, met with President Clinton in Seattle”. 2 SDP is equivalent to the linked chains described in Stevenson and Greenwood (2006) when the dependency of a sentence is represented as a tree not a graph. 1195 2.3 Pre-processing We tag and parse each sentence in our corpus with the NYU named entity tagger 3 an"
C10-2137,P05-1047,0,0.184672,"Missing"
C10-2137,R09-2014,1,0.620024,"y will be moved to the target dependencies in the dependency path.) We work on these three relations mainly cluster and invoke the recomputation of because of the availability of benchmark Cluster_Conf of NE pairs connected by these evaluation data. These are the most frequent patterns. The ranking functions in step 1 and 2 relations in our evaluation data. Conf (Ni ) = 2• ∏ 4 The Cluster_Conf of <Clinton, Arkansas&gt; related to the LocatedIn relation is indeed very low (less than 0.1) in our experiments. 5 We provide more seeds (executives and staff) for EMP because it has been pointed out in (Sun, 2009) that EMP contains a lot of job titles. 1198 4.3 Unsupervised Experiments We run the clustering algorithm described in Section 2 using all the 37 years’ data. We require that a pattern match at least 7 distinct NE pairs and that an NE pair must be connected by at least 7 unique patterns. As a result, there are 635,128 patterns (22,225 unique ones) used in experiments. We use 0.005 as the cutoff threshold of complete linkage. The threshold is decided by trying a series of thresholds and searching for the maximal6 one that is capable of placing the seed patterns for each relation into a single c"
C10-2137,P03-1044,0,0.122059,"Missing"
C10-2137,P95-1026,0,0.0895285,"labeled data of a new task or a new domain to conduct supervised learning, semi-supervised learning (SSL) is particularly attractive to NLP researchers since it only requires a handful of labeled examples, known as seeds. SSL starts with these seeds to train an initial model; it then applies this model to a large volume of unlabeled data to get more labeled examples and adds the most confident ones as new seeds to re-train the model. This iterative procedure has been successfully applied to a variety of NLP tasks, such as hypernym/hyponym extraction (Hearst, 1992), word sense disambiguation (Yarowsky, 1995), question answering (Ravichandran and Hovy, 2002), and information extraction (Brin, 1998; Collins and Singer, 1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Yangarber et al., 2000; Chen and Ji, 2009). While SSL can give good performance for many tasks, it is a procedure born with two defects. One is semantic drift. When SSL is under-constrained, the semantics of newly promoted examples might stray away from the original meaning of seed examples as discussed in (Brin, 1998; Curran et al., 2007; Carlson et al., 2010). For example, a SSL procedure to learn semantic patterns for the"
C10-2137,I05-1034,0,0.191872,"Missing"
C10-2137,I08-1005,0,0.046287,"Missing"
C10-2137,C00-2136,1,\N,Missing
C10-2137,P08-1000,0,\N,Missing
C10-2137,W99-0613,0,\N,Missing
C10-2137,W06-0200,0,\N,Missing
C12-1177,P05-1074,0,0.0667073,"Missing"
C12-1177,N03-1003,0,0.0552004,"Missing"
C12-1177,D08-1021,0,0.0816547,"Missing"
C12-1177,C08-1013,0,0.0540476,"Missing"
C12-1177,C96-2183,0,0.047339,"Missing"
C12-1177,P11-1020,0,0.164662,"Missing"
C12-1177,P09-1053,0,0.0276086,"Missing"
C12-1177,C04-1051,1,0.720944,"Missing"
C12-1177,C04-1088,0,0.0176159,"Missing"
C12-1177,W07-1401,1,0.326763,"Missing"
C12-1177,D10-1051,0,0.0161918,"Missing"
C12-1177,P07-2045,0,0.0606319,"Missing"
C12-1177,N10-1017,0,0.0920108,"Missing"
C12-1177,D10-1090,0,0.0393625,"Missing"
C12-1177,J10-3003,0,0.0626954,"Missing"
C12-1177,moore-2002-fast,0,0.0606931,"Missing"
C12-1177,J03-1002,0,0.0531454,"Missing"
C12-1177,P02-1040,0,0.110879,"Missing"
C12-1177,W04-3219,0,0.0160521,"Missing"
C12-1177,P10-2008,0,0.0191379,"Missing"
C12-1177,W03-1609,0,0.0609861,"Missing"
C12-1177,D08-1027,0,0.0349755,"Missing"
C12-1177,N10-1056,0,0.0714576,"Missing"
C12-1177,P03-1021,0,\N,Missing
C80-1075,P79-1025,1,\N,Missing
C82-1014,C82-2029,1,0.713995,"re of the same or closely related classes. In ""Patient had stiff neck and fever"" there are two readings. The reading in which ""stiff"" is the left adjunct of both ""neck"" a n d ""fever"" is eliminated because ""neck"" and ""fever"" have different subclasses: ""fever"" is a SIGN-SYMPTOM word whereas ""neck"" is a BODY-PART word. However the phrase ""stiff neck"" has a SIGN-SYMPTOM ""computed attribute"" and is in the same class as ""fever""~ therefore we do get the analysis where ""fever"" is conjoined to ""stiff neck"". A more detailed description of constraints on noun phrase conjunction is described by Hirschman [8]. FORMATTING The format itself can be viewed as a derivative of the DIS, obtained by merging several predicate-argument relations into a single larger relation. Because the formats, like the predicate-argument relations, are based on the semantic classes of the DIS, the mapping from decomposition trees into formats can be driven by a table of the correspondences between semantic classes and format columns. QUESTION-ANSWERING The predicate names used in the predicate calculus representation within the question-answering system correspond to the predicate-argument patterns of semantic classes in"
C90-3023,J88-2005,0,0.141941,"ich correspond to time intervals over which a certain state holds or a certain activity is taking place (we call such states and activities elementary facts). In addition, the time graph has directed edges which represent the relative time ordering of the elementary facts and the causal relationships between them. This graph is created in throe phrases: creation of elementary facts; analysis of explicit temporal relations; and causal analysis. Our approach to temporal analysis, which is described in more fully in [31] and [4], has been influenced by earlier work by Dowty [ 1] ,'rod Passonneau [6]. The Language Analyzer The language analyzer has three top-level como ponents: syntactic analysis, semantic analysis, and discourse analysis. Syntactic and semantic analysis are applied to each sentence in turn; discouese analysis is applied to the entire report at the end of processing. Syntax analysis is pertbrmed using an augmented context-fi'ee grammar based on linguistic string theory. Tim parse tree is regularized (primarily transforming &quot;all clause structures into a standard torm) by a set of translation rules associated with the grammar productions and applied compositionally. Semanti"
C90-3071,P86-1004,0,0.0348855,"Missing"
C90-3071,A88-1029,0,0.0309422,"Missing"
C90-3071,H86-1011,0,\N,Missing
C90-3071,A88-1018,0,\N,Missing
C90-3071,H89-1032,0,\N,Missing
C90-3071,H89-2011,1,\N,Missing
C90-3071,C82-1032,0,\N,Missing
C92-2099,H90-1056,0,0.0587746,"the use of p r o d u c t s is consistent w i t h the idea t h a t the score for a triple to some degree reflects the probability t h a t this triple is s e m a n t i c a l l y valid). The parse or parses with the highest t o t a l score are then selected for evaluation. We tested three approaches to assigning a score to a triple: 1. We used the frequency of head-functionvalue triples relative to the frequency of the head as an e s t i m a t e of the p r o b a b i l i t y t h a t this head would a p p e a r with this functionvalue combination. We used the &quot;expected likelihood e s t i m a t e &quot; [8] in order to assure t h a t triples which do not a p p e a r in the training corpus are still assigned non-zero probability; this simple e s t i m a t o r adds 1/2 to each observed frequency: score = 0.82 if freq. of triple &gt; 0.9 0.47 if freq. of triple &lt; 0.9 score = 0.52 if freq. of pair &gt; 0.9, else 0.40 if freq. of pair &lt; 0.9 Using these three scoring flmctions for selection, we parsed our test set of sentences and then scored the resulting parses a g a i n s t our &quot; s t a n d a r d parses&quot;. As a further comparison, we also parsed the same set using selectional c o n s t r a i n t s which ha"
C92-2099,J86-3002,1,0.903596,"Missing"
C92-2099,H90-1053,1,0.87984,"o m a i n and have no s e m a n t i c constraints, we lnust rely entirely upon s y n t a c t i c c o n s t r a i n t s and so will be confronted with a large n u m b e r of incorrect parses for each sentence, along with (hopefully) the correct one. We have exl)erim e n t e d with several a p p r o a c h e s to dealing with this problem: 1. If a sentence has N parses, we can generate triples front all the parses and tllen include each triple with a weight of 1/N. 2. We can g e n e r a t e a s t o c h a s t i c g r a m m a r t h r o u g h unsupervised t r a i n i n g on a portion of the corpus [2]. We can then parse the corpus with this s t o c h a s t i c g r a m m a r a t , l take ACRESDECOLING-92, NANrES, 23-28 Ao(rr 1992 659 All of these approaches rely on the e x p e c t a t i o n t h a t correct p a t t e r n s arising from correct parses will occur repeatedly, while the d i s t r i b u t i o n of incorrect p a t t e r n s from incorrect parses will be more scattered, and so over a sufficiently large c o r p u s - we cat, distinguish correct from incorrect p a t t e r n s on the basis of frequency. 5 Evaluation M e t h o d s ~['o g a t h e r p a t t e r n s , we analyzed a series"
C92-2099,H90-1052,0,0.0565849,"Missing"
C92-2099,J91-2002,0,0.151714,"Missing"
C92-2099,H91-1059,0,0.056188,"repeatedly, while the d i s t r i b u t i o n of incorrect p a t t e r n s from incorrect parses will be more scattered, and so over a sufficiently large c o r p u s - we cat, distinguish correct from incorrect p a t t e r n s on the basis of frequency. 5 Evaluation M e t h o d s ~['o g a t h e r p a t t e r n s , we analyzed a series of articles on terrorism which were o b t a i n e d from tim Foreign l l r o a d c a s t l n h ) r m a t i o n Service and used as the d e v e l o p m e n t era'pus for the T h i r d Message U n d e r s t a n d i n g Confiwence (held in San Diego, CA, May 1991) [4]. l'br p a t t e r n collection, we used 1000 such articles with a total of :14,196 sentences and 330,769 wor(ls. Not all sentences parsed, both because of l i m i t a t i o n s in our gramm a r and becanse we inlpose a l i m i t on the search which the parser can perform for each sentence. W i t h i n these limits, we were able to parse a total of 7,455 s e n t e n c e s J The most clearly definable function of tbe triples we collect is to act as a selectional constraint: to differentiate between meaningfld and meaningless triples in new text, and thus identify the correct attalysls. We used"
C92-2099,P91-1036,0,0.162056,"Missing"
C92-2099,H91-1060,0,0.0491879,"second evaluation method involves the use of the triples in selection and a comparison of the parses produced against a set of known correct parses. In this case the known correct parses were prepared manually by the University of Pennsylvania as part of their &quot;'Free Bank&quot; project. For this evaluation, we used a set of 317 sentences, again distinct from the training set, In comparing the parser output against the standard trees, we measured the degree to which the tree structures coincide, stated as recall, precision, and number of crossings. These measures have been defined in earlier papers [5,6,7]. v+ number of triples in test set which were classified as valid and which appeared in training set with count &gt; T Our first set of experiments were conducted to compare three methods of coping with multiple parses. These methods, as described in section 4, are (1) generating all N parses of a sentence, and weighting each by l/N; (2)selecting the N most likely parses as determined by a stochastic grammar, and weighting those each by 1/N; (3) generating all parses, but assigning weights to alternative parses using a form of the inside-outside procedure. These experiments were conducted using a"
C92-2099,A92-1022,1,0.868563,"second evaluation method involves the use of the triples in selection and a comparison of the parses produced against a set of known correct parses. In this case the known correct parses were prepared manually by the University of Pennsylvania as part of their &quot;'Free Bank&quot; project. For this evaluation, we used a set of 317 sentences, again distinct from the training set, In comparing the parser output against the standard trees, we measured the degree to which the tree structures coincide, stated as recall, precision, and number of crossings. These measures have been defined in earlier papers [5,6,7]. v+ number of triples in test set which were classified as valid and which appeared in training set with count &gt; T Our first set of experiments were conducted to compare three methods of coping with multiple parses. These methods, as described in section 4, are (1) generating all N parses of a sentence, and weighting each by l/N; (2)selecting the N most likely parses as determined by a stochastic grammar, and weighting those each by 1/N; (3) generating all parses, but assigning weights to alternative parses using a form of the inside-outside procedure. These experiments were conducted using a"
C92-2099,J93-1005,0,\N,Missing
C92-2099,C90-3010,0,\N,Missing
C94-1042,J93-2001,0,0.023363,"Missing"
C94-1042,J93-2002,0,0.505393,"Missing"
C94-1042,P91-1030,0,0.0281914,"Missing"
C94-1042,H93-1061,0,0.0129667,"Missing"
C94-1042,J81-4005,0,0.127209,"s (countable :pval (""wlth"")), indicating that it must appear in the singular with a deter,niner unless it is preceded by the preposZion ""with"". 2.1 Subcategorization We have paid p~u'ticular attention to providing detailed subcategorization information (information about complement structure), both for verbs and for tllose nouns and adjectives which do take cmnl)lements. In order to insure the COml)leteness of our codes, we studied the codiug e)ul)loyed by s(weral other u,ajor texicous, includh,g (,he Ih'andeis Verh Lexlcolt 2, the A(JQIJII,EX Prc,ject [10], the NYU Linguistic String l'roject [9], the OALI), and IA)OCI'], a,nd, whenever feasiMe, haw~ sought to incorporate distinctions made in a n y o f t h e s e all(tie,tortes. ()ur r e s u l t i n g feature systen, includes 92 subcategorization features Ibr w~rbs, 14 for adjectives, and 9 for llO,,ns. These features record dilforences in grammatical functional structure as well as constituent structure. In particular, tl,ey Calfl.ure four different types of control: subject control, object control, variable control, and arbitrary control. Furthermore, the notation allows us to indicate that verl) Irlay haw~ dill>rent control features"
C94-1042,P84-1044,0,0.075567,"r example, the noun ""abandon"" is marked as (countable :pval (""wlth"")), indicating that it must appear in the singular with a deter,niner unless it is preceded by the preposZion ""with"". 2.1 Subcategorization We have paid p~u'ticular attention to providing detailed subcategorization information (information about complement structure), both for verbs and for tllose nouns and adjectives which do take cmnl)lements. In order to insure the COml)leteness of our codes, we studied the codiug e)ul)loyed by s(weral other u,ajor texicous, includh,g (,he Ih'andeis Verh Lexlcolt 2, the A(JQIJII,EX Prc,ject [10], the NYU Linguistic String l'roject [9], the OALI), and IA)OCI'], a,nd, whenever feasiMe, haw~ sought to incorporate distinctions made in a n y o f t h e s e all(tie,tortes. ()ur r e s u l t i n g feature systen, includes 92 subcategorization features Ibr w~rbs, 14 for adjectives, and 9 for llO,,ns. These features record dilforences in grammatical functional structure as well as constituent structure. In particular, tl,ey Calfl.ure four different types of control: subject control, object control, variable control, and arbitrary control. Furthermore, the notation allows us to indicate that ver"
C94-1042,P93-1032,0,\N,Missing
C94-2119,J86-3002,1,\N,Missing
C94-2119,J91-2002,0,\N,Missing
C94-2119,H93-1050,1,\N,Missing
C94-2119,C92-2085,0,\N,Missing
C94-2119,C92-2099,1,\N,Missing
C94-2119,P93-1022,0,\N,Missing
C94-2119,P90-1034,0,\N,Missing
C94-2119,P92-1053,0,\N,Missing
C94-2119,P92-1023,0,\N,Missing
C94-2119,P93-1024,0,\N,Missing
C96-1078,C94-1015,0,\N,Missing
C96-1078,J93-2003,0,\N,Missing
C96-1078,C90-3044,0,\N,Missing
C96-1078,C90-3031,0,\N,Missing
C96-1078,C92-2101,0,\N,Missing
C96-1078,P85-1016,0,\N,Missing
C96-1078,P93-1004,0,\N,Missing
C96-1078,P85-1017,0,\N,Missing
C96-1078,P93-1002,0,\N,Missing
C96-1078,1992.tmi-1.23,1,\N,Missing
C98-2134,C90-3001,0,0.0674849,"Missing"
C98-2134,C94-1015,0,0.161477,"Missing"
C98-2134,C92-2101,0,0.764355,"Missing"
C98-2134,P93-1004,0,0.417268,"Missing"
C98-2134,C96-1078,1,0.927343,"Rules from Dominance-Preserving Alignments Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, Antonio Moreno-Sandoval* New York University 715 Broadway, 7th Floor, NY, NY 10003, USA tUniversidad Autdnoma de Madrid Cantoblanco, 28049-Madrid, SPAIN meyers/roman/grishman/macleod©cs, nyu. edu sandoval©lola.lllf.uam.es 1 Introduction Automatic acquisition of translation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et al., 1992), (Matsumoto et al., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (I{aji et al.. 1992), and (Furuse and Iida. 1994)). This paper I describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this a"
C98-2134,C90-3044,0,0.435501,"lation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et al., 1992), (Matsumoto et al., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (I{aji et al.. 1992), and (Furuse and Iida. 1994)). This paper I describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived bv ""cutting up"" the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a transla.tion of the source tree. This technique resembles work on MT using synchr"
calzolari-etal-2002-towards,J87-3006,0,\N,Missing
calzolari-etal-2002-towards,bel-etal-2000-simple,1,\N,Missing
D11-1119,P10-1089,0,0.400416,"y slightly. 1 Introduction The function of context-sensitive text correction is to identify word-choice errors in text (Bergsma et al., 2009). It can be viewed as a lexical disambiguation task (Lapata and Keller, 2005), where a system selects from a predefined confusion word set, such as {affect, effect} or {complement, compliment}, and provides the most appropriate word choice given the context. Typically, one determines if a word has been used correctly based on lexical, syntactic and semantic information from the context of the word. One of the top performing models of spelling correction (Bergsma et al., 2010) is based on web-scale n-gram counts, which reflect both syntax and meaning. However, even with a large-scale n-gram corpus, data sparsity can hurt performance in two ways. ∗ This work was done when the first author was an intern for Educational Testing Service. Take a sentence from The New York Times (NYT) for example: “‘This fellow’s won a war,’ the dean of the capital’s press corps, David Broder, announced on ‘Meet the Press’ after complimenting the president on the ‘great sense of authority and command’ he exhibited in a flight suit.” Unfortunately, neither the phrase “complementing the pr"
D11-1119,W07-1604,1,0.878565,"antic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for"
D11-1119,W02-1005,0,0.0287773,"udanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model"
D11-1119,de-marneffe-etal-2006-generating,0,0.0106983,"Missing"
D11-1119,P08-2008,0,0.0201236,"ey focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the tes"
D11-1119,P97-1067,0,0.0409743,"mplimenting the president” exists in the web-scale Google N-gram corpus (Brants and Franz, 2006). The n-gram models decide solely based on the frequency of the bi-grams “after comple(i)menting” and “comple(i)menting the”, which are common usages for both words. The real question is whether we are more likely to “compliment” or “complement” a person, the “president”. Several clues could help us answer that question. A dependency parser can identify the word “president” as the subject of “compliment” or “complement” which also may be the case in some of the training data. Lexical co-occurrence (Edmonds, 1997) and semantic word relatedness measurements, such as Random Indexing (Sahlgren, 2006), could provide evidence that “compliment” is more likely to co-occur with “president” than “complement”. Fur1291 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291–1300, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics thermore, some important clues can be quite distant from the target word, e.g. outside the 9-word context window Bergsma et al. (2010) and Carlson (2007) used. Consider another sentence in the NYT corpus,"
D11-1119,P98-1059,0,0.0306819,"sight, site}, {peace, piece} and {raise, rise}. They reported that the SVM model with NG features outperformed its unsupervised version, sumLM. However, the limited confusion word sets they evaluated may not comprehensively represent the word usage errors that writers typically make. In this paper, we test nine additional commonly confused word pairs to expand the scope of the evaluation. These words were selected based on their lower frequencies compared to the five pairs in the above work (as shown later in Table 2). 1293 3 Enhanced N-gram Models with Parse Features To our knowledge, only (Elmi and Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et"
D11-1119,C08-1022,0,0.0241586,"Missing"
D11-1119,I08-1059,0,0.0242171,"use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for contextual spelling correction and other lexical disambiguation tasks. These systems make the word choice depending on how frequently each candidate word has been seen in the given context in web-scale data. As n-gram data has become more readily available, such as the Google N-gram Corpus, the likelihood of a word being used in a certain context can be better estimated. Bergsma et al. (2009; 2010) presented a series of simple but powerful models which relied heavily on web-scale n-gram counts."
D11-1119,W95-0104,0,0.419802,"e intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has be"
D11-1119,hermet-etal-2008-using,0,0.0660695,"Missing"
D11-1119,P03-2026,0,0.0382898,"redicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to"
D11-1119,A97-1025,0,0.0775056,"and how our approach differs from these approaches. In Sections 3 and 4, we discuss our methods for using parse features and word co-occurrence information. In Section 5, we present experimental results and analysis. 2 Related Work A variety of approaches have been proposed for context-sensitive spelling correction ranging from semantic methods to machine learning classifiers to large-scale n-gram models. Some semantics-based systems have been developed based on an intuitive assumption that the intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), trans"
D11-1119,W06-1605,0,0.0690266,"Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for conte"
D11-1119,N10-1018,0,0.0119634,"ny word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion words. The problem with mor"
D11-1119,C08-1109,1,0.808685,"istance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In"
D11-1119,P10-2065,1,0.802253,"Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by differ"
D11-1119,D09-1093,0,0.0288853,"g errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion"
D11-1119,C98-1057,0,\N,Missing
D12-1094,P08-1004,0,0.0613743,"Missing"
D12-1094,P11-1062,0,0.0127454,"Missing"
D12-1094,P04-1056,0,0.0133143,"Missing"
D12-1094,I05-2045,0,0.0163377,"Missing"
D12-1094,D11-1142,0,0.258339,"Missing"
D12-1094,P04-1053,1,0.931988,"Missing"
D12-1094,C92-2082,0,0.229379,"complexity. 4.1 Knowledge Sources Entity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and anot"
D12-1094,P08-1119,0,0.342981,"ings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist sever"
D12-1094,N04-1041,0,0.0169342,"these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity:"
D12-1094,I05-1011,0,0.0506813,"Missing"
D12-1094,D09-1025,0,0.0418617,"Missing"
D12-1094,I05-5011,0,0.0637491,"Missing"
D12-1094,C10-1112,1,0.817003,"been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy c"
D12-1094,D08-1061,0,0.0252795,"ge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph"
D12-1094,P10-2068,0,0.0211762,"Missing"
D12-1094,N09-1033,0,0.0312473,"Missing"
D12-1094,P09-1050,0,0.0297532,"Missing"
D12-1094,P10-1013,0,0.166586,"Missing"
D12-1094,P03-1016,0,0.079683,"Missing"
D12-1094,D11-1135,0,0.228249,"Missing"
D12-1094,N07-1016,0,0.124788,"Missing"
D12-1094,P11-1116,1,0.779565,"ity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph for relation phrases"
D12-1094,P09-1052,1,0.836204,"tity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hype"
D12-1094,N06-1039,0,\N,Missing
D12-1094,D09-1098,0,\N,Missing
D16-1085,W06-0901,0,0.0361632,"other models on the source domain. This is consistent with the conclusions in Section 3.2 and further confirms the effectiveness of NC-CNN. More importantly, NC-CNN outperforms CNN and the other models on the target domains bc, cts and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED ver"
D16-1085,W06-1615,0,0.155357,"Missing"
D16-1085,R15-1011,1,0.911002,"Missing"
D16-1085,R15-1010,1,0.893886,"Missing"
D16-1085,P15-1017,0,0.758339,"state-of-the-art systems. 1 Introduction The goal of event detection (ED) is to locate event triggers of some specified types in text. Triggers are generally single verbs or nominalizations that evoke the events of interest. This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might appear in various expressions, and an expression might express different events depending on contexts. The current state-of-the-art systems for ED have involved the application of convolutional neural networks (CNN) (Nguyen and Grishman, 2015b; Chen et al., 2015) that automatically learn effective feature representations for ED from sentences. This has overcome the two fundamental limitations of the traditional feature-based methods for ED: (i) the complicated feature engineering for rich feature sets and (ii) the error propagation from the NLP toolkits and resources (i.e, parsers, part of speech taggers etc) that generate such features. The prior CNN models for ED are characterized by the temporal convolution operators that linearly map the vectors for the k-grams in the sentences into the feature space. Such k-gram vectors are obtained by concatenat"
D16-1085,P07-1033,0,0.0215556,"Missing"
D16-1085,P09-2093,0,0.0512251,"N. More importantly, NC-CNN outperforms CNN and the other models on the target domains bc, cts and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen e"
D16-1085,P11-1113,0,0.123082,"valuation criteria follow those in (Nguyen and Grishman, 2015b). 3.2 The General Setting We compares the non-consecutive CNN model (NCCNN) with the state-of-the-art systems on the ACE 2005 dataset in Table 1. These systems include: 1) The feature-based systems with rich handdesigned feature sets, including: the MaxEnt model with local features in (Li et al., 2013) (MaxEnt); the structured perceptron model for joint beam search with local features (Joint+Local), and with both local and global features (Joint+Local+Global) in (Li et al., 2013); and the sentence-level and cross-entity models in (Hong et al., 2011). 2) The neural network models, i.e, the CNN model in (Nguyen and Grishman, 2015b) (CNN), the dynamic multi-pooling CNN model (DM-CNN) in (Chen et al., 2015) and the bidirectional recurrent neural networks (B-RNN) in (Nguyen et al., 2016a). 3) The probabilistic soft logic based model to capture the event-event correlation in (Liu et al., 2016). Methods Sentence-level in Hong et al (2011) MaxEnt (Li et al., 2013) Joint+Local (Li et al., 2013) Joint+Local+Global (Li et al., 2013) Cross-entity in Hong et al. (2011) † Probabilistic soft logic (Liu et al., 2016) † CNN (Nguyen and Grishman, 2015b) D"
D16-1085,N16-1056,0,0.059859,"ecent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and argument labeling jointly, (Jagannatha and Yu, 2016) who extract event instances from health records with recurrent neural networks and (Nguyen et al., 2016b) who propose a two-stage training algorithm for event extension with neural networks. 4 We present a new CNN architecture for ED that exploits the non-consecutive convolution for sentences. Our evaluation of the proposed model on the general setting and the DA setting demonstrates the effectiveness of the non-consecutive mechanism. We achieve the state-of-the-art performance for ED in both settings. In the future, we plan to investigate the non-consecutive architecture on other problems su"
D16-1085,P08-1030,1,0.883064,"effectiveness of NC-CNN. More importantly, NC-CNN outperforms CNN and the other models on the target domains bc, cts and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work"
D16-1085,D15-1180,0,0.0148629,"pose to improve the previous CNN models for ED by operating the convolution on all possible non-consecutive k-grams 886 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 886–891, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics in the sentences. We aggregate the resulting convolution scores via the max-pooling function to unveil the most important non-consecutive k-grams for ED. The aggregation over all the possible nonconsecutive k-grams is made efficient with dynamic programming. Note that our work is related to (Lei et al., 2015) who employ the non-consecutive convolution for the sentence and news classification problems. Our work is different from (Lei et al., 2015) in that we model the relative distances of words to the trigger candidates in the sentences via position embeddings, while (Lei et al., 2015) use the absolute distances between words in the k-grams to compute the decay weights for aggregation. To the best of our knowledge, this is the first work on non-consecutive CNN for ED. We systematically evaluate the proposed model in the general setting as well as the domain adaptation setting. The experiment resul"
D16-1085,P13-1008,0,0.352819,"arameters and resources as (Nguyen and Grishman, 2015b) to ensure the compatible comparison. Specifically, we employ the window sizes in the set {2, 3, 4, 5} for the convolution operation with 150 filters for each window size. The window size of the trigger candidate is 31 while the dimensionality of the position embeddings and entity type embeddings is 50. We use word2vec from (Mikolov et al., 2013b) as the pretrained word embeddings. The other parameters include the dropout rate ⇢ = 0.5, the mini-batch size = 50, the predefined threshold for the l2 norms = 3. Following the previous studies (Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015b), we evaluate the models on the ACE 2005 corpus 2 : 0  i1 < i2 < . . . < ik  2n} 1 1 We ignore the base cases as they are trivial. with 33 event subtypes. In order to make it compatible, we use the same test set with 40 newswire articles, the same development set with 30 other documents and the same training set with the remaining 529 documents. All the data preprocessing and evaluation criteria follow those in (Nguyen and Grishman, 2015b). 3.2 The General Setting We compares the non-consecutive CNN model (NCCNN) with the state-of-the-art syste"
D16-1085,W15-4502,1,0.86005,"comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and argument labeling jo"
D16-1085,R11-1002,1,0.855704,"he other models on the target domains bc, cts and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neura"
D16-1085,P11-1163,0,0.034387,"s and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and ar"
D16-1085,W15-1506,1,0.0835837,"mprovement over the current state-of-the-art systems. 1 Introduction The goal of event detection (ED) is to locate event triggers of some specified types in text. Triggers are generally single verbs or nominalizations that evoke the events of interest. This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might appear in various expressions, and an expression might express different events depending on contexts. The current state-of-the-art systems for ED have involved the application of convolutional neural networks (CNN) (Nguyen and Grishman, 2015b; Chen et al., 2015) that automatically learn effective feature representations for ED from sentences. This has overcome the two fundamental limitations of the traditional feature-based methods for ED: (i) the complicated feature engineering for rich feature sets and (ii) the error propagation from the NLP toolkits and resources (i.e, parsers, part of speech taggers etc) that generate such features. The prior CNN models for ED are characterized by the temporal convolution operators that linearly map the vectors for the k-grams in the sentences into the feature space. Such k-gram vectors are o"
D16-1085,P15-2060,1,0.700861,"Missing"
D16-1085,P15-1062,1,0.455113,"Missing"
D16-1085,N16-1034,1,0.745221,"he feature-based systems with rich handdesigned feature sets, including: the MaxEnt model with local features in (Li et al., 2013) (MaxEnt); the structured perceptron model for joint beam search with local features (Joint+Local), and with both local and global features (Joint+Local+Global) in (Li et al., 2013); and the sentence-level and cross-entity models in (Hong et al., 2011). 2) The neural network models, i.e, the CNN model in (Nguyen and Grishman, 2015b) (CNN), the dynamic multi-pooling CNN model (DM-CNN) in (Chen et al., 2015) and the bidirectional recurrent neural networks (B-RNN) in (Nguyen et al., 2016a). 3) The probabilistic soft logic based model to capture the event-event correlation in (Liu et al., 2016). Methods Sentence-level in Hong et al (2011) MaxEnt (Li et al., 2013) Joint+Local (Li et al., 2013) Joint+Local+Global (Li et al., 2013) Cross-entity in Hong et al. (2011) † Probabilistic soft logic (Liu et al., 2016) † CNN (Nguyen and Grishman, 2015b) DM-CNN (Chen et al., 2015) B-RNN (Nguyen et al., 2016a) NC-CNN F 59.7 65.9 65.7 67.5 68.3 69.4 69.0 69.1 69.3 71.3 sentence-level information, it is still better than the other models that further exploit the document-level information fo"
D16-1085,W16-1618,1,0.812027,"he feature-based systems with rich handdesigned feature sets, including: the MaxEnt model with local features in (Li et al., 2013) (MaxEnt); the structured perceptron model for joint beam search with local features (Joint+Local), and with both local and global features (Joint+Local+Global) in (Li et al., 2013); and the sentence-level and cross-entity models in (Hong et al., 2011). 2) The neural network models, i.e, the CNN model in (Nguyen and Grishman, 2015b) (CNN), the dynamic multi-pooling CNN model (DM-CNN) in (Chen et al., 2015) and the bidirectional recurrent neural networks (B-RNN) in (Nguyen et al., 2016a). 3) The probabilistic soft logic based model to capture the event-event correlation in (Liu et al., 2016). Methods Sentence-level in Hong et al (2011) MaxEnt (Li et al., 2013) Joint+Local (Li et al., 2013) Joint+Local+Global (Li et al., 2013) Cross-entity in Hong et al. (2011) † Probabilistic soft logic (Liu et al., 2016) † CNN (Nguyen and Grishman, 2015b) DM-CNN (Chen et al., 2015) B-RNN (Nguyen et al., 2016a) NC-CNN F 59.7 65.9 65.7 67.5 68.3 69.4 69.0 69.1 69.3 71.3 sentence-level information, it is still better than the other models that further exploit the document-level information fo"
D16-1085,D09-1016,0,0.068529,"NC-CNN outperforms CNN and the other models on the target domains bc, cts and un, and performs comparably with CNN on wl. The performance improvement is significant on bc and un (p < 0.05), thereby verifying the robustness of NC-CNN for ED across domains. Second, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidi"
D16-1085,P13-1147,0,0.042707,"Missing"
D16-1085,N10-1123,0,0.082444,"approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and argument labeling jointly, (Jagannatha and Yu, 2016) who extract event instances from health records with recurrent neural networks and (Nguyen et al., 2016b) who propose a two-stage training algorithm for event exten"
D16-1085,W09-1406,0,0.0606611,"d, the feature-based approach relies on linguistic intuition to design effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and argument labeling jointly, (Jagannatha and Yu, 2016) who extract event instances from health records with recurrent neural networks and (Nguyen et al., 2016b) who propose a two-stage traini"
D16-1085,P10-1040,0,0.0247854,"limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. Let 2n + 1 be the fixed window size, and W = [w0 , w1 , . . . , wn , . . . , w2n 1 , w2n ] be some trigger candidate where the current token is positioned in the middle of the window (token wn ). Before entering CNN, each token wi is first transformed into a real-valued vector xi using the concatenation of the following vectors: 1. The word embedding vector of wi : This is obtained by looking up a pre-trained word embedding table D (Turian et al., 2010; Mikolov et al., 2013a). 887 2. The position embedding vector of wi : We obtain this vector by looking up the position embedding table for the relative distance i n from the token wi to the current token wn . The position embedding table is initialized randomly. 3. The real-valued embedding vector for the entity type of wi : This vector is generated by looking up the entity type embedding table (initialized randomly) for the entity type of wi . Note that we employ the BIO annotation schema to assign entity type labels to each token in the sentences using the entity mention heads as in (Nguyen"
D16-1085,D14-1090,0,0.084659,"sign effective feature sets for statistical models for ED, ranging from the local sentence-level representations (Ahn, 2006; Li et al., 2013), to the higher level structures such as the cross-sentence or cross-event information (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Li et al., 2015). Some recent work on the feature-based approach has also investigated event trigger detection in the joint inference with event argument prediction (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013; Venugopal et al., 2014) to benefit from their inter-dependencies. Finally, neural networks have been introduced into ED very recently with the early work on convolutional neural networks (Nguyen and Grishman, 2015b; Chen et al., 2015). The other work includes: (Nguyen et al., 2016a) who employ bidirectional recurrent neural networks to perform event trigger and argument labeling jointly, (Jagannatha and Yu, 2016) who extract event instances from health records with recurrent neural networks and (Nguyen et al., 2016b) who propose a two-stage training algorithm for event extension with neural networks. 4 We present a"
E12-1020,C08-1088,0,0.0203777,"D. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney, 2005b) or convolution tree kernel (Collins and Duffy, 2001), to calculate their similarity. These objects are usually augmented with features such as semantic features. In this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only. The relation classification stage remains unchanged and we will show that it benefits from improved detection."
E12-1020,P11-1053,1,0.802456,"uments. More details about the ACE evaluation are on the ACE official website. Given a sentence s and two entity mentions arg1 and arg2 contained in s, a candidate relation mention r with argument arg1 preceding arg2 is defined as r=(s, arg1, arg2). The goal of Relation Detection and Classification (RDC) is to determine whether r expresses one of the types defined. If so, classify it into one of the types. Supervised learning treats RDC as a classification problem and solves it with supervised Machine Learning algorithms such as MaxEnt and SVM. There are two commonly used learning strategies (Sun et al., 2011). Given an annotated corpus, one could apply a flat learning strategy, which trains a single multiclass classifier on training examples labeled as one of the relation types or not-a-relation, and apply it to determine its type or output not-a relation for each candidate relation mention during testing. The examples of each type are the relation mentions that are tagged as instances of that type, and the not-a-relation examples are constructed from pairs of entities that appear in the same sentence but are not tagged as any of the types. Alternatively, one could apply a hierarchical learning st"
E12-1020,H05-1091,0,0.156669,"Missing"
E12-1020,P09-2092,0,0.0317863,"Missing"
E12-1020,N06-1037,0,0.0189929,"e the most likely type only if it is detected as correct by RD. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney, 2005b) or convolution tree kernel (Collins and Duffy, 2001), to calculate their similarity. These objects are usually augmented with features such as semantic features. In this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only. The relation classification stage remains unchange"
E12-1020,P11-1056,0,0.0284709,"Missing"
E12-1020,H89-2010,0,0.101162,"Missing"
E12-1020,W10-1808,0,0.029769,"Missing"
E12-1020,I05-1036,0,0.0503282,"Missing"
E12-1020,P05-1052,1,0.798015,"RC is applied to determine the most likely type only if it is detected as correct by RD. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney, 2005b) or convolution tree kernel (Collins and Duffy, 2001), to calculate their similarity. These objects are usually augmented with features such as semantic features. In this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only. The relation classification st"
E12-1020,P05-1053,0,0.482553,"ositive instances and using all the not-a-relation cases (same as described above) as negative examples. RC is trained on the annotated examples with their tagged types. During testing, RD is applied first to identify whether an example expresses some relation, then RC is applied to determine the most likely type only if it is detected as correct by RD. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney, 2005b) or convolution tree kernel (Collins and Duffy, 2001), to"
E12-1020,D07-1076,0,0.0176106,"ted as correct by RD. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney, 2005b) or convolution tree kernel (Collins and Duffy, 2001), to calculate their similarity. These objects are usually augmented with features such as semantic features. In this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only. The relation classification stage remains unchanged and we will show that it benefits from"
E12-1020,N07-1015,0,0.0237883,"nd using all the not-a-relation cases (same as described above) as negative examples. RC is trained on the annotated examples with their tagged types. During testing, RD is applied first to identify whether an example expresses some relation, then RC is applied to determine the most likely type only if it is detected as correct by RD. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney, 2005b) or convolution tree kernel (Collins and Duffy, 2001), to calculate their simil"
E12-1020,P04-3022,0,0.0146242,"trained by grouping tagged relation mentions of all types as positive instances and using all the not-a-relation cases (same as described above) as negative examples. RC is trained on the annotated examples with their tagged types. During testing, RD is applied first to identify whether an example expresses some relation, then RC is applied to determine the most likely type only if it is detected as correct by RD. State-of-the-art supervised methods for relation extraction also differ from each other on data representation. Given a relation mention, feature-based methods (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007; Sun et al., 2011) extract a rich list of structural, lexical, syntactic and semantic features to represent it; in contrast, the kernel based methods (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Qian et al., 2008) represent each instance with an object such as augmented token sequences or a parse tree, and used a carefully designed kernel function, e.g. subsequence kernel (Bunescu and Mooney,"
E12-1020,A00-2030,0,\N,Missing
E12-1020,P06-1104,0,\N,Missing
grishman-2010-impact,C00-2136,1,\N,Missing
grishman-2010-impact,D09-1016,0,\N,Missing
grishman-2010-impact,P03-1029,1,\N,Missing
grishman-2010-impact,P03-1044,0,\N,Missing
grishman-2010-impact,huttunen-etal-2002-diversity,1,\N,Missing
grishman-2010-impact,M98-1003,0,\N,Missing
H01-1009,W98-1120,1,0.899463,"logical analyzer and NE-tagger. Then the system retrieves the relevant documents for the scenario as a relevant document set. The system, further, selects a set of relevant sentences as a relevant sentence set from those in the relevant document set. Finally, all the sentences in the relevant sentence set are parsed and the paths in the dependency tree are taken as patterns. 3.1 Document Preprocessing Morphological analysis and Named Entity (NE) tagging is performed on the training data at this stage. We used JUMAN [2] for the former and a NE-system which is based on a decision tree algorithm [5] for the latter. Also the part-of-speech information given by JUMAN is used in the later stages. 3.2 Document Retrieval The system first retrieves the documents that describe the events of the scenario of interest, called the relevant document set. A set of narrative sentences describing the scenario is selected to create a query for the retrieval. For this experiment, we set the size of the relevant document set to 300 and retrieved the documents using CRL’s stochastic-model-based IR system [3], which performed well in the IR task in IREX, Information Retrieval and Extraction evaluation proje"
H01-1009,A00-1039,1,0.509615,"ring and the common paths in the parse tree of relevant sentences are taken as extracted patterns. Keywords Information Extraction, Pattern Acquisition 1. INTRODUCTION Information Extraction (IE) systems today are commonly based on pattern matching. New patterns need to be written when we customize an IE system for a new scenario (extraction task); this is costly if done by hand. This has led to recent research on automated acquisition of patterns from text with minimal pre-annotation. Riloff [4] reported a successful result for her procedure that needs only a pre-classified corpus. Yangarber [6] developed a procedure for unannotated natural language texts. One of their common assumption is that the relevant documents include good patterns. Riloff implemented this idea by applying the pre-defined heuristic rules to pre-classified (relevant) documents and Yangarber advanced further so that the system can classify the documents by itself given seed patterns specific to a scenario and then find the best patterns from the relevant document set. Considering how they represent the patterns, we can see that, in general, Riloff and Yangarber relied on the sentence structure of English. Riloff"
H05-1003,M95-1005,0,0.421288,"2003 training corpora. We trained the relation tagger on 328 ACE 2004 texts. We used 126 newswire texts from the ACE 2004 data to train the English second-stage model, and 65 newswire texts from the ACE 2004 evaluation set as a test set for the English system. Chinese For Chinese, the baseline reference resolver was trained on 767 texts from ACE 2003 and ACE 2004 training data. Both the baseline relation tagger and the rescoring model were trained on 646 texts from ACE 2004 training data. We used 100 ACE texts for a final blind test. 6.2 Experiments We used the MUC coreference scoring metric (Vilain et al 1995) to evaluate3 our systems. To establish an upper limit for the possible improvement offered by our models, we first did experiments using perfect (hand-tagged) mentions and perfect relations as inputs. The algorithms for 3 In our scoring, we use the ACE keys and only score mentions which appear in both the key and system response. This therefore includes only mentions identified as being in the ACE semantic categories by both the key and the system response. Thus these scores cannot be directly compared against coreference scores involving all noun phrases. (Ng 2005) applies another variation"
H05-1003,A97-1029,0,0.0346084,"tric between two examples based on: 1 whether the heads of the mentions match 1 whether the ACE types of the heads of the mentions match (for example, both are people or both are organizations) 1 whether the intervening words match To tag a test example, we find the k nearest training examples, use the distance to weight each neighbor, and then select the most heavily weighted class in the weighted neighbor set. Name tagger and noun phrase chunker Our baseline name tagger consists of a HMM tagger augmented with a set of post-processing rules. The HMM tagger generally follows the Nymble model (Bikel et al. 1997), but with a larger number of states (12 for Chinese, 30 for English) to handle name prefixes and suffixes, and, for Chinese, transliterated foreign names separately. For Chinese it operates on the output of a word segmenter from Tsinghua University. Our nominal mention tagger (noun phrase chunker) is a maximum entropy tagger trained on treebanks from the University of Pennsylvania. 5.2 Rescoring stage To incorporate information from the relation tagger into the final coreference decision, we split the maxent classification into two stages. The first stage simply applies the baseline maxent mo"
H05-1003,C88-1021,0,0.101855,"upenn.edu/Projects/ACE/ 17 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 17–24, Vancouver, October 2005. 2005 Association for Computational Linguistics Section 6 presents the results of experiments on both English and Chinese test data. Section 7 presents our conclusions and directions for future work. 2 Prior Work Much of the earlier work in anaphora resolution (from the 1970’s and 1980’s, in particular) relied heavily on deep semantic analysis and inference procedures (Charniak 1972; Wilensky 1983; Carbonell and Brown 1988; Hobbs et al. 1993). Using these methods, researchers were able to give accounts of some difficult examples, often by encoding quite elaborate world knowledge. Capturing sufficient knowledge to provide adequate coverage of even a limited but realistic domain was very difficult. Applying these reference resolution methods to a broad domain would require a large scale knowledge-engineering effort. The focus for the last decade has been primarily on broad coverage systems using relatively shallow knowledge, and in particular on corpus-trained statistical models. Some of these systems attempt to"
H05-1003,W98-1119,0,0.0920301,"hods, researchers were able to give accounts of some difficult examples, often by encoding quite elaborate world knowledge. Capturing sufficient knowledge to provide adequate coverage of even a limited but realistic domain was very difficult. Applying these reference resolution methods to a broad domain would require a large scale knowledge-engineering effort. The focus for the last decade has been primarily on broad coverage systems using relatively shallow knowledge, and in particular on corpus-trained statistical models. Some of these systems attempt to apply shallow semantic information. (Ge et al. 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. (Tetreault and Allen 2004) use a semantic parser to add semantic constraints to the syntactic and agreement constraints in their Left-Right Centering algorithm. (Soon et al. 2001) use WordNet to test the semantic compatibility of individual noun phrase pairs. In general these approaches do not explore the possibility of exploiting the global semantic context provided by the document as a whole. Recently Bean and Rilof"
H05-1003,mitkov-2000-towards,0,0.0285571,"ng, text summarization and a number of other natural language processing tasks. Most reference resolution systems use representations built out of the lexical and syntactic attributes of the noun phrases (or “mentions”) for which reference is to be established. These attributes may involve string matching, agreement, syntactic distance, and positional information, and they tend to rely primarily on the immediate context of the noun phrases (with the possible exception of sentence-spanning distance measures such as Hobbs distance). Though gains have been made with such methods (Tetreault 2001; Mitkov 2000; Soon et al. 2001; Ng and Cardie 2002), there are clearly cases where this sort of local information will not be sufficient to resolve coreference correctly. Coreference is by definition a semantic relationship: two noun phrases corefer if they both refer to the same real-world entity. We should therefore expect a successful coreference system to exploit world knowledge, inference, and other forms of semantic information in order to resolve hard cases. If, for example, two nouns refer to people who work for two different organizations, we want our system to infer that these noun phrases canno"
H05-1003,N04-1038,0,\N,Missing
H05-1003,J01-4003,0,\N,Missing
H05-1003,P02-1014,0,\N,Missing
H05-1003,J01-4004,0,\N,Missing
H86-1002,P86-1004,1,0.856118,"Missing"
H86-1009,C88-2110,0,0.0325646,"Missing"
H86-1009,P82-1014,0,0.0614895,"Missing"
H86-1009,A83-1016,0,0.0584776,"Missing"
H86-1009,A83-1009,0,0.0419811,"Missing"
H86-1009,P86-1004,0,0.0464053,"Missing"
H86-1009,J81-4005,0,0.0744968,"Missing"
H86-1009,H86-1011,0,\N,Missing
H86-1010,H86-1009,1,0.82515,"Missing"
H89-1034,J83-3005,0,0.0321736,"rocess such messages by computer. Even though people don&apos;t send many telegrams anymore, this problem is still of importance because many military messages are written in this telegraphic style: 2 FLARES SIGHTED 230704Z6 SOUTH A P P R O X 5 MI SPA ESTABLISHED (here 230704Z6 is the time, and SPA is the Submarine Probability Area). Alternative Strategies The particular class of messages which we have studied are a set of Navy tactical messages called RAINFORM (ship) sighting messages [8]. Several other researchers have previously constructed systems to analyze these messages. In the NOMAD system [1] the knowledge was principally realized as procedures associated with individual words. This made it difficult to extend the system, as Granger has noted [1]. Some of the shortcomings of the internal knowledge representation were remedied in a later system named VOX [5] which used a conceptual grammar, mixing syntactic and semantic constraints. However, the power of the grammar was still quite limited when compared to grammars traditionally used in computational linguistics applications. In the development of our system, in contrast, we have taken as our starting point a relatively broad cover"
H89-1034,P88-1002,0,0.0500472,"pproach: • Using general-purpose components minimizes the labor in porting the system to a new domain. • Using a standard English grammar makes it easier to analyze the complex constructions (involving subordinating and coordinating conjunctions, for example) which occur with some frequency in these messages. • Starting from a standard grammar clarifies the ways in which these messages differ from standard English. This approach is in keeping with earlier work at NYU, on medical records and equipment failure reports [4,3], and more recent work at UNISYS, primarily on equipment failure reports [6,2]. In the next section, we briefly describe the overall structure of the message understanding system. In the two sections which follow, we focus on the two core problems of analyzing such telegraphic text: first, the problem of analyzing the structure of the text (&quot;parsing&quot;); second, the problem of recovering the arguments which are omitted in the telegraphic text. 204 System structure The text processing system is organized as a pipeline consisting of the following modules: 1. A parser using an augmented context-free grammar consisting of context-free rules plus procedural restrictions. The g"
H89-1034,A83-1016,0,0.06075,"eneral-purpose English language analyzer. We see several benefits to such an approach: • Using general-purpose components minimizes the labor in porting the system to a new domain. • Using a standard English grammar makes it easier to analyze the complex constructions (involving subordinating and coordinating conjunctions, for example) which occur with some frequency in these messages. • Starting from a standard grammar clarifies the ways in which these messages differ from standard English. This approach is in keeping with earlier work at NYU, on medical records and equipment failure reports [4,3], and more recent work at UNISYS, primarily on equipment failure reports [6,2]. In the next section, we briefly describe the overall structure of the message understanding system. In the two sections which follow, we focus on the two core problems of analyzing such telegraphic text: first, the problem of analyzing the structure of the text (&quot;parsing&quot;); second, the problem of recovering the arguments which are omitted in the telegraphic text. 204 System structure The text processing system is organized as a pipeline consisting of the following modules: 1. A parser using an augmented context-fre"
H89-1034,P86-1004,0,0.283573,"pproach: • Using general-purpose components minimizes the labor in porting the system to a new domain. • Using a standard English grammar makes it easier to analyze the complex constructions (involving subordinating and coordinating conjunctions, for example) which occur with some frequency in these messages. • Starting from a standard grammar clarifies the ways in which these messages differ from standard English. This approach is in keeping with earlier work at NYU, on medical records and equipment failure reports [4,3], and more recent work at UNISYS, primarily on equipment failure reports [6,2]. In the next section, we briefly describe the overall structure of the message understanding system. In the two sections which follow, we focus on the two core problems of analyzing such telegraphic text: first, the problem of analyzing the structure of the text (&quot;parsing&quot;); second, the problem of recovering the arguments which are omitted in the telegraphic text. 204 System structure The text processing system is organized as a pipeline consisting of the following modules: 1. A parser using an augmented context-free grammar consisting of context-free rules plus procedural restrictions. The g"
H89-1034,H86-1011,0,\N,Missing
H89-1034,C82-1032,0,\N,Missing
H89-2011,J86-3002,1,0.86622,"Missing"
H89-2011,H89-1034,1,0.843241,"Missing"
H89-2011,A88-1007,0,0.0231474,"Missing"
H89-2011,P86-1004,0,0.115327,"Missing"
H89-2011,H89-1032,0,0.0803998,"Missing"
H89-2011,H86-1011,0,\N,Missing
H90-1053,H90-1053,1,0.0512417,"of an edge is the sum of the priority of the production used by the edge and priorities of its constituents, the overall priority of an edge varies according to the context. This effect can be best captured by breaking up the priority of an edge into two components - the absolute component and the relative component. While the absolute component for an edge is constant, the relative component depends on the context in which the priority of this edge is being computed, since priorities of the same production are different in different contexts. These changes are discussed in greater detail in [2] Heuristic Penalties Even with the improvements detailed in the two prior sections, the parser suffered from a serious problem. The modified search algorithm, which is best first, ignored the length of hypotheses being compared. As a result, as the parsing progressed towards the right of the sentence, it would often thrash after the lengthier (and possibly correc0 partial parses gathered more penalty than short, unpromising hypotheses near the left of the sentence. In order to have a strictly best-first search strategy, this phenomenon is unavoidable 4. However, we studied the trade-off that e"
H90-1053,P84-1005,0,0.242098,"Missing"
H90-1053,H89-2011,1,0.872338,"Missing"
H90-1053,C67-1009,0,0.277825,"lar constructs corresponds to that observed in a sample corpus. The resulting g r a m m a r gives us not only the probabilities of sentences, but also the probabilities of alternate parses for a single sentence. It is the latter probabilities which are of primary interest to us in analyzing new sentences. Initially, probabilities of all individual productions were determined using a corpus of text. This body of text was parsed using the NYU P R O T E U S g r a m m a r which is a subset of Sager's grammar[10] based on Linguistic String Theory [7]. The parsing was performed using a chart parser [8]. The probabilities of the productions were computed by an iterative procedure described below. The parser was then modified to use these probabilities. The probability of a parse tree is computed as the product of probabilities of all the productions used in that particular parse tree. The parsing algorithm generates parses in the best-first order [9] (i.e. most probable parse first). The prioritizing of productions not only helps in the disambiguation of parses but also forces the parser to try more likely productions first. As a result, there is a twofold improvement attributable to this mo"
H93-1050,P92-1023,0,0.0395122,"ANTIC PATTERNS would produce the regularized syntactic structure (s like (subject (np Mary)) (object (np linguist (a-pos young) (from (np Limerick))))) from which the following four triples are generated: like like linguist linguist subject object a-pos from Mary linguist young Limerick The procedure described above produces a set of frequencies and probability estimates based on specific words. The &quot;traditional&quot; approach to generalizing this information has been to assign the words to a set of semantic classes, and then to collect the frequency information on combinations of semantic classes [7,3]. Since at least some of these classes will be domain specific, there has been interest in automating the acquisition of these classes as well. This can be done by clustering together words which appear in the same context. Starting from the file of triples, this involves: Given the frequency information F, we can then estimate the probability that a particular head wi appears with a particular argument or modifier &lt; v wj >:2 1. collecting for each word the frequency with which it occurs in each possible context; for example, for a noun we would collect the frequency with which it occurs as th"
H93-1050,C92-2085,0,0.0647803,"d then be used in scoring alternative parse trees. For the evaluation below, however, we will use the frequency data F directly. 1But with somewhat more regulafization than is done in LFG; in particular, passive structures are converted to corresponding active forms. 2Note that F(wlappears as a head in a parse tree) is different from F(wi appears as a head in a triple) since a single head in a parse tree may produce several such triples, one for each argument or modifier of that head. 3. forming clusters based on this similarity measure Such a procedure was performed by Sekine et al. at UMIST [6]; these clusters were then manually reviewed and the resulting clusters were used to generalize selectional patterns. 255 twice. Furthermore, if the value computed by the formula for Pc is less than some threshold ~&apos;c, the value is taken to be zero; we have used rc = 0.001 in the experiments reported below. (These filters are not applied for the case i = j; the diagonal elements of the confusion matrix are always computed exactly.) Because these filters may yeild an un-normalized confusion matrix (i.e., ~ o j Pc(wj Iwi) &lt; 1), we renormalize the matrix so that ~ w Pc(wj Iwi) = 1. A similar appr"
H93-1050,J91-2002,0,0.0284011,"ANTIC PATTERNS would produce the regularized syntactic structure (s like (subject (np Mary)) (object (np linguist (a-pos young) (from (np Limerick))))) from which the following four triples are generated: like like linguist linguist subject object a-pos from Mary linguist young Limerick The procedure described above produces a set of frequencies and probability estimates based on specific words. The &quot;traditional&quot; approach to generalizing this information has been to assign the words to a set of semantic classes, and then to collect the frequency information on combinations of semantic classes [7,3]. Since at least some of these classes will be domain specific, there has been interest in automating the acquisition of these classes as well. This can be done by clustering together words which appear in the same context. Starting from the file of triples, this involves: Given the frequency information F, we can then estimate the probability that a particular head wi appears with a particular argument or modifier &lt; v wj >:2 1. collecting for each word the frequency with which it occurs in each possible context; for example, for a noun we would collect the frequency with which it occurs as th"
H93-1060,J81-4005,0,0.057229,"ect verbs - - verbs for which the surface subject is the functional subject only of the embedded clause. The functional subject position in the matrix clause is unfilled, as indicated by the notation :gs (:subject 0 :comp 2). We have compared our subcategofization codes to those used by a number of other major lexicon projects in order to insure that our codes are reasonably complete and that it would not be too difficult to map our codes into those of other systems. Among the projects we have studied are the Brandeis Verb Lexicon 2, the ACQUILEX Project [3], the NYU Linguistic String Project [2], and the Oxford Advanced Learner's Dictionary [1]. 1The general format used for constituent structures was suggested by Bob Ingria for the DARPA Common Lexicon. 2Developed by J. Gfimshaw and R. Jackendoff. 301 (vp-frame s :cs ((s 2 :that-comp optional)) :gs (:subject 1 :comp 2) :ex ""they thought (that) he was always late"") (vp-frame to-inf-sc :cs ((vp 2 :mood to-infinitive :subject 1)) :features (:control subject) :gs (:subject 1 :comp 2) :ex ""1 wanted to come."") (vp-frame to-inf-rs :cs ((vp 2 :mood to-infinitive :subject 1)) :features (:raising subject) :gs (:subject 0 :comp 2) :ex ""1 seemed"
H94-1003,J93-2001,0,0.0607101,"Missing"
H94-1003,P91-1030,0,0.069877,"Missing"
H94-1003,H93-1061,0,0.0394644,"Missing"
H94-1003,J93-2002,0,\N,Missing
H94-1003,P93-1032,0,\N,Missing
H94-1021,H91-1060,0,0.0199816,"&lt;/wd> Japan &lt;/lot> • &lt;/wd> &lt;/s> Figure 2: Named entity / word sense / coreference annotation of first sentence. value of a number, a 6.-digit number for dates, a standardized form for company names (following MUC-5 rules for company names). variable i, and treat the type and each argument of the event as a separate predication. So, for example, Fred fed Francis on Friday would be represented as 2 3.2. Parseval Parseval is a measure of the ability of a system to bracket the syntactic constituents in a sentence. This metric has now been in use for several years, and has been described elsewhere [1]. Parseval may eventually be supplanted in large part by the &quot;deeper&quot; and more detailed predicate-argument evaluation. However, for the present Parseval is being retained in order to accomodate participants focussed on surface grammar and participants reluctant to commit to predicate-argument evaluation until its design is stabilized and proven. (ev-type 1 eat) (1-subj 1 Fred) (1-obj 1 Francis) (on 1 Friday) Each elementary predication can be numbered by preceding it with a number and colon. Roughly speaking, a system would be scored on the number of such elementary predications it gets correc"
H94-1021,H93-1061,0,0.0472527,"[l-subj 6] [l-obj 1990])>)]) (with [l-subj I] [1-obj 7:(NO-DET &lt;(produce [event 7] [l-subj 2]) [l-obj 8:(NO-DET &lt;(club [l-subj 8]) (PLURAL [l-subj 8]) (and &lt;(iron [l-subj 8]) (metal-wood [l-subj 8])>) (PER [l-subj (CARDINALITY [l-subj 8] [l-obj 20000] [l-obj month])>)])>)])>) Figure 4: Predicate-argumentstructure. 124 3.5. Word sense identification The third element of the Semeval triad is sense identification. As a sense inventory, we hayed used WordNet, which is widely and freely available and is broad in coverage [4]. The notation used to refer to particular WordNet sense was described in [5]. 3.6. Mini-MUC This component is the direct descendant of the information extraction tasks in the previous MUCs [2,3]• In response to criticism that the evaluation task had gotten too complex, we have endeavored to make the new information extraction as simple as possible• The template will have a hierarchical structure, as in MUC-5, but probably with only two levels of &quot;objects&quot;. The objects at the lower level will represent common business news entities such as people and companies. A small inventory of such objects will be defined in advance. The upper level object will then be a simple st"
H94-1109,H94-1049,0,\N,Missing
H94-1109,P81-1001,0,\N,Missing
H94-1109,J94-2001,0,\N,Missing
H94-1109,P83-1020,0,\N,Missing
huttunen-etal-2002-diversity,M95-1014,1,\N,Missing
huttunen-etal-2002-diversity,O98-3001,0,\N,Missing
huttunen-etal-2002-diversity,X98-1016,1,\N,Missing
huttunen-etal-2002-diversity,O97-1012,0,\N,Missing
I11-1080,N06-1016,0,0.0176713,"ng tasks, such as named entity recognition (Shen et al. 2004; Hachey, Alex and Becker 2005; Kim et al. 2006), text categorization (Schohn and Cohn 2000; Tong and Koller 2002; Hoi, Jin & Lyu 2006), part of speech tagging (Ringger et al. 4 The existing system had both pattern matching and statistical components; we integrated these components so that the resulting system would have a uniform probabilistic model suitable for the active learning strategies we employed. 2007), parsing (Osborne and Baldridge 2004; Becker and Osborne 2005; Reichart and Rappoport 2007), and word sense disambiguation (Chen et al. 2006; Zhu and Hovy 2007). However, there have not yet been any studies to use active learning in event extraction. There are several sampling methods in active learning; the most commonly used ones include uncertainty-based sampling, committee-based sampling, and co-testing. Co-testing (Muslea et al. 2000) involves two (or more) redundant views; it simultaneously trains a separate classifier for each view, and the system selects a query based on the degree of disagreement among the learners. Because well-informed classifiers for the two views should agree, cotesting will select an example which is"
I11-1080,W05-0619,0,0.062198,"Missing"
I11-1080,P04-1075,0,0.341563,"ch word (each potential trigger), the argument / role classifier is applied to collect the possible arguments/roles connected to this word, and then the trigger classifier is used to decide whether it is a trigger or not. If it is, an event mention including the trigger and all its roles will be reported, else the word will not be tagged as a trigger, and all the arguments/roles collected by previous classifiers are discarded. 3 Active Learning for Event Extraction Active learning has been successfully applied to a number of natural language processing tasks, such as named entity recognition (Shen et al. 2004; Hachey, Alex and Becker 2005; Kim et al. 2006), text categorization (Schohn and Cohn 2000; Tong and Koller 2002; Hoi, Jin & Lyu 2006), part of speech tagging (Ringger et al. 4 The existing system had both pattern matching and statistical components; we integrated these components so that the resulting system would have a uniform probabilistic model suitable for the active learning strategies we employed. 2007), parsing (Osborne and Baldridge 2004; Becker and Osborne 2005; Reichart and Rappoport 2007), and word sense disambiguation (Chen et al. 2006; Zhu and Hovy 2007). However, there have no"
I11-1080,P02-1016,0,0.220104,"ollows: |S1 ||S2 | =================== Given: SenSet = (S1,...,SN) and the BatchSet with the maximal size K. Initialization: BatchSet = empty Loop until BatchSet is full Select Si based on some measure from SenSet; RepeatFlag = false; Loop from j = 1 to CurrentSize of BatchSet If Score(Si, Sj) > threshold Then RepeatFlag = true; break; If RepeatFlag == false Then Add Si into BatchSet. ===================== where sim( f i , f j ) is 1 when f i and f j are the same, otherwise 0. € 4.2 Representativeness € considered € A few prior studies have € this selection criterion (McCallum and Nigam 1998; Tang et al. 2002; Shen et al. 2004). The representativeness of a sample can be evaluated based on how many samples are similar to this sample. Adding samples which are more representative to the training set will have an effect on a larger number of unlabeled samples. For every sentence in the sampling pool, we measure its representativeness based on its average similarity to other sentences in the sampling pool: ∑ sim(S ,S ) i Represent(Si ) = j S j ∈P,i≠ j |P |−1 where P is the current sampling pool. In this way, we will filter out the samples that are rare in the €whole sampling pool, and focus our effort"
I11-1080,N04-1012,0,0.0201312,"arning for Event Extraction Active learning has been successfully applied to a number of natural language processing tasks, such as named entity recognition (Shen et al. 2004; Hachey, Alex and Becker 2005; Kim et al. 2006), text categorization (Schohn and Cohn 2000; Tong and Koller 2002; Hoi, Jin & Lyu 2006), part of speech tagging (Ringger et al. 4 The existing system had both pattern matching and statistical components; we integrated these components so that the resulting system would have a uniform probabilistic model suitable for the active learning strategies we employed. 2007), parsing (Osborne and Baldridge 2004; Becker and Osborne 2005; Reichart and Rappoport 2007), and word sense disambiguation (Chen et al. 2006; Zhu and Hovy 2007). However, there have not yet been any studies to use active learning in event extraction. There are several sampling methods in active learning; the most commonly used ones include uncertainty-based sampling, committee-based sampling, and co-testing. Co-testing (Muslea et al. 2000) involves two (or more) redundant views; it simultaneously trains a separate classifier for each view, and the system selects a query based on the degree of disagreement among the learners. Bec"
I11-1080,D09-1016,0,0.116385,"Missing"
I11-1080,D07-1082,0,\N,Missing
I11-1080,W07-1516,0,\N,Missing
I11-1080,P07-1052,0,\N,Missing
I11-1080,N06-2018,0,\N,Missing
I11-1117,W08-1805,0,0.0302105,"ities as slot types. In past KBP competitions, many participants (Li et al., 2009; Byrne and Dunnion, 2010; Chen et al., 2010) exploited a QA system to fill slots by constructing queries based on target entities and slot types. However, their query templates contain only a few additional query terms other than the target entity name, which are mostly obtained manually. Most of QA systems use the question words as-is or with expansion to form the retrieval system query. Various query expansion approaches have been used to tackle the passage-query mismatch problem, including relevance feedback (Derczynski et al., 2008), ontologies (Bhogal et al., 2007), semantic lexica (Ofoghi et al., 2006), etc. As a data-driven approach, relevance feedback is sensitive to the quality of first time retrieval. Our use of Freebase, a freely available large semantic database, to provide distant supervision requires neither labeled data nor costly constructed knowledge models. Some researchers (Grishman and Min, 2010; Chrupala et al., 2010; Surdeanu et al., 2010) integrated IR and IE together. Surdeanu et al. (2010) coupled the entity name with a handful of hand-selected trigger words for each slot type as queries to IR system"
I11-1117,P05-1045,0,0.00390891,"ather arbitrarily. 1) The target named entity ( ): We ensure that passages that (partly) include the string of entity names rank higher than those only containing pronouns. 1049 ( ) { ( ) where are arbitrary constants larger than any possible value of . The passage partly contains the entity when it shares at least a word in common with the entity name. 2) The named entity of the expected type ( ): The named entity of the type that is to be found for the relation is called the expected named entity, (e.g. „ORGANIZATION‟ for relation „employee_of‟). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) on Lemur‟s passage retrieval outputs, preferring passages that contain a named entity of the sought type, and more strongly preferring names that have not appeared in previously retrieved passages (novel names). ( ) ( ) { where and are arbitrary constants larger than any possible value of . 3) The expansion terms ( ): The expansion terms include predefined words that are predictive of or related to a specific relation. We adapt the indicative word list used by Surdeanu et al. (2010) in their KBP system, which include several words for each relation. We also use a list containing 635 common ti"
I11-1117,P08-1030,1,0.815256,"semantic and ontological knowledge. To generate the final results, there could be different strategies. We use a simple strategy in this paper, which suffices to show the capability of our system, outputting the answers of the top-ranked passages that provide an answer (possibly duplicate). A more delicate design is not a focus of this research. Traditional IE Pipeline We exploit a simple two-stage pipeline architecture for the KBP task. First, we retrieve passages related to the target entity. Then we apply to those passages a traditional information extraction system (Grishman et al., 2005; Ji and Grishman, 2008) to extract relations, which was originally created for the NIST Automatic Content Extraction (ACE) Evaluations. Its relation QA-like Pipeline The new passage retrieval IKFB system also allows us to create a QA-like pipeline for largescale information extraction. Besides applying a sophisticated IE system to the retrieved passages using deep NLP techniques, such as coreference resolution, we can exploit answer extraction/selection components similar to many QA systems. Some common answer extraction/selection approaches, e.g. using distance from keywords, can possibly boost the speed by avoidin"
I11-1117,P09-1113,0,0.698663,". However, unlike QA, we have a fixed inventory of relations and a fixed set of expected answer Le Zhao* *Carnegie Mellon University Pittsburgh, PA, USA lezhao@cs.cmu.edu types (e.g. employer of a person). This allows us to bring to bear the more specialized learning methods of IE to tune the passage retrieval for each relation of interest. To the best of our knowledge, we are the first to systematically study the passage retrieval algorithm for information extraction and propose a novel distant supervision approach to obtain a list of weighted keywords for each relation. Distant supervision (Mintz et al., 2009) makes use of noisy training data generated automatically from a related, but different, type dataset to solve problems on another type of data. Instead of a handful of human-selected keywords, we automatically learn hundreds or thousands of indicative keywords from a freely available online resource, Freebase, which is similar to Wikipedia Infoboxes. Passages are ranked and retrieved based on these keywords indicative of certain relations. We then feed individual passages to a traditional IE system or to an answer extraction component as used in QA systems to obtain the final outputs. Both th"
I11-1117,N07-2032,0,0.18625,"which is to return a ranked list of related entities given an expected type of entity and a brief description (query) of the relation in free text. Fang et al. (2010) ranked entities by their relevance to the query at the document, passage and entity level, primarily based on the similarity between terms. In all this previous work, the limited number of query terms has become the performance bottleneck of the passage retrieval for large-corpus information extraction. Perhaps most similar to our distant supervision keyword learning approach for passage retrieval is the semi-automatic method of Nguyen et al. (2007), who extract only several keywords for each relation from Wikipedia and study only the dependency subtrees that contain those keywords. In contrast to their tf-idf model followed by a manual selection step, our algorithm allows us to fully automatically extract hundreds or even thousands of keywords with a weight indicating their relevance to each relation. Mintz et al. (2009) proposed a distant supervision approach for relation extraction using a richfeatured logistic regression model. Like us, they used Freebase as a source of known relation instances and Wikipedia as a text source to creat"
I13-1081,W02-1001,0,0.0689264,"sk. (Zhang et al., 2012) proposed a unified framework for biomedical relation extraction. They used an SVM as the local classifier and tried both uncertainty-based and density-based query functions and showed comparable results for the two methods. They also proposed using cosine-distance to ensure the diversity of queries. (Roth and Small 2008) used a dual strategy active learner (Donmez, Carbonell, & Bennett 2007) in their pipeline models of segmentation, entity classification and relation classification at the same time. They also adopted a regularized version of the structured perceptron (Collins 2002) instead of SVM and reported better results in active learning. Their work simulated the whole pipeline in active learning to achieve relation extraction, but had no specific research on the stage of relation extraction in the pipeline. (Zhang 2010) proposed multi-task active learning with output constraints as a generalization of multi-view learning. The multi-task method relied on constraints on output between different tasks; this might be extended to situations where we need to learn relation sub-types as well as types, but was not applicable when relation extraction is an individual task."
I13-1081,N07-1015,0,0.0920893,"tart selective sampling and pose queries to improve the model. We use a co-testing method similar to LGCo-Testing (Sun and Grishman 2012), the state-of-the-art active learning algorithm for relation type extension, but give preference to the weaker classifier to get some additional benefit in the early iterations. LGCo-Testing uses co-testing based on the local view and the global view to select queries. The local classifier uses a rich set of lexical and syntactic features (from both constituent and dependency parses) as well as semantic type information for the arguments. (Zhou et al. 2005; Jiang and Zhai 2007) studied the effectiveness of different features. The global classifier relies on the similarity of relation phrases (the words between the entity mentions), computed based on the shared contexts of these phrases across a large news corpus. The global classifier returns the relation type of the labeled instances to which the unlabeled instance is most similar (a knearest-neighbor strategy, with k=3). 2 The instances on which the two classifiers disagree is the contention set, from which queries are selected. Elements of the contention set are ranked by the KL-divergence, and elements with grea"
I13-1081,N10-1087,0,0.0176087,"eled. Section 5 concludes the paper. 2 Related Work For reducing the cost of annotation in the task of relation extraction, most prior work used semi692 International Joint Conference on Natural Language Processing, pages 692–698, Nagoya, Japan, 14-18 October 2013. supervised learning. (Uszkoreit 2011) introduced a bootstrapping system for relation extraction rules, which achieved good performance under some circumstances. However, most previous semi-supervised methods have large performance gaps from supervised systems, and their performance depends on the choice of seeds (Vyas et al., 2009; Kozareva and Hovy, 2010). Recent studies have shown the effectiveness of active learning for this task. (Zhang et al., 2012) proposed a unified framework for biomedical relation extraction. They used an SVM as the local classifier and tried both uncertainty-based and density-based query functions and showed comparable results for the two methods. They also proposed using cosine-distance to ensure the diversity of queries. (Roth and Small 2008) used a dual strategy active learner (Donmez, Carbonell, & Bennett 2007) in their pipeline models of segmentation, entity classification and relation classification at the same"
I13-1081,P11-1053,1,0.902792,"Missing"
I13-1081,P05-1053,0,\N,Missing
I17-2072,C14-1220,0,0.0140055,"oken’s chunk type. The size of embedding table is (|C |+ 1) ∗ dc , where |C |is the number of chunk types, 3 Model We formulate the relation extraction task as a classification problem over all entity pairs (relation candidates) in a sentence. The overall structure of the model is shown in Figure 1. The model will first convert a relation candidate into a fixed-length matrix, then uses a singlelayer Convolutional Neural Network (CNN) with dropout to learn its hidden representation repr. On top of repr, it uses two decoders: a fullyconnected layer with dropout for predicting the relation type (Zeng et al., 2014) (Section 3.1), and another decoder with domain adversarial neural network(Ganin and Lempitsky, 2015) to predict its domain. The additional domain-adversarial decoder is used to enforce the feature layer to be domain-invariant (Section 3.2). 3.1 CNN-based Encoder-Decoder Model for Relations Each sentence is truncated or padded to a fixed length (ls ) of tokens. Each token of the text is then 426 and dc is the embedding dimension. On dep path embedding: For each token, we have a vector to indicate whether the token is on the dependency path between the two entities. The vector size is dd . The"
I17-2072,P05-1053,0,0.0354718,"or Relation Extraction with Domain Adversarial Neural Network Lisheng Fu Thien Huu Nguyen∗ Bonan Min† Ralph Grishman New York University, New York, NY, USA {lisheng, grishman}@cs.nyu.edu ∗ University of Oregon, Eugene, OR, USA thien@cs.uoregon.edu † Raytheon BBN Technologies, Cambridge, MA, USA bonan.min@raytheon.com Abstract laborious to obtain, not to mention that relation mentions are sparse in the text. Take ACE 2004 as an example, Personal/Social relations appear only once on average per document. Such a method will not scale to the open-ended set of possible domains. Among the features (Zhou et al., 2005) used for relation extraction, shortest dependency path can be applied cross-domain while argument-specific features (e.g., entity types, lexical forms) are likely to be more domain-specific. We hypothesize that it is possible to learn both domain-invariant and domain-specific representations with neural networks, and use the domain-invariant representation for many new domains. In this paper, we propose to use a Domain Adversarial Neural Network (DANN) (Ganin and Lempitsky, 2015; Ajakan et al., 2014) to learn a domain-invariant representation for relations. Our contributions are twofold: Rela"
I17-2072,W06-1615,0,0.145208,"n features by itself and that requires no labels in targets. 1 Introduction Relation Extraction (RE) captures the semantic relation between two entities within a sentence, such as the Located relation between e1 and e2 in the sentence: “in the &lt;e2&gt;West Bank&lt;/e2&gt;, a &lt;e1&gt;passenger &lt;/e1&gt;was wounded when an Israeli bus came under fire.” The same relation might be expressed differently across diverse documents, topics and genres. We often observed that a relation extractor’s performance degrades when applied to a domain other than the domain it is trained on. A simple method for domain adaptation (Blitzer et al., 2006; Daume, 2007; Jing and Zhai, 2007) is to construct a labeled dataset for the target domain, and then adjust a trained model with it. This is inefficient for relations - annotation is • Experiments on the ACE domains show that our approach improves on the state-of-the-art across all domains. In the rest of the paper, we will first briefly summarize related work, then describe the model (Section 3). We will present experimental results and conclusion at the end. 2 Related Work There has been a lot of research on domain adaptation in natural language processing (Blitzer et al., 2006; Daume, 2007"
I17-2072,P07-1033,0,0.0462532,"nd that requires no labels in targets. 1 Introduction Relation Extraction (RE) captures the semantic relation between two entities within a sentence, such as the Located relation between e1 and e2 in the sentence: “in the &lt;e2&gt;West Bank&lt;/e2&gt;, a &lt;e1&gt;passenger &lt;/e1&gt;was wounded when an Israeli bus came under fire.” The same relation might be expressed differently across diverse documents, topics and genres. We often observed that a relation extractor’s performance degrades when applied to a domain other than the domain it is trained on. A simple method for domain adaptation (Blitzer et al., 2006; Daume, 2007; Jing and Zhai, 2007) is to construct a labeled dataset for the target domain, and then adjust a trained model with it. This is inefficient for relations - annotation is • Experiments on the ACE domains show that our approach improves on the state-of-the-art across all domains. In the rest of the paper, we will first briefly summarize related work, then describe the model (Section 3). We will present experimental results and conclusion at the end. 2 Related Work There has been a lot of research on domain adaptation in natural language processing (Blitzer et al., 2006; Daume, 2007; Jing and Zh"
I17-2072,D15-1205,0,0.0654979,"fully connected layers. The GRL is defined as an identity function with reversed gradient for backpropagation. For input layer x: d GRL(x) = x, dx GRL(x) = −I where I is the identity matrix. We use a binary cross-entropy loss for the domain classifier: NsP +Nt {di log(dˆi ) + (1 − di )log(1 − Ldomain = 4 Experiements 4.1 Dataset We use the ACE 2005 dataset to evaluate domain adaptation by dividing its articles from its six genres into respective domains: broadcast conversation (bc), broadcast news (bn), telephone conversation (cts), newswire (nw), usenet (un) and weblogs (wl). Previous work (Gormley et al., 2015; Nguyen and Grishman, 2016) uses newswire (bn & nw) as the training set, half of bc as the development set, the other half of bc, cts and wl as the test sets. We use the same data split. Our model requires unlabeled target domain instances. To meet this requirement and avoid train-on-test, we also split cts and wl when adapting to them. For all three test domains, we use half of the dataset as the development set, and the other half as the test set (Table 1). We use the same training set and the same preprocessing. This results in 43,497 entity pairs for training. We also use the same label s"
I17-2072,P07-1034,0,0.164687,"res no labels in targets. 1 Introduction Relation Extraction (RE) captures the semantic relation between two entities within a sentence, such as the Located relation between e1 and e2 in the sentence: “in the &lt;e2&gt;West Bank&lt;/e2&gt;, a &lt;e1&gt;passenger &lt;/e1&gt;was wounded when an Israeli bus came under fire.” The same relation might be expressed differently across diverse documents, topics and genres. We often observed that a relation extractor’s performance degrades when applied to a domain other than the domain it is trained on. A simple method for domain adaptation (Blitzer et al., 2006; Daume, 2007; Jing and Zhai, 2007) is to construct a labeled dataset for the target domain, and then adjust a trained model with it. This is inefficient for relations - annotation is • Experiments on the ACE domains show that our approach improves on the state-of-the-art across all domains. In the rest of the paper, we will first briefly summarize related work, then describe the model (Section 3). We will present experimental results and conclusion at the end. 2 Related Work There has been a lot of research on domain adaptation in natural language processing (Blitzer et al., 2006; Daume, 2007; Jing and Zhai, 2007; Glorot et al"
I17-2072,P14-2012,1,0.872462,"ere has been a lot of research on domain adaptation in natural language processing (Blitzer et al., 2006; Daume, 2007; Jing and Zhai, 2007; Glorot et al., 2011; Ajakan et al., 2014; 425 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 425–429, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Ganin and Lempitsky, 2015). Most of the existing domain adaptation methods are based on discrete feature representations and linear classifiers. There is also recent work on domain adaptation for relation extraction including feature-based systems (Nguyen and Grishman, 2014; Nguyen et al., 2014) and kernelbased system (Plank and Moschitti, 2013). Nguyen and Grishman (2014) and Nguyen et al. (2014) both require a few labels in the target domain. Our proposed method can perform domain adaptation without target labels. Some other methods also do not have such requirement. Plank and Moschitti (2013) designed the semantic syntactic tree kernel (SSTK) to learn cross-domain patterns. Nguyen et al. (2015b) constructed a case study comparing feature-based methods and kernel-based models. They presented some effective features and kernels (e.g. word embedding).We share th"
J86-3002,C82-1014,1,0.874014,"Missing"
J86-3002,A83-1007,0,0.0253866,"Missing"
J86-3002,P84-1023,1,0.814419,"Missing"
J86-3002,A83-1006,0,0.0364551,"our studies of discovery procedures for domain-specific knowledge. 3 3.1 DISCOVERY PROCEDURES EXPERT VS. TEXT-BASED PROCEDURES Two basic approaches have been proposed for mechanizing (or partially mechanizing) the acquisition of domainspecific information for natural language systems. One of these is based on the systematic interviewing of a domain expert, who provides information on the basic semantic classes and relations of the domain and their linguistic forms and properties. Such an approach has been incorporated into some natural language interfaces for database retrieval, such as TEAM (Grosz 1983) and LDC (Ballard, Lusth, and Tinkham 1984). This approach assumes that the domain expert has some model of the relations in the domain, and a knowledge of the different ways in which these relations can be referenced. This is not unreasonable in the database context, since the database schema can serve as a domain model (divining all the ways in which a relationship can be referenced may still be difficult, however). This approach is more difficult, however, in text analysis applications, particularly because the user may not have such a clear model of the domain semantics from which to work."
J86-3002,J83-3003,0,0.0512428,"Missing"
J86-3002,A83-1016,0,\N,Missing
L16-1088,P14-2013,0,0.190502,"tem to construct a KB from scratch. In the Wikification community (Bunescu and Pas¸ca, 2006) text mentions are linked to Wikipedia, a large and publicly available knowledge base. There are two paradigms to solve the EL problem: local, non-collective approaches for Entity Linking resolve one mention at a time relying on a context and local features, while collective approaches try to disambiguate the set of relevant mentions simultaneously assuming that entities appearing in the same document should be coherent (Cucerzan, 2007; Kulkarni et al., 2009; Ratinov et al., 2011; Hoffart et al., 2011; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015b). Our approach in this paper is based on PPRSim, a state-of-the-art collective model for named entity disambiguation, utilizing the Personalized PageRank (PPR) algorithm (Pershina et al., 2015b). Nevertheless, we still try to capture the local similarity between the entity mention and its candidates in our model. To measure the local similarity, we propose to use an approach which was proven effective for paraphrase detection. For this purpose we adopt the state-of-the-art ASOBEK paraphrase model (Eyecioglu and Keller, 2015). It was developed for paraphrase identificat"
L16-1088,P05-1074,0,0.0644768,"orrespondingly. One can view name variations as paraphrases of the same entity mention. There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across phrases. Thus, in a broad sense, detecting whether two phrases refer to the same entity mention is a particular case of the paraphrase problem. 1 A growing body of research studied the problem of paraphrases in Twitter (Xu et al., 2015b; Guo et al., 2013; 0.995 Guo and Diab, 2013; Socher et al., 2011), in bilingual data (Bannard and Callison-Burch, 2005), and even paraphrases between0.99 idioms (Pershina et al., 2015a). Finally, there was a new Paraphrase In Twitter track (PIT) pro0.9852015 (Xu et al., 2015a). Most paraposed in SemEval phrase models are tailored for a data set that they will be 0.98 applied to. Thus, Twitter paraphrase models often make use of hashtags, timestamps, geotags, or require topic and anchor words (Xu et al., 2015b). None of this is applicable 0.975 to named entity mentions. Based on this observation, we focus on a holistic ASOBEK 0.97 approach (Eyecioglu and Keller, 2015) for paraphrase identification in entity lin"
L16-1088,E06-1002,0,0.0469982,"Missing"
L16-1088,D07-1074,0,0.18869,"Missing"
L16-1088,S15-2011,0,0.12828,"tinov et al., 2011; Hoffart et al., 2011; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015b). Our approach in this paper is based on PPRSim, a state-of-the-art collective model for named entity disambiguation, utilizing the Personalized PageRank (PPR) algorithm (Pershina et al., 2015b). Nevertheless, we still try to capture the local similarity between the entity mention and its candidates in our model. To measure the local similarity, we propose to use an approach which was proven effective for paraphrase detection. For this purpose we adopt the state-of-the-art ASOBEK paraphrase model (Eyecioglu and Keller, 2015). It was developed for paraphrase identification in Twitter and was ranked first among 19 teams on the Paraphrase In Twitter (PIT) 2015 task. It uses six simple character and word features and trains an SVM. This universal system is trained on pairs of entity name variations, which we make publicly available, and provides an accurate similarity measure between entity mention strings. We make the following contributions in this paper: 1) we propose to use the paraphrase model to measure the similarity between entity mention strings and provide publicly available training data for this model; 2)"
L16-1088,P13-1024,0,0.0197756,"f differences. For example, the above pairs have edit distance of 4, 12, 11, and 15 correspondingly. One can view name variations as paraphrases of the same entity mention. There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across phrases. Thus, in a broad sense, detecting whether two phrases refer to the same entity mention is a particular case of the paraphrase problem. 1 A growing body of research studied the problem of paraphrases in Twitter (Xu et al., 2015b; Guo et al., 2013; 0.995 Guo and Diab, 2013; Socher et al., 2011), in bilingual data (Bannard and Callison-Burch, 2005), and even paraphrases between0.99 idioms (Pershina et al., 2015a). Finally, there was a new Paraphrase In Twitter track (PIT) pro0.9852015 (Xu et al., 2015a). Most paraposed in SemEval phrase models are tailored for a data set that they will be 0.98 applied to. Thus, Twitter paraphrase models often make use of hashtags, timestamps, geotags, or require topic and anchor words (Xu et al., 2015b). None of this is applicable 0.975 to named entity mentions. Based on this observation, we focus on a"
L16-1088,D11-1072,0,0.0902985,"Missing"
L16-1088,W15-2709,1,0.507864,"ch. In the Wikification community (Bunescu and Pas¸ca, 2006) text mentions are linked to Wikipedia, a large and publicly available knowledge base. There are two paradigms to solve the EL problem: local, non-collective approaches for Entity Linking resolve one mention at a time relying on a context and local features, while collective approaches try to disambiguate the set of relevant mentions simultaneously assuming that entities appearing in the same document should be coherent (Cucerzan, 2007; Kulkarni et al., 2009; Ratinov et al., 2011; Hoffart et al., 2011; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015b). Our approach in this paper is based on PPRSim, a state-of-the-art collective model for named entity disambiguation, utilizing the Personalized PageRank (PPR) algorithm (Pershina et al., 2015b). Nevertheless, we still try to capture the local similarity between the entity mention and its candidates in our model. To measure the local similarity, we propose to use an approach which was proven effective for paraphrase detection. For this purpose we adopt the state-of-the-art ASOBEK paraphrase model (Eyecioglu and Keller, 2015). It was developed for paraphrase identification in Twitter and was"
L16-1088,N15-1026,1,0.458044,"ch. In the Wikification community (Bunescu and Pas¸ca, 2006) text mentions are linked to Wikipedia, a large and publicly available knowledge base. There are two paradigms to solve the EL problem: local, non-collective approaches for Entity Linking resolve one mention at a time relying on a context and local features, while collective approaches try to disambiguate the set of relevant mentions simultaneously assuming that entities appearing in the same document should be coherent (Cucerzan, 2007; Kulkarni et al., 2009; Ratinov et al., 2011; Hoffart et al., 2011; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015b). Our approach in this paper is based on PPRSim, a state-of-the-art collective model for named entity disambiguation, utilizing the Personalized PageRank (PPR) algorithm (Pershina et al., 2015b). Nevertheless, we still try to capture the local similarity between the entity mention and its candidates in our model. To measure the local similarity, we propose to use an approach which was proven effective for paraphrase detection. For this purpose we adopt the state-of-the-art ASOBEK paraphrase model (Eyecioglu and Keller, 2015). It was developed for paraphrase identification in Twitter and was"
L16-1088,P11-1138,0,0.041075,"Missing"
L16-1088,S15-2001,0,0.0495767,"Missing"
L16-1088,P12-1091,0,\N,Missing
L16-1088,Q14-1034,0,\N,Missing
L16-1088,J13-3001,0,\N,Missing
M91-1013,A92-1022,1,\N,Missing
M91-1013,C92-2099,1,\N,Missing
M91-1028,C90-3071,1,0.885292,"Missing"
M91-1028,J81-4005,0,0.00851334,"mposes a heavy penalty if the structure doe s not match any lexico-semantic model, and a lesser penalty if the structure matches a model but with som e operands or modifiers left over) [2,3 ] relaxation of certain syntactic constraints, such as the count noun constraint, adverb position constraints, an d comma constraints disfavoring (penalizing) headless noun phrases and headless relatives (this is important for parsin g efficiency) • • The grammar is based on Harris's Linguistic String Theory and adapted from the larger Linguistic Strin g Parser (LSP) grammar developed by Naomi Sager at NYU [4] . The grammar is gradually being enlarged to cove r more of the LSP grammar . The current grammar is 1200 lines of BNF and Restriction Language plus 300 lines of Lisp ; it includes 150 non-terminals, 365 productions, and 103 restrictions . Over the course of MUC-2 and MUC-3 we have added several mechanisms for recovering from sentence s the grammar cannot fully parse ; these are described in our site report. SEMANTIC ANALYSIS AND REFERENCE RESOLUTIO N The output of syntactic analysis goes through semantic analysis and reference resolution and is then added t o the accumulating logical form fo"
M91-1028,H89-2011,1,\N,Missing
M91-1028,C92-2099,1,\N,Missing
M91-1036,P89-1030,1,0.82301,"ate as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumption that a non-trivial amount of an input message may not be understood . This led u s to focus more on approaches that depend less on reliable"
M91-1036,H90-1047,0,0.0290048,"a model of the relevant events in the domain (e .g . , &quot;murder&quot;), it attempts to derive any information which was not already found by the semantic interpreter . The primary output of the discourse module is a sequence of frame-like event structures, which are in tur n the input to the template generator . The discourse module of PLUM is new (as are all the other PLUM components except the parser), an d in flux—what is reported here is its state as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus s"
M91-1036,H91-1033,0,0.0123249,"a model of the relevant events in the domain (e .g . , &quot;murder&quot;), it attempts to derive any information which was not already found by the semantic interpreter . The primary output of the discourse module is a sequence of frame-like event structures, which are in tur n the input to the template generator . The discourse module of PLUM is new (as are all the other PLUM components except the parser), an d in flux—what is reported here is its state as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus s"
M91-1036,P87-1022,0,0.0286572,"module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumption that a non-trivial amount of an input message may not be understood . This led u s to focus more on approaches that depend less on reliable complete understanding of the text"
M91-1036,C90-3023,1,0.832015,"lities . We did not specify which problems should or should not be included . 2 We consider the fifteen systems that participated in MUC-3 as a representative sample of implemented natural languag e text processing systems in the USA . 256 actions, discourse structure etc [15] [6] . Moreover, other versions of some of the text understanding system s represented in MUC-3 have incorporated such capabilities for discourse analysis in narrow domains (se e references in the following sections) . While there have been implementations of much more detailed discours e analysis for narrow domains [26] [11] no implementation of discourse analyses for broad domains exist whic h could guide efforts on the MUC-3 domain . If implemented, those missing capabilities would most certainly improve system performance . There are several reasons why the MUC-3 versions of the participating systems lack them at this point . First, given time constraints (it has been proposed to hold MUC annually) and the fact that the existing capabilities directl y contribute to the performance in the MUC-3 domain, their improvement takes priority over implementin g new ones . Second, the significant effort required to rese"
M91-1036,P83-1007,0,0.0299913,"module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumption that a non-trivial amount of an input message may not be understood . This led u s to focus more on approaches that depend less on reliable complete understanding of the text"
M91-1036,J86-3001,0,0.340785,"of topi c identification would have permitted an even finer-grained, more correct segmentation . This is because i n many texts a dominating topic segment can contain antecedents of anaphora, and play a role inside dominate d segments . In a proper anaphora resolution algorithm, antecedents should be sought in the current segmen t and certain dominating segments . The tree ITP constructed for message 99 does not fully recover th e discourse structure of message 99, which actually has a dominating segment consisting of sentences 1,2,3 an d 14 . Sentence 15 `pops &apos; up to the dominating segment [13] [14] . If this dominance structure is represented, it is then possible to find the correct antecedent for `the attacks &apos; in sentence 5, which refers to the attack o n the two embassies . Its antecedent can be found in sentence 1 of the dominating segment . Future research will enable the ITP discourse segmentation algorithm to recover discourse dominanc e relations and become sensitive to tense, aspect and topic . LSI&apos;S DBG MESSAGE UNDERSTANDING SYSTEM : DISCOURSE PROCESSIN G LS I &apos;s approach to discourse processing is based on the notion of text grammar, which was originally defined by Propp"
M91-1036,P88-1012,0,0.0360817,"n phrase to the individuals with which it coreferential unless these individuals appear syntacticall y conjoined in the text . In other words, it will not form new groupings of individuals . Thus, it would no t handle `X was attacked . Y was attacked . The attacks were conducted by Z .&apos; Nor is the system able to figure out that the `TWO VEHICLES&apos; mentioned in TST1-MUC3-0099 are the CAR-BOMB and USS R EMBASSY VEHICLE, since these antecedents do not appear conjoined . SRI&apos;S TACITUS : DISCOURSE PROCESSIN G The TACITUS system employs the general method of abductive explanation to understand texts [16] . This method of explanation is quite well suited to the narrative texts of the MUC-3 domain, because the text s consist almost entirely of declarative sentences that are intended to convey information to the reader . TACITUS does not have an explicit discourse processing module, and does not currently employ any theory o f discourse structure, but rather relies on the assumption that the correct resolution of anaphora and individuation of events will be a consequence of generating the best explanation for the truth of its constituen t sentences, subject to minimizing the extension of certain"
M91-1036,P89-1032,0,0.016045,"or . The discourse module of PLUM is new (as are all the other PLUM components except the parser), an d in flux—what is reported here is its state as of the MUC-3 test . This module is a significant departure from the discourse module in BBN &apos; s Janus [3] and the related module in BBN &apos; s Delphi [4, 5] . The discourse components of those systems, which were developed primarily for question-answering applications in limite d domains, are able to take advantage of having complete analyses of the input sentences . For example , syntactic information is used to constrain intra-sentential anaphora [19] ; complete semantic representation s (including quantification information) are used in generating discourse entities in a principled way [2] ; and centering heuristics [7, 12] are used for tracking focus, improving anaphora resolution . 3 For a more detailed characterization of the MUC-3 corpus see the introduction to these proceedings . 4 If the description includes an existing module not used for MUC-3, then a brief explanation as to why it was not used i s given . 257 A fundamental characteristic of PLUM, however, is the emphasis on fragmentary processing at all levels , and the assumptio"
M91-1036,P91-1008,0,0.0307206,"m at this point . First, given time constraints (it has been proposed to hold MUC annually) and the fact that the existing capabilities directl y contribute to the performance in the MUC-3 domain, their improvement takes priority over implementin g new ones . Second, the significant effort required to research and implement any of these capabilities may not be proportional to the increase in performance . Many aspects of discourse, e .g ., constructing a theoretical and computational model of the relation tha t holds between different sentences of a text, are still open research problems [18] [22] . A typical MUC- 3 message contains fairly unrestricted text with long and complex sentences . A fair amount of knowledge i s required in order to correctly interpret it . Both solving theoretical aspects of these problems and implementing them on such a broad and complex domain as MUC-3 3 is an extremely challenging goal, practicall y unachievable within the limited resources most systems have at their disposal . The purpose of this paper is to : 1. present and compare the approaches used in the systems represented here in addressing the thre e discourse understanding capabilities mentioned"
M91-1036,J88-2005,0,0.0242778,"apabilities . We did not specify which problems should or should not be included . 2 We consider the fifteen systems that participated in MUC-3 as a representative sample of implemented natural languag e text processing systems in the USA . 256 actions, discourse structure etc [15] [6] . Moreover, other versions of some of the text understanding system s represented in MUC-3 have incorporated such capabilities for discourse analysis in narrow domains (se e references in the following sections) . While there have been implementations of much more detailed discours e analysis for narrow domains [26] [11] no implementation of discourse analyses for broad domains exist whic h could guide efforts on the MUC-3 domain . If implemented, those missing capabilities would most certainly improve system performance . There are several reasons why the MUC-3 versions of the participating systems lack them at this point . First, given time constraints (it has been proposed to hold MUC annually) and the fact that the existing capabilities directl y contribute to the performance in the MUC-3 domain, their improvement takes priority over implementin g new ones . Second, the significant effort required to"
M91-1036,A88-1018,0,0.0303999,"cking of tense and time, explore further the issues of merging of events , and add a representation of ambiguity at various levels . GE&apos;S NLTOOLSET : DPM AND TRUMPET Discourse processing is the ability to understand connected text . This includes the ability to recogniz e fragments of text that describe individual events . We do discourse-related processing in two stages : before parsing, the Discourse Processing Module (DPM) [20], produces an initial segmentation of the input story int o fragments relevant for different events ; then, after parsing, the top-down expectation module (TRUMPET ) [28] uses special domain knowledge to connect the representations of individual sentences relating to th e same event . Our intuition is that discourse should drive text understanding, including parsing and interpretation . Placing DPM before parsing is the first step toward this mode of text understanding . DPM and TRUMPE T currently overlap somewhat in their discourse-related processing . We maintain them as independent modules in order to ensure fewer errors by employing multiple strategies, to evaluate the performance of the syste m in different configurations and to experiment with different"
M91-1036,J88-2002,0,\N,Missing
M92-1015,C92-2099,1,0.713696,"e past year, we focussed on the task of acquiring the selectional constraints needed for the MUC texts . We have tried to automate this task by parsing 1000 MUC messages (without semantic constraints) and collectin g frequency information on subject-verb-object and head-modifier patterns . Where possible, we used the classification hierarchy (which we had built by hand) to generalize words in these patterns to word classes . We then used these patterns as selectional constraints in parsing new text ; we found that they did slightly better than th e constraints we had created by hand last year [1] . The gain was small -- not likely to affect template score -- but should be an advantage in moving to a new domain, particularly if even larger corpora are available . We have not yet completed the complementary task of building the word classes from this distributional information . GRAMMAR EVALUATION To understand why some systems did better than others, we need some glass-box evaluation of individua l components. As we know, it is very hard to define any glass-box evaluation which can be applied across systems . 126 We have experimented with one aspect of this, grammar (parse) evaluation,"
M92-1015,A92-1022,1,0.809854,"those systems which generate a full sentence parse. We use as our standard for comparison the Univ . of Pennsylvania Tree Bank, which includes parse trees for a portion of the MUC terrorist corpus . We take our parse trees, restructure them (automatically) to conform bette r to the Penn parses, strip labels from brackets, and then compare the bracket structure to that of the Tree Bank . The result is a recall/precision score which should be meaningful across systems . We have experimented with a number of parsing strategies, and found that parse recall is well correlate d with template recall [2] . In principle, we would like to try to extend these comparisons to ""deeper"" relations, such as functiona l subject/object relations. These will be harder to define, but may be applicable over a broader range of systems . MULTI-LINGUAL MUC We were fortunate to have two researchers from Spain, Antonio Moreno Sandoval and Cristina Olmeda Moreno, who over the past nine months have built a Spanish version of our MUC system (a Spanish grammar, dictionary, and lexico-semantic models) [3] . As this system has developed, we have gradually revised and extende d our system so that we can have a languag"
M92-1032,C90-3071,1,0.891149,"Missing"
M92-1032,J81-4005,0,0.0120449,"mposes a heavy penalty if the structure doe s not match any lexico-semantic model, and a lesser penalty if the structure matches a model but with som e operands or modifiers left over) [2,3 ] • relaxation of certain syntactic constraints, such as the count noun constraint, adverb position constraints, and comma constraint s • disfavoring (penalizing) headless noun phrases and headless relatives (this is important for parsing efficiency) The grammar is based on Harris&apos;s Linguistic String Theory and adapted from the larger Linguistic Strin g Project (LSP) grammar developed by Naomi Sager at NYU [4] . The grammar is gradually being enlarged to cove r more of the LSP grammar. The current grammar is 1600 lines of BNF and Restriction Language plus 300 lines o f Lisp; it includes 186 non-terminals, 464 productions, and 132 restrictions . Over the course of the MUCs we have added several mechanisms for recovering from sentences the gram mar cannot fully parse : 237 • • • allowing the grammar to skip a single word, or a series of words enclosed in parentheses or dashes, with a large score penalty if no parse is obtained for the entire sentence, taking the analysis which, starting at the first"
M93-1016,C90-3071,1,0.891128,"Missing"
M93-1016,J81-4005,0,0.0148414,"Missing"
M95-1001,A83-1009,0,0.0971758,"Missing"
M95-1001,M95-1005,0,\N,Missing
M95-1014,C94-1042,1,0.757187,"forth the advantage s of this approach . This approach can be viewed as a form of conservative parsing, although the high-leve l structures which are created are not explicitly syntactic . At the end of this paper we return to the questio n of the relation of pattern matching to approaches which use a comprehensive grammar . THE SYSTEM We exaggerate, of course, the radicalness of our change since MUC-5 [4] (and since the MUC-6 dry run , which was conducted with our traditional syntactic system) . Several components were direct descendant s of earlier modules : the dictionary was Comlex Syntax [3] ; the lexical analyzer (for names, etc .) had bee n gradually enhanced at least since MUC-3 ; the concept hierarchy code and reference resolution were essentiall y unchanged from earlier versions . In addition, our grammatical approach was not entirely abandoned ; ou r noun group patterns were a direct adaptation of the corresponding portion of our grammar, just as Hobbs &apos; patterns were an adaptation from his grammar .&apos; And, as we shall see, more of the grammar crept in a s our effort progressed . In essence, one could say that our MUC-6 system was built (in late August and earl y September,"
M95-1014,M93-1016,1,0.721327,"matching approach, in order to better appreciate its strengths and weaknesses . In particular, we carefull y studied the FASTUS system of Hobbs et al . [1], who have clearly and eloquently set forth the advantage s of this approach . This approach can be viewed as a form of conservative parsing, although the high-leve l structures which are created are not explicitly syntactic . At the end of this paper we return to the questio n of the relation of pattern matching to approaches which use a comprehensive grammar . THE SYSTEM We exaggerate, of course, the radicalness of our change since MUC-5 [4] (and since the MUC-6 dry run , which was conducted with our traditional syntactic system) . Several components were direct descendant s of earlier modules : the dictionary was Comlex Syntax [3] ; the lexical analyzer (for names, etc .) had bee n gradually enhanced at least since MUC-3 ; the concept hierarchy code and reference resolution were essentiall y unchanged from earlier versions . In addition, our grammatical approach was not entirely abandoned ; ou r noun group patterns were a direct adaptation of the corresponding portion of our grammar, just as Hobbs &apos; patterns were an adaptation f"
M95-1014,M91-1028,1,\N,Missing
M95-1014,M93-1019,0,\N,Missing
M98-1011,M95-1014,1,0.603041,"Missing"
M98-1018,A97-1029,0,0.410851,"is due to the fact that it can incorporate just about any binary-valued feature which is a function of the history and future of the current token. In the following sections, we will discuss each of MENE's feature classes in turn. Binary Features While all of MENE's features have binary-valued output, the inary"" features are features whose history"" can be considered to be either on or o for a given token. Examples are 	he token begins with a capitalized letter"" or 	he token is a four-digit number"". The binary features which MENE uses are very similar to those used in BBN's Nymble system [1]. Figure 1 gives an example of a binary feature. Lexical Features To create a lexical history, the tokens at w,2 : : : w2 (where the current token is denoted as w0 ) are compared with the vocabulary and their vocabulary indices are recorded. 8 < g(h; f ) = : 1 0 View(token,1 (h)) = Mr"" and f = per: ifsonLexical unique : else 9 = ; (4)  Correctly predicts: Mr Jones A more subtle feature picked up by MENE: preceding word is 	o"" and future is location unique"". Given the domain of the MUC-7 training data, 	o"" is a weak indicator, but a real one. This is an example of a feature which MENE can"
M98-1018,M95-1014,1,0.571878,". I.e. we can leave dangerous-looking names like Storm"" in the rst-name dictionary because whenever the rst-name feature res on Storm, the lexical feature for Storm will also re and, assuming that the use of Storm as other"" exceeded the use of Storm as person start, we can expect that the lexical feature will have a high enough alpha value to outweigh the dictionary feature. External Systems Features For NYU's ocial entry in the MUC-7 evaluation, MENE took in the output of a signi cantly enhanced version of the traditional, hand-coded Proteus"" named-entity tagger which we entered in MUC-6 [2]. In addition, subsequent to the evaluation, the University of Manitoba [4] and IsoQuest, Inc. [3] shared with us the outputs of their systems on our training corpora as well as on various test corpora. The output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us. These systems were incorporated into MENE by a fairly simple process of token alignment which resulted in the futures"" produced by the three external systems become three di erent histories"" for MENE. The external system features can query this data in a window of w,1 : :"
M98-1018,M98-1015,0,0.0505779,"er the rst-name feature res on Storm, the lexical feature for Storm will also re and, assuming that the use of Storm as other"" exceeded the use of Storm as person start, we can expect that the lexical feature will have a high enough alpha value to outweigh the dictionary feature. External Systems Features For NYU's ocial entry in the MUC-7 evaluation, MENE took in the output of a signi cantly enhanced version of the traditional, hand-coded Proteus"" named-entity tagger which we entered in MUC-6 [2]. In addition, subsequent to the evaluation, the University of Manitoba [4] and IsoQuest, Inc. [3] shared with us the outputs of their systems on our training corpora as well as on various test corpora. The output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us. These systems were incorporated into MENE by a fairly simple process of token alignment which resulted in the futures"" produced by the three external systems become three di erent histories"" for MENE. The external system features can query this data in a window of w,1 : : : w1 around the current token. 8 < g(h; f ) = : 1 0  if Proteus System Future(token0 (h)) = per"
M98-1018,M98-1006,0,0.0318856,"ctionary because whenever the rst-name feature res on Storm, the lexical feature for Storm will also re and, assuming that the use of Storm as other"" exceeded the use of Storm as person start, we can expect that the lexical feature will have a high enough alpha value to outweigh the dictionary feature. External Systems Features For NYU's ocial entry in the MUC-7 evaluation, MENE took in the output of a signi cantly enhanced version of the traditional, hand-coded Proteus"" named-entity tagger which we entered in MUC-6 [2]. In addition, subsequent to the evaluation, the University of Manitoba [4] and IsoQuest, Inc. [3] shared with us the outputs of their systems on our training corpora as well as on various test corpora. The output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us. These systems were incorporated into MENE by a fairly simple process of token alignment which resulted in the futures"" produced by the three external systems become three di erent histories"" for MENE. The external system features can query this data in a window of w,1 : : : w1 around the current token. 8 < g(h; f ) = : 1 0  if Proteus System Fu"
macleod-etal-2000-american,ide-etal-2000-xces,1,\N,Missing
macleod-etal-2000-american,J93-2004,0,\N,Missing
macleod-etal-2000-american,A97-1015,0,\N,Missing
macleod-etal-2000-american,J95-4004,0,\N,Missing
macleod-etal-2000-american,P93-1032,0,\N,Missing
macleod-etal-2000-american,C94-1103,0,\N,Missing
macleod-etal-2000-american,W98-1102,1,\N,Missing
meyers-etal-1998-multilingual,J93-1004,0,\N,Missing
meyers-etal-1998-multilingual,W96-0201,0,\N,Missing
meyers-etal-1998-multilingual,C94-1015,0,\N,Missing
meyers-etal-1998-multilingual,C96-1030,0,\N,Missing
meyers-etal-1998-multilingual,J93-2003,0,\N,Missing
meyers-etal-1998-multilingual,C90-3044,0,\N,Missing
meyers-etal-1998-multilingual,C94-2175,0,\N,Missing
meyers-etal-1998-multilingual,C92-2101,0,\N,Missing
meyers-etal-1998-multilingual,P91-1022,0,\N,Missing
meyers-etal-1998-multilingual,P93-1004,0,\N,Missing
meyers-etal-1998-multilingual,J93-1006,0,\N,Missing
meyers-etal-1998-multilingual,C96-1078,1,\N,Missing
meyers-etal-1998-multilingual,J97-2004,0,\N,Missing
meyers-etal-1998-multilingual,P97-1039,0,\N,Missing
meyers-etal-1998-multilingual,P93-1002,0,\N,Missing
meyers-etal-1998-multilingual,P98-2139,1,\N,Missing
meyers-etal-1998-multilingual,C98-2134,1,\N,Missing
meyers-etal-1998-multilingual,P98-1117,0,\N,Missing
meyers-etal-1998-multilingual,C98-1113,0,\N,Missing
meyers-etal-1998-multilingual,1992.tmi-1.23,1,\N,Missing
meyers-etal-2002-formal,A00-2031,0,\N,Missing
meyers-etal-2002-formal,brants-plaehn-2000-interactive,0,\N,Missing
meyers-etal-2002-formal,W98-0604,1,\N,Missing
meyers-etal-2002-formal,W01-1511,1,\N,Missing
meyers-etal-2002-formal,C00-1041,0,\N,Missing
meyers-etal-2002-formal,H94-1020,0,\N,Missing
meyers-etal-2004-annotating,kingsbury-palmer-2002-treebank,0,\N,Missing
meyers-etal-2004-annotating,meyers-etal-2004-cross,1,\N,Missing
min-grishman-2012-challenges,strassel-etal-2010-darpa,0,\N,Missing
min-grishman-2012-challenges,P11-1115,1,\N,Missing
moreno-etal-2000-treebank,J93-2004,0,\N,Missing
moreno-etal-2000-treebank,A97-1014,0,\N,Missing
mota-grishman-2008-ne,P06-2004,0,\N,Missing
mota-grishman-2008-ne,W99-0613,0,\N,Missing
mota-grishman-2008-ne,W06-0206,1,\N,Missing
mota-grishman-2008-ne,A00-1040,0,\N,Missing
mota-grishman-2008-ne,M95-1001,1,\N,Missing
N03-4013,H01-1009,1,0.89314,"Missing"
N10-1036,N03-4004,0,0.0252912,"ents as ‘centroid entities’; and then for each centroid entity, link and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 3.1 Evaluation Methods Study Execution Our measurement challenge is to assess how IE techniques affect users’ abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read the news articles, with assistance of keyword based sentence search; • Level (II): (I) + with assistance from singledocument IE results; • Level (III): (I) + with assistance from crossdocument IE results. The summary writing task for each entity using any level should be finished in 10 minutes. The users can choose to trust the IE results to create new sen"
N10-1036,N09-5001,1,0.838892,"form. The cross-document IE system can identify important person entities which are frequently in1 http://www.itl.nist.gov/iad/mig/tests/ace/2005/ 285 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 285–288, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics volved in events as ‘centroid entities’; and then for each centroid entity, link and order the events centered around it on a time line and associate them to a geographical map. The event chains are presented in a user-friendly graphical interface (Ji and Chen, 2009). Both systems link the events back to their context documents. 3 3.1 Evaluation Methods Study Execution Our measurement challenge is to assess how IE techniques affect users’ abilities to perform realworld tasks. We followed the summary writing task described in the Integrated Feasibility Experiment of the DARPA TIDES program (Colbath and Kubala, 2003) and the daily task conducted by intelligence analysts (Bodnar, 2003). Each task in our evaluation is based on writing a summary of ACE-type events involving a specific centroid entity, using one of three levels of support: • Level (I): Read th"
N10-1036,R09-1032,1,\N,Missing
N13-1095,P07-1073,0,0.256008,"cal domain. 2 Freebase is a large collaboratively-edited KB. It is available at http://www.freebase.com. 3 There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair < e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v ′ . These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Ta"
N13-1095,P11-1055,0,0.722057,"available at http://www.freebase.com. 3 There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair < e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v ′ . These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern sho"
N13-1095,N07-1015,0,0.0201723,"Missing"
N13-1095,P04-3022,0,0.00681371,"In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1 An occurrence"
N13-1095,A00-2030,0,0.013435,"hoice for training relation extractors. In this paper, we show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly"
N13-1095,D12-1094,1,0.321574,"show that a significant number of “negative“ examples generated by the labeling process are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1 An occurrence of a pair of entit"
N13-1095,P09-1113,0,0.914853,"rocess are false negatives because the knowledge base is incomplete. Therefore the heuristic for generating negative examples has a serious flaw. Building on a state-of-the-art distantly-supervised extraction algorithm, we proposed an algorithm that learns from only positive and unlabeled labels at the pair-of-entity level. Experimental results demonstrate its advantage over existing algorithms. 1 Introduction Relation Extraction is a well-studied problem (Miller et al., 2000; Zhou et al., 2005; Kambhatla, 2004; Min et al., 2012a). Recently, Distant Supervision (DS) (Craven and Kumlien, 1999; Mintz et al., 2009) has emerged to be a popular choice for training relation extractors without using manually labeled data. It automatically generates training examples by labeling relation mentions1 in the source corpus according to whether the argument pair is listed in the target relational tables in a knowledge base (KB). This method significantly reduces human efforts for relation extraction. The labeling heuristic has a serious flaw. Knowledge bases are usually highly incomplete. For exam1 An occurrence of a pair of entities with the source sentence. Chang Wang, David Gondek IBM T. J. Watson Research Cent"
N13-1095,P11-2048,0,0.271373,"are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair < e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v ′ . These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimated from the heuristically lab"
N13-1095,D12-1042,0,0.691067,"w.freebase.com. 3 There are variants of labeling heuristics. For example, Surdeanu et al. (2011) and Sun et al. (2011) use a pair < e, v &gt; as a negative example, when it is not listed in Freebase, but e is listed with a different v ′ . These assumptions are also problematic in cases where the relation is not functional. 777 Proceedings of NAACL-HLT 2013, pages 777–782, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Since then, it has gain popularity (Mintz et al., 2009; Bunescu and Mooney, 2007; Wu and Weld, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Nguyen and Moschitti, 2011). To tolerate noisy labels in positive examples, Riedel et al. (2010) use Multiple Instance Learning (MIL), which assumes only at-least-one of the relation mentions in each “bag“ of mentions sharing a pair of argument entities which bears a relation, indeed expresses the target relation. MultiR (Hoffmann et al., 2011) and Multi-Instance Multi-Label (MIML) learning (Surdeanu et al., 2012) further improve it to support multiple relations expressed by different sentences in a bag. Takamatsu et al. (2012) models the probabilities of a pattern showing relations, estimat"
N13-1095,P05-1053,0,0.227585,"Missing"
N13-1095,P12-1076,0,\N,Missing
N15-1026,E09-1005,0,0.0125483,"and (Cheng and Roth, 2013)) and have been applied successfully in TAC shared tasks (Cucerzan, 2011). These methods often involve optimizing an objective function that contains both local and global terms, and thus requires training on an annotated or distantly annotated dataset. Our system performs collective NED using a random walk algorithm that does not require supervision. Random walk algorithms such as PageRank (Page et al., 1999) and Personalized PageRank (Jeh and Widom, 2003) have been successfully applied to NLP tasks, such as Word Sense Disambiguation (WSD: (Sinha and Mihalcea, 2007; Agirre and Soroa, 2009)). Alhelbawy and Gaizauskas (2014) successfully apply the PageRank algorithm to the NED task. Their work is the closest in spirit to ours and performs well without supervision. We try to further improve their model by using a PPR model to better utilize local features, and by adding constraints to the random walk to reduce noise. 3 The Graph Model We construct a graph representation G(V, E) from the document D with pre-tagged named entity textual mentions M = {m1 , ..., mk }. For each entity mention mi ∈ M there is a list of candidates in KB Ci = {ci1 , ..., cini }. Vertices V are defined as p"
N15-1026,P14-2013,0,0.521843,"the task in which entity mentions in a document are mapped to real world entities. NED is both useful on its own, and serves as a valuable component in larger Knowledge Base Construction systems (Mayfield, 2014). Since the surge of large, publicly available knowledge bases (KB) such as Wikipedia, the most popular approach has been linking text mentions to KB nodes (Bunescu and Pas¸ca, 2006). In this paradigm, the NED system links text mentions to the KB, and quite naturally utilizes information in the KB to support the linking process. Recent NED systems (Cucerzan, 2007; Ratinov et al., 2011; Alhelbawy and Gaizauskas, 2014) usually exploit two types of KB information: local information, which measures the similarity between the text mention and the a candidate KB node; and global information, which measures how well the candidate entities in a document are connected to each other, with the assumption that entities appearing in the same document should be coherent. Both types of features have their strengths and drawbacks: local features better encode similarity between a candidate and a KB node, but overlook the coherence between entities; global features are able to exploit interlinking information between enti"
N15-1026,E06-1002,0,0.0560495,"Missing"
N15-1026,D13-1184,0,0.200282,"the ACL, pages 238–243, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics (2006) first utilize information in a knowledge base (Wikipedia) to disambiguate names, by calculating similarity between the context of a name mention and the taxonomy of a KB node. Later research, such as Cucerzan (2007) and Milne and Witten (2008) extends this line by exploring richer feature sets, such as coherence features between entities. Global coherence features have therefore been widely used in NED research (see e.g. (Ratinov et al., 2011), (Hoffart et al., 2011), and (Cheng and Roth, 2013)) and have been applied successfully in TAC shared tasks (Cucerzan, 2011). These methods often involve optimizing an objective function that contains both local and global terms, and thus requires training on an annotated or distantly annotated dataset. Our system performs collective NED using a random walk algorithm that does not require supervision. Random walk algorithms such as PageRank (Page et al., 1999) and Personalized PageRank (Jeh and Widom, 2003) have been successfully applied to NLP tasks, such as Word Sense Disambiguation (WSD: (Sinha and Mihalcea, 2007; Agirre and Soroa, 2009))."
N15-1026,D07-1074,0,0.497791,"n Name entity disambiguation (NED) is the task in which entity mentions in a document are mapped to real world entities. NED is both useful on its own, and serves as a valuable component in larger Knowledge Base Construction systems (Mayfield, 2014). Since the surge of large, publicly available knowledge bases (KB) such as Wikipedia, the most popular approach has been linking text mentions to KB nodes (Bunescu and Pas¸ca, 2006). In this paradigm, the NED system links text mentions to the KB, and quite naturally utilizes information in the KB to support the linking process. Recent NED systems (Cucerzan, 2007; Ratinov et al., 2011; Alhelbawy and Gaizauskas, 2014) usually exploit two types of KB information: local information, which measures the similarity between the text mention and the a candidate KB node; and global information, which measures how well the candidate entities in a document are connected to each other, with the assumption that entities appearing in the same document should be coherent. Both types of features have their strengths and drawbacks: local features better encode similarity between a candidate and a KB node, but overlook the coherence between entities; global features ar"
N15-1026,D11-1072,0,0.751934,"Missing"
N15-1026,P11-1138,0,0.816697,"sambiguation (NED) is the task in which entity mentions in a document are mapped to real world entities. NED is both useful on its own, and serves as a valuable component in larger Knowledge Base Construction systems (Mayfield, 2014). Since the surge of large, publicly available knowledge bases (KB) such as Wikipedia, the most popular approach has been linking text mentions to KB nodes (Bunescu and Pas¸ca, 2006). In this paradigm, the NED system links text mentions to the KB, and quite naturally utilizes information in the KB to support the linking process. Recent NED systems (Cucerzan, 2007; Ratinov et al., 2011; Alhelbawy and Gaizauskas, 2014) usually exploit two types of KB information: local information, which measures the similarity between the text mention and the a candidate KB node; and global information, which measures how well the candidate entities in a document are connected to each other, with the assumption that entities appearing in the same document should be coherent. Both types of features have their strengths and drawbacks: local features better encode similarity between a candidate and a KB node, but overlook the coherence between entities; global features are able to exploit inte"
N15-3007,P13-4027,0,0.0151228,"formal notation. In this paper, we review related systems and explain the technologies behind ICE. The code, documentation, and a demo video of ICE can be found at http://nlp.cs.nyu.edu/ice/ 31 Proceedings of NAACL-HLT 2015, pages 31–35, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics the other hand, aims at extracting both entities and relations. Furthermore, S PIED produces token sequence rules, while our system helps the user to construct lexico-syntactic extraction rules that are based on dependency paths. The P ROPMINER system from T. U. Berlin (Akbik et al., 2013) takes an approach more similar to our own. In particular, it is based on a dependency analysis of the text corpus and emphasizes exploratory development of the IE system, supported by search operations over the dependency structures. However, the responsibility for generalizing initial patterns lies primarily with the user, whereas we support the generalization process through distributional analysis. Corpus in new domain Processed corpus in new domain Processed corpus in general domain Text extraction 1. Preprocessing Tokenization 2. Key phrase extraction Key phrase Index 3. Entity set const"
N15-3007,D11-1133,0,0.162203,"Missing"
N15-3007,W14-3106,0,0.0127135,"g of rules from annotated corpora. A few groups have focused on use by NLP novices: The W IZ IE system from IBM Research (Li et al., 2012) is based on a finite-state rule language. Users prepare some sample annotated texts and are then guided in preparing an extraction plan (sequences of rule applications) and in writing the individual rules. IE development is seen as a rule programming task. This offers less in the way of linguistic support (corpus analysis, syntactic analysis) but can provide greater flexibility for extraction tasks where linguistic models are a poor fit. The S PIED system (Gupta and Manning, 2014) focuses on extracting lexical patterns for entity recognition in an interactive fashion. Our system, on Several groups have developed integrated systems for IE development: 1 https://www.ldc.upenn.edu/ collaborations/past-projects/ace We showcase ICE, an Integrated Customization Environment for Information Extraction. ICE is an easy tool for non-NLP experts to rapidly build customized IE systems for a new domain. 1 Introduction Creating an information extraction (IE) system for a new domain, with new vocabulary and new classes of entities and relations, remains a task requiring substantial ti"
N15-3007,P12-3019,0,0.11518,"ntrast to our system, it is aimed at skilled computational linguists. The Language Computer Corporation has described several tools developed to rapidly extend an IE system to a new task (Lehmann et al., 2010; Surdeanu and Harabagiu, 2002). Here too the emphasis is on tools for use by experienced IE system developers. Events and relations are recognized using finite-state rules, with meta-rules to efficiently capture syntactic variants and a provision for supervised learning of rules from annotated corpora. A few groups have focused on use by NLP novices: The W IZ IE system from IBM Research (Li et al., 2012) is based on a finite-state rule language. Users prepare some sample annotated texts and are then guided in preparing an extraction plan (sequences of rule applications) and in writing the individual rules. IE development is seen as a rule programming task. This offers less in the way of linguistic support (corpus analysis, syntactic analysis) but can provide greater flexibility for extraction tasks where linguistic models are a poor fit. The S PIED system (Gupta and Manning, 2014) focuses on extracting lexical patterns for entity recognition in an interactive fashion. Our system, on Several g"
N15-3007,W11-4002,1,0.775877,"ons dobj(sell, drug) and dobj(transport, drug) (where dobj is the direct object relation), thus members in the D RUGS set will share the features dobj sell and dobj transport. We use pointwise mutual information (PMI) to weight the feature vectors and use a cosine metric to measure the similarity between two term vectors. The terms are displayed as a ranked list, and the user can accept or reject individual members of the entity set. At any point the user can recompute the similarities and rerank the list (where the ranking is based the centroids of the accepted and rejected terms, following (Min and Grishman, 2011)). When the user is satisfied, the set of accepted terms will become a new semantic type for tagging further text. 3.5 Dependency path extraction and linearization ICE captures the semantic relation (if any) between two entity mentions by the lexicalized 33 nsubj Parker oversaw dobj business nn PERSON distribution nn a sophisticated crack cocaine DRUGS Figure 2: A parse tree; dotted relations ignored by LDP dependency path (LDP) and the semantic types of the two entities. LDP includes both the labels of the dependency arcs and the lemmatized form of the lexical items along the path. For exampl"
N16-1034,W06-0901,0,0.0632536,"Missing"
N16-1034,P14-1023,0,0.0168519,"indow of 2 for the local features, and the feed-forward neural networks with one hidden layer for F trg , F arg and F binary (the size of the hidden layers are 600, 600 and 300 respectively). Finally, for training, we use the mini-batch size = 50 and the parameter for the Frobenius norms = 3. These parameter values are either inherited from the prior research (Nguyen and Grishman, 2015b; Chen et al., 2015) or selected according to the validation data. We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit4 (modified to add the C-CBOW model). Following Baroni et al. (2014), we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples. We evaluate the model with the ACE 2005 corpus. For the purpose of comparison, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Nguyen and Grishman, 2015b; Chen et al., 2015). This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set. Also, 4 https://code.google.com/p/word"
N16-1034,P15-1017,0,0.470371,"cameraman as the Target argument for the event Attack with their local features. The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence. Despite the advantages presented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, it still suffers from"
N16-1034,P09-2093,0,0.0233377,"Missing"
N16-1034,P11-1113,0,0.350837,"Missing"
N16-1034,P08-1030,1,0.809113,"ter for the Frobenius norms = 3. These parameter values are either inherited from the prior research (Nguyen and Grishman, 2015b; Chen et al., 2015) or selected according to the validation data. We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit4 (modified to add the C-CBOW model). Following Baroni et al. (2014), we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples. We evaluate the model with the ACE 2005 corpus. For the purpose of comparison, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Nguyen and Grishman, 2015b; Chen et al., 2015). This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set. Also, 4 https://code.google.com/p/word2vec/ we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) to judge the correctness of the predicted event mentions. 5.2 Memory Vector/Matrices This section evaluates the effectiveness o"
N16-1034,D14-1181,0,0.00360714,"triggers and argument roles: C(T, A, X, E) = − log P (T, A|X, E) = − log P (T |X, E) − log P (A|T, X, E) n X trg =− log Pi;t∗ − i=1 n X i I(ti 6= “Other”) i=1 k X j=1 arg log Pij;a∗ ij where I is the indicator function. We apply the stochastic gradient descent algorithm with mini-batches and the AdaDelta update rule (Zeiler, 2012). The gradients are computed using back-propagation. During training, besides the weight matrices, we also optimize the word and entity type embedding tables to achieve the optimal states. Finally, we rescale the weights whose Frobenius norms exceed a hyperparameter (Kim, 2014; Nguyen and Grishman, 2015a). 4 Word Representation Following the prior work (Nguyen and Grishman, 2015b; Chen et al., 2015), we pre-train word embeddings from a large corpus and employ them to initialize the word embedding table. One of the models to train word embeddings have been proposed in Mikolov et al. (2013a; 2013b) that introduce two log-linear models, i.e the continuous bag305 of-words model (CBOW) and the continuous skipgram model (SKIP-GRAM). The CBOW model attempts to predict the current word based on the average of the context word vectors while the SKIPGRAM model aims to predic"
N16-1034,P13-1008,0,0.157368,"articipating into such events. This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might be present in various expressions, and an expression might expresses different events in different contexts. There are two main approaches to EE: (i) the joint approach that predicts event triggers and arguments for sentences simultaneously as a structured prediction problem, and (ii) the pipelined approach that first performs trigger prediction and then identifies arguments in separate stages. The most successful joint system for EE (Li et al., 2013) is based on the structured perceptron algorithm with a large set of local and global features1 . These features are designed to capture the discrete structures that are intuitively helpful for EE using the NLP toolkits (e.g., part of speech tags, dependency and constituent tags). The advantages of such a joint system are twofold: (i) mitigating the error propagation from the upstream component (trigger identification) to the downstream classifier (argument identification), and (ii) benefiting from the the inter-dependencies among event triggers and argument roles via global features. For exam"
N16-1034,D14-1198,0,0.0615326,"Missing"
N16-1034,P10-1081,1,0.602342,"orms = 3. These parameter values are either inherited from the prior research (Nguyen and Grishman, 2015b; Chen et al., 2015) or selected according to the validation data. We pre-train the word embeddings from the English Gigaword corpus utilizing the word2vec toolkit4 (modified to add the C-CBOW model). Following Baroni et al. (2014), we employ the context window of 5, the subsampling of the frequent words set to 1e-05 and 10 negative samples. We evaluate the model with the ACE 2005 corpus. For the purpose of comparison, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Nguyen and Grishman, 2015b; Chen et al., 2015). This data split includes 40 newswire articles (672 sentences) for the test set, 30 other documents (836 sentences) for the development set and 529 remaining documents (14,849 sentences) for the training set. Also, 4 https://code.google.com/p/word2vec/ we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015) to judge the correctness of the predicted event mentions. 5.2 Memory Vector/Matrices This section evaluates the effectiveness of the memory vector and m"
N16-1034,R11-1002,1,0.897403,"mention. ACE annotates 8 types and 33 subtypes (e.g., Attack, Die, Start-Position) for event mentions that also correspond to the types and subtypes of the event triggers. Each event subtype has its own set of roles to be filled by the event arguments. For instance, the roles for the Die event include Place, Victim and Time. The total number of roles for all the event subtypes is 36. Given an English text document, an event extraction system needs to recognize event triggers with specific subtypes and their corresponding arguments with the roles for each sentence. Following the previous work (Liao and Grishman, 2011; Li et al., 2013; Chen et al., 2015), we assume that the argument candidates (i.e, the entity mentions, temporal expressions and values) are provided (by the ACE annotation) to the event extraction systems. 2 http://projects.ldc.upenn.edu/ace 3 Model We formalize the EE task as follow. Let W = w1 w2 . . . wn be a sentence where n is the sentence length and wi is the i-th token. Also, let E = e1 , e2 , . . . , ek be the entity mentions3 in this sentence (k is the number of the entity mentions and can be zero). Each entity mention comes with the offsets of the head and the entity type. We furth"
N16-1034,P11-1163,0,0.265714,"Missing"
N16-1034,W15-1506,1,0.433652,"oach might fail to recognize cameraman as the Target argument for the event Attack with their local features. The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence. Despite the advantages presented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, i"
N16-1034,P15-2060,1,0.885827,"oach might fail to recognize cameraman as the Target argument for the event Attack with their local features. The joint approach can overcome this issue by relying on the global features to encode the fact that a Victim argument for the Die event is often the Target argument for the Attack event in the same sentence. Despite the advantages presented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, i"
N16-1034,D09-1016,0,0.0598437,"Missing"
N16-1034,N10-1123,0,0.0610405,"Missing"
N16-1034,D11-1001,0,0.0188606,"Missing"
N16-1034,W11-1807,0,0.0351499,"Missing"
N16-1034,W09-1406,0,0.0324773,"Missing"
N16-1034,P10-1040,0,0.0736485,"sented above, the joint system by Li et al. (2013) suffers from the lack of generalization over the unseen words/features and the inability to extract the underlying structures for EE (due to its discrete representation from the handcrafted feature set) (Nguyen and Grishman, 2015b; Chen et al., 2015). The most successful pipelined system for EE to date (Chen et al., 2015) addresses these drawbacks of the joint system by Li et al. (2013) via dynamic multi-pooling convolutional neural networks (DMCNN). In this system, words are represented by the continuous representations (Bengio et al., 2003; Turian et al., 2010; Mikolov et al., 2013a) and features are automatically learnt from data by the DMCNN, thereby alleviating the unseen word/feature problem and extracting more effective features for the given dataset. However, as the system by Chen et al. (2015) is pipelined, it still suffers from the inherent limitations of error propagation and failure to exploit the inter-dependencies between event triggers and argument roles (Li et al., 2013). Finally, we notice that the discrete features, shown to be helpful in the previous studies for EE (Li et al., 2013), are not considered in Chen et al. (2015). Guided"
N16-1034,D14-1090,0,0.135969,"Missing"
nobata-etal-2002-summarization,P98-1009,0,\N,Missing
nobata-etal-2002-summarization,C98-1009,0,\N,Missing
nobata-etal-2002-summarization,H01-1009,1,\N,Missing
nobata-etal-2002-summarization,A00-1039,1,\N,Missing
P03-1029,sekine-etal-2002-extended,1,\N,Missing
P03-1029,H01-1009,1,\N,Missing
P03-1029,A00-1039,1,\N,Missing
P04-1053,sekine-etal-2002-extended,1,\N,Missing
P04-1053,P02-1006,0,\N,Missing
P04-1053,W02-1010,0,\N,Missing
P05-1051,A97-1029,0,0.537895,"ities). There were 7 types of relations, with 23 subtypes. Examples of these relations are “the CEO of Microsoft” (an employ-exec relation), “Fred’s wife” (a family relation), and “a military base in Germany” (a located relation). In this paper we look at the problem of identifying name mentions in Chinese text and classifying them as persons, organizations, or GPEs. Because Chinese has neither capitalization nor overt word boundaries, it poses particular problems for name identification. Prior Work A wide variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same toke"
P05-1051,C02-1025,0,0.0421356,"variety of trainable models have been applied to the name tagging task, including HMMs (Bikel et al., 1997), maximum entropy models (Borthwick, 1999), support vector machines (SVMs), and conditional random fields. People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name. Some other named entity systems have explored global information for name tagging. (Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token. Recently, in (Ji and Grishman, 2004) we proposed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upp"
P05-1051,N04-4010,0,0.0727642,"e rules to correct and recover some names. One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names. We attempted to recover some of these names by heuristic rules which are quite language specific. In addition, this singlehypothesis method placed an upper bound on recall. Traditional statistical name tagging methods have generated a single name hypothesis. BBN proposed the N-Best algorithm for speech recognition in (Chow and Schwartz, 1989). Since then NBest methods have been widely used by other researchers (Collins, 2002; Zhai et al., 2004). In this paper, we tried to combine the advantages of the prior work, and incorporate broader knowledge into a more general re-ranking model. 412 Task and Terminology Our experiments were conducted in the context of the ACE Information Extraction evaluations, and we will use the terminology of these evaluations: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) name mention: a reference by name to an entity nominal mention: a reference by a common noun or noun phrase to an entity relation: one of a"
P05-1051,H89-2027,0,\N,Missing
P05-1051,W04-0705,1,\N,Missing
P05-1051,P02-1062,0,\N,Missing
P05-1052,P02-1031,0,0.0149045,"Missing"
P05-1052,kingsbury-palmer-2002-treebank,0,0.0199831,"rnel properties to integrate the existing syntactic kernels. In another direction, training data is often sparse for IE tasks. String matching is not sufficient to capture semantic similarity of words. One solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like WordNet. These word similarities can be readily incorporated into the kernel framework. To deal with sparse data, we can also use deeper text analysis to capture more regularities from the data. Such analysis may be based on newly-annotated corpora like PropBank (Kingsbury and Palmer, 2002) at the University of Pennsylvania and NomBank (Meyers et al., 2004) at New York University. Analyzers based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures. Although deeper analysis may even be less accurate, our framework is designed to handle this and still obtain some improvement in performance. 8 Acknowledgement This research was supported in part by the Defense Advanced Research Projects Agency under Grant N66001-04-1-8920 from SPAWAR San Diego, and by the National Science Foundation under Grant ITS-0325657."
P05-1052,W01-1511,1,0.576538,"ople Table 1. ACE relation types and examples. The heads of the two entity arguments in a relation are marked. Types are listed in decreasing order of frequency of occurrence in the ACE corpus. Figure 1 shows a sample newswire sentence, in which three relations are marked. In this sentence, we expect to find a PHYS relation between Hezbollah forces and areas, a PHYS relation between Syrian troops and areas and an EMP-ORG relation between Syrian troops and Syrian. In our approach, input text is preprocessed by the Charniak sentence parser (including tokenization and POS tagging) and the GLARF (Meyers et al., 2001) dependency analyzer produced by NYU. Based on treebank parsing, GLARF produces labeled deep dependencies between words (syntactic relations such as logical subject and logical object). It handles linguistic phenomena like passives, relatives, reduced relatives, conjunctions, etc. That&apos;s because Israel was expected to retaliate against Hezbollah forces in areas controlled by Syrian troops. PHYS PHYS EMP-ORG Figure 1. Example sentence from newswire text 4.2 Definitions In our model, kernels incorporate information from tokenization, parsing and deep dependency analysis. A relation candidate R i"
P05-1052,meyers-etal-2004-cross,0,0.0119844,"ection, training data is often sparse for IE tasks. String matching is not sufficient to capture semantic similarity of words. One solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like WordNet. These word similarities can be readily incorporated into the kernel framework. To deal with sparse data, we can also use deeper text analysis to capture more regularities from the data. Such analysis may be based on newly-annotated corpora like PropBank (Kingsbury and Palmer, 2002) at the University of Pennsylvania and NomBank (Meyers et al., 2004) at New York University. Analyzers based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures. Although deeper analysis may even be less accurate, our framework is designed to handle this and still obtain some improvement in performance. 8 Acknowledgement This research was supported in part by the Defense Advanced Research Projects Agency under Grant N66001-04-1-8920 from SPAWAR San Diego, and by the National Science Foundation under Grant ITS-0325657. This paper does not necessarily reflect the position of the U.S. Gov"
P05-1052,A00-2030,0,0.645661,"an pick up the features that best separate the targets from other examples, no matter which level these features are from. In cases where an error occurs in one processing result (especially deep processing) and the features related to it become noisy, SVM may pick up clues from other sources which are not so noisy. This forms the basic idea of our approach. Therefore under this scheme we can overcome errors introduced by one processing level; more particularly, we expect accurate low level information to help with less accurate deep level information. 3 Related Work Collins et al. (1997) and Miller et al. (2000) used statistical parsing models to extract relational facts from text, which avoided pipeline processing of data. However, their results are essentially based on the output of sentence parsing, which is a deep processing of text. So their approaches are vulnerable to errors in parsing. Collins et al. (1997) addressed a simplified task within a confined context in a target sentence. Zelenko et al. (2003) described a recursive kernel based on shallow parse trees to detect personaffiliation and organization-location relations, in which a relation example is the least common subtree containing tw"
P05-1052,C04-1109,1,0.766439,"Missing"
P05-1052,W98-1105,0,\N,Missing
P05-1052,P04-1054,0,\N,Missing
P06-2055,A97-1029,0,0.199668,"best of the N hypotheses1, using different models: English MonoCase (EN-Mono, without capitalization), English Mixed Case (EN-Mix, with capitalization), Chinese without the usable character restriction (CH-NoRes) and Chinese with the usable character restriction (CH-WithRes). Baseline Name Tagger We apply a multi-lingual (English / Chinese) bigram HMM tagger to identify four named entity types: Person, Organization, GPE (‘geopolitical entities’ – locations which are also political units, such as countries, counties, and cities) and Location. The HMM tagger generally follows the Nymble model (Bikel et al, 1997), and uses best-first search to generate N-Best hypotheses for each input sentence. In mixed-case English texts, most proper names are capitalized. So capitalization provides a crucial clue for name boundaries. In contrast, a Chinese sentence is composed of a string of characters without any word boundaries or capitalization. Even after word segmentation there are still no obvious clues for the name boundaries. However, we can apply the following coarse “usable-character” restrictions to reduce the search space. Standard Chinese family names are generally single characters drawn from a set of"
P06-2055,P05-1051,1,0.935459,"human annotators. 1 Introduction High-performance named entity (NE) tagging is crucial in many natural language processing tasks, such as information extraction and machine translation. In &apos;traditional&apos; pipelined system architectures, NE tagging is one of the first steps in the pipeline. NE errors adversely affect subsequent stages, and error rates are often compounded by later stages. However, (Roth and Yi 2002, 2004) and our recent work have focused on incorporating richer linguistic analysis, using the “feedback” from later stages to improve name taggers. We expanded our last year’s model (Ji and Grishman, 2005) that used the results of coreference analysis and relation extraction, by adding ‘feedback’ from more information extraction components – name structure parsing, cross-document coreference, and event extraction – to incrementally rerank the multiple hypotheses from a baseline name tagger. While together these components produced a further improvement on last year’s model, our goal in this paper is to look behind the overall performance figures in order to understand how these varied components contribute to the improvement, and compare the remaining system errors with the human annotator’s pe"
P06-2055,W06-3607,1,0.87638,"since these names are not tagged in the key -- the automatic scorer treats them as spurious names. Sentence Document Boundary Boundary Name Local Related Event Coreferring Candidate Context Mention trigger&arg Mentions Mutual Inferences between Information Extraction Stages Name tagging is typically one of the first stages 2 in an information extraction pipeline. Specifically, we will consider a system which was developed for the ACE (Automatic Content Extraction) task 3 and includes the following stages: name structure parsing, coreference, semantic relation extraction and event extraction (Ji et al., 2006). All these stages are performed after name tagging since they take names as input “objects”. However, the inferences from these subsequent stages can also provide valuable constraints to identify and classify names. Each of these stages connects the name candidate to other linguistic elements in the sentence, document, or corpus, as shown in Figure 3. The ACE task description can be found at http://www.itl.nist.gov/iad/894.01/tests/ace/ and the ACE guidelines at http://www.ldc.upenn.edu/Projects/ACE/ 4 Rather than offer the most fluent translation, we have provided one that more closely corre"
P06-2055,W04-2401,0,0.0574269,"Missing"
P06-2055,C02-1151,0,0.230639,"Missing"
P06-2055,N04-4010,0,0.0599371,"Missing"
P06-2055,J05-4005,0,\N,Missing
P08-1030,W06-0901,0,0.110363,"“Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve"
P08-1030,N07-1042,0,0.0638666,"ency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa260 tion retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion and Future Work One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further log"
P08-1030,D07-1075,0,0.0405993,"single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa260 tion retrieval techniques to obtain wider background knowledge from related documents. 8 Conclusion and Future Work One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further logical reasoning on the database. The artificial constraint that extraction should be done independently for each document was introduced in part to simplify the task and its"
P08-1030,H05-1008,0,0.0494685,"ic annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa260 tion retrieval techniques to obtain wider background knowledge from related do"
P08-1030,P95-1026,0,0.150088,"fact, compared to a statistical tagger trained on the corpus after expert adjudication, a human annotator tends to make more mistakes in trigger classification. For example it’s hard to decide whether “named” represents a “Personnel_Nominate” or “Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines. 7 Related Work The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006). We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific infere"
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P09-2089,mota-grishman-2008-ne,1,0.844478,"n for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of view of a developer who wants to analyze a new d"
P09-2089,N03-1023,0,0.0178289,"e first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of view of a developer wh"
P09-2089,W01-0501,0,0.020662,"arce. For instance, in the first NER evaluation for Portuguese, HAREM (Santos and Cardoso, 2007), only two out of the nine participants presented systems based on machine learning, and they both argued they could have achieved significantly better results if they had larger training sets. Semi-supervised methods are commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of v"
P09-2089,P08-2001,0,0.0280951,"Missing"
P09-2089,W99-0613,0,0.0504833,"commonly chosen as an alternative to overcome the lack of annotated resources, because they present a good trade-off between amount of labeled data needed and performance achieved. Co-training is one of those methods, and has been extensively studied in NLP (Nigam and Ghani, 2000; Pierce and Cardie, 2001; Ng and Cardie, 2003; Mota and Grishman, 2008). In particular, we showed that the performance of a name tagger based on co-training decays as the time gap between training data (seeds and unlabeled data) and test data increases (Mota and Grishman, 2008). Compared to the original classifier of Collins and Singer (1999) that uses seven seeds, we used substantially larger seed sets (more than 1000), which raises the question of which of the parameters (seeds or unlabeled data) are causing the performance deterioration. In the present study, we investigated two main questions, from the point of view of a developer who wants to analyze a new data set, given an NE tagger trained with older data. First, we studied whether it was better to update the seeds or the unlabeled data; then, we analyzed whether using a smaller amount of current unlabeled data could be better than increasing the amount of unlabeled data d"
P10-1081,W06-0901,0,0.311446,"l allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. Time 0001-T1-1 Table2. An example of event trigger and roles In this paper, we treat the 33 event subtypes as separate event types and do not consider the hierarchical structure among them. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context to refine the performance of relation extraction. They claimed that discourse information could filter noisy dependency paths as well as increasing the reliability of dependency path extraction. Finkel et al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using sim"
P10-1081,P09-2093,0,0.219155,"tion. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-event information within ACE extraction, but only for recovering implicit time information for events. Time 0001-T1-1 Table2. An example of event trigger and roles In this paper, we treat the 33 event subtypes as separate event types and do not consider the hierarchical structure among them. 3 Related Work Almost all the current ACE event extraction systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al. 2006). However, there have been several studies using high-level information from a wider scope: Maslennikov and Chua (2007) use discours"
P10-1081,P08-1030,1,0.580769,"s, pages 789–797, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics examples of a broader textual inference problem, and in general such knowledge is quite difficult to acquire and apply. However, in the present case we can take advantage of event extraction to learn these rules in a simpler fashion, which we present below. Most current event extraction systems are based on phrase or sentence level extraction. Several recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and Chua (2007), Ji and Grishman (2008), and Patwardhan and Riloff (2007, 2009) tried to use discourse, document, or cross-document information to improve information extraction. However, most of this research focuses on single event extraction, or focuses on high-level information within a single event type, and does not consider information acquired from other event types. We extend these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. Cross-event information is quite useful: first, some events co-occur frequently, while other events do not. For example, Attack,"
P10-1081,P07-1075,0,0.0653254,"for Computational Linguistics, pages 789–797, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics examples of a broader textual inference problem, and in general such knowledge is quite difficult to acquire and apply. However, in the present case we can take advantage of event extraction to learn these rules in a simpler fashion, which we present below. Most current event extraction systems are based on phrase or sentence level extraction. Several recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and Chua (2007), Ji and Grishman (2008), and Patwardhan and Riloff (2007, 2009) tried to use discourse, document, or cross-document information to improve information extraction. However, most of this research focuses on single event extraction, or focuses on high-level information within a single event type, and does not consider information acquired from other event types. We extend these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. Cross-event information is quite useful: first, some events co-occur frequently, while other events do n"
P10-1081,D07-1075,0,0.0156905,"Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics examples of a broader textual inference problem, and in general such knowledge is quite difficult to acquire and apply. However, in the present case we can take advantage of event extraction to learn these rules in a simpler fashion, which we present below. Most current event extraction systems are based on phrase or sentence level extraction. Several recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and Chua (2007), Ji and Grishman (2008), and Patwardhan and Riloff (2007, 2009) tried to use discourse, document, or cross-document information to improve information extraction. However, most of this research focuses on single event extraction, or focuses on high-level information within a single event type, and does not consider information acquired from other event types. We extend these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. Cross-event information is quite useful: first, some events co-occur frequently, while other events do not. For example, Attack, Die, and Injure events very freq"
P10-1081,D09-1016,0,0.578287,"s, with different Victim roles. And there is one Attack event sharing the same Place and Time roles with the Die events. Event type Trigger Die murder 0001-1-1 Die death 0001-1-1 0001-2-1 0001-T1-1 Die killing 0001-1-1 0001-3-1 0001-T1-1 Event type Attack Trigger Place 0001-1-1 Role Target 0001-2-3 Time 0001-T1-1 Place attack Role Victim to propagate consistent trigger classification and event arguments across sentences and documents. Combining global evidence from related documents with local decisions, they obtained an appreciable improvement in both event and event argument identification. Patwardhan and Riloff (2009) proposed an event extraction model which consists of two components: a model for sentential event recognition, which offers a probabilistic assessment of whether a sentence is discussing a domain-relevant event; and a model for recognizing plausible role fillers, which identifies phrases as role fillers based upon the assumption that the surrounding context is discussing a relevant event. This unified probabilistic model allows the two components to jointly make decisions based upon both the local evidence surrounding each phrase and the “peripheral vision”. Gupta and Ji (2009) used cross-eve"
P10-1081,P95-1026,0,0.0893826,"t al. (2005) used Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. They used this technique to augment an information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. Ji and Grishman (2008) were inspired from the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995); they extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach 4 Motivation We analyzed the sentence-level baseline event extraction, and found that many events are missing or spuriously tagged because the local information is not sufficient to make a confident decision. In some local contexts, it is easy to identify an event; in others, it is hard to do so. Thus, if we first tag the easier cases, and use such knowledge to help tag the harder cases, we might get better overall performance. In addition, global information can make the"
P10-1081,P05-1045,0,\N,Missing
P11-1053,J92-4003,0,0.445595,"Missing"
P11-1053,H05-1091,0,0.917,"c parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simp"
P11-1053,C10-1018,0,0.862302,"We study the impact of the size of training data on cluster features and analyze the performance improvements through an extensive experimental study. The rest of this paper is organized as follows: Section 2 presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm. Section 4 describes in detail a state-of-the-art supervised baseline system. Section 5 describes the clusterbased features and the cluster selection methods. We present experimental results in Section 6 and conclude in Section 7. Though Boschee et al. (2005) and Chan and Roth (2010) used word clusters in relation extraction, they shared the same limitation as the above approaches in choosing clusters. For example, Boschee et al. (2005) chose clusters of different granularities and Chan and Roth (2010) simply used a single threshold for cutting the word hierarchy. Moreover, Boschee et al. (2005) only augmented the predicate (typically a verb or a noun of the most importance in a relation in their definition) with word clusters while Chan and Roth (2010) performed this for any lexical feature consisting of a single word. In this paper, we systematically explore the effecti"
P11-1053,N07-1015,0,0.863911,"upervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical"
P11-1053,P04-3022,0,0.104269,"lation between the entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of"
P11-1053,P08-1068,0,0.0680594,"this experiment. The training documents for each size setup were split into a real training set and a development set in a ratio of 7:3 for selecting clusters. There are some clear trends in Table 8. Under each training size, PC4 consistently outperformed the baseline and the system Prefix10 for relation classification. For PC4, the gain for classification was more pronounced than detection. The mixed detection results of Prefix10 indicated that only using a single prefix may not be stable. We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al. (2008) for dependency parsing. PC4 with sizes 50, 125, 175 outperformed the baseline with sizes 75, 175, 225 respectively. But this was not the case when PC4 was tested with sizes 75 and 225. This might due to the complexity of the relation extraction task. 6.5 Analysis There were on average 69 cross-type errors in the baseline in Section 6.1 which were reduced to 56 528 by using PC4. Table 9 showed that most of the improvements involved EMP-ORG, GPE-AFF, DISC, ART and OTHER-AFF. The performance gain for PER-SOC was not as pronounced as the other five types. The five types of relations are ambiguous"
P11-1053,P09-1116,0,0.0257204,"Missing"
P11-1053,A00-2030,0,0.0270118,"ract an Employment relation between the entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. T"
P11-1053,N04-1043,0,0.146331,"clusters of various granularities for feature the previous work, this paper only focuses on decoding. Ratinov and Roth (2009) and Turian et relation extraction of major types. al. (2010) also explored this approach for name Given a relation instance x  ( s, mi , m j ) , where tagging. Though all of them used the same mi and m j are a pair of mentions and s is the hierarchical word clustering algorithm for the task sentence containing the pair, the goal is to learn a of name tagging and reported improvements, we function which maps the instance x to a type c, noticed that the clusters used by Miller et al. (2004) where c is one of the 7 defined relation types or the were quite different from that of Ratinov and Roth type Nil (no relation exists). There are two (2009) and Turian et al. (2010). To our knowledge, commonly used learning paradigms for relation there has not been work on selecting clusters in a extraction: Flat: This strategy performs relation detection principled way. We move a step further to explore and classification at the same time. One multi-class several methods in choosing effective clusters. A classifier is trained to discriminate among the 7 second difference between this work an"
P11-1053,C08-1088,0,0.27258,"nd semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting the relation. This mo"
P11-1053,W09-1119,0,0.0160139,"roper name), NOM al. (2004), who augmented name tagging training (nominal) or PRO (pronoun). A relation was data with hierarchical word clusters generated by defined over a pair of entity mentions within a the Brown clustering algorithm (Brown et al., 1992) single sentence. The 7 major relation types with from a large unlabeled corpus. They used different examples are shown in Table 1. ACE 2004 also thresholds to cut the word hierarchy to obtain defined 23 relation subtypes. Following most of clusters of various granularities for feature the previous work, this paper only focuses on decoding. Ratinov and Roth (2009) and Turian et relation extraction of major types. al. (2010) also explored this approach for name Given a relation instance x  ( s, mi , m j ) , where tagging. Though all of them used the same mi and m j are a pair of mentions and s is the hierarchical word clustering algorithm for the task sentence containing the pair, the goal is to learn a of name tagging and reported improvements, we function which maps the instance x to a type c, noticed that the clusters used by Miller et al. (2004) where c is one of the 7 defined relation types or the were quite different from that of Ratinov and Roth"
P11-1053,R09-2014,1,0.824651,"pes of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Before M1 Between Feature BM1F BM1L WM1 HM1 WBNULL WBFL WBF WBL WBO M2 M12 After WM2 HM2 HM12 AM2F AM2L Description first word before M1 second word before M1 bag-of-words in M1 head3 word of M1 when no word in between the only word in between when only one word in between first word in between when at least two words in between last word in between when at least two words in between other words in between except first and last words when at least three words in between bag-of-words in M2 head word of M2 combination of HM1 and HM2 first word after M2 second word after M2 Table 3: Le"
P11-1053,C10-2137,1,0.534869,"Missing"
P11-1053,P10-1040,0,0.116661,"010) also explored this approach for name Given a relation instance x  ( s, mi , m j ) , where tagging. Though all of them used the same mi and m j are a pair of mentions and s is the hierarchical word clustering algorithm for the task sentence containing the pair, the goal is to learn a of name tagging and reported improvements, we function which maps the instance x to a type c, noticed that the clusters used by Miller et al. (2004) where c is one of the 7 defined relation types or the were quite different from that of Ratinov and Roth type Nil (no relation exists). There are two (2009) and Turian et al. (2010). To our knowledge, commonly used learning paradigms for relation there has not been work on selecting clusters in a extraction: Flat: This strategy performs relation detection principled way. We move a step further to explore and classification at the same time. One multi-class several methods in choosing effective clusters. A classifier is trained to discriminate among the 7 second difference between this work and the above relation types plus the Nil type. ones is that we utilize word clusters in the task of Hierarchical: This one separates relation relation extraction which is very differe"
P11-1053,P06-1104,0,0.210243,"racts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are cruci"
P11-1053,P05-1052,1,0.946513,"sed method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the w"
P11-1053,P05-1053,0,0.911563,"soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by"
P11-1053,D07-1076,0,0.435276,"exical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting t"
P11-1053,P08-1000,0,\N,Missing
P11-1115,S07-1012,0,0.00903138,"Missing"
P11-1115,D08-1029,0,0.0318388,"Missing"
P11-1115,C10-1032,0,0.358436,"Missing"
P11-1115,mcnamee-etal-2010-evaluation,0,0.0214604,"Missing"
P11-1115,I08-2112,0,0.00582762,"y the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents. Furthermore, we may want to use the extracted information to augment an existing data base. This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the data base. On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g. Pizzato et al., 2006), and limited use of relation/event extraction in answer search (e.g. McNamee et al., 2008). The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts. KBP is done through two separate subtasks, Entity Linking and Slot Filling; in 2010, 23 teams submitted results for one or both sub-tasks. A variety of approaches have been proposed to address both tasks with considerable success; nevertheless, there are many aspects of the task that remain unclear. W"
P11-1115,U06-1013,0,0.0182869,"s scattered among the documents of a large collection. This requires the ability to identify the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents. Furthermore, we may want to use the extracted information to augment an existing data base. This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the data base. On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g. Pizzato et al., 2006), and limited use of relation/event extraction in answer search (e.g. McNamee et al., 2008). The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts. KBP is done through two separate subtasks, Entity Linking and Slot Filling; in 2010, 23 teams submitted results for one or both sub-tasks. A variety of approaches have been proposed to address both tasks with co"
P11-1115,P06-1135,0,0.0384318,"Missing"
P11-1115,D10-1033,0,\N,Missing
P11-2045,W06-2207,0,0.0273225,"delines_v5.4.3.pdf 2 In this paper, we treat the event subtypes separately, and no type hierarchy is considered. 3 Note that we do not deal with event mention coreference in this paper, so each event mention is treated separately. 261 Related Work Self-training has been applied to several natural language processing tasks. For event extraction, there are several studies on bootstrapping from a seed pattern set. Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000, 2003) incorporated this into a bootstrapping approach, extended by Surdeanu et al. (2006) to co-training. Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm. However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns. Liu (2009) proposed the BEAR system, w"
P11-2045,C00-2136,1,0.778877,"le of event triggers and roles 1 http://projects.ldc.upenn.edu/ace/docs/English-Event s-Guidelines_v5.4.3.pdf 2 In this paper, we treat the event subtypes separately, and no type hierarchy is considered. 3 Note that we do not deal with event mention coreference in this paper, so each event mention is treated separately. 261 Related Work Self-training has been applied to several natural language processing tasks. For event extraction, there are several studies on bootstrapping from a seed pattern set. Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000, 2003) incorporated this into a bootstrapping approach, extended by Surdeanu et al. (2006) to co-training. Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm. However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automa"
P11-2045,P09-2093,0,0.0856722,"cal models. The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start year, end year). Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2010a) use a statistical model to infer the cross-event information within a document to improve event extraction. 4 for selecting the most confident examples is critical to the effectiveness of self-training. To acquire confident samples, we need to first decide how to evaluate the confidence for each event. However, as an event contains one trigger and an arbitrary number of roles, a confident event might contain unconfident arguments. Thus, instead of taking the whole event, we select a partial even"
P11-2045,P03-1044,0,0.139722,"Missing"
P11-2045,P08-1030,1,0.926036,"patterns were boostrapped based on the frequencies of sub-pattern mutations or on rules from linguistic contexts, and not on statistical models. The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start year, end year). Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2010a) use a statistical model to infer the cross-event information within a document to improve event extraction. 4 for selecting the most confident examples is critical to the effectiveness of self-training. To acquire confident samples, we need to first decide how to evaluate the confidence for each event. However, as an event contains one trigger and an arbitrary number"
P11-2045,H05-1008,0,0.0194012,"separately, and no type hierarchy is considered. 3 Note that we do not deal with event mention coreference in this paper, so each event mention is treated separately. 261 Related Work Self-training has been applied to several natural language processing tasks. For event extraction, there are several studies on bootstrapping from a seed pattern set. Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000, 2003) incorporated this into a bootstrapping approach, extended by Surdeanu et al. (2006) to co-training. Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm. However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns. Liu (2009) proposed the BEAR system, which tagged both the events and their roles. Ho"
P11-2045,P10-1081,1,0.955754,"eference in this paper, so each event mention is treated separately. 261 Related Work Self-training has been applied to several natural language processing tasks. For event extraction, there are several studies on bootstrapping from a seed pattern set. Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000, 2003) incorporated this into a bootstrapping approach, extended by Surdeanu et al. (2006) to co-training. Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm. However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns. Liu (2009) proposed the BEAR system, which tagged both the events and their roles. However, the new patterns were boostrapped based on the frequencies of sub-pattern mutations or on rules from linguist"
P11-2045,C10-1077,1,0.941377,"eference in this paper, so each event mention is treated separately. 261 Related Work Self-training has been applied to several natural language processing tasks. For event extraction, there are several studies on bootstrapping from a seed pattern set. Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. (2000, 2003) incorporated this into a bootstrapping approach, extended by Surdeanu et al. (2006) to co-training. Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. Liao and Grishman (2010b) combined these two approaches to build a filtered ranking algorithm. However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns. Liu (2009) proposed the BEAR system, which tagged both the events and their roles. However, the new patterns were boostrapped based on the frequencies of sub-pattern mutations or on rules from linguist"
P11-2045,N07-1042,0,0.0302345,"relevant extraction patterns. Liu (2009) proposed the BEAR system, which tagged both the events and their roles. However, the new patterns were boostrapped based on the frequencies of sub-pattern mutations or on rules from linguistic contexts, and not on statistical models. The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start year, end year). Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2010a) use a statistical model to infer the cross-event information within a document to improve event extraction. 4 for selecting the most confident examples is critical to the effectiveness of self-training. To acquire confident sampl"
P11-2045,P95-1026,0,0.0381383,"ent type rather than on argument role labeling. Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns. Liu (2009) proposed the BEAR system, which tagged both the events and their roles. However, the new patterns were boostrapped based on the frequencies of sub-pattern mutations or on rules from linguistic contexts, and not on statistical models. The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start year, end year). Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. Gupta and Ji (2009) used a similar approach to recover implicit time information for events. Liao and Grishman (2010a) use a statis"
P11-2045,E12-1030,0,\N,Missing
P11-2045,D07-1075,0,\N,Missing
P11-2045,P05-1047,0,\N,Missing
P13-2117,P07-1073,0,0.401219,"Missing"
P13-2117,P11-2048,0,0.373716,"Missing"
P13-2117,D10-1048,0,0.017931,"additional latent variables to a multi-instance multi-label model (Surdeanu et al., 2012) to solve this same problem.  Passage Retriever Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. 2 Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage ret"
P13-2117,P11-1055,1,0.887885,"can significantly improve the performance of distant supervision. In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance * This work was done while Le Zhao was at Carnegie Mellon University. 1 http://www.freebase.com 665 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Documents   Relation Extractor Knowledge Base   Pseudo-relevant Relation Instances  training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these iss"
P13-2117,D12-1042,0,0.869486,"ve the performance of distant supervision. In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance * This work was done while Le Zhao was at Carnegie Mellon University. 1 http://www.freebase.com 665 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Documents   Relation Extractor Knowledge Base   Pseudo-relevant Relation Instances  training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve signific"
P13-2117,P12-1076,0,0.656475,"Missing"
P13-2117,D11-1132,0,0.0165186,"Missing"
P13-2117,N13-1095,1,0.731943,"Missing"
P13-2117,P09-1113,0,0.982706,"and are thus mislabeled as negative. These mislabelings dilute the discriminative capability of useful features and confuse the models. In this paper, we will show how reducing this source of noise can significantly improve the performance of distant supervision. In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance * This work was done while Le Zhao was at Carnegie Mellon University. 1 http://www.freebase.com 665 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Documents   Relation Extractor Knowledge Base   Pseudo-rel"
P13-2117,I11-1117,1,\N,Missing
P14-2012,W10-2604,0,0.0639616,"s and Regularization for Domain Adaptation of Relation Extraction Thien Huu Nguyen Computer Science Department New York University New York, NY 10003 USA thien@cs.nyu.edu Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu Abstract source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although thi"
P14-2012,W06-1615,0,0.954812,"ng Word Representations and Regularization for Domain Adaptation of Relation Extraction Thien Huu Nguyen Computer Science Department New York University New York, NY 10003 USA thien@cs.nyu.edu Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu Abstract source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tre"
P14-2012,N07-1015,0,0.819416,"We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these s"
P14-2012,P07-1056,0,0.0748457,"niversity New York, NY 10003 USA thien@cs.nyu.edu Ralph Grishman Computer Science Department New York University New York, NY 10003 USA grishman@cs.nyu.edu Abstract source domain) into a new model which can perform well on new domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at diff"
P14-2012,P04-3022,0,0.0125361,"embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distributi"
P14-2012,J92-4003,0,0.093496,"based RE systems. After that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance. For each of these group combinations, we assess the system performance with different numbers of dimensions for both C&W and HLBL word embeddings. Let M1 and M2 be the first and second mentions in the relation. Table 1 describes the lexical feature groups. Word Representations We consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering (Brown et al., 1992) and word embeddings (Bengio et al., 2001). While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, lowdimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al., 2010). We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&W) (Collobert and Weston, 2008; Turian et al., 2010) and Hierarchical log-biline"
P14-2012,H05-1091,0,0.568775,"ement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to dom"
P14-2012,C10-1018,0,0.68633,"ore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrad"
P14-2012,D09-1143,0,0.0345313,"ffectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the 68 Proceedings"
P14-2012,P07-1033,0,0.242837,"Missing"
P14-2012,P13-1147,0,0.811978,"w domains (the target domains). The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging (Blitzer et al., 2006; Huang and Yates, 2010; Schnabel and Sch¨utze, 2014), named entity recognition (Daum´e III, 2007) and sentiment analysis (Blitzer et al., 2007; Daum´e III, 2007; Daum´e III et al., 2010; Blitzer et al., 2011), etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti (2013) who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations: + It does not incorporate word cluster information at different levels of granularity. In fact, Plank and Moschitti (2013) only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition"
P14-2012,C08-1088,0,0.254676,"e demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one doma"
P14-2012,Q14-1002,0,0.0330316,"Missing"
P14-2012,D12-1110,0,0.0198885,"presentation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors. 2 3 Related Work Although word embeddings have been successfully employed in many NLP tasks (Collobert and Weston, 2008; Turian et al., 2010; Maas and Ng, 2010), the application of word embeddings in RE is very recent. Kuksa et al. (2010) propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. (2012) and Khashabi (2013) use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper. Regarding domain adaptation, in representation learning, Blitzer et al. (2006) propose structural correspondence learning (SCL) while Huang and Yates (2010) attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-sy"
P14-2012,P11-1053,1,0.854437,"pply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Pla"
P14-2012,P10-1040,0,0.412106,"Missing"
P14-2012,P06-1104,0,0.0226561,"ormation. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically (Plank and Moschitti, 2013). This is where we need to resort to domain adaptation techniques (DA) to adapt a model"
P14-2012,P05-1053,0,0.798826,"feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors. 1 Introduction The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as Employment or Citizenship relationships. Recent research in this area, whether feature-based (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011) or kernelbased (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data"
P14-2119,P09-1113,0,0.515421,"(Yarowsky, 2013; Blum and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our proIntroduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2 ) that participate in a relation, r, is likely to express the fact r(e1 ,e2 ) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example."
P14-2119,P07-1073,0,0.122629,"um and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our proIntroduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2 ) that participate in a relation, r, is likely to express the fact r(e1 ,e2 ) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instan"
P14-2119,W99-0613,0,0.436382,"Missing"
P14-2119,P11-2048,0,0.0398241,"Missing"
P14-2119,P10-1030,0,0.12917,"Missing"
P14-2119,P11-1055,0,0.354374,"7). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2 ) that participate in a relation, r, is likely to express the fact r(e1 ,e2 ) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University 732 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–738, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Guideline g = {gi |i = 1, 2, 3}: types of entities, dependency path, span word (optional) person person, nsubj →← dobj, married person organization, nsubj →← prep of , became organization organization, nsubj →← prep of , company person person, poss →← appos, sister person person, poss →← appos, father person t"
P14-2119,P11-1115,1,0.771299,"Missing"
P14-2119,P03-1054,0,0.0202535,"Missing"
P14-2119,N13-1095,1,0.602879,"Missing"
P14-2119,D12-1042,0,0.465982,"ined by running experiments on the development dataset). Table 2 shows some examples in the final set G of extracted guidelines. 3 • zij ∈R ∪ NR: a latent variable that denotes the relation of the jth mention in the ith bag • hij ∈ R ∪ NR: a latent variable that denotes the refined relation of the mention xij We define relabeled relations hij as following:  r(g), if ∃!g ∈ G s.t.g ={gk}⊆{xij} hij (xij , zij )= zij , otherwise Guided DS Our goal is to jointly model human-labeled ground truth and structured data from a knowledge base in distant supervision. To do this, we extend the MIML model (Surdeanu et al., 2012) by adding a new layer as shown in Figure 1. The input to the model consists of (1) distantly supervised data, represented as a list of n bags1 with a vector yi of binary gold-standard labels, either P ositive(P ) or N egative(N ) for each relation r∈R; (2) generalized human-labeled ground truth, represented as a set G of feature conjunctions g={gi |i=1,2,3} associated with a unique relation r(g). Given a bag of sentences, xi , which mention an ith entity pair (e1 , e2 ), our goal is to correctly predict which relation is mentioned in each sentence, or NR if none of the relations under conside"
P14-2119,P12-1076,0,0.114183,"Missing"
P14-2119,P13-2117,1,0.827334,"Missing"
P14-2119,P95-1026,0,0.349402,"Missing"
P14-2119,P12-1087,0,0.581099,"Missing"
P14-2119,P05-1053,0,0.0361621,"ween the two arguments The Challenge Simply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data. An effective approach must recognize that the handlabeled data is more reliable than the automatically labeled data and so must take precedence in cases of conflict. Conflicts cannot be limited to those cases where all the features in two examples are the same; this would almost never occur, because of the dozens of features used by a typical relation extractor (Zhou et al., 2005). Instead we propose to perform feature selection to generalize human labeled data into training guidelines, and integrate them into latent variable model. 2.1 These three features are strong indicators of the type of relation between two entities. In some cases the semantic types of the arguments alone narrows the possibilities to one or two relation types. For example, entity types such as person and title often implies the relation personTitle. Some lexical items are clear indicators of particular relations, such as “brother” and “sister” for a sibling relationship We extract guidelines fro"
P14-2119,Q13-1030,0,\N,Missing
P15-1062,H05-1091,0,0.148681,"ompare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeti"
P15-1062,P07-1073,0,0.0249017,"ompatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the"
P15-1062,C10-1018,0,0.0199233,"t the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics chitti (2"
P15-1062,P07-1033,0,0.543746,"Missing"
P15-1062,W10-2604,0,0.0185843,"14), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance"
P15-1062,N07-1015,0,0.246901,"rror analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational"
P15-1062,D10-1115,0,0.0295108,"der the following methods to obtain the semantic representation Vi from the word embeddings of the context words of Ri (assuming d is the dimensionality of the word embeddings): HEAD: Vi = the concatenation of the word embeddings of the two entity mention heads of Ri . This representation is inherited from Nguyen and Grishman (2014) that only examine embeddings at the word level separately for the feature-based method without considering the compositionality embeddings of relation mentions. The dimensionality of HEAD is 2d. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semant"
P15-1062,P07-1034,0,0.625569,"rror analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational"
P15-1062,P04-3022,0,0.0514675,"system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beij"
P15-1062,D12-1050,0,0.0269934,"ciple of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree to represent the relation"
P15-1062,W06-1615,0,0.38594,"ional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting (Jiang and Zhai, 2007b). However, as shown by Plank and Moschitti (2013), in"
P15-1062,P07-1056,0,0.36694,"Missing"
P15-1062,P14-2012,1,0.114933,"stem trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (2013) and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a"
P15-1062,W15-1506,1,0.63793,"ngton Post is reporting she shot several Iraqi soldiers before she was captured and she was shot herself , too.”. However, as the syntactical structure of X1 is more similar to X2’s, and is remarkably different from X3 as well as the other closest phrases (ranked from 2nd to 8th), the new kernel function Knew would still prefer X2 due to its trade-off between syntax and semantics. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to l"
P15-1062,D09-1143,0,0.0167878,"es and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat"
P15-1062,P14-1076,0,0.0259328,"Missing"
P15-1062,N15-1155,0,0.124471,"Missing"
P15-1062,C14-1220,0,0.048275,"the sentence “The Washington Post is reporting she shot several Iraqi soldiers before she was captured and she was shot herself , too.”. However, as the syntactical structure of X1 is more similar to X2’s, and is remarkably different from X3 as well as the other closest phrases (ranked from 2nd to 8th), the new kernel function Knew would still prefer X2 due to its trade-off between syntax and semantics. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Hua"
P15-1062,P06-1104,0,0.394474,"method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Lingu"
P15-1062,P13-1147,1,0.0781232,", New York University, New York, NY 10003, USA § Center for Language Technology, University of Copenhagen, Denmark thien@cs.nyu.edu,bplank@cst.dk,grishman@cs.nyu.edu Abstract of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (2013) and Nguyen and Grishman ("
P15-1062,P05-1052,1,0.231135,"Missing"
P15-1062,C08-1088,0,0.0525648,"on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International J"
P15-1062,P05-1053,0,0.690766,". Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Associa"
P15-1062,D12-1110,0,0.0862588,"According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree"
P15-1062,P11-1053,1,0.953305,"ed method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics chitti (2013) consider the d"
P15-1062,P10-1040,0,0.610192,"e of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010) for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels. It is unclear at the first glance how to encode word embeddings into th"
P15-1062,I08-2119,0,0.0156253,"ing between relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction. Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions. In order to overcome this limitation, we conduct an evaluation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation"
P15-1062,J92-4003,0,\N,Missing
P15-1062,P14-1009,0,\N,Missing
P15-1062,D13-1170,0,\N,Missing
P15-1062,J08-3010,0,\N,Missing
P15-2060,P10-1081,1,0.801338,"ng backpropagation; regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012), and training is done via stochastic gradient descent with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014). During the training, we also optimize the weights of the three embedding tables at the same time to reach an effective state (Kim, 2014). 3 3.1 (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaning 529 documents (14,849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem. In order to evaluate the effectiveness of the position embeddings and the entity type embeddings, Table 1 reports the performance of the proposed CNN on the development set when these embeddings are either included or excluded from the systems. With the large margins of performance, it is very clear from the table that the position embeddings are crucial while the entity embeddings are also very useful for CNNs on ED. System"
P15-2060,W06-1615,0,0.644493,"hese features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih"
P15-2060,R11-1002,1,0.907637,"Missing"
P15-2060,N10-1004,0,0.0248636,"classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (She"
P15-2060,P11-1163,0,0.359711,"Missing"
P15-2060,P07-1033,0,0.238303,"Missing"
P15-2060,P09-2093,0,0.18052,"er was killed in New Jersey today”, an event detection system should be able to recognize the word “killed” as a trigger for the event “Die”. This task is quite challenging, as the same event might appear in the form of various trigger expressions and an expression might represent different events in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms. In order to prepare for"
P15-2060,P14-2012,1,0.889927,"Missing"
P15-2060,W15-1506,1,0.560299,"oken xi to the current token x0 . In practice, we initialize this table randomly. - Entity Type Embedding Table: If we further know the entity mentions and their entity types2 in the sentence, we can also capture this information for each token by looking up the entity type embedding table (initialized randomly) using the entity type associated with each token. We employ the BIO annotation scheme to assign entity type labels to each token in the trigger candidate 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). However, to the best of our knowledge, this is the first work on event detection via CNNs so far. First, we evaluate CNNs for ED in the general setting and show that CNNs, though not requiring complicated feature engineering, can still outperform the state-of-the-art feature-based methods extensively relying on the other supervised modules and manual resources for features. Second, we investigate CNNs in a domain adaptation (DA) setting for ED. We demonstrate that CNNs significantly outperform the traditional featurebased methods with respect to generalization performance across domains due"
P15-2060,P11-1113,0,0.753052,"joint inference architecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper pre"
P15-2060,D09-1016,0,0.330518,"Missing"
P15-2060,P13-1147,0,0.0146567,"obal features in Li et al. (2013b) CNN1: CNN without any external features 63.7 65.6 67.6 Table 3: Performance with Predicted Entity Mentions and Types. data in some source domain and learning models that can work well on target domains. The target domains are supposed to be so dissimilar from the source domain that the learning techniques would suffer from a significant performance loss when trained on the source domain and applied to the target domains. To make it clear, we address the unsupervised DA problem in this section, i.e no training data in the target domains (Blitzer et al., 2006; Plank and Moschitti, 2013). The fundamental reason for the performance loss of the featurebased systems on the target domains is twofold: (i) The behavioral changes of features across domains: As domains differ, some features might be informative in the source domain but become less relevant in the target domains and vice versa. (ii) The propagated errors of the pre-processing toolkits for lower-level tasks (POS tagging, name tagging, parsing etc) to extract features: These pre-processing toolkits are also known to degrade when shifted to target domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), i"
P15-2060,D11-1001,0,0.0186566,"Missing"
P15-2060,P08-1030,1,0.369163,"entence “A police officer was killed in New Jersey today”, an event detection system should be able to recognize the word “killed” as a trigger for the event “Die”. This task is quite challenging, as the same event might appear in the form of various trigger expressions and an expression might represent different events in different contexts. ED is a crucial component in the overall task of event extraction, which also involves event argument discovery. Recent systems for event extraction have employed either a pipeline architecture with separate classifiers for trigger and argument labeling (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan 1 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/ english-events-guidelines-v5.4.3.pdf 365 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 365–371, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: Convolutional Neural Network for Event Detection. along with its context in the sentence constitute an event trigger candidate or an example in multiclass classification terms. In o"
P15-2060,W11-1807,0,0.0147348,"Missing"
P15-2060,D14-1181,0,0.00826551,"in this paper, we always include ACE timex and values. 366 using the heads of the entity mentions. For each token xi , the vectors obtained from the three look-ups above are concatenated into a single vector xi to represent the token. As a result, the original event trigger x is transformed into a matrix x = [x−w , x−w+1 , . . . , x0 , . . . , xw−1 , xw ] of size mt × (2w + 1) (mt is the dimensionality of the concatenated vectors of the tokens). The matrix representation x is then passed through a convolution layer, a max pooling layer and a softmax at the end to perform classification (like (Kim, 2014; Kalchbrenner et al., 2014)). In the convolution layer, we have a set of feature maps (filters) {f1 , f2 , . . . , fn } for the convolution operation. Each feature map fi corresponds to some window size k and can be essentially seen as a weight matrix of size mt × k. Figure 1 illustrates the proposed CNN. The gradients are computed using backpropagation; regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012), and training is done via stochastic gradient descent with shuffled mini-batches and the AdaDelta update rule (Zeiler, 2012; Kim, 2014). During the training, we also"
P15-2060,P10-1040,0,0.0286579,"rter sentences with a special token when necessary. Let 2w + 1 be the fixed window size, and x = [x−w , x−w+1 , . . . , x0 , . . . , xw−1 , xw ] be some trigger candidate where the current token is positioned in the middle of the window (token x0 ). Before entering the CNNs, each token xi is transformed into a real-valued vector by looking up the following embedding tables to capture different characteristics of the token: - Word Embedding Table (initialized by some pre-trained word embeddings): to capture the hidden semantic and syntactic properties of the tokens (Collobert and Weston, 2008; Turian et al., 2010). - Position Embedding Table: to embed the relative distance i of the token xi to the current token x0 . In practice, we initialize this table randomly. - Entity Type Embedding Table: If we further know the entity mentions and their entity types2 in the sentence, we can also capture this information for each token by looking up the entity type embedding table (initialized randomly) using the entity type associated with each token. We employ the BIO annotation scheme to assign entity type labels to each token in the trigger candidate 2014), name tagging and semantic role labeling (Collobert et"
P15-2060,P13-1145,0,0.0880761,"chitecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convoluti"
P15-2060,D14-1090,0,0.137255,"Missing"
P15-2060,P13-1008,0,0.572837,"chitecture that performs the two subtasks at the same time to benefit from their inter-dependencies (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013b; Venugopal et al., 2014). Both approaches have coped with the ED task by elaborately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers etc) to extract these features to be fed into statistical classifiers. Although this approach has achieved the top performance (Hong et al., 2011; Li et al., 2013b), it suffers from at least two issues: (i) The choice of features is a manual process and requires linguistic intuition as well as domain expertise, implying additional studies for new application domains and limiting the capacity to quickly adapt to these new domains. (ii) The supervised NLP toolkits and resources for feature extraction might involve errors (either due to the imperfect nature or the performance loss of the toolkits on new domains (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convoluti"
P15-2060,P14-2105,0,0.00673781,"2006; Daum´e III, 2007; McClosky et al., 2010)), probably propagated to the final event detector. This paper presents a convolutional neural network (LeCun et al., 1988; Kalchbrenner et al., 2014) for the ED task that automatically learns features from sentences, and minimizes the dependence on supervised toolkits and resources for features, thus alleviating the error propagation and improving the performance for this task. Due to the emerging interest of the NLP community in deep learning recently, CNNs have been studied extensively and applied effectively in various tasks: semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), semantic matching (Hu et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as t"
P15-2060,C14-1220,0,0.0678473,"distance i of the token xi to the current token x0 . In practice, we initialize this table randomly. - Entity Type Embedding Table: If we further know the entity mentions and their entity types2 in the sentence, we can also capture this information for each token by looking up the entity type embedding table (initialized randomly) using the entity type associated with each token. We employ the BIO annotation scheme to assign entity type labels to each token in the trigger candidate 2014), name tagging and semantic role labeling (Collobert et al., 2011), relation classification and extraction (Zeng et al., 2014; Nguyen and Grishman, 2015). However, to the best of our knowledge, this is the first work on event detection via CNNs so far. First, we evaluate CNNs for ED in the general setting and show that CNNs, though not requiring complicated feature engineering, can still outperform the state-of-the-art feature-based methods extensively relying on the other supervised modules and manual resources for features. Second, we investigate CNNs in a domain adaptation (DA) setting for ED. We demonstrate that CNNs significantly outperform the traditional featurebased methods with respect to generalization per"
P15-2060,P14-1062,0,\N,Missing
P84-1023,C82-1040,0,\N,Missing
P84-1023,A83-1016,1,\N,Missing
P84-1023,A83-1007,0,\N,Missing
P98-2139,C90-3001,0,0.0745159,"Missing"
P98-2139,C94-1015,0,0.151793,"Missing"
P98-2139,C92-2101,0,0.758007,"Missing"
P98-2139,P93-1004,0,0.398705,"Missing"
P98-2139,C96-1078,1,0.924922,"erving Alignments Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, Antonio Moreno-Sandoval t New York U n i v e r s i t y 715 Broadway, 7th Floor, NY, NY 10003, USA tUniversidad A u t 6 n o m a de M a d r i d Cantoblanco, 28049-Madrid, SPAIN meyers/roman/grishman/macleod©cs, nyu. edu sandoval©lola, lllf. uam. es 1 Introduction Automatic acquisition of translation rules from parallel sentence-aligned text takes a variety of forms. Some machine translation (MT) systems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et hi., 1992), (Matsumoto et hi., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structural as well as lexical correspondences. A syntactically analyzed, aligned corpus may serve as an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (l(aji et al., 1992), and (Furuse and Iida. 1994)). This paper 1 describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; and (9) a procedure for deriving transfer rules from this al"
P98-2139,C90-3044,0,0.413629,"lation rules from parallel sentence-aligned text takes a variety of forms. Some machine translation (MT) systems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et hi., 1992), (Matsumoto et hi., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structural as well as lexical correspondences. A syntactically analyzed, aligned corpus may serve as an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (l(aji et al., 1992), and (Furuse and Iida. 1994)). This paper 1 describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; and (9) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived by &quot;cutting up&quot; the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a translation of the source tree. This technique resembles work on NIT using synchr"
R09-1032,J95-2003,0,0.0479436,"racted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts and ambiguities. In addition, we study the temporal event linking task on top of IE results. In this way we extend the representation of each node in the chains from an event trigger to a structured aggregated event including fine-grained information such as event types, arguments and their roles. Compared to [5, 6], we also extend the definition of “centroid” from a word"
R09-1032,P08-1030,1,0.958566,"ents of events. temporal event chain: a list of temporally-ordered events involving the same centroid entity. Our cross-document IE task is defined as follows: Input: A test set of documents Ouput: Identify a set of centroid entities, and then for each centroid entity, link and order the events centered around it on a time line. For example, Figure 1 presents a temporal event chain involving “Toefting”. As for other ACE tasks, the ACE 2005 official evaluation scorer can produce an overall score called “ACE value” for event extraction. However, most of the ACE event extraction literature (e.g. [3]; [4]) used a simpler argument-based F-measure to evaluate ACE event extraction, and we will adapt this measure to our task. 2.3 Limitations In the ACE single-document event extraction task, each event mention is extracted from a single sentence. The results are reasonably useful for hundreds of documents. However, when we apply the same system to process much larger corpora, the net result is a very large collection of events which are: (1) Unconnected. Related events (for example, “Tony Blair’s foreign trips) appear unconnected and unordered. (2) Unranked. Event mentions are presented in the"
R09-1032,N09-2053,1,0.846917,"of events. temporal event chain: a list of temporally-ordered events involving the same centroid entity. Our cross-document IE task is defined as follows: Input: A test set of documents Ouput: Identify a set of centroid entities, and then for each centroid entity, link and order the events centered around it on a time line. For example, Figure 1 presents a temporal event chain involving “Toefting”. As for other ACE tasks, the ACE 2005 official evaluation scorer can produce an overall score called “ACE value” for event extraction. However, most of the ACE event extraction literature (e.g. [3]; [4]) used a simpler argument-based F-measure to evaluate ACE event extraction, and we will adapt this measure to our task. 2.3 Limitations In the ACE single-document event extraction task, each event mention is extracted from a single sentence. The results are reasonably useful for hundreds of documents. However, when we apply the same system to process much larger corpora, the net result is a very large collection of events which are: (1) Unconnected. Related events (for example, “Tony Blair’s foreign trips) appear unconnected and unordered. (2) Unranked. Event mentions are presented in the orde"
R09-1032,D08-1029,0,0.0188386,"> into one candidate centroid if they satisfy either of the following two conditions: • identified as coreferential by single-document coreference resolution; or • in different documents, there is a namei referring to mentioni and a namej referring to mentionj (if several names, taking the maximal name in each document), and namei and namei are equal or one is a substring of the other. Using this approach we can avoid linking “Rod Stewart” and “Martha Stewart” into the same entity. In the future we intend to exploit more advanced crossdocument person name disambiguation techniques (e.g. [11], [12]) to resolve ambiguities. 5.2 Global Entity Ranking Because the candidate entities are extracted automatically, and so may be erroneous, we want to promote those arguments which are both central to the collection (high frequency) and more likely to be accurate (high confidence). We exploit global confidence metrics to reach both of these goals. The intuition is that if an entity is involved in events frequently as well as with high extraction confidence, it is more salient. Our basic underlying hypothesis is that the salience of an entity ei should be calculated by taking into consideration bo"
R09-1032,P09-2093,1,0.825347,"el Sharon] Eventj Arguments Place [Jerusalem] Subset Subsumption Complement Table 1. Cross-document Event Aggregation Examples [Test Sentence] <person>Diller</person> started his entertainment career at <entity>ABC</entity>, where he is credited with creating the ``movie of the week&apos;&apos; concept. [Sentence from Wikipedia] <person>Diller</person> was hired by <entity> ABC</entity> in <time>1966</time> and was soon placed in charge of negotiating broadcast rights to feature films. (2) Statistical Implicit Time Prediction Furthermore, we exploited a time argument prediction approach as described in [14]. We manually labeled 40 ACE05 newswire texts and trained a MaxEnt classifier to determine whether a time argument from an event mention EMi can be propagated to the other event mention EMj. The features used include the event types of EMi and EMj, whether they are located in the same sentence, if so the number of time expressions in the sentence; whether they share coreferential arguments, if so the roles of the arguments. This predictor is able to propagate time arguments between two events which indicate some precursor/consequence, subevent or causal relation (e.g. from a “Conflict-Attack”"
R09-1032,W99-0201,0,0.0482159,"n generated to maximize diversity among the event nodes in a chain and completeness for each event node. In order to reach these goals, a simple event coreference solution is not enough. We also aggregate other relation types between two events: Subset, Subsumption and Complement as shown in Table 1. Besides using cross-document name coreference to measure the similarity between a pair of arguments, we adopted some results from ACE relation extraction, e.g. using “PART-WHOLE” relations between arguments to determine whether one event subsumes the other. Earlier work on event coreference (e.g. [15]) in the MUC program was limited to several scenarios such as terrorist attacks and management succession. In our task we are targeting wider and more fine-grained event types. 170 8. Experimental Results In this section we will describe our answer-key event chain annotation and then present experimental results. 8.1 Data and Answer-key Annotation We used 106 newswire texts from ACE 2005 training corpora as our test set. Then we extracted the top 40 ranked person names as centroid entities, and manually created temporal event chains by two steps: (1) Aggregated reference event mentions; (2) Fi"
R09-1032,N07-1042,0,0.183179,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,P08-2045,0,0.0255199,"p has combined ranking and linking for cross-document IE. Hence in this section, we present related work in other areas for ranking and linking separately. Text summarization progressed from single-document to multi-document processing by centroid based sentence linking and ranking (e.g. [5], [6]). Accurate ranking techniques such as PageRank [13] have greatly enhanced information retrieval. Recently there has been heightened interest in discovering temporal event chains, especially, the shared task evaluation TempEval [9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we"
R09-1032,P08-1090,0,0.0130347,"s for ranking and linking separately. Text summarization progressed from single-document to multi-document processing by centroid based sentence linking and ranking (e.g. [5], [6]). Accurate ranking techniques such as PageRank [13] have greatly enhanced information retrieval. Recently there has been heightened interest in discovering temporal event chains, especially, the shared task evaluation TempEval [9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge fro"
R09-1032,P05-1045,0,0.0313205,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,D07-1075,0,0.0374964,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,D09-1016,0,0.0155515,"[9] involved identifying temporal relations in TimeBank [17]. For example, [18] applied supervised learning to classify temporal and causal relations simultaneously for predicates in TimeBank. [19] extracted narrative event chains based on common protagonists. Our work is also similar to the task of topic detection and tracking [20] under the condition that each ‘node’ for linking is an event extracted by IE instead of a story. Several recent studies have stressed the benefits of going beyond traditional single-document extraction and taking advantage of information redundancy. In particular, [3, 16, 21, 22, 23, 24, 25] have emphasized this potential in their work. As we present in section 6, the central idea of cross-document argument refinement can be applied to discover knowledge from background data, and thus significantly improve local decisions. In this paper we import these ideas into IE while taking into account some major differences. Following the original idea of centering [2] and the approach of centering events involving protagonists [19], we present a similar idea of detecting ‘centroid’ arguments. We operate cross-document instead of single-document, which requires us to resolve more conflicts"
R09-1032,S07-1014,0,\N,Missing
R11-1002,grishman-2010-impact,1,0.908238,"of the document to the specific scenario or event type. For scenario extraction in MUC-3/4, Riloff (1996) initiated this approach and claimed that if a corpus can be divided into documents involving a certain event type and those not involving that type, patterns can be evaluated based on their frequency in relevant and irrelevant documents. Yangarber et al. (2000) incorporated Riloff’s metric into a bootstrapping procedure. Patwardhan and Riloff (2007) presented an information extraction system that finds relevant regions of text and applies extraction patterns within those regions. Liao and Grishman (2010b) also pointed out that the pre-selection of the bootstrapping corpus (based on document topic) is quite essential to this approach. Although their approach involved bootstrapping, it gives the intuition that the event/scenario and the document topic are strongly connected. For ACE event extraction, most current systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al. 2006). However, there have been several studies using high-level information at the document level. Finkel et al. (2005) used Gibbs Figure1. Distribution of trigger probability (X axis r"
R11-1002,C10-1077,1,0.946843,"elevance of the document to the specific scenario or event type. For scenario extraction in MUC-3/4, Riloff (1996) initiated this approach and claimed that if a corpus can be divided into documents involving a certain event type and those not involving that type, patterns can be evaluated based on their frequency in relevant and irrelevant documents. Yangarber et al. (2000) incorporated Riloff’s metric into a bootstrapping procedure. Patwardhan and Riloff (2007) presented an information extraction system that finds relevant regions of text and applies extraction patterns within those regions. Liao and Grishman (2010b) also pointed out that the pre-selection of the bootstrapping corpus (based on document topic) is quite essential to this approach. Although their approach involved bootstrapping, it gives the intuition that the event/scenario and the document topic are strongly connected. For ACE event extraction, most current systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al. 2006). However, there have been several studies using high-level information at the document level. Finkel et al. (2005) used Gibbs Figure1. Distribution of trigger probability (X axis r"
R11-1002,P10-1081,1,0.910378,"elevance of the document to the specific scenario or event type. For scenario extraction in MUC-3/4, Riloff (1996) initiated this approach and claimed that if a corpus can be divided into documents involving a certain event type and those not involving that type, patterns can be evaluated based on their frequency in relevant and irrelevant documents. Yangarber et al. (2000) incorporated Riloff’s metric into a bootstrapping procedure. Patwardhan and Riloff (2007) presented an information extraction system that finds relevant regions of text and applies extraction patterns within those regions. Liao and Grishman (2010b) also pointed out that the pre-selection of the bootstrapping corpus (based on document topic) is quite essential to this approach. Although their approach involved bootstrapping, it gives the intuition that the event/scenario and the document topic are strongly connected. For ACE event extraction, most current systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al. 2006). However, there have been several studies using high-level information at the document level. Finkel et al. (2005) used Gibbs Figure1. Distribution of trigger probability (X axis r"
R11-1002,D07-1075,0,0.032224,"o the best of our knowledge, we are the first to use unsupervised topic models in event extraction. However, there are some similar approaches that consider the relevance of the document to the specific scenario or event type. For scenario extraction in MUC-3/4, Riloff (1996) initiated this approach and claimed that if a corpus can be divided into documents involving a certain event type and those not involving that type, patterns can be evaluated based on their frequency in relevant and irrelevant documents. Yangarber et al. (2000) incorporated Riloff’s metric into a bootstrapping procedure. Patwardhan and Riloff (2007) presented an information extraction system that finds relevant regions of text and applies extraction patterns within those regions. Liao and Grishman (2010b) also pointed out that the pre-selection of the bootstrapping corpus (based on document topic) is quite essential to this approach. Although their approach involved bootstrapping, it gives the intuition that the event/scenario and the document topic are strongly connected. For ACE event extraction, most current systems focus on processing one sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al. 2006). However, there have be"
R11-1002,D09-1026,0,0.075586,"Missing"
R11-1002,P07-1075,0,\N,Missing
R11-1002,P08-1030,1,\N,Missing
R11-1002,W06-0901,0,\N,Missing
R11-1002,P05-1045,0,\N,Missing
R13-1051,J96-1002,0,0.21711,"Missing"
R13-1051,N04-4028,0,0.0297943,"n engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Many approaches for confidence estimation have also been explored and implemented in other NLP research areas. There are also many previous confidence estimation studies in IE, and most of these have been in the Active Learning literature. Thompson et al. (1999) proposed a rule-based extraction method to compute confidence. Scheffer et al. (2001) utilized hidden Markov models to measure the confidence in an IE system, but they only estimated the confidence of singleton tokens. Culotta and McCallum (2004)’s work is the most relevant to our work, since they also utilized a machine learning model to estimate the confidence values for IE outputs. They estimated the confidence of both extracted fields and entire multi-field records mainly through a linear-chain Conditional Random Field (CRF) model, but their case studies are not as complicated and challenging as slot filling, since SF systems need to handle difficult crossdocument coreference resolution, sophisticated inference, and also other challenges (Min and Grishman, 2012). Furthermore, to the best of our knowledge, there is no previous work"
R13-1051,W03-0413,0,0.0360649,"et al., 2006). There is previous work in IE using probabilistic and heuristic methods to estimate confidence for extracting fields using a sequential model, but to the best of our knowledge, this work is the first probabilistic CE model for the multi-stage systems employed for the Knowledge Base Population (KBP) Slot Filling task (Section 2). 2 Related Work Confidence estimation is a generic machine learning approach for measuring confidence of a given output, and many different CE methods have been used extensively in various Natural Language Processing (NLP) fields (Gandrabur et al., 2006). Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed 396 Proceedings of Recent Advances in Natural Language Processing, pages 396–401, Hissar, Bulgaria, 7-13 September 2013. Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic"
R13-1051,P11-1115,1,0.87933,"Missing"
R13-1051,E09-1062,0,0.0137873,"anguage Processing (NLP) fields (Gandrabur et al., 2006). Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed 396 Proceedings of Recent Advances in Natural Language Processing, pages 396–401, Hissar, Bulgaria, 7-13 September 2013. Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Many approaches for confidence estimation have also been explored and implemented in other NLP research areas. There are also many previous confidence estimation studies in IE, and most of these have been in the Active Learning literature. Thompson et al. (1999) proposed a rule-based extraction method to compute confidence. Scheffer et al. (2001) utilized hidden Markov models to measure the confidence in an IE system, but they only estimated the confidence of singleton tokens. Culotta and McCallum (2004)’s work"
R13-1051,P11-1022,0,0.0323237,"work in IE using probabilistic and heuristic methods to estimate confidence for extracting fields using a sequential model, but to the best of our knowledge, this work is the first probabilistic CE model for the multi-stage systems employed for the Knowledge Base Population (KBP) Slot Filling task (Section 2). 2 Related Work Confidence estimation is a generic machine learning approach for measuring confidence of a given output, and many different CE methods have been used extensively in various Natural Language Processing (NLP) fields (Gandrabur et al., 2006). Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed 396 Proceedings of Recent Advances in Natural Language Processing, pages 396–401, Hissar, Bulgaria, 7-13 September 2013. Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Ma"
R13-1052,W06-1613,0,0.324414,"t belonging to other functions The weakness of the cited work is discussed Table 1: Annotation Scheme for Citation Function: + represents POSITIVE sentiment, NEUTRAL sentiment, and − represents negative sentiment = represents citations. Dong and Sch¨afer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a), Athar (2011), and Dong and Sch¨afer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Researchers have introduced several annotation schemes for citation analysis. The work of Teufel et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-R¨using (1977). This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or b"
R13-1052,W12-4303,0,0.0320289,"Missing"
R13-1052,W06-1312,0,0.0978912,"t belonging to other functions The weakness of the cited work is discussed Table 1: Annotation Scheme for Citation Function: + represents POSITIVE sentiment, NEUTRAL sentiment, and − represents negative sentiment = represents citations. Dong and Sch¨afer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a), Athar (2011), and Dong and Sch¨afer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Researchers have introduced several annotation schemes for citation analysis. The work of Teufel et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-R¨using (1977). This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or b"
R13-1052,N12-1073,0,0.24777,"Missing"
R13-1052,J96-2004,0,0.272417,"Missing"
R13-1052,I11-1070,0,0.0483684,"Missing"
R13-1052,N09-1066,0,0.0180753,"sis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to 2 Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002), analyzing bibliometric networks (Radev et al., 2009), summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011), generating surveys of scientific paradigms (Mohammad et al., 2009), among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. 402 Proceedings of Recent Advances in Natural Language Processing, pages 402–407, Hissar, Bulgaria, 7-13 September 2013. Citation Function Based on+ Corroboration+ Discover+ Positive+ Practical+ Significant+ Standard+ Supply+ Contrast= Co-citation= Neutral= Negative− Description A work is based on the cited work Two works corroborate each other Acknowledge the invention of a technique The cited work is successful The cited work has a practical use The cited work is important The cited work"
R13-1052,C08-1087,0,0.0202032,"ent, which has achieved good accuracy (see, e.g., (Teufel et al., 2006a)). Citation sentiment analysis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to 2 Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002), analyzing bibliometric networks (Radev et al., 2009), summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011), generating surveys of scientific paradigms (Mohammad et al., 2009), among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. 402 Proceedings of Recent Advances in Natural Language Processing, pages 402–407, Hissar, Bulgaria, 7-13 September 2013. Citation Function Based on+ Corroboration+ Discover+ Positive+ Practical+ Significant+ Standard+ Supply+ Contrast= Co-citation= Neutral= Negative− Description A work is based on the cited work Two works corroborate each other Acknowledge the invention of a technique The cited"
R13-1052,P11-3015,0,\N,Missing
R13-1052,P11-1051,0,\N,Missing
R15-1010,P05-1045,0,0.0264124,"ect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features. Most event extraction frameworks are feature-based systems. Some of the featurebased systems are based on phrase or sentence level extraction. Several recent studies use highlevel information to aid local event extraction systems. For example, (Finkel et al., 2005), (Maslennikov and Chua, 2007), (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. (Li et 2. Sports Patterns Since ACE events are mainly about commercial and security-related news, patterns related to sports sh"
R15-1010,I13-1081,1,0.897092,"Missing"
R15-1010,E12-1029,0,0.0354177,"Missing"
R15-1010,P08-1030,1,0.957992,"classifiers for event arguments and argument roles.) We can see from Table 1 that the resulting system performance is competitive with other recent system results, such as the joint beam search described in (Li et al., 2013). 4 4.3 Discussion Experiments In this section, we will introduce the evaluation dataset, compare the performance of applying pattern expansion with other state-of-the-art systems, and discuss the contribution of pattern expansion. 4.1 Data We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria, following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitute"
R15-1010,P11-1113,0,0.489201,"Missing"
R15-1010,P13-1008,0,0.111191,"tion. At test time, to classify a candidate trigger (any word which has appeared at least once as a trigger in the training corpus) the tagger finds the best match between an event pattern and the input sentence and computes an event score. This score, along with other features, serves as input to the maximum entropy model to make the final ED prediction. (This brief description omits the classifiers for event arguments and argument roles.) We can see from Table 1 that the resulting system performance is competitive with other recent system results, such as the joint beam search described in (Li et al., 2013). 4 4.3 Discussion Experiments In this section, we will introduce the evaluation dataset, compare the performance of applying pattern expansion with other state-of-the-art systems, and discuss the contribution of pattern expansion. 4.1 Data We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are u"
R15-1010,P10-1081,1,0.933065,"Missing"
R15-1010,W13-5711,0,0.0304095,"Missing"
R15-1010,P07-1075,0,0.0193811,", where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features. Most event extraction frameworks are feature-based systems. Some of the featurebased systems are based on phrase or sentence level extraction. Several recent studies use highlevel information to aid local event extraction systems. For example, (Finkel et al., 2005), (Maslennikov and Chua, 2007), (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. (Li et 2. Sports Patterns Since ACE events are mainly about commercial and security-related news, patterns related to sports should be removed. For example,"
R15-1010,D07-1075,0,0.0269549,", and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Efforts to improve event extraction performance have focused largely on either improving the pattern-matching kernel or adding new reasonable features. Most event extraction frameworks are feature-based systems. Some of the featurebased systems are based on phrase or sentence level extraction. Several recent studies use highlevel information to aid local event extraction systems. For example, (Finkel et al., 2005), (Maslennikov and Chua, 2007), (Ji and Grishman, 2008) and (Patwardhan and Riloff, 2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multi-event-type extraction systems. (Liao and Grishman, 2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. (Li et 2. Sports Patterns Since ACE events are mainly about commercial and security-related news, patterns related to sports should be removed. For example, “win a title” is one of the top 5 high-frequency dependency"
R15-1010,P05-1047,0,0.0849158,"Missing"
R15-1010,P03-1029,1,0.771002,"Missing"
R15-1010,C00-2136,1,0.548333,"Missing"
R15-1010,P03-1044,0,\N,Missing
R15-1011,P10-1081,1,0.928248,"of applying dependency regularization with other state-of-the-art systems, and discuss the contributions of these different dependency regularization rules. 4.1 3. With Nomlex Regularization, the sentence “The acquisition of Banco Zaragozano...” is detected as a TRANSFER - OWNERSHIP event, which was ignored in the original framework. This is because all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification pr"
R15-1011,P07-1075,0,0.0770946,"Missing"
R15-1011,C14-1214,0,0.011351,"(2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multievent-type extraction systems. Liao and Grishman (2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. Li et al. (2013) implements a joint model via structured prediction with crossevent features. Event extraction systems have used patterns and features based on a range of linguistic representations. For example, Miwa et al. (2014) used both a deep analysis and a dependency parse. The original NYU system for the 2005 ACE evaluation (Grishman et al., 2005) incorporated GLARF, a representation which captured both notions of transparency and verb-nominalization correspondences.4 However, assessment of the impact of individual regularizations has been limited; this prompted the investigation reported here. Contributions of different dependency regularizations Table 2 lists the system performance applying the different dependency regularization rules. The last line shows the performance with the combination of three types of"
R15-1011,D07-1075,0,0.026477,"setting, and also advances the current state-of-the-art systems. 4.2 5 Related Work Although there have been quite a few distinct designs for event extraction systems, most are loosely based on using patterns to detect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Some recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and seng Chua (2007), Ji and Grishman (2008) and Patwardhan and Riloff (2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multievent-type extraction systems. Liao and Grishman (2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. Li et al. (2013) implements a joint model via structured prediction with crossevent features. Event extraction systems have used patterns and features based on a range of linguistic representations. For example, Miwa e"
R15-1011,D11-1116,0,0.0523809,"ransparent word regularization Dependency Regularization 2.1 The ACE 2005 Event Guidelines specify a set of 33 types of events; these have been widely used for research on event extraction over the past decade. Some trigger words are unambiguous indicators of particular types of events. For example, the word murder indicates an event of type Die. However, most words have multiple senses and so may be associated with multiple types of events. Many of these cases can be disambiguated based on the semantic types of the trigger arguments: Verb Chain Regularization We use a fast dependency parser (Tratz and Hovy, 2011) that analyzes multi-word verb groups (with auxiliaries) into chains with the first word at the head of the chain. Verb Chain (vch) Regularization reverses the verb chains to place the main (final) verb at the top of the dependency parse tree. This reduces the variation in the dependency paths from trigger to arguments due to differences in tense, aspect, and modality. Here is an example sentence containing a verb chain: • fire can be either an ATTACK event (“fire a weapon”) or and END - POSITION event (“fire a person”), with the cases distinguishable by the semantic type of the direct object."
R15-1011,P05-1045,0,0.0478138,"stem with dependency regularizations can improve the performance over our baseline setting, and also advances the current state-of-the-art systems. 4.2 5 Related Work Although there have been quite a few distinct designs for event extraction systems, most are loosely based on using patterns to detect instances of events, where the patterns consist of a predicate, event trigger, and constraints on its local syntactic context. The constraints may involve specific lexical items or semantic classes. Some recent studies use high-level information to aid local event extraction systems. For example, Finkel et al. (2005), Maslennikov and seng Chua (2007), Ji and Grishman (2008) and Patwardhan and Riloff (2007) tried to use discourse, document, or cross-document information to improve information extraction. Other research extends these approaches by introducing cross-event information to enhance the performance of multievent-type extraction systems. Liao and Grishman (2010) use information about other types of events to make predictions or resolve ambiguities regarding a given event. Li et al. (2013) implements a joint model via structured prediction with crossevent features. Event extraction systems have use"
R15-1011,P08-1030,1,0.957347,"compare the performance of applying dependency regularization with other state-of-the-art systems, and discuss the contributions of these different dependency regularization rules. 4.1 3. With Nomlex Regularization, the sentence “The acquisition of Banco Zaragozano...” is detected as a TRANSFER - OWNERSHIP event, which was ignored in the original framework. This is because all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a"
R15-1011,P11-1113,0,0.187324,"amework. This is because all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1) to evaluate the overall performance. Table 1 presents the overall performance of the systems with gold-standard entity mention and type information. We can see that our system with dependency regularizations can improve the performance over our"
R15-1011,P13-1008,0,0.377677,"e all the relevant sentences in the training data use the same trigger “acquire”. Data set We used the ACE 2005 corpus as our testbed. For comparison, we used the same test set with 40 newswire articles (672 sentences) as in (Ji and Grishman, 2008; Liao and Grishman, 2010) for the experiments, and randomly selected 30 other documents (863 sentences) from different genres as the development set. The remaining 529 documents (14,840 sentences) are used for training. Regarding the correctness criteria: following previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Ji and Grishman, 2011; Li et al., 2013), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “None” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1) to evaluate the overall performance. Table 1 presents the overall performance of the systems with gold-standard entity mention and type information. We can see that our system with dependency regularizations can improve the performance over our baseline setting,"
R15-1026,P09-1113,0,0.637806,"e and Technology, Tsinghua University, Beijing, 100084, China. ‡ Proteus Group, New York University, NY, 10003, U.S.A. fanmiao.cslt.thu@gmail.com, carson529@gmail.com; me@yifanhe.org, grishman@cs.nyu.edu public text datasets such as ACE1 (GuoDong et al., 2005) and MUC2 (Zelenko et al., 2003), or look for effective approaches (Gardner et al., 2013; Lao et al., 2011) on improving the accuracy of link prediction within knowledge bases such as NELL3 (Carlson et al., 2010) and Freebase4 (Bollacker et al., 2007). Thanks to the research of distantly supervised relation extraction (Fan et al., 2014a; Mintz et al., 2009) which facilitates the manual annotation via automatically aligning with the relation mentions in free texts, NELL can not only extract triplets, i.e. hhead entity, relation, tail entityi, but also collect the texts between two entities as the evidence of relation mention. We take an example from NELL which originally records a belief: hconcept : city : caroline, concept : citylocatedinstate, concept : stateorprovince : maryland, County and State of i, where “County and State of ” is the mention between the head entity concept : city : caroline, and the tail entity concept : stateorprovince :"
R15-1026,P14-1079,1,0.913897,"Information Science and Technology, Tsinghua University, Beijing, 100084, China. ‡ Proteus Group, New York University, NY, 10003, U.S.A. fanmiao.cslt.thu@gmail.com, carson529@gmail.com; me@yifanhe.org, grishman@cs.nyu.edu public text datasets such as ACE1 (GuoDong et al., 2005) and MUC2 (Zelenko et al., 2003), or look for effective approaches (Gardner et al., 2013; Lao et al., 2011) on improving the accuracy of link prediction within knowledge bases such as NELL3 (Carlson et al., 2010) and Freebase4 (Bollacker et al., 2007). Thanks to the research of distantly supervised relation extraction (Fan et al., 2014a; Mintz et al., 2009) which facilitates the manual annotation via automatically aligning with the relation mentions in free texts, NELL can not only extract triplets, i.e. hhead entity, relation, tail entityi, but also collect the texts between two entities as the evidence of relation mention. We take an example from NELL which originally records a belief: hconcept : city : caroline, concept : citylocatedinstate, concept : stateorprovince : maryland, County and State of i, where “County and State of ” is the mention between the head entity concept : city : caroline, and the tail entity concep"
R15-1026,Y14-1039,1,0.929194,"Information Science and Technology, Tsinghua University, Beijing, 100084, China. ‡ Proteus Group, New York University, NY, 10003, U.S.A. fanmiao.cslt.thu@gmail.com, carson529@gmail.com; me@yifanhe.org, grishman@cs.nyu.edu public text datasets such as ACE1 (GuoDong et al., 2005) and MUC2 (Zelenko et al., 2003), or look for effective approaches (Gardner et al., 2013; Lao et al., 2011) on improving the accuracy of link prediction within knowledge bases such as NELL3 (Carlson et al., 2010) and Freebase4 (Bollacker et al., 2007). Thanks to the research of distantly supervised relation extraction (Fan et al., 2014a; Mintz et al., 2009) which facilitates the manual annotation via automatically aligning with the relation mentions in free texts, NELL can not only extract triplets, i.e. hhead entity, relation, tail entityi, but also collect the texts between two entities as the evidence of relation mention. We take an example from NELL which originally records a belief: hconcept : city : caroline, concept : citylocatedinstate, concept : stateorprovince : maryland, County and State of i, where “County and State of ” is the mention between the head entity concept : city : caroline, and the tail entity concep"
R15-1026,D13-1080,0,0.0205923,"work on relation extraction into two categories, i.e. text-based approaches and knowledge-based methods. Generally speaking, both of the parties seek better evidences to make more accurate predictions. The text-based community focuses on linguistic features such as the words combined with POS tags that indicate the relations, but the other side conducts relation inference depending on the local connecting patterns between entity pairs learnt from the knowledge graph which is established by beliefs. 2.1 Knowledge-based Methods • Relation prediction with graph patterns: Some canonical studies (Gardner et al., 2013; Lao et al., 2011) adopt a data-driven random walk model, which follows the paths from the head entity to the tail entity on the local graph structure to generate non-linear feature combinations to represent relations, and then uses logistic regression to select the significant features that contribute to classifying other entity pairs which also have the given relation. Text-based Approaches It is believed that the text between two recognized entities in a sentence indicate their relationships to some extent. To implement a relation extraction system guided by supervised learning, a key step"
R15-1026,P05-1053,0,0.0692114,"embedding representations: Bordes et al. (Bordes et al., 2013; ?) propose an alternative way that embedding the whole knowledge graph via learning a specific low-dimensional vector for each entity and relation, so that we just need simple vector calculation instead to predict relations. • Relation extraction with manual annotated corpora: Traditional approaches compete the performance on the public text datasets which are annotated by experts, such as ACE and MUC. They choose different features extracted from the texts, like kernel features (Zelenko et al., 2003) or semantic parser features (GuoDong et al., 2005), and there is a comprehensive survey (Sarawagi, 2008) which shows more details about this branch. Our model (JRME) benefits more from the latest and state-of-the art embedding approaches, TransE (Bordes et al., 2013) and IIKE (Fan et al., 2015a). Therefore, we re-implement them as the rival methods, and conduct extensive comparisons in the subsequent experiments. 3 • Relation extraction with distant supervision: Due to the limited scale and tedious labor caused by manual annotation, scientists explore an alternative way to automatically generate large-scale annotated corpora, named by distant"
R15-1026,D11-1049,0,0.0975434,"Missing"
W01-1511,W97-0307,0,0.129411,"NAME2 instead of PROVINCE and COUNTRY, where name roles (NAME1, NAME2) are more general than PROVINCE and COUNTRY in a subsumption hierarchy. In contrast, attempts to convert PTB into Susanne would fail because detail would be unavailable. Similarly, attempts to convert Susanne into the PTB framework would lose information. In summary, GLARF’s ability to represent varying levels of detail allows different types of treebank formats to be converted into GLARF, even if they cannot be converted into each other. Perhaps, GLARF can become a lingua franca among annotated treebanks. The Negra Corpus (Brants et al., 1997) provides PRED-ARG information for German, similar in granularity to GLARF. The most significant difference is that GLARF regularizes some phenomena which a Negra version of English would probably not, e.g., control phenomena. Another novel feature of GLARF is the ability to represent paraphrases (in the Harrisian sense) that are not entirely syntactic, e.g., nominalizations as sentences. Other schemes seem to only regularize strictly syntactic phenomena. 3 The Structure of GLARF In GLARF, each sentence is represented by a typed feature structure. As is standard, we model feature structures as"
W01-1511,P93-1014,0,0.0307285,"subject of a sentence is the head of a SBJ arc, an attribute like SINGULAR is the head of a GRAM-NUMBER arc, etc. A constituent involved in multiple surface or logical relations may be at the head of multiple arcs. For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These two roles are represented as two arcs which share the same head. This sort of structure sharing analysis originates with Relational Grammar and related frameworks (Perlmutter, 1984; Johnson and Postal, 1980) and is common in Feature Structure frameworks (LFG, HPSG, etc.). Following (Johnson et al., 1993)2 , arcs are typed. There are five different types of role labels:  Attribute roles: Gram-Number (grammatical number), Mood, Tense, Sem-Feature (semantic features like temporal/locative), etc.  Surface-only relations (prefixed with S-), e.g., the surface subject (S-SBJ) of a passive.  Logical-only Roles (prefixed with L-), e.g., the logical object (L-OBJ) of a passive.  Intermediate roles (prefixed with I-) representing neither surface, nor logical positions. In “John seemed to be kidnapped by aliens”, “John” is the surface subject of “seem”, the logical object of “kidnapped”, and the inte"
W01-1511,H94-1020,0,0.145198,"Missing"
W01-1511,P93-1004,0,0.0179952,"tc. — rather than trying to squeeze these phrases into an X-bar mold, we customized our representations to reflect their head-less properties. We believe that a framework for PRED-ARG needs to satisfy these objectives to adequately cover a corpus like PTB. We believe that GLARF, because of its uniform treatment of PRED-ARG relations, will be valuable for many applications, including question answering, information extraction, and machine translation. In particular, for MT, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (Matsumoto et al., 1993; Meyers et al., 1996). Much closer alignments will be possible using GLARF, because of its multiple levels of representation, than would be possible with surface structure alone (An example is provided at the end of Section 2). For this reason, we are currently investigating the extension of our mapping procedure to treebanks of Japanese (the Kyoto Corpus) and Spanish (the UAM Treebank (Moreno et al., 2000)). Ultimately, we intend to create a parallel trilingual treebank using a combination of automatic methods and human correction. Such a treebank would be valuable resource for corpus-traine"
W01-1511,C00-1078,1,0.88402,"Missing"
W01-1511,moreno-etal-2000-treebank,1,0.810879,"tion extraction, and machine translation. In particular, for MT, we expect it will benefit procedures which learn translation rules from syntactically analyzed parallel corpora, such as (Matsumoto et al., 1993; Meyers et al., 1996). Much closer alignments will be possible using GLARF, because of its multiple levels of representation, than would be possible with surface structure alone (An example is provided at the end of Section 2). For this reason, we are currently investigating the extension of our mapping procedure to treebanks of Japanese (the Kyoto Corpus) and Spanish (the UAM Treebank (Moreno et al., 2000)). Ultimately, we intend to create a parallel trilingual treebank using a combination of automatic methods and human correction. Such a treebank would be valuable resource for corpus-trained MT systems. The primary goal of this paper is to discuss the considerations for adding PRED-ARG information to PTB, and to report on the performance of our mapping procedure. We intend to wait until these procedures are mature before beginning annotation on a larger scale. We also describe our initial research on covering the Kyoto Corpus of Japanese with GLARF. 2 Previous Treebanks There are several corpo"
W01-1511,brants-plaehn-2000-interactive,0,0.0297827,"via different arcs. We also reanalyze certain postpositions as being complementizers (subordinators) or adverbs, thus excluding them from canonical roles. By reanalyzing this way, we arrived at two types of true stacked postpositions: nominalization and topicalization. For example, in Figure 3, the topicalized NP is at the head of two arcs, labeled S-TOP and L-COMP and the associated postpositions are analyzed as morphological case attributes. 6 Testing the Procedures To test our mapping procedures, we apply them to some PTB files and then correct the resulting representation using ANNOTATE (Brants and Plaehn, 2000), a program for annotating edgelabeled trees and DAGs, originally created for the NEGRA corpus. We chose both files that we have used extensively to tune the mapping procedures (training) and other files. We then convert the 6.1 The Test and the Results We developed our mapping procedures in two stages. We implemented some mapping procedures based on PTB manuals, related papers and actual usage of labels in PTB. After our initial implementation, we tuned the procedures based on a training set of 64 sentences from two PTB files: wsj 0003 and wsj 0051, yielding 1285 + triples. Then we tested the"
W01-1511,C96-1078,1,\N,Missing
W04-0705,W98-1118,1,0.888321,"Missing"
W04-0705,C02-1025,0,0.0486199,"t was made to select or cluster documents. 6 Scores are still computed on the 153 test documents ; the retrieved documents are excluded from the scoring. 8.4 Comparison to Cache Model Some named entity systems use a name cache, in which tokens or complete names which have been previously assigned a tag are available as features in tagging the remainder of a document. Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002). Our system, while more complex, makes use of a richer set of global features, involving the detailed structure of individual mentions, and in particular makes use of both name – name and name – nominal relations. We have compared the performance of our method (applied to single documents) with a voted cache model, which takes into account the number of times a particular name has been previously assigned each type of tag: System baseline voted cache current Precision 88.8 87.6 92.2 Recall 90.5 92.8 89.6 F 89.1 90.1 90.9 Table 9. Comparison with voted cache Compared to a simple voted cache mo"
W04-0705,C96-1079,1,0.547286,"Missing"
W04-0705,W98-1120,1,0.879101,"Missing"
W04-0705,M95-1005,0,0.239404,"Missing"
W04-0705,C02-1080,0,0.0588911,"Missing"
W04-0705,C02-1012,0,\N,Missing
W04-0705,W03-0430,0,\N,Missing
W04-0705,A97-1029,0,\N,Missing
W04-2705,J02-3001,0,0.302593,"rting a new phase in this project: the creation of an automatic annotator. Using techniques similar to those described in (Meyers et al., 1998) in combination with our work on GLARF (Meyers et al., 2001a; Meyers et al., 2001b), we expect to build a hand-coded PROPBANKER a program designed to produce a PropBank/NomBank style analysis from Penn Treebank style input. Although the PropBanker should work with input in the form of either treebank annotation or treebankbased parser output, this project only requires application to the Penn Treebank itself. While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. Depending on its accuracy, automatically produced annotation should be useful as either a preprocessor or as an error detector. We expect high precision for very simple frames, e.g., nouns like lot as in figure 10. Annotators will have the opportunity to judge whether particular automatic annotation is “good enough” to serve as a preprocessor. We hypothesize that a comparison of automatic annotation that fails this level of accuracy against the hand annotation will still be useful for detectin"
W04-2705,W98-0604,1,0.600273,"Missing"
W04-2705,W01-1511,1,0.541016,"hould not appear more than once (the stratal uniqueness condition in Relational Grammar or the theta criterion in Principles and parameters, etc.). Furthermore, it is unlikely for the first word of a sentence to be an argument unless the main predicate is nearby (within three words) or unless there is a nearby support verb. Finally, it is unlikely that there is an empty category that is an argument of a predicate noun unless the empty category is linked to some real NP.8 WRONG-POS We use procedures that are part of our systems for generating GLARF, a predicate argument framework discussed in (Meyers et al., 2001a; Meyers et al., 2001b), to detect incorrect parts of speech in the Penn Treebank. If an instance is predicted to be a part of speech other than a common noun, but it is still tagged, that instance is flagged. For example, if a word tagged as a singular common noun is the first word in a VP, it is probably tagged with the wrong part of speech. 6.4 The Results of Error Detection The processes described in the previous subsections are used to create a list of annotation instances to check along with short standardized descriptions of what was wrong, e.g., wrong-pos, non-functional (if there wer"
W04-2705,meyers-etal-2004-cross,1,0.7174,"n the corpus. We needed to know which nouns were markable and make initial approximations of the inventories of senses and arguments for each noun. Toward this end, we pooled a number of resources: COMLEX Syntax (Macleod et al., 1998a), NOMLEX (Macleod et al., 1998b) and the verb classes from (Levin, 1993). We also used string matching techniques and hand classification in combination with programs that automatically merge crucial features of these resources. The result was NOMLEX-PLUS, a NOMLEX-style dictionary, which includes the original 1000 entries in NOMLEX plus 6000 additional entries (Meyers et al., 2004). The resulting noun classes include verbal nominalizations (e.g., destruction, knowledge, believer, recipient), adjectival nominalizations (ability, bitterness), and 16 other classes such as relational (father, president) and partitive nouns (set, variety). NOMLEX-PLUS helped us break down 3 To make our examples more readable, we have replaced pointers to the corpus with the corresponding strings of words. 4 For a particular noun instance, only a subset of these arguments may appear, e.g., the ARG2 (indirect object) to Dorothy can be left out of the phrase Glinda’s gift of the slippers. the n"
W04-2705,2003.mtsummit-systems.9,0,0.0102805,"icular nominal predicates, adjunct prenominal modifiers usually behave the same way regardless of the noun with which they occur. In order to identify these lexical properties of prenominals, we created a list of all time nouns from COMLEX Syntax (ntime1 and ntime2) and we created a specialized dictionary of adjectives with adverbial properties which we call ADJADV. The list of adjective/adverb pairs in ADJADV came from two sources: (1) a list of adjectives that are morphologically linked to -ly adverbs created using some string matching techniques; and (2) adjective/adverb pairs from CATVAR (Habash and Dorr, 2003). We pruned this list to only include adjectives found in the Penn Treebank and then edited out inappropriate word pairs. We completed the dictionary by transferring portions of the COMLEX Syntax adverb entries to the corresponding adjectives. We now use ADJADV and our list of temporal nouns to evaluate NOMBANK annotation of modifiers. Each annotated left modifier is compared against our dictionaries. If a modifier is a temporal noun, it can bear the ARGM-TMP role (temporal adjunct role), e.g., the temporal noun morning can fill the ARGM-TMP slot in the morning broadcast. Most other common nou"
W04-2705,J98-2001,0,0.00495354,"eatures of Support be implied by discourse processes, but which we do not mark (as we are only handling sentence-level phenomena). For example, the words proponent and rival strongly imply that certain arguments appear in the discourse, but not necessarily in the same sentence. For example in They didn’t want the company to fall into the hands of a rival, there is an implication that the company is an ARG1 of rival, i.e., a rival should be interpreted as a rival of the company.7 The connection between a rival and the company is called a “bridging” relation (a process akin to coreference, cf. (Poesio and Vieira, 1998)) In other words, fall into the hands of does not link “rival” with the company by means of SUPPORT. The fact that a discourse relation is responsible for this connection becomes evident when you see that the link between rival and company can cross sentence boundaries, e.g., The company was losing money. This was because a rival had come up with a really clever marketing strategy. 6.2 Prenominal Adjectives and Error Detection ARGM is the annotation tag used for nonarguments, also known as adjuncts. For nouns, it was decided to only tag such types of adjuncts as are also found with verbs, e.g."
W04-2705,kingsbury-palmer-2002-treebank,0,0.0246365,"sylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource. 1 Introduction This paper introduces the NomBank project. When complete, NomBank will provide argument structure for instances of about 5000 common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add layers of annotation to the Penn Treebank II corpus. PropBank (Kingsbury et al., 2002; Kingsbury and Palmer, 2002; University of Pennsylvania, 2002), NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text. These annotation projects may be viewed as part of what we think of as an a la carte strategy for corpus-based natural language processing. The fragile and inaccurate multistage parsers of a few decades were replaced by treebank-based parsers, which had better performance, but typically provided more shallow analyses.1 As the same set of data is annotated with more and more levels of annotation, a new type of multistage proces"
W06-0206,A97-1029,0,0.0300663,"performance as corpus size increases without performance reaching an upper bound. Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing that collecting data over a very large corpus significantly improves system performance. However, (Curran, 2002) and (Curran and Osborne, 2002) claimed that the choice of statistical model is more important than relying upon large corpora. 3 4 Baseline Multi-lingual Name Tagger Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al, 1997). Then it uses best-first search to generate NBest hypotheses, and also computes the margin – the difference between the log probabilities of the top two hypotheses. This is used as a rough measure of confidence in our name tagging.1 In processing Chinese, to take advantage of name structures, we do name structure parsing using an extended HMM which includes a larger number of states (14). This new HMM can handle name prefixes and suffixes, and transliterated foreign names separately. We also augmented the HMM model with a set of post-processing rules to correct some omissions and systematic e"
W06-0206,W99-0613,0,0.0187463,"beled data, and how the size and relevance of data impact the performance. 2 Prior Work This work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Ando and Zhang, 2005) 48 Proceedings of the Workshop on Information Extraction Beyond The Document, pages 48–55, c Sydney, July 2006. 2006 Association for Computational Linguistics that all focus on reducing annotation requirements. For the specific task of named entity annotation, some researchers have emphasized the creation of taggers from minimal seed sets (Strzalkowski and Wang, 1996; Collins and Singer, 1999; Lin et al., 2003) while another line of inquiry (which we are pursuing) has sought to improve on high-performance baseline taggers (Miller et al., 2004). Banko and Brill (2001) suggested that the development of very large training corpora may be most effective for progress in empirical natural language processing. Their experiments show a logarithmic trend in performance as corpus size increases without performance reaching an upper bound. Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing t"
W06-0206,P02-1030,0,0.0103549,"on of taggers from minimal seed sets (Strzalkowski and Wang, 1996; Collins and Singer, 1999; Lin et al., 2003) while another line of inquiry (which we are pursuing) has sought to improve on high-performance baseline taggers (Miller et al., 2004). Banko and Brill (2001) suggested that the development of very large training corpora may be most effective for progress in empirical natural language processing. Their experiments show a logarithmic trend in performance as corpus size increases without performance reaching an upper bound. Recent work has replicated their work on thesaurus extraction (Curran and Moens, 2002) and is-a relation extraction (Ravichandran et al., 2004), showing that collecting data over a very large corpus significantly improves system performance. However, (Curran, 2002) and (Curran and Osborne, 2002) claimed that the choice of statistical model is more important than relying upon large corpora. 3 4 Baseline Multi-lingual Name Tagger Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al, 1997). Then it uses best-first search to generate NBest hypotheses, and also computes the margin – the difference between the log probabilities of the top t"
W06-0206,W02-2008,0,0.0372141,"Missing"
W06-0206,P01-1005,0,0.0714479,"estigated whether we can improve the system by simply using a lot of unlabeled data. By dramatically increasing the size of the corpus with unlabeled data, we did get a significant improvement compared to the baseline system. But we found that adding off-topic unlabeled data sometimes makes the performance worse. Then we tried to select relevant documents from the unlabeled data in advance, and got clear further improvements. We also obtained significant improvement by self-training (bootstrapping on the test data) without any additional unlabeled data. Therefore, in contrast to the claim in (Banko and Brill, 2001), we concluded that, for some applications, effective use of large unlabeled corpora demands good data selection measures. We propose and quantify some effective measures to select documents and sentences in this paper. The rest of this paper is structured as follows. Section 2 briefly describes the efforts made by previous researchers to use semi-supervised learning as well as the work of (Banko and Brill, 2001). Section 3 presents our baseline name tagger. Section 4 describes the motivation for our approach while Section 5 presents the details of two semi-supervised learning methods. Section"
W06-0206,N04-1043,0,\N,Missing
W06-0206,C96-2157,0,\N,Missing
W06-0206,W02-1029,0,\N,Missing
W06-0206,N04-1038,0,\N,Missing
W06-0206,P05-1051,1,\N,Missing
W06-0206,P05-1001,0,\N,Missing
W06-0206,W04-0705,1,\N,Missing
W06-3607,A97-1029,0,0.0374317,"o apply the baseline name tagger to generate N-Best multiple hypotheses for each sentence; the results from subsequent components are then exploited to re-rank these hypotheses and the new top hypothesis is output as the final result. In our name re-ranking model, each hypothesis is an NE tagging of the entire sentence. For example, “&lt;PER&gt;John&lt;/PER&gt; was born in &lt;GPE&gt;New York&lt;/GPE&gt;.” is one hypothesis for the sentence “John was born in New York”. We apply a HMM tagger to identify four named entity types: Person, GPE, Organization and Location. The HMM tagger generally follows the Nymble model (Bikel et al, 1997), and uses bestfirst search to generate N-Best hypotheses. It also computes the “margin”, which is the difference between the log probabilities of the top two hypotheses. This is used as a rough measure of confidence in the top hypothesis. A large margin indicates greater confidence that the first hypothesis is correct. The margin also determines the number of hypotheses (N) that we will store. Using cross-validation on the training data, we determine the value of N required to include the best hypothesis, as a function of the margin. We then divide the margin into ranges of values, and set a"
W06-3607,P05-1022,0,0.0910128,"Missing"
W06-3607,P05-1023,0,0.0119745,"king techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to Ralph Grishman Dept. of Computer Science grishman@cs.nyu.edu improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order to bring to"
W06-3607,P05-1051,1,0.949896,"and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to Ralph Grishman Dept. of Computer Science grishman@cs.nyu.edu improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving r"
W06-3607,P06-2055,1,0.843278,"Missing"
W06-3607,W03-0402,0,0.0199274,"w the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to Ralph Grishman Dept. of Computer Science grishman@cs.nyu.edu improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some"
W06-3607,W04-2401,0,\N,Missing
W06-3607,C02-1151,0,\N,Missing
W06-3607,C96-1079,1,\N,Missing
W06-3607,J05-1003,0,\N,Missing
W06-3607,P02-1034,0,\N,Missing
W06-3607,P02-1062,0,\N,Missing
W06-3607,P05-1024,0,\N,Missing
W06-3607,P05-1033,0,\N,Missing
W09-2809,P06-1048,0,0.0611244,"gh the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally by Hori and Furui (2004) for spoken text in Japanese, and later modified by Clarke and Lapata (2006) for English text. Their model searches for the compression with highest score according to the significance of each word, the existence of SubjectVerb-Object structures and the language model probability of the resulting word combination. The weight factors to balance the three measurements are experimentally optimized by a parallel corpus or estimated by experience. Turner and Charniak (2005) present semisupervised and unsupervised variants of the noisy channel model. They approximate the rules of compression from a non-parallel corpus (e.g. the Penn Treebank) based on probabilistic context"
W09-2809,D07-1008,0,0.0537197,"t previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally"
W09-2809,P05-1036,0,0.023337,"s have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally by Hori and Furui (2004) for spoken text in Japanese, and later modified by Clarke and Lapata (2006) for English text. Their model searches for the compression with highest score according to the significance of each word, the existence of SubjectVerb-Object structures and the language model probability of the resulting word combination"
W09-2809,W03-0501,0,0.421676,"possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally by Hori and Furui (2004) for spoken text in Japanese, and later modified by Clarke and Lapata (2006) for English text. Their model searches for the compression with highest score according to the significance of each word, the existence of SubjectVerb-Object structures and the language m"
W09-2809,N07-1023,0,0.0258587,"ly useless compressed sentence. Another major drawback is that it requires considerable linguistic skill to produce proper rules in a proper order. Previous Work Most previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English."
W09-2809,A00-1043,0,0.065826,"Missing"
W09-2809,P06-1047,1,0.903871,"n contrast to probabilistic methods, the heuristics are more likely to produce grammatical and fluent compressed sentences. We reduce the difficulty and linguistic skills required for composing heuristics by only requiring these heuristics to identify possibly removable constituents instead of selecting specific constituents for removal. The word significance helps to preserve informative constituents and overcome some POS and parsing errors. In particular, we seek to assess the event information during the compression process, according to the previous successes in event-based summarization (Li et al, 2006) and a new eventoriented 5W summarization task (Parton et al, 2009). The next section presents previous approaches to sentence compression. In section 3, we describe our system with three modules, viz. linguistically-motivated heuristics, word significance scoring and candidate compression selection. We also develop a heuristics-only approach for comparison. In section 4, we evaluate the compressions in terms of grammaticality, inforAbstract In this paper, we propose an event-based approach for Chinese sentence compression without using any training corpus. We enhance the linguistically-motiva"
W09-2809,E06-1038,0,0.0184001,"revious Work Most previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora"
W09-2809,C04-1107,0,0.0139409,"ll to produce proper rules in a proper order. Previous Work Most previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorit"
W09-2809,P09-1048,1,\N,Missing
W09-2809,D09-1087,0,\N,Missing
W10-3909,C08-1001,0,0.0213877,"extual compatibility feature does not help much for pronoun coreference: existing learningbased approaches already performed well; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of 'templates' which share an 'anchor' argument; these correspond closely to our role pairs. Association statistics of the template pairs were used to acquire verb entailments. Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations. Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model. Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pai"
W10-3909,N04-1038,0,0.783689,"words and syntactic structure. People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from"
W10-3909,P08-1090,0,0.0260235,"ll; such statistics are simply not good predictors for pronoun interpretation; data is sparse in the collected predicate-argument statistics. The role pair feature has not been studied for general, broad-domain pronoun co-reference, but it has been used for other tasks: Pekar (2006) built pairs of 'templates' which share an 'anchor' argument; these correspond closely to our role pairs. Association statistics of the template pairs were used to acquire verb entailments. Abe et al. (2008) looked for pairs appearing in specific syntactic patterns in order to acquire finer-grained event relations. Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist. They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model. Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features. They use these features to improve coreference resolution for two domain-specific cor"
W10-3909,C90-3063,0,0.898648,"re the same antecedent words and syntactic structure. People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference res"
W10-3909,N04-1037,0,0.745797,"ds for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, which is automatically extracted from a large corpus. We studied two features: the contextual compatibility feature which has been demonstrated to work at the syntactic level by previous work; and the role pair feature, which has not previously been applied to general domain pronoun co-reference. In addi"
W10-3909,N06-1025,0,0.10947,"resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, which is automatically extracted fr"
W10-3909,C04-1033,0,0.0772529,"Missing"
W10-3909,P05-1021,0,0.377401,"Missing"
W10-3909,P07-1067,0,0.0205423,"cture. People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot. In such cases, semantic constraints and preferences are required for correct coreference resolution. Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60–68, Beijing, August 2010 recent work on anaphora resolution. Dagan and Itai (1990), Bean and Riloff (2004), Yang and Su (2007), and Ponzetto and Strube (2006) all explored this task. However, this task is difficult because it requires the acquisition of a large amount of semantic information. Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference. Kehler et al. (2004) reported that such information did not produce apparent improvement in overall pronoun resolution. In this paper, we will extract semantic features from a semantic role labeling system instead of a parse tree, and explore whether pronoun coreference resolution can benefit from such knowledge, whi"
W10-3909,N06-1007,0,\N,Missing
W10-3909,W09-2423,1,\N,Missing
W11-4002,D09-1025,0,0.0366786,"Missing"
W11-4002,P04-1075,0,0.0855695,"Missing"
W11-4002,P10-2068,0,0.0143113,"sets are usually incorrectly expanded to contain elements of a more general concept. We show that fine-grained control is necessary for refining such sets and propose an algorithm which uses both positive and negative user feedback for iterative refinement. Experimental results show that it improves the quality of fine-grained sets significantly. 1 Introduction Entity set expansion is a well-studied problem with several techniques proposed (Bunescu and Mooney 2004, Etzioni et al. 2005, Wang and Cohen 2007, Sarmento et al. 2007, Pasca 2007, Pasca 2004, Pantel et al. 2009, Pantel and Lin 2002, Vickrey et al. 2010). In practice, semisupervised methods are preferred since they require only a handful of seeds and are more flexible for growing various types of entity sets. However, they usually produce noisy sets, which need to be refined (Vyas and Pantel, 2009). Finegrained sets such as National Capitals are particularly noisy. Such concepts are intrinsically hard because they’re not well represented by initial seeds. Moreover, most related instances have a limited number of features, thus making it hard to retrieve them. We examined a few sets expanded for finegrained concepts and observed that lots of e"
W11-4002,P04-1056,0,0.0110108,"Sets expanded for intended fine-grained concepts are especially noisy because these concepts are not well represented by the limited number of seeds. Such sets are usually incorrectly expanded to contain elements of a more general concept. We show that fine-grained control is necessary for refining such sets and propose an algorithm which uses both positive and negative user feedback for iterative refinement. Experimental results show that it improves the quality of fine-grained sets significantly. 1 Introduction Entity set expansion is a well-studied problem with several techniques proposed (Bunescu and Mooney 2004, Etzioni et al. 2005, Wang and Cohen 2007, Sarmento et al. 2007, Pasca 2007, Pasca 2004, Pantel et al. 2009, Pantel and Lin 2002, Vickrey et al. 2010). In practice, semisupervised methods are preferred since they require only a handful of seeds and are more flexible for growing various types of entity sets. However, they usually produce noisy sets, which need to be refined (Vyas and Pantel, 2009). Finegrained sets such as National Capitals are particularly noisy. Such concepts are intrinsically hard because they’re not well represented by initial seeds. Moreover, most related instances have a"
W11-4002,N09-1033,0,0.344271,"refinement. Experimental results show that it improves the quality of fine-grained sets significantly. 1 Introduction Entity set expansion is a well-studied problem with several techniques proposed (Bunescu and Mooney 2004, Etzioni et al. 2005, Wang and Cohen 2007, Sarmento et al. 2007, Pasca 2007, Pasca 2004, Pantel et al. 2009, Pantel and Lin 2002, Vickrey et al. 2010). In practice, semisupervised methods are preferred since they require only a handful of seeds and are more flexible for growing various types of entity sets. However, they usually produce noisy sets, which need to be refined (Vyas and Pantel, 2009). Finegrained sets such as National Capitals are particularly noisy. Such concepts are intrinsically hard because they’re not well represented by initial seeds. Moreover, most related instances have a limited number of features, thus making it hard to retrieve them. We examined a few sets expanded for finegrained concepts and observed that lots of erroneous expansions are elements of a more general concept, whose sense overlaps and subsumes the intended sense. For example, the concept National Capitals is expanded to contain Major ci2 Related work There is a large body of research on growing n"
W11-4002,C92-2082,0,0.0476166,"ined a few sets expanded for finegrained concepts and observed that lots of erroneous expansions are elements of a more general concept, whose sense overlaps and subsumes the intended sense. For example, the concept National Capitals is expanded to contain Major ci2 Related work There is a large body of research on growing named entity sets from a handful of seeds. Some are pattern-based algorithms. Sarmento et al. (2007) uses explicit patterns, e.g. “…NEa, NEb and NEc…”, to find named entities of the same class. Pasca (2004) uses the pattern &lt;[StartOfSent] X [such as|including] N [and|,|.]> (Hearst 1992) to find instances and their class labels from web logs. Some are based on distributional similarity. The distributional hypothesis states that similar terms tend to appear with similar contexts (Harris 1954). For example, Pasca (2007) extracts templates (prefixes and suffixes around seeds) from search engine query logs as features, and then ranks new instances by their similarity with the seeds in the vector space of pattern features for growing sets. Their method 2 Proceedings of the Workshop on Information Extraction and Knowledge Acquisition, pages 2–6, Hissar, Bulgaria, 16 September 2011."
W11-4002,D10-1035,0,0.0151647,"t is to find a subset of entities which are similar to the target concept, based on a certain similarity metric (Cosine, Dice, etc). The concept is usually approximated with a set of seed instances. A previous feature pruning technique (Vyas and Pantel 2009) aims at reducing semantic drift introduced by ambiguous seeds. We’re particularly interested in fine-grained classes since they’re intrinsically hard to expand because of the crude representation from the limited number of seeds. In practice, we observed, when expanding fine-grained classes, that semantic spread instead of semantic drift (McIntosh 2010) severely affects expansion quality. By semantic spread we mean a situation where an initial concept, represented by its member entities, changes in the course of entity set expansion into a broader concept which subsumes the original concept. Semantic spread is usually introduced when erroneous instances, which belong to a more general concept, are incorrectly included during the set expansion process. For example, when using Google Sets (labs.google.com/sets) to exCentroid  I I S  P |SP|   C C N N N |N| where I is an entity that is a member of seed set S or the set of user-tagged p"
W11-4002,D09-1098,0,\N,Missing
W12-3011,P11-1098,0,0.0523558,"8 phrase and USP is generally not aimed at creating an inventory of argument classes associated with particular predicates. All of this work has been done on binary relations.1 Discovering more general (n-ary) structures is more difficult because in most cases arguments will be optional, complicating the alignment process. (Shinyama and Sekine 2006), who built a system for unsupervised event template construction, addressed this problem in part through two levels of clustering – one level capturing stories about the same event, the second level capturing stories about the same type of event. (Chambers and Jurafsky 2011) clustered events to create templates for terrorist events which involve up to four distinct roles. 3 Discovery of Linguistic Structure Current researchers on unsupervised IE share some goals with the structural linguists of the early and mid 20th century, such as Bloomfield, Saussure, and especially Zellig Harris. “The goal of structural linguistics was to discover a grammar by performing a set of operations on a corpus of data” (Newmeyer 1980, p. 6). Harris in particular pushed this effort in two directions: to discover the relations between sentences and capture them in transformations; and"
W12-3011,I05-1035,0,0.0397821,"Missing"
W12-3011,W09-1201,0,0.0308086,"has been steady progress in this area over the last two decades, based on increasingly rich representations: starting with the function tags and indexing of Penn TreeBank II, which permitted regularization of passives, relatives, and infinitival subjects; PropBank, which enabled additional regularization of verbal complements (Kingsbury and Palmer 2002), and NomBank (Meyers et al. 2004) and NomLex, which support regularization between verbal and nominal constructs. These regularizations have been captured in systems such as GLARF (Meyers et al. 2009), and fostered by recent CoNLL evaluations (Hajic et al. 2009). In addition to these transformations which can be implicitly realized through predicate-argument analysis, Harris (1989, pp. 21-23) described several other classes of transformations essential to alignment. One of these is modifier movement: modifiers which may attach to entities or arguments (“In San Diego, the weather is sunny.” vs. “The weather in San Diego is sunny.”). If we had a two-level representation, with both entities and predicates, this would have to be accommodated through the alignment procedure. There will be serious obstacles to automating the full discovery process. The man"
W12-3011,J89-3006,0,0.338601,"function tags and indexing of Penn TreeBank II, which permitted regularization of passives, relatives, and infinitival subjects; PropBank, which enabled additional regularization of verbal complements (Kingsbury and Palmer 2002), and NomBank (Meyers et al. 2004) and NomLex, which support regularization between verbal and nominal constructs. These regularizations have been captured in systems such as GLARF (Meyers et al. 2009), and fostered by recent CoNLL evaluations (Hajic et al. 2009). In addition to these transformations which can be implicitly realized through predicate-argument analysis, Harris (1989, pp. 21-23) described several other classes of transformations essential to alignment. One of these is modifier movement: modifiers which may attach to entities or arguments (“In San Diego, the weather is sunny.” vs. “The weather in San Diego is sunny.”). If we had a two-level representation, with both entities and predicates, this would have to be accommodated through the alignment procedure. There will be serious obstacles to automating the full discovery process. The manual ‘mechanical’ process is no doubt not as mechanical as Harris would have us believe. Knowledge of the meaning of indiv"
W12-3011,P04-1053,1,0.793997,"Missing"
W12-3011,kingsbury-palmer-2002-treebank,0,0.0573075,"systems, but it will be essential in the long term to get adequate cluster recall – in other words, to unify different relation phrases representing the same semantic relation. Its importance will increase when we move from binary to n-ary structures. Fortunately there has been steady progress in this area over the last two decades, based on increasingly rich representations: starting with the function tags and indexing of Penn TreeBank II, which permitted regularization of passives, relatives, and infinitival subjects; PropBank, which enabled additional regularization of verbal complements (Kingsbury and Palmer 2002), and NomBank (Meyers et al. 2004) and NomLex, which support regularization between verbal and nominal constructs. These regularizations have been captured in systems such as GLARF (Meyers et al. 2009), and fostered by recent CoNLL evaluations (Hajic et al. 2009). In addition to these transformations which can be implicitly realized through predicate-argument analysis, Harris (1989, pp. 21-23) described several other classes of transformations essential to alignment. One of these is modifier movement: modifiers which may attach to entities or arguments (“In San Diego, the weather is sunny.” vs"
W12-3011,meyers-etal-2004-annotating,1,0.707667,"long term to get adequate cluster recall – in other words, to unify different relation phrases representing the same semantic relation. Its importance will increase when we move from binary to n-ary structures. Fortunately there has been steady progress in this area over the last two decades, based on increasingly rich representations: starting with the function tags and indexing of Penn TreeBank II, which permitted regularization of passives, relatives, and infinitival subjects; PropBank, which enabled additional regularization of verbal complements (Kingsbury and Palmer 2002), and NomBank (Meyers et al. 2004) and NomLex, which support regularization between verbal and nominal constructs. These regularizations have been captured in systems such as GLARF (Meyers et al. 2009), and fostered by recent CoNLL evaluations (Hajic et al. 2009). In addition to these transformations which can be implicitly realized through predicate-argument analysis, Harris (1989, pp. 21-23) described several other classes of transformations essential to alignment. One of these is modifier movement: modifiers which may attach to entities or arguments (“In San Diego, the weather is sunny.” vs. “The weather in San Diego is sun"
W12-3011,W09-3019,0,0.0167315,"hen we move from binary to n-ary structures. Fortunately there has been steady progress in this area over the last two decades, based on increasingly rich representations: starting with the function tags and indexing of Penn TreeBank II, which permitted regularization of passives, relatives, and infinitival subjects; PropBank, which enabled additional regularization of verbal complements (Kingsbury and Palmer 2002), and NomBank (Meyers et al. 2004) and NomLex, which support regularization between verbal and nominal constructs. These regularizations have been captured in systems such as GLARF (Meyers et al. 2009), and fostered by recent CoNLL evaluations (Hajic et al. 2009). In addition to these transformations which can be implicitly realized through predicate-argument analysis, Harris (1989, pp. 21-23) described several other classes of transformations essential to alignment. One of these is modifier movement: modifiers which may attach to entities or arguments (“In San Diego, the weather is sunny.” vs. “The weather in San Diego is sunny.”). If we had a two-level representation, with both entities and predicates, this would have to be accommodated through the alignment procedure. There will be serio"
W12-3011,D12-1094,1,0.865075,"ses. SNE (Kok and Domingos 2008) used entity similarities computed from the set of triples to construct such entity classes concurrently with the construction of relation sets. (Yao et al. 2011) learned fine-grained argument classes based on a predefined set of coarse-grained argument types. Resolver (Yates and Etzioni 2007) identified coreferential names while building relation classes. Moving to Web-scale discovery also magnified the problem of relation-phrase polysemy, where a phrase has different senses which should be clustered with different sets of triples. This was addressed in WEBRE (Min et al. 2012), which clusters generalized triples (consisting of a relation phrase and two argument classes). WEBRE also uses a larger range of semantic resources to form the argument classes, including hyponymy relations, coordination patterns, and HTML structures. Most of this work has been applied in the news domain or on Web documents, but there has also been some work on technical domains such as medical records (Rink and Harabagiu 2011). Closely related to the work on unsupervised IE is the work on collecting paraphrase and inference rules from corpora (e.g., DIRT (Lin and Pantel 2001)) and some of t"
W12-3011,D09-1001,0,0.202945,"ces to form the argument classes, including hyponymy relations, coordination patterns, and HTML structures. Most of this work has been applied in the news domain or on Web documents, but there has also been some work on technical domains such as medical records (Rink and Harabagiu 2011). Closely related to the work on unsupervised IE is the work on collecting paraphrase and inference rules from corpora (e.g., DIRT (Lin and Pantel 2001)) and some of the work on unsupervised semantic parsing (USP), which maps text or syntactically-analyzed text into logical forms. The predicate clusters of USP (Poon and Domingos 2009) capture both syntactic and semantic paraphrases of predicates. However, the work on para58 phrase and USP is generally not aimed at creating an inventory of argument classes associated with particular predicates. All of this work has been done on binary relations.1 Discovering more general (n-ary) structures is more difficult because in most cases arguments will be optional, complicating the alignment process. (Shinyama and Sekine 2006), who built a system for unsupervised event template construction, addressed this problem in part through two levels of clustering – one level capturing storie"
W12-3011,D11-1048,0,0.0125516,"magnified the problem of relation-phrase polysemy, where a phrase has different senses which should be clustered with different sets of triples. This was addressed in WEBRE (Min et al. 2012), which clusters generalized triples (consisting of a relation phrase and two argument classes). WEBRE also uses a larger range of semantic resources to form the argument classes, including hyponymy relations, coordination patterns, and HTML structures. Most of this work has been applied in the news domain or on Web documents, but there has also been some work on technical domains such as medical records (Rink and Harabagiu 2011). Closely related to the work on unsupervised IE is the work on collecting paraphrase and inference rules from corpora (e.g., DIRT (Lin and Pantel 2001)) and some of the work on unsupervised semantic parsing (USP), which maps text or syntactically-analyzed text into logical forms. The predicate clusters of USP (Poon and Domingos 2009) capture both syntactic and semantic paraphrases of predicates. However, the work on para58 phrase and USP is generally not aimed at creating an inventory of argument classes associated with particular predicates. All of this work has been done on binary relations"
W12-3011,N06-1039,0,0.0244526,") and some of the work on unsupervised semantic parsing (USP), which maps text or syntactically-analyzed text into logical forms. The predicate clusters of USP (Poon and Domingos 2009) capture both syntactic and semantic paraphrases of predicates. However, the work on para58 phrase and USP is generally not aimed at creating an inventory of argument classes associated with particular predicates. All of this work has been done on binary relations.1 Discovering more general (n-ary) structures is more difficult because in most cases arguments will be optional, complicating the alignment process. (Shinyama and Sekine 2006), who built a system for unsupervised event template construction, addressed this problem in part through two levels of clustering – one level capturing stories about the same event, the second level capturing stories about the same type of event. (Chambers and Jurafsky 2011) clustered events to create templates for terrorist events which involve up to four distinct roles. 3 Discovery of Linguistic Structure Current researchers on unsupervised IE share some goals with the structural linguists of the early and mid 20th century, such as Bloomfield, Saussure, and especially Zellig Harris. “The go"
W12-3011,D11-1135,0,0.0144347,"g strategy based on a simple similarity metric between 57 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 57–61, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics relation instances (treating relation phrases as bags of words). Subsequent research has expanded this research along several dimensions. (Zhang et al. 2005) introduced a more elaborate similarity metric based on similarity of relation phrase constituent structure. However, this direction was not generally pursued; (Yao et al. 2011) used the dependency path between arguments as a feature in their generative model, but most URE systems have treated relation phrases as unanalyzed entities or bags of words. Several researchers have extended the scope from news reports to the Web, using in particular robust triples extractors such as TextRunner (Banko et al. 2007) and ReVerb (Fader et al. 2011), which capture relation phrases involving verbal predicates. Moving to a nearly unrestricted domain has meant that predefined argument classes would no longer suffice, leading to systems which constructed both entity classes and relat"
W12-3011,I05-1034,0,0.0133093,"3), which used a corpus of news stories and identified the most common relations between people and companies. They relied on standard predefined named entity tags and a clustering strategy based on a simple similarity metric between 57 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 57–61, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics relation instances (treating relation phrases as bags of words). Subsequent research has expanded this research along several dimensions. (Zhang et al. 2005) introduced a more elaborate similarity metric based on similarity of relation phrase constituent structure. However, this direction was not generally pursued; (Yao et al. 2011) used the dependency path between arguments as a feature in their generative model, but most URE systems have treated relation phrases as unanalyzed entities or bags of words. Several researchers have extended the scope from news reports to the Web, using in particular robust triples extractors such as TextRunner (Banko et al. 2007) and ReVerb (Fader et al. 2011), which capture relation phrases involving verbal predicat"
W12-3011,N07-1016,0,\N,Missing
W13-1103,J93-1003,0,0.217259,"another one with a flexible number (vary from 1 to 4) of tweets. Both ↵ and are set to 0.1 in our implementation. All parameters are set experimentally over a small development dataset consisting of 10 events in Twitter data of September 2012. EventRank−Flexible 0 a Twitter specific name entity tagger1 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Tweets published during the whole period are aggregated together to find top events that happen on each calendar day. We applied the G2 test for statistical significance (Dunning, 1993) to rank the event clusters, considering the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. We randomly picked the events of one day for human evaluation, that is the day of January 16, 2013 with 38 events and an average of 465 tweets per event cluster. 4 5 compactness completeness overall EventRank−Flexible EventRank−Fixed SumBasic Figure 5: human judgments evaluating tweet summarization systems Event System EventRank (Flexible) Google 1/16/2013 SumBasic EventRank (Flexible) Instagram 1/16/2013 SumBa"
W13-1103,P11-2008,0,0.0183447,"Missing"
W13-1103,P06-1047,1,0.898837,"(e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explici"
W13-1103,W11-0709,0,0.423753,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P11-1037,0,0.32789,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P12-3003,0,0.028684,"generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects of the initial event to be summarized to create comprehen20 Proceedings of the W"
W13-1103,P00-1010,0,0.136012,"epresent the relationship between them. We then rank and partition the events using PageRank-like algorithms, and create summaries of variable length for different topics. 3.1 Event Extraction from Tweets As a first step towards summarizing popular events discussed on Twitter, we need a way to identify events from Tweets. We utilize several natural language processing tools that specially developed for noisy text to extract text phrases that bear essential event information, including named entities (Ritter et al., 2011), event-referring phrases (Ritter et al., 2012) and temporal expressions (Mani and Wilson, 2000). Both the named entity and event taggers utilize Conditional Random Fields models (Lafferty, 2001) trained on annotated data, while the temporal expression resolver uses a mix of hand-crafted and machine-learned rules. Example event information extracted from Tweets are presented in Table 2. The self-contained nature of tweets allows efficient extraction of event information without deep analysis (e.g. co-reference resolution). On the other hand, individual tweets are also very terse, often lacking sufficient context to access the importance of events. It is crucial to exploit the highly redu"
W13-1103,radev-etal-2004-mead,0,0.0270509,"event cluster with a single but complex focus 25 4.2 Figure 4: Event graph of ’West Ham - 1/16/2013’, an example of event cluster with a single focus Baseline SumBasic (Vanderwende et al., 2007) is a simple and effective summarization approach based on term frequency, which we use as our baseline. It uses word probabilities with an update function to avoid redundancy to select sentences or posts in a social media setting. It is shown to outperform three other well-known multi-document summarization methods, namely LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004) and MEAD (Radev et al., 2004) on tweets in (Inouye and Kalita, 2011), possibly because that the relationship between tweets is much simpler than between sentences in news articles and can be well captured by simple frequency methods. The improvement over the LexRank model on tweets is gained by considering the number of retweets and influential users is another side-proof (Wei et al., 2012) of the effectiveness of frequency. Annotator 1 1 https://github.com/aritter/twitter_nlp 26 3 2 1 0 EventRank−Fixed SumBasic Annotator 2 2 3 4 5 compactness completeness overall 1 For each cluster, our systems produce two versions of su"
W13-1103,D11-1141,1,0.233403,"ted advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects"
W13-1103,C12-1047,0,0.0630086,"quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to ext"
W13-1103,W04-3252,0,\N,Missing
W13-2515,N03-1003,0,0.0737541,"This paper presents the first investigation into automatically collecting a large paraphrase corpus of tweets, which can be used for building paraphrase systems adapted to Twitter using techniques from statistical machine translation (SMT). We show experimental results demonstrating the benefits of an in-domain parallel corpus when paraphrasing tweets. In addition, our paraphrase models can be applied to the task of normalizing noisy text where we show improvements over the state-of-the-art. Relevant previous work has extracted sentencelevel paraphrases from news corpora (Dolan et al., 2004; Barzilay and Lee, 2003; Quirk et al., 2004). Paraphrases gathered from noisy usergenerated text on Twitter have unique characteristics which make this comparable corpus a valuable new resource for mining sentence-level paraphrases. Twitter also has much less context than news articles and much more diverse content, thus posing new challenges to control the noise in mining paraphrases while retaining the desired superficial dissimilarity. We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a"
W13-2515,2011.iwslt-evaluation.18,0,0.00622054,"Missing"
W13-2515,P11-1020,0,0.229551,"ine at https://github.com/cocoxu/ twitterparaphrase/ 121 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121–128, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics we apply a typical phrase-based statistical MT pipeline, performing word alignment on the parallel data using GIZA++ (Och and Ney, 2003), then extracting phrase pairs and performing decoding uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task"
W13-2515,C04-1051,0,0.644793,"Missing"
W13-2515,J10-3003,0,0.141397,"ty of this new resource on the task of paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art paraphrase and normalization systems 1 . 1 Introduction Social media services provide a massive amount of valuable information and demand NLP tools specifically developed to accommodate their noisy style. So far not much success has been reported on a key NLP technology on social media data: paraphrasing. Paraphrases are alternative ways to express the same meaning in the same language and commonly employed to improve the performance of many other NLP applications (Madnani and Dorr, 2010). In the case of Twitter, Petrovi´c et al. (2012) showed improvements on first story detection by using paraphrases extracted from WordNet. Learning paraphrases from tweets could be especially beneficial. First, the high level of information redundancy in Twitter provides a good opportunity to collect many different expressions. Second, tweets contain many kinds of paraphrases not available elsewhere including typos, abbreviations, ungrammatical expressions and slang, 2 Related Work There are several key strands of related work, including previous work on gathering parallel monolingual text fr"
W13-2515,I08-1059,0,0.015274,"Missing"
W13-2515,P00-1010,0,0.0216211,"se pair in our system, in addition to the four basic components (translation model, distortion model, language model and word penalty) in SMT. Paraphrasing Tweets 5.1.1 Data Our paraphrase dataset is distilled from a large corpus of tweets gathered over a one-year period spanning November 2011 to October 2012 using the Twitter Streaming API. Following Ritter et al. (2012), we grouped together all tweets which mention the same named entity (recognized using a Twitter specific name entity tagger3 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Then we applied a statistical significance test (the G test) to rank the events, which considers the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. Altogether we collected more than 3 million tweets from the 50 top events of each day according to the p-value from the statistical test, with an average of 229 tweets per event cluster. Each of these tweets was passed through a Twitter tokenizer4 and a simple sentence splitter, which also removes emoticons, URLs and most of the hashtags and usernames."
W13-2515,W11-2210,0,0.0157816,"ecause Twitter contains both normal and noisy language, with appropriate tuning, our models have the capability to translate between these two styles, e.g. paraphrasing into noisy style or normalizing into standard language. Here we demonstrate its capability to normalize tweets at the sentence-level. 5.2.1 Baselines Much effort has been devoted recently for developing normalization dictionaries for Microblogs. One of the most competitive dictionaries available today is HB-dict+GHM-dict+S-dict used by Han et al. (2012), which combines a manuallyconstructed Internet slang dictionary , a small (Gouws et al., 2011) and a large automaticallyTable 4: Example paraphrases of a given sentence “who want to get a beer” 126 derived dictionary based on distributional and string similarity. We evaluate two baselines using this large dictionary consisting of 41181 words; following Han et. al. (2012), one is a simple dictionary look up. The other baseline uses the machinery of statistical machine translation using this dictionary as a phrase table in combination with Twitter and NYT language models. 5.2.2 No-Change SMT+TwitterLM SMT+TwitterNYTLM Dictionary Dicionary+TwitterNYTLM SMT+Dictionary+TwitterNYTLM System D"
W13-2515,moore-2002-fast,0,0.019796,"and less agreement. 123 In addition, because no previous work has evaluated these metrics in the context of noisy Twitter data, we perform a human evaluation in which annotators are asked to choose which system generates the best paraphrase. Finally we evaluate our phrase-based normalization system against a state-of-the-art word-based normalizer developed for Twitter (Han et al., 2012). 5.1 velopment data and the exact configuration are released together with the phrase table for system replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure ever"
W13-2515,P11-1038,0,0.387701,"ng uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent"
W13-2515,W04-3243,0,0.0749033,"tem replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure every phrase has an option to remain unchanged during paraphrasing and normalization. Without these noise reduction steps, our system will produce paraphrases with serious errors (e.g. change a person’s last name) for 100 out of 200 test tweets in the evaluation in §5.1.5. At the same time, it is also important to promote lexical dissimilarity in the paraphrase task. Following Ritter et. al. (2011) we add a lexical similarity penalty to each phrase pair in our system, in addition to the fo"
W13-2515,D12-1039,0,0.361981,"al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying"
W13-2515,J03-1002,0,0.00357666,"d Work There are several key strands of related work, including previous work on gathering parallel monolingual text from topically clustered news articles, normalizing noisy Twitter text using word-based 1 Our Twitter paraphrase models are available online at https://github.com/cocoxu/ twitterparaphrase/ 121 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121–128, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics we apply a typical phrase-based statistical MT pipeline, performing word alignment on the parallel data using GIZA++ (Och and Ney, 2003), then extracting phrase pairs and performing decoding uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related"
W13-2515,P02-1040,0,0.104655,"cal phrase-based statistical MT pipeline on the parallel data, which uses GIZA++ for word alignment and Moses for extracting phrase pairs, training and decoding. We use a language model trained on the 3 million collected tweets in the decoding process. The parameters are tuned over de5.1.2 Evaluation Details The beauty of lexical similarity penalty is that it gives control over the degree of paraphrasing by adjusting its weight versus the other components. Thus we can plot a BLEU-PINC curve to express the tradeoff between semantic adequacy and lexical dissimilarity with the input, where BLUE (Papineni et al., 2002) and PINC (Chen and Dolan, 2011) are previously proposed automatic evaluation metrics to measure respectively the two criteria of paraphrase quality. To compute these automatic evaluation metrics, we manually prepared a dataset of gold paraphrases by tracking the trending topics on Twitter5 and gathering groups of paraphrases in November 2012. In total 20 sets of sentences were collected and each set contains 5 different sentences that express the same meaning. Each sentence is used 3 https://github.com/aritter/twitter_ nlp 4 https://github.com/brendano/ tweetmotif 5 https://support.twitter.co"
W13-2515,W12-3153,0,0.0127172,"ses tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain paraphrase systems to improve performance at first story detection in Twitter (Petrovi´c et al., 2012). By building better paraphrase models adapted to Twitter, it should be possible to improve performance at such tasks, which benefit from paraphrasing Tweets. 3 3.1 Extracting Events from Tweets As a first step towards extracting paraphrases from popular events discussed on Twitter, we need a way to identify Tweets which mentio"
W13-2515,N12-1034,0,0.367404,"Missing"
W13-2515,W04-3219,0,0.0109492,"first investigation into automatically collecting a large paraphrase corpus of tweets, which can be used for building paraphrase systems adapted to Twitter using techniques from statistical machine translation (SMT). We show experimental results demonstrating the benefits of an in-domain parallel corpus when paraphrasing tweets. In addition, our paraphrase models can be applied to the task of normalizing noisy text where we show improvements over the state-of-the-art. Relevant previous work has extracted sentencelevel paraphrases from news corpora (Dolan et al., 2004; Barzilay and Lee, 2003; Quirk et al., 2004). Paraphrases gathered from noisy usergenerated text on Twitter have unique characteristics which make this comparable corpus a valuable new resource for mining sentence-level paraphrases. Twitter also has much less context than news articles and much more diverse content, thus posing new challenges to control the noise in mining paraphrases while retaining the desired superficial dissimilarity. We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a comparable corpus of"
W13-2515,D07-1103,0,0.0134095,"ormalizer developed for Twitter (Han et al., 2012). 5.1 velopment data and the exact configuration are released together with the phrase table for system replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure every phrase has an option to remain unchanged during paraphrasing and normalization. Without these noise reduction steps, our system will produce paraphrases with serious errors (e.g. change a person’s last name) for 100 out of 200 test tweets in the evaluation in §5.1.5. At the same time, it is also important to promote lexical dissimilarity in the"
W13-2515,D11-1054,1,0.557611,"Missing"
W13-2515,D08-1027,0,0.0184776,"Missing"
W13-2515,P13-1018,0,0.0118995,"much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain paraphrase systems to improve performance at first story detection in Twitter (Petrovi´c et al., 2012). By building better paraphrase models adapted to Twitter, it should be possible to improve performance at such tasks, which benefit from paraphrasing Tweets. 3 3.1 Extracting Events from Tweets As a first step towards extracting paraphrases from popular events discussed on Twitter, we need a way to identify Tweets which mention the same event. To"
W13-2515,P12-1109,0,0.0224913,"s, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain parap"
W13-2515,P07-2045,0,\N,Missing
W14-6002,W10-1833,0,0.0163904,", 2003) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology"
W14-6002,W07-1008,0,0.0273224,"cquemin and Bourigault, 2003) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we cal"
W14-6002,E06-1051,0,0.0371738,"ng topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a current biology article, but will not include potato, a non-technical"
W14-6002,C92-2082,0,0.172001,"teA1 , also known as CH3-(CH2)20COOAgA2 ”, the chemical name establishes that this substance is a salt, whereas the formula provides the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person, organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manufacturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation. Examples include the following: “EagleA1 ’s minimum essential mediaA2 and DOPGA2 was obtained from Avanti Polar LipidsA1 ”. (3) EXEMPLIFY, an IS-A relation (Hearst, 1992) between terms so that ARG1 is an instance of ARG2, e.g., “CytokinesA2 , for instance interferonA1 ”; and “proteinsA2 such as insulinA1 ”; (4) CONTRAST relations, e.g., “necrotrophic effector systemA1 that is an exciting contrast to the biotrophic effector modelsA2 ”; (5) BETTER THAN relations, e.g., “Bayesian networksA1 hold a considerable advantage over pairwise association testsA2 ”; and (6) SIGNIFICANT relations, e.g., “Anaerobic SBsA2 are an emerging area of research and development” (ARG1, the author of the article, is implicit). These relations are applicable to most technical genres. 7"
W14-6002,D13-1073,0,0.127817,"logy topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al., 2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast to this previous work, we have built a system that extracts a larger set of terminology, which we call jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a current biology article, but will not include potato, a non-technical word that could be a valid topic-term. We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations. As we show, jargon-terminology closely matches the notional (e.g., Webste"
W14-6002,meyers-etal-2004-cross,1,0.580909,"ules out tokens beginning with numbers that include letters; tokens including plus signs, ampersands, subscripts, superscripts; tokens containing no alphanumeric characters at all, etc. 6. J must not contain a word that is a member of a list of common patent section headings. Secondly, a jargon-term J must satisfy at least one of the following additional conditions: 1. J = highly ranked topic-term or a substring of J is a highly ranked topic-term. 2. J contains at least one O-NOUN. 3. J consists of at least 4 words, at least 3 of which are either nominalizations (C-NOUNs found in NOMLEX-PLUS (Meyers et al., 2004; Meyers, 2007)) or TECH-ADJs. 4. J = nominalization at least 11 characters long. 5. J = multi-word ending in a common noun and containing a nominalization. A final stage aims to distinguish named entities from jargon-terms. It turns out that named entities, like jargon terms, include many out of vocabulary words. Thus we look for NEs among those PJs that remain after stage 3 and contain capitalized words (a single capital letter followed by lowercase letters). These NE filters are based on manually collected lists of named entities and nationality adjectives, as well as common NE endings. Dic"
W14-6002,meyers-etal-2014-annotating,1,0.834118,"also inaccurate at determining the extent of terms. Baseline 2 restricts the noun groups from this same chunker to those with O-NOUN heads. This improves the precision at a high cost to recall. Similarly, we first ran our system without filtering the potential jargon-terms, and then we ran the full system. Clearly our more complex strategy performs better than these baselines and the linguistic filters increase precision more than they reduce recall, resulting in higher F-measures (though low-precision high-recall output may be better for some applications). 17 6 Relations with Jargon-Terms (Meyers et al., 2014) describes the annotation of 200 PubMed articles from and 26 patents with several relations, as well as a system for automatically extracting relations. It turned out that the automatic system depended on the creation of a jargon-term extraction system and thus that work was the major motivating factor for the research described here. Choosing topic-terms as potential arguments would have resulted in low recall. In contrast, allowing any noun-group to be an argument would have lowered precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of relati"
W14-6002,J04-2002,0,0.0345046,"eativecommons.org/licenses/by/4.0/ 11 Proceedings of SADAATL 2014, pages 11–20, Dublin, Ireland, August 24, 2014. This paper describes a system for extracting jargon-terms in technical documents (patents and journal articles); the evaluation of this system using manually annotated documents; and a set of information extraction (IE) relations which take jargon-terms as arguments. We incorporate previous work in terminology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers) (Justeson and Katz, 1995); 1 and we use both topic-term extraction techniques (Navigli and Velardi, 2004) and relation-based extraction techniques (Jin et al., 2013) in components of our system. Rather than looking at the distribution of noun groups as a whole for determining term-hood, we refine the classes used by the noun group chunker itself, placing limitations on the candidate noun groups proposed and then filtering the output by setting thresholds on the number and quality of the “jargon-like” components of the phrase. The resulting system admits not only topic-terms, but also other non-topic instances of terminology. Using the more inclusive set of jargon-terms (rather than just topic-ter"
W14-6002,W95-0107,0,0.276741,"nd thus provide a smaller search space). In the second stage, potential jargon-term (PJs) are generated by processing tokens from left to right and classifying them using a finite state machine (FSM). The third stage filters the PJs generated with a set of manually constructed constraints, yielding a set of jargon-terms. A final filter (stage 4) identifies named entities and separates them out from the true jargon-terms: it turns out that many named entities have similar phrase-internal properties as jargon-terms. The FSM (that generates PJs) in the second stage includes the following states (Ramshaw and Marcus, 1995): START (S) (marking the beginning of a segment), Begin Term (B-T), Inside Term (I-T), End Term (E-T), and Other (O). A PJ is a sequence consisting of: (a) a single E-T; or (b) exactly one B-T, followed by zero or more instances of I-T, followed by zero or one instances of E-T. Each transition to a new state is conditioned on: (a) the (extended) POS tag of the current word; (b) the extended POS tag of the previous word; and (c) the previous state. The extended POSs are derived from the output of a Penn-Treebank-based POS tagger and refinements based on machine readable dictionaries, including"
W14-6002,W03-1805,0,0.0131788,"a wide variety of genres (unlike those in (BioCreAtIvE, 2006)) – a genre-neutral definition of terminology makes this possible. For example, the CONTRAST relation between the two bold face terms in necrotrophic effector systemA1 that is an exciting contrast to the biotrophic effector modelsA2 . would be applicable in most academic genres. Our jargon-terms also contrast with the tactic of filling terminology slots in relations with any noun-group (Justeson and Katz, 1995), as such a strategy overgenerates, lowering precision. 2 Topic-term Extraction Topic-term extractors (Velardi et al., 2001; Tomokiyo and Hurst, 2003) collect candidate terms (N-grams, noun groups, words) that are more representative of a foreground corpus (documents about a specific topic) than they are of a background corpus (documents about a wide range of topics), using statistical F requency measures such as InverseT erm Document F requency (TFIDF), or a variation thereof. Due to the metrics used and cutoffs assumed, the list of terms selected is usually no more than a few hundred distinct terms, even for a large set of foreground documents and tend to be especially salient to that topic. The terms can be phrases that lay people would"
W14-6002,W01-1005,0,0.31008,"terms and usages appropriate to a particular field, subject, science, or art. Systems for automatically extracting instances of terminology (terms) usually assume narrow operational definitions that are compatible with particular tasks. Terminology, in the context of Information Retrieval (IR) (Jacquemin and Bourigault, 2003) refers to keyword search terms (microarray, potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing topics of documents that contain them. These same terms are also used for creating domain-specific thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names, biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al., 2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may not be termin"
W15-1506,W06-1615,0,0.120351,"often performed by existing natural language processing (NLP) modules. 39 Proceedings of NAACL-HLT 2015, pages 39–48, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics While these methods allow the RE systems to inherit the knowledge discovered by the NLP community for the pre-processing tasks, they might be subject to the error propagation introduced by the imperfect quality of the supervised NLP toolkits. For instance, all the tasks mentioned in the pipeline above are known to suffer from a performance loss when they are applied to out-of-domain data (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), causing the collapse of the RE systems based on them. In this paper, we target an independent RE system that both avoids complicated feature engineering and minimizes the reliance on the supervised NLP modules for features, potentially alleviating the error propagation and advancing our performance in this area. To be concrete, our relation extraction system is provided only with raw sentences marked with the positions of the two entities of interest1 . The only elements we can derive from this structure are the words, the n-grams and their positions"
W15-1506,H05-1091,0,0.53247,"aking relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing, often performed by existin"
W15-1506,C10-1018,0,0.13533,"on, on the other hand, often comes with a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipel"
W15-1506,P04-1054,0,0.0466908,"es far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing,"
W15-1506,P07-1033,0,0.0341814,"Missing"
W15-1506,N07-1015,0,0.0400894,"ass. Relation extraction, on the other hand, often comes with a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The ling"
W15-1506,P11-1055,0,0.0356559,"in Section 5. 2 Related Work As our present work focuses on the supervised framework for relation extraction, we concentrate on the supervised systems in this section. Besides the supervised systems (either feature-based or kernelbased) mentioned above, some recent systems have employed the distant supervision (DS) approach for relation extraction. This approach is essentially similar to the traditional systems in representing relation mentions but attempts to generate training data automatically by leveraging large knowledge bases of facts and corpus (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Regarding neural networks, their first application to NLP is language modeling which has been useful to learn distributed representations (embeddings) for words (Bengio et al., 2001; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mnih and Hinton, 2009; Turian et al., 2010; Mikolov et al., 2013). These word embeddings have opened a new direction for many other NLP tasks grounded on neural networks. Some of them are mentioned above. Other than that, a class of recursive neural networks (RNNs) and neural tensor networks are proposed for paraphrase detection (Socher e"
W15-1506,P14-1062,0,0.202045,"elements we can derive from this structure are the words, the n-grams and their positions in the sentences, suggesting a paradigm in which relation mentions are represented by features that depend on these elements. Eventually, word embeddings that are capable of capturing latent semantic and syntactic properties of words (Bengio et al., 2001; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mnih and Hinton, 2009; Turian et al., 2010; Mikolov et al., 2013) and convolutional neural networks (CNNs) that are able to recognize specific classes of n-gram and induce more abstract representations (Kalchbrenner et al., 2014) are a natural combination one should apply to obtain more effective representations for RE in this setting. Convolutional neural networks (dating back to the 1980s) are a type of feed-forward artificial neural networks whose layers are formed by a convolution operation followed by a pooling operation (LeCun et al., 1988; Kalchbrenner et al., 2014). Recently, with the emerging interests of the community in deep learning, CNNs have been revived and effectively applied in various NLP tasks, including semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeli"
W15-1506,P04-3022,0,0.15341,"ples. The non-relation examples, therefore, can be treated as a usual relation class. Relation extraction, on the other hand, often comes with a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical"
W15-1506,D14-1181,0,0.272335,"ain more effective representations for RE in this setting. Convolutional neural networks (dating back to the 1980s) are a type of feed-forward artificial neural networks whose layers are formed by a convolution operation followed by a pooling operation (LeCun et al., 1988; Kalchbrenner et al., 2014). Recently, with the emerging interests of the community in deep learning, CNNs have been revived and effectively applied in various NLP tasks, including semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), name tagging and semantic role labeling (Collobert et al., 2011). For relation classification and extraction, there are two very recent works on CNNs for relation classification (Liu et al., 2013)2 and (Zeng et al., 2014); however, to the best of our knowledge, there has been no work on employing CNNs for relation extraction so far. This paper is the first attempt to fill in that gap and serves as a baseline for future research in this area. Our convolutional neural network is built upon that of Kalchbrenner et al. (2014) and Kim (2014) which are originally proposed for sentence classificati"
W15-1506,N10-1004,0,0.0145528,"uage processing (NLP) modules. 39 Proceedings of NAACL-HLT 2015, pages 39–48, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics While these methods allow the RE systems to inherit the knowledge discovered by the NLP community for the pre-processing tasks, they might be subject to the error propagation introduced by the imperfect quality of the supervised NLP toolkits. For instance, all the tasks mentioned in the pipeline above are known to suffer from a performance loss when they are applied to out-of-domain data (Blitzer et al., 2006; Daum´e III, 2007; McClosky et al., 2010), causing the collapse of the RE systems based on them. In this paper, we target an independent RE system that both avoids complicated feature engineering and minimizes the reliance on the supervised NLP modules for features, potentially alleviating the error propagation and advancing our performance in this area. To be concrete, our relation extraction system is provided only with raw sentences marked with the positions of the two entities of interest1 . The only elements we can derive from this structure are the words, the n-grams and their positions in the sentences, suggesting a paradigm i"
W15-1506,P09-1113,0,0.656407,"uation in Section 4 and finally conclude in Section 5. 2 Related Work As our present work focuses on the supervised framework for relation extraction, we concentrate on the supervised systems in this section. Besides the supervised systems (either feature-based or kernelbased) mentioned above, some recent systems have employed the distant supervision (DS) approach for relation extraction. This approach is essentially similar to the traditional systems in representing relation mentions but attempts to generate training data automatically by leveraging large knowledge bases of facts and corpus (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Regarding neural networks, their first application to NLP is language modeling which has been useful to learn distributed representations (embeddings) for words (Bengio et al., 2001; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mnih and Hinton, 2009; Turian et al., 2010; Mikolov et al., 2013). These word embeddings have opened a new direction for many other NLP tasks grounded on neural networks. Some of them are mentioned above. Other than that, a class of recursive neural networks (RNNs) and neural tensor networks are"
W15-1506,D14-1070,0,0.0110704,"Missing"
W15-1506,P14-2012,1,0.876358,"a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tok"
W15-1506,D09-1143,0,0.0127323,"uses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing, often performed by existing natural language processing (NLP) modules. 39 Proceedings of NAACL-HLT 2015, pages 39–48, c Denver, Color"
W15-1506,P13-1147,0,0.0568758,"Missing"
W15-1506,C08-1088,0,0.0725268,"ur present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing, often performed by existing natural language processing (NLP) modules. 39 Proceedings of NAACL-HLT 2015, pages 3"
W15-1506,D12-1110,0,0.30152,"009; Turian et al., 2010; Mikolov et al., 2013). These word embeddings have opened a new direction for many other NLP tasks grounded on neural networks. Some of them are mentioned above. Other than that, a class of recursive neural networks (RNNs) and neural tensor networks are proposed for paraphrase detection (Socher et al., 2011), parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), knowledge base completion (Socher et al., 2013c), question answering (Mohit et al., 2014) etc. Among these RNN systems, the study that is most related to our relation extraction problem is Socher et al. (2012) that learns compositional vector representations for phrases and sentences through syntactic parse trees and applies these representations for relation classification. However, this 41 method inherently requires syntactic parse trees in contrast to our target of avoiding use of any external features and resources for RC. 3 Convolutional Neural Network for Relation Extraction Our convolutional neural network for relation extraction consists of four main layers: (i) the look-up tables to encode words in sentences by real-valued vectors, (ii) the convolutional layer to recognize ngrams, (iii) th"
W15-1506,P13-1045,0,0.015865,"). Regarding neural networks, their first application to NLP is language modeling which has been useful to learn distributed representations (embeddings) for words (Bengio et al., 2001; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mnih and Hinton, 2009; Turian et al., 2010; Mikolov et al., 2013). These word embeddings have opened a new direction for many other NLP tasks grounded on neural networks. Some of them are mentioned above. Other than that, a class of recursive neural networks (RNNs) and neural tensor networks are proposed for paraphrase detection (Socher et al., 2011), parsing (Socher et al., 2013a), sentiment analysis (Socher et al., 2013b), knowledge base completion (Socher et al., 2013c), question answering (Mohit et al., 2014) etc. Among these RNN systems, the study that is most related to our relation extraction problem is Socher et al. (2012) that learns compositional vector representations for phrases and sentences through syntactic parse trees and applies these representations for relation classification. However, this 41 method inherently requires syntactic parse trees in contrast to our target of avoiding use of any external features and resources for RC. 3 Convolutional Neur"
W15-1506,D13-1170,0,0.0351732,"to the best of our knowledge, there has been no work on employing CNNs for relation extraction so far. This paper is the first attempt to fill in that gap and serves as a baseline for future research in this area. Our convolutional neural network is built upon that of Kalchbrenner et al. (2014) and Kim (2014) which are originally proposed for sentence classification and modeling. We adapt the network for relation extraction by introducing the position embeddings to encode the relative distances of the words in the sentence to the two entities of interest. Compared to the models in Liu et al. (2013) and Zeng et al. (2014) for relation classification that apply a single window size, our model for relation extraction incorporates various window sizes for convolutional filters, allowing the network to capture wider ranges of n-grams to be helpful for relation extraction. In addition, rather than initializing the word embeddings randomly as do Liu et al. (2013) and fixing the randomly generated position embeddings during training as do Zeng et al. (2014), we use pretrained word embeddings for initialization and optimize both word embeddings and position embeddings as model parameters. More i"
W15-1506,P11-1053,1,0.646626,", often comes with a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-"
W15-1506,P14-2011,0,0.0411358,"Missing"
W15-1506,D12-1042,0,0.141651,"Work As our present work focuses on the supervised framework for relation extraction, we concentrate on the supervised systems in this section. Besides the supervised systems (either feature-based or kernelbased) mentioned above, some recent systems have employed the distant supervision (DS) approach for relation extraction. This approach is essentially similar to the traditional systems in representing relation mentions but attempts to generate training data automatically by leveraging large knowledge bases of facts and corpus (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Regarding neural networks, their first application to NLP is language modeling which has been useful to learn distributed representations (embeddings) for words (Bengio et al., 2001; Mnih and Hinton, 2007; Collobert and Weston, 2008; Mnih and Hinton, 2009; Turian et al., 2010; Mikolov et al., 2013). These word embeddings have opened a new direction for many other NLP tasks grounded on neural networks. Some of them are mentioned above. Other than that, a class of recursive neural networks (RNNs) and neural tensor networks are proposed for paraphrase detection (Socher et al., 2011), parsing (S"
W15-1506,P10-1040,0,0.0269226,"Missing"
W15-1506,P14-2105,0,0.0196539,"of n-gram and induce more abstract representations (Kalchbrenner et al., 2014) are a natural combination one should apply to obtain more effective representations for RE in this setting. Convolutional neural networks (dating back to the 1980s) are a type of feed-forward artificial neural networks whose layers are formed by a convolution operation followed by a pooling operation (LeCun et al., 1988; Kalchbrenner et al., 2014). Recently, with the emerging interests of the community in deep learning, CNNs have been revived and effectively applied in various NLP tasks, including semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), name tagging and semantic role labeling (Collobert et al., 2011). For relation classification and extraction, there are two very recent works on CNNs for relation classification (Liu et al., 2013)2 and (Zeng et al., 2014); however, to the best of our knowledge, there has been no work on employing CNNs for relation extraction so far. This paper is the first attempt to fill in that gap and serves as a baseline for future research in this area. Our convolutional neural networ"
W15-1506,C14-1220,0,0.811561,"ation followed by a pooling operation (LeCun et al., 1988; Kalchbrenner et al., 2014). Recently, with the emerging interests of the community in deep learning, CNNs have been revived and effectively applied in various NLP tasks, including semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling and classification (Kalchbrenner et al., 2014; Kim, 2014), name tagging and semantic role labeling (Collobert et al., 2011). For relation classification and extraction, there are two very recent works on CNNs for relation classification (Liu et al., 2013)2 and (Zeng et al., 2014); however, to the best of our knowledge, there has been no work on employing CNNs for relation extraction so far. This paper is the first attempt to fill in that gap and serves as a baseline for future research in this area. Our convolutional neural network is built upon that of Kalchbrenner et al. (2014) and Kim (2014) which are originally proposed for sentence classification and modeling. We adapt the network for relation extraction by introducing the position embeddings to encode the relative distances of the words in the sentence to the two entities of interest. Compared to the models in L"
W15-1506,P06-1104,0,0.0235243,"actical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing, often performed by existing natural language processing (NLP) modules. 39"
W15-1506,P05-1053,0,0.977313,"ore, can be treated as a usual relation class. Relation extraction, on the other hand, often comes with a tremendously unbalanced dataset where the number of the non-relation examples far exceeds the others, making relation extraction more challenging but more practical than relation classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machine"
W15-1506,D07-1076,0,0.00899967,"n classification. Our present work focuses on the relation extraction task with an unbalanced corpus. In the last decade, the relation extraction literature has been dominated by two methods, distinguished by the nature of the relation representation: the feature-based method (Kambhatla, 2004; Boschee et al., 2005; Zhou et al., 2005; Grishman et al., 2005; Jiang and Zhai, 2007; Chan and Roth, 2010; Sun et al., 2011; Nguyen and Grishman, 2014) and the kernel-based method (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008; Nguyen et al., 2009; Sun and Han, 2014). The common characteristic of these methods is the leverage of a large body of linguistic analysis and knowledge resources to transform relation mentions into some rich representation to be used by some statistical classifier such as Support Vector Machines (SVM) or Maximum Entropy (MaxEnt). The linguistic analysis pipeline which is hand-designed itself includes tokenization, part of speech tagging, chunking, name tagging as well as parsing, often performed by existing natural language processing (NLP) modules. 39 Proceedings of NAAC"
W15-1506,S10-1006,0,\N,Missing
W15-2709,S12-1051,0,0.094347,"Missing"
W15-2709,P05-1074,0,0.690042,"mong short texts, such as SMS and Twitter. In this paper, we present idiomatic expressions as a new domain for short-text paraphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 1 Introduction The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years (Bannard and Callison-Burch, 2005; Sekine, 2005; Socher et al., 2011; Guo et al., 2013; Xu et al., 2013; Wang et al., 2013; Zhang and Weld, 2013; Xu et al., 2015). This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (Zhao et al., 2014), text summarization (Lloret et al., 2008), first story detection (Petrovich, 2012), search (Zanzotto et al., 2011), question answering (Celikyilmaz, 2010), etc. In this paper we explore a new domain for the task"
W15-2709,P13-1024,0,0.123415,"idiomatic expressions as a new domain for short-text paraphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 1 Introduction The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years (Bannard and Callison-Burch, 2005; Sekine, 2005; Socher et al., 2011; Guo et al., 2013; Xu et al., 2013; Wang et al., 2013; Zhang and Weld, 2013; Xu et al., 2015). This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (Zhao et al., 2014), text summarization (Lloret et al., 2008), first story detection (Petrovich, 2012), search (Zanzotto et al., 2011), question answering (Celikyilmaz, 2010), etc. In this paper we explore a new domain for the task of paraphrase identification - idiomatic expressions,"
W15-2709,P14-1023,0,0.0309117,"Missing"
W15-2709,E06-1042,0,0.0970851,"Missing"
W15-2709,D13-1090,0,0.0130521,"performed on our dataset of idiomatic expressions by a simple technique, raising a question on how well existing paraphrase models generalize to new data. Related Work There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across sentences or phrases. A growing body of research investigates ways of paraphrase detection in both supervised (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani and Tetreault, 2012; Ji and Eisenstein, 2013) and unsupervised settings (Bannard and Callison-Burch, 2005; Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). These methods mainly work on large scale news data. News data is very different from ours in two aspects: most news text can be interpreted literally and similar news events (passing a legislation, death of a person, elections) happen repeatedly. Therefore, lexical anchors or event anchors can work well on news text, but not necessarily on our task. Millions of tweets generated by Twitter users every day provide"
W15-2709,D12-1050,0,0.0198306,"two state-of-the-art paraphrasing models that are outperformed on our dataset of idiomatic expressions by a simple technique, raising a question on how well existing paraphrase models generalize to new data. Related Work There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across sentences or phrases. A growing body of research investigates ways of paraphrase detection in both supervised (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani and Tetreault, 2012; Ji and Eisenstein, 2013) and unsupervised settings (Bannard and Callison-Burch, 2005; Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). These methods mainly work on large scale news data. News data is very different from ours in two aspects: most news text can be interpreted literally and similar news events (passing a legislation, death of a person, elections) happen repeatedly. Therefore, lexical anchors or event anchors can work well on news text, but not necessarily on our task. Millions"
W15-2709,W06-1203,0,0.031761,"way, e.g. the idioms (1) make a mountain out of a molehill (2) tempest in a teapot convey similar ideas1 : (1) If somebody makes a mountain out of a molehill they exaggerate the importance or seriousness of a problem. (2) If people exaggerate the seriousness of a situation or problem they are making a tempest in a teapot. There is a line of research focused on extracting idioms from the text or identifying whether a particular expression is idiomatic (or a noncompositional multi-word expression) (Muzny and Zettlemoyer, 2013; Shutova et al., 2010; Li and Sporleder, 2009; Gedigian et al., 2006; Katz and Giesbrecht, 2006). Without linguistic sources such as Wiktionary, usingenglish.com, etc, it is often hard to understand what the meaning of a particular idiom is. It is even harder to determine whether two idioms convey the same idea or find alternative idiomatic expressions. Using idiom definitions, given by linguistic resources, one can view this problem as identifying paraphrases between definitions and thus deciding on paraphrases between corresponding idioms. Efficient techniques for identifying idiom paraphrases would complement any paraphrase identification system, and thus improve the downstream applic"
W15-2709,W10-1201,0,0.0371869,"Missing"
W15-2709,P09-1053,0,0.0285646,"uilt on. In this paper, we experiment with two state-of-the-art paraphrasing models that are outperformed on our dataset of idiomatic expressions by a simple technique, raising a question on how well existing paraphrase models generalize to new data. Related Work There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across sentences or phrases. A growing body of research investigates ways of paraphrase detection in both supervised (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani and Tetreault, 2012; Ji and Eisenstein, 2013) and unsupervised settings (Bannard and Callison-Burch, 2005; Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). These methods mainly work on large scale news data. News data is very different from ours in two aspects: most news text can be interpreted literally and similar news events (passing a legislation, death of a person, elections) happen repeatedly. Therefore, lexical anchors or event anchors can work well on news t"
W15-2709,S15-2011,0,0.394803,"Missing"
W15-2709,D09-1033,0,0.0198563,"of the idiom is often expressed in an indirect way, e.g. the idioms (1) make a mountain out of a molehill (2) tempest in a teapot convey similar ideas1 : (1) If somebody makes a mountain out of a molehill they exaggerate the importance or seriousness of a problem. (2) If people exaggerate the seriousness of a situation or problem they are making a tempest in a teapot. There is a line of research focused on extracting idioms from the text or identifying whether a particular expression is idiomatic (or a noncompositional multi-word expression) (Muzny and Zettlemoyer, 2013; Shutova et al., 2010; Li and Sporleder, 2009; Gedigian et al., 2006; Katz and Giesbrecht, 2006). Without linguistic sources such as Wiktionary, usingenglish.com, etc, it is often hard to understand what the meaning of a particular idiom is. It is even harder to determine whether two idioms convey the same idea or find alternative idiomatic expressions. Using idiom definitions, given by linguistic resources, one can view this problem as identifying paraphrases between definitions and thus deciding on paraphrases between corresponding idioms. Efficient techniques for identifying idiom paraphrases would complement any paraphrase identifica"
W15-2709,N12-1019,0,0.019667,"aphrasing models that are outperformed on our dataset of idiomatic expressions by a simple technique, raising a question on how well existing paraphrase models generalize to new data. Related Work There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across sentences or phrases. A growing body of research investigates ways of paraphrase detection in both supervised (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani and Tetreault, 2012; Ji and Eisenstein, 2013) and unsupervised settings (Bannard and Callison-Burch, 2005; Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). These methods mainly work on large scale news data. News data is very different from ours in two aspects: most news text can be interpreted literally and similar news events (passing a legislation, death of a person, elections) happen repeatedly. Therefore, lexical anchors or event anchors can work well on news text, but not necessarily on our task. Millions of tweets generated by Twitt"
W15-2709,W06-3506,0,0.0341548,"pressed in an indirect way, e.g. the idioms (1) make a mountain out of a molehill (2) tempest in a teapot convey similar ideas1 : (1) If somebody makes a mountain out of a molehill they exaggerate the importance or seriousness of a problem. (2) If people exaggerate the seriousness of a situation or problem they are making a tempest in a teapot. There is a line of research focused on extracting idioms from the text or identifying whether a particular expression is idiomatic (or a noncompositional multi-word expression) (Muzny and Zettlemoyer, 2013; Shutova et al., 2010; Li and Sporleder, 2009; Gedigian et al., 2006; Katz and Giesbrecht, 2006). Without linguistic sources such as Wiktionary, usingenglish.com, etc, it is often hard to understand what the meaning of a particular idiom is. It is even harder to determine whether two idioms convey the same idea or find alternative idiomatic expressions. Using idiom definitions, given by linguistic resources, one can view this problem as identifying paraphrases between definitions and thus deciding on paraphrases between corresponding idioms. Efficient techniques for identifying idiom paraphrases would complement any paraphrase identification system, and thus i"
W15-2709,D13-1145,0,0.0145641,"ilar expressions. In addition, an idea, or a moral of the idiom is often expressed in an indirect way, e.g. the idioms (1) make a mountain out of a molehill (2) tempest in a teapot convey similar ideas1 : (1) If somebody makes a mountain out of a molehill they exaggerate the importance or seriousness of a problem. (2) If people exaggerate the seriousness of a situation or problem they are making a tempest in a teapot. There is a line of research focused on extracting idioms from the text or identifying whether a particular expression is idiomatic (or a noncompositional multi-word expression) (Muzny and Zettlemoyer, 2013; Shutova et al., 2010; Li and Sporleder, 2009; Gedigian et al., 2006; Katz and Giesbrecht, 2006). Without linguistic sources such as Wiktionary, usingenglish.com, etc, it is often hard to understand what the meaning of a particular idiom is. It is even harder to determine whether two idioms convey the same idea or find alternative idiomatic expressions. Using idiom definitions, given by linguistic resources, one can view this problem as identifying paraphrases between definitions and thus deciding on paraphrases between corresponding idioms. Efficient techniques for identifying idiom paraphra"
W15-2709,N12-1034,0,0.0754156,"Missing"
W15-2709,W06-1603,0,0.041231,"rom the data that those models are built on. In this paper, we experiment with two state-of-the-art paraphrasing models that are outperformed on our dataset of idiomatic expressions by a simple technique, raising a question on how well existing paraphrase models generalize to new data. Related Work There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across sentences or phrases. A growing body of research investigates ways of paraphrase detection in both supervised (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani and Tetreault, 2012; Ji and Eisenstein, 2013) and unsupervised settings (Bannard and Callison-Burch, 2005; Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). These methods mainly work on large scale news data. News data is very different from ours in two aspects: most news text can be interpreted literally and similar news events (passing a legislation, death of a person, elections) happen repeatedly. Therefore, lexical anchors o"
W15-2709,I05-5011,0,0.0401803,"Twitter. In this paper, we present idiomatic expressions as a new domain for short-text paraphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 1 Introduction The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years (Bannard and Callison-Burch, 2005; Sekine, 2005; Socher et al., 2011; Guo et al., 2013; Xu et al., 2013; Wang et al., 2013; Zhang and Weld, 2013; Xu et al., 2015). This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (Zhao et al., 2014), text summarization (Lloret et al., 2008), first story detection (Petrovich, 2012), search (Zanzotto et al., 2011), question answering (Celikyilmaz, 2010), etc. In this paper we explore a new domain for the task of paraphrase"
W15-2709,C10-1113,0,0.0690761,"Missing"
W15-2709,P10-1040,0,0.0998499,"Missing"
W15-2709,U06-1019,0,0.0142729,"those models are built on. In this paper, we experiment with two state-of-the-art paraphrasing models that are outperformed on our dataset of idiomatic expressions by a simple technique, raising a question on how well existing paraphrase models generalize to new data. Related Work There is no strict definition of a paraphrase (Bhagat and Hovy, 2013) and in linguistic literature paraphrases are most often characterized by an approximate equivalence of meanings across sentences or phrases. A growing body of research investigates ways of paraphrase detection in both supervised (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani and Tetreault, 2012; Ji and Eisenstein, 2013) and unsupervised settings (Bannard and Callison-Burch, 2005; Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). These methods mainly work on large scale news data. News data is very different from ours in two aspects: most news text can be interpreted literally and similar news events (passing a legislation, death of a person, elections) happen repeatedly. Therefore, lexical anchors or event anchors ca"
W15-2709,D13-1008,0,0.0193165,"in for short-text paraphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 1 Introduction The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years (Bannard and Callison-Burch, 2005; Sekine, 2005; Socher et al., 2011; Guo et al., 2013; Xu et al., 2013; Wang et al., 2013; Zhang and Weld, 2013; Xu et al., 2015). This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (Zhao et al., 2014), text summarization (Lloret et al., 2008), first story detection (Petrovich, 2012), search (Zanzotto et al., 2011), question answering (Celikyilmaz, 2010), etc. In this paper we explore a new domain for the task of paraphrase identification - idiomatic expressions, in which the goal is to determine w"
W15-2709,S15-2001,0,0.172799,"Missing"
W15-2709,W13-2515,1,0.849006,"ons as a new domain for short-text paraphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 1 Introduction The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years (Bannard and Callison-Burch, 2005; Sekine, 2005; Socher et al., 2011; Guo et al., 2013; Xu et al., 2013; Wang et al., 2013; Zhang and Weld, 2013; Xu et al., 2015). This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (Zhao et al., 2014), text summarization (Lloret et al., 2008), first story detection (Petrovich, 2012), search (Zanzotto et al., 2011), question answering (Celikyilmaz, 2010), etc. In this paper we explore a new domain for the task of paraphrase identification - idiomatic expressions, in which the goa"
W15-2709,D11-1061,0,0.04046,"Missing"
W15-2709,D13-1183,0,0.028285,"araphrase identification. We propose a technique, utilizing idiom definitions and continuous space word representations that performs competitively on a dataset of 1.4K annotated idiom paraphrase pairs, which we make publicly available for the research community. 1 Introduction The task of paraphrase identification, i.e. finding alternative linguistic expressions of the same or similar meaning, attracted a great deal of attention in the research community in recent years (Bannard and Callison-Burch, 2005; Sekine, 2005; Socher et al., 2011; Guo et al., 2013; Xu et al., 2013; Wang et al., 2013; Zhang and Weld, 2013; Xu et al., 2015). This task was extensively studied in Twitter data, where millions of user-generated tweets talk about the same topics and thus present a natural challenge to resolve redundancy in tweets for many applications, such as textual entailment (Zhao et al., 2014), text summarization (Lloret et al., 2008), first story detection (Petrovich, 2012), search (Zanzotto et al., 2011), question answering (Celikyilmaz, 2010), etc. In this paper we explore a new domain for the task of paraphrase identification - idiomatic expressions, in which the goal is to determine whether two idioms conv"
W15-2709,S15-2021,0,0.0308851,"Missing"
W15-2709,S15-2045,0,\N,Missing
W15-2709,P12-1091,0,\N,Missing
W15-2709,Q14-1034,0,\N,Missing
W15-2709,J13-3001,0,\N,Missing
W15-4502,P13-1008,0,0.172747,"ntext in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standard entity mention and Framework and Featur"
W15-4502,W06-0901,0,0.363076,"Missing"
W15-4502,P10-1081,1,0.890682,"only used to represent context in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standard entity mention and Fra"
W15-4502,W13-2322,0,0.0400666,"Missing"
W15-4502,R11-1002,1,0.927597,"Missing"
W15-4502,N15-1114,0,0.0103745,", including methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptrons (Li et al., 2013b), and dual decomposition (Riedel and McCallum (2009; 2011a; 2011b)). However, all of these methods as mentioned above have not exploited the knowledge captured in the AMR. A growing number of researchers are studying how to incorporate the knowledge encoded in the AMR parse and representations to help solve other NLP problems, such as entity linking (Pan et al., 2015), machine translation (Jones et al., 2015), and summarization (Liu et al., 2015). Especially the appearance of the first published AMR parser (Flanigan et al., 2014) will benefit and spur a lot of new research conducted using AMR. Discussion Applying the AMR features separately, we find that the features extracted from the sibling nodes are the best predictors of correctness, which indicates that the contexts of sibling nodes associated with the AMR tags can provide better evidence for word sense disambiguation of the trigger candidate as needed for event type classification. Features from the parent node and children nodes are also significant contributors. Performance o"
W15-4502,dorr-etal-1998-thematic,0,0.0693321,"Missing"
W15-4502,P11-1163,0,0.040647,"Missing"
W15-4502,P14-1134,0,0.243288,"be syntactic “sugar” and are not explicitly represented in AMR, except for the semantic relations they signal. Hence, it assigns the same AMR parse graph to sentences that have the same basic meaning.3 Compared to traditional dependency parsing and semantic role labeling, the nodes in AMR are entities instead of words, and the edge types are much more fine-grained. AMR thus captures deeper meaning compared with other representations which are more commonly used to represent context in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigg"
W15-4502,J05-1004,0,0.0338394,"ure 1: Two equivalent ways of representing the AMR parse graph, and the corresponding feature AMR parse for the example sentence, “The acquivalues, for trigger candidate “acquisition”, from sition of Edison GE will boost AIG’s annual life the above example AMR graph. insurance revenue.” boost-01 Improve Event Detection with Abstract Meaning Representation ARG0 ARG1 4 In this section, we will compare our MaxEnt classifiers using both baseline features and additional proposed AMR features with the state-of-the-art systems on the blind test set, and then discuss the results in more detail. 2002; Palmer et al., 2005). For example, a phrase like “bond investor” is represented using the frame “invest-01”, even though no verbs appear. In addition, many function words (determiners, prepositions) are considered to be syntactic “sugar” and are not explicitly represented in AMR, except for the semantic relations they signal. Hence, it assigns the same AMR parse graph to sentences that have the same basic meaning.3 Compared to traditional dependency parsing and semantic role labeling, the nodes in AMR are entities instead of words, and the edge types are much more fine-grained. AMR thus captures deeper meaning co"
W15-4502,N15-1119,0,0.0309298,") of baseline and AMR on a subset of event types. 4.2 has worked on joint models, including methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptrons (Li et al., 2013b), and dual decomposition (Riedel and McCallum (2009; 2011a; 2011b)). However, all of these methods as mentioned above have not exploited the knowledge captured in the AMR. A growing number of researchers are studying how to incorporate the knowledge encoded in the AMR parse and representations to help solve other NLP problems, such as entity linking (Pan et al., 2015), machine translation (Jones et al., 2015), and summarization (Liu et al., 2015). Especially the appearance of the first published AMR parser (Flanigan et al., 2014) will benefit and spur a lot of new research conducted using AMR. Discussion Applying the AMR features separately, we find that the features extracted from the sibling nodes are the best predictors of correctness, which indicates that the contexts of sibling nodes associated with the AMR tags can provide better evidence for word sense disambiguation of the trigger candidate as needed for event type classification. Features from the"
W15-4502,P09-2093,0,0.0487434,"Missing"
W15-4502,D09-1016,0,0.0224803,"Missing"
W15-4502,P11-1113,0,0.102381,"shed AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standard entity mention and Framework and Features To compare our proposed AMR features with the previous approaches, we implemented a Maximum Ent"
W15-4502,N10-1123,0,0.0178759,"Missing"
W15-4502,D11-1001,0,0.024998,"Missing"
W15-4502,P08-1030,1,0.682387,"ons which are more commonly used to represent context in ED. In this work, all AMR parse graphs are automatically generated from the first published AMR parser, JAMR (Flanigan et al., 2014). 3 Experiments 4.1 Dataset and Evaluation Metric We evaluate our system with above presented features over the ACE 2005 corpus. For comparison purposes, we utilize the same test set with 40 newswire articles (672 sentences), the same development set with 30 other documents (836 sentences) and the same training set with the remaining 529 documents (14, 849 sentences) as the previous studies on this dataset (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013b). Following the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013b), a trigger candidate is counted as correct if its event subtype and offsets match those of a reference trigger. The ACE 2005 corpus has 33 event subtypes that, along with one class “Other” for the non-trigger tokens, constitutes a 34-class classification problem in this work. Finally we use Precision (P), Recall (R), and F-measure (F1 ) to evaluate the performance. Table 2 presents the overall performance of the systems with gold-standa"
W15-4502,W11-1807,0,0.0180918,"Missing"
W15-4502,W09-1406,0,0.0383609,"Missing"
W15-4502,kingsbury-palmer-2002-treebank,0,0.0917821,"Missing"
W15-4502,D12-1092,0,0.0248754,"Missing"
W15-4502,P13-1145,0,0.0316416,"Missing"
W15-4502,C12-1083,0,\N,Missing
W16-1618,W06-0901,0,0.0379377,"concepts or topics covered by the corresponding event types in the ACE 2005 corpus. As we can see from the 5 Taken from the ACE 2005 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of"
W16-1618,W06-1615,0,0.0242467,"abeled instances for each event type in the training data. Unfortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting). How can we effectively leverage the training data of old event types to facilitate the extraction of the new event type? Inspired by the work on transfer learning and domain adaptation (Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Jiang, 2009), in this paper, we systematically evaluate the representative methods (i.e, the feature based model and the CNN model) for ED to gain an insight into which kind of method performs better in the new extension setting. In addition, we propose a two-stage algorithm to train a CNN model that effectively learns and transfers the knowledge from the old event types for the extraction of the target type. We study the event detection problem in the new type extension setting. In particular, our task involves identifying the event instances of a targ"
W16-1618,D14-1199,0,0.034208,"Missing"
W16-1618,P15-1017,0,0.447118,"he feature-based approach that has dominated the ED research in the last decade (Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Riedel and McCallum, 2011; Li et al., 2013; Venugopal et al., 2014). The second approach, on the other hand, is proposed very recently and uses convolutional neural networks (CNN) to exploit the continuous representations of words. These continuous representations have been shown to effectively capture the underlying structures of a sentence, thereby significantly improving the performance for ED (Nguyen and Grishman, 2015; Chen et al., 2015). The previous research has mainly focused on building an ED system in a supervised setting. The performance of such systems strongly depends on a sufficient amount of labeled instances for each event type in the training data. Unfortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting). How can we effect"
W16-1618,P07-1033,0,0.0333017,"Missing"
W16-1618,P09-2093,0,0.0127275,"ken from the ACE 2005 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to l"
W16-1618,P11-1113,0,0.20635,"he two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings"
W16-1618,P08-1030,1,0.37819,"e can see from the 5 Taken from the ACE 2005 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use"
W16-1618,P09-1114,0,0.107311,"fortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting). How can we effectively leverage the training data of old event types to facilitate the extraction of the new event type? Inspired by the work on transfer learning and domain adaptation (Blitzer et al., 2006; Jiang and Zhai, 2007; Daume III, 2007; Jiang, 2009), in this paper, we systematically evaluate the representative methods (i.e, the feature based model and the CNN model) for ED to gain an insight into which kind of method performs better in the new extension setting. In addition, we propose a two-stage algorithm to train a CNN model that effectively learns and transfers the knowledge from the old event types for the extraction of the target type. We study the event detection problem in the new type extension setting. In particular, our task involves identifying the event instances of a target type that is only specified by a small set of seed"
W16-1618,P13-1008,0,0.566443,"involves event argument discovery. There have been two major approaches to ED in the literature. The first approach extensively leverages linguistic analysis and knowledge resources to capture the discrete structures for ED, focusing on the combination of various properties 1 most often a single verb or nominalization 158 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 158–165, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics entropy (MaxEnt) and classified as the type T or not. In this work, we employ the feature set for ED from (Li et al., 2013), which is the state-of-the-art FET. The experimental results show that this two-stage algorithm significantly outperforms the traditional methods in the type extension setting for ED and demonstrates the benefit of CNN in transfer learning. To our knowledge, this is the first work on the type extension setting as well as on transferring knowledge with neural networks for ED of natural language processing. 2 3.2 In a CNN for ED, we limit the context of the trigger candidates to a fixed window size by trimming longer sentences and padding shorter sentences with a special token when necessary. L"
W16-1618,D14-1198,0,0.0351828,"entence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally, domain adaptation and transfer learning have been studied extensively for variou"
W16-1618,P10-1081,1,0.725734,"fined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains"
W16-1618,R11-1002,1,0.880877,"uideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utiliz"
W16-1618,P10-1040,0,0.0179284,"might only be partial and not necessarily include all the trigger words of type T in D. Also, we call DN the set of the negative instances generated from D under this setting (to be discussed in more details later). In general, DN might contains unannotated trigger words of T (false negatives), making this task more challenging. Eventually, our goal is to learn an event detector for T , leveraging the training data DT , DA and DN for both the target and auxiliary types. Note that our work is related to Jiang (2009) who studies the relation type extension problem. 3 1. Word Embedding Table E (Turian et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b). 2. Position Embedding Table: to embed the relative distance i of xi to the current token x0 . 3. Entity Type Embedding Table: to capture the entity type information for each token. Following Nguyen and Grishman (2015), we assign the entity type labels to each token using the heads of the entity mentions in x with the BIO schema. As a result, the original event trigger candidate x is transformed into a matrix x = [x−w , x−w+1 , . . . , x0 , . . . , xw−1 , xw ]. This matrix will serve as the input for CNN. Models for Event Detection In this sectio"
W16-1618,N10-1004,0,0.0148524,"Missing"
W16-1618,D14-1090,0,0.109324,"rly research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally, domain adaptation and t"
W16-1618,P11-1163,0,0.0172154,"thm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statisti"
W16-1618,P14-2012,1,0.527304,"Missing"
W16-1618,P15-2060,1,0.307446,"azetteers. This is called the feature-based approach that has dominated the ED research in the last decade (Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2011; McClosky et al., 2011; Riedel and McCallum, 2011; Li et al., 2013; Venugopal et al., 2014). The second approach, on the other hand, is proposed very recently and uses convolutional neural networks (CNN) to exploit the continuous representations of words. These continuous representations have been shown to effectively capture the underlying structures of a sentence, thereby significantly improving the performance for ED (Nguyen and Grishman, 2015; Chen et al., 2015). The previous research has mainly focused on building an ED system in a supervised setting. The performance of such systems strongly depends on a sufficient amount of labeled instances for each event type in the training data. Unfortunately, this setting does not reflect the real world situation very well. In practice, we often have a large amount of training data for some old event types but are interested in extracting instances of a new event type. The new event type is only specified by a small set of seed instances provided by clients (the event type extension setting"
W16-1618,P15-1062,1,0.900189,"Missing"
W16-1618,N16-1034,1,0.371819,"the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally, domain adaptation and transfer learning have been studied extensively for various NLP tasks, including part of speech tagging (Blitzer et al., 2006), name tagging (Daume III, 2007), parsing (McClosky et al., 2010), relation extraction (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2015a), to name a few. For event extraction, Miwa et al. (2013) study instance weighting and stacking models while Riedel and McCallum (2011b) examine joint models with domain adaptation. However, none of them studies the"
W16-1618,D09-1016,0,0.0113689,"5 Annotation Guideline 163 Defined by the annotation guideline. plan to apply the two-stage algorithm to other tasks such as relation extension to further verify its effectiveness. table, the Meet and Phone-Write subtypes or topics of Contact are quite separate from those of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a c"
W16-1618,P13-1147,0,0.0300964,"Missing"
W16-1618,N10-1123,0,0.0141137,"7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurrent neural networks. Finally"
W16-1618,D11-1001,0,0.0243931,"Missing"
W16-1618,W11-1807,0,0.0518334,"Missing"
W16-1618,W09-1406,0,0.0191274,"of the other types. 7 Related Work References Early research on event extraction has primarily focused on local sentence-level representations in a pipelined architecture (Grishman et al., 2005; Ahn, 2006). Afterward, higher level features have been found to improve the performance (Ji and Grishman, 2008; Gupta and Ji, 2009; Patwardhan and Riloff, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Hong et al., 2011; McClosky et al., 2011; Huang and Riloff, 2012; Li et al., 2013). Some recent research has proposed joint models for EE, including the methods based on Markov Logic Networks (Riedel et al., 2009; Poon and Vanderwende, 2010; Venugopal et al., 2014), structured perceptron (Li et al., 2013; Li et al., 2014b), and dual decomposition (Riedel et al. (2009; 2011b)). The application of neural networks to EE is very recent. In particular, Zhou et al. (2014) and Boros et al. (2014) use neural networks to learn word embeddings from a corpus of specific domains and then directly utilize these embeddings as features in statistical classifiers. Chen et al. (2015) apply dynamic multi-pooling CNNs for EE in a pipelined framework, while Nguyen et al. (2016) propose joint event extraction using recurr"
W18-6126,W14-2907,0,0.177062,"Missing"
W18-6126,P13-1147,0,0.0250685,"ture representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both datasets. 1 2 Related Work Relation extraction is typically reduced to a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic re"
W18-6126,W15-0812,0,0.0147816,"ents did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil. Relation extraction is a crucial task for many applications such as knowledge base population. Several relation schemas and annotated corpora have been developed such as the Automatic Content Extraction (ACE), and the Entities, Relations and Events (ERE) annotation (Song et al., 2015). These schemas share some similarity, but differ in details. A relation type may exist in one schema but not in another. An example might be annotated as different types in different datasets. For example, Part-whole.Geographical relations in ACE05 are annotated as Physcial.Located relations in ERE. Most of these corpora are relatively small. Models trained on a single corpus may be biased or overfitted towards the corpus. Despite the difference in relation schemas, we hypothesize that we can learn a more general rep1 https://catalog.ldc.upenn.edu/LDC2006T06 We use 6 LDC releases combined: LD"
W18-6126,P15-2139,0,0.0761717,"Missing"
W18-6126,D15-1203,0,0.208795,"Missing"
W18-6126,I17-2072,1,0.946739,"Relation extraction is typically reduced to a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil. Relation extraction is a crucial task for many appl"
W18-6126,C14-1220,0,0.746591,"ignificant improvement on both datasets. 1 2 Related Work Relation extraction is typically reduced to a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Br"
W18-6126,Y15-1009,0,0.0193064,"a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil. Relation extraction is a crucial task for many applications such as knowledge base population. S"
W18-6126,D15-1205,0,0.180496,"Missing"
W18-6126,P05-1053,0,0.0990245,"gularization by adversarial training. The additional corpora feeding the encoder can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both datasets. 1 2 Related Work Relation extraction is typically reduced to a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and"
W18-6126,P07-1034,0,0.0138097,"ersarial training. The additional corpora feeding the encoder can help to learn a better feature representation layer even though the relation schemas are different. We use ACE05 and ERE datasets as our case study for experiments. The multi-task model obtains significant improvement on both datasets. 1 2 Related Work Relation extraction is typically reduced to a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et"
W18-6126,P16-2034,0,0.358936,"oblem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil. Relation extraction is a crucial task for many applications such as knowledge base population. Several relation sch"
W18-6126,I17-1068,1,0.890146,"Missing"
W18-6126,P16-1105,0,0.212293,"machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil. Relation extraction is a crucial task for many applications such as knowledge base population. Several relation schemas and annotated corpo"
W18-6126,W15-1506,1,0.956821,"both datasets. 1 2 Related Work Relation extraction is typically reduced to a classification problem. A supervised machine learning model is designed and trained on a single dataset to predict the relation type of pairs of entities. Traditional methods rely on linguistic or semantic features (Zhou et al., 2005; Jing and Zhai, 2007), or kernels based on syntax or sequences (Bunescu and Mooney, 2005a,b; Plank and Moschitti, 2013) to represent sentences of relations. More recently, deep neural nets start to show promising results. Most rely on convolutional neural nets (Zeng et al., 2014, 2015; Nguyen and Grishman, 2015, 2016; Fu et al., 2017) or recurrent neural nets (Zhang et al., 2015; Zhou et al., 2016; Miwa and Bansal, 2016) to learn the representation of relations. Our supervised base model will be similar to (Zhou et al., 2016). Our initial experiments did not use syntactic features (Nguyen and Grishman, 2016; Fu et al., 2017) that require additional parsers. Introduction Relations represent specific semantic relationships between two entities. For example, there is Physical.Located relationship between Smith and Brazil in the sentence: Smith went to a conference in Brazil. Relation extraction is a cr"
W98-0604,M95-1014,1,0.820264,"position) Introduction Although, nominalizationQ are very common in written text, the computational linguistics literature provides few systematic accounts of how to deal with phrases containing these words. This paper focuses on this problem in the context of Information Extraction (IE). 2 Many extraction systems use either parsing combined with some form of syntactic regularization, or a meta-rule mechanism to automatically match variants of clausal syntactic structures (active main clause, passive, relative clause etc.), e.g., FASTUS (Appelt et al., 1995) and the Proteus Extraction System (Grishman, 1995). However, this mechanism does not extend to nominalization patterns, which must be coded separately from the clausal patterns. NOMLEX, a dictionary of nominalizations currently under development at NYU, (Macleod et al., 1997) provides a way to handle nominalizations more automatically, and with INominalizations are nouns which are related to words of another part of speech, most commonly verbs. In this paper, only verbal nominalizatious will be discussed. 2The Message Understanding Colfference Scenario Template Task (MUC, 1995), (MUC, 1998) is ore&quot; model for the kind of information that we ar"
W98-0604,P87-1019,0,\N,Missing
W98-1118,W98-1512,0,0.014202,"value 156 Note that this method of feature selection would probably break down if we tried to incorporate general compound features into our model as described in the previous section. The model currently has about 24,000 features when trained on 350 articles of text. If we even considered all pairs of features as potential compound features, the O(n 2) compound features which we could build from our atomic features would undoubtedly yield an unacceptable slowdown in the model's performance. Clearly a more sophisticated feature selection routine such as the ones in (Berger et al., 1996), or (Berger and Printz, 1998) would be required in this case. 7 DECODING SEARCH and VITERBI After having trained the features of an M.E. model and assigned the proper weight (a values) to each of the features, decoding (i.e. ""marking up"") a new piece of text is a fairly simple process: 1. Tokenize the text. Systems 2. Compute each of the history views by looking up words in the dictionary, checking the output of the external systems, checking whether words are capitalized or not, etc. MENE (ME) Manitoba (Ma) Proteus (Pr) MENE + lsoQuest MENE + Proteus MENE + Manitoba ME + Ma + IQ ME + Pr + IQ ME + Pr + Ma ME + Pr + Ma + I"
W98-1118,J96-1002,0,0.227336,"of finding the probability of f associated with the token at index ~ in the test corpus as: p(f]ht) p 1 0 : capitalized(h) = true and f = location_start : else (1) Here ""current-token-capitalized(h)"" is a binary function which returns true if the ""current token"" of the history h (the token whose tag we are trying to determine) has an initial capitalized letter. Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai. This allows us to compute the conditional probability as follows (Berger et al., 1996): P(flh) = ~i°~ '(h'I) Z~(h) Z~(h) = ~ I ~ I ~ ff '(h'~) (2) 4 (a) i The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words: Z t5(h' f).gi(h, f) = Z 15(h)'Z PME(flh)'gi(h, f) h,f h [ ARCHITECTURE: and Futures MENE consists of a set of C + + and Perl modules which forms a wrapper around a publicly available M.E. toolkit (Ristad, 1998) which computes the values of the a parameters of equation 2 from a pair of training files created by MENE"
W98-1118,A97-1029,0,0.477361,"ne whether or not they ""fire"". In the following sections, we will look at each of MENE's feature classes in turn. 4.1 B i n a r y F e a t u r e s While all of MENE's features have binary-valued output, the ""binary"" features are features whose associated history-view can be considered to be either on or off for a given token. Examples are ""the token begins with a capitalized letter"" or ""the token is a four-digit number"". Equation 1 gives an example of a binary feature. The 11 binary history-views used by MENE's binary features are very similar to those used in BBN's Nymble/ldentifinder system (Bikel et al., 1997) with two exceptions: • Nymble used a feature for ""significant"" (i.e. non-sentence-beginning) capitalization. We didn't include this, believing that MENE could make these judgments from the surrounding lexicai content. • Nymble's features were non-overlapping. I.e. the all-cap feature took precedence over the initial-cap feature. Given two features, a and b, when the (history, filture) space on which feature b activates must be a subset of the space for feature a, it can be shown that the M.E. model will yield the same results whether a and b are included as features or if (a - b) arid b are f"
W98-1118,M98-1018,1,0.699036,"was tagged by BBN and Science Applications International Corporation (SAIC). MENE has also been run against all-uppercase data. On this we achieved an F-measure of 88.19 for the MENE-only system and 91.38 for the MENE + Proteus system. The latter figure matches the best currently published result (Bikel et al., 1997) on within-domain all-caps data. On the other hand, we scored lower on all-caps than BBN's Identifinder in the MUC-7 formal evaluation for reasons which are probably similar to the ones discussed in section 9 in the comparison of our mixed case performances (Miller et al., 1998) (Borthwick et al., 1998). We have put very little effort into optimizing MENE on Systems MENE MENE + MENE + MENE + M E + Pr Proteus Manitoba ]soQuest + Ma+IQ , 425 ~ 5 0 250 150 100 80 40 20 10 5 ~-~""F'~O'~-9""~,' 90,64 89~17 87,85 84.14 80.97 76.43 63.13 II 95.73 I 95.61 [ 9 5 . 5 6 [ 9 4 4 . 6 [ 9 4 . 3 0 [ 93.44 [91.69 [ [ I I 11 95-6°1 95.49 1 95-2¢~ [ 948- 6 ] 74_-5° I 94.15 1 93.06 1 I I ] II 96.73 [ 96.55 [ 96.70 Table 3: Systems' performances with different numbers of articles this type of corpus and believe that there is room for improvement here. In another experiment, we stripped out all features other than"
W98-1118,M95-1014,1,0.604654,"word appeared in only one dictionary and did not appear often enough in the training corpus to be included in the vocabulary, but did appear in the test corpus, we would probably mistag it. 4.5 if Proteus-System- / ~/iew( tokeno(h ) ) g(h,f) = 1 : 0 : ""person_start' = person.start else = and f * Example: R i c h a r d M. Nixon, in a case where Proteus has correctly tagged ""Richard"". External System Features For NYU's official entry in the MUC-7 evaluation, MENE took in the output of an enhanced version of the more traditional, hand-coded ""Proteus"" namedentity tagger which we entered in MUC-6(Grishman, 1995). In addition, subsequent to the evaluation, the University of Manitoba (Lin, 1998) and IsoQuest, Inc. (Krupka and Hausman, 1998) shared with us the outputs of their systems on our training corpora as well as on various test corpora. The output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us. These systems were incorporated into MENE as simply three more history views by the following 2 step process: 1. Each system's output is tokenized by MENE's tokenizer and cross-system tokenization discrepancies are resolved. It is important to"
W98-1118,W97-0319,0,0.00658021,"wick et al., 1998). 9 RELATED WORK M.E. has been successfully applied to m a n y other tasks in computational linguistics. Some recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania. He has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi, 1997). Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997). M.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998). Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition. Ratnaparkhi's tagger is similar to MENE, in 158 that his features look at the surrounding two-word lexical context, but his system makes less use of dictionaries. On the other hand, his system looks at word suffixes and prefixes in the case of unknown words, which is something we haven't tried with MENE and looks at its own output by looking at its prev"
W98-1118,M98-1006,0,0.0215738,"rpus to be included in the vocabulary, but did appear in the test corpus, we would probably mistag it. 4.5 if Proteus-System- / ~/iew( tokeno(h ) ) g(h,f) = 1 : 0 : ""person_start' = person.start else = and f * Example: R i c h a r d M. Nixon, in a case where Proteus has correctly tagged ""Richard"". External System Features For NYU's official entry in the MUC-7 evaluation, MENE took in the output of an enhanced version of the more traditional, hand-coded ""Proteus"" namedentity tagger which we entered in MUC-6(Grishman, 1995). In addition, subsequent to the evaluation, the University of Manitoba (Lin, 1998) and IsoQuest, Inc. (Krupka and Hausman, 1998) shared with us the outputs of their systems on our training corpora as well as on various test corpora. The output sent to us was the standard MUC-7 output, so our collaborators didn't have to do any special processing for us. These systems were incorporated into MENE as simply three more history views by the following 2 step process: 1. Each system's output is tokenized by MENE's tokenizer and cross-system tokenization discrepancies are resolved. It is important to note that MENE has features which predict a different future than the future predi"
W98-1118,M98-1002,0,0.0198582,"0 words, which our system turned into 321,000 tokens). Note the smooth progression of the scores as more data is added to the system. Also note that, when combined under MENE, the three weakest systems, MENE, Proteus, and Manitoba outperform the strongest single system, IsoQuest's. Finally, the top score of 97.12 from combining all three systems is a very strong result. On a different set of data, the MUC-7 formal run data, the accuracy of the two human taggers who were preparing the answer key was tested and it was discovered that one of them had an F-Measure of 96.95 and the other of 97.60 (Marsh and Perzanowski, 1998). Although we don't have human performance measures on the dry run test set, it seems that we have attained a result which is at least competitive with that of a human. We also did a series of runs to examine how the systems performed with different amounts of training data. These experiments are summarized in table 3. Note the 97.38 all-systems result which we 157 achieved by adding 75 articles from the formal-run test corpus to the basic 350-article training data. In addition to being an outstanding performance figure, this number shows MENE's responsiveness to good training material. A few"
W98-1118,M98-1021,0,0.0281909,"Some recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania. He has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi, 1997). Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997). M.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998). Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition. Ratnaparkhi's tagger is similar to MENE, in 158 that his features look at the surrounding two-word lexical context, but his system makes less use of dictionaries. On the other hand, his system looks at word suffixes and prefixes in the case of unknown words, which is something we haven't tried with MENE and looks at its own output by looking at its previous two tags when making its decision. We do this implicitly through our requirement that the futures we output be consistent, but we"
W98-1118,W96-0213,0,0.0851774,"elieve that if the system had been allowed to train on missile/rocket launch articles, its performance on these articles would have been much better. More MENE test results and discussion of the formal run can be found in (Borthwick et al., 1998). 9 RELATED WORK M.E. has been successfully applied to m a n y other tasks in computational linguistics. Some recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania. He has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi, 1997). Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997). M.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998). Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition. Ratnaparkhi's tagger is similar to MENE, in 158 that his features look at the surrounding two-word lexical context, but his system m"
W98-1118,W97-0301,0,0.012623,"unseen data within its training domain. We believe that if the system had been allowed to train on missile/rocket launch articles, its performance on these articles would have been much better. More MENE test results and discussion of the formal run can be found in (Borthwick et al., 1998). 9 RELATED WORK M.E. has been successfully applied to m a n y other tasks in computational linguistics. Some recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania. He has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi, 1997). Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997). M.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998). Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition. Ratnaparkhi's tagger is similar to MENE, in 158 that his features look at the surround"
W98-1118,A97-1004,0,0.0271472,"n missile/rocket launch articles, its performance on these articles would have been much better. More MENE test results and discussion of the formal run can be found in (Borthwick et al., 1998). 9 RELATED WORK M.E. has been successfully applied to m a n y other tasks in computational linguistics. Some recent work for which there are solid comparable benchmarks is the work of Adwait Ratnaparkhi at the University of Pennsylvania. He has achieved state-of-the art results by applying M.E. to parsing (Ratnaparkhi, 1997a), part-of-speech tagging (Ratnaparkhi, 1996), and sentence-boundary detection (Reynar and Ratnaparkhi, 1997). Other recent work has applied M.E. to language modeling (Rosenfeld, 1994), machine translation (Berger et al., 1996), and reference resolution (Kehler, 1997). M.E. was first applied to named entity recognition at the MUC-7 conference by (Borthwick et al., 1998) and (Mikheev and Grover, 1998). Note that part-of-speech tagging is, in many ways, a very similar task to that of named-entity recognition. Ratnaparkhi's tagger is similar to MENE, in 158 that his features look at the surrounding two-word lexical context, but his system makes less use of dictionaries. On the other hand, his system loo"
W98-1118,W98-1120,1,0.586634,"7, n = 7) tags which define the name categories of the task at hand~ the problem of named entity recognition can be reduced to the problem of assigning one of 4n + l tags to each token. For any particular tag x from the set of n tags, we could be in one of 4 states: x_start, x_continue, x_end, and x_unique. In addition, a token could be tagged as ""other"" to indicate that it is not part of a named entity. For instance, we would tag the phrase [Jerry Lee Lewis flew to Paris] as [person_start, person_continue, person_end, other, other, location_unique I. This approach is essentially the same as (Sekine et al., 1998). The 29 tags of MUC-7 form the space of ""futures"" for a m a x i m u m entropy formulation of our N.E. problem. A m a x i m u m entropy solution to this, or any other similar problem allows the computation of p(f[h) for any f from the space of possible futures, F, for every h from the space of possible histories, H. A ""history"" in m a x i m u m entropy is all of the conditioning data which enables you to make a decision among the space of futures. In the named 3 entity problem, we Could reformulate this in terms of finding the probability of f associated with the token at index ~ in the test c"
W98-1118,M98-1015,0,\N,Missing
W98-1118,M98-1004,0,\N,Missing
X96-1029,M93-1016,1,0.684552,"al goal sometimes led to incorrect local choices of analyses; an analyzer which trusted local decisions could in many cases have done better. Our group at New York University has developed a number of information extraction systems over the past decade. In particular, we have been participants in the Message Understanding Conferences (MUCs) since MUC-1. During this time, while experimenting with many aspects of system design, we have retained a basic approach in which information extraction involves a phase of full syntactic analysis, followed by a semantic analysis of the syntactic structure [2]. Because we have a good, broad-coverage English grammar and a moderately effective method for recovering from parse failures, this approach held us in fairly good stead. However, we have recently found ourselves at a disadvantage with respect to groups which performed more local pattern matching, in three regards: . adding syntactic constructs needed new scenario was hard Having a broad-coverage, linguistically-principled grammar meant that relatively few additions were needed when moving to a new scenario. However, when specialized constructs did have to be added, the task was relatively dif"
X96-1029,M91-1028,1,\N,Missing
X96-1029,M93-1019,0,\N,Missing
X96-1047,A83-1009,0,0.0507895,"these can been seen in part as a reaction to text shown is all upper case, but (for the first time) the trends in the prior MUCs. The MUC-5 tasks, in the test materials contained mixed-case text as well. 4The representatives of the research community were Jim One innovation of MUC-5 was the use of a nested Cowie, Ralph Grishman (committee chair), Jerry Hobbs, Paul structure of objects. In earlier MUCs, each event Jacobs, Len Schubert, Carl Weir, and Ralph Weischedel. The had been represented as a single template - in effect, government people attending were George Doddington, Donna Rome Labs) [2]. Harman, Boyan Onyshkevyeh, John Prange, Bill Schultheis, and Beth Sundheim. 414 TST1-MUC3-0080 BOGOTA, 3 APR 90 (INRAVISION TELEVISION CADENA 1) - [REPORT] [JORGE ALONSO SIERRA VALENCIA] [TEXT] LIBERAL SENATOR FEDERICO ESTRADA VELEZ WAS KIDNAPPED ON 3 APRIL AT THE CORNER OF 60TH AND 48TH STREETS IN WESTERN M.EDELLIN, ONLY 100 METERS FROM A METROPOLITAN POLICE CAI [IMMEDIATE ATTENTION CENTER]. THE ANTIOQUIA DEPARTMENT LIBERAL PARTY LEADER HAD LEFT HIS HOUSE WITHOUT ANY BODYGUARDS ONLY MINUTES EARLIER. AS HE WAITED FOR THE TRAFFIC LIGHT TO CHANGE, THREE HEAVILY ARMED MEN FORCED HIM TO GET OUT"
X96-1047,M95-1005,0,\N,Missing
X98-1012,W98-1118,1,0.877629,"c., were gathered from the training corpus and the WWW. Named Entity The named entity task involves identifying and classifying several types of names -- people, 57 Maximum Entropy portability. However, the approach was somewhat different because we expected that the environment would be different. The named entity task is applicable across a range of domains, and so we can justify preparing a substantial number of training examples; furthermore, such data is relatively easy to prepare because the task is simple and the phenomenon frequent. Our maximum entropy method is described in detail in [6,7]. Again, we are developing a function which takes as input various features of the tokens in a text, and yields the probability that a given token starts, continues, or ends a name of a given type. However, the form of the function is different: instead of being a sequence of discrete decisions (a decision tree), the probability is computed as a product of functions on the individual features, with coefficients determined from the corpus. In contrast, scenario template is really a large collection of very diverse tasks (one task for each type of event), and each instance is more complex than t"
X98-1012,M98-1011,1,0.826778,"st the text. Our interface is able to inspect and modify all these knowledge bases, as well as manipulate documents and observe the results and intermediate stages of extraction on these documents. At the heart of this interface is a capability for taking a sample sentence along with its mapping into templates and produce an extraction pattern which is suitably generalized syntactically and semantically to operate on new text. The syntactic generalization is done fully automatically, while the semantic generalization is done in interaction with the user. This system is described more fully in [8,9,10]. For English named entity, the performance of the system with features based on word form (e.g., capitalization), individual lexical items, and handcollected word lists was already quite good (F=92.9 on test data from the training domain). However, there had been substantial work at NYU and elsewhere on building by hand patterns for named entity classification, and we wanted to take advantage of that work. In particular, while there might be gaps in these hand-written patterns, they did capture some situations where complex combinations of features could be used to classify names with high pr"
X98-1012,X98-1016,1,0.80253,"st the text. Our interface is able to inspect and modify all these knowledge bases, as well as manipulate documents and observe the results and intermediate stages of extraction on these documents. At the heart of this interface is a capability for taking a sample sentence along with its mapping into templates and produce an extraction pattern which is suitably generalized syntactically and semantically to operate on new text. The syntactic generalization is done fully automatically, while the semantic generalization is done in interaction with the user. This system is described more fully in [8,9,10]. For English named entity, the performance of the system with features based on word form (e.g., capitalization), individual lexical items, and handcollected word lists was already quite good (F=92.9 on test data from the training domain). However, there had been substantial work at NYU and elsewhere on building by hand patterns for named entity classification, and we wanted to take advantage of that work. In particular, while there might be gaps in these hand-written patterns, they did capture some situations where complex combinations of features could be used to classify names with high pr"
X98-1012,C96-1079,1,0.775168,"must be resolved based on many different types of evidence: the words starting or ending a name (&quot;Mr.&quot;, &quot;Corp.&quot;), the context of a name C... died&quot;); other mentions of a name in a text (&quot;Mr. Smith ... Smith reported...&quot;). Corpus-based learning may be helpful in gathering and balancing these types of evidence. Fortunately, names are very frequent in many types of text, so it is easy to get a substantial training set for this task. Definitions and Goals Information extraction involves picking out specified types of information from natural language text. Recent Message Understanding Conferences [1,2,3] have developed a spectrum of such tasks, and we have worked on two of them, at opposite ends of the spectrum: the named entity task, which involves identifying and classifying names, and the scenario template task, which involves extracting critical information (participants, location, date, etc.) about specified classes of events. We explored two learning methods, decision trees and maximum entropy. In both cases, we sought to combine criteria which could be gathered from the training corpus with generalizations which could be obtained from external sources and rules developed by hand. We ha"
X98-1012,M98-1019,0,0.0615953,"ed about performance: trying to build systems which come close to human accuracy, or at least perform with sufficient accuracy to be of practical value. In addition, we have long been concerned with portability: the ability to adapt our systems to new classes of events, to new domains, and even to new languages. We want to create systems which can be ported easily and, if possible, by people who don&apos;t know the internal workings of the system. Only in this way can systems for new tasks be created cheaply enough to be widely used. Decision Tree Our decision tree method is described in detail in [4,5]. The internal nodes of the tree test various properties of a token; based on these properties, the leaves of the tree specify the probability that a given token starts, continues, or ends a name of a given type (person, organization . . . . ). The tree is built automatically from a training corpus annotated with the various types of names. In tagging new text, we first use the decision tree to determine these probabilities; we then use a Viterbi algorithm to find the most likely consistent tagging (e.g., one in which a &apos;start person&apos; is followed by an &apos;end person&apos; and not an &apos;end organization"
X98-1012,W98-1120,1,0.850595,"ed about performance: trying to build systems which come close to human accuracy, or at least perform with sufficient accuracy to be of practical value. In addition, we have long been concerned with portability: the ability to adapt our systems to new classes of events, to new domains, and even to new languages. We want to create systems which can be ported easily and, if possible, by people who don&apos;t know the internal workings of the system. Only in this way can systems for new tasks be created cheaply enough to be widely used. Decision Tree Our decision tree method is described in detail in [4,5]. The internal nodes of the tree test various properties of a token; based on these properties, the leaves of the tree specify the probability that a given token starts, continues, or ends a name of a given type (person, organization . . . . ). The tree is built automatically from a training corpus annotated with the various types of names. In tagging new text, we first use the decision tree to determine these probabilities; we then use a Viterbi algorithm to find the most likely consistent tagging (e.g., one in which a &apos;start person&apos; is followed by an &apos;end person&apos; and not an &apos;end organization"
X98-1012,M98-1018,1,0.77223,"c., were gathered from the training corpus and the WWW. Named Entity The named entity task involves identifying and classifying several types of names -- people, 57 Maximum Entropy portability. However, the approach was somewhat different because we expected that the environment would be different. The named entity task is applicable across a range of domains, and so we can justify preparing a substantial number of training examples; furthermore, such data is relatively easy to prepare because the task is simple and the phenomenon frequent. Our maximum entropy method is described in detail in [6,7]. Again, we are developing a function which takes as input various features of the tokens in a text, and yields the probability that a given token starts, continues, or ends a name of a given type. However, the form of the function is different: instead of being a sequence of discrete decisions (a decision tree), the probability is computed as a product of functions on the individual features, with coefficients determined from the corpus. In contrast, scenario template is really a large collection of very diverse tasks (one task for each type of event), and each instance is more complex than t"
X98-1016,W98-1118,1,0.835143,"peration of the patterns. The user&apos;s input is reduced to • providing textual examples of events of interest, • describing the corresponding output structures (LFs) which the example text should induce. In the remaining sections we discuss how the system can use this information to * automatically build patterns to m a p the userspecified text into the user-specified LF, • generalize the newly created patterns to boost coverage. 6 T o a limited degree, the system is able to adapt to a new domain automatically: given training data in the domain, we can train a statistical proper name recognizer [3], in effect, obviating the need for building domain-specific name patterns. 99 ...Information Resources Inc.&apos;s Londonbased European Information Services operation has appointed George Garrick, .40 years old, president ... Field Position Company Location Person Status ;;; For &lt;company&gt; appoints &lt;person&gt; &lt;position&gt; (definePattern Appoint &quot;np(C-company)? vg(C-appoint) np(C-person) to-be? np(C-position): company=l.attributes, person=3.attributes, position=5.attributes I Value president European Information Services London George Garrick In (definehction Appoint (phrase-type) ( l e t ( ( p e r s o"
X98-1016,P93-1022,0,0.0138849,"co-descrip. Similarly, the classes C-city for city names and C-state for state names would be gathered under a concept C-location. The GUI tools then allow the user to perform semantic generalization on the individual constituents of the pattern&apos;s precondition; its final form becomes: In(C-company) &apos;s]? [n(C-location)-based]? n(C-company) n(C-co-descrip)? The semantic hierarchy is scenario-specific. It is built up dynamically through tools that draw on pre-existing domain-independent hierarchies, such as WordNet, as well as domain-specific word similarity measures and co-occurrence statistics [4]. By a similar process, we can now acquire a clausal pattern from the example in figure 3 at the beginning of this •section. The system proposes the precondition: np(C-company) vg(C-appoint) np(C-person) np(president) Applying semantic generalization to the last constituent yields: np(C-company) vg(C-appoint) np(C-person) np(C-title) where C-title is a semantic class that gathers all corporate titles. The user can now fill the slots in the LF for the event as in figure 7. 5 Meta-rules Consider the following variant of the original example: ... George Garrick, an avowed anticapitalist, was appo"
X98-1016,M95-1014,1,0.785154,"gn: Control is encapsulated in immutable core engines, which draw upon domain- or scenario-specific information stored in knowledge bases (KB) which are customized for each new domain and scenario. • Text analysis is based on pattern matching: regular expression pattern matching is a widely used strategy in the IE community. P a t t e r n matching is a form of deterministic b o t t o m - u p partial parsing. This approach has gained considerable popularity due to limitations on the accuracy of full syntactic parsers, and the adequacy of partial, semantically-constrained, parsing for this task [2, 1, 5]. Introduction The task of Information Extraction (IE) as understood in this paper is the selective extraction of meaning from free natural language text. 1 This kind of text analysis is distinguished from others in Natural Language Processing in t h a t &quot;meaning&quot; is understood in a narrow sense - in terms of a fixed set of semantic objects, namely, entities, relationships among these entities, and events in which these entities participate. These objects belong to a small number of types, all having fixed regular structure, within a fixed and closely circumscribed subject domain, which permit"
X98-1016,C96-1079,1,0.66057,"ns stored in a customizable knowledge base. Adapting an IE system to a new subject domain entails the construction of a new pattern base a time-consuming and expensive task. We describe a strategy for building patterns from examples. To a d a p t the IE system to a new domain quickly, the user chooses a set of examples in a training text, and for each example gives the logical form entries which the example induces. The system transforms these examples into patterns and then applies meta-rules to generalize these patterns. - ferences (MUCs), 2 conducted over the last decade - are described in [8, 6]. The MUCs have yielded some widely (if not universally) accepted wisdom regarding IE: • Customization and portability is an important problem: to be considered a useful tool, an IE system must be able to perform in a variety of domains. - 1 • Systems have modular design: Control is encapsulated in immutable core engines, which draw upon domain- or scenario-specific information stored in knowledge bases (KB) which are customized for each new domain and scenario. • Text analysis is based on pattern matching: regular expression pattern matching is a widely used strategy in the IE community. P a"
X98-1016,M95-1018,0,0.0697367,"Missing"
X98-1016,M98-1011,1,0.878511,"Missing"
