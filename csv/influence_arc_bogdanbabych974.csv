2004.eamt-1.3,W03-2201,1,0.926972,"tion and disambiguation of the source text – aspects which could be significantly improved by named entity recognition. We further suggest an automatic method for distinguishing and lexical differences in MT output that could have applications in automated MT evaluation for morphologically rich languages. 1. different knowledge sources, in a similar way to WSD (Stevenson and Wilks, 2001). But the cross-level nature of this problem also suggests that improvement in MT quality could be achieved through improving related aspects of the source-text analysis, such as Named Entity (NE) recognition (Babych and Hartley, 2003; Somers, 2003:524). For the purposes of this discussion, we assimilate proper nouns to NEs and investigate NE recognition as a possible solution to the PCD problem insofar as it might enable the selection of the correct strategy. Accurate NE recognition is important for the general quality of MT for the following reasons: 1. The translation of the same token may be different depending on whether the token is a common noun or part of an NE, e.g. in Russian if a common name is a part of an organization name, a “do-not-translate” or “transliterate” strategy should be used instead of a default tr"
2004.eamt-1.3,M95-1017,0,0.0475425,"Missing"
2004.eamt-1.3,2001.mtsummit-papers.68,0,0.0644277,"Missing"
2005.mtsummit-posters.13,P04-1079,1,0.83624,"Missing"
2005.mtsummit-posters.13,babych-hartley-2004-modelling,1,0.86669,"Missing"
2005.mtsummit-posters.13,J82-2006,0,0.586519,"for concepts). The difference in the degree of analytism may explain the differences in the parameters for French and Spanish whitepaper texts. It should be also noted that within a particular language ‘typological distance’ between sublanguages (or text-types) could be different: it is intuitively plausible that the colloquial style of emails in German is very different from the style of legal documents, such as the whitepaper – in terms of lexicon and syntax – and that such a distance is possibly greater than between English or French emails and the whitepaper texts in those languages (cf. Kittredge, 1982). This could provide a clue as to why there is a difference in regression parameters across text types in German, but there is no such difference in English, French or Italian. However, the most important and interesting result of our experiment is the very fact that the regression parameters do vary across text types and target languages (TLs), so they cannot be re-used for previously untested combinations of TLs/texttypes. This means that knowing the regression line parameters for a certain combination of these evaluation-external factors is not helpful for predicting human evaluation scores"
2005.mtsummit-posters.13,P02-1040,0,0.0786338,"ial 30 directed language pairs. The source texts consisted of a collection of emails and an EU whitepaper, text types given as representative of the end users’ translation inputs. The quality attribute we focused on in the human evaluations was adequacy – the extent to which the information content of the original, source text is judged to be preserved in the translation produced by the MT system. This decision reflected the projected use of the service for gisting and transactional correspondence rather than for publication. The n-gram metrics we used for the automated evaluations were BLEU (Papineni et al, 2002) developed at IBM and a weighted n-gram metric WNM (Babych, 2004). The correlations between the human judgements and the scores produced by both automated metrics were found to be highly reliable, but not in themselves sufficient for an automated score to be extrapolated to a human score. This prediction depends on two parameters of the regression line, namely target language and text type. Abstract The use of n-gram metrics to evaluate the output of MT systems is widespread. Typically, they are used in system development, where an increase in the score is taken to represent an improvement in"
2005.mtsummit-posters.13,1994.amta-1.25,0,0.841117,"Missing"
2007.mtsummit-papers.5,2005.mtsummit-osmtw.4,0,0.0172384,"compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline, which is usually the stage between more distant languages. From an engineering perspective, therefore, it is beneficial to use the best available MT system for that stage, even if there is no access to its source code. The only existing reference to an approach involving pivot-based translati"
2007.mtsummit-papers.5,C04-1016,1,0.899644,"terview and an article by a British diplomat). Table 1 presents the characteristics of the corpus. Language Texts Paras Sentences Ukrainian 35 1449 4675 Russian 35 1449 4528 English 35 1449 3513 Table 1: Parameters of MT evaluation corpus Words 64575 65181 68445 The size of our corpus is almost twice that of the DARPA 94 MT evaluation corpus of 36k words (White et al., 1994), which has been widely used for such tasks and has been shown to be sufficient for automated MT evaluation methods (e.g., BLEU) to ensure high correlation with human evaluation scores for translation adequacy and fluency (Babych et al., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni"
2007.mtsummit-papers.5,P04-1079,1,0.835977,"erisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of function words. BLEU was computed with one reference and N-gram size 5 (BLEUr1n5). The automated scores were computed for direct translation from Russian and Ukrainian into English, then fo"
2007.mtsummit-papers.5,2005.mtsummit-posters.13,1,0.882248,"both automated scores the best direct translation quality for English–Russian direction is achieved by ProMT (which is not surprising for a mainstream translation direction developed by a wellresourced Russian team working for many years). Thirdly, BLEU scores for closely related translation (ua&gt;ru) are much higher than for distant translation (ua&gt;en and ru&gt;en). Even though BLEU scores for translation into different languages (English vs. Russian) are not directly comparable – the difference in scores does not necessarily correspond to a difference in human judgment about translation quality (Babych et al., 2005) – there is still no doubt that for MT between closely related languages the number of N-gram matches between MT output and human reference is much higher, especially for longer N-grams. Interestingly, the distribution of BLEU scores for Ngrams of different length is different for MT between closely related languages and MT for distant languages. Chart 3 illustrates these distributions for N-grams N=1 to N=5. The most surprising fact is not the even greater Ngram precision for closely related translation, but the different rates of decline in precision for longer N-grams: the decline is close"
2007.mtsummit-papers.5,feldman-etal-2006-cross,0,0.0385578,"Missing"
2007.mtsummit-papers.5,A00-1002,0,0.315821,"anecdotal experience, but to our knowledge there has been no published evaluation of the actual drop in quality. The method proposed in this paper is novel in two respects. First, our pivot is closely related to the source language. Second, we use a parallel corpus to evaluate and compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline,"
2007.mtsummit-papers.5,2005.mtsummit-papers.11,0,0.0357083,"for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly when the system is applied to a slightly different domain, e.g. news (Babych et al., 2007). In this paper we investigate the performance of translation from an under-resourced language into English via a closely-related, or cognate, pivot language with well-developed translation resources. Typically any language can be used as the pivot if it covers the bridge for a language pair that is not available in a given MT system. For instance, if no system translating from French to Japanese"
2007.mtsummit-papers.5,J03-1002,0,0.00373206,"eater than for others. There are commercial systems for translation into English from well-resourced languages, such as French or Russian, that can achieve acceptable quality for many practical applications of machine translation. At the same time there are many more languages for which good quality translation resources are not available. For some of those languages MT systems have occasionally been developed, but their lexical and syntactic coverage is very far from what has been achieved for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly"
2007.mtsummit-papers.5,P02-1040,0,0.0820725,"l., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of"
2007.mtsummit-papers.5,P99-1067,0,0.0607218,"stem for translation into the related pivot. Existing research with Czech and Slovak (Hajic et al., 2000a) shows that simple transfer systems operating at the word level can produce reasonable results for closely related languages. The induction of transfer rules for closely related languages can be achieved using comparable corpora: bootstrapping from a small initial bilingual lexicon or the set of orthographic cognates, the system can identify words of the two languages that occur in contexts with a large number of words that are known mutual translations from the seed lexicon. As shown in (Rapp, 1999) this automatic procedure can produce a reliable bilingual lexicon without resorting to parallel corpora. This procedure relies on the availability of morphological resources and sufficiently large comparable corpora (of the size of 20-100 million words). The feasibility of semi-automatic acquisition of such corpora has already been demonstrated (Sharoff, 2006). Experiments with creating taggers and lemmatisers (Feldman et al., 2006) also show that it is possible to bootstrap a sufficiently accurate tagger on the basis of existing resources for cognate languages. This opens the possibility to"
2007.mtsummit-papers.5,1994.amta-1.25,0,0.213082,"Missing"
2007.tc-1.3,P98-1117,0,0.0129615,"semantic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its potential multiple senses. In such cases, the tags are arranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the performance of our system on a selection of indirect translation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It was aligned at the sentence level by JAPA (Langlais et al., 1998), and further at the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of ASSIST to find indirect translation equivalents in comparable corpora can be compared with the results of automatic alignment of parallel texts used in translation models in SMT: one of the major 6/10 advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionaries). Thus, automatically generated GIZA++ dictionaries with wor"
2007.tc-1.3,J03-1002,0,0.00770091,"tch, but also because of the paucity of suitable aligned (parallel) corpora. The approach adopted here includes the use of comparable corpora in source and target languages, i.e. corpora of texts dealing with similar subject matter and intended for similar readerships. These are relatively easy to create, by ‘harvesting’ them from the internet, for example, and there are no alignment costs. The greatest challenge is to generate a list of solutions that translators will find usable and to rank them such that the best are at the top. While ASSIST is unlike statistical machine translation (SMT – Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from 2/10 now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terms (Grefenstette, 2002) which exhibit a one-to-one correspondence irrespective of the context. ASSIST addresses difficulties with expressions from the general lexicon, whose translation is context-dependent. 2 Methodology The software acts as a decision support system for nologies for extra"
2007.tc-1.3,2001.mtsummit-papers.68,0,0.0213128,"actory condition, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the performance of a system should be studied on the basis of multiple reference translations. For the purposes of evaluating a fully automatic MT tool, the typical practice of using just two reference translations may be sufficient (Papineni, et al, 2001). However, in the context of a translator’s amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of acceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional translators, members of ITI and the Chartered Institute of Linguists. Test materials were provided in which problematic expressions were highlighted and the translators were asked to find suitable suggestions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (‘not accepta"
2007.tc-1.3,P99-1067,0,0.0508505,"create, by ‘harvesting’ them from the internet, for example, and there are no alignment costs. The greatest challenge is to generate a list of solutions that translators will find usable and to rank them such that the best are at the top. While ASSIST is unlike statistical machine translation (SMT – Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from 2/10 now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terms (Grefenstette, 2002) which exhibit a one-to-one correspondence irrespective of the context. ASSIST addresses difficulties with expressions from the general lexicon, whose translation is context-dependent. 2 Methodology The software acts as a decision support system for nologies for extracting indirect translation equivalents following subsections we give the user perspective on ogy underlying each of its sub-tasks. Explanations of vided by (Babych et al., 2007). 2.1 translators. It integrates different techfrom large comparable corpora. In the the system and describe the m"
2007.tc-1.3,rapp-2004-freely,0,0.0655345,"Missing"
2007.tc-1.3,P06-2095,1,0.807661,"the following: Children attend schools that are in poor repair and lacking basic essentials Thus ASSIST supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual contexts. 2.2 Generating and ranking translation equivalents The method for generating translation equivalents is a generalisation of one used in previous work (Sharoff et al., 2006) on extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search queries for each word and its dictionary translations with entries from thesauri automatically computed from the corpora. It then checks which combinations are possible in the TL corpus or corpora. These potential translation equivalents are now ranked by their distributional similarity to the original query and presented to the user. In this way, the range of retrievable equivalents has been extended from a relatively limited range of two-word constructions which mirror POS categ"
2009.eamt-1.6,2007.mtsummit-papers.5,1,0.845111,"Missing"
2010.jec-1.2,2005.mtsummit-posters.13,1,0.711357,"Missing"
2010.jec-1.2,2007.mtsummit-papers.5,1,0.851377,"Missing"
2010.jec-1.2,2005.mtsummit-papers.11,0,0.00355751,"ce occasional challenges from translators, and have typically safeguarded this content as confidential. Even public bodies have protected “their” taxpayer-funded translation content. There has, however, been significant recent progress on many of these points. First, large minable multilingual corpora have been released online. Some parallel multilingual content was already freely available, e.g. English-French Parliament of Canada’s Hansard (Brown et al., 1990), UN texts in six languages (Eisele and Chen, 2010), Europarl corpus, currently available for MT developers in 11 European languages (Koehn, 2005). Since the 1990s, these have been aligned on the sentence and word levels and used to build datadriven MT systems. Large-scale statistical MT platforms like Google still rely heavily on these freely available parallel corpora. The use of such parallel resources (those which are freely available in the public domain) for building MT systems does not involve serious ethical issues: all the data are produced using public funds and from the outset, it is intended they will be available for everyone to consult and use. Importantly, they also do not contain any confidential information. However, th"
2010.jec-1.2,2007.tc-1.15,0,0.033589,"Ethical Implications of Sharing Translation Resources Jo Drugan Bogdan Babych Centre for Translation Studies University of Leeds Leeds, LS2 9JT UK J.Drugan@leeds.ac.uk Centre for Translation Studies University of Leeds Leeds, LS2 9JT UK B.Babych@leeds.ac.uk Abstract The exploitation of large corpora to create and populate shared translation resources has been hampered in two areas: first, practical problems (“locked-in” data, ineffective exchange formats, client reservations); and second, ethical and legal problems. Recent developments, notably on-line collaborative translation environments (Desillets, 2007) and greater industry openness, might have been expected to highlight such issues. Yet the growing use of shared data is being addressed only gingerly. Good reasons lie behind the failure to broach the ethics of shared resources. The issues are challenging: confidentiality, ownership, copyright, authorial rights, attribution, the law, protectionism, costs, fairness, motivation, trust, quality, reliability. However, we argue that, though complex, these issues should not be swept under the carpet. The huge demand for translation cannot be met without intelligent sharing of resources (Kelly, 2009"
2010.jec-1.2,eisele-chen-2010-multiun,0,0.0116404,"ion resources has been client objection. Translation clients have viewed TM content as their property, pace occasional challenges from translators, and have typically safeguarded this content as confidential. Even public bodies have protected “their” taxpayer-funded translation content. There has, however, been significant recent progress on many of these points. First, large minable multilingual corpora have been released online. Some parallel multilingual content was already freely available, e.g. English-French Parliament of Canada’s Hansard (Brown et al., 1990), UN texts in six languages (Eisele and Chen, 2010), Europarl corpus, currently available for MT developers in 11 European languages (Koehn, 2005). Since the 1990s, these have been aligned on the sentence and word levels and used to build datadriven MT systems. Large-scale statistical MT platforms like Google still rely heavily on these freely available parallel corpora. The use of such parallel resources (those which are freely available in the public domain) for building MT systems does not involve serious ethical issues: all the data are produced using public funds and from the outset, it is intended they will be available for everyone to c"
2012.tc-1.1,P07-2002,1,0.672352,"ide-by-side with differences highlighted. Babych, Hartley, Kageura, Thomas, Utiyama 5 MNH-TT: a collaborative platform for translator training Translating and the Computer 34 29-30 November 2012, London, UK Figure 1: Dictionary lookup in QRedit MNH assumes translators work voluntarily and not upon request by customers, so the management of overall workflow in commercial settings is not explicitly provided, although it can be simulated using existing MNH functions. Translation work is carried out using the translation-aid editor QRedit, a two-pane editor which provides the following functions (Abekawa and Kageura, 2007):      lookup of dictionaries and terminologies (idiom variants are matched to their canonical forms; multi-word units and idioms are prioritised) seamless connection to bilingual corpora (TMs) seamless connection to Wikipedia monolingual and bilingual entries seamless connection to Google webpage and dictionary search registration of terms. These functions are triggered by mouse actions starting from the relevant words or phrases in the SL text area. Throughout these actions, the keyboard remains active in the TL text area in order to improve the efficiency of translation (Figure 1). The"
2012.tc-1.1,abekawa-kageura-2008-constructing,1,0.654492,"contribution. The poster may also specify their role and the object or ‘prop’ to which they are referring; these include: translation-brief, set-of-targetdocuments, research-data, glossary, tms, mt-raw-output, as well as text spans such as sentence or word. Figure 3 illustrates the bulletin board displaying a number of interactions between a project manager and volunteers signing up to play various roles in the project. Figure 3: Interaction in MNH-TT structured by dialogue act and referencing role 2.4 MNH-TT: revision categorisation The third extension provides a set of categories based on (Abekawa and Kageura, 2008; Castagnoli et al., 2006; Secara, 2005) to allow revisers to motivate and justify individual revisions (Table 3). The defined categories are grouped thematically.     content- revisions bear on the perceived transfer of ideas between the source and the target document lexis- revisions bear on the choice of words and terms grammar- revisions bear on the well-formedness of the target document text- revisions relate to departures from the conventions holding for the genre of the target document, or to clumsiness, or to a lack of cohesion. Revision categories are represented as a menu. Each t"
2012.tc-1.1,abekawa-etal-2010-community,1,0.806611,"Missing"
2012.tc-1.1,W07-0732,0,0.148105,"Missing"
2012.tc-1.1,2012.tc-1.1,1,0.0530913,"Missing"
2012.tc-1.1,2009.mtsummit-posters.22,1,0.370012,"Missing"
2012.tc-1.1,2009.tc-1.4,1,0.205718,"Missing"
2021.bsnlp-1.15,doddington-etal-2004-automatic,0,0.292153,"y recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Worth mentioning in this context is Entity Discovery and Linking (EDL) (Ji et al., 2014, 2015), a track of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of documents in multiple languages (English, Chinese, and Spanis"
2021.bsnlp-1.15,huttunen-etal-2002-diversity,1,0.607213,"ukasz Kobyli´nski, 2018, 2020). and a recent shared task on NE Recognition in Russian (Starostin et al., 2016). ing for Slavic Languages, (Piskorski et al., 2017, 2019), which, to the best of our knowledge, are the first attempts at such shared tasks covering multiple Slavic languages. High-quality recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE"
2021.bsnlp-1.15,P16-1060,0,0.0711935,"Missing"
2021.bsnlp-1.15,W19-3709,1,0.734272,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.13,0,0.0284518,"lovenian. The system uses contemporary BERT and RoBERTa multilingual pre-trained models, which include Slovene among other languages. The system was further trained on the SlavNER dataset for the NER task and used the Dedupe method for the Entity Matching task. The best performing models were pre-trained on Slovene. The results also indicate that two-step prediction of NE could be beneficial. The team made their code publicly available. The Priberam Labs system, (Ferreira et al., 2021), focuses on the NER task. It uses three components: a multilingual contextual embedding The TraSpaS system, (Suppa and Jariabka, 2021), tests the assumption that the universal open-source NLP toolkits (such as SpaCy, Stanza or Trankit) could achieve competitive performance on the Multilingual NER task, using large pretrained Transformer-based language models available from HuggingfaceTransformers, which have not been available in previous editions of the Shared Task. The team tests the generalizability of the models to new low-resourced domains, and to languages such as Slovene and Ukrainian. The UWr-VL system, (Rychlikowski et al., 2021), utilizes large collections of unstructured and structured documents for unsupervised t"
2021.bsnlp-1.15,W17-1412,1,0.890203,"jubeši´c, 2014), tools for NE recognition in Slovene (Štajner et al., 2013; Ljubeši´c et al., 2013), a Czech corpus of 11K annotated NEs (Ševˇcíková et al., 2007), NER tools for Czech (Konkol and Konopík, 2013), tools and resources for fine-grained annotation of NEs in the National Corpus of Polish (Waszczuk et al., 2010; Savary and Piskorski, 2011), NER shared tasks for Polish organized under the umbrella of POLEVAL2 evaluation campaigns (Ogrodniczuk and Łukasz Kobyli´nski, 2018, 2020). and a recent shared task on NE Recognition in Russian (Starostin et al., 2016). ing for Slavic Languages, (Piskorski et al., 2017, 2019), which, to the best of our knowledge, are the first attempts at such shared tasks covering multiple Slavic languages. High-quality recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japane"
2021.bsnlp-1.15,W02-2024,0,0.125599,"elines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Worth mentioning in this context is Entity Discovery and Linking (EDL) (Ji et al., 2014, 2015), a track of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of documents in multiple languages (English, Chinese, and Spanish), and to partition the entities into cross-document equivalence classes, by either linking mentions to a knowledge base or directly clustering them. An important difference between EDL and our ta"
2021.bsnlp-1.15,W03-0419,0,0.550493,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.9,0,0.0409736,"aptation algorithm. It also uses other techniques to improve system’s NER performance, such as marking and enrichment of uppercase tokens, prediction of NE boundaries with a multitask approach, prediction of masked tokens, fine-tuning the language model to the domain of the document. Six teams submitted descriptions of their systems as BSNLP Workshop papers. We briefly review these systems here; for complete descriptions, please see the corresponding papers. Two additional teams submitted their results with short descriptions of their systems, which appear in this section. The UL FRI system, (Prelevikj and Zitnik, 2021), generated results for several settings, models and languages, although the team’s main motivation is to develop effective NER tools for Slovenian. The system uses contemporary BERT and RoBERTa multilingual pre-trained models, which include Slovene among other languages. The system was further trained on the SlavNER dataset for the NER task and used the Dedupe method for the Entity Matching task. The best performing models were pre-trained on Slovene. The results also indicate that two-step prediction of NE could be beneficial. The team made their code publicly available. The Priberam Labs sy"
2021.bsnlp-1.15,2021.bsnlp-1.11,0,0.0823823,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.14,0,0.0748319,"Missing"
babych-etal-2004-calibrating,P02-1040,0,\N,Missing
babych-etal-2004-calibrating,2001.mtsummit-papers.3,0,\N,Missing
babych-etal-2008-generalising,rapp-2004-freely,0,\N,Missing
babych-etal-2008-generalising,W06-2405,0,\N,Missing
babych-etal-2008-generalising,P07-1018,1,\N,Missing
babych-etal-2008-generalising,P06-1085,0,\N,Missing
babych-etal-2008-generalising,P06-1011,0,\N,Missing
babych-etal-2008-generalising,P07-1084,0,\N,Missing
babych-etal-2008-generalising,2005.mtsummit-papers.11,0,\N,Missing
babych-etal-2008-generalising,sharoff-2006-uniform,1,\N,Missing
babych-hartley-2004-modelling,P02-1040,0,\N,Missing
babych-hartley-2004-modelling,2001.mtsummit-papers.3,0,\N,Missing
babych-hartley-2008-sensitivity,E06-1032,0,\N,Missing
babych-hartley-2008-sensitivity,C04-1016,1,\N,Missing
babych-hartley-2008-sensitivity,P02-1040,0,\N,Missing
babych-hartley-2008-sensitivity,W06-1606,0,\N,Missing
C04-1016,2001.mtsummit-papers.3,0,0.0253953,"kes a fairer comparison between the MT systems evaluated on different corpora. The translation complexity metric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 1 Introduction Automated evaluation tools for MT systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency. Automated metrics such as BLEU (Papineni et al., 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters. Automated evaluation is much quicker and cheaper than human evaluation. Another advantage of the scores produced by automated MT evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges may adjust their evaluation"
C04-1016,E03-1029,0,0.0221245,"Missing"
C04-1016,P02-1040,0,0.0721199,"es. The suggested approach makes a fairer comparison between the MT systems evaluated on different corpora. The translation complexity metric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 1 Introduction Automated evaluation tools for MT systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency. Automated metrics such as BLEU (Papineni et al., 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters. Automated evaluation is much quicker and cheaper than human evaluation. Another advantage of the scores produced by automated MT evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges ma"
C04-1016,2001.mtsummit-eval.6,0,0.0307525,"tric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 1 Introduction Automated evaluation tools for MT systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency. Automated metrics such as BLEU (Papineni et al., 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters. Automated evaluation is much quicker and cheaper than human evaluation. Another advantage of the scores produced by automated MT evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges may adjust their evaluation scale in order to discriminate between slightly better and slightly worse variants – but only those variants which"
C04-1016,1994.amta-1.25,0,0.796466,"Missing"
C04-1016,E03-1004,0,\N,Missing
E06-2014,bennison-bowker-2000-designing,0,0.0158976,"or a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 1 Introduction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation. It is important to populate the terminological database with terms that are missed in dictionaries or specific to a problem domain. However"
E06-2014,C02-1166,0,0.0130442,"duction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general words can vary significantly in their translation. It is important to populate the terminological database with terms that are missed in dictionaries or specific to a problem domain. However, once the translation of a term in a domain has been identified, stored in a dictionary and learned by the translator, the process of translation can go on without consulting a dictionary or a corpus. In"
E06-2014,C00-2090,0,0.0118943,"contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus. 1 Introduction It is widely acknowledged that human translators can benefit from a wide range of applications in computational linguistics, including Machine Translation (Carl and Way, 2003), Translation Memory (Planas and Furuse, 2000), etc. There have been recent research on tools detecting translation equivalents for technical vocabulary in a restricted domain, e.g. (Dagan and Church, 1997; Bennison and Bowker, 2000). The methodology in this case is based on extraction of terminology (both single and multiword units) and alignment of extracted terms using linguistic and/or statistical techniques (Déjean et al., 2002). In this project we concentrate on words from the general lexicon instead of terminology. The rationale for this focus is related to the fact that translation of terms is (should be) stable, while general wor"
E06-2014,rapp-2004-freely,0,0.0194653,"are common for concepts in both languages. The search space is further restricted by applying knowledge-based and statistical filters (such as part-of-speech and semantic class filters, IDF filter, etc), by testing the co-occurrence of members of different similarity classes or by manually selecting the presented variants. These procedures are elementary building blocks that are used in designing different search strategies efficient for different types of translation equivalents 4 Simclasses consist of words sharing collocates and are computed using Singular Value Decomposition, as used by (Rapp, 2004), e.g. Paris and Strasbourg are produced for Brussels, or bus, tram and driver for passenger. and contexts. The core functionality of the system is intended to be self-explanatory and to have a shallow learning curve: in many cases default search parameters work well, so it is sufficient to input a word or an expression in the source language in order to get back a useful list of translation equivalents, which can be manually checked by a translator to identify the most suitable solution for a given context. For example, the word combination frustrated passenger is not found in the major Engli"
L16-1581,W12-3108,0,0.0590944,"Missing"
L16-1581,babych-hartley-2008-sensitivity,1,0.803416,"great variation of otherwise more or less acceptable translation options, even for the same translator, while MT errors are often system-bound, with more or less foreseeable changes. Secondly, MT output is often below what is acceptable, for example, for post-editing tasks, while human translations usually have reasonable quality, at least acceptable for post-editing. Therefore, TQA distinguishes between translations of usable quality that would in any case require only a minimum amount of post-editing, while MT evaluation aims at distinguishing low-quality from reasonable quality MT output (Babych & Hartley, 2008). As the authors propose, automated MT evaluation metrics that compute proximity of MT output to its human gold-standard reference measure structural matches and heavily rely on the lexical level, so they tend to be insensitive to higher-level errors that are more typical for better translations. Human translations typically contain errors beyond the lexical level, to which proximity-based MT evaluation metrics are less sensitive. In spite of such gaps, automatic MT estimation can still lend some insight into automatic human translation estimation. As in automatic MT evaluation, automatic 3663"
L16-1581,eisele-chen-2010-multiun,0,0.0285594,"ic word properties, and such induced representations can be used as features in a supervised classifier(typically discriminative). In recent years, there has been an increased interest in using semantic embeddings as high-quality semantic features that embody bilingual translation equivalence across languages. The methods and rationale to train bilingual word embeddings have been well explained in Hermann & Blunsom (2014). Following the same approach, in order to train the English-Chinese word embedding model for this study, we make use of a combination of the English-Chinese part of MultiUN (Eisele & Chen, 2010) and UM corpora (Tian et al., 2014) that is roughly about 312 million tokens for English and 289 million for Chinese of ≈11million lines, with misaligned sentences eliminated from both corpora, and BiCVM 1 code to train the bilingual embedding 1 models (100 dimensions in order to save computing power and time) first and then extract the word embeddings for the source texts and all the translations from the English embedding model and Chinese embedding model respectively. For each word in the source text and its translation, we extracted a 100 dimension vector and then sum the total vectors of"
L16-1581,W12-3110,0,0.0186538,"her 11 high frequent part-of-speeches (POS) shared between the STs and TTs are selected as POS features in our experiment. We referred to Universal POS-Tagset to match pos tags in both English source text and Chinese translations in order to achieve better comparability (Petrov, Das & McDonald, 2011). As linguistically motivated features, part-of-speech related features are used as baseline features in the WMT Quality Estimation shared-task 2012 (Callison-Burch et al., 2012). For instance, number, percentage and ratio of content words and function words are extracted as linguistic features in Felice & Specia (2012). In the similar vein, POS tags were counted as shallow grammatical matches on both the source and the target (Avramidis, 2012). We have good reason that it might be contributing to the meaning transference from the source texts to the target texts. As a consequence, the researcher included frequencies of these 11 POS-tags (excluding foreign words as it is sparse) in both STs and TTs as a feature group. In both the source and the target texts. Dependencies have found their way into translation quality prediction (Fox, 2002; Owczarzak et al., 2007; Padó et al., 2009; Shah et al., 2013). In our"
L16-1581,W02-1039,0,0.0113431,"and function words are extracted as linguistic features in Felice & Specia (2012). In the similar vein, POS tags were counted as shallow grammatical matches on both the source and the target (Avramidis, 2012). We have good reason that it might be contributing to the meaning transference from the source texts to the target texts. As a consequence, the researcher included frequencies of these 11 POS-tags (excluding foreign words as it is sparse) in both STs and TTs as a feature group. In both the source and the target texts. Dependencies have found their way into translation quality prediction (Fox, 2002; Owczarzak et al., 2007; Padó et al., 2009; Shah et al., 2013). In our experiment, we extracted 28 types of target translation dependency structures, and used them in the prediction model. See Table 1 for the detailed list of monolingual features # F1-22 F23-78 F79-80 F81-82 F83-84 F85-86 F87-88 F89-90 F91-100 F101-108 2. Translation Quality Indicators Translation is subject to a continuous interaction of inner linguistic-textual factors, e.g., language norms and their constrains, and extra-linguistic factors, such as intertextuality, the translation brief, working conditions, translator’s co"
L16-1581,N13-1045,0,0.0184329,"r, Intellimetric and PaperRater, to just name a few, but it is not the case with human translation assessment, particularly student translations. A different set of technologies have been applied for automated translation quality evaluation, where a lot of attention has been paid to automatic evaluation of Machine Translation (MT), in the form of methods based on parallel corpora, for example, BLEU (Papineni et al., 2002), or MT Quality Estimation (QE), which estimates the suitability of MT output without a reference translation, QuEst (Shah et al., 2013) selects high quality MT translations (Ma & McKeown, 2013) or detects machine translation errors (Xiong, Zhang, & Li, 2010). However, automatic Translation Quality Assessment (TQA) for human translations is a more complicated problem compared to the automated evaluation of MT. One of the reasons is that machine translations, generally inferior to human translation, usually contain a much smaller range of translation errors in comparison with human translations. Human translation errors only partially overlap with errors made by MT and display greater variability, which makes human translations less predictable, see section 4.2 our pilot experiment fo"
L16-1581,P14-5010,0,0.00313667,"of this algorithm can be found in (Zhu & Ghahramani, 2002). In this study, we will use label spreading implemented on scikit-learn to see if we can improve the prediction accuracy with the majority of unlabelled data in our data set. Details will be reported in the following section. 4. Dataset and Results Chinese EFL Learners (Wen & Wang, 2008). The translations are produced by upper-intermediate level English and Non-English Majors, and thus can be viewed as trainee translators’ work. The data has been processed, sentence aligned and annotated with Stanford-Corenlp for English and Chinese (Manning et al., 2014). We extracted 165 features from both Six English source texts spanning from general texts to mildly scientific domain, with ad-hoc python scripts, plus the 200 word embedding features. Each translation text measures approximately 300-400 Chinese characters. Among the 2119 training samples, we manually scored 277 pieces of them in terms of their adequacy and fluency on a scale of 60 points (mean=38.23, interquartile range=7, range=18) for content adequacy and 40 points (mean= 27.84, interquartile range=8, range=22) for language fluency. Two Chinese native annotators, both are University Englis"
L16-1581,W07-0411,0,0.0379538,"Missing"
L16-1581,P02-1040,0,0.103953,"scoring have been sufficiently reliable to be commercialized and deployed for large international language test (Dodigovic, 2005: 104). Some well-known systems include Project Essay Grader, E2rater, Intellimetric and PaperRater, to just name a few, but it is not the case with human translation assessment, particularly student translations. A different set of technologies have been applied for automated translation quality evaluation, where a lot of attention has been paid to automatic evaluation of Machine Translation (MT), in the form of methods based on parallel corpora, for example, BLEU (Papineni et al., 2002), or MT Quality Estimation (QE), which estimates the suitability of MT output without a reference translation, QuEst (Shah et al., 2013) selects high quality MT translations (Ma & McKeown, 2013) or detects machine translation errors (Xiong, Zhang, & Li, 2010). However, automatic Translation Quality Assessment (TQA) for human translations is a more complicated problem compared to the automated evaluation of MT. One of the reasons is that machine translations, generally inferior to human translation, usually contain a much smaller range of translation errors in comparison with human translations"
L16-1581,petrov-etal-2012-universal,0,0.076107,"Missing"
L16-1581,W15-5710,1,0.768876,"ortance to it. Through many iterations, the top n predictors yielding the best performance are then selected (Kirkpatrick, 1984; Guyon et al., 2002). The experiment was performed using the Caret 2 package in R. 3.5 Label Propagation Label propagation finds communities in the real, complex networks. This algorithm, in comparison to others, has advantage in its running time, amount of priori information required, with the exception that it produces an aggregate of multiple solution instead. This approach resembles the k-NN nearest neighbours where closer data points tend to have similar labels (Rios & Sharoff, 2015). More detailed explanation of this algorithm can be found in (Zhu & Ghahramani, 2002). In this study, we will use label spreading implemented on scikit-learn to see if we can improve the prediction accuracy with the majority of unlabelled data in our data set. Details will be reported in the following section. 4. Dataset and Results Chinese EFL Learners (Wen & Wang, 2008). The translations are produced by upper-intermediate level English and Non-English Majors, and thus can be viewed as trainee translators’ work. The data has been processed, sentence aligned and annotated with Stanford-Corenl"
L16-1581,tian-etal-2014-um,0,0.0490161,"representations can be used as features in a supervised classifier(typically discriminative). In recent years, there has been an increased interest in using semantic embeddings as high-quality semantic features that embody bilingual translation equivalence across languages. The methods and rationale to train bilingual word embeddings have been well explained in Hermann & Blunsom (2014). Following the same approach, in order to train the English-Chinese word embedding model for this study, we make use of a combination of the English-Chinese part of MultiUN (Eisele & Chen, 2010) and UM corpora (Tian et al., 2014) that is roughly about 312 million tokens for English and 289 million for Chinese of ≈11million lines, with misaligned sentences eliminated from both corpora, and BiCVM 1 code to train the bilingual embedding 1 models (100 dimensions in order to save computing power and time) first and then extract the word embeddings for the source texts and all the translations from the English embedding model and Chinese embedding model respectively. For each word in the source text and its translation, we extracted a 100 dimension vector and then sum the total vectors of all words in the source text and it"
L16-1581,P10-1062,0,0.0166113,"is not the case with human translation assessment, particularly student translations. A different set of technologies have been applied for automated translation quality evaluation, where a lot of attention has been paid to automatic evaluation of Machine Translation (MT), in the form of methods based on parallel corpora, for example, BLEU (Papineni et al., 2002), or MT Quality Estimation (QE), which estimates the suitability of MT output without a reference translation, QuEst (Shah et al., 2013) selects high quality MT translations (Ma & McKeown, 2013) or detects machine translation errors (Xiong, Zhang, & Li, 2010). However, automatic Translation Quality Assessment (TQA) for human translations is a more complicated problem compared to the automated evaluation of MT. One of the reasons is that machine translations, generally inferior to human translation, usually contain a much smaller range of translation errors in comparison with human translations. Human translation errors only partially overlap with errors made by MT and display greater variability, which makes human translations less predictable, see section 4.2 our pilot experiment for reference. As a result, this non-uniformity of the creative hu"
L16-1581,P14-1006,0,\N,Missing
P04-1079,2001.mtsummit-papers.3,0,0.113156,"et al. 2002:314). Notably, Recall of weighted N-grams is found to be a good estimation of human judgements about translation Adequacy. Using weighted N-grams is essential for predicting Adequacy, since correlation of Recall for non-weighted N-grams is much lower. It is possible that other automatic methods which use human translations as a reference may also benefit from an introduction of an explicit model for term significance, since so far these methods also implicitly assume that all words are equally important in human translation, and use all of them, e.g., for measuring edit distances (Akiba et al, 2001; 2003). The weighted N-gram model has been implemented as an MT evaluation toolkit (which includes a Perl script, example files and documentation). It computes evaluation scores with tf.idf and S-score weights for translation Adequacy and Fluency. The toolkit is available at http://www.comp.leeds.ac.uk/bogdan/evalMT.html 2. Set-up of the experiment The experiment used French–English translations available in the DARPA-94 MT evaluation corpus. The corpus contains 100 French news texts (each text is about 350 words long) translated into English by 5 different MT systems: “Systran”, “Reverso”, “"
P04-1079,babych-hartley-2004-modelling,1,0.707935,"multiple human references makes automatic evaluation more expensive. However, the “significance” problem is not directly addressed by the BLEU method. On the one hand, the matched items that are present in several human references receive the same weights as items found in just one of the references. On the other hand the model of legitimate translation variation cannot fully accommodate the issue of varying degrees of “salience” for matched lexical items, since alternative synonymic translation equivalents may also be highly significant for an adequate translation from the human perspective (Babych and Hartley, 2004). Therefore it is reasonable to suggest that introduction of a model which approximates intuitions about the significance of the matched N-grams will improve the correlation between automatically computed MT evaluation scores and human evaluation scores for translation Adequacy. In this paper we present the result of an experiment on augmenting BLEU N-gram comparison with statistical weight coefficients which capture a word’s salience within a given document: the standard tf.idf measure used in the vector-space model for Information Retrieval (Salton and Leck, 1968) and the S-score proposed fo"
P04-1079,P02-1040,0,0.134687,"ates frequency weights and human intuition about translation Adequacy and Fluency. 1. Introduction Automatic methods for evaluating different aspects of MT quality – such as Adequacy, Fluency and Informativeness – provide an alternative to an expensive and time-consuming process of human MT evaluation. They are intended to yield scores that correlate with human judgments of translation quality and enable systems (machine or human) to be ranked on this basis. Several such automatic methods have been proposed in recent years. Some of them use human reference translations, e.g., the BLEU method (Papineni et al., 2002), which is based on comparison of N-gram models in MT output and in a set of human reference translations. Anthony Hartley Centre for Translation Studies University of Leeds Leeds, LS2 9JT, UK a.hartley@leeds.ac.uk However, a serious problem for the BLEU method is the lack of a model for relative importance of matched and mismatched items. Words in text usually carry an unequal informational load, and as a result are of differing importance for translation. It is reasonable to expect that the choices of right translation equivalents for certain key items, such as expressions denoting principal"
P04-1079,1994.amta-1.25,0,0.793831,"Missing"
P06-2095,P98-2127,0,0.00486332,"languages. Unlike aligned parallel corpora, comparable corpora provide a model for each individual language, while dictionaries, which can serve as a bridge, are inadequate for the task in question, because the problem we want to address involves precisely translation equivalents that are not listed there. Therefore, a specific query needs first to be generalised in order to then retrieve a suitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms re"
P06-2095,J03-1002,0,0.00684622,"Missing"
P06-2095,C00-2090,0,0.0263067,"working on an option to identify semantic contexts by means of ‘semantic signatures’ obtained from a broad-coverage semantic parser, such as USAS (Rayson et al., 2004). The semantic tagset used by USAS is a languageindependent multi-tier structure with 21 major discourse fields, subdivided into 232 sub-categories (such as I1.1- = Money: lack; A5.1- = Evaluation: bad), which can be used to detect the semantic context. Identification of semantically similar situations can be also improved by the use of segment-matching algorithms as employed in Example-Based MT (EBMT) and translation memories (Planas and Furuse, 2000; Carl and Way, 2003). The proposed model looks similar to some implementations of statistical machine translation (SMT), which typically uses a parallel corpus for its translation model, and then finds the best possible recombination that fits into the target language model (Och and Ney, 2003). Just like an MT system, our tool can find translation equivalents for queries which are not explicitly coded as entries in system dictionaries. However, from the user perspective it resembles a dynamic dictionary or thesaurus: it translates difficult words and phrases, not entire sentences. The main th"
P06-2095,rapp-2004-freely,0,0.485094,"uitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms returned using the BoW approach, cf. the results of human evaluation for English and German by (Rapp, 2004). Finding translations in comparable corpora The proposed model finds potential translation equivalents in four steps, which include 1. expansion of words in the original expression using related words; 2. translation of the resultant set using existing bilingual dictionaries; 3. further ex"
P06-2095,P04-1079,1,0.82229,"terpretation of the results The results were surprising in so far as for the majority of problems translators preferred very different translation solutions and did not agree in their scores for the same solutions. For instance, concrete plan in Table 3 received the score 1 from translator t1 and 5 from t2. In general, the translators very often picked up on different opportunities presented by the suggestions from the lists, and most suggestions were equally legitimate ways of conveying the intended content, cf. the study of legitimate translation variation with respect to the BLEU score in (Babych and Hartley, 2004). In this respect it may be unfair to compute average scores for each potential solution, since for most interesting cases the scores do not fit into the normal distribution model. So averaging scores would mask the potential usability of really inventive solutions. In this case it is more reasonable to evaluate two sets of solutions – the one generated by ASSIST and the other found in dictionaries – but not each solution individually. In order to do that for each translation problem the best scores given by each translator in each of these two sets were selected. This way of generalising data"
P06-2095,C98-2122,0,\N,Missing
P07-1018,P98-1117,0,0.0155735,"tic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its potential multiple senses. In such cases, the tags are arranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the performance of our system on a selection of indirect translation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It has been aligned on the sentence level by JAPA (Langlais et al., 1998), and further on the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of our tool to find indirect translation equivalents in comparable corpora can be compared with the results of automatic alignment of parallel texts used in translation models in SMT: one of the major advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionaries). Thus, automatically generated GIZA++ dictionaries with word a"
P07-1018,J03-1002,0,0.0109802,"ions are indirect in that they involve lexical shifts or POS transformations. Finding such translations is a hard task that can benefit from automated assistance. 'Mining' such indirect equivalents is difficult, precisely because of the structural mismatch, but also because of the paucity of suitable aligned corpora. The approach adopted here includes the use of comparable corpora in source and target languages, which are relatively easy to create. The challenge is to generate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Section 2 we present the method w"
P07-1018,2001.mtsummit-papers.68,0,0.017632,"ression плохо отремонтированные. It is also possible to translate it as unsatisfactory condition, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the performance of a system should be studied on the basis of multiple reference translations, though typically just two reference translations are used (Papineni, et al, 2001). This might be enough for the purposes of a fully automatic MT tool, but in the context of a translator's amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of acceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional translators. Problematic expressions were highlighted and the translators were asked to find suitable suggestions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (not acceptable to fully idiomatic, so 1 mean"
P07-1018,P99-1067,0,0.112298,"able aligned corpora. The approach adopted here includes the use of comparable corpora in source and target languages, which are relatively easy to create. The challenge is to generate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Section 2 we present the method we use for mining translation equivalents. In Section 3 we present the results of an objective evaluation of the quality of suggestions produced by the system by comparing our output against a parallel corpus. Finally, in Section 4 we present a subjective evaluation focusing on the integrat"
P07-1018,rapp-2004-freely,0,0.174703,"general lexicon, which do not have established equivalents, but not yet for terminology. It relies on a high-quality bilingual dictionary (en-ru ~30k, ru-en ~50K words, combining ORD and the core part of Multitran) and large comparable corpora (~200M En, ~70M Ru) of news texts. For each of the SL query terms q the system generates its dictionary translation Tr(q) and its similarity class S(q) – a set of words with a similar distribution in a monolingual corpus. Similarity is measured as the cosine between collocation vectors, whose dimensionality is reduced by SVD using the implementation by Rapp (2004). The descriptor and each word in the similarity class are then translated into the TL using ORD or the Multitran dictionary, resulting in {Tr(q)∪ Tr(S(q))}. On the TL side we also generate similarity classes, 138 but only for dictionary translations of query terms Tr(q) (not for Tr(S(q)), which can make output too noisy). We refer to the resulting set of TL words as a translation class T. T = {Tr(q) ∪ Tr(S(q)) ∪ S(Tr(q))} Translation classes approximate lexical and structural transformations which can potentially be applied to each of the query terms. Automatically computed similarity classes"
P07-1018,P06-2095,1,0.875587,"transformation. The resulting translation may be the following: Children attend schools that are in poor repair and lacking basic essentials Thus our system supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual contexts. 2.2 Generating translation equivalents We have generalised the method used in our previous study (Sharoff et al., 2006) for extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search space for each word and its dictionary translations with entries from automatically computed thesauri, and then checks which combinations are possible in target corpora. These potential translation equivalents are then ranked by their similarity to the original query and presented to the user. The range of retrievable equivalents is now extended from a relatively limited range of two-word constructions which mirror POS categories in SL and TL to a much wider set of co-occurring l"
P07-1018,P02-1040,0,\N,Missing
P07-1018,C98-1113,0,\N,Missing
P12-3016,P12-3016,1,0.0512347,"Missing"
P12-3016,I08-1013,0,0.218261,"Missing"
P12-3016,P00-1056,0,0.308786,"le. The evaluation results suggest that the comparability scores reliably reflect comparability levels. In addition, there is a strong correlation between human defined comparability levels and the confidence scores derived from the comparability metric, as the Pearson R correlation scores vary between 0.966 and 0.999, depending on the language pair. The Dictionary based metric (Su and Babych, 2012b) is a lightweight approach, which uses bilingual dictionaries to lexically map documents from one language to another. The dictionaries are automatically generated via word alignment using GIZA++ (Och and Ney, 2000) on parallel corpora. For each word in the source language, the top two translation candidates (based on the word alignment probability in GIZA++) are retrieved as possible translations into the target language. This metric provides a much faster lexical translation process, although word-for-word lexical mapping produces less reliable translations than MT based translations. Moreover, the lower quality of text translation in the dictionary based metric does not necessarily degrade its performance in predicting comparability levels of comparable document pairs. The evaluation on the gold stand"
P12-3016,P95-1050,0,0.370603,"Missing"
P12-3016,W97-0119,0,0.234424,"Missing"
P12-3016,N10-1063,0,0.0457099,". At the same time, comparable corpora, i.e., non-parallel bi- or multilingual text resources such as daily news articles and large knowledge bases like Wikipedia, are much more widely available than parallel translation data. While methods for the use of parallel corpora in machine translation are well studied (Koehn, 2010), similar techniques for comparable corpora have not been thoroughly worked out. Only the latest research has shown that language pairs and domains with little parallel data can benefit from the exploitation of comparable corpora (Munteanu and Marcu, 2005; Lu et al., 2010; Smith et al., 2010; Abdul-Rauf and Schwenk, 2009 and 2011). In this paper we present the ACCURAT toolkit1 - a collection of tools that are capable of analysing comparable corpora and extracting parallel data which can be used to improve the performance of statistical and rule/example-based MT systems. Although the toolkit may be used for parallel data acquisition for open (broad) domain systems, it will be most beneficial for under-resourced languages or specific domains which are not covered by available parallel resources. The ACCURAT toolkit produces:  comparable document pairs with comparability scores, al"
P12-3016,W08-0509,0,0.0424775,"). LEXACC requires aligned document pairs (also m to n alignments) for sentence extraction. It also allows extraction from comparable corpora as a whole; however, precision may decrease due to larger search space. LEXACC scores sentence pairs according to five lexical overlap and structural matching feature functions. These functions are combined using linear interpolation with weights trained for each language pair and direction using logistic regression. The feature functions are:  a lexical (translation) overlap score for content words (nouns, verbs, adjectives, and adverbs) using GIZA++ (Gao and Vogel, 2008) format dictionaries;  a lexical (translation) overlap score for functional words (all except content words) constrained by the content word alignment from the previous feature;  the alignment obliqueness score, a measure that quantifies the degree to which the relative positions of source and target aligned words differ;  a score indicating whether strong content word translations are found at the beginning and the end of each sentence in the given pair;  a punctuation score which indicates whether the sentences have identical sentence ending punctuation. For different language pairs, the"
P12-3016,N03-1017,0,0.00518591,"necessarily degrade its performance in predicting comparability levels of comparable document pairs. The evaluation on the gold standard shows a strong correlation (between 0.883 and 0.999) between human defined comparability levels and the confidence scores of the metric. 2.2 Parallel Sentence Extractor Comparable Corpora from Phrase-based statistical translation models are among the most successful translation models that currently exist (Callison-Burch et al., 2010). Usually, phrases are extracted from parallel corpora by means of symmetrical word alignment 93 and/or by phrase generation (Koehn et al., 2003). Our toolkit exploits comparable corpora in order to find and extract comparable sentences for SMT training using a tool named LEXACC (Ştefănescu et al., 2012). LEXACC requires aligned document pairs (also m to n alignments) for sentence extraction. It also allows extraction from comparable corpora as a whole; however, precision may decrease due to larger search space. LEXACC scores sentence pairs according to five lexical overlap and structural matching feature functions. These functions are combined using linear interpolation with weights trained for each language pair and direction using l"
P12-3016,J10-4005,0,0.0447505,"recent decades, data-driven approaches have significantly advanced the development of machine translation (MT). However, lack of sufficient bilingual linguistic resources for many languages and domains is still one of the major obstacles for further advancement of automated translation. At the same time, comparable corpora, i.e., non-parallel bi- or multilingual text resources such as daily news articles and large knowledge bases like Wikipedia, are much more widely available than parallel translation data. While methods for the use of parallel corpora in machine translation are well studied (Koehn, 2010), similar techniques for comparable corpora have not been thoroughly worked out. Only the latest research has shown that language pairs and domains with little parallel data can benefit from the exploitation of comparable corpora (Munteanu and Marcu, 2005; Lu et al., 2010; Smith et al., 2010; Abdul-Rauf and Schwenk, 2009 and 2011). In this paper we present the ACCURAT toolkit1 - a collection of tools that are capable of analysing comparable corpora and extracting parallel data which can be used to improve the performance of statistical and rule/example-based MT systems. Although the toolkit ma"
P12-3016,P06-1011,0,0.162192,"Missing"
P12-3016,W11-1205,0,0.0453047,"ducing bilingual NE or term dictionaries. The workflow also accepts pre-processed documents, thus skipping the tagging process. Since all tools use command line interfaces, task automation and workflow specification can be done with simple console/terminal scripts. All tools can be run on the Windows operating system (some are also platform independent). 1 Overview of the Workflows 2 The toolkit’s tools are integrated within two workflows (visualised in Figure 1). This section provides an overview of the main tools and methods in the toolkit. A full list of tools is described in ACCURAT D2.6. (2011). 2.1 Figure 1. Workflows of the ACCURAT toolkit. The workflow for parallel data mining from comparable corpora aligns comparable corpora in the document level (section 2.1). This step is crucial as the further steps are computationally intensive. To minimise search space, documents are aligned with possible candidates that are likely to contain parallel data. Then parallel sentence pairs are extracted from the aligned comparable corpora (section 2.2). The workflow for named entity (NE) and terminology extraction and mapping from comparable corpora extracts data in a dictionarylike format. Pro"
P12-3016,pinnis-2012-latvian,1,0.443653,"Missing"
P12-3016,su-babych-2012-development,1,0.644215,"Missing"
P12-3016,W12-0102,1,0.920916,"orkflow tags NEs or terms in all documents using 92 Tools and Methods Comparability Metrics We define comparability by how useful a pair of documents is for parallel data extraction. The higher the comparability score, the more likely two documents contain more overlapping parallel data. The methods are developed to perform lightweight comparability estimation that minimises search space of relatively large corpora (e.g., 10,000 documents in each language). There are two comparability metric tools in the toolkit: a translation based and a dictionary based metric. The Translation based metric (Su and Babych, 2012a) uses MT APIs for document translation into English. Then four independent similarity feature functions are applied to a document pair:  Lexical feature ― both documents are preprocessed (tokenised, lemmatised, and stop-words are filtered) and then vectorised. The lexical overlap score is calculated as a cosine similarity function over the vectors of two documents.  Structural feature ― the difference of sentence counts and content word counts (equally interpolated).  Keyword feature ― the cosine similarity of top 20 keywords.  NE feature ― the cosine similarity of NEs (extracted using S"
P12-3016,2012.eamt-1.37,1,0.831478,"Missing"
P12-3016,E09-1003,0,\N,Missing
P12-3016,W10-1703,0,\N,Missing
rapp-etal-2012-identifying,C10-2070,0,\N,Missing
rapp-etal-2012-identifying,sharoff-etal-2008-designing,1,\N,Missing
rapp-etal-2012-identifying,W00-0901,0,\N,Missing
rapp-etal-2012-identifying,P99-1067,1,\N,Missing
rapp-etal-2012-identifying,J05-4003,0,\N,Missing
rapp-etal-2012-identifying,A00-1031,0,\N,Missing
rapp-etal-2012-identifying,P09-1017,0,\N,Missing
sharoff-etal-2006-using,rapp-2004-freely,0,\N,Missing
sharoff-etal-2006-using,bennison-bowker-2000-designing,0,\N,Missing
sharoff-etal-2006-using,C00-2090,0,\N,Missing
sharoff-etal-2006-using,P02-1040,0,\N,Missing
sharoff-etal-2006-using,P04-1079,1,\N,Missing
sharoff-etal-2006-using,sharoff-2006-uniform,1,\N,Missing
skadina-etal-2012-collecting,W06-2810,0,\N,Missing
skadina-etal-2012-collecting,E06-1020,0,\N,Missing
skadina-etal-2012-collecting,W11-1217,0,\N,Missing
skadina-etal-2012-collecting,E09-1003,0,\N,Missing
skadina-etal-2012-collecting,W07-1702,0,\N,Missing
skadina-etal-2012-collecting,J03-3002,0,\N,Missing
skadina-etal-2012-collecting,W09-1605,0,\N,Missing
skadina-etal-2012-collecting,P02-1040,0,\N,Missing
skadina-etal-2012-collecting,J05-4003,0,\N,Missing
skadina-etal-2012-collecting,C10-1073,0,\N,Missing
skadina-etal-2012-collecting,R11-1106,0,\N,Missing
skadina-etal-2012-collecting,P07-2045,0,\N,Missing
skadina-etal-2012-collecting,P12-3016,1,\N,Missing
skadina-etal-2012-collecting,2005.mtsummit-papers.11,0,\N,Missing
skadina-etal-2012-collecting,aker-etal-2012-light,1,\N,Missing
skadina-etal-2012-collecting,su-babych-2012-development,1,\N,Missing
skadina-etal-2012-collecting,pinnis-2012-latvian,1,\N,Missing
skadina-etal-2012-collecting,C10-2054,0,\N,Missing
skadina-etal-2012-collecting,P00-1056,0,\N,Missing
su-babych-2012-development,C04-1151,0,\N,Missing
su-babych-2012-development,C02-2020,0,\N,Missing
su-babych-2012-development,J03-3002,0,\N,Missing
su-babych-2012-development,P99-1067,0,\N,Missing
su-babych-2012-development,N10-1063,0,\N,Missing
su-babych-2012-development,J05-4003,0,\N,Missing
su-babych-2012-development,C10-1073,0,\N,Missing
su-babych-2012-development,P06-2095,1,\N,Missing
su-babych-2012-development,W98-1506,0,\N,Missing
su-babych-2012-development,N09-2031,0,\N,Missing
su-babych-2012-development,P06-1011,0,\N,Missing
su-babych-2012-development,N09-1070,0,\N,Missing
su-babych-2012-development,J03-1002,0,\N,Missing
su-babych-2012-development,P07-1084,0,\N,Missing
su-babych-2012-development,P11-1133,0,\N,Missing
su-babych-2012-development,ion-2012-pexacc,0,\N,Missing
su-babych-2012-development,babych-etal-2008-generalising,1,\N,Missing
su-babych-2012-development,N04-1034,0,\N,Missing
su-babych-2012-development,P05-1045,0,\N,Missing
W03-2201,1998.amta-tutorials.5,0,\N,Missing
W03-2201,P02-1040,0,\N,Missing
W03-2201,M95-1017,0,\N,Missing
W03-2201,P02-1051,0,\N,Missing
W03-2201,white-etal-2000-determining,0,\N,Missing
W03-2201,2001.mtsummit-papers.3,0,\N,Missing
W03-2201,A97-2017,0,\N,Missing
W12-0102,babych-etal-2008-generalising,1,0.840937,"es are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relie"
W12-0102,C02-2020,0,0.0219283,"Missing"
W12-0102,P05-1045,0,0.0157052,"onaries which contain English have better word coverage as they have many more dictionary entries. 5 We use WordNet (Fellbaum, 1998) for word lemmatization. Machine translation based metrics • Named entity feature: Named entities of each document. If more named entities cooccur in two documents, they are very likely to talk about the same event or subject and 6 Available at translator-java-api/ 13 http://code.google.com/p/microsoftLanguage pair DE-EN ET-EN LT-EN LV-EN SL-EN EL-RO thus should be more comparable. We use Stanford named entity recognizer7 to extract named entities from the texts (Finkel et al., 2005). Again, cosine is then applied to measure the similarity of named entities (denoted by WN ) between a document pair. We then combine these four different types of score in an ensemble manner. Specifically, a weighted average strategy is applied: each individual score is associated with a constant weight, indicating the relative confidence (importance) of the corresponding type of score. The overall comparability score (denoted by SC) of a document pair is thus computed as below: SC = α ∗ WL + β ∗ WS + γ ∗ WK + δ ∗ WN where α, β, γ, and δ ∈ [0, 1], and α + β + γ + δ = 1. SC should be a value b"
W12-0102,W04-3208,0,0.0151402,"MT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject dom"
W12-0102,C04-1151,0,0.085241,"MT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject dom"
W12-0102,W03-1028,0,0.00891435,"cal mapping based metric takes all the words in the text into account for comparability measure, but if we only retain a small number of representative words (keywords) and discard all the other less informative words in each document, can we judge the comparability of a document pair by comparing these words? Our intuition is that, if two document share more keywords, they should be more comparable. To validate this, we then perform keyword extraction by using a simple TFIDF based approach, which has been shown effective for keyword or keyphrase extraction from the texts (Frank et al., 1999; Hulth, 2003; Liu et al., 2009). More specifically, the keyword based metric can be described as below. First, similar to the lexical mapping based metric, bilingual dictionaries are used to map non-English texts into English. Thus, only the English resources are applied for stop-word filtering and word lemmatization, which are useful text preprocessing steps for keyword extraction. We then use TFIDF to measure the weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Finally, the compa"
W12-0102,ion-2012-pexacc,0,0.0656573,"rpora. But are they also useful for other NLP tasks, such as translation equivalent detection from comparable corpora? In this section, we further measure the impact of the metrics on parallel phrase extraction (PPE) from comparable corpora. Our intuition is that, if document pairs are assigned higher comparability scores by the metrics, they should be more comparable and thus more parallel phrases can be extracted from them. The algorithm of parallel phrase extraction, which develops the approached presented in Munteanu and Marcu (2006), uses lexical overlap and structural matching measures (Ion, 2012). Taking a list of bilingual comparable document pairs as input, the extraction algorithm involves the following steps. 1. Split the source and target language documents into phrases. 2. Compute the degree of parallelism for each candidate pair of phrases by using the bilingual dictionary generated from GIZA++ (base dictionary), and retain all the phrase pairs with a score larger than a predefined parallelism threshold. 9 Remember that in our experiment, English is used as the pivot language for non-English langauge pairs. 15 3. Apply GIZA++ to the retained phrase pairs to detect new dictionar"
W12-0102,W98-1506,0,0.0418619,"extraction in comparable corpora usually assumes that the corpora they use are reliably comparable and focuses on the design of efficient extraction algorithms. Therefore, there has been very little literature discussing the characteristics of comparable corpora (Maia, 2003). In this section, we introduce some representative work which tackles comparability metrics. Some studies (Sharoff, 2007; Maia, 2003; McEnery and Xiao, 2007) analyse comparability by assessing corpus composition, such as structural criteria (e.g., format and size), and linguistic criteria (e.g., topic, domain, and genre). Kilgarriff and Rose (1998) measure similarity and homogeneity between monolingual corpora. They generate word frequency list from each corpus and then apply χ2 statistic on the most frequent n (e.g., 500) words of the compared corpora. 11 The work which deals with comparability measures in cross-lingual comparable corpora is closer to our work. Saralegi et al. (2008) measure the degree of comparability of comparable corpora (English and Basque) according to the distribution of topics and publication dates of documents. They compute content similarity for all the document pairs between two corpora. These similarity scor"
W12-0102,C10-1073,0,0.357859,"requent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research o"
W12-0102,N09-1070,0,0.0131768,"ased metric takes all the words in the text into account for comparability measure, but if we only retain a small number of representative words (keywords) and discard all the other less informative words in each document, can we judge the comparability of a document pair by comparing these words? Our intuition is that, if two document share more keywords, they should be more comparable. To validate this, we then perform keyword extraction by using a simple TFIDF based approach, which has been shown effective for keyword or keyphrase extraction from the texts (Frank et al., 1999; Hulth, 2003; Liu et al., 2009). More specifically, the keyword based metric can be described as below. First, similar to the lexical mapping based metric, bilingual dictionaries are used to map non-English texts into English. Thus, only the English resources are applied for stop-word filtering and word lemmatization, which are useful text preprocessing steps for keyword extraction. We then use TFIDF to measure the weight of words in the document and rank the words by their TFIDF weights in descending order. The top n (e.g., 30) words are extracted as keywords to represent the document. Finally, the comparability of each do"
W12-0102,P07-1084,0,0.0122438,"tically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and"
W12-0102,P06-1011,0,0.707665,"vailable, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a cle"
W12-0102,J05-4003,0,0.338474,"T aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so"
W12-0102,N04-1034,0,0.0316457,"oss-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a clearer, technologicallygrounded and quantifiable definition of comparability in the first place. In this pape"
W12-0102,P00-1056,0,0.214744,"can be used for lexical mapping between a language pair. However, unlike the language pairs in which both languages are rich-resourced (e.g., English-French, or English-Spanish) and dictionary resources are relatively easy to obtain, it is likely that bilingual dictionaries with good word coverage are not publicly available for underresourced languages (e.g., English-Slovenian, or English-Lithuanian). In order to address this problem, we automatically construct dictionaries by using word alignment on large-scale parallel corpora (e.g., Europarl and JRC-Acquis2 ). Specifically, GIZA++ toolkit (Och and Ney, 2000) with default setting is used for word alignment on the JRC-Acquis parallel corpora (Steinberger et al., 2006). The aligned word pairs together with the alignment probabilities are then converted into dictionary entries. For example, in Estonian-English language pair, the alignment example “kompanii company 0.625” in the word alignment table means the Estonian word “kompanii” can be translated as (or aligned with) the English candidate word “company” with a probability of 0.625. In the dictionary, the translation candidates are ranked by translation probability in descending order. Note that t"
W12-0102,P11-1133,0,0.0272668,"Missing"
W12-0102,P95-1050,0,0.388654,"n equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of co"
W12-0102,P99-1067,0,0.556796,"s and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability"
W12-0102,P06-2095,1,0.823651,"tly, in Rule-Based (RBMT) architectures: in SMT aligned parallel resources are used for building translation phrase tables and calculating translation probabilities; and in RBMT, they are used for automatically building bilingual dictionaries of translation equivalents and automatically deriving bilingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging"
W12-0102,N10-1063,0,0.17616,"corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguistic concepts. Research on comparable corpora needs not only good measures for comparability, but also a clearer, technologicallygrounded and quantifiable definition of comparability in the first place. In this paper we relate comparabi"
W12-0102,N09-2031,0,0.0160634,"ingual mappings for frequent structural patterns. However, large parallel resources are not always available, especially for under-resourced languages or narrow domains. Therefore, in recent years, the use of cross-lingual comparable corpora has attracted considerable attention in the MT community (Sharoff et al., 2006; Fung and Cheung, 2004a; Munteanu and Marcu, 2005; Babych et al., 2008). Most of the applications of comparable corpora focus on discovering translation equivalents to support machine translation, such as bilingual lexicon extraction (Rapp, 1995; Rapp, 1999; Morin et al., 2007; Yu and Tsujii, 2009; Li and Gaussier, 2010; Prachasson and Fung, 2011), parallel phrase extraction (Munteanu and Marcu, 2006), and parallel sentence extraction (Fung and Cheung, 2004b; Munteanu and Marcu, 2005; Munteanu et al., 2004; Smith et al., 2010). Comparability between documents is often understood as belonging to the same subject domain, genre or text type, so this definition relies on these vague linguistic concepts. The problem with this definition then is that it cannot be exactly benchmarked, since it becomes hard to relate automated measures of comparability to such inexact and unmeasurable linguist"
W12-0102,steinberger-etal-2006-jrc,0,\N,Missing
W12-0114,P07-1018,1,0.869022,"translation quality. (III) Extension to other languages: Structural similarity and translation by pivot languages is used to obtain extension to further languages: High-quality translation between closely related languages (e.g., Russian and Ukrainian or Portuguese and Spanish) can be achieved with relatively simple resources (using linguistic similarity, but also homomorphism assumptions with respect to parallel text, if available), while greater efforts are put into ensuring better-quality translation between more distant languages (e.g. German and Russian). According to our prior research (Babych et al., 2007b) the pipeline between languages of different similarity results in improved translation quality for a larger number of language pairs (e.g., MT from Portuguese or Ukrainian into German is easier if there are highquality analysis and transfer modules for Spanish and Russian into German (respectively). Of course, (III) draws heavily on the detailed analysis and MT systems that the industrial partner in HyghTra provides for a number of languages. In the following sections we give more details of the work currently done with regard to (I) and with regard to parts of (II): the creation of a new M"
W12-0114,2007.mtsummit-papers.5,1,0.957669,"Missing"
W12-0114,E06-1032,0,0.0769042,"Missing"
W12-0114,2001.mtsummit-papers.18,1,0.831384,"able and are unlikely to become available in the future. Also, SMT tends to disregard important classificatory knowledge (such as morphosyntactic, categorical and lexical class features), which can be provided and used relatively easily within non-statistical representations. On the other hand, advantages of RBMT are that its (grammar and lexical) rules and information are understandable by humans and can be exploited for a lot of applications outside of translation (dictionaries, text understanding, dialogue systems, etc.). The slot grammar approach used in Lingenio systems (cf. McCord 1989, Eberle 2001) is a prime example of such linguistically rich representations that can be used for a number of different applications. Fig.1 shows this by a visualization of (an excerpt of) the entry for the ambiguous German verb einstellen in the database that underlies (a) the Lingenio translation products, where it links up with corresponding set of the transfer rules, and (b) Lingenio’s dictionary product TranslateDict, which is primarily intended for human translators. 101 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101–112, c Avign"
W12-0114,eberle-etal-2012-tool,1,0.868581,"Missing"
W12-0114,A94-1016,0,0.083521,"ation and statistical extension and training): (a) We start out with declarative analysis and generation components of the considered languages, and with basic bilingual dictionaries connecting to one another the entries of relatively small vocabularies comprising the most frequent words of each language in a given translation pair (cf. Fig 1 a). (b) Having completed this phase, we extend the dictionaries and train the analysis-, transfer- and generation-components of the rule-based core systems using monolingual and bilingual corpora. 1 A prominent early example is Frederking and colleagues (Frederking & Nirenburg, 1994). For an overview of hybrid MT till the late nineties see Streiter et al. (1999). More recent approaches include Groves & Way (2006a, 2006b). Commercial implementations include AppTek (http://www.apptek.com) and Language Weaver (http://www.languageweaver.com). An ongoing MT important project investigating hybrid methods is EuroMatrixPlus (http://www.euromatrixplus.net/) 102 (II) Error detection and improvement cycle: (a) We automatically discover the most frequent problematic grammatical constructions and multiword expressions for commercial RBMT and SMT systems using automatic construction-ba"
W12-0114,W97-0119,0,0.0461742,"Missing"
W12-0114,J93-1004,0,0.311866,"Missing"
W12-0114,2004.eamt-1.9,0,0.0664357,"Missing"
W12-0114,2006.eamt-1.15,0,0.0619246,"Missing"
W12-0114,habash-dorr-2002-handling,0,0.0417473,"Missing"
W12-0114,J06-4003,0,0.0132216,"Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/sentrick/index.html. 104 Europarl-texts considered (as stored in our database). In order to optimize the results we use the dictionaries of step 1 as set of cognates (cf. Simard at al 1992, Gough & Way 2004), as well as other words easily obtainable from the internet that can be used for this purpose (like company names and other named entities with cross-language identity and terminology translations). Using the morphology component of the new language and the"
W12-0114,2005.mtsummit-papers.11,0,0.0671921,"Missing"
W12-0114,J99-1003,0,0.113762,"Missing"
W12-0114,J05-4003,0,0.0183273,"quire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010, 2012). Section Results"
W12-0114,P02-1038,0,0.0217034,"Missing"
W12-0114,J03-1002,0,0.00439785,"enerationoriented representations from grammar models and statistical combinatorial properties of annotated features. Step 3: Generating dictionary extensions from parallel corpora Based on parallel corpora, dictionaries can be derived using established techniques of automatic sentence alignment and word alignment. For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or – alternatively – Dan Melamed’s GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/se"
W12-0114,P02-1040,0,0.0873178,"Missing"
W12-0114,P95-1050,1,0.218313,"Missing"
W12-0114,rapp-2004-freely,1,0.85454,"Missing"
W12-0114,P99-1068,0,0.0503705,"ps. Step 1: Acquire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010,"
W12-0114,C90-3044,0,0.0668576,"Missing"
W12-0114,P06-2095,1,0.899134,"f a manually compiled kernel does not show 105 an ambiguity problem of similar significance), and as experience shows that most low frequency words in a full-size lexicon tend to be unambiguous, the ambiguity problem is reduced further for the words investigated and extracted by this comparison method. Step 5: Expanding dictionaries comparable corpora (multiword units) using In order to account for technical terms, idioms, collocations, and typical short phrases, an important feature of an MT lexicon is a high coverage of multiword units. Very recent work conducted at the University of Leeds (Sharoff et al., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja,"
W12-0114,sharoff-2006-uniform,1,0.886741,"in a machinetranslated corpus In a later work package of the project, we will run a large parallel corpus through available (competitive) MT engines, which will be enhanced by automatic dictionaries developed during the previous stages. On the source-language side of the corpus we will automatically generate lists of frequent multiword expressions (MWEs) and grammatical constructions using the methodology proposed in (Sharoff et al., 2006). For each of the identified MWEs and constructions we will generate a parallel concordance using open-source CSAR architecture developed by the Leeds team (Sharoff, 2006). The concordance will be generated by running queries to the sentencealigned parallel corpora and will return lists of corresponding sentences from gold-standard human translations and corresponding sentences generated by MT. Each of these concordances will be automatically evaluated using standard MT evaluation metrics, such as BLEU. Under these settings parallel concordances will be used as standard MT evaluation corpora in an automated MT evaluation scenario. Normally BLEU gives reliable results for MT corpora over 7000 words. However, in (Babych and Hartley, 2009; Babych and Hartley, 2008"
W12-0114,J93-1007,0,0.0374134,"., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja, 1993) • To translate a collocation, look up all its words using any dictionary • Generate all possible permutations (sequences) of the word translations • Count the occurrence frequencies of these sequences in a corpus of the target language and test for significance • Consider the most significant sequence to be the translation of the source language collocation Of course, in later steps of the project, we will experiment on filtering these sequences by exploiting structural knowledge similarly to what was described in the two previous steps. This can be obtained on the basis of the declarative an"
W12-0114,2007.mtsummit-aptme.6,0,0.11913,"Missing"
W12-0114,I05-1023,0,\N,Missing
W12-0114,P99-1067,1,\N,Missing
W12-0114,W02-0902,0,\N,Missing
W12-0114,baroni-bernardini-2004-bootcat,0,\N,Missing
W13-2801,W13-2816,0,0.0501134,"Missing"
W13-2801,W13-2813,0,0.0217301,"Missing"
W13-2801,2008.eamt-1.6,0,0.0725213,"Missing"
W13-2801,W13-2804,0,0.0365849,"Missing"
W13-2801,W13-2805,0,0.0466293,"Missing"
W13-2801,W13-2814,0,0.0251731,"d in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving transl"
W13-2801,W13-2806,0,0.0246185,"Missing"
W13-2801,W13-2807,0,0.0435616,"Missing"
W13-2801,W13-2817,0,0.0294611,"responding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the fol"
W13-2801,W13-2815,0,0.0580296,"Missing"
W13-2801,W13-2811,0,0.014586,"nd corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 2013 there are three approaches dealing with multilevel information: • Turki Khemakhem et al. (2013) present work about an English-Arabic SMT system that uses morphological decomposition and morpho-syntactic annotation of the target language and incorporates the corresponding information in a statistical feature model. Essentially, the statistical feature language model replaces words by feature arrays. 3.4 Other multilevel approaches • Pal et al. (2013) propose a combination of aligners: GIZA++, Berkeley and rule-based for English-Bengali. • Hsieh et al. (2013) use comparable corpora extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali train"
W13-2801,W13-2810,0,0.0239078,"Missing"
W13-2801,W13-2818,0,0.0263833,"extracted from Wikipedia to extract parallel fragments for the purpose of extending an English-Bengali training corpus. Semantic approaches The introduction of semantics in statistical MT has been approached to solve word sense disambiguation challenges covering the area of lexical semantics and, more recently, there have been different techniques using semantic roles covering shallow semantics, as well as the use of distributional semantics for improving translation unit selection. Approaches treating the incorporation of semantics into MT in HyTra 2013 include the following research work: • Tambouratzis et al. (2013) describe a hybrid MT architecture that uses very few bilingual corpus and a large monolingual one. The linguistic information is extracted using pattern recognition techniques. Table 1 summarizes the papers that have been presented in the Second HyTra Workshop. The papers are arranged into the table according to the linguistic level they address. • Rudnick et al. (2013) present a combination of Maximum Entropy Markov Models and HMM to perform lexical selection in the sense of cross-lingual word sense disambiguation (i.e. by choice from the set of translation alternatives). The system is meant"
W13-2801,W13-2808,0,0.058822,"Missing"
W13-2801,W10-1737,0,0.0651783,"Missing"
W13-2801,W13-2803,0,0.0495517,"Missing"
W13-2801,W13-2809,0,0.0597288,"Missing"
W13-2801,W13-2812,0,0.0196328,"often considered and represented simultaneously (not only in unification-based approaches) and the same is true for MT systems. Syntax had been addressed originally in SMT in the form of so called phrase-based SMT without any reference to linguistic structures; during 3 • Bouillon et al. (2013) presents two methodologies to correct homophone confusions. The first one is based on hand-coded rules and the second one is based on weighted graphs derived from a pronunciation resource. • Laki et al. (2013) combine pre-reordering rules with morphological and factored models for English-to-Turkish. • Li et al. (2013) propose pre-reordering rules to be used for alignment-based reordering, and corresponding POS-based restructuring of the input. Basically, they focus on taking advantage of the fact that Korean has compound words, which - for the purpose of alignment - are split and reordered similarly to Chinese. 3.5 In a number of linguistic theories information from the morphological, syntactic and semantic level is considered conjointly and merged in corresponding representations (a RBMT example is LFG (Lexical Functional Grammars) analysis and the corresponding XLE translation architecture). In HyTra 201"
W14-1014,A00-1031,0,0.0131634,"s and data. The infrastructure uses large monolingual corpora annotated by openly available part-of-speech taggers and lemmatisers, and semi-automatically derives a set of morphological and syntactic patterns for the lexical items 79 tion of texts on the one hand, and resources for language generation on the other hand. Text annotation resources, such as part-of-speech taggers, lemmatisers, parsers, chunkers – have a longer history of research and development, e.g., (Greene and Rubin, 1971), have created common standards and are more widely available in the public domain, e.g., (Schmid, 1994; Brants, 2000). In their existing form they can be applied to new languages and are more widely used in practical applications. On the other hand, generation-oriented tools are much less accessible, often propitiatory, and lack common standards and shared frameworks for integration of new languages. The predominant unidirectional textannotation focus might be explained by a historic reason that text annotation was seen as an interesting computational problem with a clearly defined evaluation procedure, which was much harder to develop for the generation tasks. The idea behind the infrastructure is that if a"
W14-1014,oostdijk-etal-2008-coi,0,0.0244543,"Missing"
W16-3402,W16-3402,1,0.0513221,"Missing"
W16-3402,C04-1016,1,0.704726,"Missing"
W16-3402,2007.mtsummit-papers.5,1,0.927994,"Missing"
W16-3402,I13-1112,0,0.240551,"Missing"
W16-3402,P14-2017,0,0.348901,"Missing"
W16-3402,W12-0114,1,0.629636,"Missing"
W16-3402,N07-2008,0,0.480504,"Missing"
W16-3402,W06-2005,0,0.756311,"Missing"
W16-3402,W02-0902,0,0.158404,"Missing"
W16-3402,2003.mtsummit-papers.32,0,0.687638,"Missing"
W16-3402,mulloni-pekar-2006-automatic,0,0.647058,"Missing"
W16-3402,W97-1102,0,0.796402,"Missing"
W16-3402,niessen-etal-2000-evaluation,0,0.384587,"Missing"
W19-3701,grefenstette-etal-2002-expanding,0,0.20994,"Missing"
W19-3701,N15-1107,0,0.295465,"2010), (Babych and Sharoff, 2016). For the experiments presented in this paper I use the most complete morphological toolkit from (Rysin and Starko, 2019), which in its current For Ukrainian there exist wide-coverage lexical resources (see Section 2), however, extending them in a traditional rule-based way would involve continuous annotation effort requiring linguistic expertise and near-native knowledge of the 2 Corpus News Wikipedia Fiction Law Total No of words 461,451,019 185,645,357 18,323,509 578,988,264 1,244,408,149 No of sent 31,021,650 15,786,948 1,811,548 29,208,302 77,828,448 as (Ahlberg et al., 2015), (Silfverberg et al., 2018), the term ‘paradigm’ is used to describe a generalised inflection pattern, which could apply to a class of words, while the term ‘inflection table’ characterises an individual system of inflection for a single word. This usage differs from the traditional understanding of the notion of a paradigm as a system of word forms for a given word, see e.g., (Spencer, 2001). In this paper I adhere to the traditional terminological usage for the term ‘paradigm’ as a system of word forms, and use the term ‘inflection tables’ referring only to tables of inflections, which may"
W19-3701,clement-etal-2004-morphology,0,0.0832705,"nerating paradigms. It can be seen from these figures that the coverage of Fiction and Law corpora is harder to improve, while News and Wiki corpora are most complementary, improving each other well. Also an interesting effect can be observed when a corpus is used to improve itself: the improvement rate peaks at the value of filtered frequencies up to 4, Discussion The algorithm proposed in this paper uses the fundamental idea that “the existence of a hypothetical lemma can be guessed if several different words found in the corpus are best interpreted as morphological variants of this lemma"" (Clément et al., 2004). This idea has been developed for automated induction of morphological lexica for different languages and implemented in practical applications such as spell checking (e.g., ispell) and information retrieval systems: (Krovetz, 1993), (Grefenstette et al., 2002), (Segalovich, 2003), (Clément et al., 2004), (Oliver and Tadi´c, 2004), (Sagot, 2005); recent work in this area uses more accurate machine learning approaches: (Šnajder, 2013), (Ljubeši´c et al., 2015). The approach proposed in this paper develops these ideas further by explicitly focussing on the following conceptual points: (1) The e"
W19-3701,D17-1074,0,0.0241339,"veloping morphological lexicons can be found in (Ahlberg et al., 2015), (Koskenniemi et al., 2018) and (Fam and Lepage, 2018). For our purposes the existing approaches can be characterised by their application scenarios and assumptions about available datasets. Interesting work has been done within the neural, supervised and semi-supervised frameworks, e.g., (Ahlberg et al., 2015), (Ahlberg et al., 2014), (Koskenniemi et al., 2018), (Silfverberg et al., 2018), (WolfSonkin et al., 2018), (Kirov and Cotterell, 2018), (Faruqui et al., 2016), (Faruqui et al., 2015), (Aharoni and Goldberg, 2016), (Cotterell et al., 2017). Much of this work assumes availability of partially labelled data, such as word paradigms and/or clean datasets, such as lists of ‘headwords’ (lemmas) from which paradigms are generated. (Fam and Lepage, 2018) identify three main approaches to learning morphological inflection: the handengineered rule-based approach, which requires much cost and time for construction, the supervised approach, which relies on initial labelled datasets and the neural approach, which needs more training time and even more data. However, for low-resource scenarios more attention need to be given to unsupervised"
W19-3701,W18-0206,0,0.0369372,"Missing"
W19-3701,Q16-1001,0,0.028352,"describes these collections. Detailed overviews of different approaches to developing morphological lexicons can be found in (Ahlberg et al., 2015), (Koskenniemi et al., 2018) and (Fam and Lepage, 2018). For our purposes the existing approaches can be characterised by their application scenarios and assumptions about available datasets. Interesting work has been done within the neural, supervised and semi-supervised frameworks, e.g., (Ahlberg et al., 2015), (Ahlberg et al., 2014), (Koskenniemi et al., 2018), (Silfverberg et al., 2018), (WolfSonkin et al., 2018), (Kirov and Cotterell, 2018), (Faruqui et al., 2016), (Faruqui et al., 2015), (Aharoni and Goldberg, 2016), (Cotterell et al., 2017). Much of this work assumes availability of partially labelled data, such as word paradigms and/or clean datasets, such as lists of ‘headwords’ (lemmas) from which paradigms are generated. (Fam and Lepage, 2018) identify three main approaches to learning morphological inflection: the handengineered rule-based approach, which requires much cost and time for construction, the supervised approach, which relies on initial labelled datasets and the neural approach, which needs more training time and even more data. Howe"
W19-3701,P18-1245,0,0.049108,"Missing"
W19-3701,R15-1050,0,0.0295842,"Missing"
W19-3701,oliver-tadic-2004-enlarging,0,0.168942,"Missing"
W19-3701,C18-1137,0,0.0575301,"Missing"
