2020.acl-main.441,D15-1075,0,0.487871,"chmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-theart models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. 1 Introduction Progress in AI has been driven by, among other things, the development of challenging large-scale benchmarks like ImageNet (Russakovsky et al., 2015) in computer vision, and SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), and others in natural language processing (NLP). Recently, for natural language understanding (NLU) in particular, the focus has shifted to combined benchmarks like SentEval (Conneau and Kiela, 2018) and GLUE (Wang et al., 2018), which track model performance on multiple tasks and provide a unified platform for analysis. With the rapid pace of advancement in AI, however, NLU benchmarks struggle to keep up with model improvement. Whereas it took around 15 years to achieve “near-human performance” on MNIST (LeCun et al., 1998; Cires¸an et al., 2012; Wan et al.,"
2020.acl-main.441,P17-1171,1,0.856888,"Missing"
2020.acl-main.441,W19-2008,0,0.0318535,"Missing"
2020.acl-main.441,L18-1269,1,0.839755,"es. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. 1 Introduction Progress in AI has been driven by, among other things, the development of challenging large-scale benchmarks like ImageNet (Russakovsky et al., 2015) in computer vision, and SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), and others in natural language processing (NLP). Recently, for natural language understanding (NLU) in particular, the focus has shifted to combined benchmarks like SentEval (Conneau and Kiela, 2018) and GLUE (Wang et al., 2018), which track model performance on multiple tasks and provide a unified platform for analysis. With the rapid pace of advancement in AI, however, NLU benchmarks struggle to keep up with model improvement. Whereas it took around 15 years to achieve “near-human performance” on MNIST (LeCun et al., 1998; Cires¸an et al., 2012; Wan et al., 2013) and approximately 7 years to surpass humans on ImageNet (Deng et al., 2009; Russakovsky et al., 2015; He et al., 2016), the GLUE benchmark did not last as long as we would have hoped after the advent of BERT (Devlin et al., 201"
2020.acl-main.441,D17-1070,1,0.886955,"Missing"
2020.acl-main.441,D19-1461,1,0.933183,"fy examples and make splits Verifier Agree Step 4: Retrain model for next round Figure 1: Adversarial NLI data collection via human-and-model-in-the-loop enabled training (HAMLET). The four steps make up one round of data collection. In step 3, model-correct examples are included in the training set; development and test sets are constructed solely from model-wrong verified-correct examples. forts that gamify collaborative training of machine learning agents over multiple rounds (Yang et al., 2017) and pit “builders” against “breakers” to learn better models (Ettinger et al., 2017). Recently, Dinan et al. (2019) showed that such an approach can be used to make dialogue safety classifiers more robust. Here, we focus on natural language inference (NLI), arguably the most canonical task in NLU. We collected three rounds of data, and call our new dataset Adversarial NLI (ANLI). Our contributions are as follows: 1) We introduce a novel human-and-model-in-the-loop dataset, consisting of three rounds that progressively increase in difficulty and complexity, that includes annotator-provided explanations. 2) We show that training models on this new dataset leads to state-of-the-art performance on a variety of"
2020.acl-main.441,W17-5401,0,0.115146,"Missing"
2020.acl-main.441,N18-1074,0,0.0577718,"Missing"
2020.emnlp-main.23,W19-3804,0,0.0370541,"nally (iv) we illustrate our classifiers’ utility for several downstream applications. All datasets, annotations, and classifiers will be released publicly to facilitate further research into the important problem of gender bias in text. 2 Related Work Gender affects myriad aspects of NLP, including corpora, tasks, algorithms, and systems (Chang et al., 2019; Costa-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan et al. (2020) in particular relied on counterfactual data to alter the training distribution to offset gender-based statistical imbalances (see §4.2 for more discussion of training set imbalances). Also relevant is Kang et al. ("
2020.emnlp-main.23,D19-2004,0,0.0314687,"aset for performing gender identification that contains utterances re-written from the perspective of a specific gender along all three dimensions, (iii) we build a suite of classifiers capable of labeling gender in both a single and multitask set up, and finally (iv) we illustrate our classifiers’ utility for several downstream applications. All datasets, annotations, and classifiers will be released publicly to facilitate further research into the important problem of gender bias in text. 2 Related Work Gender affects myriad aspects of NLP, including corpora, tasks, algorithms, and systems (Chang et al., 2019; Costa-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. ("
2020.emnlp-main.23,W19-3823,0,0.0318843,"structure. Decomposing gender into separate dimensions also allows for better identification of gender bias, which subsequently enables us to train a suite of classifiers for detecting different kinds of gender 314 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), a"
2020.emnlp-main.23,N19-1063,0,0.0906205,"oles (i.e., topics, addressees, and creators of text). 315 Finally, when investigating gender biases, one cannot ignore the intersectionality of gender identities, i.e., when gender non-additively interacts with other identity characteristics. Negative gender stereotyping is known to be alternatively weakened or reinforced by the presence of social attributes like dialect (Tatman, 2017), class (DegaetanoOrtlieb, 2018) and race (Davis, 1981; Crenshaw, 1989). These differences have been found to affect gender classification in images (Buolamwini and Gebru, 2018), and also in sentences encoders (May et al., 2019). We acknowledge that these are crucial considerations, and intend to incorporate them in future work. For a thorough survey and a critical discussion of best practices for researching social “biases” in NLP, including and beyond gender, see Blodgett et al. (2020). 3 Dimensions of Gender Bias Gender permeates language differently depending on the conversational role played by the people using that language (see Figure 1). We decompose gender bias along multiple dimensions: bias when speaking ABOUT someone, bias when speaking TO someone, and bias from speaking AS someone. This framework enables"
2020.emnlp-main.23,D17-2014,1,0.940799,"ose gender bias over sentences into semantic and/or pragmatic dimensions (about/to/as), including gender information that (i) falls outside the malefemale binary, (ii) can be contextually determined, and (iii) is statistically as opposed to explicitly gendered. In the subsequent sections, we provide details regarding the annotation of data, and details for training these classifiers. 4.1 sion of inferrable information about one or more of our dimensions, diversity in textual domain, and high quality, open-source data. The datasets are: Wikipedia, Funpedia (a less formal version of Wikipedia) (Miller et al., 2017), Wizard of Wikipedia (knowledge-based conversation) (Dinan et al., 2019c), Yelp Reviews2 , ConvAI2 (chit-chat dialogue) (Dinan et al., 2019b), ImageChat (chit-chat dialogue about an image) (Shuster et al., 2018), OpenSubtitles (dialogue from movies) (Lison and Tiedemann, 2016), and LIGHT (chit-chat fantasy dialogue) (Urbanek et al., 2019). Table 2 presents dataset statistics. Some of the datasets contain gender annotations provided by existing work. For example, classifiers trained for style transfer algorithms have previously annotated the gender of Yelp reviewers (Subramanian et al., 2018)."
2020.emnlp-main.23,L16-1147,0,0.0399975,"sequent sections, we provide details regarding the annotation of data, and details for training these classifiers. 4.1 sion of inferrable information about one or more of our dimensions, diversity in textual domain, and high quality, open-source data. The datasets are: Wikipedia, Funpedia (a less formal version of Wikipedia) (Miller et al., 2017), Wizard of Wikipedia (knowledge-based conversation) (Dinan et al., 2019c), Yelp Reviews2 , ConvAI2 (chit-chat dialogue) (Dinan et al., 2019b), ImageChat (chit-chat dialogue about an image) (Shuster et al., 2018), OpenSubtitles (dialogue from movies) (Lison and Tiedemann, 2016), and LIGHT (chit-chat fantasy dialogue) (Urbanek et al., 2019). Table 2 presents dataset statistics. Some of the datasets contain gender annotations provided by existing work. For example, classifiers trained for style transfer algorithms have previously annotated the gender of Yelp reviewers (Subramanian et al., 2018). In other datasets, we infer the gender labels. For example, in datasets where users are first assigned a persona to represent before chatting, often the gender of the persona is predetermined. In some cases gender annotations are not provided. In these cases, we sometimes impu"
2020.emnlp-main.23,W17-1609,0,0.091099,"Missing"
2020.emnlp-main.23,2020.acl-main.486,0,0.105491,"9; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan et al. (2020) in particular relied on counterfactual data to alter the training distribution to offset gender-based statistical imbalances (see §4.2 for more discussion of training set imbalances). Also relevant is Kang et al. (2019, PASTEL), which introduced a parallel style corpus and showed gains on style-transfer across binary genders. Most relevant to this work, Sap et al. (2020) proposed a framework for modeling pragmatic aspects of many social biases in text. Our work and theirs focus on complementary aspects of a larger goal—namely, making NLP safe and inclusive for everyone—but the two approaches differ in several ways. We treat statistical gender bias in human or model generated text specifically, and in detail. Sap et al. (2020) proposed a different but compatible perspective, and aimed to situate gender bias within the broader landscape of negative stereotypes in social media text, an approach that can make parallels apparent across different kinds of harmful t"
2020.emnlp-main.23,N19-1170,1,0.820569,"The gender classifiers along the TO , AS and ABOUT dimensions are trained on a variety of different existing datasets across multiple domains. We analyze which datasets are the most difficult to classify correctly 319 6.1 Controllable Generation By learning to associate control variables with textual properties, generative models can be controlled at inference time to adjust the generated text based on the desired properties of the user. This has been applied to a variety of different cases, including generating text of different lengths (Fan et al., 2018a), generating questions in chit-chat (See et al., 2019), and reducing bias (Dinan et al., 2020). Previous work in gender bias used word lists to control bias, but found that word lists were limGeneration Statistics Control Token # Gend. words Pct. masc. TO:feminine AS:feminine ABOUT:feminine Word list, feminine 246 227 1151 1158 48.0 51.0 19.72 18.22 TO:masculine AS:masculine ABOUT:masculine Word list, masculine 372 402 800 1459 75.0 71.6 91.62 94.8 Table 6: Word statistics measured on text generated from 1000 different seed utterances from ConvAI2 for each control token. We measure the number of gendered words (from a word list) that appear in th"
2020.emnlp-main.23,D19-1062,1,0.8993,"Missing"
2020.emnlp-main.23,W19-3609,0,0.0461279,", 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-speech tagger (Honnibal and Montani, 2017), and computed P (word |gender)/P (word) for words that appear more than 500 times. bias in text. We train several classifiers on publicly available data that we annotate with gender information along our dimensions. We also collect a new crowdsourced dataset (MDG ENDER) for better fine-grained evaluation of gender c"
2020.emnlp-main.23,W19-3616,0,0.0922372,"Missing"
2020.emnlp-main.23,P19-1164,0,0.0489935,"g et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-speech tagger (Honnibal and Montani, 2017), and computed P (word |gender)/P (word) for words that appear more than 500 times. bias in text. We train several classifiers on publicly available data that we annotate with gender information along our dimensions. We also collect a new crowdsourced dataset (MDG ENDER) for better fine-grained evaluation of gender classifier performance. The classifiers we train h"
2020.emnlp-main.23,2020.acl-main.484,0,0.0188642,"ate dimensions also allows for better identification of gender bias, which subsequently enables us to train a suite of classifiers for detecting different kinds of gender 314 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al"
2020.emnlp-main.23,P19-1159,0,0.018543,"n that contains utterances re-written from the perspective of a specific gender along all three dimensions, (iii) we build a suite of classifiers capable of labeling gender in both a single and multitask set up, and finally (iv) we illustrate our classifiers’ utility for several downstream applications. All datasets, annotations, and classifiers will be released publicly to facilitate further research into the important problem of gender bias in text. 2 Related Work Gender affects myriad aspects of NLP, including corpora, tasks, algorithms, and systems (Chang et al., 2019; Costa-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan"
2020.emnlp-main.23,W17-1606,0,0.0253012,"rent pragmatic dimensions than we do: they targeted negatively stereotyped commonsense implications in arguably innocuous statements, whereas we investigate pragmatic dimensions that straightforwardly map to conversational roles (i.e., topics, addressees, and creators of text). 315 Finally, when investigating gender biases, one cannot ignore the intersectionality of gender identities, i.e., when gender non-additively interacts with other identity characteristics. Negative gender stereotyping is known to be alternatively weakened or reinforced by the presence of social attributes like dialect (Tatman, 2017), class (DegaetanoOrtlieb, 2018) and race (Davis, 1981; Crenshaw, 1989). These differences have been found to affect gender classification in images (Buolamwini and Gebru, 2018), and also in sentences encoders (May et al., 2019). We acknowledge that these are crucial considerations, and intend to incorporate them in future work. For a thorough survey and a critical discussion of best practices for researching social “biases” in NLP, including and beyond gender, see Blodgett et al. (2020). 3 Dimensions of Gender Bias Gender permeates language differently depending on the conversational role pla"
2020.emnlp-main.23,N19-1064,0,0.0745267,"Missing"
2020.emnlp-main.23,N18-2003,0,0.449802,"ference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-spe"
2020.emnlp-main.23,D18-1521,0,0.466172,"ference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-spe"
2020.emnlp-main.23,D19-1531,0,0.0211547,"in a suite of classifiers for detecting different kinds of gender 314 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives"
2020.emnlp-main.23,P19-1161,0,0.0367524,"-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan et al. (2020) in particular relied on counterfactual data to alter the training distribution to offset gender-based statistical imbalances (see §4.2 for more discussion of training set imbalances). Also relevant is Kang et al. (2019, PASTEL), which introduced a parallel style corpus and showed gains on style-transfer across binary genders. Most relevant to this work, Sap et al. (2020) proposed a framework for modeling pragmatic aspects of many social biases in text. Our work and theirs focus on complementary aspects of a larger goal—namely, making NLP safe and inclusive for everyon"
2020.emnlp-main.23,E99-1021,0,\N,Missing
2020.emnlp-main.23,S12-1047,0,\N,Missing
2020.emnlp-main.23,P10-2008,0,\N,Missing
2020.emnlp-main.23,C08-1065,0,\N,Missing
2020.emnlp-main.23,W11-0310,0,\N,Missing
2020.emnlp-main.23,Q14-1029,0,\N,Missing
2020.emnlp-main.23,P16-2096,0,\N,Missing
2020.emnlp-main.23,W17-1602,0,\N,Missing
2020.emnlp-main.23,E17-1107,0,\N,Missing
2020.emnlp-main.23,P18-1082,1,\N,Missing
2020.emnlp-main.23,P19-1160,0,\N,Missing
2020.emnlp-main.23,P19-1163,0,\N,Missing
2020.emnlp-main.23,P19-2007,0,\N,Missing
2020.emnlp-main.23,P19-2031,0,\N,Missing
2020.emnlp-main.23,N19-1423,0,\N,Missing
2020.emnlp-main.23,D19-1461,1,\N,Missing
2020.emnlp-main.23,D19-1635,0,\N,Missing
2020.emnlp-main.23,D19-1179,0,\N,Missing
2020.emnlp-main.23,D19-1530,0,\N,Missing
2020.emnlp-main.23,W19-3622,0,\N,Missing
2020.emnlp-main.23,K19-1043,0,\N,Missing
2020.emnlp-main.23,2020.acl-main.647,0,\N,Missing
2020.emnlp-main.656,D19-2004,0,0.0588153,"Missing"
2020.emnlp-main.713,N16-1181,0,0.0387079,"Missing"
2020.emnlp-main.713,D18-1549,0,0.0223216,"seudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g., Guu et al., 2018), we replace each sub-question entity not in q with an entity from q of the same type (e.g., “Date” or “Location”) if possible.5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 3.2.3 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods (Artetxe et al., 2018; Lample et al., 2018), so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (2019), a 12-block transformer (Vaswani et al., 2017). We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix §B.2 for details. Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU bet"
2020.emnlp-main.713,P19-1309,0,0.0593854,"rvision, we find it possible to decompose questions in a fully unsupervised way. We propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map from the distribution of hard questions to that of many simple questions. First, we automatically create a noisy “pseudo-decomposition” for each hard question by using embedding similarity to retrieve sub-question candidates. We mine over 10M possible sub-questions from Common Crawl with a classifier, showcasing the effectiveness of parallel corpus mining, a common approach in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019), for QA. Second, we train a decomposition model on the mined data with unsupervised sequence-to-sequence learning, allowing ONUS to improve over pseudo-decompositions. As a result, we are able to train a large transformer model to generate decompositions, surpassing the fluency of heuristic/extractive decompositions. Figure 2 overviews our approach to decomposition. We validate ONUS on multi-hop QA, where questions require reasoning over multiple pieces of evidence. We use an off-the-shelf single-hop QA model to answer decomposed sub-questions. Then, we give sub-questions and their answers to"
2020.emnlp-main.713,Q17-1010,0,0.0230205,"y increases the number of single-hop questions (130K → 10.1M) and multi-hop questions (90K → 2.4M), showing the power of parallel corpus mining in QA. 2 3.2.2 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N per question (Appendix §A.1), but we found similar QA results with a fixed N = 2, which we use in the remainder for simplicity. Similarity-based Retrieval To retrieve relevant sub-questions, we embed any text t into a vector vt by summing the FastText vectors (Bojanowski et al., 2017)3 for words in t and use cosine as our similarity metric f .4 Let q be a multi-hop question ˆ be the with a pseudo-decomposition (s∗1 , s∗2 ) and v unit vector of v. Since N = 2, Eq. 1 simplifies to: h i ˆ s2 − v ˆq> v ˆs1 + v ˆq> v ˆs>1 v ˆ s2 (s∗1 , s∗2 ) = argmax v {s1 ,s2 }∈S The last term requires O(|S|2 ) comparisons, which is expensive as |S |> 10M. Instead of solving the above equation exactly, we find an approximate 0 , s0 ) by computing over pseudo-decomposition (s 1 2   S 0 = topK{s∈S} v ˆq> v ˆs with K = 1000. We efficiently build S 0 with FAISS (Johnson et al., 2017a). Random Re"
2020.emnlp-main.713,P17-1171,0,0.0844335,"Missing"
2020.emnlp-main.713,P18-1078,0,0.0373823,"hop) 7 (Baseline) 66.7 77.0±.2 63.7 65.2±.2 66.5 67.1±.5 PseudoD Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict “no answer” otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in Nie et al. (2019) on H OTPOT QA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (2018), we subtract a paragraph’s “no answer” logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(sp ) of each span sp in a paragraph p ∈ {1, . . . , P } using the predicted span logit l(sp ) and “no answer” paragraph logit n(p) with p(sp ) ∝ el(sp )−n(p) . RO BERTA LARGE (Liu et al., 2019) is used as our pretrained model. Seq2Seq Random FastText Random FastText Random FastText 78.4±.2 78.9±.2 77.7±.2 78.9±.2 79.8±.1 80.1±.2 70.9±.2 72.4±.1 69.4±.3 73.1±.2 76.0±.2 76.2±.1 70.7±.4 72.0±.1 70.0±.7 73.0"
2020.emnlp-main.713,N19-1423,0,0.0236315,"set versions: (1) the original version,9 (2) the multi-hop version from Jiang and Bansal (2019a) who created some distractor paragraphs adversarially to test multi-hop reasoning, and (3) the out-of-domain (OOD) version from Min et al. (2019b) who retrieved distractor paragraphs with the same procedure as the original version but excluded the original paragraphs. Main Results Table 1 shows how unsupervised decompositions affect QA. Our RO BERTA baseline does quite well on H OTPOT QA (77.0 F1), in line with Min et al. (2019a) who achieved strong results using a BERT-based version of the model (Devlin et al., 2019). We achieve large gains over the RO BERTA baseline by simply adding sub-questions and sub-answers to the input. Using decompositions from ONUS trained on FastText 9 Test set is private, so we randomly halve the dev set to form validation/held-out dev sets. Our codebase has our splits. 8868 QType Using Decomps. 7 X SQs SAs QA F1 7 7 77.0±.2 Bridge Comp. Inters. 1-hop 80.1±.2 73.8±.4 79.4±.6 73.9±.6 81.7±.4 80.1±.3 82.3±.5 76.9±.6 X X X X 7 Sent. Span Rand. 7 Sent. 80.1±.2 77.8±.3 76.9±.2 76.9±.2 80.2±.1 Table 2: Left: Decompositions improve QA F1 for all 4 H OTPOT QA types. Right (Ablation): Q"
2020.emnlp-main.713,E17-2068,0,0.0335695,"m large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard 8866 question. Thus, we take inspiration from parallel corpus mining in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019). We use questions from SQ UAD 2 and H OTPOT QA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with “wh”-words or end in “?” Next, we train an efficient, FastText classifier (Joulin et al., 2017) to classify between questions sampled from Common Crawl, SQ UAD 2, and H OTPOT QA (60K in total). Then, we classify our Common Crawl questions, adding those classified as SQ UAD 2 questions to S and those classified as H OTPOT QA questions to Q. Mining greatly increases the number of single-hop questions (130K → 10.1M) and multi-hop questions (90K → 2.4M), showing the power of parallel corpus mining in QA. 2 3.2.2 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N p"
2020.emnlp-main.713,P16-1004,0,0.0292627,"COMP RC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and D E COMP RC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language (Talmor and Berant, 2018; Wolfson et al., 2020). Examples include classical QA systems like SHRDLU (Winograd, 1972) and LUNAR (Woods et al., 1974), as well as neural Seq2Seq semantic parsers (Dong and Lapata, 2016) and neural module networks (Andreas et al., 2015, 2016). Such methods usually require 8871 strong, program-level supervision to generate programs, as in visual QA (Johnson et al., 2017c) and on H OTPOT QA (Jiang and Bansal, 2019b). Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by H OTPOT QA. Such an approach is taken by SAE (Tu et al., 2020) and HGN (Fang et al., 2019), whose methods may be combined with ours. Unsupervised decomposition complements strongly and weakly supervised decomposition approaches. Our unsupervised appro"
2020.emnlp-main.713,D18-1091,0,0.0218187,"Q2 What is the name of the variation on a popular anecdote? x “Mrs. Bixby and the Colonel’s Coat” is a short story by Roald Dahl that first appeared in the 1959 issue of Nugget. ˆ more than 250 million A: 4.4 GPT2 NLL Unsupervised Decomposition Model Intrinsic Evaluation of Decompositions We evaluate the quality of decompositions on other metrics aside from downstream QA. To measure the fluency of decompositions, we compute the likelihood of decompositions using the pretrained GPT-2 language model (Radford et al., 2019). We train a BERT BASE classifier on the questionwellformedness dataset of Faruqui and Das (2018), and we use the classifier to estimate the proportion of sub-questions that are well-formed. We measure how abstractive decompositions are by computing (i) the token Levenstein distance between the multihop question and its generated decomposition and (ii) the ratio between the length of the decomposition and the length of the multi-hop question. We compare ONUS to D ECOMP RC (Min et al., 2019b), a supervised+heuristic decomposition method. As shown in Table 4, ONUS decompositions are more natural and well-formed than D ECOMP RC decompositions. As an example, for Table 3 Q3, D ECOMP RC produc"
2020.emnlp-main.713,Q18-1031,0,0.0158481,"h FAISS (Johnson et al., 2017a). Random Retrieval For comparison, we test a random pseudo-decomposition baseline, where we retrieve s1 , . . . , sN by sampling uniformly from S. 2 See Appendix §A.3 for details on question classifier. 300-dim. English Common Crawl vectors: https:// fasttext.cc/docs/en/english-vectors.html 4 We also tried TFIDF and BERT representations but did not see improvements over FastText (see Appendix §A.4). 3 Editing Pseudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g., Guu et al., 2018), we replace each sub-question entity not in q with an entity from q of the same type (e.g., “Date” or “Location”) if possible.5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 3.2.3 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods (Artetxe et al., 2018; Lample et al., 2018), so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with"
2020.emnlp-main.713,P19-1262,0,0.275833,"er decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for H OTPOT QA, a standard benchmark for multi-hop QA (Yang et al., 2018), including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline (Liu et al., 2019; Min et al., 2019a) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from Min et al. (2019b), and 11 points on the multi-hop dev set from Jiang and Bansal (2019a). Our method is competitive with state-of-the-art methods SAE (Tu et al., 2020) and HGN (Fang et al., 2019) that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in H OTPOT QA, highlighting the general"
2020.emnlp-main.713,P19-1484,1,0.847698,") between predicted and gold spans. 3.2 3.2.1 2.1.2 Learning to Decompose With the above pseudo-decompositions, we explore various decomposition methods (details in §3.2.3): PseudoD We use sub-questions from pseudodecompositions directly in downstream QA. Sequence-to-Sequence (Seq2Seq) We train a Seq2Seq model pθ to maximize log pθ (d0 |q). Question Answering Task Unsupervised Decomposition Training Data and Question Mining Supervised decomposition methods are limited by the amount of available human annotation, but our unsupervised method faces no such limitation, similar to unsupervised QA (Lewis et al., 2019). Since we need to train data-hungry Seq2Seq models, we would benefit from large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard 8866 question. Thus, we take inspiration from parallel corpus mining in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019). We use questions from SQ UAD 2 and H OTPOT QA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with “w"
2020.emnlp-main.713,2021.ccl-1.108,0,0.209817,"Missing"
2020.emnlp-main.713,P19-1416,0,0.0732106,"Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significant human effort. For example, D ECOMP RC (Min et al., 2019b) decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part8864 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8864–8880, c November 16–20, 2020. 2020 Association for Computational Linguistics ? ? ? Hard Question ? Simple Question ? ? Step 1 Pseudo Decomp. ? ? ? ? Step 2 ONUS ? ? ? ? or Step 2 Seq2Seq ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2: One-to-N Unsupervised Sequence tra"
2020.emnlp-main.713,D19-1455,0,0.314797,"er decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for H OTPOT QA, a standard benchmark for multi-hop QA (Yang et al., 2018), including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline (Liu et al., 2019; Min et al., 2019a) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from Min et al. (2019b), and 11 points on the multi-hop dev set from Jiang and Bansal (2019a). Our method is competitive with state-of-the-art methods SAE (Tu et al., 2020) and HGN (Fang et al., 2019) that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in H OTPOT QA, highlighting the general"
2020.emnlp-main.713,P19-1613,0,0.175238,"Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significant human effort. For example, D ECOMP RC (Min et al., 2019b) decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part8864 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8864–8880, c November 16–20, 2020. 2020 Association for Computational Linguistics ? ? ? Hard Question ? Simple Question ? ? Step 1 Pseudo Decomp. ? ? ? ? Step 2 ONUS ? ? ? ? or Step 2 Seq2Seq ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2: One-to-N Unsupervised Sequence tra"
2020.emnlp-main.713,D19-1258,0,0.0617453,"tune a pretrained model for single-hop QA following prior work from Min et al. (2019b) on H OTPOT QA, as described below.8 7 7 7 (1hop) 7 (Baseline) 66.7 77.0±.2 63.7 65.2±.2 66.5 67.1±.5 PseudoD Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict “no answer” otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in Nie et al. (2019) on H OTPOT QA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (2018), we subtract a paragraph’s “no answer” logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(sp ) of each span sp in a paragraph p ∈ {1, . . . , P } using the predicted span logit l(sp ) and “no answer” paragraph logit n(p) with p(sp ) ∝ el(sp )−n(p) . RO BERTA LARGE (Liu et al., 2019) is used as our pretrained model. Seq2Seq Random FastText Random FastText Ra"
2020.emnlp-main.713,D17-1061,1,0.84176,"thods to leverage millions of otherwise unusable questions, similar to work on unsupervised QA (Lewis et al., 2019). When decomposition examples exist, supervised and unsupervised learning can be used in tandem to learn from both labeled and unlabeled examples. Such semi-supervised methods outperform supervised learning for tasks like machine translation (Sennrich et al., 2016). Other work on weakly supervised question generation uses a downstream QA model’s accuracy as a signal for learning to generate useful questions. Weakly supervised question generation often uses reinforcement learning (Nogueira and Cho, 2017; Wang and Lake, 2019; Strub et al., 2017; Das et al., 2017; Liang et al., 2018), where an unsupervised initialization can greatly mitigate the issues of exploring from scratch (Jaderberg et al., 2017). 7 Conclusion We proposed a QA system that answers a question via decomposition, without supervised question decompositions, using three stages: (1) decompose a question into many sub-questions using One-toN Unsupervised Sequence transduction (ONUS), (2) answer sub-questions with an off-the-shelf QA system, and (3) recompose sub-answers into a final answer. When evaluated on three H OTPOT QA dev"
2020.emnlp-main.713,D18-1051,0,0.0253497,"Question answering (QA) systems struggle to answer complex questions such as “What profession do H. L. Mencken and Albert Camus have in common?” since the required information is scattered in different places (Yang et al., 2018). However, QA systems accurately answer ∗ journalist KC was a part-time research scientist at Facebook AI Research while working on this paper. 1 Our code, data, and pretrained models are available at https://github.com/facebookresearch/ UnsupervisedDecomposition. simpler, related questions such as “What profession does H. L. Mencken have?” and “Who was Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significan"
2020.emnlp-main.713,P16-1009,0,0.155251,"tart our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (2019), a 12-block transformer (Vaswani et al., 2017). We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix §B.2 for details. Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU between generated decompositions and pseudo-decompositions. ONUS We finetune the pretrained encoderdecoder with back-translation (Sennrich et al., 2016) and denoising objectives simultaneously, similar to Lample and Conneau (2019) in unsupervised one-to-one translation.6 For denoising, we produce a noisy input d0 by randomly masking, dropping, and locally shuffling tokens in d ∼ D, and we train a model with parameters θ to maximize log pθ (d|d0 ). We likewise maximize log pθ (q|q 0 ) for a noised version q 0 of q ∼ Q. For back-translation, we generate a multihop question qˆ for a decomposition d ∼ D, and we maximize log pθ (d|ˆ q ). Similarly, we maximize ˆ for a model-generated decomposition dˆ log pθ (q|d) of q ∼ Q. To stop training without"
2020.emnlp-main.713,2020.tacl-1.13,0,0.126801,"frames subquestions as extractive spans of a question, learning to predict span-based sub-questions via supervised learning on human annotations. In other cases, D E COMP RC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and D E COMP RC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language (Talmor and Berant, 2018; Wolfson et al., 2020). Examples include classical QA systems like SHRDLU (Winograd, 1972) and LUNAR (Woods et al., 1974), as well as neural Seq2Seq semantic parsers (Dong and Lapata, 2016) and neural module networks (Andreas et al., 2015, 2016). Such methods usually require 8871 strong, program-level supervision to generate programs, as in visual QA (Johnson et al., 2017c) and on H OTPOT QA (Jiang and Bansal, 2019b). Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by H OTPOT QA. Such an approach is taken by SAE (Tu et al., 2020) and HGN (Fang et al.,"
2020.emnlp-main.713,P02-1040,0,\N,Missing
2020.emnlp-main.713,D17-1319,0,\N,Missing
2020.emnlp-main.713,D18-1259,0,\N,Missing
2021.acl-long.132,S19-2007,0,0.0228644,"Missing"
2021.acl-long.132,Q18-1041,0,0.0416442,"Missing"
2021.acl-long.132,2020.lrec-1.760,0,0.0915432,"Missing"
2021.acl-long.132,P19-1271,0,0.0463559,"Missing"
2021.acl-long.132,2020.alw-1.17,0,0.0284502,"Missing"
2021.acl-long.132,2021.ccl-1.108,0,0.0932551,"Missing"
2021.acl-long.132,2020.acl-main.441,1,0.784199,"2019a). Dynamic benchmarking and contrast sets Addressing the numerous flaws of hate detection models is a difficult task. The problem may partly lie in the use of static benchmark datasets and fixed model evaluations. In other areas of Natural Language Processing, several alternative model training and dataset construction paradigms have been presented, involving dynamic and iterative approaches. In a dynamic dataset creation setup, annotators are incentivised to produce high-quality ‘adversarial’ samples which are challenging for baseline models, repeating the process over multiple rounds (Nie et al., 2020). This offers a more targeted way of collecting data. Dinan et al. (2019) ask crowd-workers to ‘break’ a BERT model trained to identify toxic comments and then retrain it using the new examples. Their final model is more robust to complex forms of offensive content, such as entries with figurative language and without profanities. Another way of addressing the limitations of static datasets is through creating ‘contrast sets’ of perturbations (Kaushik et al., 2019; Gardner et al., 2020). By making minimal label-changing modifications that preserve ‘lexical/syntactic artifacts present in the or"
2021.acl-long.132,2020.acl-main.442,0,0.0845178,"Missing"
2021.acl-long.132,W17-1101,0,0.0652497,"Missing"
2021.acl-long.132,K19-1088,0,0.034509,"Missing"
2021.acl-long.132,2020.alw-1.19,1,0.823225,"Missing"
2021.acl-long.132,N16-2013,1,0.802321,"y different schemas. The hierarchical taxonomy we present aims for a balance between granularity versus conceptual distinctiveness and annotation simplicity, following the guidance of Nickerson et al. (2013). All entries are assigned to either ‘Hate’ or ‘Not Hate’. ‘Hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” (Warner and Hirschberg, 2012). For ‘Hate’, we also annotate secondary labels for the type and target of hate. The taxonomy for the type of hate draws on and extends previous work, including Waseem and Hovy (2016); Vidgen et al. (2019a); Zampieri et al. (2019). 3.1 Derogation Content which explicitly attacks, demonizes, demeans or insults a group. This resembles similar definitions from Davidson et al. (2017), who define hate as content that is ‘derogatory’, Waseem and Hovy (2016) who include ‘attacks’ in their definition, and Zampieri et al. (2019) who include ‘insults’. Animosity Content which expresses abuse against a group in an implicit or subtle manner. It is similar to the ‘implicit’ and ‘covert’ categories used in other taxonomies (Waseem et al., 2017; Vidgen and Yasseri, 2019; Kumar et al., 20"
2021.acl-long.132,N19-1060,0,0.0448043,"Missing"
2021.acl-long.132,W19-3509,1,0.797866,"hierarchical taxonomy we present aims for a balance between granularity versus conceptual distinctiveness and annotation simplicity, following the guidance of Nickerson et al. (2013). All entries are assigned to either ‘Hate’ or ‘Not Hate’. ‘Hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” (Warner and Hirschberg, 2012). For ‘Hate’, we also annotate secondary labels for the type and target of hate. The taxonomy for the type of hate draws on and extends previous work, including Waseem and Hovy (2016); Vidgen et al. (2019a); Zampieri et al. (2019). 3.1 Derogation Content which explicitly attacks, demonizes, demeans or insults a group. This resembles similar definitions from Davidson et al. (2017), who define hate as content that is ‘derogatory’, Waseem and Hovy (2016) who include ‘attacks’ in their definition, and Zampieri et al. (2019) who include ‘insults’. Animosity Content which expresses abuse against a group in an implicit or subtle manner. It is similar to the ‘implicit’ and ‘covert’ categories used in other taxonomies (Waseem et al., 2017; Vidgen and Yasseri, 2019; Kumar et al., 2018). Threatening lang"
2021.acl-long.132,N19-1144,0,0.0222695,"we present aims for a balance between granularity versus conceptual distinctiveness and annotation simplicity, following the guidance of Nickerson et al. (2013). All entries are assigned to either ‘Hate’ or ‘Not Hate’. ‘Hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” (Warner and Hirschberg, 2012). For ‘Hate’, we also annotate secondary labels for the type and target of hate. The taxonomy for the type of hate draws on and extends previous work, including Waseem and Hovy (2016); Vidgen et al. (2019a); Zampieri et al. (2019). 3.1 Derogation Content which explicitly attacks, demonizes, demeans or insults a group. This resembles similar definitions from Davidson et al. (2017), who define hate as content that is ‘derogatory’, Waseem and Hovy (2016) who include ‘attacks’ in their definition, and Zampieri et al. (2019) who include ‘insults’. Animosity Content which expresses abuse against a group in an implicit or subtle manner. It is similar to the ‘implicit’ and ‘covert’ categories used in other taxonomies (Waseem et al., 2017; Vidgen and Yasseri, 2019; Kumar et al., 2018). Threatening language Content which express"
2021.acl-long.132,W16-5618,1,0.795048,"Missing"
2021.acl-long.132,W17-3012,1,0.825877,"on and extends previous work, including Waseem and Hovy (2016); Vidgen et al. (2019a); Zampieri et al. (2019). 3.1 Derogation Content which explicitly attacks, demonizes, demeans or insults a group. This resembles similar definitions from Davidson et al. (2017), who define hate as content that is ‘derogatory’, Waseem and Hovy (2016) who include ‘attacks’ in their definition, and Zampieri et al. (2019) who include ‘insults’. Animosity Content which expresses abuse against a group in an implicit or subtle manner. It is similar to the ‘implicit’ and ‘covert’ categories used in other taxonomies (Waseem et al., 2017; Vidgen and Yasseri, 2019; Kumar et al., 2018). Threatening language Content which expresses intention to, support for, or encourages inflicting harm on a group, or identified members of the group. This category is used in datasets by Hammer (2014), Golbeck et al. (2017) and Anzovino et al. (2018). Support for hateful entities Content which explicitly glorifies, justifies or supports hateful actions, events, organizations, tropes and individuals (collectively, ‘entities’). Dehumanization Content which ‘perceiv[es] or treat[s] people as less than human’ (Haslam and Stratemeyer, 2016). It often"
2021.acl-long.134,D15-1075,0,0.308314,"stand what they are saying at all (Marcus, 2018). From a listener’s perspective, such inconsistent bots fail to gain user trust and their long-term communication confidence. From a speaker’s perspective, it violates the maxim of quality in Grice’s cooperative principles (Grice, 1975) —”Do not say what you believe to be false.” Hence, efforts on reducing contradicting or inconsistent conversations by open-domain chatbots are imperative. Prior works (Welleck et al., 2019) characterized the modeling of persona-related consistency as a natural language inference (NLI) problem (Dagan et al., 2005; Bowman et al., 2015), and constructed a dialog NLI dataset based on Persona-Chat (Zhang et al., 2018), but so far state-of-the-art chatbots (Roller et al., 2020) have not been able to make use of NLI techniques in improving dialogue consistency. Overall, the challenge remains that we are still unable to answer the simple yet important question—“how good are we at modeling consistency (including persona, logic, causality, etc.) in a general conversation?”. The inability to measure this obscures to what degree building new modules or techniques can in turn help prevent contradicting responses during generation. See"
2021.acl-long.134,N19-1423,0,0.164167,"se aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and outof-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. 1 Introduction Recent progress on neural approaches to natural language processing (Devlin et al., 2019; Brown et al., 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020) have triggered a resurgent interest on building intelligent open-domain chatbots. Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al., 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation. While the success is ind"
2021.acl-long.134,W19-4103,0,0.0269653,"roving the consistency of state-of-the-art generative chatbots. 2 Related Work Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as Blen"
2021.acl-long.134,W19-3646,0,0.160375,"roving the consistency of state-of-the-art generative chatbots. 2 Related Work Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as Blen"
2021.acl-long.134,P18-1082,0,0.0525208,"Missing"
2021.acl-long.134,2020.acl-main.428,1,0.895103,"iction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as BlenderBot with up to 9.4B parameter Transformers (Roller et al., 2020). More similar to our work is utilizing NLI models in dialogue consistency. Dziri et al. (2019b) attempted to use entailment models trained on syn1700 thetic datasets for dialogue topic coherence evaluation. Particularly, Welleck et al. (2019) constructed the dialogue NLI dataset and (Li et al., 2020) utilized it to try to reduce inconsistency in generative models via unlikelihood training in a preliminary study that reports perplexity results, but did not measure actual generations or contradiction rates. We note that the dialogue NLI dataset is only semi-automatically generated, with limited coverage of only Persona-chat data (Zhang et al., 2018), whereas our DECODE is human-written and across multiple domains. Our task also involves logical and context-related reasoning beyond personal facts. We show that transfer of DECODE is subsequently more robust than dialogue NLI on both human-hum"
2021.acl-long.134,2021.ccl-1.108,0,0.0674875,"Missing"
2021.acl-long.134,D17-2014,1,0.889652,"Missing"
2021.acl-long.134,2020.acl-main.441,1,0.83802,"ANLI-R3, DECODE. “All DNLI” denotes all the datasets with DNLI removed. BART (Lewis et al., 2020). They represent the start-of-the-art language representation models and have yielded successes in many NLU tasks. The input format of fθ follows how these models handle sequence-pairs (C and u) for classification tasks with padding, separator and other special tokens such as position embeddings and segment features inserted at designated locations accordingly. We fine-tune fθ on different combinations of NLI training data including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), ANLIR3 (Nie et al., 2020a)4 , DNLI (Welleck et al., 2019), as well as our DECODE Main training set. We convert the 3-way labels of the examples in existing NLI datasets to 2-way, as described before, and θ is optimized using cross-entropy loss. When training fθU B in the utterance-based approach using the DECODE training set, the input sequences 4 ANLI data is collected in three rounds resulting in three subsets (R1, R2, R3). We only used training data in R3 since it contains some dialogue-related examples. 1703 5 5.1 Results and Analysis Performance on Constructed Dataset Our main results comparing various detectors"
2021.acl-long.134,2020.emnlp-main.734,1,0.806327,"Missing"
2021.acl-long.186,S18-2005,0,0.16288,"ained for each sentence. On Dynabench, we explore conditions with and without prompt sentences that workers can edit to achieve their goal. Introduction Sentiment analysis is an early success story for NLP, in both a technical and an industrial sense. It has, however, entered into a more challenging phase for research and technology development: while present-day models achieve outstanding results on all available benchmark tasks, they still fall short when deployed as part of real-world systems (Burn-Murdoch, 2013; Grimes, 2014, 2017; Gossett, 2020) and display a range of clear shortcomings (Kiritchenko and Mohammad, 2018; Hanwen Shen et al., 2018; Wallace et al., 2019; Tsai et al., 2019; Jin et al., 2019; Zhang et al., 2020). In this paper, we seek to address the gap between benchmark results and actual utility by introduc∗ Model 0 RoBERTa fine-tuned on sentiment benchmarks ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.1 DynaSent is intended to be a dynamic benchmark that expands in response to new models, new modeling goals, and new adversarial attacks. We present the first two rounds here and motivate some specific data collection and modeli"
2021.acl-long.186,2020.acl-main.441,1,0.910072,"roduc∗ Model 0 RoBERTa fine-tuned on sentiment benchmarks ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.1 DynaSent is intended to be a dynamic benchmark that expands in response to new models, new modeling goals, and new adversarial attacks. We present the first two rounds here and motivate some specific data collection and modeling choices, and we propose that, when future models solve these rounds, we use those models to create additional DynaSent rounds. This is an instance of “the ‘moving post’ dynamic target” for NLP that Nie et al. (2020) envision. Figure 1 summarizes our method, which incorporates both naturally occurring sentences and sentences created by crowdworkers with the goal of fooling a top-performing model. The starting point is Model 0, which is trained on standard sentiment 1 Equal contribution. https://github.com/cgpotts/dynasent 2388 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2388–2404 August 1–6, 2021. ©2021 Association for Computational Linguistics benchmarks and used to find challengi"
2021.acl-long.186,N18-2002,0,0.0380916,"Missing"
2021.acl-long.186,P19-1163,0,0.013625,"grad, 1972; Levesque, 2013) have risen to prominence in response to the sense that benchmark results are over-stating the quality of the models we are developing (Linzen, 2020). These efforts seek to determine whether models have met specific learning targets (Alzantot et al., 2018; Glockner et al., 2018; Naik et al., 2018; Nie et al., 2019), exploit relatively superficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al., 2019; Schuster et al., 2019). For the most part, challenge and adversarial datasets are meant to be used primarily for evaluation (though Liu et al. (2019a) show that even small amounts of training on them can be fruitful in some scenarios). However, there are existing adversarial datasets that are large enough to support full-scale training efforts (Zellers et al., 2018, 2019; Chen et al., 2019; Dua et al., 2019; Bartolo et al., 2020). DynaSent falls into this class; it has large train sets that can support from-scratch training as well as fine-tuning. Our approach is closest to, and directly ins"
2021.acl-long.186,P04-1035,0,0.0586021,"ion 5.3), which favors our strategy of training models from scratch for each round. 2 Related Work Sentiment analysis was one of the first natural language understanding tasks to be revolutionized by data-driven methods. Rather than trying to survey the field (see Pang and Lee 2008; Liu 2012; Grimes 2014), we focus on the benchmark tasks that have emerged in this space, and then seek to situate these benchmarks with respect to challenge (adversarial) datasets and crowdsourcing methods. 2.1 Sentiment Benchmarks Many sentiment datasets are derived from customer reviews of products and services (Pang and Lee, 2004, 2005; Socher et al., 2013; Maas et al., 2011; Jindal and Liu, 2008; Ni et al., 2019; McAuley et al., 2012; Zhang et al., 2015). This is an appealing source of data, since such texts are accessible and abundant in many languages and regions of the world, and they tend to come with their own authorprovided labels (star ratings). On the other hand, over-reliance on such texts is likely also limiting progress; DynaSent begins moving away from such texts, though it remains rooted in this domain. Not all sentiment benchmarks are based in review texts. The MPQA Opinion Corpus of Wiebe et al. (2005)"
2021.acl-long.186,D19-1341,0,0.0210931,"ue, 2013) have risen to prominence in response to the sense that benchmark results are over-stating the quality of the models we are developing (Linzen, 2020). These efforts seek to determine whether models have met specific learning targets (Alzantot et al., 2018; Glockner et al., 2018; Naik et al., 2018; Nie et al., 2019), exploit relatively superficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al., 2019; Schuster et al., 2019). For the most part, challenge and adversarial datasets are meant to be used primarily for evaluation (though Liu et al. (2019a) show that even small amounts of training on them can be fruitful in some scenarios). However, there are existing adversarial datasets that are large enough to support full-scale training efforts (Zellers et al., 2018, 2019; Chen et al., 2019; Dua et al., 2019; Bartolo et al., 2020). DynaSent falls into this class; it has large train sets that can support from-scratch training as well as fine-tuning. Our approach is closest to, and directly inspired by, the Adversaria"
2021.acl-long.186,P05-1015,0,0.40513,"Missing"
2021.acl-long.186,P16-1162,0,0.0110857,"Missing"
2021.acl-long.186,D08-1027,0,0.197074,"Missing"
2021.acl-long.186,Q19-1043,0,0.0242673,"e highest-rated workers. This led to a substantial increase in dataset quality by removing a lot of labels that seemed to us to be randomly assigned. Appendix B describes the process in more detail, and our Datasheet enumerates the known unwanted biases that this process can introduce. 3.4 Round 1 Dataset The Round 1 dataset is summarized in Table 5, and Table 4 gives randomly selected short examples. Because each sentence has five ratings, there are two perspectives we can take on the dataset: Distributional Labels We can repeat each example with each of its labels (de Marneffe et al., 2012; Pavlick and Kwiatkowski, 2019). For instance, the first sentence in Table 4 would be repeated three times with ‘Mixed’ as the label and twice with ‘Negative’. For many classifier models, this reduces to labeling each example with its probability distribution over the labels. This is an appealing approach to creating training data, since it allows us to make use of all the examples,4 even those that do not have a majority label, and it allows us to make maximal use of the labeling information. In our experiments, we found that training on the distributional labels consistently led to slightly better 4 For ‘Mixed’ labels, we"
2021.acl-long.186,S18-2023,0,0.0519742,"Missing"
2021.acl-long.186,D13-1170,1,0.0189406,"s the labels Positive, Negative, and Neutral. This is a minimal expansion of the usual binary (Positive/Negative) sentiment task, but a crucial one, as it avoids the false presupposition that all texts convey binary sentiment. We chose this version of the problem to show that even basic sentiment analysis poses substantial challenges for our field. 2 https://dynabench.org/ We find that the Neutral category is especially difficult. While it is common to synthesize such a category from middle-scale product and service reviews, we use an independent validation of the Stanford Sentiment Treebank (Socher et al., 2013) dev set to argue that this tends to blur neutrality together with mixed sentiment and uncertain sentiment (Section 5.2). DynaSent can help tease these phenomena apart, since it already has a large number of Neutral examples and a large number of examples displaying substantial variation in validation. Finally, we argue that the variable nature of the Neutral category is an obstacle to fine-tuning (Section 5.3), which favors our strategy of training models from scratch for each round. 2 Related Work Sentiment analysis was one of the first natural language understanding tasks to be revolutioniz"
2021.acl-long.186,S17-2088,0,0.0621535,"Missing"
2021.acl-long.186,W19-4824,0,0.0476531,"Missing"
2021.acl-long.186,W17-1609,0,0.065098,"Missing"
2021.acl-long.186,L18-1239,0,0.0564547,"Missing"
2021.acl-long.186,D19-1221,0,0.0131273,"ns with and without prompt sentences that workers can edit to achieve their goal. Introduction Sentiment analysis is an early success story for NLP, in both a technical and an industrial sense. It has, however, entered into a more challenging phase for research and technology development: while present-day models achieve outstanding results on all available benchmark tasks, they still fall short when deployed as part of real-world systems (Burn-Murdoch, 2013; Grimes, 2014, 2017; Gossett, 2020) and display a range of clear shortcomings (Kiritchenko and Mohammad, 2018; Hanwen Shen et al., 2018; Wallace et al., 2019; Tsai et al., 2019; Jin et al., 2019; Zhang et al., 2020). In this paper, we seek to address the gap between benchmark results and actual utility by introduc∗ Model 0 RoBERTa fine-tuned on sentiment benchmarks ing version 1 of the DynaSent dataset for Englishlanguage ternary (positive/negative/neutral) sentiment analysis.1 DynaSent is intended to be a dynamic benchmark that expands in response to new models, new modeling goals, and new adversarial attacks. We present the first two rounds here and motivate some specific data collection and modeling choices, and we propose that, when future mod"
2021.acl-long.186,P19-1472,0,0.0433071,"Missing"
2021.acl-long.186,H89-1033,0,0.771875,"MPQA Opinion Corpus of Wiebe et al. (2005) contains news articles labeled at the phrase-level for a variety of subjective states; it presents an exciting vision for how sentiment analysis might become more multidimensional. SemEval 2016 and 2017 (Nakov et al., 2016; Rosenthal et al., 2017) offered Twitter-based sentiment datasets. And of course there are numerous additional datasets for specific languages, domains, and emotional dimensions; Google’s Dataset Search currently reports over 100 datasets for sentiment. 2389 2.2 Challenge and Adversarial Datasets Challenge and adversarial datasets (Winograd, 1972; Levesque, 2013) have risen to prominence in response to the sense that benchmark results are over-stating the quality of the models we are developing (Linzen, 2020). These efforts seek to determine whether models have met specific learning targets (Alzantot et al., 2018; Glockner et al., 2018; Naik et al., 2018; Nie et al., 2019), exploit relatively superficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al."
2021.acl-long.186,D18-1009,0,0.0205107,"uperficial properties of their training data, (Jia and Liang, 2017; Kaushik and Lipton, 2018; Zhang et al., 2020), or inherit social biases in the data they were trained on (Kiritchenko and Mohammad, 2018; Rudinger et al., 2017, 2018; Sap et al., 2019; Schuster et al., 2019). For the most part, challenge and adversarial datasets are meant to be used primarily for evaluation (though Liu et al. (2019a) show that even small amounts of training on them can be fruitful in some scenarios). However, there are existing adversarial datasets that are large enough to support full-scale training efforts (Zellers et al., 2018, 2019; Chen et al., 2019; Dua et al., 2019; Bartolo et al., 2020). DynaSent falls into this class; it has large train sets that can support from-scratch training as well as fine-tuning. Our approach is closest to, and directly inspired by, the Adversarial NLI (ANLI) project, which is reported on by Nie et al. (2020) and which continues on Dynabench. In ANLI, human annotators construct new examples that fool a top-performing model but make sense to other human annotators. This is an iterative process that allows the annotation project itself to organically find phenomena that fool current mode"
2021.acl-long.331,2020.emnlp-main.19,0,0.0423297,"that modern NLP models are able to function with different numbers of layers for different examples (Elbayad et al., 2019; Fan et al., 2019; He et al., 2021); that different layers specialize for different purposes (Zhang et al., 2019); that layers can be compressed (Li et al., 2020; Zhu et al., 2019; Shen et al., 2020; Sun et al., 2020); and, that layers can be reordered (Press et al., 2019). There is a growing body of work in efficient self-attention networks (Tay et al., 2020b), such as linear attention (Wang et al., 2020), on how to process long context information (Beltagy et al., 2020; Ainslie et al., 2020) and on approximations to make transformers more scalable (Kitaev et al., 2020; Katharopoulos et al., 2020). BigBIRD (Zaheer et al., 2020) provides random keys as additional inputs to its attention mechanism. Locality sensitive hashing (LSH) as employed e.g. in Reformer (Kitaev et al., 2020) utilizes a fixed random projection. Random Feature Attention (Peng et al., 2021) uses random fea4301 ture methods to approximate the softmax function. Performer (Choromanski et al., 2020) computes the transformer’s multi-head attention weights as a fixed orthogonal random projection. Closely related to thi"
2021.acl-long.331,P19-1375,0,0.0295658,"spersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks. 1 Introduction Transformers (Vaswani et al., 2017) have dominated natural language processing (NLP) in recent years, from large scale machine translation (Ott et al., 2018) to pre-trained (masked) language modeling (Devlin et al., 2018; Radford et al., 2018), and are becoming more popular in other fields as well, from reinforcement learning (Vinyals et al., 2019) to speech recognition (Baevski et al., 2019) and computer vision (Carion et al., 2020). Their success is enabled in part by ever increasing computational demands, which has naturally led to an increased interest in improving their efficiency. Scalability gains in transformers could facilitate bigger, deeper networks with longer contexts (Kitaev et al., 2020; Wang et al., 2020; Beltagy et al., 2020; Kaplan et al., 2020; Tay et al., 2020b). Conversely, improved efficiency could reduce environmental costs (Strubell et al., 2019) and hopefully help democratize the technology. In this work, we explore a simple question: if some layers of the"
2021.acl-long.331,D13-1170,0,0.0128778,"mance on SST-2 (left) and MultiNLI-matched (right). Model Max BLEU AUCC Train time Transformer 34.59 ± 0.11 114.57 ± 0.08 142.28 ± 1.87 T Reservoir 34.80 ± 0.07 115.26 ± 0.26 134.49 ± 1.70 Backskip Reservoir 34.75 ± 0.05 115.99 ± 0.23 119.54 ± 1.78 Table 3: Validation max BLEU, AUCC at 4h and wallclock time per epoch (averaged over multiple runs, in seconds) on IWSLT comparing backskipping with regular and reservoir transformers. and similar AUCC perplexity (see Appendix D). We then examine the performance of these models when fine-tuned on downstream tasks, specifically the well known SST-2 (Socher et al., 2013) and MultiNLI-matched (Williams et al., 2017) tasks. When fine-tuning the reservoir models, we keep the reservoir layers fixed (also fine-tuning them did not work very well, see Appendix D). Figure 2 shows the results of fine-tuning. We observe that the reservoir transformer outperforms normal RoBERTa at all depths in both tasks. At lower depth, the improvements are substantial. As a sanity check, we also experiment with freezing some of the layers in a regular pre-trained RoBERTa model during fine-tuning only (Transformer “frozen finetuned” in the Figure) and show that this helps a little but"
2021.acl-long.517,D19-1539,0,0.0185336,"SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Training set ↓ DRoBERTa EM DBERT F1 EM DBiDAF F1 EM F1 Finetuned model: BERTlarge 6.0 5.40.3 13.5 12.20.1 8.1 7.00.6 14.2 13.60.8 12.6 11.00.9 21.4 19.40.7 BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k) 11.02.6 12.41.6 9.10.7 21.03.0 22.12.2 20.40.7 14.63.7 16.43.0 14.01.0 24.74.0 26.22.7 24.60.7 25.16.5 29.63.7 30.11.2 39.16.9 43.74.0 43.81.2 Orig + BERTfooled (34.4k) Orig + BERTrandom (34.4k) Orig + SDC (34.4k) 15.20.8 16.90.5 9.40.6 25.10.6 23.90.5 20.20.5 20.40.4 20.50.6 15.31.0 31.00.4 31.20.9 25.81.1 32.40.6 34.10.4 32."
2021.acl-long.517,2020.tacl-1.43,0,0.310854,"Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6618–6633 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Platform shown to workers generating questions in the ADC setting. generate five questions for each context that they see. Workers are shown similar instructions (with minimal changes), and paid the same base amount. We fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al., 2019) datasets. For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data). We observe a similar pattern when augmenting the existing dataset with the adversarial data. Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-`a-vis robustness under distribution shif"
2021.acl-long.517,D19-1606,0,0.024551,"Missing"
2021.acl-long.517,N19-1423,0,0.161318,"how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy? In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to 6618 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joi"
2021.acl-long.517,D19-1461,0,0.252934,"publicly available at https://github.com/facebookresearch/aqa-study. investigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020). The hope is that by identifying parts of the input domain where the model fails one might make the model more robust. Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019). While adversarial data may indeed provide more challenging benchmarks, the process and its actual benefits vis-a-vis tasks of interest remain poorly understood, raising several key questions: (i) do the resulting models typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strateg"
2021.acl-long.517,W17-5401,0,0.0676695,"Missing"
2021.acl-long.517,D19-5801,0,0.0153683,"utational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6618–6633 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Platform shown to workers generating questions in the ADC setting. generate five questions for each context that they see. Workers are shown similar instructions (with minimal changes), and paid the same base amount. We fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al., 2019) datasets. For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data). We observe a similar pattern when augmenting the existing dataset with the adversarial data. Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-`a-vis robustness under distribution shift. To study the differences betwee"
2021.acl-long.517,N18-2017,0,0.0527588,"Missing"
2021.acl-long.517,2020.acl-main.244,0,0.0406829,"Missing"
2021.acl-long.517,P17-1147,0,0.0277592,"e better to out-ofdomain adversarial test sets than models fine-tuned on SDC data, confirming the findings by Dinan et al. (2019). Out-of-domain generalization to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on 4 The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Traini"
2021.acl-long.517,2020.emnlp-main.550,1,0.904447,"typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy? In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to 6618 Proceedings of the 59th"
2021.acl-long.517,2021.ccl-1.108,0,0.0448477,"Missing"
2021.acl-long.517,Q19-1026,0,0.0981119,"We provide all workers with the same base pay and for those assigned to ADC, pay out an additional bonus for each question that fools the QA model. Finally, we field a different set of workers to validate the generated examples. Context passages For context passages, we use the first 100 words of Wikipedia articles. Truncating the articles keeps the task of generating questions from growing unwieldy. These segments typically contain an overview, providing ample material for factoid questions. We restrict the pool of candidate contexts by leveraging a variant of the Natural Questions dataset (Kwiatkowski et al., 2019; Lee et al., 2019). We first keep only a subset of 23.1k question/answer pairs for which the context passages are the first 100 words of Wikipedia articles2 . From these passages, we sample 10k at random for our study. Models in the loop We use BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020) models as our adversarial models in the loop, using the implementations provided by Wolf et al. (2020). We fine-tune these models for span-based question-answering, using the 23.1k training examples (subsampled previously) for 20 epochs, with early-stopping based on word-overlap F13"
2021.acl-long.517,D17-1082,0,0.0295301,"ation to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on 4 The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Training set ↓ DRoBERTa EM DBERT F1 EM DBiDAF F1 EM F1 Finetuned model: BERTlarge 6.0 5.40.3 13.5 12.20.1 8.1 7.00.6 14.2 13.60.8 12.6 11.00.9 21.4 19.40.7 BE"
2021.acl-long.517,P19-1612,0,0.0933511,"e resulting models typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy? In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to 661"
2021.acl-long.517,K17-1034,0,0.0252732,"). Out-of-domain generalization to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on 4 The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Training set ↓ DRoBERTa EM DBERT F1 EM DBiDAF F1 EM F1 Finetuned model: BERTlarge 6.0 5.40.3 13.5 12.20.1 8.1 7.00.6 14.2 13.60.8 12."
2021.acl-long.517,2020.acl-main.441,1,0.892934,"more widely. Despite performing well on independent and identically distributed (i.i.d.) data, these models are liable under plausible domain shifts. With the goal of providing more challenging benchmarks that require this stronger form of generalization, an emerging line of research has 1 Data collected during this study is publicly available at https://github.com/facebookresearch/aqa-study. investigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020). The hope is that by identifying parts of the input domain where the model fails one might make the model more robust. Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019). While adversarial data may indeed provide more challenging benchmarks, the process and its actual benefits vis-a-vis tasks of interest remain poorly understood, raising several key questions: (i) do the resulting models typically generalize better out of distribution co"
2021.acl-long.517,P16-1144,0,0.0368826,"Missing"
2021.acl-long.517,P19-1472,0,0.0373793,"Missing"
2021.blackboxnlp-1.1,D15-1075,0,0.317011,"portions of the input people expected to be important in influencing models’ decisions. We then use Integrated Gradients (IG, Sundararajan et al. 2017) to determine which portions actually influenced the models’ decisions. We term the alignment between them as Importance Alignment. We formulate three different methods of using human-generated natural language explanations to quantify human expectations of model behavior, resulting in three different methods for calculating importance alignment. As a case study, we applied our method to the Natural Language Inference task (Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) in which models are tasked with classifying pairs of sentences according to whether the first sentence entails, contradicts, or is neutral with respect to the second. Concretely, we measured the extent to which the inference decisions of eight models (six state-of-the-art transformers, an LSTM model and a bag-of-words model) aligned with humangenerated explanations from the Adversarial NLI dataset (ANLI, Nie et al. 2020). In all three methods for calculating Importance Alignment, BERT-base had the highest importance alignment score. We also found that the smaller, ‘bas"
2021.blackboxnlp-1.1,N18-2017,0,0.054916,"Missing"
2021.blackboxnlp-1.1,P19-1487,0,0.0223535,"Selvaraju et al., 2020), such as answering “Is the rose red?” with no, but then “What color is the rose?” In this paper, we measure how well model decisions are aligned with human expectations ∗ The work was conducted during an internship at Facebook AI Research. 1 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 1–14 Online, November 11, 2021. ©2021 Association for Computational Linguistics about those decisions. Building on work that aims to extract or generate interpretable or faithful descriptions of model behavior (Lipton, 2018; Rajani et al., 2019; Kalouli et al., 2020; Silva et al., 2020; Jacovi and Goldberg, 2020; Zhao and Vydiswaran, 2020), we use human-generated natural language explanations to determine which portions of the input people expected to be important in influencing models’ decisions. We then use Integrated Gradients (IG, Sundararajan et al. 2017) to determine which portions actually influenced the models’ decisions. We term the alignment between them as Importance Alignment. We formulate three different methods of using human-generated natural language explanations to quantify human expectations of model behavior, resu"
2021.blackboxnlp-1.1,2020.acl-main.441,1,0.902958,"nt methods for calculating importance alignment. As a case study, we applied our method to the Natural Language Inference task (Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) in which models are tasked with classifying pairs of sentences according to whether the first sentence entails, contradicts, or is neutral with respect to the second. Concretely, we measured the extent to which the inference decisions of eight models (six state-of-the-art transformers, an LSTM model and a bag-of-words model) aligned with humangenerated explanations from the Adversarial NLI dataset (ANLI, Nie et al. 2020). In all three methods for calculating Importance Alignment, BERT-base had the highest importance alignment score. We also found that the smaller, ‘base’ versions of transformers tended to have higher importance alignment scores than the corresponding large versions. However, being smaller doesn’t always result in higher importance alignment, since both small non-transformer models had lower importance alignment scores than ‘base’ transformers. Finally, we demonstrate that more accurate models (both for classic test accuracy and on the diagnostic dataset HANS; McCoy et al. 2019) do not necessa"
2021.blackboxnlp-1.1,2020.inlg-1.43,0,0.0159291,"rsions. However, being smaller doesn’t always result in higher importance alignment, since both small non-transformer models had lower importance alignment scores than ‘base’ transformers. Finally, we demonstrate that more accurate models (both for classic test accuracy and on the diagnostic dataset HANS; McCoy et al. 2019) do not necessarily have higher importance alignment, suggesting that accuracy and alignment with human expectations are orthogonal dimensions along which models should be evaluated. 2 behaviour with normative notions of human ethics (“value alignment”; Russell et al. 2015; Peng et al. 2020), alignment between tokens from source to target in machine translation, alignment between images and text in image-caption alignment models, etc. In this paper, we propose a new type of alignment, Importance Alignment: we want models to not only generate accurate outputs, but also to generate these accurate outputs for reasons that align with human expectations. High importance alignment can be valuable because prior work has demonstrated that when people form correct mental models of AI decision boundaries, they make better AI-assisted decisions (Bansal et al., 2019a,b). For example, when an"
2021.blackboxnlp-1.1,D19-6601,0,0.0115275,"dels (finetuned on MNLI + SNLI + ANLI + re-cast FEVER) and models used as the soft oracle (finetuned on 6-way NLI and reason classification on a subset of ANLI).  X Target models Experimental details Models Target models. We measured the importance alignment for six pretrained Transformer language models: BERT base and large (Devlin et al., 2019); RoBERTa base and large (Liu et al., 2019); and ELECTRA base and large (Clark et al., 2020). We fine-tuned these models on the combination of the following NLI datasets: SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), NLI-recast FEVER (Thorne et al., 2019) and ANLI rounds 1–3 (Nie et al., 2020). We used the hyperparameters in the ANLI codebase for finetuning.4 We used the same datasets to also train two non-transformer models: a bag-of-words (BOW) 4 https://github.com/facebookresearch/ anli/blob/master/script/example_scripts/ 5 Model Importance Alignment ∆AH ∆AS Acc. ANLI Model Importance Alignment ∆AH ∆AS ∆AE BERT-Base RoBERTa-Base ELECTRA-Base BERT-Large RoBERTa-Large ELECTRA-Large 0.21*** 0.11*** 0.17*** 0.18*** 0.04* 0.07 0.11*** 0.02* 0.06*** -0.02 0.01 0.01 48.02 50.47 52.33 49.24 55.37 58.06 BERT-Base RoBERTa-Base ELECTRA-Base BERT-Large"
2021.blackboxnlp-1.1,L18-1239,0,0.0282887,"Missing"
2021.blackboxnlp-1.1,D19-1221,0,0.0423921,"Missing"
2021.blackboxnlp-1.1,D19-1002,0,0.0283059,"e current example or about some other example. This task requires the model to perform not only the target task (e.g., NLI) but also requires the model to establish a relationship between the provided explanation and the input, thereby incorporating information from the natural language explanation. Why Integrated Gradients? We use Integrated Gradients because they are axiomatically both interpretable and faithful (Sundararajan et al., 2017), unlike attention based methods which have been argued are not faithful explanations of models’ decision making processes (Jain and Wallace 2019, but see Wiegreffe and Pinter 2019 for a counterpoint). Other perturbation methods which are more faithful than attention such as LIME (Ribeiro et al., 2016), SHAP (Lundberg and Lee, 2017), and their variants can be used, although these methods have been argued to be unreliable (Camburu et al., 2019; Slack et al., 2020; Camburu et al., 2021). No current consensus exists on which methods should be employed (Hase and Bansal, 2020; Ghorbani et al., 2019). Although we use IG, crucially, our method is not dependent on it; IG 2 We used the list of stop words from NLTK (Bird et al., 2009) 3 Formally, for some input x with gold label"
2021.emnlp-main.230,W07-1401,0,0.168436,"tric and non-parametric tasks (see §5). GLUE. The GLUE (Wang et al., 2018) benchmark is a collection of 9 datasets for evaluating natural language understanding systems, of which we use Corpus of Linguistic Acceptability (CoLA, Warstadt et al., 2019b), Stanford Sentiment Treebank (SST, Socher et al., 2013), Microsoft Research Paragraph Corpus (MRPC, Dolan and Brockett, 2005), Quora Question Pairs (QQP)4 , Multi-Genre NLI (MNLI, Williams et al., 2018b), Question NLI (QNLI, Rajpurkar et al., 2016; Demszky et al., 2018), Recognizing Textual Entailment (RTE, Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). Pham et al. (2020) show the word order insensitivity of several GLUE tasks (QQP, SST-2), evaluated on public regularly pre-trained checkpoints. PAWS. The PAWS task (Zhang et al., 2019) consists of predicting whether a given pair of sentences are paraphrases. This dataset contains both paraphrase and non-paraphrase pairs with high lexical overlap, which are generated by controlled word swapping and back translation. Since even a small word swap and perturbation can drastically modify the meaning of the sentence, we hypothesize the randomized pre-trained models will s"
2021.emnlp-main.230,W18-5426,1,0.828936,"dhuber, 2889 1997) on non-linguistic data with latent structure such as MIDI music or Java code provides better test performance on downstream tasks than a randomly initialized model. They observe that even when there is no vocabulary overlap among source and target languages, LSTM language models leverage the latent hierarchical structure of the input to obtain better performance than a random, Zipfian corpus of the same vocabulary. On the utility of probing tasks. Many recent papers provide compelling evidence that BERT contains a surprising amount of syntax, semantics, and world knowledge (Giulianelli et al., 2018; Rogers et al., 2020; Lakretz et al., 2019; Jumelet et al., 2019, 2021). Many of these works involve diagnostic classifiers (Hupkes et al., 2018) or parametric probes, i.e. a function atop learned representations that is optimized to find linguistic information. How well the probe learns a given signal can be seen as a proxy for linguistic knowledge encoded in the representations. However, the community is divided on many aspects of probing (Belinkov, 2021) including how complex probes should be. Many prefer simple linear probes over the complex ones (Alain and Bengio, 2017; Hewitt and Mannin"
2021.emnlp-main.230,2021.eacl-main.270,0,0.0850123,"Missing"
2021.emnlp-main.230,N19-1357,0,0.0201969,"tasks as “syntax-light” making them more easily decodeable from images of the brain. Shen et al. (2021) show that entire layers of MLM transformers can be randomly initialized and kept frozen throughout training without detrimental effect and that those layers perform better on some probing tasks than their frozen counterparts. Models have been found to be surprisingly robust to randomizing or cutting syntactic tree structures they were hoped to rely on (Scheible and Schütze, 2013; Williams et al., 2018a), and randomly permuting attention weights often induces only minimal changes in output (Jain and Wallace, 2019). In computer vision, it is well known that certain architectures constitute good “deep image priors” for fine-tuning (Ulyanov et al., 2018) or pruning (Frankle et al., 2020), and that even randomly wired networks can perform well at image recognition (Xie et al., 2019). Here, we explore randomizing the data, rather than the model, to assess whether certain claims about which phenomena the model has learned are established in fact. Sensitivity to word order in NLU. Information order has been a topic of research in computational linguistics since Barzilay and Lee (2004) introduced the task of r"
2021.emnlp-main.230,N18-2017,0,0.0608539,"Missing"
2021.emnlp-main.230,P19-1340,0,0.0203718,"frozen. SentEval trains probes on top of fixed representations individually for each task. We follow the recommended setup and run grid search over the following hyperparams: number of hidden layer dimensions 10 We experimented with a much stronger, state-of-the-art ([0, 50, 100, 200]), dropout ([0, 0.1, 0.2]), 4 epochs, Second order Tree CRF Neural Dependency Parser (Zhang et al., 2020), but did not observe any difference in UAS with 64 batch size. We select the best performance based different pre-trained models (see Appendix G) on the dev set, and report the test set accuracy. 11 PTB data (Kitaev et al., 2019) is used from github.com/nikitakit/self-attentive-parser/tree/master/data. Results. We provide the results in Table 3. The 12 Pimentel et al. (2020a) propose computing the Pareto MN pre-trained model scores better than the unHypervolume over all hyperparameters in each task. We did natural word order models for only one out of five not observe a significant difference in the hypervolumes for the models, as reported in Appendix K. semantic tasks and in none of the lexical tasks. 2894 Model Length (Surface) WordContent (Surface) TreeDepth (Syntactic) TopConstituents (Syntactic) BigramShift (Synt"
2021.emnlp-main.230,2021.acl-short.134,0,0.0336441,"domly wired networks can perform well at image recognition (Xie et al., 2019). Here, we explore randomizing the data, rather than the model, to assess whether certain claims about which phenomena the model has learned are established in fact. Sensitivity to word order in NLU. Information order has been a topic of research in computational linguistics since Barzilay and Lee (2004) introduced the task of ranking sentence orders as an evaluation for language generation quality, an approach which was subsequently also used to evaluate readability and dialogue coherence (Barzilay and Lapata, 2008; Laban et al., 2021). More recently, several research groups have investigated information order for words rather than sentences as an evaluation of model humanlikeness. Sinha et al. (2021) investigate the task of natural language inference (NLI) and find high accuracy on permuted examples for different Transformer and pre-Transformer era models, across English and Chinese datasets (Hu et al., 2020). Gupta et al. (2021) use targeted permutations on RoBERTabased models and show word order insensitivity across natural language inference (MNLI), para- Synthetic pre-training. Kataoka et al. (2020) phrase detection (Q"
2021.emnlp-main.230,N19-1002,1,0.846169,"latent structure such as MIDI music or Java code provides better test performance on downstream tasks than a randomly initialized model. They observe that even when there is no vocabulary overlap among source and target languages, LSTM language models leverage the latent hierarchical structure of the input to obtain better performance than a random, Zipfian corpus of the same vocabulary. On the utility of probing tasks. Many recent papers provide compelling evidence that BERT contains a surprising amount of syntax, semantics, and world knowledge (Giulianelli et al., 2018; Rogers et al., 2020; Lakretz et al., 2019; Jumelet et al., 2019, 2021). Many of these works involve diagnostic classifiers (Hupkes et al., 2018) or parametric probes, i.e. a function atop learned representations that is optimized to find linguistic information. How well the probe learns a given signal can be seen as a proxy for linguistic knowledge encoded in the representations. However, the community is divided on many aspects of probing (Belinkov, 2021) including how complex probes should be. Many prefer simple linear probes over the complex ones (Alain and Bengio, 2017; Hewitt and Manning, 2019; Hall Maudslay et al., 2020). Howev"
2021.emnlp-main.230,2021.eacl-main.215,0,0.0222867,"not simply learn what the correct word order is from unordered text? First, the lower nonparametric probing accuracies of the randomized models indicate that they are not able to accurately reconstruct the original word order (see also Appendix D). But even if models were able to “unshuffle” the words under our unnatural pre-training set up, they would only be doing so based on distributional information. Models would then abductively learn only the most likely word order. While models might infer a distribution over possible orders and use that information to structure their representations (Papadimitriou et al., 2021), syntax is not about possible or even the most likely orders: it is about the actual order. That is, even if one concludes in the end that Transformers are able to perform word order reconstruction based on distributional information, and recover almost all downstream performance based solely on that, we ought to be a lot more careful when making claims about what our evaluation datasets are telling us. Thus, our results seem to suggest that we may need to revisit what we mean by “linguistic structure,” and perhaps subsequently acknowledge that we may not need human-like linguistic abilities"
2021.emnlp-main.230,N18-1202,0,0.137743,"Missing"
2021.emnlp-main.230,2020.tacl-1.54,0,0.0311123,"linguistic data with latent structure such as MIDI music or Java code provides better test performance on downstream tasks than a randomly initialized model. They observe that even when there is no vocabulary overlap among source and target languages, LSTM language models leverage the latent hierarchical structure of the input to obtain better performance than a random, Zipfian corpus of the same vocabulary. On the utility of probing tasks. Many recent papers provide compelling evidence that BERT contains a surprising amount of syntax, semantics, and world knowledge (Giulianelli et al., 2018; Rogers et al., 2020; Lakretz et al., 2019; Jumelet et al., 2019, 2021). Many of these works involve diagnostic classifiers (Hupkes et al., 2018) or parametric probes, i.e. a function atop learned representations that is optimized to find linguistic information. How well the probe learns a given signal can be seen as a proxy for linguistic knowledge encoded in the representations. However, the community is divided on many aspects of probing (Belinkov, 2021) including how complex probes should be. Many prefer simple linear probes over the complex ones (Alain and Bengio, 2017; Hewitt and Manning, 2019; Hall Maudsla"
2021.emnlp-main.230,2020.acl-main.240,0,0.0448259,"Missing"
2021.emnlp-main.230,2021.acl-long.331,1,0.747104,"ng. Since models can learn whatever word order information they do need largely from fine-tuning alone, this likely suggests that our downstream tasks don’t actually require much complex word order information in the first place (cf., Glavaš and Vuli´c 2021). Randomization ablations. Random controls have been explored in a variety of prior work. Wieting and Kiela (2019) show that random sentence encoders are surprisingly powerful baselines. Gauthier and Levy (2019) use random sentence reordering to label some tasks as “syntax-light” making them more easily decodeable from images of the brain. Shen et al. (2021) show that entire layers of MLM transformers can be randomly initialized and kept frozen throughout training without detrimental effect and that those layers perform better on some probing tasks than their frozen counterparts. Models have been found to be surprisingly robust to randomizing or cutting syntactic tree structures they were hoped to rely on (Scheible and Schütze, 2013; Williams et al., 2018a), and randomly permuting attention weights often induces only minimal changes in output (Jain and Wallace, 2019). In computer vision, it is well known that certain architectures constitute good"
2021.emnlp-main.230,silveira-etal-2014-gold,0,0.0709107,"Missing"
2021.emnlp-main.230,2020.emnlp-main.254,1,0.926225,"top learned representations that is optimized to find linguistic information. How well the probe learns a given signal can be seen as a proxy for linguistic knowledge encoded in the representations. However, the community is divided on many aspects of probing (Belinkov, 2021) including how complex probes should be. Many prefer simple linear probes over the complex ones (Alain and Bengio, 2017; Hewitt and Manning, 2019; Hall Maudslay et al., 2020). However, complex probes with strong representational capacity are able to extract the most information from representations (Voita and Titov, 2020; Pimentel et al., 2020b; Hall Maudslay et al., 2020). Here, we follow Pimentel et al. (2020a) and use both simple (linear) and complex (non-linear) models, as well as “complex” tasks (dependency parsing). As an alternative to parametric probes, stimulus-based non-parametric probing (Linzen et al., 2016; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Gulordava et al., 2018a; Warstadt et al., 2019a, 2020a,b; Ettinger, 2020; Lakretz et al., 2021) has been used to show that even without a learned probe, BERT can predict syntactic properties with high confidence (Goldberg, 2019; Wolf, 2019). We use this class of non"
2021.emnlp-main.230,2021.acl-long.569,1,0.813317,"r, masked lan- with regularly-pre-trained models. We demonstrate that pre-training on permuted guage model (MLM) pre-training, as epitomized by BERT (Devlin et al., 2019), has proven wildly suc- data has surprisingly little effect on downstream task performance after fine-tuning (on non-shuffled cessful, although the precise reason for this success training data). It has recently been found that has remained unclear. On one hand, we can view BERT as the newest in a long line of NLP tech- MLMs are quite robust to permuting downstream niques (Deerwester et al., 1990; Landauer and Du- test data (Sinha et al., 2021; Pham et al., 2020; Gupta et al., 2021) and even do quite well using mais, 1997; Collobert and Weston, 2008; Mikolov et al., 2013; Peters et al., 2018) that exploit the well- permuted “unnatural” downstream train data (Sinha et al., 2021; Gupta et al., 2021). Here, we show that known distributional hypothesis (Harris, 1954).1 downstream performance for “unnatural language On the other hand, it has been claimed that BERT pre-training” is much closer to standard MLM pre1 One might even argue that BERT is not actually training than one might expect. all that different from earlier distributional"
2021.emnlp-main.230,2020.acl-main.420,1,0.914374,"top learned representations that is optimized to find linguistic information. How well the probe learns a given signal can be seen as a proxy for linguistic knowledge encoded in the representations. However, the community is divided on many aspects of probing (Belinkov, 2021) including how complex probes should be. Many prefer simple linear probes over the complex ones (Alain and Bengio, 2017; Hewitt and Manning, 2019; Hall Maudslay et al., 2020). However, complex probes with strong representational capacity are able to extract the most information from representations (Voita and Titov, 2020; Pimentel et al., 2020b; Hall Maudslay et al., 2020). Here, we follow Pimentel et al. (2020a) and use both simple (linear) and complex (non-linear) models, as well as “complex” tasks (dependency parsing). As an alternative to parametric probes, stimulus-based non-parametric probing (Linzen et al., 2016; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Gulordava et al., 2018a; Warstadt et al., 2019a, 2020a,b; Ettinger, 2020; Lakretz et al., 2021) has been used to show that even without a learned probe, BERT can predict syntactic properties with high confidence (Goldberg, 2019; Wolf, 2019). We use this class of non"
2021.emnlp-main.230,S18-2023,0,0.0741543,"Missing"
2021.emnlp-main.230,D16-1264,0,0.0603821,"tanding and Evaluation (GLUE) benchmark, the Paraphrase Adversaries from Word Scrambling (PAWS) dataset, and various parametric and non-parametric tasks (see §5). GLUE. The GLUE (Wang et al., 2018) benchmark is a collection of 9 datasets for evaluating natural language understanding systems, of which we use Corpus of Linguistic Acceptability (CoLA, Warstadt et al., 2019b), Stanford Sentiment Treebank (SST, Socher et al., 2013), Microsoft Research Paragraph Corpus (MRPC, Dolan and Brockett, 2005), Quora Question Pairs (QQP)4 , Multi-Genre NLI (MNLI, Williams et al., 2018b), Question NLI (QNLI, Rajpurkar et al., 2016; Demszky et al., 2018), Recognizing Textual Entailment (RTE, Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). Pham et al. (2020) show the word order insensitivity of several GLUE tasks (QQP, SST-2), evaluated on public regularly pre-trained checkpoints. PAWS. The PAWS task (Zhang et al., 2019) consists of predicting whether a given pair of sentences are paraphrases. This dataset contains both paraphrase and non-paraphrase pairs with high lexical overlap, which are generated by controlled word swapping and back translation. Since even a small word swap"
2021.emnlp-main.230,D13-1170,0,0.00567905,"se FairSeq (Ott et al., 2019) for the pre-training and fine-tuning experiments. 3 https://spacy.io/ 3.2 Fine-tuning tasks We evaluate downstream performance using the General Language Understanding and Evaluation (GLUE) benchmark, the Paraphrase Adversaries from Word Scrambling (PAWS) dataset, and various parametric and non-parametric tasks (see §5). GLUE. The GLUE (Wang et al., 2018) benchmark is a collection of 9 datasets for evaluating natural language understanding systems, of which we use Corpus of Linguistic Acceptability (CoLA, Warstadt et al., 2019b), Stanford Sentiment Treebank (SST, Socher et al., 2013), Microsoft Research Paragraph Corpus (MRPC, Dolan and Brockett, 2005), Quora Question Pairs (QQP)4 , Multi-Genre NLI (MNLI, Williams et al., 2018b), Question NLI (QNLI, Rajpurkar et al., 2016; Demszky et al., 2018), Recognizing Textual Entailment (RTE, Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). Pham et al. (2020) show the word order insensitivity of several GLUE tasks (QQP, SST-2), evaluated on public regularly pre-trained checkpoints. PAWS. The PAWS task (Zhang et al., 2019) consists of predicting whether a given pair of sentences are paraphras"
2021.emnlp-main.230,2020.tacl-1.25,0,0.0814454,"Missing"
2021.emnlp-main.230,Q19-1040,0,0.219306,"nd Bengio, 2017; Hewitt and Manning, 2019; Hall Maudslay et al., 2020). However, complex probes with strong representational capacity are able to extract the most information from representations (Voita and Titov, 2020; Pimentel et al., 2020b; Hall Maudslay et al., 2020). Here, we follow Pimentel et al. (2020a) and use both simple (linear) and complex (non-linear) models, as well as “complex” tasks (dependency parsing). As an alternative to parametric probes, stimulus-based non-parametric probing (Linzen et al., 2016; Jumelet and Hupkes, 2018; Marvin and Linzen, 2018; Gulordava et al., 2018a; Warstadt et al., 2019a, 2020a,b; Ettinger, 2020; Lakretz et al., 2021) has been used to show that even without a learned probe, BERT can predict syntactic properties with high confidence (Goldberg, 2019; Wolf, 2019). We use this class of non-parametric probes to investigate RoBERTa’s ability to learn word order during pre-training. 3 Approach pect that other variants of MLMs would provide similar insights, given their similar characteristics. 3.1 Models In all of our experiments, we use the original 16GB BookWiki corpus (the Toronto Books Corpus, Zhu et al. 2015, plus English Wikipedia) from Liu et al. (2019).2 We"
2021.emnlp-main.231,D17-1070,1,0.8692,"Missing"
2021.emnlp-main.231,N18-1033,0,0.0670821,"Missing"
2021.emnlp-main.231,2020.findings-emnlp.372,0,0.0814959,"Missing"
2021.emnlp-main.231,2021.ccl-1.108,0,0.0471224,"Missing"
2021.emnlp-main.231,W18-6301,0,0.0270772,"curacy 75 mentioned fully-weighted model performance when using the larger hidden size with one-layer randomly weighted RoBERTalarge . Implementation Details. We evaluate on IWSLT14 de-en (Cettolo et al., 2015) and WMT14 en-de (Bojar et al., 2014) for machine translation; QQP (Iyer et al., 2017) and MultiNLImatched (MNLI) (Williams et al., 2017) for natural language understanding.5 We use 8 Volta V100 GPUs for WMT, and one V100 for IWSLT, QQP, and MNLI. The hyperparameters on IWSLT14 and WMT14 for training a one-layer randomized Transformer were set the same to the best-performing values from Ott et al. (2018) for training fully-weighted Transformer. The QQP and MNLI experiments followed Liu et al. (2019). 5 70 65 60 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Remaining Parameter Ratio 0.8 0.9 Figure 7: Prune Randomly Weighted Transformer performance on MNLI. the random feature (Wieting and Kiela, 2019; Ramanujan et al., 2020; Shen et al., 2021). We experiment with kaiming uniform (Ramanujan et al., 2020) and Xavier uniform (Vaswani et al., 2017) initialization methods, and we scale the standard p deviation by 1/σ when we retain σ randomized weights. As shown in Fig. 5, the performance of the one-layer randomized"
2021.emnlp-main.231,D14-1162,0,0.0966482,"Missing"
2021.emnlp-main.231,2020.emnlp-main.259,0,0.0358074,"Missing"
2021.emnlp-main.231,2021.acl-long.331,1,0.791833,"e can simply construct and 2003; Lukoˇseviˇcius and Jaeger, 2009), “random store the binary Supermask M and the floatingkitchen sink” kernel machines (Rahimi and Recht, point W while dropping S for future usage. 2008, 2009), and so on. Recently, random feaOne-layer randomly weighted Transformer. tures have also been extensively explored for modWe use the Transformer architecture (see Vaswani ern neural networks in deep reservoir computing et al. (2017) for more details). For a general networks (Scardapane and Wang, 2017; Gallicchio randomly weighted Transformer model with Suand Micheli, 2017; Shen et al., 2021), random kerpermask, there exist Ml s and Wl s for all layers nel feature (Peng et al., 2021; Choromanski et al., l ∈ {1, ...L}. Due to the natural property of layer 2020), and applications in text classification (Constacking in Transformers, all Wl s have the same neau et al., 2017; Wieting and Kiela, 2019), sumshape with the same initialization method. This marization (Pilault et al., 2020) and probing (Voita leads to an unexplored question: “What’s hidand Titov, 2020). den in a one-layer (instead of L-layer) randomly weighted transformer?” Compressing Transformer. A wide range of neuLet us"
2021.emnlp-main.231,2020.acl-main.195,0,0.040935,"Missing"
2021.emnlp-main.231,2020.emnlp-main.14,0,0.0526411,"Missing"
2021.emnlp-main.464,D18-1316,0,0.0814907,"g a token-level edit distance of 1. Prior work. Several attack algorithms have been proposed to circumvent these two issues, using a multitude of approaches. For attacks that operate on the character level, perceptibility can be approximated by the number of character edits, i.e., replacements, swaps, insertions and deletions (Ebrahimi et al., 2017; Li et al., 2018; Gao et al., 2018). Attacks that operate on the word level adopt heuristics such as synonym substitution (Samanta and Mehta, 2017; Zang et al., 2020; Maheshwary et al., 2020) or replacing words by ones with similar word embeddings (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020). More recent attacks have also leveraged masked language models such as BERT (Devlin et al., 2019) to generate word substitutions by replacing masked tokens (Garg and Ramakrishnan, 2020; Li et al., 2020a,b). Most of the aforementioned attacks follow the common recipe of proposing characterlevel or word-level perturbations to generate a constrained candidate set and optimizing the adversarial loss greedily or using beam search. only reduces the test accuracy of the target model on the AG News dataset (Zhang et al., 2015) from 95.1 to 10.6. In comparison, at"
2021.emnlp-main.464,D18-2029,0,0.0712038,"Missing"
2021.emnlp-main.464,N19-1423,0,0.531459,"itude of approaches. For attacks that operate on the character level, perceptibility can be approximated by the number of character edits, i.e., replacements, swaps, insertions and deletions (Ebrahimi et al., 2017; Li et al., 2018; Gao et al., 2018). Attacks that operate on the word level adopt heuristics such as synonym substitution (Samanta and Mehta, 2017; Zang et al., 2020; Maheshwary et al., 2020) or replacing words by ones with similar word embeddings (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020). More recent attacks have also leveraged masked language models such as BERT (Devlin et al., 2019) to generate word substitutions by replacing masked tokens (Garg and Ramakrishnan, 2020; Li et al., 2020a,b). Most of the aforementioned attacks follow the common recipe of proposing characterlevel or word-level perturbations to generate a constrained candidate set and optimizing the adversarial loss greedily or using beam search. only reduces the test accuracy of the target model on the AG News dataset (Zhang et al., 2015) from 95.1 to 10.6. In comparison, attacks against image models can consistently reduce the model’s accuracy to 0 on almost all computer vision tasks (Akhtar and Mian, 2018)"
2021.emnlp-main.464,2020.emnlp-main.498,0,0.0357497,"Missing"
2021.emnlp-main.464,2021.naacl-main.400,0,0.0404476,"Missing"
2021.emnlp-main.464,2020.emnlp-main.500,0,0.159282,"e approximated with L2 We propose the first general-purpose gradientand L∞ -norms, but such metrics are not readily apbased adversarial attack against transformer plicable to text data. To circumvent this issue, some models. Instead of searching for a single adexisting attack approaches have opted for heurisversarial example, we search for a distributic word replacement strategies and optimizing by tion of adversarial examples parameterized by greedy or beam search using black-box queries (Jin a continuous-valued matrix, hence enabling gradient-based optimization. We empirically et al., 2020; Li et al., 2020a,b; Garg and Ramakrdemonstrate that our white-box attack attains ishnan, 2020). Such heuristic strategies typically state-of-the-art attack performance on a variety introduce unnatural changes that are grammatically of natural language tasks, outperforming prior or semantically incorrect (Morris et al., 2020a). work in terms of adversarial success rate with In this paper, we propose a general-purpose matching imperceptibility as per automated framework for gradient-based adversarial attacks, and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enand apply it a"
2021.emnlp-main.464,2021.ccl-1.108,0,0.0413339,"Missing"
2021.emnlp-main.464,P11-1015,0,0.0162483,"2020; Li et al., 2020a,b). We demonstrate in subsection 4.2 that this transfer attack enabled by the adversarial distribution PΘ is very effective at attacking a variety of target models. 4 Experiments In this section, we empirically validate our attack framework on a benchmark suite of natural language tasks. Code to reproduce our results is open sourced on GitHub1 . 4.1 Setup Tasks. We evaluate on several benchmark text classification datasets, including DBPedia (Zhang et al., 2015) and AG News (Zhang et al., 2015) for article/news categorization, Yelp Reviews (Zhang et al., 2015) and IMDB (Maas et al., 2011) for binary sentiment classification, and MNLI (Williams et al., 2017) for natural language inference. The MNLI dataset contains two evaluation sets: where λlm , λsim &gt; 0 are hyperparameters that control the strength of the soft constraints. We minimize L(Θ) stochastically using Adam (Kingma 1 and Ba, 2014) by sampling a batch of inputs from https://github.com/facebookresearch/ P˜Θ at every iteration. text-adversarial-attack 5751 Task Clean Acc. GPT-2 Adv. Acc. Cosine Sim. Clean Acc. 99.2 94.8 97.8 93.8 81.7 82.5 5.2 6.6 2.9 7.6 2.8/11.0 4.2/13.5 0.91 0.90 0.94 0.98 0.82/0.88 0.85/0.88 99.1 94"
2021.emnlp-main.464,2020.findings-emnlp.341,0,0.203702,"risversarial example, we search for a distributic word replacement strategies and optimizing by tion of adversarial examples parameterized by greedy or beam search using black-box queries (Jin a continuous-valued matrix, hence enabling gradient-based optimization. We empirically et al., 2020; Li et al., 2020a,b; Garg and Ramakrdemonstrate that our white-box attack attains ishnan, 2020). Such heuristic strategies typically state-of-the-art attack performance on a variety introduce unnatural changes that are grammatically of natural language tasks, outperforming prior or semantically incorrect (Morris et al., 2020a). work in terms of adversarial success rate with In this paper, we propose a general-purpose matching imperceptibility as per automated framework for gradient-based adversarial attacks, and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enand apply it against transformer models on text abled by sampling from the adversarial distridata. Our framework, GBDA (Gradient-based Disbution, matches or exceeds existing methods, tributional Attack), consists of two key components while only requiring hard-label outputs. that circumvent the difficulties of gradient des"
2021.emnlp-main.464,2020.acl-main.540,0,0.0335524,"ce, inserting the word not into a sentence can negate the meaning of the whole sentence despite having a token-level edit distance of 1. Prior work. Several attack algorithms have been proposed to circumvent these two issues, using a multitude of approaches. For attacks that operate on the character level, perceptibility can be approximated by the number of character edits, i.e., replacements, swaps, insertions and deletions (Ebrahimi et al., 2017; Li et al., 2018; Gao et al., 2018). Attacks that operate on the word level adopt heuristics such as synonym substitution (Samanta and Mehta, 2017; Zang et al., 2020; Maheshwary et al., 2020) or replacing words by ones with similar word embeddings (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020). More recent attacks have also leveraged masked language models such as BERT (Devlin et al., 2019) to generate word substitutions by replacing masked tokens (Garg and Ramakrishnan, 2020; Li et al., 2020a,b). Most of the aforementioned attacks follow the common recipe of proposing characterlevel or word-level perturbations to generate a constrained candidate set and optimizing the adversarial loss greedily or using beam search. only reduces the test accu"
2021.emnlp-main.464,D14-1162,0,0.0894974,"· πi−1 ) i=1 n X V X (πi )j g(e(π1 ) · · · e(πi−1 ))j , i=1 j=1 where gi,j ∼ Gumbel(0, 1) and T &gt; 0 is a tem- with log pg (πi |π1 · · · πi−1 ) being the crossperature parameter that controls the smoothness entropy between the next token distribution 5750 πi and the predicted next token distribution g(e(π1 ) · · · e(πi−1 )). This extension coincides with the NLL for a token sequence x when each πi is a delta distribution for the token xi . Similarity constraint with BERTScore. Prior work on word-level attacks often used contextfree embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) or synonym substitution to constrain semantic similarity between the original and perturbed text (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020). These constraints tend to produce out-of-context and unnatural changes that alter the semantic meaning of the perturbed text (Garg and Ramakrishnan, 2020). Instead, we propose to use BERTScore (Zhang et al., 2019), a similarity score for evaluating text generation that captures the semantic similarity between pairwise tokens in contextualized embeddings of a transformer model. Let x = x1 · · · xn and z = z1 · · · zm be two token sequence"
2021.emnlp-main.464,P19-1103,0,0.130234,"stance of 1. Prior work. Several attack algorithms have been proposed to circumvent these two issues, using a multitude of approaches. For attacks that operate on the character level, perceptibility can be approximated by the number of character edits, i.e., replacements, swaps, insertions and deletions (Ebrahimi et al., 2017; Li et al., 2018; Gao et al., 2018). Attacks that operate on the word level adopt heuristics such as synonym substitution (Samanta and Mehta, 2017; Zang et al., 2020; Maheshwary et al., 2020) or replacing words by ones with similar word embeddings (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020). More recent attacks have also leveraged masked language models such as BERT (Devlin et al., 2019) to generate word substitutions by replacing masked tokens (Garg and Ramakrishnan, 2020; Li et al., 2020a,b). Most of the aforementioned attacks follow the common recipe of proposing characterlevel or word-level perturbations to generate a constrained candidate set and optimizing the adversarial loss greedily or using beam search. only reduces the test accuracy of the target model on the AG News dataset (Zhang et al., 2015) from 95.1 to 10.6. In comparison, attacks against imag"
2021.emnlp-main.464,D19-1221,0,0.023588,"e same model’s accuracy from 95.1 to 3.5 while being more semantically-faithful to the original text. Our result shows that using gradient-based search for text adversarial examples can indeed close the performance gap between vision and text attacks. 2.2 Other Attacks While most works on adversarial attack on text fall within the formulation defined at the beginning of section 2, other notions of adversarial perturbation exist as well. One class of such attacks is known as universal adversarial triggers—a short snippet of text that when appended to any input, causes the model to misclassify (Wallace et al., 2019; Song et al., 2020). However, such triggers often contain unnatural combinations of words or tokens, and hence are very perceptible to a human observer. Our work falls within the general area of adversarial learning, and many prior works in this area have explored the notion of adversarial example on different data modalities. Although the most prominent data modality by far is image, adversarial examples can be constructed for speech (Carlini and Wagner, 2018) and graphs (Dai et al., 2018; Zügner et al., 2018) as well. 3 GBDA: Gradient-based Distributional Attack Shortcomings in prior work."
2021.emnlp-main.696,P19-1620,0,0.278606,"ction (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples using influence functions, or methods that attempt to maximise diversity; we find that"
2021.emnlp-main.696,2020.tacl-1.43,1,0.809231,"century (iv) Q: When did Old English begin to be used? A: 5th century Figure 1: The Synthetic Adversarial Data Generation Pipeline showing: (i) passage selection from Wikipedia; (ii) answer candidate selection and filtering by model confidence (an example retained answer shown in green, and a dropped answer candidate in red); (iii) question generation using BARTLarge ; and (iv) answer re-labelling using self-training. The generated synthetic data is then used as part of the training data for a downstream Reading Comprehension model. A recently proposed alternative is dynamic data collection (Bartolo et al., 2020; Nie et al., 2020), Large-scale labelled datasets like SQuAD (Ra- where data is collected with both humans and modjpurkar et al., 2016) and SNLI (Bowman et al., els in the annotation loop. Usually, these humans 2015) have been driving forces in natural language are instructed to ask adversarial questions that fool processing research. Over the past few years, how- existing models. Dynamic adversarial data colever, such “statically collected” datasets have been lection is often used to evaluate the capabilities shown to suffer from various problems. In particu- of current state-of-the-art mode"
2021.emnlp-main.696,D15-1075,0,0.0867811,"Missing"
2021.emnlp-main.696,E06-1032,0,0.0453541,"g. 2.3 Self-training In self-training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020). In our setting, large amounts of unlabelled adversarial-style questions are not readily available, which motivates our use of a question generation model. 2.4 Human Evaluation The ultimate goal of automatic machine learning model evaluation is usually stated as capturing human judgements (Callison-Burch et al., 2006; Hill et al., 2015; Vedantam et al., 2015; Liu et al., 2016). Evaluation with real humans is considered beneficial, but not easily scalable, and as such is rarely conducted in-the-loop. With NLP model capabilities ever improving, adversarial worst case evaluation becomes even more pertinent. To our knowledge, this work is the first to compare models explicitly by their adversarial validated model error rate (vMER), which we define in Section 4.4. 3 Synthetic Data Generation We develop a synthetic data generation pipeline for QA that involves four stages: passage selection, answer candidate se"
2021.emnlp-main.696,P18-1177,0,0.0471497,"Missing"
2021.emnlp-main.696,P17-1123,0,0.0217455,"llace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples usi"
2021.emnlp-main.696,D19-5801,1,0.897458,"Missing"
2021.emnlp-main.696,D19-1107,0,0.0399652,"Missing"
2021.emnlp-main.696,N18-2017,0,0.045407,"Missing"
2021.emnlp-main.696,J15-4004,0,0.046805,"training, a model is trained to both predict correctly on labelled examples and increase its confidence on unlabelled examples. Self-training can yield complementary accuracy gains with pretraining (Du et al., 2020) and can improve robustness to domain shift (Kumar et al., 2020). In our setting, large amounts of unlabelled adversarial-style questions are not readily available, which motivates our use of a question generation model. 2.4 Human Evaluation The ultimate goal of automatic machine learning model evaluation is usually stated as capturing human judgements (Callison-Burch et al., 2006; Hill et al., 2015; Vedantam et al., 2015; Liu et al., 2016). Evaluation with real humans is considered beneficial, but not easily scalable, and as such is rarely conducted in-the-loop. With NLP model capabilities ever improving, adversarial worst case evaluation becomes even more pertinent. To our knowledge, this work is the first to compare models explicitly by their adversarial validated model error rate (vMER), which we define in Section 4.4. 3 Synthetic Data Generation We develop a synthetic data generation pipeline for QA that involves four stages: passage selection, answer candidate selection, question g"
2021.emnlp-main.696,D17-1215,1,0.769976,"Missing"
2021.emnlp-main.696,2020.acl-main.441,1,0.791342,"Missing"
2021.emnlp-main.696,2021.acl-long.186,1,0.733505,"part of the evaluation for a new round of the Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lew"
2021.emnlp-main.696,2020.acl-main.703,0,0.172923,"Baseline Systems We investigate three baseline systems; noun phrases and named entities following Lewis et al. (2019), as well as an extended part-of-speech tagger incorporating named entities, adjectives, noun phrases, numbers, distinct proper nouns, and clauses. Span Extraction We fine-tune a RoBERTaLarge span extraction model as investigated in previous work (Alberti et al., 2019; Lewis and Fan, 2019). We treat the number of candidates to sample as a hyper-parameter and select the optimal value for k ∈ {1, 5, 10, 15, 20} on the validation set. Generative Answer Detection We use BARTLarge (Lewis et al., 2020) in two settings; one generating answer and question, and the other where we generate the answer only, as we find that this setting provides better control of answer diversity. We use the same range of k ∈ {1, 5, 10, 15, 20} for both settings. 3.1.1 Passage Selection The text passages we use are sourced from SQuAD (further details can be found in Appendix A). We Self-Attention Labelling (SAL) We propose a also experiment with using passages external to multi-label classification head to jointly model canSQuAD, which also sourced from Wikipedia. To didate start and end tokens, and provide a bin"
2021.emnlp-main.696,P19-1484,1,0.842616,"ends, with improved performance. Since SQuAD and the AdversarialQA datasets use the same passages partitioned into the same data splits, we align the annotated answers to create representative answer selection training, validation and test sets. Dataset statistics (see Appendix C), highlight the high percentage of overlapping answers suggesting that existing answer tagging methods (Zhou et al., 2017; Zhao et al., 2018) might struggle, and models should ideally be capable of handling span overlap. Baseline Systems We investigate three baseline systems; noun phrases and named entities following Lewis et al. (2019), as well as an extended part-of-speech tagger incorporating named entities, adjectives, noun phrases, numbers, distinct proper nouns, and clauses. Span Extraction We fine-tune a RoBERTaLarge span extraction model as investigated in previous work (Alberti et al., 2019; Lewis and Fan, 2019). We treat the number of candidates to sample as a hyper-parameter and select the optimal value for k ∈ {1, 5, 10, 15, 20} on the validation set. Generative Answer Detection We use BARTLarge (Lewis et al., 2020) in two settings; one generating answer and question, and the other where we generate the answer on"
2021.emnlp-main.696,2021.eacl-main.86,1,0.886215,"een lection is often used to evaluate the capabilities shown to suffer from various problems. In particu- of current state-of-the-art models, but it can also lar, they often exhibit inadvertent spurious statisti- create higher-quality training data (Bartolo et al., cal patterns that models learn to exploit, leading to 2020; Nie et al., 2020) due to the added incentive poor model robustness and generalisation (Jia and for crowdworkers to provide challenging examples. Liang, 2017; Gururangan et al., 2018; Geva et al., It can also reduce the prevalence of dataset biases 2019; McCoy et al., 2019; Lewis et al., 2021a). and annotator artefacts over time (Bartolo et al., ∗ 2020; Nie et al., 2020), since such phenomena can Most of this work was carried out while MB was at Facebook AI Research. be subverted by model-fooling examples collected 8830 1 Introduction Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8830–8848 c November 7–11, 2021. 2021 Association for Computational Linguistics in subsequent rounds. However, dynamic data collection can be more expensive than its static predecessor as creating examples that elicit a certain model response (i.e., fooling"
2021.emnlp-main.696,D16-1264,0,0.115275,"Missing"
2021.emnlp-main.696,2020.acl-main.442,0,0.0352994,"Missing"
2021.emnlp-main.696,2021.acl-long.132,1,0.719606,"he Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri"
2021.emnlp-main.696,Q19-1029,0,0.0183855,"t baseline. The collected dataset will form part of the evaluation for a new round of the Dynabench QA task.1 2 Related Work 2.1 Adversarial Data Collection We directly extend the AdversarialQA dataset collected in “Beat the AI” (Bartolo et al., 2020), which uses the same passages as SQuAD1.1. AdversarialQA was collected by asking crowdworkers to write extractive question-answering examples that three different models-in-the-loop were unable to answer correctly, creating the DBiDAF , DBERT , and DRoBERTa subsets. Other datasets for question answering (Rajpurkar et al., 2018; Dua et al., 2019; Wallace et al., 2019), sentiment analysis (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du"
2021.emnlp-main.696,2020.findings-emnlp.90,0,0.0320765,"Missing"
2021.emnlp-main.696,D18-1424,0,0.110175,"s (Potts et al., 2021), hate speech detection (Vidgen et al., 2021), and natural language inference (Nie et al., 2020) have been collected in a similar manner. While appealing, human-generated adversarial data is expensive to collect; our work is complementary in that it explores methods to extract further value from existing adversarially collected datasets without requiring additional annotation effort. 1 https://dynabench.org/tasks/qa 2.2 Synthetic Question Generation Many approaches have been proposed to generate question-answer pairs given a passage (Du et al., 2017; Du and Cardie, 2018; Zhao et al., 2018; Lewis and Fan, 2019; Alberti et al., 2019; Puri et al., 2020; Lewis et al., 2021b). These generally use a two-stage pipeline that first identifies an answer conditioned on a passage, then generates a question conditioned on the passage and answer; we train a similar pipeline in our work. G-DAUG (Yang et al., 2020) trains generative models to synthesise training data for commonsense reasoning. Our work focuses on extractive question-answering (QA), which motivates the need for different generative models. Yang et al. (2020) filter generated examples using influence functions, or methods that"
2021.findings-emnlp.11,Q14-1006,0,0.0577922,"cross-modal translation from images to text. Being able to seamlessly “hot swap” knowledge sources without the need for re-training the model affords a unique scalability not typically seen in the traditional deep learning literature. Nearest neighbor methods are known to be strong baselines in the vision and language domain (Devlin et al., 2015). Our contributions are as follows. We introduce a simple, yet effective, novel cross-modal alignment architecture called DXR (Dense X-modal Retriever). DXR achieves a substantial increase in performance on both COCO (Chen et al., 2015) and Flickr30k (Young et al., 2014) image-caption retrieval, with respect to similar methods. We subsequently use DXR as a retrieval component augmenting several multi-modal transformer architectures. We show that retrieval augmentation yields impressive results irrespective of the exact input strategy, with good performs on VQA for retrieval-augmented versions of well-known multi-modal transformer architectures, from VisualBERT (Li et al., 2019b) and ViLBERT (Lu et al., 2019)—which use bounding-box features— to Movie+MCAN (Nguyen et al., 2020)—which uses grid features. We name our overall method XTRA, for X-modal Transformer R"
2021.findings-emnlp.320,P17-1171,1,0.888066,"Missing"
2021.findings-emnlp.320,2021.tacl-1.6,0,0.34195,"ecent methods have focused on: determining which elements of a given piece of knowledge are informative to the dialogue, which is commonly referred to as “knowledge selection” (Zhao et al., 2020b; Kim et al., 2020; Bruyn et al., 2020); learning how to attend to the relevant knowledge (Ma et al., 2020; Cai et al., 2020; Zhao et al., 2020a); or examining how much knowledge is present in large language models (Zhao et al., 2020c). Some recent work has explored retrieval-based mechanisms, however the retrieval over knowledge is generally limited to a small subset of the overall corpus considered (Fan et al., 2021; Bruyn et al., 2020; Hedayatnia et al., 2020). Incorporating unstructured textual knowledge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b). We note that very recently retrieval augmented generation has been applied to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as"
2021.findings-emnlp.320,P18-1082,0,0.0706773,"Missing"
2021.findings-emnlp.320,P17-1147,0,0.0765016,"Missing"
2021.findings-emnlp.320,2020.emnlp-main.550,0,0.0233327,"ion. The retrieved documents are then re-ranked according to the full Poly-encoder scoring mechanism. Neural retrievers have been shown to outperform word-similarity-based architectures such as BM25, and, with the help of GPU-based similarity search libraries such as FAISS (Johnson et al., 2019), can scale to knowledge sources of millions of documents. We first discuss these new architectures. Lewis et al. (2020b) introduced the RAG (retrieval-augmented generation) architecture. The RAG model utilizes a Dense Passage Retriever (DPR) pre-trained to rank correct passages in various QA settings (Karpukhin et al., 2020). A large FAISS index stores d(zj ), with q(xi ) as the query for relevant documents. RAG-Sequence considers documents independently, generating an output sequence for each concatenated context separately and marginalizing over the output generations. RAG-Token marginalizes the output distribution over all documents, allowing the generator to attend over a different document for each token. Though d(zj ) remains fixed during training, token losses are propagated to the retriever itself, and the context representations q(xi ) are updated in order to better fit the retriever for the task. Izacar"
2021.findings-emnlp.320,2021.acl-short.47,0,0.0193211,"ge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b). We note that very recently retrieval augmented generation has been applied to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as a memory (Yogatama et al., 2021), especially using k-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017). Hallucination in text-generation models is a topic that has received attention recently, particularly in the settings of summarization (Maynez et al., 2020), machine translation (Zhou et al., 2021), and news generation (Zellers et al., 2019). For dialogue, it has been observed in state-of-the-art models (Roller et al., 2021) and studied in depth (Mielke et al., 2020), but so far without resolution. Open-domain question answering (QA) has 3 Model Architectures long considered retrieval as an intermediate step (Voorhees and Tice, 2000). It has be"
2021.findings-emnlp.320,Q19-1026,0,0.0293049,"Missing"
2021.findings-emnlp.320,2020.acl-main.703,0,0.654437,"the text is the GPT3 generation using default parameters. Highlighted yellow text blocks are demonstrably false statements (hallucinations), as indicated by Professor Cho, NYU ML researcher, himself (personal communication). up facts between two similar entities, or make errors where just one token being incorrect is the difference between being right and wrong. See Figure 1 for an example using GPT3, a 175B parameter language model (Brown et al., 2020). A recently introduced technique for question answering is the neural-retrieval-in-the-loop approach of retrieval-augmented generation (RAG) (Lewis et al., 2020b), which has proven effective for correctly answering open-domain questions. The tech1 Introduction nique employs an encoder-decoder to encode the Large language models trained on large corpora question and decode (generate) the answer, where have made great inroads in the fluency and con- the encoding is augmented with documents or pasversational ability of dialogue agents (Adiwardana sages retrieved from a large unstructured document et al., 2020; Roller et al., 2021), yielding low per- set using a learnt matching function; the entire neuplexity models that have corresponding high to- ral n"
2021.findings-emnlp.320,D16-1230,0,0.0937326,"Missing"
2021.findings-emnlp.320,2020.findings-emnlp.122,0,0.0338973,"its occurrence (Dinan et al., 2019b; Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019; Galetzka et al., 2020). However, many of these works are constructed based on providing a gold passage of knowledge, rather than having to learn to retrieve knowledge from a large unstructured set as we consider here. Recent methods have focused on: determining which elements of a given piece of knowledge are informative to the dialogue, which is commonly referred to as “knowledge selection” (Zhao et al., 2020b; Kim et al., 2020; Bruyn et al., 2020); learning how to attend to the relevant knowledge (Ma et al., 2020; Cai et al., 2020; Zhao et al., 2020a); or examining how much knowledge is present in large language models (Zhao et al., 2020c). Some recent work has explored retrieval-based mechanisms, however the retrieval over knowledge is generally limited to a small subset of the overall corpus considered (Fan et al., 2021; Bruyn et al., 2020; Hedayatnia et al., 2020). Incorporating unstructured textual knowledge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b). We note that very recently retrieval augmented generation"
2021.findings-emnlp.320,2020.acl-main.173,0,0.0342436,"ed to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as a memory (Yogatama et al., 2021), especially using k-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017). Hallucination in text-generation models is a topic that has received attention recently, particularly in the settings of summarization (Maynez et al., 2020), machine translation (Zhou et al., 2021), and news generation (Zellers et al., 2019). For dialogue, it has been observed in state-of-the-art models (Roller et al., 2021) and studied in depth (Mielke et al., 2020), but so far without resolution. Open-domain question answering (QA) has 3 Model Architectures long considered retrieval as an intermediate step (Voorhees and Tice, 2000). It has become a We extend neural-retriever-in-the-loop generativemore intensively studied topic recently, first using based architectures, which have performed well in simple vector-space based retrievers (Chen et a"
2021.findings-emnlp.320,2020.acl-main.64,0,0.0158586,"logue context and retrieving knowledge from the entire of Wikipedia. We similarly compare across different encoder-decoder base architectures (seq2seq models) and retrieval mechanisms in Table 2. Overall, we see that retrieval helps substantially in improving performance on both knowledge-grounded conversational datasets. 4.2 Eliminating Hallucination fields such as machine translation and QA, standard automated metrics such as F1, BLEU, and ROUGE have been shown to be not totally correlated with how well neural conversational models perform in the wild (Liu et al., 2016; Dinan et al., 2019a; Mehri and Eskenazi, 2020). We thus introduce an additional metric, Knowledge F1. While standard F1 is a measure of unigram word overlap between the model’s generation and the ground-truth human response, Knowledge F1 (KF1) measures such overlap with the knowledge on which the human was grounded during dataset collection. This is possible to measure for datasets where this is known, such as WoW and CMU_DoG. KF1 attempts to capture whether a model is speaking knowledgeably by using relevant knowledge as judged by humans, whereas standard F1 captures conversational ability, including token overlap that is unrelated to kn"
2021.findings-emnlp.320,D17-2014,1,0.877898,"Missing"
2021.findings-emnlp.320,2021.findings-acl.120,0,0.0312853,"Missing"
2021.findings-emnlp.320,D18-1076,0,0.127772,"r, knowledge-grounded dialogue offers a more challenging (or at the very least, materially different) retrieval task than question answering. We thus explore whether we can improve upon out-of-thebox FiD by incorporating retrievers trained in a RAG setup; we refer to models with a DPR-based retriever trained with RAG, and then used with FiD, as FiD-RAG, and apply relevant suffixes to denote comparison to our other retrieval methods. 4 Experiments Datasets: We conduct experiments on two datasets: Wizard of Wikipedia (WoW) (Dinan et al., 2019b) and CMU Document Grounded Conversations (CMU_DoG) (Zhou et al., 2018) which are both sets of knowledge-grounded dialogues collected through human-human crowdworker chats in English, where one of the crowdworkers had access to external knowledge from Wikipedia; WoW discusses various topics, and CMU_DoG discusses movies. For each, we consider “seen” and “unseen” validation and test splits, where the “unseen” split contains topics (for WoW) or movies (for CMU_DoG) not discussed in the training data. WoW provides these splits, and we constructed our own for CMU_DoG. We employ the standard KiLT Wikipedia dump (Petroni et al., 2021) as our knowledge source for retrie"
2021.findings-emnlp.320,P19-1417,0,0.047496,"Missing"
2021.naacl-main.324,2020.emnlp-main.393,0,0.0658177,"hoice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likeli"
2021.naacl-main.324,2020.tacl-1.3,0,0.0240418,"med “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,W17-5401,0,0.025796,", 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah a"
2021.naacl-main.324,2020.acl-main.465,0,0.0806274,"solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al.,"
2021.naacl-main.324,2020.insights-1.13,0,0.0263233,"er, e.g., the multiple iterations of SemEval or WMT datasets over the years, we’ve already been handling this quite well—we accept that a model’s BLEU score on WMT16 is not comparable to WMT14. That is, it is perfectly natural for benchmark datasets to evolve as the community makes progress. The only thing Dynabench does differently is that it anticipates dataset saturation and embraces the loop so that we can make faster and more sustained progress. ever, it has also been found that model-in-the-loop counterfactually-augmented training data does not necessarily lead to better generalization (Huang et al., 2020). Given the distributional shift induced by adversarial settings, it would probably be wisest to combine adversarially collected data with nonadversarial data during training (ANLI takes this approach), and to also test models in both scenarios. To get the most useful training and testing data, it seems the focus should be on collecting adversarial data with the best available model(s), preferably with a wide range of expertise, as that will likely be beneficial to future models also. That said, we expect this to be both task and model dependent. Much more research is required, and we encourag"
2021.naacl-main.324,2020.acl-main.768,1,0.846016,"e the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle t"
2021.naacl-main.324,N19-1225,0,0.0738848,"bstantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and that encour- introduce Dynabench, an open-source, web-based age apples-to-apples model comparisons. Bench- research platform for dynamic data collection and marks provide a north star goal for researchers, and model benchmarking. The guiding hypothesis be4110 Proceedings of the 2021 Con"
2021.naacl-main.324,D17-1215,1,0.795486,"ard et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et a"
2021.naacl-main.324,2021.ccl-1.108,0,0.0774911,"Missing"
2021.naacl-main.324,D15-1166,0,0.00793057,"rk tasks, that milestone is now rou- cording to the narrow criteria used to define human performance) nonetheless fail on simple chaltinely reached within just a few years for newer lenge examples and falter in real-world scenarios. datasets (see Figure 1). As with the rest of AI, NLP A substantial part of the problem is that our benchhas advanced rapidly thanks to improvements in computational power, as well as algorithmic break- mark tasks are not adequate proxies for the sothroughs, ranging from attention mechanisms (Bah- phisticated and wide-ranging capabilities we are danau et al., 2014; Luong et al., 2015), to Trans- targeting: they contain inadvertent and unwanted formers (Vaswani et al., 2017), to pre-trained lan- statistical and social biases that make them artificially easy and misaligned with our true goals. guage models (Howard and Ruder, 2018; Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019; We believe the time is ripe to radically rethink Brown et al., 2020). Equally important has been the benchmarking. In this paper, which both takes a rise of benchmarks that support the development of position and seeks to offer a partial solution, we ambitious new data-driven models and"
2021.naacl-main.324,2020.emnlp-main.154,0,0.0295127,"uperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring"
2021.naacl-main.324,J93-2004,0,0.0749322,"humans? This reveals the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang"
2021.naacl-main.324,C10-1091,0,0.0606954,"Missing"
2021.naacl-main.324,N19-1063,0,0.0241688,"d its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not s"
2021.naacl-main.324,P19-1334,0,0.0218097,"ang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al.,"
2021.naacl-main.324,K18-1007,1,0.843089,"for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing tar"
2021.naacl-main.324,W10-0719,1,0.726488,"collaborative effort, the platform is meant to be a platform technology for humanand-model-in-the-loop evaluation that belongs to the entire community. In the current iteration, the platform is set up for dynamic adversarial data collection, where humans can attempt to find modelfooling examples. This design choice is due to the 2.3 Other Related Work fact that the average case, as measured by maxiWhile crowdsourcing has been a boon for large- mum likelihood training on i.i.d. datasets, is much scale NLP dataset creation (Snow et al., 2008; less interesting than the worst (i.e., adversarial) Munro et al., 2010), we ultimately want NLP sys- case, which is what we want our systems to be able tems to handle “natural” data (Kwiatkowski et al., to handle if they are put in critical systems where 2019) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics."
2021.naacl-main.324,C18-1198,0,0.0239103,"ard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Mo"
2021.naacl-main.324,2020.acl-main.441,1,0.878791,"our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of resource collection and architectural improvements. Similar to Dynabench, recent work seeks to embrace this phenomenon, addressing many of the previously mentioned issues through an iterative human-and-model-in-the-loop annotation process (Yang et al., 2017; Dinan et al., 2019; Chen et al., 2019; Bartolo et al., 2020; Nie et al., 2020), to find “unknown unknowns” (Attenberg et al., 2015) or in a never-ending or life-long learning setting (Silver et al., 2013; Mitchell et al., 2018). The Adversarial NLI (ANLI) dataset (Nie et al., 2020), for example, was collected with an adversarial setting over multiple rounds to yield “a ‘moving post’ dynamic target for NLU systems, rather than a static benchmark that will eventually saturate”. In its few-shot learning mode, GPT-3 barely shows “signs of life” (Brown et al., 2020) (i.e., it is barely above random) on ANLI, which is evidence that we are still far away from human performance"
2021.naacl-main.324,S18-2023,0,0.0556136,"Missing"
2021.naacl-main.324,W12-4501,0,0.0439542,"the shortcomings of state-of-the-art models, and it yields valuable training and assessment data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However"
2021.naacl-main.324,P18-2124,1,0.869954,"Missing"
2021.naacl-main.324,D16-1264,0,0.230428,"n use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its lead"
2021.naacl-main.324,2020.acl-main.442,0,0.230761,"performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by simply adding a relevant sentence to the passage (Jia and Liang, 2017). Text classification models have been shown to be sensitive to single input character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuri"
2021.naacl-main.324,K19-1019,0,0.019609,"al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models"
2021.naacl-main.324,N18-2002,0,0.0355102,"Missing"
2021.naacl-main.324,2020.emnlp-main.661,1,0.822242,"Missing"
2021.naacl-main.324,P19-1004,0,0.0188742,"put character change (Ebrahimi et al., 2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally"
2021.naacl-main.324,2020.acl-main.479,0,0.0170779,"ead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Etting"
2021.naacl-main.324,2020.acl-main.222,0,0.0281863,"19) and be “ecologically valid” (de Vries et al., they interact with humans in real-world settings. 2020). Ethayarajh and Jurafsky (2020) analyze the However, Dynabench is not limited to the adverdistinction between what leaderboards incentivize sarial setting, and one can imagine scenarios where and “what is useful in practice” through the lens of humans are rewarded not for fooling a model or microeconomics. A natural setting for exploring ensemble of models, but for finding examples that these ideas might be dialogue (Hancock et al., 2019; models, even if they are right, are very uncertain Shuster et al., 2020). Other works have pointed about, perhaps in an active learning setting. Simout misalignments between maximum-likelihood ilarly, the paradigm is perfectly compatible with training on i.i.d. train/test splits and human lan- collaborative settings that utilize human feedback, guage (Linzen, 2020; Stiennon et al., 2020). or even negotiation. The crucial aspect of this proposal is the fact that models and humans interact We think there is widespread agreement that something has to change about our standard eval- live “in the loop” for evaluation and data collection. uation paradigm and that we nee"
2021.naacl-main.324,D08-1027,0,0.352191,"Missing"
2021.naacl-main.324,D13-1170,1,0.0131607,"t data which the community can use to develop even stronger models. In this paper, we first document the background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather tha"
2021.naacl-main.324,2020.emnlp-main.746,0,0.0673253,"Missing"
2021.naacl-main.324,N18-1074,0,0.0416359,"Missing"
2021.naacl-main.324,L18-1239,0,0.0544033,"Missing"
2021.naacl-main.324,W19-3509,1,0.836306,"Missing"
2021.naacl-main.324,D19-1221,0,0.0180989,"2018b) and first-order logic inconsistencies (Minervini and Riedel, 2018). Similarly, machine translation systems have been found susceptible to character-level perturbations (Ebrahimi et al., 2018a) and synthetic and natural noise (Belinkov and Bisk, 2018; Khayrallah and Koehn, 2018). Natural language inference models can be fooled by simple syntactic heuristics or hypothesis-only biases (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Belinkov et al., 2019; McCoy et al., 2019). Dialogue models may ignore perturbations of dialogue history (Sankar et al., 2019). More generally, Wallace et al. (2019) find universal adversarial perturbations forcing targeted model errors across a range of tasks. Recent work has also focused on evaluating model diagnostics through counterfactual augmentation (Kaushik et al., 2020), 2.1 Challenge Sets and Adversarial Settings decision boundary analysis (Gardner et al., 2020; Whether our models have learned to solve tasks Swayamdipta et al., 2020), and behavioural testin robust and generalizable ways has been a topic ing (Ribeiro et al., 2020). 4111 2.2 Adversarial Training and Testing Research progress has traditionally been driven by a cyclical process of r"
2021.naacl-main.324,W18-5446,1,0.794698,"Missing"
2021.naacl-main.324,D19-1286,0,0.0188342,"ive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressiv"
2021.naacl-main.324,2020.tacl-1.25,0,0.0303378,"enging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzi"
2021.naacl-main.324,W17-3012,1,0.867269,"Missing"
2021.naacl-main.324,D18-1501,0,0.0349494,"Missing"
2021.naacl-main.324,N18-1101,1,0.774917,"he background that led us to propose this platform. We then describe the platform in technical detail, report on findings for four initial tasks, and address possible objections. We finish with a discussion of future plans and next steps. 2 Background Progress in NLP has traditionally been measured through a selection of task-level datasets that gradually became accepted benchmarks (Marcus et al., 1993; Pradhan et al., 2012). Recent well-known examples include the Stanford Sentiment Treebank (Socher et al., 2013), SQuAD (Rajpurkar et al., 2016, 2018), SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018). More recently, multi-task benchmarks such as SentEval (Conneau and Kiela, 2018), DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) were proposed with the aim of measuring general progress across several tasks. When the GLUE dataset was introduced, “solving GLUE” was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive bod"
2021.naacl-main.324,D18-1259,0,0.0459996,"Missing"
2021.naacl-main.324,2020.emnlp-main.397,0,0.0238252,"was deemed “beyond the capability of current transfer learning methods” (Wang et al., 2018). However, GLUE saturated within a year and its successor, SuperGLUE, already has models rather than humans at the top of its leaderboard. These are remarkable achievements, but there is an extensive body of evidence indicating that these models do not in fact have the humanlevel natural language capabilities one might be lead to believe. of much recent interest. Challenging test sets have shown that many state-of-the-art NLP models struggle with compositionality (Nie et al., 2019; Kim and Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack s"
2021.naacl-main.324,2020.emnlp-main.659,1,0.819658,"Linzen, 2020; Yu and Ettinger, 2020; White et al., 2020), and find it difficult to pass the myriad stress tests for social (Rudinger et al., 2018; May et al., 2019; Nangia et al., 2020) and/or linguistic competencies (Geiger et al., 2018; Naik et al., 2018; Glockner et al., 2018; White et al., 2018; Warstadt et al., 2019; Gauthier et al., 2020; Hossain et al., 2020; Jeretic et al., 2020; Lewis et al., 2020; Saha et al., 2020; Schuster et al., 2020; Sugawara et al., 2020; Warstadt et al., 2020). Yet, challenge sets may suffer from performance instability (Liu et al., 2019a; Rozen et al., 2019; Zhou et al., 2020) and often lack sufficient statistical power (Card et al., 2020), suggesting that, although they may be valuable assessment tools, they are not sufficient for ensuring that our models have achieved the learning targets we set for them. Models are susceptible to adversarial attacks, and despite impressive task-level performance, state-of-the-art systems still struggle to learn robust representations of linguistic knowledge (Ettinger et al., 2017), as also shown by work analyzing model diagnostics (Ettinger, 2020; Ribeiro et al., 2020). For example, question answering models can be fooled by sim"
2021.woah-1.21,Q19-1038,0,0.0174625,"ng entry in the hateful memes challenge (Zhu, 2020) - a VLBERT multimodal model with image specific metadata. It was fine-tuned on the fine-grained data. The system was only submitted for Task A. Duisburg-Essen System 2 (LTL-UDE2) An additional emotion tags are added to DE1 which are extracted from the facial expressions of persons objects available in the meme image. The system was only submitted for Task A. Queen Mary University London (QMUL) The submitted system is a multimodal model that uses CLIP (Radford et al., 2021) image encoder to embed the meme images, and CLIP text encoder, LASER (Artetxe and Schwenk, 2019) & LaBSE (Feng et al., 2020) to embed the meme text. All the representations are concatenated, and a multi-label logistic regression classifier is trained, one for each task, to predict the labels. Stockholm University System 1 (SU1) A BERT-base based model that only uses the text of the meme as input. The BERT model was fine-tuned independently for each task. 5 Conclusion Detecting hate remains technically difficult, with many unaddressed or unsolved challenges and frontiers. Hateful memes are one issue that has received little attention, despite the ubiquity of such media online. The shared"
2021.woah-1.21,2020.lrec-1.760,0,0.0728204,"Missing"
2021.woah-1.21,W19-3504,0,0.0182822,"and ability to engage in open discussions. Ensuring that online spaces are both open and safe requires being able to reliably and accurately find, rate and remove harmful content such as hate. Scalable machine learning based solutions offer a powerful way of solving this problem, reducing the burden on human moderators. To date, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalizability and fairness of even state-of-the-art models (Waseem et al., 2018; Vidgen et al., 2019; Caselli et al., 2020b; Mishra et al., 2019; Davidson et al., 2019). To advance the field, and develop models which can be used in real-world settings, research needs to go beyond simple binary classifications of textual content. To this end, we have used trained professional moderators to reannotate the hateful memes dataset from (Kiela 3 Dataset 3.1 Dataset Size The dataset we present for the shared task is from phase 1 of the hateful memes challenge Kiela et al. (2020)2 . Table 1 shows the distribution and data splits associated with the released dataset. We reannotated the hateful memes for the two finegrained categories (Protected category and Attack typ"
2021.woah-1.21,W19-3509,1,0.719612,"s must be balanced with protecting people’s freedom of expression and ability to engage in open discussions. Ensuring that online spaces are both open and safe requires being able to reliably and accurately find, rate and remove harmful content such as hate. Scalable machine learning based solutions offer a powerful way of solving this problem, reducing the burden on human moderators. To date, detecting online hate has proven remarkably difficult and concerns have been raised about the performance, robustness, generalizability and fairness of even state-of-the-art models (Waseem et al., 2018; Vidgen et al., 2019; Caselli et al., 2020b; Mishra et al., 2019; Davidson et al., 2019). To advance the field, and develop models which can be used in real-world settings, research needs to go beyond simple binary classifications of textual content. To this end, we have used trained professional moderators to reannotate the hateful memes dataset from (Kiela 3 Dataset 3.1 Dataset Size The dataset we present for the shared task is from phase 1 of the hateful memes challenge Kiela et al. (2020)2 . Table 1 shows the distribution and data splits associated with the released dataset. We reannotated the hateful memes f"
2021.woah-1.21,W17-3012,1,0.822973,"k Results & Analysis Shared Task Setup For WOAH 5, collocated with ACL, we introduced two hateful meme detection tasks: Task A: Protected Category For each meme, detect the protected category. The protected categories recorded in the dataset are: race, disability, religion, nationality, sex.4 If the meme is not hateful the protected category is recorded as “pc empty”. Dataset Labels Each meme was originally labelled as ‘Hateful’ or ‘Not Hateful’ by Kiela et al. (2020). Hate is a contested concept and there is no generally agreed upon definition or taxonomy in the field (Caselli et al., 2020a; Waseem et al., 2017; Zampieri et al., 2019). For the purposes of this work, hate is defined as a direct attack against people based on ‘protected characteristics’3 . Protected characteristics are core aspects of a person’s social identity which are generally fixed or immutable. Table 2 provides the set of fine-grained labels for protected classes and attack types. 3.3 4 Task B: Attack Type For each meme, detect the attack type. The attack types recorded in the dataset are: contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting violence. If the meme is not hateful the attack type is recorded as"
2021.woah-1.21,N19-1144,0,0.0182533,"Shared Task Setup For WOAH 5, collocated with ACL, we introduced two hateful meme detection tasks: Task A: Protected Category For each meme, detect the protected category. The protected categories recorded in the dataset are: race, disability, religion, nationality, sex.4 If the meme is not hateful the protected category is recorded as “pc empty”. Dataset Labels Each meme was originally labelled as ‘Hateful’ or ‘Not Hateful’ by Kiela et al. (2020). Hate is a contested concept and there is no generally agreed upon definition or taxonomy in the field (Caselli et al., 2020a; Waseem et al., 2017; Zampieri et al., 2019). For the purposes of this work, hate is defined as a direct attack against people based on ‘protected characteristics’3 . Protected characteristics are core aspects of a person’s social identity which are generally fixed or immutable. Table 2 provides the set of fine-grained labels for protected classes and attack types. 3.3 4 Task B: Attack Type For each meme, detect the attack type. The attack types recorded in the dataset are: contempt, mocking, inferiority, slurs, exclusion, dehumanizing, inciting violence. If the meme is not hateful the attack type is recorded as “attack empty”. Tasks A"
C16-1220,W02-2203,0,0.0832599,"er et al., 2016). Table 4.1 shows the distribution of 1,580 abstracts and sentences for each of the hallmark categories. The inter-annotator agreement is k = 0.81. Hallmark # Abstracts # Sentences PS 462 993 GS 242 468 CD 430 883 RI 115 295 A 143 357 IM 291 667 GI 333 771 PI 240 520 CE 105 213 ID 108 226 Table 1: Distribution of data for the ten hallmarks. 4.2 Handcrafted supervised model We employ a fully supervised handcrafted baseline for this task, classifying using binary classifiers for each hallmark category. Sentences are first tokenised and part-of-speech tagged using the C&C tagger (Clark, 2002) trained on biomedical texts. The text is lemmatised using BioLemmatizer (Liu et al., 2012) and grammatical relations are extracted using the C&C Parser. The parser was trained using molecular biology annotations (Rimell and Clark, 2009). Finally, named entities are extracted from parsed data using ABNER (Settles, 2005), trained on the NLPBA and BioCreative corpora (Leitner et al., 2010). We experimented with several types of handcrafted features for hallmark classification, chosen based on their inclusion in other state-of-the-art biomedical text classification systems. Only the first five ar"
C16-1220,W10-1913,1,0.821517,"gument by segmenting a document into several zones, such as: “Objective”, “Background”, “Method”, “Result”, and “Conclusion”. This task differs from Task 1 in that the objective is to classify scientific text according to generic labels (i.e., unrelated to domain-specific knowledge) and the focus is on a different classification features, such as the position of the text in the document and the author’s writing style. For example, the “Objective” zone of the argument generally appears very early in the article using an active voice. 5.1 Data We evaluate using an expert-annotated dataset from (Guo et al., 2010) comprising of 1000 PubMed abstracts relevant to cancer biology. The dataset consists of 7985 labelled sentences, with an interannotator agreement of k = 0.85. There are five mutually non-exclusive classes, described together with their frequencies in Table 5.1. Class Objective (OBJ) Background (BKG) Method (METH) Result (RES) Conclusion (CON) Description The background and the aim of the research The circumstances pertaining to the current work The way to achieve the goal The principal findings Analysis, discussion and the main conclusions # Abstracts 744 692 640 889 859 # Sentences 812 1517"
C16-1220,C12-3023,1,0.812301,"epigenetic alterations (Marusyk et al., 2012), this framework provides an organizing principle to simplify the complexity of cancer biological processes (Baker et al., 2016). 4.1 Data Baker et al. (2016) acquired a collection of PubMed abstracts using a set of search terms representative for each of the 10 hallmarks. The terms and their synonyms appearing in Hanahan and Weinberg (2000) and Hanahan and Weinberg (2011) were employed along with additional ones selected by a team of cancer researchers. Annotation was conducted by experts in cancer research, using the annotation tool described in Guo et al. (2012). Annotations are assigned at a sentence-level: a sentence is annotated if contains clear evidence relating to one or several hallmarks (Baker et al., 2016). Table 4.1 shows the distribution of 1,580 abstracts and sentences for each of the hallmark categories. The inter-annotator agreement is k = 0.81. Hallmark # Abstracts # Sentences PS 462 993 GS 242 468 CD 430 883 RI 115 295 A 143 357 IM 291 667 GI 333 771 PI 240 520 CE 105 213 ID 108 226 Table 1: Distribution of data for the ten hallmarks. 4.2 Handcrafted supervised model We employ a fully supervised handcrafted baseline for this task, cla"
C16-1220,reschke-etal-2014-event,0,0.0197833,"small labelled datasets. There are works that target small labelled data text classification in sparse domains using techniques such as active learning (Guo et al., 2013; Figueroa et al., 2012; Nissim et al., 2015). The idea of active learning is to reduce annotation effort by iteratively selecting the most informative instances to be labelled by interactively querying an expert. Although good accuracy can be achieved, the approach relies on expert knowledge and interaction, and may still require feature engineering. Other works tackle the sparsity of labelled data using distant supervision (Reschke et al., 2014; Vivaldi and Rodr´ıguez, 2015). Here, a classifier is trained using data labelled automatically using approximate heuristics rather than annotators. However, due to the assumptions and bias that are inherent in such labelling heuristics, this may result in lower performance. The work presented in this paper differs from the above as it focuses on learning embeddings for sparse domains with small labelled datasets; moreover, we focus on utilizing these embeddings specifically for text classification. 3 Approach This section first describes the Distributed Memory model (Section 3.1), and then e"
C16-1220,D09-1067,1,0.806461,"feature employs all words occurring in input texts. We lemmatise the words in order to reduce sparsity. Noun bigrams: Noun bigrams are used because they can be useful in capturing two word-concepts in texts (e.g., Gene silencing). Grammatical relations: we use the dobj (direct object), ncsubj (non-clausal subject), and iobj (indirect object) relations, plus the head and dependent words in relations. Verb classes: verb classes group semantically similar verbs together, abstracting away from individual words when faced with data sparsity. We used the hierarchical classification of 399 verbs by Sun and Korhonen (2009). Named entities: domain-specific concepts, providing another way to group bags of words into meaningful categories. We use five types which are particularly relevant for cancer research: Proteins, DNA, RNA, Cell Line, and Cell Type. Medical Subject Headings (MeSH): is a comprehensive controlled vocabulary for indexing journal articles and books in the life sciences. Most abstracts in our dataset contain an associated list of MeSH terms which we employ as features. Chemicals list: a total of 3,021 associated chemicals (manually annotated). We use these as features, since processes involved wit"
C16-1220,J02-4002,0,0.0425339,"ehensive controlled vocabulary for indexing journal articles and books in the life sciences. Most abstracts in our dataset contain an associated list of MeSH terms which we employ as features. Chemicals list: a total of 3,021 associated chemicals (manually annotated). We use these as features, since processes involved with hallmarks might involve similar chemicals. 5 Task 2: Rhetorical text classification Rhetorical text classification (also known as information structure analysis) segments scientific text into information categories. One such classification technique is argumentative zoning (Teufel and Moens, 2002) which captures the rhetorical progression of the scientific argument by segmenting a document into several zones, such as: “Objective”, “Background”, “Method”, “Result”, and “Conclusion”. This task differs from Task 1 in that the objective is to classify scientific text according to generic labels (i.e., unrelated to domain-specific knowledge) and the focus is on a different classification features, such as the position of the text in the document and the author’s writing style. For example, the “Objective” zone of the argument generally appears very early in the article using an active voice"
C16-1220,P14-1074,0,0.0310738,"ta. 2 Related Work Embedded distributed representations have been used widely for document and sentence classification. For example, Huang et al. (2014) learn document-level embeddings using word-level embeddings as input. Yan et al. (2015) learn document-embeddings by combing a Deep Boltzmann Machine and a Deep Belief Network. Bhatia et al. (2015) learn embeddings for large multi-label classification in situations where the label set is extremely large. Liu et al. (2015) use latent topic models to learn a topic from each word, and then learn an embedding based on both the topic and the word. Yogatama and Smith (2014) use structured regularizers based on parse trees, topics, and hierarchical word clusters, as well as hierarchical sparse coding for regularization using stochastic proximal methods (Yogatama et al., 2015). All of these works have been trained and evaluated on general domains such as newswire rather than on sparse domains with small labelled datasets. There are works that target small labelled data text classification in sparse domains using techniques such as active learning (Guo et al., 2013; Figueroa et al., 2012; Nissim et al., 2015). The idea of active learning is to reduce annotation eff"
D13-1147,W03-1809,0,0.0106348,"fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 201"
D13-1147,D10-1115,0,0.27911,"of verb-noun pairs: 84 phrases contain pronouns, while there are also several examples containing words that WordNet considers to be adjectives rather than nouns. This problem was mitigated by part-of-speech tagging the dataset. As neighbours for pronouns (which are not included in WordNet), we used the other pronouns present in the dataset. For the remaining words, we included the part-of-speech when looking up the word in WordNet. 3.1 Average distance compositionality score We considered several different ways of constructing phrasal vectors. We chose not to use the compositional models of Baroni and Zamparelli (2010) and Socher et al. (2011) because we believe that it is important that our methods are completely unsupervised and do not require any initial learning phase. 1429 Hence, we experimented with different ways of constructing phrasal vectors according to Mitchell and Lapata (2010) and found that pointwise multiplication worked best in our experiments. Thus, we −−−−→ define the composed vector eat hat as: −→ −→ eat hat We can now compute a compositionality score sc by averaging the distance between the original phrase vector and its substituted neighbour phrase vectors via the following formula: k"
D13-1147,W06-1203,0,0.0168707,"al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distribution"
D13-1147,P99-1041,0,0.038644,"ty scores for compound nouns: Reddy et al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Wa"
D13-1147,W03-1810,0,0.764549,"Missing"
D13-1147,D07-1039,0,0.875809,"ble 2. Substituting the verb in the non-compositional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. This is a result of using pointwise multiplication, where overlapping components are stressed: since the vectors for take and breath have little overlap outside of Neighbour get breath find breath use breath work breath hold breath run breath carry breath look breath play breath buy breath AvgDist Dist 0.049 0.051 0.050 0.060 0.094 0.079 0.076 0.065 0.071 0.100 0.069 System Venkatapathy and Joshi (2005) McCarthy et al. (2007) AvgDist VSM neighbours-both AvgDist VSM neighbours-verb AvgDist VSM neighbours-noun AvgDist WN-ranked neighbours-both AvgDist WN-ranked neighbours-verb AvgDist WN-ranked neighbours-noun Table 3: Spearman ρs results Table 1: Example take breath Neighbour pay money put money bring money provide money owe money sell money cost money look money distribute money offer money AvgDist Dist 0.446 0.432 0.405 0.442 0.559 0.404 0.482 0.425 0.544 0.428 0.457 Table 2: Example lend money the idiomatic sense in take breath, its neighboursubstituted phrases also have little overlap, resulting in a smaller ch"
D13-1147,I11-1024,0,0.278924,"ct this not to be the case, since e.g. eat trousers, where the noun has been substituted, does not make a lot of sense either — which we would expect to be informative for determining compositionality. There are two possible explanations for this, which might be at play simultaneously: since our dataset consists of verbobject pairs, the verb constituent is always the head word of the phrase, and the dataset contains several so-called “light verbs”, which have little semantic content of their own. Head words have been found to have a higher impact on compositionality scores for compound nouns: Reddy et al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Gi"
D13-1147,W01-0513,0,0.0227808,"or compound nouns: Reddy et al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent d"
D13-1147,W11-0806,0,0.0270309,"en.clark@cl.cam.ac.uk 1. consume her hat 2. eat her trousers Introduction Multi-word expressions (MWEs) are defined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). They tend to have a standard syntactic structure but are often semantically non-compositional; i.e. their meaning is not fully determined by their syntactic structure and the meanings of their constituents. A classic example is kick the bucket, which means to die rather than to hit a bucket with the foot. These types of expressions account for a large proportion of day-to-day language interactions (Schuler and Joshi, 2011) and present a significant problem for natural language processing systems (Sag et al., 2002). This paper presents a novel unsupervised approach to detecting the compositionality of MWEs, specifically of verb-noun collocations. The idea is Both phrases are semantically anomalous, implying that eat hat is a highly non-compositional verb-noun collocation. Following a similar procedure for eat apple, however, would not lead to an anomaly: consume apple and eat pear are perfectly meaningful, leading us to believe that eat apple is compositional. In the context of distributional models, this idea c"
D13-1147,S13-1038,0,0.0751707,"Missing"
D13-1147,E09-1086,0,0.0153543,"ntribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models"
D13-1147,W11-1301,0,0.0148097,"ecade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human"
D13-1147,H05-1113,0,0.823443,"e, are shown in Table 1 and Table 2. Substituting the verb in the non-compositional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. This is a result of using pointwise multiplication, where overlapping components are stressed: since the vectors for take and breath have little overlap outside of Neighbour get breath find breath use breath work breath hold breath run breath carry breath look breath play breath buy breath AvgDist Dist 0.049 0.051 0.050 0.060 0.094 0.079 0.076 0.065 0.071 0.100 0.069 System Venkatapathy and Joshi (2005) McCarthy et al. (2007) AvgDist VSM neighbours-both AvgDist VSM neighbours-verb AvgDist VSM neighbours-noun AvgDist WN-ranked neighbours-both AvgDist WN-ranked neighbours-verb AvgDist WN-ranked neighbours-noun Table 3: Spearman ρs results Table 1: Example take breath Neighbour pay money put money bring money provide money owe money sell money cost money look money distribute money offer money AvgDist Dist 0.446 0.432 0.405 0.442 0.559 0.404 0.482 0.425 0.544 0.428 0.457 Table 2: Example lend money the idiomatic sense in take breath, its neighboursubstituted phrases also have little overlap, re"
D13-1147,R13-1047,0,\N,Missing
D13-1147,W11-1300,0,\N,Missing
D14-1005,N09-1003,0,0.2064,"Missing"
D14-1005,P12-1015,0,0.54992,"ect recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep"
D14-1005,W14-1503,1,0.905208,"ethod of sampling up to N on the ESP Game dataset. In all following experiments, N = 1.000. We used the WordNet lemmatizer from NLTK (Bird et al., 2009) to lemmatize tags and concept words so as to further improve the dataset’s coverage. 4.3 Image Processing The ImageNet images were preprocessed as described by (Krizhevsky et al., 2012). The largest centered square contained in each image is resam39 with at least 50 images in the ESP Game dataset were included in the evaluation pairs. The MEN dataset has been found to mirror the aggregate score over a variety of tasks and similarity datasets (Kiela and Clark, 2014). It is also much larger, with 3000 words pairs consisting of 751 individual words. Although MEN was constructed so as to have at least a minimum amount of images available in the ESP Game dataset for each concept, this is not the case for ImageNet. Hence, similarly to WordSim353, we also evaluate on a subset (MEN-Relevant) for which images are available in both datasets. We evaluate the models in terms of their Spearman ρ correlation with the human relatedness ratings. The similarity between the representations associated with a pair of words is calculated using the cosine similarity: pled to"
D14-1005,P14-2135,1,0.891054,"h brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision community. First, a deep convolutional neural network (CNN) is trained on a large"
D14-1005,D12-1130,0,0.0383266,"state-of-the-art performance on the Pascal VOC classification task. mantic space models extract meanings solely from linguistic data, even though we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional semantics is to obtain representations that combine information from linguistic corpora with information from another modality, obtained from e.g. property norming experiments (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013) or from processing and extracting features from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 3 Figure 1 illustrates how our system computes multi-modal semantic representations. Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep"
D14-1005,P14-1132,0,0.0755408,"from images (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012). This approach has met with quite some success (Bruni et al., 2014). 2.2 3 Figure 1 illustrates how our system computes multi-modal semantic representations. Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating linguistic and visual representations in a single semantic space model. This has the advantage that it allows for separate data sources for the individual modalities. We also learn v"
D14-1005,P14-1068,0,0.414466,"012). This approach has met with quite some success (Bruni et al., 2014). 2.2 3 Figure 1 illustrates how our system computes multi-modal semantic representations. Multi-modal Deep Learning Other examples that apply multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), auto-encoders (Wu et al., 2013) or recursive neural networks (Socher et al., 2014). Multimodal models with deep learning components have also successfully been employed in crossmodal tasks (Lazaridou et al., 2014). Work that is closely related in spirit to ours is by Silberer and Lapata (2014). They use a stacked auto-encoder to learn combined embeddings of textual and visual input. Their visual inputs consist of vectors of visual attributes obtained from learning SVM classifiers on attribute prediction tasks. In contrast, our work keeps the modalities separate and follows the standard multi-modal approach of concatenating linguistic and visual representations in a single semantic space model. This has the advantage that it allows for separate data sources for the individual modalities. We also learn visual representations directly from the images (i.e., we apply deep learning dire"
D14-1005,C94-1103,0,0.086832,"nn/ Training visual features (after Oquab et al., 2014) Convolutional layers C1-C2-C3-C4-C5 C1-C2-C3-C4-C5 FC6 FC7 FC6 6144-dim feature vector Imagenet labels African elephant FC8 Wall clock … Aggregate FC7 6144-dim feature vectors Word 100-dim word projections Multimodal word vector Select images from ImageNet or ESP Fully-connected layers 100-dim word projections w(t-2) w(t-2) w(t) w(t+1) w(t+2) Training linguistic features (after Mikolov et al., 2013) Figure 1: Computing word feature vectors. the 400M word Text8 corpus of Wikipedia text2 together with the 100M word British National Corpus (Leech et al., 1994). We also experimented with dependency-based skip-grams (Levy and Goldberg, 2014) but this did not improve results. The skip-gram model learns high quality semantic representations based on the distributional properties of words in text, and outperforms standard distributional models on a variety of semantic similarity and relatedness tasks. However we note that Bruni et al. (2014) have recently reported an even better performance for their linguistic component using a standard distributional model, although this may have been tuned to the task. layer produces a vector of 1512 scores associate"
D14-1005,I11-1162,0,0.0522386,"ned on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images. 1 Introduction Recent works have shown that multi-modal semantic representation models outperform unimodal linguistic models on a variety of tasks, including modeling semantic relatedness and predicting compositionality (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2012; Roller and Schulte im Walde, 2013; Kiela et al., 2014). These results were obtained by combining linguistic feature representations with robust visual features extracted from a set of images associated with the concept in question. This extraction of visual features usually follows the popular computer vision approach consisting of computing local features, such as SIFT features (Lowe, 1999), and aggregating them as bags of visual words (Sivic and Zisserman, 2003). Meanwhile, deep transfer learning techniques have gained considerable attention in the computer vision commu"
D14-1005,Q14-1017,0,0.555609,"was an intern at Microsoft Research, New York. 36 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36–45, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics systems on a variety of visual recognition tasks (Razavian et al., 2014). Embeddings from stateof-the-art CNNs (such as Krizhevsky et al. (2012)) have been applied successfully to a number of problems in computer vision (Girshick et al., 2013; Zeiler and Fergus, 2013; Donahue et al., 2014). This contribution follows the approach described by Oquab et al. (2014): they train a CNN on 1512 ImageNet synsets (Deng et al., 2009), use the first seven layers of the trained network as feature extractors on the Pascal VOC dataset, and achieve state-of-the-art performance on the Pascal VOC classification task. mantic space models extract meanings solely from linguistic data, even though we know that human semantic knowledge relies heavily on perceptual information (Louwerse, 2011). That is, there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to do this grounding in the context of distributional s"
D14-1005,P14-2050,0,0.0140313,"C1-C2-C3-C4-C5 C1-C2-C3-C4-C5 FC6 FC7 FC6 6144-dim feature vector Imagenet labels African elephant FC8 Wall clock … Aggregate FC7 6144-dim feature vectors Word 100-dim word projections Multimodal word vector Select images from ImageNet or ESP Fully-connected layers 100-dim word projections w(t-2) w(t-2) w(t) w(t+1) w(t+2) Training linguistic features (after Mikolov et al., 2013) Figure 1: Computing word feature vectors. the 400M word Text8 corpus of Wikipedia text2 together with the 100M word British National Corpus (Leech et al., 1994). We also experimented with dependency-based skip-grams (Levy and Goldberg, 2014) but this did not improve results. The skip-gram model learns high quality semantic representations based on the distributional properties of words in text, and outperforms standard distributional models on a variety of semantic similarity and relatedness tasks. However we note that Bruni et al. (2014) have recently reported an even better performance for their linguistic component using a standard distributional model, although this may have been tuned to the task. layer produces a vector of 1512 scores associated with 1000 categories of the ILSVRC-2012 challenge and the 512 additional catego"
D14-1005,D13-1115,0,0.152648,"Missing"
D14-1005,N10-1011,0,\N,Missing
D15-1015,N09-1003,0,0.0221124,"l similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-M EAN and CNN-M AX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are interIt is clear that the per-image similarity metrics perform better on genuine similarity, as measured by SimLex-999, than on relate"
D15-1015,P98-1069,0,0.0757273,"aning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 199"
D15-1015,P04-1067,0,0.0822964,"Missing"
D15-1015,R11-1055,0,0.188877,"Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al.,"
D15-1015,P08-1088,0,0.297869,"when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some langu"
D15-1015,D14-1032,0,0.0322433,"ulti-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (F"
D15-1015,D09-1092,0,0.0138313,"n induction models to learn translations from comparable data (see sect. 3.3). We do not necessarily expect visual methods to outperform linguistic ones, but it is instructive to see the comparison. We compare our visual models against the current state-of-the-art lexicon induction model using comparable data (Vuli´c and Moens, 2013b). This model induces translations from comparable Wikipedia data in two steps: (1) It learns a set of highly reliable one-to-one translation pairs using a shared bilingual space obtained by applying the multilingual probabilistic topic modeling (MuPTM) framework (Mimno et al., 2009). (2) These highly reliable one-to-one translation pairs serve as dimensions of a word-based bilingual semantic space (Gaussier et al., 2004; Tamura et al., 2012). The model then bootstraps from the high-precision seed lexicon of translations and learns new dimensions of the bilingual space until convergence. This model, which we call B OOTS TRAP, obtains the current best results on the evaluation dataset. For more details about the bootstrapping model and its comparison against other approaches, we refer to Vuli´c and Moens (2013b). Table 4 shows the results for the language pairs in the V UL"
D15-1015,D14-1005,1,0.742838,"orming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sou"
D15-1015,P14-2135,1,0.898971,"edge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use"
D15-1015,J03-1002,0,0.00373644,"isual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Assoc"
D15-1015,W02-0902,0,0.108859,"in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003),"
D15-1015,P99-1067,0,0.0295644,"oehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual s"
D15-1015,J99-4009,0,0.872954,"n down by language, are shown in Table 6. B ERGSMA 500 has a lower average image dispersion score in general, and thus is more concrete than V ULIC 1000. It also has less variance. This may explain why we score higher, in absolute terms, on that dataset than on the more abstract one. When examining individual languages in the datasets, we note that the worst performing language on V ULIC 1000 is Italian, which is also the most abstract dataset, with the highest average image dispersion score and the lowest variance. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), but in a more complex way, since abstract concepts express more varied situations (Barsalou and Wiemer-Hastings, 2005). Using an image resource like Google Images that has full coverage for almost any word, means that we can retrieve what we might call “associated” images (such as images of voters for words like democracy) as opposed to “extensional” images (such as images of cats for cat). This explains why we still obtain good performance on the more abstract V ULIC 1000 dataset, in some cases outperforming linguistic methods: even abstract concepts can have a clear visual representation,"
D15-1015,P14-1132,0,0.0141507,"ata (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also h"
D15-1015,D13-1115,0,0.258685,"Missing"
D15-1015,I11-1162,0,0.0481252,"isition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other e"
D15-1015,W02-2026,0,0.0492709,"014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language p"
D15-1015,P10-1011,0,0.0134096,"aches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic m"
D15-1015,lin-etal-2010-new,0,0.0150856,"ambiamento (change) in Italian. Using the two evaluation datasets can potentially provide Evaluations Test Sets. Bergsma and Van Durme’s primary evaluation dataset consists of a set of five hundred matching lexical items for fifteen language pairs, based on six languages. (The fifteen pairs results from all ways of pairing six languages). The data is publicly available online.1 In order to get the five hundred lexical items, they first rank nouns by the conditional probability of them occurring in the pattern “{image,photo,photograph,picture} of {a,an} ” in the web-scale Google N-gram corpus (Lin et al., 2010), and take the top five hundred words as their English lexicon. For each item 1 1 n 2 http://www.clsp.jhu.edu/˜sbergsma/LexImg/ 151 http://people.cs.kuleuven.be/˜ivan.vulic/software/ Figure 2: Example images for the languages in the Bergsma and Van Durme dataset. Method P@1 P@5 P@20 MRR B&VD Visual-Only B&VD Visual + NED 31.1 48.0 41.4 59.5 53.7 68.7 0.367 0.536 CNN-AVG M AX CNN-M AX M AX CNN-M EAN CNN-M AX 56.7 42.8 50.5 51.4 69.2 60.0 62.7 64.9 77.4 64.5 71.1 74.8 0.658 0.529 0.586 0.608 4 We evaluate the four similarity metrics on the B ERGSMA 500 dataset and compare the results to the syst"
D15-1015,D12-1130,0,0.113474,"ith using multiple layers from the same network in an attempt to improve performance. 1 There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequenc"
D15-1015,W13-3523,0,0.0364082,"ver, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and Moens, 2013b). However, these models require document alignments as initial bilingual signals. In this work, following recent research in multi-modal semantics and image representation learning—in particular deep learning and convolutional neural networks—we test the ability of purely visual data to induce shared bilingual spaces and to consequently learn bilingual word correspondences in these spaces. By compiling images related to linguistic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is remove"
D15-1015,P14-1068,0,0.101262,"(Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004)."
D15-1015,P13-1056,0,0.0195593,"3 A Purely Visual Approach to Bilingual Lexicon Learning Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques"
D15-1015,Q14-1017,0,0.0416793,"ures from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 200"
D15-1015,D12-1003,0,0.136163,"ge pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al.,"
D15-1015,N13-1011,1,0.87388,"Missing"
D15-1015,D13-1168,1,0.763446,"Missing"
D15-1015,P11-2084,1,0.863258,"Missing"
D15-1015,N10-1011,0,\N,Missing
D15-1015,J15-4004,0,\N,Missing
D15-1015,C98-1066,0,\N,Missing
D15-1242,N09-1003,0,0.103463,"dness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related"
D15-1242,P14-1023,0,0.0819888,"e same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: t=1 −c≤j≤c u> w ∈Awt We exclude articles with multiple topic labels in order to avoid multi-class document classification. The dataset contains a total of 78 topic labels and 33,226 news articles. We call this approach skip-gram retrofitting. In all cases, our embeddings have 300 dimensions, which has been found to work well (Mikolov et al., 2013a; Baroni et al., 2014) 3 Results for Intrinsic Evaluation We compare standard skip-gram embeddings with retrofitted and jointly learned specialized embeddings, as well as with “fitted” embeddings that were randomly initialized and learned only from the additional semantic resource. In each case, the 2045 Method SimLex-999 MEN Skip-gram 0.31 0.68 Fit-Norms 0.08 0.14 Fit-Thesaurus 0.26 0.14 Joint-Norms-Sampled 0.43 0.72 Joint-Norms-All 0.42 0.67 Joint-Thesaurus-Sampled 0.38 0.69 Joint-Thesaurus-All 0.44 0.60 GB-Retrofit-Norms 0.32 0.71 GB-Retrofit-Thesaurus 0.38 0.68 SG-Retrofit-Norms 0.35 0.71 SG-Retrofit-Thesaurus"
D15-1242,N15-1184,0,0.673336,"well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly. In this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness. Specializing for similarity is achieved by learning from both a corpus and a thesaurus, and for relatedness by learning from both a corpus and a collection of psychological association norms. We also compare the recentlyintroduced technique of graph-based retrofitting (Faruqui et al., 2015) with a skip-gram retrofitting and a skip-gram joint-learning approach. All three methods yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness significantly better than unspecialized spaces, in one case yielding state-of-the-art results for word similarity. More importantly, we show clear improvements in downstream tasks and applications: specialized similarity spaces improve synonym detection, while association spaces work better than both general-purpose and similarityspecialized spaces for document classification. 2 Approach The underlying ass"
D15-1242,W05-0604,0,0.0998654,"zation when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus. 4 4.1 Downstream Tasks and Applications TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4 Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampled 78.75 84.46 Joint-Norms-All 66.25 84.82 Joint-Thesaurus-"
D15-1242,D14-1012,0,0.0133372,"atedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to c"
D15-1242,C00-1058,0,0.0445341,"source. Its information may be detrimental to model optimization when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus. 4 4.1 Downstream Tasks and Applications TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4 Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampl"
D15-1242,D14-1162,0,0.0909535,"Missing"
D15-1242,D13-1170,0,0.00230888,"sification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conv"
D15-1242,P10-1040,0,0.035032,"her similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is c"
D15-1242,J06-3003,0,0.00916113,"ity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly"
D15-1242,J15-4004,1,\N,Missing
D15-1293,P14-1023,0,0.0562673,"re searching for audio files of cars, we retrieve up to 50 files with a maximum duration of 1 minute per file that have been tagged with the label “car”. 4.1 Linguistic Representations For the linguistic representations we use the continuous vector representations from the log-linear skip-gram model of Mikolov et al. (2013). Specifically, we trained 300-dimensional vector representations trained on a dump of the English Wikipedia plus newswire (8 billion words in total).5 These types of representations have been found to yield the highest performance on a variety of semantic similarity tasks (Baroni et al., 2014). 3 http://www.freesound.org. http://www.vorbis.com. 5 We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus. 2463 4 4.2 Auditory Representations els in each category. A common approach to obtaining acoustic features of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) (O’Shaughnessy, 1987). MFCC features are abundant in a wide variety of applications in audio signal processing, ranging from audio information retrieval, to speech and speaker recognition, and music analysis (Eronen, 2003). Such features are derived from the mel-"
D15-1293,R11-1055,0,0.0770156,"ade publicly available at http://www.cl.cam.ac.uk/˜dk427/audio.html. 2 To avoid introducing another parameter, we set the number of latent variables in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation. 4 Approach One reason for using raw image data in multimodal models is that there is a wide variety of resources that contain tagged images, such as ImageNet (Deng et al., 2009) and the ESP Game dataset (Von Ahn and Dabbish, 2004). However, such resources do not exist for audio files, and so we follow a similar approach to Fergus et al. (2005) and Bergsma and Goebel (2011), who use Google Images to obtain images. We use the online search engine Freesound3 to obtain audio files. Freesound is a collaborative database released under Creative Commons licenses, in the form of snippets, samples and recordings, that is aimed at sound artists. The Freesound API allows users to easily search for audio files that have been tagged using certain keywords. For each of the concepts in the evaluation datasets, we used the Freesound API to obtain samples encoded in the standard open source OGG format4 . Because the database contains variable numbers of files, with varying dura"
D15-1293,N10-1011,0,0.0208359,"leads to conceptual representations that can be processed and reasoned with? MEN automobile-car 1.00 taxi-cab 0.92 A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of"
D15-1293,D14-1005,1,0.908705,"popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). rain-storm 0.98 plane-jet 0.81 cat-feline 0.96 horse-mare 0.83 jazz-musician 0.88 sheep-lamb 0.84 bird-eagle 0.88 bird-hawk 0.79 highway-traffic 0.88 band-orchestra 0.71 guitar-piano 0.86 music-melody 0.70 Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited cov"
D15-1293,P14-2135,1,0.917865,"genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater). In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping to the correct label in the auditory modality from the linguistic one and vice-versa. 3.1 Multi-modal Semantics Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of comparisons such as guitar-piano, the auditory modal2462 Dataset MEN AMEN SLex ASLex Linguistic 3000 258 999 296 Auditory 2590 233 534 216 ever having heard a guitar; or map it to the appropriate place in linguistic space without ever having read about a guitar (having only heard it). Table 2: Number of concept pairs for which representations are available in each modality. ity is certainly meaningful, whereas in the case of democracy-anarchism it is probably less so. Therefore, we had two graduate students annotat"
D15-1293,P15-2038,1,0.292746,"ng strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the middle and late fusion models, e.g. using the “audio dispersion” of a word to determine how much auditory input should be included in the multi-modal representation (Kiela et al., 2014). Another interesting possibility is to improve auditory representations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics. Lastly, now that the perceptual modalities of vision, audio and even olfaction (Kiela et al., 2015) have been investigated in the context of distributional semantics, the logical next step for future work is to explore different fusion strategies for multi-modal models that combine various sources of perceptual input into a single grounded model. Acknowledgments DK is supported by EPSRC grant EP/I037512/1. SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. We are grateful to Xavier Serra, Frederic Font Corbera, Alessandro Lopopolo and Emiel van Miltenburg 2468 for useful suggestions and thank the anonymous reviewers for their helpful comments. Felix Hill a"
D15-1293,P14-1132,0,0.0876778,". Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much rescore SimLex-999 score Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et"
D15-1293,I11-1162,0,0.0860397,"resentations that can be processed and reasoned with? MEN automobile-car 1.00 taxi-cab 0.92 A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) appro"
D15-1293,W15-0110,0,0.0886942,"Missing"
D15-1293,D13-1115,0,0.262768,"Missing"
D15-1293,D07-1043,0,0.0310278,"We used Wikipedia to collect a total of 52 instruments and divided them into 5 classes: brass, percussion, piano-based, string and woodwind instruments. For each of the instruments, we collected as many audio files from FreeSound as possible, and used the MM - MIDDLE model with parameter settings that yielded good results in the previous experiments (k = 300 and α = 0.6). We then performed k-means clustering with five cluster centroids and compared results between auditory, linguistic and multi-modal, evaluating the clustering quality using the standard V-measure clustering evaluation metric (Rosenberg and Hirschberg, 2007). This is an interesting problem because instrument classes are determined somewhat by conven2467 Model Auditory Linguistic MM - MIDDLE 0.39 0.47 0.54 V-measure Linguistic 1 baritone 2 lute, zither, xylophone, lyre, cymbals 3 piano, trombone, clarinet, cello, violin 4 castanets, tambourine, claves, maracas 5 trumpet, horn, bugle, cowbell, carillon Multi-modal Figure 4: Performance of uni-modal auditory representations on the four datasets when varying the maximum duration. tion (is a saxophone a brass or a woodwind instrument?). What is more, how instruments actually sound is rarely described"
D15-1293,D12-1130,0,0.254234,"for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Although feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models. However, if the objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre. To understand how perceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary s"
D15-1293,P14-1068,0,0.20521,"ive to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). rain-storm 0.98 plane-jet 0.81 cat-feline 0.96 horse-mare 0.83 jazz-musician 0.88 sheep-lamb 0.84 bird-eagle 0.88 bird-hawk 0.79 highway-traffic 0.88 band-orchestra 0.71 guitar-piano 0.86 music-melody 0.70 Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference"
D15-1293,Q14-1017,0,0.0260286,"semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much rescore SimLex-999 score Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) a"
D15-1293,N15-1016,0,\N,Missing
D15-1293,J15-4004,0,\N,Missing
D15-1293,D14-1032,0,\N,Missing
D16-1043,R11-1055,0,0.265608,"sentational quality (Bullinaria and Levy, 2007; Kiela and Clark, 2014). The same is likely to hold in the case of Google Bing Flickr ImageNet ESP Game Type Search engine Search engine Photo sharing Image database Game Annotation Automatic Automatic Human Human Human Coverage Unlimited Unlimited Unlimited Limited Limited Multi-lingual Yes Yes No No No Sorted Yes Yes Yes No No Tag specificity Unknown Unknown Loose Specific Loose Table 2: Sources of image data. visual representations. Various sources of image data have been used in multi-modal semantics, but there have not been many comparisons: Bergsma and Goebel (2011) compare Google and Flickr, and Kiela and Bottou (2014) compare ImageNet (Deng et al., 2009) and the ESP Game dataset (von Ahn and Dabbish, 2004), but most works use a single data source. In this study, one of our objectives is to asses the quality of various sources of image data. Table 2 provides an overview of the data sources, and Figure 1 shows some example images. We examine the following corpora: Google Images Google’s image search2 results have been found to be comparable to hand-crafted image datasets (Fergus et al., 2005). Bing Images An alternative image search engine is Bing Images"
D16-1043,D15-1172,0,0.0251272,"Missing"
D16-1043,P12-1015,0,0.0410624,"n representation as the sampled image representations. We use the same method for the ESP Game dataset. In all cases, images are resized and centercropped to ensure that they are the correct size input. 4 Evaluation Representation quality in semantics is usually evaluated using intrinsic datasets of human similarity and relatedness judgments. Model performance is assessed through the Spearman ρs rank correlation between the system’s similarity scores for a given pair of words, together with human judgments. Here, we evaluate on two well-known similarity and relatedness judgment datasets: MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015). MEN focuses explicitly on relatedness (i.e. coffee-tea and coffee-mug get high scores, while bakery-zebra gets a low score), while SimLex-999 focuses on what it calls “genuine” similarity (i.e., coffee-tea gets a high score, while both coffee-mug and bakery-zebra get low scores). They are standard evaluations for evaluating representational quality in semantics. In each experiment, we examine performance of the visual representations compared to text-based representations, as well as performance of the multimodal representation that fuses the two. In this A"
D16-1043,N16-1071,1,0.890774,"Missing"
D16-1043,J15-4004,0,0.0909181,"e representations. We use the same method for the ESP Game dataset. In all cases, images are resized and centercropped to ensure that they are the correct size input. 4 Evaluation Representation quality in semantics is usually evaluated using intrinsic datasets of human similarity and relatedness judgments. Model performance is assessed through the Spearman ρs rank correlation between the system’s similarity scores for a given pair of words, together with human judgments. Here, we evaluate on two well-known similarity and relatedness judgment datasets: MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015). MEN focuses explicitly on relatedness (i.e. coffee-tea and coffee-mug get high scores, while bakery-zebra gets a low score), while SimLex-999 focuses on what it calls “genuine” similarity (i.e., coffee-tea gets a high score, while both coffee-mug and bakery-zebra get low scores). They are standard evaluations for evaluating representational quality in semantics. In each experiment, we examine performance of the visual representations compared to text-based representations, as well as performance of the multimodal representation that fuses the two. In this Arch. AlexNet Agg. Mean GoogLeNet Ma"
D16-1043,D14-1005,1,0.941874,"in the data source. Introduction Multi-modal distributional semantics addresses the fact that text-based semantic models, which represent word meanings as a distribution over other words (Turney and Pantel, 2010; Clark, 2015), suffer from the grounding problem (Harnad, 1990). Recent work has shown that this theoretical motivation can be successfully exploited for practical gain. Indeed, multi-modal representation learning leads to improvements over language-only models in a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et Traditionally, representations for images were learned through bag-of-visual words (Sivic and Zisserman, 2003), using SIFT-based local feature descriptors (Lowe, 2004). Kiela and Bottou (2014) showed that transferring representations from deep convolutional neural networks (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics. ConvNets (LeCun et al., 1998) have become very popular in recent years: they are now the dominant approach for almost all recognition and detection tasks in the computer vision community (LeCun et al., 2015), approachin"
D16-1043,W14-1503,1,0.828012,"yers. These networks were selected because they are very well-known in the computer vision community. They exhibit interesting qualitative differences in terms of their depth (i.e., the number of layers), the number of parameters, regularization methods and the use of fully connected layers. They have all been winning network architectures in the ILSVRC ImageNet classification challenges. 3 Sources of Image Data Some systematic studies of parameters for textbased distributional methods have found that the source corpus has a large impact on representational quality (Bullinaria and Levy, 2007; Kiela and Clark, 2014). The same is likely to hold in the case of Google Bing Flickr ImageNet ESP Game Type Search engine Search engine Photo sharing Image database Game Annotation Automatic Automatic Human Human Human Coverage Unlimited Unlimited Unlimited Limited Limited Multi-lingual Yes Yes No No No Sorted Yes Yes Yes No No Tag specificity Unknown Unknown Loose Specific Loose Table 2: Sources of image data. visual representations. Various sources of image data have been used in multi-modal semantics, but there have not been many comparisons: Bergsma and Goebel (2011) compare Google and Flickr, and Kiela and Bot"
D16-1043,P15-2020,1,0.865352,"Missing"
D16-1043,D15-1015,1,0.832353,"Missing"
D16-1043,P16-4010,1,0.817635,"hese findings extend to different languages beyond English? We evaluate semantic representation quality through examining how well a system’s similarity scores correlate with human similarity and relatedness judgments. We examine both the visual representations themselves as well as the multi-modal representations that fuse visual representations with linguistic input, in this case using middle fusion (i.e., concatenation). To the best of our knowledge, this work is the first to systematically compare these aspects of visual representation learning. 2 Architectures We use the MMFeat toolkit1 (Kiela, 2016) to obtain image representations for three different convolutional network architectures: AlexNet (Krizhevsky 1 https://github.com/douwekiela/mmfeat 448 et al., 2012), GoogLeNet (Szegedy et al., 2015) and VGGNet (Simonyan and Zisserman, 2014). Image representations are turned into an overall word-level visual representation by either taking the mean or the elementwise maximum of the relevant image representations. All three networks are trained to maximize the multinomial logistic regression objective using mini-batch gradient descent with momentum: − D X K X exp(θ(k)&gt; x(i) ) 1{y (i) = k} log"
D16-1043,D13-1115,0,0.237121,"Missing"
D16-1043,N16-1020,1,0.845642,"and Architectures for Deep Visual Representation Learning in Semantics Douwe Kiela, Anita L. Ver˝o and Stephen Clark Computer Laboratory University of Cambridge douwe.kiela,alv34,stephen.clark@cl.cam.ac.uk Abstract al., 2015), improving lexical entailment (Kiela et al., 2015a), predicting compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Bergsma and Van Durme, 2011), selectional preference prediction (Bergsma and Goebel, 2011), linguistic ambiguity resolution (Berzak et al., 2015), visual information retrieval (Bulat et al., 2016) and metaphor identification (Shutova et al., 2016). Multi-modal distributional models learn grounded representations for improved performance in semantics. Deep visual representations, learned using convolutional neural networks, have been shown to achieve particularly high performance. In this study, we systematically compare deep visual representation learning techniques, experimenting with three well-known network architectures. In addition, we explore the various data sources that can be used for retrieving relevant images, showing that images from search engines perform as well as, or better than, those from manually crafted resources su"
D16-1043,P14-1068,0,0.062773,"ted with the target word(s) in the data source. Introduction Multi-modal distributional semantics addresses the fact that text-based semantic models, which represent word meanings as a distribution over other words (Turney and Pantel, 2010; Clark, 2015), suffer from the grounding problem (Harnad, 1990). Recent work has shown that this theoretical motivation can be successfully exploited for practical gain. Indeed, multi-modal representation learning leads to improvements over language-only models in a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et Traditionally, representations for images were learned through bag-of-visual words (Sivic and Zisserman, 2003), using SIFT-based local feature descriptors (Lowe, 2004). Kiela and Bottou (2014) showed that transferring representations from deep convolutional neural networks (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics. ConvNets (LeCun et al., 1998) have become very popular in recent years: they are now the dominant approach for almost all recognition and detection tasks in the computer vision community (LeCun e"
D16-1043,P16-2031,1,0.602892,"Missing"
D16-1043,N15-1016,0,\N,Missing
D17-1070,S14-2010,0,0.0512224,"ion x and the image y to the same embedding space. We use a margin α = 0.2 and 30 contrastive terms. We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test. For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014). This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with 3 https://www.github.com/ facebookresearch/SentEval 674 name SNLI task NLI N 560k SICK-E NLI 10k SICK-R STS 10k STS14 STS 4.5k premise ”Two women are embracing while holding to go packages.” A man is typing on a machine used for stenography ”A man is singing a song and playing the guitar” ”Liquid ammonia leak kills 15 in Shanghai” hypothesis ”Two woman are holding packages.” label entailment The man isn’t operating a"
D17-1070,D15-1075,0,0.924908,"vel understanding task that involves reasoning about the semantic relationships within sentences. Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks. Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes. Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings comMany modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently ou"
D17-1070,N13-1092,0,0.0224448,"Missing"
D17-1070,N16-1162,0,0.72628,"Missing"
D17-1070,marelli-etal-2014-sick,0,0.201032,"arch/InferSent 670 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014). These models obtained significantly lower results compared to the unsupervised Skip-Thought approach. Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015). The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017). To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders. As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data. pared to all existing alternat"
D17-1070,D13-1090,0,0.0314371,"8.7 88.2 68.4/76.8 72.7/80.9 75.1/82.3 76.2/83.1 0.849 0.863 0.885 0.884 83.1 83.1 86.3 86.3 .46/.42 .67/.70 .43/.42 .71/ .55/.54 .68/.65 .70/.67 92.4 - 80.4/85.9 - 0.868 84.5 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods. regard to the embedding size. 5.2 Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demons"
D17-1070,D16-1046,0,0.0556965,"ds in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014). These models obtained significantly lower results compared to the unsupervised Skip-Thought approach. Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015). The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017). To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders. As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data. pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while bei"
D17-1070,D14-1162,0,0.115818,"erence datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1 . 1 Holger Schwenk Facebook AI Research schwenk@fb.com Introduction Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision. While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence. That is, how to capture the 1 https://www.github.com/ facebookresearch/InferSent 670 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics machine tran"
D17-1070,S14-2055,0,0.0154055,"3.1 86.3 86.3 .46/.42 .67/.70 .43/.42 .71/ .55/.54 .68/.65 .70/.67 92.4 - 80.4/85.9 - 0.868 84.5 - - Table 4: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016). For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods. regard to the embedding size. 5.2 Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows. We hyp"
D17-1070,D16-1157,0,0.0885981,"information or semantics of the input data by specializing too much on these biases. Learning models on large unsupervised task makes it harder for the model to specialize. Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder. They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect. Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures. SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level. By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks. They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016). Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality."
D17-1070,P15-1150,0,\N,Missing
D17-1070,W14-4012,0,\N,Missing
D17-1070,L18-1269,1,\N,Missing
D17-1162,P16-2017,1,0.746585,"l associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014"
D17-1162,E17-2084,1,0.556468,"sentations for the metaphor identification task via supervised training; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity. We experimented with two types of word representations 1537 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1537–1546 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics as inputs to the network: the standard skip-gram word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof. We evaluate our method in the metaphor identification task, focusing on adjective–noun, verb– subject and verb–direct object constructions where the verbs and adjectives can be used metaphorically. Our results show that our architecture outperforms both a metaphor agnostic deep learning baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification. We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existin"
D17-1162,W16-1104,0,0.224064,"cepts from corpora. For example, the feature vector for politics would contain GAME or MECHA NISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way. Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context. Guti´errez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework. Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional ”single-sense” compositional distributional model. They then used these representations in the metaphor identification task, achieving promising result"
D17-1162,W15-0107,0,0.0198019,"misclassified examples. The diagram of the complete network can be seen in Figure 1. 4 Word Representations Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations. The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair. We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space. 5 Datasets We evaluate our method using two datasets of phrases manually annotated for metaphoricity. Literal bloody nose cold weather dry skin empty can frosty morning hot chocolate gold coin soft leather sour cherry steep hill Table 2: Annotated adjective–noun pairs from TSV- TEST . Since these datasets include examples for different senses (both metaphorical and literal) of"
D17-1162,W06-3506,0,0.539391,"ge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-"
D17-1162,W13-0907,0,0.669226,"concept). Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word em"
D17-1162,D14-1005,1,0.818881,"to the traditional ”single-sense” compositional distributional model. They then used these representations in the metaphor identification task, achieving promising results. The more recent approaches of Shutova et al. (2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features. Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features. They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively. They then measured the difference between the phrase representation and those of its component words in terms of their cosine similarity, which served as a predictor of metaphoricity. They found basic cosine similarity between the component words in the phrase to be a powerful measure – the neural embeddings of the words were compared with cosine similar1538 Figure 1: The network architecture for supervised metaphorical phrase classification. The symbol is used to indicate element-wise multiplication. ity and a threshold was tuned on the development set to distinguish between li"
D17-1162,S16-2003,1,0.553565,"Missing"
D17-1162,N16-1020,1,0.690494,"nd higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to impro"
D17-1162,N13-1118,1,0.956289,"al and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a s"
D17-1162,C10-1113,1,0.957575,"tures, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition. Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which sug"
D17-1162,shutova-teufel-2010-metaphor,1,0.874656,"ns between two distinct concepts or domains. For instance, when we talk about “curing juvenile delinquency” or “corruption transmitting through the government ranks”, we view the general concept of crime (the target concept) in terms of the properties of a disease (the source concept). Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-eng"
D17-1162,W13-0909,0,0.0517548,"s. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patt"
D17-1162,P14-1024,0,0.734219,"n important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data. We"
D17-1162,D11-1063,0,0.371488,"guage is a reflection of this process. Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010). A number of approaches to metaphor processing have thus been proposed, focusing predominantly on classifying linguistic expressions as literal or metaphorical. They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014). While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them. In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016). Their experiments have demonstrated that corpus-driven lexical representations already encode information about"
D17-1162,W13-0904,0,0.0694063,"nd event status ( PROCESS , STATE , OBJECT ). Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of sentence trees. Mohler et al. (2013) aimed at modelling conceptual information. They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures of the known ones. With the aim of reducing the dependence on manually-annotated lexical resources, other research focused on modelling metaphor using corpus-driven information alone. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large po"
D17-1162,W14-1608,1,0.811713,"using annotated metaphor examples, resulting in word representations that are more suitable for this task. Furthermore, the adjectives and nouns use separate mapping weights, which allows the model to better distinguish between the different functionalities of these words. In contrast, the original cosine similarity is not position-specific and would give the same result regardless of the word order. 3.3 Metaphorical absorb cost attack problem attack cancer breathe life design excuse deflate economy leak news swallow anger Table 1: Annotated verb-direct object and verbsubject pairs from MOH. Rei and Briscoe (2014) used a fixed formula to calculate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations. We extend this even further and allow the network to use multiple different weighting strategies which are all optimised during training. This is done by first creating a vector m, which is an element-wise multiplication of the two word representations: mi = z1,i z2,i d = γ(Wd m) If the vectors x1 and x2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multi"
D18-1176,N18-2031,0,0.157046,"ning many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alternative is to try to combine the strengths of different word embeddings. Recent work in socalled “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Bollegala et al., 2017; Murom¨agi et al., 2017; Coates and Bollegala, 2018). Metaembeddings are usually created in a separate preprocessing step, rather than in a process that is dynamically adapted to the task. In this work, we explore the supervised learning of task-specific, dynamic meta-embeddings, and apply the technique to sentence representations. The proposed approach turns out to be highly effective, leading to state-of-the-art performance within the same model class on a variety of tasks, opening up new areas for exploration and yielding insights into the usage of word embeddings. Why Is This a Good Idea? Our technique brings several important benefits to N"
D18-1176,D17-1070,1,0.911117,"word embeddings. • Interpretability and Linguistic Analysis Different word embeddings work well on different tasks. This is well-known in the field, but knowing why this happens is less wellunderstood. Our method sheds light on which embeddings are preferred in which linguistic contexts, for different tasks, and allows us to speculate as to why that is the case. Outline In what follows, we explore dynamic meta-embeddings and show that this method outperforms the naive concatenation of various word embeddings, while being more efficient. We apply the technique in a BiLSTM-max sentence encoder (Conneau et al., 2017) and evaluate it on wellknown tasks in the field: natural language inference (SNLI and MultiNLI; §4), sentiment analysis (SST; §5), and image-caption retrieval (Flickr30k; §6). In each case we show state-of-the-art performance within the class of single sentence encoder models. Furthermore, we include an extensive analysis (§7) to highlight the general usefulness of our technique and to illustrate how it can lead to new insights. 2 Related Work Thanks to their widespread popularity in NLP, a sprawling literature has emerged about learning and applying word embeddings—much too large to fully co"
D18-1176,W16-2524,0,0.0134293,"explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016), where the attention mechanism is applied within the same sentence instead of for aligning multiple sentences. Here, we learn (optionally contextualized) attention weights for different embedding sets and apply the technique in sentence representations (Kiros et al."
D18-1176,W16-2506,0,0.0196495,"(1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016), where the attention mechanism is applied within the same sentence instead of for aligning mul"
D18-1176,D14-1005,1,0.923595,"here has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has bee"
D18-1176,D15-1242,1,0.879536,"Missing"
D18-1176,P18-1085,0,0.0569833,"ngs is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work wit"
D18-1176,W17-7508,0,0.0217995,"et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnati"
D18-1176,J81-4005,0,0.677773,"Missing"
D18-1176,P16-1160,1,0.812507,"they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alternative is to try to combine the strengths of different word embeddings. Recent work in socalled “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Boll"
D18-1176,P14-2050,0,0.597324,"distributional semantic models (Turney and Pantel, 2010; Erk, 2012; Clark, 2015) to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of"
D18-1176,N16-1162,1,0.898778,"Missing"
D18-1176,P15-1010,0,0.0313794,"eprocessing step, rather than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Her"
D18-1176,Q15-1016,0,0.0455047,"word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016),"
D18-1176,N16-1018,0,0.0568473,"Missing"
D18-1176,E17-1038,0,0.0531809,"Missing"
D18-1176,P11-1015,0,0.277151,"eston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alterna"
D18-1176,K16-1006,0,0.0287886,"supervised embeddings with supervised ones for sentiment classification. Yang et al. (2016) and Miyamoto and Cho (2016) learn to combine word-level and character-level embeddings. Contextual representations have been used in neural machine translation as well, e.g. for learning contextual word vectors and applying them in other tasks (McCann et al., 2017) or for learning context-dependent representations to solve disambiguation problems in machine translation Choi et al. (2016). Neural tensor skip-gram models learn to combine word, topic and context embeddings (Liu et al., 2015); context2vec (Melamud et al., 2016) learns a more sophisticated context representation separately from target embeddings; and Li et al. (2016) learn word representations with distributed word representation with multi-contextual mixed embedding. Recent work in “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Bollegala et al., 2017; Murom¨agi et al., 2017; Coates and Bollegala, 2018)—here, we show that the idea can be applied in context, and to sentence representations. Furthermore, these works obtain metaembeddings as a preprocessing step, rather than learning them dynamicall"
D18-1176,L18-1550,0,0.0314326,"ic representations in that setting: SNLI (Bowman et al., 2015) and the more recent MultiNLI (Williams et al., 2017). The SNLI dataset consists of 570k humangenerated English sentence pairs, manually labeled for entailment, contradiction and neutral. The MultiNLI dataset can be seen as an extension of SNLI: it contains 433k sentence pairs, taken from ten different genres (e.g. fiction, government text or spoken telephone conversations), with the same entailment labeling scheme. We train sentence encoders with dynamic metaembeddings using two well-known and often-used embedding types: FastText (Mikolov et al., 2018; Bojanowski et al., 2016) and GloVe (Pennington et al., 2014). Specifically, we make use of the 300-dimensional embeddings trained on a similar WebCrawl corpus, and compare three scenarios: when used individually, when naively concatenated or in the dynamic meta-embedding setting (unweighted, context-independent DME and contextualized CDME). We also compare our approach against other models in the same class—in this case, models that encode sentences individually and do not allow attention across the two sentences.1 We include InferSent (Conneau et al., 2017), which also makes use of a BiLSTM"
D18-1176,D14-1079,0,0.0318848,"Erk, 2012; Clark, 2015) to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard"
D18-1176,W17-0212,0,0.1399,"Missing"
D18-1176,D14-1113,0,0.0551116,"Missing"
D18-1176,W17-5308,0,0.0499894,"Missing"
D18-1176,D14-1162,0,0.0954351,"ta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to stateof-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems. 1 Introduction It is no exaggeration to say that word embeddings have revolutionized NLP. From early distributional semantic models (Turney and Pantel, 2010; Erk, 2012; Clark, 2015) to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 201"
D18-1176,N18-1202,0,0.0764596,"arately from target embeddings; and Li et al. (2016) learn word representations with distributed word representation with multi-contextual mixed embedding. Recent work in “meta-embeddings”, which ensembles embedding sets, has been gaining traction (Yin and Sch¨utze, 2015; Bollegala et al., 2017; Murom¨agi et al., 2017; Coates and Bollegala, 2018)—here, we show that the idea can be applied in context, and to sentence representations. Furthermore, these works obtain metaembeddings as a preprocessing step, rather than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et"
D18-1176,P17-1170,0,0.0200153,"t al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamically learn the weights to combine representations. Recently, related dynamic multi-modal fusion met"
D18-1176,D16-1018,0,0.0187376,"than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for instance, word-level embeddings from different modalities are often mixed via concatenation r = [αu, (1 − α)v] (Bruni et al., 2014). Here, we 1467 dynamica"
D18-1176,D15-1036,0,0.0879763,"to deep learningbased word embeddings (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2016), word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstre"
D18-1176,D13-1170,0,0.0085803,"ce. Finally, we obtain results for using the six different embedding types (marked *), and show that adding in more embeddings increases performance further. To our knowledge, these numbers constitute the state of the art within the model class of single sentence encoders on these tasks. 5 1 Sentiment To showcase the general applicability of the proposed approach, we also apply it to a case where we have to classify a single sentence, namely, sentiment classification. Sentiment analysis and opinion mining have become important applications for NLP research. We evaluate on the binary SST task (Socher et al., 2013), consisting of 70k sentences with a corresponding binary (positive or negative) sentiment label. 5.1 Implementation Details We use 256-dimensional embedding projections, 512-dimensional BiLSTM encoders and an MLP with 512-dimensional hidden layer in the classifier. The initial learning rate is set to 0.0004 and dropped by a factor of 0.2 when dev accuracy stops improving, dropout to 0.5, and we use Adam for optimization. The loss is standard crossentropy. We calculate the mean accuracy and standard deviation based on ten random seeds. 5.2 Results Table 2 shows a similar pattern as we observed"
D18-1176,P15-1150,0,0.103114,"Missing"
D18-1176,D15-1243,0,0.0223349,"amic multi-modal fusion methods have also been explored (Wang et al., 2018; Kiros et al., 2018). There has also been work on unifying multi-view embeddings from different data sources (Luo et al., 2014). The usefulness of different embeddings as initialization has been explored (Kocmi and Bojar, 2017), and different architectures and hyperparameters have been extensively examined (Levy et al., 2015). Problems with evaluating word embeddings intrinsically are well known (Faruqui et al., 2016), and various alternatives for evaluating word embeddings in downstream tasks have been proposed (e.g., Tsvetkov et al., 2015; Schnabel et al., 2015; Ettinger et al., 2016). For more related work with regard to word embeddings and their evaluation, see Bakarov (2017). Our work can be seen as an instance of the wellknown attention mechanism (Bahdanau et al., 2014), and its recent sentence-level incarnations of self-attention (Lin et al., 2017) and inner-attention (Cheng et al., 2016; Liu et al., 2016), where the attention mechanism is applied within the same sentence instead of for aligning multiple sentences. Here, we learn (optionally contextualized) attention weights for different embedding sets and apply the tech"
D18-1176,N16-1151,0,0.0220506,"ow that the idea can be applied in context, and to sentence representations. Furthermore, these works obtain metaembeddings as a preprocessing step, rather than learning them dynamically in a supervised setting, as we do here. Similarly to Peters et al. (2018), who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights. There has also been work on learning multiple embeddings per word (Chen et al., 2014; Neelakantan et al., 2015; Vu and Parker, 2016), including a lot of work in sense embeddings where the senses of a word have their own individual embeddings (Iacobacci et al., 2015; Qiu et al., 2016), as well as on how to apply such sense embeddings in downstream NLP tasks (Pilehvar et al., 2017). The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined (Kiela and Bottou, 2014; Lazaridou et al., 2015), see Baltruˇsaitis et al. (2018) for an overview. In multi-modal semantics, for ins"
D18-1176,J17-4004,1,0.875438,"Missing"
D18-1176,D16-1157,0,0.0189657,"e variety of core NLP tasks, to the extent that they are now ubiquitous in the field (Goldberg, 2016). A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn (Levy and Goldberg, 2014b), evaluating their performance (Milajevs et al., 2014; Schnabel et al., 2015; Bakarov, 2017), specializing them for certain tasks (Maas et al., 2011; Faruqui et al., 2014; Kiela et al., 2015; Mrkˇsi´c et al., 2016; Vuli´c and Mrkˇsi´c, 2017), learning sub-word level representations (Wieting et al., 2016; Bojanowski et al., 2016; Lee et al., 2016), et cetera. One of the first steps in designing many NLP systems is selecting what kinds of word embeddings to use, with people often resorting to freely available pre-trained embeddings. While this is often a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly correlated with word-level benchmarks. An alternative is to try to combine the strengths of different word embeddings. Recent work in socalled “meta-embeddings”, which ensembles embedding sets, has been ga"
D18-1176,D17-1056,0,0.084221,"various genres. This allows us to inspect the applicability of source domain data for a specific genre. We train embeddings on three kinds of data: Wikipedia, the Toronto Books Corpus (Zhu et al., 2015) and the English OpenSubtitles4 . We examine the atten1472 4 http://opus.nlpl.eu/OpenSubtitles.php Figure 5: Multi-domain weights on MultiNLI. Model Levy LEAR SNLI CDME 0.33 0.67 85.3±.9 Model GloVe Refined SST CDME 0.59 0.41 89.0±.4 Table 4: Accuracy and learned weights on SNLI using LEAR (Vuli´c and Mrkˇsi´c, 2017) or SST using sentiment-refined embeddings using the specialization method from Yu et al. (2017). tion weights on the five genres in the in-domain (matched) set, consisting of fiction; transcriptions of spoken telephone conversations; government reports, speeches, letters and press releases; popular culture articles from the Slate Magazine archive; and travel guides. Figure 5 shows the average attention weights for the three embedding types over the five genres. We observe that Toronto Books, which consists of fiction, is very appropriate for the fiction genre, while Wikipedia is highly preferred for the travel genre, perhaps because it contains a lot of factual information about geograp"
D19-1062,W05-0614,0,0.0771019,"orld (Cˆot´e et al., 2018), but these do not have human dialogue within the game. Similar single player text adventure games have also been used to study referring expressions (Gabsdil et al., 2001, 2002) and parsing (Koller et al., 2004). Yang et al. (2017) and Bordes et al. (2010) also proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nights game (Fleischman and Roy, 2005), studies of emotional response in adventure games (Fraser et al., 2018), dialogue about soccer videogames (Pasunuru and Bansal, 2018), placing blocks appropriately given a final plan (Wang et al., 2016) and a more open ended building task using a grid of voxels (Wang et al., 2017). In the latter two cases the communication is one-sided with only the human issuing instructions, rather than dialogue, with the agent only able to act. LIGHT is a large-scale, configurable text adventure environment for research on learning grounded language and actions. It features both humans and models as agents"
D19-1062,D18-1298,1,0.880452,"Missing"
D19-1062,C02-1113,0,0.259734,"Missing"
D19-1062,D17-2014,1,0.823239,"ly tedious and easy to fake. In order to mitigate these problems, during the evaluation we provide annotated examples on the training in addition to examples on the test set. We only keep the annotations of evaluators who had high accuracy on the training examples to filter low-accuracy evaluators. The training accuracy bar was selected due to the difficulty of the separate tasks as evaluated by our own success rates. Our methods for human evaluation are described in more detail in Appendix F along with how many turns were evaluated. Implementation We implement models using PyTorch in ParlAI (Miller et al., 2017). Ranking Transformer models are pretrained on Reddit data (Mazar´e et al., 2018) and fine-tuned. We use the BERT (Devlin et al., 2018) implementation provided by Hugging Face2 with pre-trained weights, then adapted to our Bi-Ranker and Cross-Ranker setups. Generative models are pretrained on the Toronto Books Corpus and fine-tuned except for emote prediction which does not leverage pretraining. We apply byte-pair encoding (Sennrich et al., 2016) to reduce the vocabulary size for generative models. We decode using beam search with beam size 5. 2 Evaluation 5 Results The ranking models are comp"
D19-1062,W14-4337,0,0.0232878,"hout explicit understanding of the world that the language describes. This work is built on the hypothesis that dialogue agents embodied in a rich and cohesive (but tractable) world can more easily be trained to use language effectively than those only exposed to standard large-scale text-only corpora. To that end, we introduce the LIGHT1 research platform. LIGHT is a multi-player fantasy text adventure world designed for studying situated dialogue, and allows interactions between humans, 1 2 Related Work Most recent work in dialogue exploring generative or retrieval models for goal-directed (Henderson et al., 2014; Bordes et al., 2017) or chitchat tasks (Vinyals and Le, 2015; Sordoni et al., 2015; Zhang et al., 2018) is not situated, or even Learning in Interactive Games with Humans and Text. 673 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 673–683, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Mostafazadeh et al., 2017). While grounded, the agent has no ability to act in these tasks. Talk the Walk (de Vries et al., 2018) introduces a naviga"
D19-1062,I17-1047,0,0.0243704,"ions between humans, 1 2 Related Work Most recent work in dialogue exploring generative or retrieval models for goal-directed (Henderson et al., 2014; Bordes et al., 2017) or chitchat tasks (Vinyals and Le, 2015; Sordoni et al., 2015; Zhang et al., 2018) is not situated, or even Learning in Interactive Games with Humans and Text. 673 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 673–683, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Mostafazadeh et al., 2017). While grounded, the agent has no ability to act in these tasks. Talk the Walk (de Vries et al., 2018) introduces a navigation game that involves action, perception and two-way dialogue, but is limited to small grids. In summary, compared to many setups, our framework allows learning from both actions and (two-way) dialogue, while many existing simulations typically address one or the other but not both. In addition, being based on a gaming setup, our hope is that LIGHT can be fun for humans to interact with, enabling future engagement with our models. All utterances in LIGHT are produced by"
D19-1062,D15-1001,0,0.0815297,"from the dialogue history as input, and output a new utterance. While some goal-directed setups may use external knowledge bases (e.g. flight data for airline booking), dialogues tend to implicitly refer to an external world during the conversations without explicit grounding to objects or actions. Several position papers have proposed virtual embodiment as a strategy for language research (Brooks, 1991; Kiela et al., 2016; Gauthier and Mordatch, 2016; Mikolov et al., 2016; Lake et al., 2017). Single-player text adventure game frameworks for training reinforcement learning agents exist, i.e., Narasimhan et al. (2015) and TextWorld (Cˆot´e et al., 2018), but these do not have human dialogue within the game. Similar single player text adventure games have also been used to study referring expressions (Gabsdil et al., 2001, 2002) and parsing (Koller et al., 2004). Yang et al. (2017) and Bordes et al. (2010) also proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nig"
D19-1062,D14-1162,0,0.085006,"Missing"
D19-1062,P16-1162,0,0.0102372,"uation are described in more detail in Appendix F along with how many turns were evaluated. Implementation We implement models using PyTorch in ParlAI (Miller et al., 2017). Ranking Transformer models are pretrained on Reddit data (Mazar´e et al., 2018) and fine-tuned. We use the BERT (Devlin et al., 2018) implementation provided by Hugging Face2 with pre-trained weights, then adapted to our Bi-Ranker and Cross-Ranker setups. Generative models are pretrained on the Toronto Books Corpus and fine-tuned except for emote prediction which does not leverage pretraining. We apply byte-pair encoding (Sennrich et al., 2016) to reduce the vocabulary size for generative models. We decode using beam search with beam size 5. 2 Evaluation 5 Results The ranking models are compared in Table 4 on the seen and unseen test sets, and ablations are shown for both the BERT-based Bi-Ranker and https://github.com/huggingface/pytorch-pretrained-BERT 678 Dialogue R@1/20 Test Seen Action Acc Emote Acc Dialogue R@1/20 5.0 23.7 53.8 70.9 76.5 74.9 12.2 20.6 17.8 24.5 42.5 50.7 4.5 7.5 13.2 11.6 17.3 25.0 25.8 5.0 21.8 27.9 66.0 70.5 69.7 12.1 20.5 16.4 21.1 38.6 51.8 4.5 8.46 9.92 9.8 16.6 25.7 28.6 *87.5±2.4 *62.0±3.1 *27.0±2.5 *9"
D19-1062,N15-1020,0,0.0822381,"Missing"
D19-1062,P18-1205,1,0.927894,"hat dialogue agents embodied in a rich and cohesive (but tractable) world can more easily be trained to use language effectively than those only exposed to standard large-scale text-only corpora. To that end, we introduce the LIGHT1 research platform. LIGHT is a multi-player fantasy text adventure world designed for studying situated dialogue, and allows interactions between humans, 1 2 Related Work Most recent work in dialogue exploring generative or retrieval models for goal-directed (Henderson et al., 2014; Bordes et al., 2017) or chitchat tasks (Vinyals and Le, 2015; Sordoni et al., 2015; Zhang et al., 2018) is not situated, or even Learning in Interactive Games with Humans and Text. 673 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 673–683, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Mostafazadeh et al., 2017). While grounded, the agent has no ability to act in these tasks. Talk the Walk (de Vries et al., 2018) introduces a navigation game that involves action, perception and two-way dialogue, but is limited to small grids. In summar"
D19-1062,P17-1086,0,0.0172141,"proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nights game (Fleischman and Roy, 2005), studies of emotional response in adventure games (Fraser et al., 2018), dialogue about soccer videogames (Pasunuru and Bansal, 2018), placing blocks appropriately given a final plan (Wang et al., 2016) and a more open ended building task using a grid of voxels (Wang et al., 2017). In the latter two cases the communication is one-sided with only the human issuing instructions, rather than dialogue, with the agent only able to act. LIGHT is a large-scale, configurable text adventure environment for research on learning grounded language and actions. It features both humans and models as agents situated (symbolically) within a multi-player fantasy MUD (multiuser dungeon)-like (Dieterle, 2009) environment. The environment is moderated by a simple game engine which passes dialogue and emote turns between characters and allows actions to cause transitions of the world state"
D19-1062,P16-1224,0,0.042343,"arsing (Koller et al., 2004). Yang et al. (2017) and Bordes et al. (2010) also proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nights game (Fleischman and Roy, 2005), studies of emotional response in adventure games (Fraser et al., 2018), dialogue about soccer videogames (Pasunuru and Bansal, 2018), placing blocks appropriately given a final plan (Wang et al., 2016) and a more open ended building task using a grid of voxels (Wang et al., 2017). In the latter two cases the communication is one-sided with only the human issuing instructions, rather than dialogue, with the agent only able to act. LIGHT is a large-scale, configurable text adventure environment for research on learning grounded language and actions. It features both humans and models as agents situated (symbolically) within a multi-player fantasy MUD (multiuser dungeon)-like (Dieterle, 2009) environment. The environment is moderated by a simple game engine which passes dialogue and emote turn"
D19-1062,N19-1423,0,\N,Missing
D19-1244,N19-1264,0,0.0485276,"Missing"
D19-1244,P17-1020,0,0.0218616,"scussed in §4.1 on (dis)similarities between patterns learned by humans and neural networks. Evidence Extraction Various papers have explored the related problem of extracting evidence or summaries to aid downstream QA. Wang et al. (2018a) concurrently introduced a neural model that extracts evidence specifically for the correct answer, as an intermediate step in a QA pipeline. Prior work uses similar methods to explain what a specific model has learned (Lei et al., 2016; Li et al., 2016; Yu et al., 2019). Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b). More broadly, extracting evidence can facilitate fact verification (Thorne et al., 2018) and debate.2 2 IBM Project Debater: www.research.ibm.com/ artificial-intelligence/project-debater 2409 Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models (Arumae and Liu, 2018, 2019; Eyal et al., 2019). Debate Evidence extraction can be viewed as a form of debate, in which multiple agents support di"
D19-1244,N19-1423,0,0.0404619,"rity between two using Adam (Kingma and Ba, 2015) on one loss texts (Perone et al., 2018). Using this funcfrom Table 1. For t &gt; 1, we find it effective to tion, we define a model class that selects the ansimply predict the judge model at t = 1 and use swer most similar to the input passage context: this distribution for all time steps during inference. L(i) = fastText(S, A(i)). This trick speeds up training by enabling us to precompute prediction targets using the judge model, BERT L(i) is computed using the multipleinstead of querying it constantly during training. choice adaptation of BERT (Devlin et al., 2019; Radford et al., 2018; Si, 2019), a pre-trained We use BERTBASE for all learned agents. transformer network (Vaswani et al., 2017). We Learned agents predict the BERTBASE judge, as fine-tune all BERT parameters during trainit is more efficient to compute than BERTLARGE . ing. This model predicts L(i) using a trainEach agent AGENT(i) is assigned the answer A(i) able vector v and BERT’s first token embedding: that it should support. We train one learned agent L(i) = v &gt; · BERT([S; Q; A(i)]). to find evidence for an arbitrary answer i. We We experiment with both the BERTBASE model condition AGEN"
D19-1244,N19-1395,0,0.043278,"Missing"
D19-1244,W18-2501,0,0.0315432,"Missing"
D19-1244,D18-1316,0,0.0260321,"Missing"
D19-1244,N18-2017,0,0.0434092,"Missing"
D19-1244,P18-3015,0,0.0467999,"Missing"
D19-1244,D17-1215,0,0.052598,"Missing"
D19-1244,E17-2068,0,0.093309,"Missing"
D19-1244,D16-1011,0,0.0337657,"duction There is great value in understanding the fundamental nature of a question (Chalmers, 2015). Distilling the core of an issue, however, is timeconsuming. Finding the correct answer to a given question may require reading large volumes of text or understanding complex arguments. Here, we examine if we can automatically discover the underlying properties of problems such as question answering by examining how machine learning models learn to solve that task. We examine this question in the context of passage-based question-answering (QA). Inspired by work in interpreting neural networks (Lei et al., 2016), we have agents find a subset of the passage (i.e., supporting evidence) that maximizes a QA model’s probability of a particular answer. Each agent (one agent per answer) finds the sentences that a QA model regards as strong evidence for its answer, using either exhaustive search or learned prediction. Figure 1 shows an example. Figure 1: Evidence agents quote sentences from the passage to convince a question-answering judge model of an answer. To examine to what extent evidence is general and independent of the model, we evaluate if humans and other models find selected evidence to be valid"
D19-1244,D17-2014,1,0.848518,"acy. However, each judge model’s accuracy is useful to know for analysis purposes. Table 2 shows model accuracies, which cover a broad range. BERT models significantly outperform word-based baselines (TFIDF and fastText), and BERTLARGE achieves the best overall accuracy. No model achieves the estimated human ceiling for either RACE (Lai et al., 2017) or DREAM (Sun et al., 2019). Our code is available at https://github. com/ethanjperez/convince. We build off AllenNLP (Gardner et al., 2018) using PyTorch (Paszke et al., 2017). For all human evaluations, we use Amazon Mechanical Turk via ParlAI (Miller et al., 2017). Appendix B describes preprocessing and training details. 4 4.1 Agents Select General Evidence Human Evaluation of Evidence Would evidence that convinces a model also be valid evidence to humans? On one hand, there is ample work suggesting that neural networks can learn similar patterns as humans do. Convolutional networks trained on ImageNet share similarities with the human visual cortex (Cadieu et al., 2014). In machine translation, attention learns to align foreign words with their native counterparts (Bahdanau et al., 2015). On the other hand, neural networks often do not behave as human"
D19-1244,P18-1079,0,0.0321551,"Missing"
D19-1244,Q19-1014,0,0.057546,"convince the judge model when supporting the correct answer (one answer per question). 3.2 Training and Evaluating Models Our setup is not directly comparable to standard QA setups, as we aim to evaluate evidence rather than raw QA accuracy. However, each judge model’s accuracy is useful to know for analysis purposes. Table 2 shows model accuracies, which cover a broad range. BERT models significantly outperform word-based baselines (TFIDF and fastText), and BERTLARGE achieves the best overall accuracy. No model achieves the estimated human ceiling for either RACE (Lai et al., 2017) or DREAM (Sun et al., 2019). Our code is available at https://github. com/ethanjperez/convince. We build off AllenNLP (Gardner et al., 2018) using PyTorch (Paszke et al., 2017). For all human evaluations, we use Amazon Mechanical Turk via ParlAI (Miller et al., 2017). Appendix B describes preprocessing and training details. 4 4.1 Agents Select General Evidence Human Evaluation of Evidence Would evidence that convinces a model also be valid evidence to humans? On one hand, there is ample work suggesting that neural networks can learn similar patterns as humans do. Convolutional networks trained on ImageNet share similari"
D19-1244,N18-1074,0,0.0201572,"the related problem of extracting evidence or summaries to aid downstream QA. Wang et al. (2018a) concurrently introduced a neural model that extracts evidence specifically for the correct answer, as an intermediate step in a QA pipeline. Prior work uses similar methods to explain what a specific model has learned (Lei et al., 2016; Li et al., 2016; Yu et al., 2019). Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b). More broadly, extracting evidence can facilitate fact verification (Thorne et al., 2018) and debate.2 2 IBM Project Debater: www.research.ibm.com/ artificial-intelligence/project-debater 2409 Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models (Arumae and Liu, 2018, 2019; Eyal et al., 2019). Debate Evidence extraction can be viewed as a form of debate, in which multiple agents support different stances (Irving et al., 2018; Irving and Askell, 2019). Chen et al. (2018) show that evidence-based debate improves the accuracy of crow"
D19-1244,D16-1264,0,0.115473,"Missing"
D19-1384,D17-1259,0,0.0502329,"rouchy et al., 2016), often used in the framework of Artificial Life (Bedau, 2003), or a gradient-based optimization algorithm, as is often used for training deep neural networks with a supervised or reinforcement learning objective function. The former simulates generations developing complex behavior over time, while the latter enables more sophisticated agents thanks to the recent advances in deep learning (LeCun et al., 2015). Recent years have seen intriguing new results in emergent communication, starting with Lazaridou et al. (2016) and Foerster et al. (2016), using deep neural agents (Lewis et al., 2017; Havrylov and Titov, 2017; Jorge et al., 2016; Evtimova et al., 2018; Das et al., 2018; Cao et al., 2018). Often, these approaches could be framed as special or generalized cases of Lewis’s signalling game (Lewis, 2008), in which agents exchange signals to achieve a common goal. In this work, deep neural agents play games within communities of similar agents, where the aim is for agents to communicate about their perceptual input. 3 Multi-agent communication In order to study emergent linguistic phenomena in a simplified but realistic setting, the communication game needs to have several prop"
D19-1384,D14-1162,0,\N,Missing
D19-1447,P17-1022,0,0.0362282,"ion, game-playing or dialogue—which either don’t have clearly defined metrics or easily available natural language data— this pivot-based translation allows us to check exactly whether the communicated sequence corresponds to the intended meaning, as well as to the gold standard sequence. In addition, every single utterance has very clear and well-known metrics such as BLEU and log-likelihood, allowing us to measure performance at every single step. Our work is inspired by recent work in protocols or languages that emerge from multi-agent interaction (Lazaridou et al., 2017; Lee et al., 2018; Andreas et al., 2017; Evtimova et al., 2018; Kottur et al., 2017; Havrylov and Titov, 2017; Mordatch and Abbeel, 2017). Work on the emergence of language in multi-agent settings goes back a long way (Steels, 1997; Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language"
D19-1447,I17-1014,0,0.0378313,"Missing"
D19-1447,D16-1026,1,0.908898,"Missing"
D19-1447,2012.eamt-1.60,0,0.0637196,"Missing"
D19-1447,D17-1303,0,0.0182176,"u et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017; Chrupała et al., 2015; Kiela et al., 2017; K´ad´ar et al., 2018). In what follows, we show that language drift happens, and quite dramatically so, when fine-tuning using policy gradients. Next, we investigate imposing syntactic conformity (i.e., “Englishness”) via language model constraints, and show that this does somewhat mitigate drift, but does not lead to semantic correspondence. We then show that additionally imposing semantic constraints via (visual) grounding leads to the best retention of original syntax and intended semantics, and minimizes drift while im"
D19-1447,D17-1210,1,0.905964,"Missing"
D19-1447,P16-1227,0,0.029764,"ed to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017; Chrupała et al., 2015; Kiela et al., 2017; K´ad´ar et al., 2018). In what follows, we show that language drift happens, and quite dramatically so, when fine-tuning using policy gradients. Next, we investigate imposing syntactic conformity (i.e., “Englishness”) via language model constraints, and show that this does somewhat mitigate drift, but does not lead"
D19-1447,K18-1039,0,0.049502,"Missing"
D19-1447,N18-1038,1,0.897595,"Missing"
D19-1447,P07-2045,0,0.00539243,"French, German and Czech (of which we only use the first three). To ensure our findings are robust, we compare four different language models, trained on WikiText103, MS COCO, Flickr30k and all of the above. The grounding model is trained on Flickr30k (Young et al., 2014). Following Faghri et al. (2018), we randomly crop training images for data augmentation. We use 2048-dimensional features from a pretrained and fixed ResNet-152 (He et al., 2016). Preprocessing The same tokenization and vocabulary are used across different tasks and datasets. We lowercase and tokenize our corpora with Moses (Koehn et al., 2007) and use subword tokenization with Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10k merge operations. This allows us to use the same vocabulary across different models seamlessly (translation, language model, image-caption ranker model). Controling the English message length When fine-tuning the agents, we observe that the length of English messages becomes excessively long. As Agent A has no explicit incentive to output the end-of-sentence (EOS) symbol, it tends to keep transmitting the same token repeatedly. While redundancy might be beneficial for communication, excessively long me"
D19-1447,D17-1321,0,0.0483672,"n’t have clearly defined metrics or easily available natural language data— this pivot-based translation allows us to check exactly whether the communicated sequence corresponds to the intended meaning, as well as to the gold standard sequence. In addition, every single utterance has very clear and well-known metrics such as BLEU and log-likelihood, allowing us to measure performance at every single step. Our work is inspired by recent work in protocols or languages that emerge from multi-agent interaction (Lazaridou et al., 2017; Lee et al., 2018; Andreas et al., 2017; Evtimova et al., 2018; Kottur et al., 2017; Havrylov and Titov, 2017; Mordatch and Abbeel, 2017). Work on the emergence of language in multi-agent settings goes back a long way (Steels, 1997; Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (R"
D19-1447,D17-1259,0,0.0533192,"s been a renewed interest in multi-agent communication (Foerster et al., 2016; Lazaridou et al., 2016). While agents can be very effective in solving the tasks that they were trained on, their multi-agent communication protocols bear little resemblance to human languages. A major open question revolves around training multi-agent systems such that their communication protocols can be interpreted by humans. One option is to pre-train in a supervised fashion with human language, but even then it is found that the protocols diverge quickly when the agents are fine-tuned on an external reward, as Lewis et al. (2017) showed on a negotiation task. Indeed, language drift is to be expected if we are optimizing for an external non-linguistic reward, such as a reward based on whether or not two agents successfully accomplish a negotiation. Language drift might be avoided by imposing a “naturalness” constraint, e.g. by factoring language model likelihood into the reward function. However, such a constraint only acts on the syntax of the generated language, ignoring its semantics. See Table 1 for an illustration of different constraints. As has been advocated by multi-modal semantics (Baroni, 2016; Kiela, 2017),"
D19-1447,D17-1230,0,0.0380497,"Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017;"
D19-1447,D17-1061,1,0.824177,"e emergence of language in multi-agent settings goes back a long way (Steels, 1997; Nowak and Krakauer, 1999; Kirby, 2001; Briscoe, 2002; Skyrms, 2010). In our case, we are specifically interested in tabula inscripta agents that are already pre-trained to generate natural language, and we are primarily concerned with keeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural"
D19-1447,P16-1162,0,0.015582,"indings are robust, we compare four different language models, trained on WikiText103, MS COCO, Flickr30k and all of the above. The grounding model is trained on Flickr30k (Young et al., 2014). Following Faghri et al. (2018), we randomly crop training images for data augmentation. We use 2048-dimensional features from a pretrained and fixed ResNet-152 (He et al., 2016). Preprocessing The same tokenization and vocabulary are used across different tasks and datasets. We lowercase and tokenize our corpora with Moses (Koehn et al., 2007) and use subword tokenization with Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10k merge operations. This allows us to use the same vocabulary across different models seamlessly (translation, language model, image-caption ranker model). Controling the English message length When fine-tuning the agents, we observe that the length of English messages becomes excessively long. As Agent A has no explicit incentive to output the end-of-sentence (EOS) symbol, it tends to keep transmitting the same token repeatedly. While redundancy might be beneficial for communication, excessively long messages obscure evaluation of the communication protocol. For instance, BLEU score q"
D19-1447,I17-1039,0,0.0188084,"eeping their generated language as natural as possible during further training. Reinforcement Learning has been applied to finetuning models for various natural language generation tasks, including summarization (Ranzato et al., 2015; Paulus et al., 2017), information retrieval (Nogueira and Cho, 2017), MT (Gu et al., 2017; Bahdanau et al., 2016) and dialogue (Li et al., 2017). Our work can be viewed as fine-tuning MT systems using an intermediary pivot language. In MT, there is a long line of work of pivot-based approaches, most notably Muraki (1986) and more recently with neural approaches (Wang et al., 2017; Cheng et al., 2017; Chen et al., 2018). There has also been work on using visual pivots directly (Hitschler et al., 2016; Nakayama and Nishida, 2017; Lee et al., 2018). Grounded language learning in general has been shown to give significant practical improvements in various natural language understanding tasks (Gella et al., 2017; Elliott and K´ad´ar, 2017; Chrupała et al., 2015; Kiela et al., 2017; K´ad´ar et al., 2018). In what follows, we show that language drift happens, and quite dramatically so, when fine-tuning using policy gradients. Next, we investigate imposing syntactic conformit"
D19-1447,Q14-1006,0,0.0421369,"sets Agents A and B are initially pre-trained on IWSLT Fr→En and En→De, respectively (Cettolo et al., 2012). Fine-tuning is performed on Multi30k Task 1 (Elliott et al., 2016). That is, importantly, there is no overlap in the pre-training data and the fine-tuning data. Multi30k Task 1 consists of 30k images and one caption per image in English, French, German and Czech (of which we only use the first three). To ensure our findings are robust, we compare four different language models, trained on WikiText103, MS COCO, Flickr30k and all of the above. The grounding model is trained on Flickr30k (Young et al., 2014). Following Faghri et al. (2018), we randomly crop training images for data augmentation. We use 2048-dimensional features from a pretrained and fixed ResNet-152 (He et al., 2016). Preprocessing The same tokenization and vocabulary are used across different tasks and datasets. We lowercase and tokenize our corpora with Moses (Koehn et al., 2007) and use subword tokenization with Byte Pair Encoding (BPE) (Sennrich et al., 2016) with 10k merge operations. This allows us to use the same vocabulary across different models seamlessly (translation, language model, image-caption ranker model). Contro"
D19-1447,N16-1004,0,0.0417725,"ts (αpg , αentr , αb ) for agent A and (βLM , βG ) for agent B, respectively (see previous section). For 4388 our joint systems with policy gradient fine-tuning, we run every model three times with different random seeds and report averaged results. Baseline and Upper Bound Our main quantitative experiment has three baselines: • Pretrained models : models pretrained on IWSLT are used without finetuning. • Ensembling : Given Fr, we let Agent A generate K En hypotheses with beam search. Then, we let Agent B generate the translation De using an ensemble of K source sentences (Firat et al., 2016; Zoph and Knight, 2016). • Agent A fixed : We fix Agent A (Fr→En) and only fine-tune Agent B using LB . This shows the communication performance achievable when Agent A cannot drift. Meanwhile, we also train an NMT model of the same architecture and size directly on the Fr→De task in Multi30k Task 1 (without English intermediary). This serves as an upper bound on the Fr→De performance achievable with available data. 5 Quantitative Results In Table 2, the top three rows are the baselines described above. The pretrained-only baseline performs relatively poorly on Fr→De, conceivably because it was pretrained on a diffe"
D19-6409,D17-1321,0,0.030686,"g step. While these agents are exchanging messages in their emergent language, we make sure that the language does not diverge too much form the original language (i.e. the language of the fixed seed population). We enforce this by having a schedule over the fine-tuning and the imitation-learning steps such that both the agents are able to solve the task while also keeping a perfect accuracy over the seed data. We call this process of generating populations as seeded self-play (S2P). 4 Problem set-up A speaker-listener game We construct a referential game similar to the Task & Talk game from (Kottur et al., 2017), except with a single turn. The game is cooperative and consists of 2 agents, a speaker and a listener. The speaker agent observes an object with a certain set of properties, and must describe the object to the listener using a sequence of words (represented by one-hot vectors). The listener then attempts to reconstruct the object. More specifically, the input space consists of p properties (e.g. shape, color) and t types per property (e.g. triangle, square). The speaker observes a symbolic representation of the input x, consisting of the concatenation of p one-hot vectors, each of length t."
D19-6409,1983.tc-1.13,0,0.47402,"Missing"
E17-1016,W13-3520,0,0.0812908,"Missing"
E17-1016,W16-2519,0,0.0207043,"system and the ground truth ranking. This protocol, however, is not directly applicable to the USF test data. First, the evaluated relation of WA is asymmetric, and the pairs (X, Y ) and (Y, X) may differ dramatically in their WA scores (see the difference in FSG and BSG values from Tab. 1). Second, instead of one global list of pairs, the data comprises a series of ranked lists conditioned on the cue/normed word wc (see Tab. 1 again). Finally, unlike with SimLex999 or MEN scores where it is difficult to interpret “what a similarity/relatedness of 7.69 exactly means” (Batchkarov et al., 2016; Avraham and Goldberg, 2016), the USF FSG scores have a direct meaningful interpretation (i.e., F SG = #P/#G). To fully capture all aspects of the ground truth USF data set, an evaluation protocol should ideally be based not only on response rankings, but also on the actual scores, i.e., the association strength. In this paper, we propose and investigate two different families of evaluation metrics on the USF data: Sect. 3.1 discusses rank correlation evaluation metrics inspired by recent work on the evaluation of vector space models in distributional semantics (Bruni et al., 2014; Hill et al., 2015; Vuli´c et al., 165 2"
E17-1016,P14-1023,0,0.0648873,"t topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ rule on the LDA output per-topic word distributions P (·|toi ) (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013).10 We train LDA with 1,000 topics using suggested parameters (Griffiths et al., 2007). Count-Based Models We evaluate the best performing reduced count-based model from (Baroni et al., 2014). We label this model count-ppmi500d.11 For a more detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow ="
E17-1016,W16-2502,0,0.0876675,"ntation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparison"
E17-1016,E14-1049,0,0.0405544,"re (Steyvers et al., 2004; Griffiths et al., 2007; Steyvers and Griffiths, 2007).9 The following quantitative model of word association has been proposed (Griffiths et al., 2007): 167 9 Griffiths et al. (2007) also experimented with LSA (Landauer and Dumais, 1997) and found that their LDA-based approach consistently outperformed LSA-based approaches. P (wr |wc ) = M X P (wr |toi )P (toi |wc ) paragram-300d, (Wieting et al., 2015)) further refined using linguistic constraints (paragram+cf300d, (Mrkˇsi´c et al., 2016)); (3) Multilingual embedding models from Luong et al. (2015) (biskip256d) and Faruqui and Dyer (2014) (bicca-512d). More detailed descriptions of all VSM models are available in the listed papers and supplementary material attached to this work. (10) i=1 where wc is a cue word, wr ∈ V r any concept from the search space, and toi is the ith latent topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ rule on the LDA output per-topic word dist"
E17-1016,N15-1184,0,0.085062,"Missing"
E17-1016,P06-1038,0,0.0664548,"Missing"
E17-1016,N13-1092,0,0.0711201,"Missing"
E17-1016,D16-1235,1,0.876283,"Missing"
E17-1016,W09-3207,0,0.142675,"Missing"
E17-1016,C16-1175,0,0.135609,"Missing"
E17-1016,D14-1032,1,0.858335,"fter post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a systematic study and comparison of current stat"
E17-1016,J15-4004,1,0.929697,"its direct analogy with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces in"
E17-1016,W16-2513,0,0.248291,"imilarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparisons between concepts on a subc"
E17-1016,W14-1503,1,0.866108,"xp. IV. Exp. III: Window Size In the next experiment, we analysed the effect of the window size on models’ ability to capture similarity, relatedness, and association. We train the sgns-pw-bow model (d = 300) with varying window sizes in the interval [1, 30]. The results on similarity (SimLex-999), relatedness (MEN), and WA benchmarks (USF) are presented in Fig. 1(a)-1(b). It is clear that using larger windows deteriorates the performance on SimLex-999 as the focus of the model is shifted from functional to topical similarity. This shift has been detected in prior work on vector space models (Kiela and Clark, 2014). However, we also observe a similar trend with MEN scores, although an opposite effect was expected, which questions the ability of MEN to accurately evaluate relatedness. The opposite effect is, however, visible with the WA evaluation, where it is evident that larger win170 dows (leading to topical similarity) lead to better WA estimates. This also provides the first hint that WA and semantic similarity capture two completely distinct semantic phenomena. Exp. IV: WA vs. Similarity vs. Relatedness We delve deeper into this conjecture by computing correlations between model rankings on the WA"
E17-1016,P14-2135,1,0.843608,"son et al., 2004). After post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a systematic study and c"
E17-1016,D15-1242,1,0.871942,"Missing"
E17-1016,Y13-1013,0,0.0480397,"d investigate whether the difference originates from inadequate evaluation data and protocols (see Fig. 1(a)-1(b) again), or whether the difference is fundamental. 6 Conclusion and Future Work In future work, we plan to test the portability of the evaluation protocol and apply it to other repositories of word association data in English (De Deyne et al., 2016), as well as in other languages, using existing WA tables in, e.g., German (Schulte im Walde et al., 2008), Dutch (De Deyne and Storms, 2008; Brysbaert et al., 2014), Italian (Guida and Lenci, 2007), Japanese (Joyce, 2005), or Cantonese (Kwong, 2013).15 In another line of future work, we will experiment with other “cognitively plausible” evaluation data such as N400 (Kutas and Federmeier, 2011; Ettinger et al., 2016), and will analyse the similarities and differences between WA and other such “cognitive” evaluation protocols, as the one relying on semantic priming (SPP) (Hutchison et al., 2013; Ettinger and Linzen, 2016). All evaluation scripts and detailed guidelines related to this work are freely available at: github.com/cambridgeltl/wa-eval/ Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL (no 648909). The auth"
E17-1016,P14-2050,0,0.0672836,"e detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The generative model closely resembles the actual process in the human brain (Griffiths et al., 2007) - when we generate responses, we first tend to associate that word with a related semantic/cognitive concept, i.e., a latent topic (the factor P (toi |wc )), and the"
E17-1016,W13-3512,0,0.0696612,"t capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparisons between concepts on a subconscious level (Kutas and Federmeier, 2011), it is at least debatable whether current similarity/relatedness evaluation sets fully capture the implicit relational structure underlying human language representation"
E17-1016,W15-1521,0,0.0259935,"ng, rooted in the psychology literature (Steyvers et al., 2004; Griffiths et al., 2007; Steyvers and Griffiths, 2007).9 The following quantitative model of word association has been proposed (Griffiths et al., 2007): 167 9 Griffiths et al. (2007) also experimented with LSA (Landauer and Dumais, 1997) and found that their LDA-based approach consistently outperformed LSA-based approaches. P (wr |wc ) = M X P (wr |toi )P (toi |wc ) paragram-300d, (Wieting et al., 2015)) further refined using linguistic constraints (paragram+cf300d, (Mrkˇsi´c et al., 2016)); (3) Multilingual embedding models from Luong et al. (2015) (biskip256d) and Faruqui and Dyer (2014) (bicca-512d). More detailed descriptions of all VSM models are available in the listed papers and supplementary material attached to this work. (10) i=1 where wc is a cue word, wr ∈ V r any concept from the search space, and toi is the ith latent topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ r"
E17-1016,P04-1003,0,0.0426851,"ystematic study and comparison of current state-of-the-art representation learning architectures on the WA task. (C3) We present a systematic quantitative analysis of the connections between the models’ performance on the subconscious WA task and their performance on benchmarking similarity and relatedness evaluation sets. 2 Motivation: Association and USF Implicit Cognitive Measures: Means of Semantic Evaluation? Several studies have shown clear correspondence between implicit cognitive measures (most notably semantic priming) and semantic relations encountered in vector space models (VSMs) (McDonald and Brew, 2004; Jones et al., 2006; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009), suggesting that some of the implicit relation structure in the human brain is already reflected in current statistical models of meaning. These findings encouraged Ettinger and Linzen (2016) to propose a preliminary evaluation framework based on semantic priming experiments (Meyer and Schvaneveldt, 1971).4 They demonstrate the feasibility of such an evaluation using a subconscious language processing task. They use the online database of the Semantic Priming Project (SPP), which compiles priming data for over 6,000 word"
E17-1016,N16-1118,0,0.0350172,"Missing"
E17-1016,N16-1018,0,0.0262075,"Missing"
E17-1016,J07-2002,0,0.137456,"Missing"
E17-1016,P15-2070,0,0.0485682,"Missing"
E17-1016,D14-1162,0,0.0961134,"and Moens, 2013).10 We train LDA with 1,000 topics using suggested parameters (Griffiths et al., 2007). Count-Based Models We evaluate the best performing reduced count-based model from (Baroni et al., 2014). We label this model count-ppmi500d.11 For a more detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The genera"
E17-1016,D15-1036,0,0.0358539,"with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variabili"
E17-1016,K15-1026,0,0.0399438,"model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The generative model closely resembles the actual process in the human brain (Griffiths et al., 2007) - when we generate responses, we first tend to associate that word with a related semantic/cognitive concept, i.e., a latent topic (the factor P (toi |wc )), and then, after establishing the co"
E17-1016,D12-1130,0,0.0310874,"s (Nelson et al., 2000; Nelson et al., 2004). After post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a sy"
E17-1016,W16-2521,0,0.0197121,"ast debatable whether current similarity/relatedness evaluation sets fully capture the implicit relational structure underlying human language representation and understanding. As evidenced by recent workshops on evaluation of semantic representations1 , the community appears to recognise that current evaluation methods are inadequate. To fill in this gap, recent work has proposed using subconscious cognitive measures of semantic connection instead, as a proxy for measuring the ability of statistical models to tackle various problems in human language understanding (Ettinger and Linzen, 2016; Søgaard, 2016; Mandera et al., 2017). Motivated by these insights, this work proposes an evaluation framework based on the word association (WA) task, firmly rooted in and described by the psychology literature, e.g., Nelson et al. (2000) and Griffiths et al. (2007)2 . Word associations, provided as simple (cue, response) concept pairs, are naturally asymmetric: they tend to be given as a repository of ranked lists of concepts col1 E.g. RepEval, https://sites.google.com/site/repevalacl16/ The WA task is a free-association task, in which participants are asked to produce the first word that came into their"
E17-1016,D15-1243,0,0.0212972,"eval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al.,"
E17-1016,N13-1011,1,0.811134,"Missing"
E17-2012,D13-1167,0,0.0193878,"not been previously used in NLP. Related Work Previous work on negation has focused on patternbased extraction of antonym pairs (Lin et al., 2003; Lobanova, 2012). Such bootstrapped lexical resources are useful for the negation task when the input words are covered. Turney (2008); Schulte im Walde and K¨oper (2013); Santus et al. (2014, 2015) use pattern-based and distributional features to distinguish synonym and antonym pairs. Schwartz et al. (2015) build a vector space using pattern-based word co-occurrence, which can be tuned to reduce the cosine similarity of antonyms. Yih et al. (2012); Chang et al. (2013) use LSA to induce antonymy-sensitive vector spaces from a thesaurus, while Zhang et al. (2014) use tensor decomposition to induce a space combining thesaurus information with neural embeddings. Pham et al. (2015); Ono et al. (2015); Nguyen et al. (2016) learn embeddings with an objective that increases the distance between antonyms, while Nguyen et al. (2016); Mrkˇsi´c et al. (2016) reweight or retrofit embeddings to fine-tune them for antonymy. Our approach differs in that we learn a negation mapping in a standard embedding space. 6 Conclusion We have shown that a representation of the seman"
E17-2012,W05-1011,0,0.0337273,"e terms.) good antonym at rank 1, or several good antonyms at rank 5, rather than returning any particular antonym as required by the GRE task. We use two datasets: the GRE test set (GRE), and a set of 99 adjectives and their antonyms from a crowdsourced dataset collected by Lenci and Benotto acccording to the guidelines of Schulte im Walde and K¨oper (2013) (LB). For each input word we retrieve the five nearest neighbors of the model prediction and check them against a gold standard. Gold standard antonyms for a word include its antonyms from the test sets and WN. Following Gorman and Curran (2005), to minimize false negatives we improve the coverage of the gold standard by expanding it with antonyms from Roget’s 21st Century Thesaurus, Third Edition.2 4 Although CCCRE achieves the highest accuracy in Experiment 1, the GRE task does not reflect our primary goal, namely to negate adjectives by generating a one-best antonym. CCCRE sometimes fails to choose the target GRE antonym, but still makes a good overall prediction. For input word doleful, the model fails to choose the GRE target word merry, preferring instead sociable. However, the top three nearest neighbors for the predicted anto"
E17-2012,D08-1103,0,0.0206901,"antonym in an arbitrary word embedding model. We show that both linear models and neural networks improve on this task when they have access to a vector representing the semantic domain of the input word, e.g. a centroid of temperature words when predicting the antonym of ‘cold’. We introduce a continuous class-conditional bilinear neural network which is able to negate adjectives with high precision. 1 Introduction Identifying antonym pairs such as hot and cold in a vector space model is a challenging task, because synonyms and antonyms are both distributionally similar (Grefenstette, 1992; Mohammad et al., 2008). Recent work on antonymy has learned specialized word embeddings using a lexical contrast objective to push antonyms further apart in the space (Pham et al., 2015; Ono et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016), which has been shown to improve both antonym detection and the overall quality of the vectors for downstream tasks. In this paper we are interested in a related scenario: given an arbitrary word embedding model, with no assumptions about pretraining for lexical contrast, we address the task of negation, which we define as the prediction of a one-best antonym for an inpu"
E17-2012,W13-3209,0,0.332009,"rs: Background Relational autoencoders (RAE), also known as gated autoencoders (GAE), have been used in 71 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 71–78, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics (a) Relational Autoencoder (b) CCRAE (c) CCCRE Figure 1: Neural network architectures and training signal for (a) RAE (Memisevic, 2013), (b) ClassConditional RAE (Rudy and Taylor, 2015), and Continuous Class-Conditional RE (this paper). Figures based on Memisevic (2013). computer vision to learn representations of transformations between images, such as rotation or translation (Memisevic and Hinton, 2007; Memisevic, 2012, 2013). RAEs are a type of gated network, which contains multiplicative connections between two related inputs. The “gating” of one image vector by another allows feature detectors to concentrate on the correspondences between the related images, rather than being distracted by the differences between untransformed images. See Figure 1(a). Multiplicative connections involve a weight for every pair of units in the input vector and gate vector"
E17-2012,J13-3004,0,0.294117,"e begin with all WN cohyponyms of an input word. If there are fewer than ten, we make up the difference with nearest neighbors from the vector space. The gate vector is the vector centroid of the resulting word list. In the standard training condition, we do not exclude antonym pairs with the target word in the test set, since we hypothesize it is important for the model to see other words with a similar semantic domain in order to learn the subtle changes necessary for negation. For example, if the pair (hot, 3.3 Evaluation Experiment 1 uses the Graduate Record Examination (GRE) questions of Mohammad et al. (2013). The task, given an input word, is to pick the best antonym from five options. An example is shown in (4), where the input word is piquant and the correct answer is bland. We use only those questions where both input and target are adjectives. piquant: (a) shocking (b) jovial (c) rigorous (4) (d) merry (e) bland We evaluate a model by predicting an antonym vector for the input word, and choosing the multiple choice option with the smallest cosine distance to the predicted vector. We report accuracy, i.e. percentage of questions answered correctly. Experiment 2 evaluates the precision of the m"
E17-2012,P82-1020,0,0.79279,"Missing"
E17-2012,J16-4003,0,0.045253,"aura.rimell@cl.cam.ac.uk Amandla Mabona University of Cambridge amandla.mabona@cl.cam.ac.uk Luana Bulat University of Cambridge ltf24@cam.ac.uk Douwe Kiela Facebook AI Research dkiela@fb.com Abstract perature; but differ in their value, or polarity—e.g. coldness (Turney, 2012; Hermann et al., 2013). Negation must alter the polarity while retaining the domain information in the word embedding. We hypothesize that a successful mapping must be conditioned on the domain, since the relevant features for negating, say, a temperature adjective, differ from those for an emotion adjective. Inspired by Kruszewski et al. (2016), who find that nearest neighbors in a vector space are a good approximation for human judgements about negation, we represent an adjective’s domain by the centroid of nearest neighbors in the embedding space or cohyponyms in WordNet. We introduce a novel variant of a bilinear relational neural network architecture which has proven successful in identifying image transformations in computer vision (Memisevic, 2012; Rudy and Taylor, 2015), and which learns a negation mapping conditioned on a gate vector representing the semantic domain of an adjective. Our model outperforms several baselines on"
E17-2012,P16-2074,0,0.193273,"e.g. a centroid of temperature words when predicting the antonym of ‘cold’. We introduce a continuous class-conditional bilinear neural network which is able to negate adjectives with high precision. 1 Introduction Identifying antonym pairs such as hot and cold in a vector space model is a challenging task, because synonyms and antonyms are both distributionally similar (Grefenstette, 1992; Mohammad et al., 2008). Recent work on antonymy has learned specialized word embeddings using a lexical contrast objective to push antonyms further apart in the space (Pham et al., 2015; Ono et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016), which has been shown to improve both antonym detection and the overall quality of the vectors for downstream tasks. In this paper we are interested in a related scenario: given an arbitrary word embedding model, with no assumptions about pretraining for lexical contrast, we address the task of negation, which we define as the prediction of a one-best antonym for an input word. For example, given the word talkative, the negation mapping should return a word from the set quiet, taciturn, uncommunicative, etc. We focus on the negation of adjectives. The intuition behind"
E17-2012,K15-1026,0,0.0144244,", 2012), and reconstructing MNIST digits and facial images (Rudy and Taylor, 2015). Wang et al. (2015) use RAEs for tag recommendation, but to our knowledge RAEs have not been previously used in NLP. Related Work Previous work on negation has focused on patternbased extraction of antonym pairs (Lin et al., 2003; Lobanova, 2012). Such bootstrapped lexical resources are useful for the negation task when the input words are covered. Turney (2008); Schulte im Walde and K¨oper (2013); Santus et al. (2014, 2015) use pattern-based and distributional features to distinguish synonym and antonym pairs. Schwartz et al. (2015) build a vector space using pattern-based word co-occurrence, which can be tuned to reduce the cosine similarity of antonyms. Yih et al. (2012); Chang et al. (2013) use LSA to induce antonymy-sensitive vector spaces from a thesaurus, while Zhang et al. (2014) use tensor decomposition to induce a space combining thesaurus information with neural embeddings. Pham et al. (2015); Ono et al. (2015); Nguyen et al. (2016) learn embeddings with an objective that increases the distance between antonyms, while Nguyen et al. (2016); Mrkˇsi´c et al. (2016) reweight or retrofit embeddings to fine-tune them"
E17-2012,N15-1100,0,0.0268111,"Missing"
E17-2012,P15-2004,0,0.159003,"he semantic domain of the input word, e.g. a centroid of temperature words when predicting the antonym of ‘cold’. We introduce a continuous class-conditional bilinear neural network which is able to negate adjectives with high precision. 1 Introduction Identifying antonym pairs such as hot and cold in a vector space model is a challenging task, because synonyms and antonyms are both distributionally similar (Grefenstette, 1992; Mohammad et al., 2008). Recent work on antonymy has learned specialized word embeddings using a lexical contrast objective to push antonyms further apart in the space (Pham et al., 2015; Ono et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016), which has been shown to improve both antonym detection and the overall quality of the vectors for downstream tasks. In this paper we are interested in a related scenario: given an arbitrary word embedding model, with no assumptions about pretraining for lexical contrast, we address the task of negation, which we define as the prediction of a one-best antonym for an input word. For example, given the word talkative, the negation mapping should return a word from the set quiet, taciturn, uncommunicative, etc. We focus on the negati"
E17-2012,C08-1114,0,0.0422099,"d images (Memisevic and Hinton, 2007), recognizing actions (Taylor et al., 2010), learning invariant features from images and videos (Grimes and Rao, 2005; Zou et al., 2012), and reconstructing MNIST digits and facial images (Rudy and Taylor, 2015). Wang et al. (2015) use RAEs for tag recommendation, but to our knowledge RAEs have not been previously used in NLP. Related Work Previous work on negation has focused on patternbased extraction of antonym pairs (Lin et al., 2003; Lobanova, 2012). Such bootstrapped lexical resources are useful for the negation task when the input words are covered. Turney (2008); Schulte im Walde and K¨oper (2013); Santus et al. (2014, 2015) use pattern-based and distributional features to distinguish synonym and antonym pairs. Schwartz et al. (2015) build a vector space using pattern-based word co-occurrence, which can be tuned to reduce the cosine similarity of antonyms. Yih et al. (2012); Chang et al. (2013) use LSA to induce antonymy-sensitive vector spaces from a thesaurus, while Zhang et al. (2014) use tensor decomposition to induce a space combining thesaurus information with neural embeddings. Pham et al. (2015); Ono et al. (2015); Nguyen et al. (2016) learn"
E17-2012,D12-1111,0,0.0209015,"Es have not been previously used in NLP. Related Work Previous work on negation has focused on patternbased extraction of antonym pairs (Lin et al., 2003; Lobanova, 2012). Such bootstrapped lexical resources are useful for the negation task when the input words are covered. Turney (2008); Schulte im Walde and K¨oper (2013); Santus et al. (2014, 2015) use pattern-based and distributional features to distinguish synonym and antonym pairs. Schwartz et al. (2015) build a vector space using pattern-based word co-occurrence, which can be tuned to reduce the cosine similarity of antonyms. Yih et al. (2012); Chang et al. (2013) use LSA to induce antonymy-sensitive vector spaces from a thesaurus, while Zhang et al. (2014) use tensor decomposition to induce a space combining thesaurus information with neural embeddings. Pham et al. (2015); Ono et al. (2015); Nguyen et al. (2016) learn embeddings with an objective that increases the distance between antonyms, while Nguyen et al. (2016); Mrkˇsi´c et al. (2016) reweight or retrofit embeddings to fine-tune them for antonymy. Our approach differs in that we learn a negation mapping in a standard embedding space. 6 Conclusion We have shown that a repres"
E17-2012,D14-1161,0,0.0604872,"ine baseline, suggesting that in the standard setting they were memorizing antonyms of semantically similar words. The Concat models and CCCRE retain a higher level of accuracy, indicating that they can generalize across different semantic classes. Table 1: Accuracy on the 367 multiple-choice adjective questions in the GRE test set. We are unable to compare directly with previous results on the GRE dataset, since our evaluation is restricted to adjectives. As an indicative comparison, Mohammad et al. (2013) report an F-score of 0.69 on the full test dataset with a thesaurusbased method, while Zhang et al. (2014) report an F-score of 0.62 using a vector space induced from WN and distributional vectors, and 0.82 with a larger thesaurus. (Previous work reported Fscore rather than accuracy due to out-of-coverage terms.) good antonym at rank 1, or several good antonyms at rank 5, rather than returning any particular antonym as required by the GRE task. We use two datasets: the GRE test set (GRE), and a set of 99 adjectives and their antonyms from a crowdsourced dataset collected by Lenci and Benotto acccording to the guidelines of Schulte im Walde and K¨oper (2013) (LB). For each input word we retrieve th"
E17-2012,Y14-1018,0,\N,Missing
J17-4004,N09-1003,0,0.567913,"alculated the average of all ratings from the accepted raters (≥ 10) for each word pair. The score was finally scaled linearly from the 0–6 to the 0–10 interval as also done by Hill, Reichart, and Korhonen (2015). 799 Computational Linguistics Volume 43, Number 4 Table 4 A comparison of HyperLex IAA with several prominent crowdsourced semantic similarity/ relatedness evaluation benchmarks that also provide scores for word pairs. Numbers in parentheses refer to the total number of word pairs in each evaluation set. Benchmark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A c"
J17-4004,W13-3520,0,0.0197215,"Missing"
J17-4004,P15-1104,0,0.0243363,"ocates the use of more sophisticated learning algorithms in future work. Another path of research work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic similarity is appropriate h"
J17-4004,E12-1004,0,0.210541,"derlines the lexical entailment (LE) relation. Simply put, an instantiation of a member concept such as a cat entails the existence of an animal. This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.1 Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing (Bos and Markert 2005; Dagan, Glickman, and Magnini 2006; Baroni et al. 2012; Beltagy et al. 2013, inter alia). The ability to effectively detect and model both lexical and phrasal entailment in a human-like way may be critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation (Androutsopoulos and Malakasiotis 2010). For instance, in order to answer a question such as “Which mammal has a strong bite?”, a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not. Although inspired to some extent by"
J17-4004,P14-1023,0,0.108108,"Missing"
J17-4004,J10-4006,0,0.0102161,"-up regarding training data, their parameter settings, and other modeling choices. DEMs and SLQS. Directional entailment measures DEM1 –DEM4 and both SLQS variants (i.e., SLQS–B ASIC and SLQS–S IM) are based on the cleaned, tokenized, and lowercased Polyglot Wikipedia (Al-Rfou, Perozzi, and Skiena 2013). We have used two set-ups for the induction of word representations, the only difference being that in Set-up 1 context/feature vectors are extracted from the Polyglot Wiki directly based on bigram co-occurrence counts, whereas in Set-up 2, these vectors are extracted from the T YPE DM tensor (Baroni and Lenci 2010) as in the original work of Lenci and Benotto (2012).27 Both set-ups use the positive LMI weighting calculated on syntactic co-occurrence links between each word and its context word (Gulordava and Baroni 2011): LMI(w1 , w2 ) = 1 ,w2 )∗Total C(w1 , w2 ) ∗ log2 C(w C(w1 )C(w2 ) , where C(w) is the unigram count in the Polyglot Wiki for the word w, C(w1 , w2 ) is the dependency based co-occurrence count of the two tokens w1 and w2 , namely (w1 , (dep_rel, w2 )), and Total is the number of all such tuples. The Polyglot Wiki was parsed with Universal Dependencies (Nivre et al. 2015) as in the work"
J17-4004,W11-2501,0,0.0591396,"owing mapping: fgraded : (X, Y) → R+ 0 (4) fgraded outputs the strength of the lexical entailment relation s ∈ R+ 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y). From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y. From the relational similarity viewpoint (Jurgens et al. 2012; Zhila et al. 2013), it also measures the prototypicality of the pair (X, Y) for the LE relation. 3.1.2 Evaluation Sets BLESS. Introduced by Baroni and Lenci (2011), the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities. A total of 175 concepts were extracted from the McRae feature norms data set (McRae et al. 2005), and the remaining 25 were selected manually by the authors. These concepts were then paired to 8,625 7 The terms intension and extension assume classical intensional and extensional definitions of a concept (van Benthem and ter Meulen 1996; Baronett 2012). ~ ), difference (Y ~ −X ~ ), or element-wise multiplication ~ ⊕"
J17-4004,W16-2502,0,0.0167661,"ave been largely overlooked in the 824 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment representation learning literature. Notable exceptions building word embeddings for LE have appeared only recently (see the work of Vendrov et al. [2016] and a short overview in Section 7.4), but a comprehensive evaluation resource for intrinsic evaluation of such LE embeddings is still missing. There is a pressing need to improve, broaden, and introduce new evaluation protocols and data sets for representation learning architectures (Schnabel et al. 2015; Tsvetkov et al. 2015; Batchkarov et al. 2016; Faruqui et al. 2016; Yaghoobzadeh and Schütze 2016, inter alia).36 We believe that one immediate application of HyperLex is its use as a comprehensive, wide-coverage large evaluation set for representationlearning architectures focused on the fundamental TYPE - OF taxonomic relation. Data Mining: Extending Knowledge Bases. Ontologies and knowledge bases such as WordNet, Yago, or DBPedia are useful resources in a variety of applications such as text generation, question answering, information retrieval, or for simply providing structured knowledge to users. Because they typically suffer from"
J17-4004,S13-1002,0,0.0247821,"entailment (LE) relation. Simply put, an instantiation of a member concept such as a cat entails the existence of an animal. This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.1 Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing (Bos and Markert 2005; Dagan, Glickman, and Magnini 2006; Baroni et al. 2012; Beltagy et al. 2013, inter alia). The ability to effectively detect and model both lexical and phrasal entailment in a human-like way may be critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation (Androutsopoulos and Malakasiotis 2010). For instance, in order to answer a question such as “Which mammal has a strong bite?”, a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not. Although inspired to some extent by theories of human sem"
J17-4004,I13-1095,0,0.26422,"exical entailment. In addition to the use of HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relat"
J17-4004,D15-1075,0,0.076472,"Missing"
J17-4004,P15-2001,0,0.111058,"Missing"
J17-4004,W09-0215,0,0.0685026,"ive than their hyponyms (Murphy 2003), which is also reflected in less specific contexts for hypernyms. Unsupervised (distributional) models of lexical entailment were instigated by the early work of Hearst (1992) on prototypicality patterns (e.g., the pattern “X such as Y” indicates that Y is a hyponym of X). The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences"
J17-4004,D10-1107,0,0.0153209,"this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations such as synonymy, met"
J17-4004,N15-1184,0,0.154452,"Missing"
J17-4004,P15-2076,0,0.0297851,"ons are interlinked. We test the following benchmarking semantic similarity models: (1) Unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) (Mikolov et al. 2013b) with various contexts (BOW = bag of words; DEPS = dependency contexts) as described by Levy and Goldberg (2014); and (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. We evaluate models that currently hold the peak scores in word similarity tasks: sparse binary vectors built from linguistic resources (N ON -D ISTRIBUTIONAL [Faruqui and Dyer 2015]), vectors fine-tuned to a paraphrase database (PARAGRAM [Wieting et al. 2015]), and further refined using linguistic constraints (PARAGRAM +CF [Mrkši´c et al. 2016]). Because these models are not the main focus of this work, the reader is referred to the relevant literature for detailed descriptions. 6.8 Gaussian Embeddings An alternative approach to learning word embeddings was proposed by Vilnis and McCallum (2015). They represent words as Gaussian densities rather than points in the embedding space. Each concept X is represented as a multivariate K-dimensional Gaussian parameterized as N"
J17-4004,W16-2506,0,0.23782,"nnotations: HyperLex may also be used in the standard format of previous LE evaluation sets (see Table 1) for detection and directionality evaluation protocols (see later in Section 7.2). Second, a typical way to evaluate word representation quality at present is by judging the similarity of representations assigned to similar words. The most popular semantic similarity evaluation sets such as SimLex-999 or SimVerb-3500 consist of word pairs with similarity ratings produced by human annotators. HyperLex is the first resource that can be used for the intrinsic evaluation (Schnabel et al. 2015; Faruqui et al. 2016) of LE-based vector space models (Vendrov et al. 2016); see later in Section 6.6. Encouraged by high inter-annotator agreement scores and evident large gaps between the human and system performance (see Section 7), we believe that HyperLex will guide the development of a new generation of representation-learning architectures that induce hypernymy/LE-specialized word representations, as opposed to the current ubiquitous word representations targeting exclusively semantic similarity and/or relatedness (see later the discussion in Sections 7.4 and 8). Finally, HyperLex provides a wide coverage o"
J17-4004,P14-1113,0,0.0669191,"X). The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences between unsupervised and supervised entailment models is provided by Turney and Mohammad (2015). Why is HyperLex Different? In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) may be straightforwardly translated into binary decision problems: (1) distinguish"
J17-4004,P05-1014,0,0.141677,"re exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym). The following mapping is defined by the joint detection and directionality function fdet+dir : fdet+dir : (X, Y) → {−1, 0, 1} (3) fdet+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to −1 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation. Standard Modeling Approaches. These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013). The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms. The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are 787 Computational Linguistics Volume 43, Number 4 semantically more general than the characteristics forming the"
J17-4004,D16-1235,1,0.310735,"Missing"
J17-4004,gheorghita-pierrel-2012-towards,0,0.064616,"Missing"
J17-4004,W16-2507,0,0.00428272,"hat ground language in the physical world (Silberer and Lapata 2012, 2014; Bruni, Tran, and Baroni 2014, inter alia). Future work might also investigate attaching graded LE scores to large hierarchical image databases such as ImageNet (Deng et al. 2009; Russakovsky et al. 2015). 9. Conclusions Although the ultimate test of semantic models is their usefulness in downstream applications, the research community is still in need of wide-coverage comprehensive gold standard resources for intrinsic evaluation (Camacho-Collados, Pilehvar, and Navigli 2015; Schnabel et al. 2015; Tsvetkov et al. 2015; Gladkova and Drozd 2016; Hashimoto, Alvarez-Melis, and Jaakkola 2016, inter alia). Such resources can measure the general quality of the representations learned by semantic models, prior to their integration in end-to-end systems. We have presented HyperLex, a large wide-coverage gold standard resource for the evaluation of semantic representations targeting the lexical relation of graded lexical entailment (LE), also known as hypernymy-hyponymy or TYPE - OF relation, a relation which is fundamental in construction and understanding of concept hierarchies, that is, semantic taxonomies. Given that the problem of conc"
J17-4004,W11-2508,0,0.00981994,"d on the cleaned, tokenized, and lowercased Polyglot Wikipedia (Al-Rfou, Perozzi, and Skiena 2013). We have used two set-ups for the induction of word representations, the only difference being that in Set-up 1 context/feature vectors are extracted from the Polyglot Wiki directly based on bigram co-occurrence counts, whereas in Set-up 2, these vectors are extracted from the T YPE DM tensor (Baroni and Lenci 2010) as in the original work of Lenci and Benotto (2012).27 Both set-ups use the positive LMI weighting calculated on syntactic co-occurrence links between each word and its context word (Gulordava and Baroni 2011): LMI(w1 , w2 ) = 1 ,w2 )∗Total C(w1 , w2 ) ∗ log2 C(w C(w1 )C(w2 ) , where C(w) is the unigram count in the Polyglot Wiki for the word w, C(w1 , w2 ) is the dependency based co-occurrence count of the two tokens w1 and w2 , namely (w1 , (dep_rel, w2 )), and Total is the number of all such tuples. The Polyglot Wiki was parsed with Universal Dependencies (Nivre et al. 2015) as in the work of Vuli´c and Korhonen (2016).28 The context vocabulary (i.e., words w2 ) is restricted to the 10K most frequent words in the Polyglot Wiki. The same two set-ups were used for the SLQS model. We also use frequ"
J17-4004,Q16-1020,0,0.0114068,"Missing"
J17-4004,C92-2082,0,0.478737,"ans perceive the concepts of typicality and graded membership within the graded LE relation. We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and mo"
J17-4004,S10-1006,0,0.0353884,"ystem has to predict the relation directionality, that is, which word is the hypernym and which word is the hyponym. More formally, the following mapping is defined by the directionality function fdir : fdir : (X, Y) → {−1, 1} (1) fdir simply maps to 1 when Y is the hypernym, and to −1 otherwise. (ii) Entailment Detection. The system has to predict whether there exists a lexical entailment relation between two words, or the words stand in some other relation (synonymy, meronymy–holonymy, causality, no relation, etc.). A more detailed overview of lexical relations is available in related work (Hendrickx et al. 2010; Jurgens et al. 2012; Vylomova et al. 2016). The following mapping is defined by the detection function fdet : fdet : (X, Y) → {0, 1} (2) fdet simply maps to 1 when (X, Y) stand in a lexical entailment relation, irrespective to the actual directionality of the relation, and to 0 otherwise. (iii) Entailment Detection and Directionality. This recently proposed evaluation protocol (Weeds et al. 2014; Kiela et al. 2015) combines (i) and (ii). The system first has to detect whether there exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predi"
J17-4004,P13-2078,0,0.08496,"s (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym). The following mapping is defined by the joint detection and directionality function fdet+dir : fdet+dir : (X, Y) → {−1, 0, 1} (3) fdet+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to −1 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation. Standard Modeling Approaches. These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013). The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms. The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are 787 Computational Linguistics Volume 43, Number 4 semantically more general than the characteristics forming the intension7 of its hyponyms (e.g., bark or has tail for the conce"
J17-4004,J15-4004,1,0.787959,"Missing"
J17-4004,N15-1070,0,0.0250832,"Missing"
J17-4004,S12-1047,0,0.0528398,"Missing"
J17-4004,D14-1005,1,0.757367,"Missing"
J17-4004,D15-1242,1,0.0670611,"k as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shw"
J17-4004,P14-2135,1,0.798609,"ndency, using sets of images associated with each concept word as returned by Google’s image search. The intuition is that the set of images returned for the broader concept animal will consist of pictures of different kinds of animals, that is, exhibiting greater visual variability and lesser concept specificity; on the other hand, the set of images for bird will consist of pictures of different birds, and the set for owl will mostly consist only of images of owls. The generality of a set of n images for each concept X is then computed. The first model relies on the image dispersion measure (Kiela et al. 2014). It is the average −→ −→ pairwise cosine distance between all image representations23 {iX,1 , . . . , iX,n } for X: id(X) = 2 n(n − 1) X j &lt; k ≤n − → −→ 1 − cos(iX,j , iX,k ) (11) Another similar measure instead of calculating the pairwise distance calculates the −→ −→ distance to the centroid − µ→ X of {iX,1 , . . . , iX,n }: 1 cent(X) = n X 1≤j≤n − → → 1 − cos(iX,j , − µX ) (12) Final Model. The following formula summarizes the visual model for ungraded LE directionality and detection that we also test in graded evaluations: ( sθ (X, Y) = 1− 0 f (X)+α f (Y) ~ ≥θ ~ Y) if cos(X, otherwise (13"
J17-4004,P15-2020,1,0.704519,"Missing"
J17-4004,N15-1016,0,0.0344693,"Missing"
J17-4004,S12-1012,0,0.0892939,"in human judgments. For instance, graded LE scores indicate that humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim"
J17-4004,W14-1610,0,0.0265556,"of substitutable LE (see Section 2). Baroni et al. (2012). The N1  N2 evaluation set contains 2,770 nominal concept pairs, with 1,385 pairs labeled as positive examples (i.e., 1 or entails) (Baroni et al. 2012). The remaining 1,385 pairs labeled as negatives were created by inverting the positive pairs and randomly matching concepts from the positive pairs. The pairs and annotations were extracted automatically from WordNet and then validated manually by the authors (e.g., the abstract concepts with a large number of hyponyms such as entity or object were removed from the pool of concepts). Levy et al. (2014). A similar data set for the standard LE evaluation may be extracted from manually annotated entailment graphs of subject–verb–object tuples (i.e., propositions) (Levy, Dagan, and Goldberger 2014): Noun LEs were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the proposition-level entailment to the word level. This data set was built for the medical domain and adopts the looser definition of substitutable LE. Custom Evaluation Sets. A plethora of relevant work on ungraded LE do not rely on established evaluation resources, but simply extrac"
J17-4004,P14-2050,0,0.56899,"overs: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we o"
J17-4004,N15-1098,0,0.247759,"s (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we observe interesting differences in the models, our findings indicate clearly that none of the currently available models or approaches accurately model the relation of graded LE reflected in human subjects. This study therefore calls for new paradigms and solutions capable of capturing the gradual nature of semantic relations such as hypernymy"
J17-4004,P98-2127,0,0.0625748,"tes the geometrical average of WeedsPrec (DEM1 ) or any other asymmetric measure (e.g., APinc from Kotlerman et al. [2010]) and the symmetric 21 Note that, unlike with similarity scores, the score now refers to an asymmetric relation stemming from the question “Is X a type of Y” for the word pair (X, Y). Therefore, the scores for two reverse pairs (X, Y) and (Y, X) should be different; see also Table 8. 806 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment similarity sim(X, Y) between X and Y, measured by cosine (Weeds, Weir, and McCarthy 2004), or the Lin measure (Lin 1998) as in the balAPinc measure of Kotlerman et al. (2010): DEM2 (X, Y) = DEM1 (X, Y) · sim(X, Y) (6) ClarkeDE (DEM3 ). A close variation of DEM1 was proposed by Clarke (2009): P DEM3 (X, Y) = ft∈FeatX ∩FeatY P min(wX ( ft), wY ( ft)) ft∈FeatX wX ( ft) (7) InvCL (DEM4 ). A variation of DEM3 was introduced by Lenci and Benotto (2012). It takes into account both the inclusion of context features of X in context features of Y and non-inclusion of features of Y in features of X.22 DEM4 (X, Y) = p DEM3 (X, Y) · (1 − DEM3 (Y, X)) (8) 6.2 Generality Measures Another related view towards the TYPE - OF rel"
J17-4004,P15-1145,0,0.0413618,"rch work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic similarity is appropriate here: It was recently demonstrated that vector space models specializing for similarity a"
J17-4004,W13-3512,0,0.0649624,"Missing"
J17-4004,W13-0904,0,0.0158973,"HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relations such as lexical entailment have been"
J17-4004,N16-1018,0,0.00692386,"Missing"
J17-4004,P17-1163,0,0.00734502,"Missing"
J17-4004,D14-1113,0,0.0146062,"that a promising step in that direction are neural net–inspired approaches to LE proposed recently (Vilnis and McCallum 2015; Vendrov et al. 2016), mostly because of their conceptual distinction from other distributional modeling approaches complemented with their modeling adaptability and flexibility. In addition, in order to model hierarchical semantic knowledge more accurately, in future work we may require algorithms that are better suited to fast learning from few examples (Lake et al. 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney 2010b; Neelakantan et al. 2014; Jauhar, Dyer, and Hovy 2015; Šuster, Titov, and van Noord 2016). Despite the abundance of reported experiments and analyses in this work, we have only scratched the surface in terms of the possible analyses with HyperLex and use of such models as components of broader phrase-level and sentence-level textual entailment systems, as well as in other applications, as quickly surveyed in Section 8. Beyond the preliminary conclusions from these initial analyses, we believe that the benefits of HyperLex will become evident as researchers use it to probe the relationship between architectures, algor"
J17-4004,N15-1100,0,0.118597,"Missing"
J17-4004,D07-1042,0,0.061095,"Missing"
J17-4004,P06-1015,0,0.0188982,"raded LE relation. We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations suc"
J17-4004,D16-1244,0,0.00552891,"(graded) semantic similarity is appropriate here: It was recently demonstrated that vector space models specializing for similarity and scoring high on SimLex-999 and SimVerb-3500 are able to boost performance of statistical systems in language understanding tasks such as dialogue state tracking (Mrkši´c et al. 2016, 2017; Vuli´c et al. 2017). We assume that the specification of what the degree of LE means for each individual pair may also boost performance of statistical end-to-end systems in another language understanding task in future work: natural language inference (Bowman et al. 2015; Parikh et al. 2016; Agi´c and Schluter 2017). Owing to their adaptability and versatility, representation architectures inspired by neural networks (e.g., Mrkši´c et al. 2016; Vendrov et al. 2016) seem to be a promising avenue for future modeling work on graded lexical entailment in both unsupervised and supervised settings, despite their low performance on the graded LE task at present. 8. Application Areas: A Quick Overview The proposed data set should have an immediate impact in the cognitive science research, providing means to analyze the effects of typicality and gradience in concept representations (Hamp"
J17-4004,N04-3012,0,0.0574719,"Missing"
J17-4004,W14-1608,0,0.0448142,"Missing"
J17-4004,D10-1114,0,0.031074,"ark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over differe"
J17-4004,N10-1013,0,0.0779609,"ark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over differe"
J17-4004,N13-1008,0,0.0139297,"but simply extract ad hoc LE evaluation data using distant supervision from readily available semantic resources and knowledge bases such as WordNet (Miller 1995), DBPedia (Auer et al. 2007), Freebase (Tanon et al. 2016), Yago (Suchanek, Kasneci, and Weikum 2007), or dictionaries (Gheorghita and Pierrel 2012). Although plenty of the custom evaluation sets are available online, there is a clear tendency to construct a new custom data set in every subsequent paper that uses the same evaluation protocol for ungraded LE. A standard practice (Snow, Jurafsky, and Ng 2004, 2006; Bordes et al. 2011; Riedel et al. 2013; Socher et al. 2013; Weeds et al. 2014; Shwartz, Goldberg, and Dagan 2016; Vendrov et al. 2016, inter alia) is to extract positive and negative pairs by coupling concepts that are directly related in at least one of the resources. Only pairs standing in an unambiguous hypernymy/LE relation, according to the set of indicators from Table 2, are annotated as positive examples (i.e., again 1 or entailing, Table 1) (Shwartz et al. 2015). All other pairs standing in other relations are taken as negative instances. Using related rather than random concept pairs as negative instances enables detectio"
J17-4004,E14-1054,0,0.125239,"unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences between unsupervised and supervised entailment models is provided by Turney and Mohammad (2015). Why is HyperLex Different? In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) may be straightforwardly translated into binary decision problems: (1) distinguishing between h"
J17-4004,D16-1234,0,0.0924128,"Missing"
J17-4004,C14-1097,0,0.050532,"Missing"
J17-4004,P15-2119,0,0.0320914,"ven distributional LE models. In current binary evaluation protocols targeting ungraded LE detection and directionality, even simple methods modeling lexical generality are able to yield very accurate predictions. However, our preliminary analysis in Section 7.2 demonstrates their fundamental limitations for graded lexical entailment. In addition to the use of HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and"
J17-4004,C08-3008,0,0.0239186,"eve that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relations such as lexical entailment have been largely overlooked in the 824 Vuli´c et al."
J17-4004,E14-4008,0,0.0900705,"at humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test split"
J17-4004,W15-4208,0,0.0243423,"ty evaluations (Santus et al. 2014; Kiela et al. 2015), only the LE subset is used. Note that the original BLESS data are always presented with the hyponym first, so gold annotations are implicitly provided here. Second, for detection evaluations (Roller, Erk, and Boleda 2014; Santus et al. 2014; Levy et al. 2015), the pairs from the LE subset are taken as positive pairs, and all the remaining pairs are considered negative pairs. That way, the evaluation data effectively measure a model’s ability to predict the positive LE relation. Another evaluation data set based on BLESS was introduced by Santus et al. (2015). Following the standard annotation scheme, it comprises 7,429 noun pairs in total, and 1,880 LE pairs in particular, covering a wider range of relations than BLESS (i.e., the data set now includes synonymy and antonymy pairs). Adaptations of the original BLESS evaluation set were proposed recently. First, relying on its LE subset, Weeds et al. (2014) created another data set called WBLESS (Kiela et al. 2015) consisting of 1,976 concept pairs in total. Only (X, Y) pairs where Y is the hypernym are annotated as positive examples. It also contains reversed LE pairs (i.e., X is the hypernym), coh"
J17-4004,D15-1036,0,0.282274,"onverted to ungraded annotations: HyperLex may also be used in the standard format of previous LE evaluation sets (see Table 1) for detection and directionality evaluation protocols (see later in Section 7.2). Second, a typical way to evaluate word representation quality at present is by judging the similarity of representations assigned to similar words. The most popular semantic similarity evaluation sets such as SimLex-999 or SimVerb-3500 consist of word pairs with similarity ratings produced by human annotators. HyperLex is the first resource that can be used for the intrinsic evaluation (Schnabel et al. 2015; Faruqui et al. 2016) of LE-based vector space models (Vendrov et al. 2016); see later in Section 6.6. Encouraged by high inter-annotator agreement scores and evident large gaps between the human and system performance (see Section 7), we believe that HyperLex will guide the development of a new generation of representation-learning architectures that induce hypernymy/LE-specialized word representations, as opposed to the current ubiquitous word representations targeting exclusively semantic similarity and/or relatedness (see later the discussion in Sections 7.4 and 8). Finally, HyperLex prov"
J17-4004,K15-1026,0,0.0344989,"Missing"
J17-4004,P16-1226,0,0.365672,"Missing"
J17-4004,K15-1018,0,0.0116299,"ata set in every subsequent paper that uses the same evaluation protocol for ungraded LE. A standard practice (Snow, Jurafsky, and Ng 2004, 2006; Bordes et al. 2011; Riedel et al. 2013; Socher et al. 2013; Weeds et al. 2014; Shwartz, Goldberg, and Dagan 2016; Vendrov et al. 2016, inter alia) is to extract positive and negative pairs by coupling concepts that are directly related in at least one of the resources. Only pairs standing in an unambiguous hypernymy/LE relation, according to the set of indicators from Table 2, are annotated as positive examples (i.e., again 1 or entailing, Table 1) (Shwartz et al. 2015). All other pairs standing in other relations are taken as negative instances. Using related rather than random concept pairs as negative instances enables detection experiments. We adopt a similar construction principle regarding wide coverage of different lexical relations in HyperLex. This decision will support a variety of interesting analyses related to graded LE and other relations. Table 2 Indicators of LE/hypernymy relation in structured semantic resources. Resource Relation WordNet Wikidata DBPedia Yago instance hypernym, hypernym subclass of, instance of type subclass of 790 Vuli´c e"
J17-4004,E17-1007,0,0.189385,"Missing"
J17-4004,D12-1130,0,0.0201593,"Missing"
J17-4004,P14-1068,0,0.0145115,"353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over different groups of pairs accordin"
J17-4004,P06-1101,0,0.0237483,"Missing"
J17-4004,Q14-1017,0,0.00703476,"image captioning can be seen as special cases of a partial order over unified visual– semantic hierarchies (Deselaers and Ferrari 2011; Vendrov et al. 2016), see also Figure 6. For instance, image captions may be seen as abstractions of images, and they can be expressed at various levels in the hierarchy. The same image may be abstracted as, for example, A boy and a girl walking their dog, People walking their dog, People walking, A boy, a girl, and a dog, Children with a dog, Children with an animal. LE might prove helpful in research on image captioning (Hodosh, Young, and Hockenmaier 2013; Socher et al. 2014; Bernardi et al. 2016) or cross-modal information retrieval (Pereira et al. 2014) based on such visual–semantic hierarchies, but it is yet to be seen whether the knowledge of gradience and prototypicality may contribute to image captioning systems. Image generality is closely linked to semantic generality, as is evident from recent work (Deselaers and Ferrari 2011; Kiela et al. 2015). The data set could also be very useful in evaluating models that ground language in the physical world (Silberer and Lapata 2012, 2014; Bruni, Tran, and Baroni 2014, inter alia). Future work might also investiga"
J17-4004,D15-1243,0,0.035761,"s lexical entailment have been largely overlooked in the 824 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment representation learning literature. Notable exceptions building word embeddings for LE have appeared only recently (see the work of Vendrov et al. [2016] and a short overview in Section 7.4), but a comprehensive evaluation resource for intrinsic evaluation of such LE embeddings is still missing. There is a pressing need to improve, broaden, and introduce new evaluation protocols and data sets for representation learning architectures (Schnabel et al. 2015; Tsvetkov et al. 2015; Batchkarov et al. 2016; Faruqui et al. 2016; Yaghoobzadeh and Schütze 2016, inter alia).36 We believe that one immediate application of HyperLex is its use as a comprehensive, wide-coverage large evaluation set for representationlearning architectures focused on the fundamental TYPE - OF taxonomic relation. Data Mining: Extending Knowledge Bases. Ontologies and knowledge bases such as WordNet, Yago, or DBPedia are useful resources in a variety of applications such as text generation, question answering, information retrieval, or for simply providing structured knowledge to users. Because the"
J17-4004,J06-3003,0,0.0147319,"r free variance in the collected data in terms of their quantity and representative concept pairs. In addition, the distinction is often not evident for verb concepts. We leave further developments with respect to the two related phenomena of typicality and vagueness for future work, and refer the interested reader to the aforementioned literature. However, we already provide preliminary qualitative analyses in this article (see Section 5) suggesting that both phenomena are captured in graded LE ratings. Relation to Relational Similarity. A strand of related research on relational similarity (Turney 2006; Jurgens et al. 2012) also assigns the score s to a pair of concepts (X, Y). Note that there exists a fundamental difference between relational similarity and graded LE. In the latter, s refers to the degree of the LE relation in the (X, Y) pair, that is, to the levels of typicality and graded membership of the instance X for the class Y, whereas the former quantifies the typicality of the pair (X, Y) for some fixed lexical relation class R (Bejar, Chaffin, and Embretson 1991; Vylomova et al. 2016), for example, to what degree the pair (snake, animal) reflects a typical LE relation or a typic"
J17-4004,N16-1160,0,0.0325268,"Missing"
J17-4004,P16-2084,1,0.882326,"Missing"
J17-4004,P17-1006,1,0.87338,"Missing"
J17-4004,P16-1158,0,0.11102,"ded LE ratings. Relation to Relational Similarity. A strand of related research on relational similarity (Turney 2006; Jurgens et al. 2012) also assigns the score s to a pair of concepts (X, Y). Note that there exists a fundamental difference between relational similarity and graded LE. In the latter, s refers to the degree of the LE relation in the (X, Y) pair, that is, to the levels of typicality and graded membership of the instance X for the class Y, whereas the former quantifies the typicality of the pair (X, Y) for some fixed lexical relation class R (Bejar, Chaffin, and Embretson 1991; Vylomova et al. 2016), for example, to what degree the pair (snake, animal) reflects a typical LE relation or a typical synonymy relation.5 Graded LE vs. Semantic Similarity. A plethora of current evaluations in NLP and representation learning almost exclusively focus on semantic similarity and relatedness. Semantic similarity as quantified by, for example, SimLex-999 or SimVerb-3500 (Gerz et al. 2016) may be redefined as graded synonymy relation. The graded scores there, in fact, refer to the strength of the synonymy relation between any pair of concepts (X, Y). One could say that semantic similarity aims to answ"
J17-4004,C14-1212,0,0.55778,"Missing"
J17-4004,W03-1011,0,0.0185238,"s well. We closely follow the work from Lenci and Benotto (2012) in the presentation. Let FeatX denote the set of distributional features ft for a concept word X, and let wX ( ft) refer to the weight of the feature ft for X. The most common choices for the weighting function in traditional count-based distributional models are positive variants of pointwise mutual information (PMI) (Bullinaria and Levy 2007) and local mutual information (LMI) (Evert 2008). WeedsPrec (DEM1 ). This DEM quantifies the weighted inclusion of the features of a concept word X within the features of a concept word Y (Weeds and Weir 2003; Weeds, Weir, and McCarthy 2004; Kotlerman et al. 2010): P DEM1 (X, Y) = ft∈FeatX ∩FeatY P ft∈FeatX wX ( ft) wX ( ft) (5) WeedsSim (DEM2 ). It computes the geometrical average of WeedsPrec (DEM1 ) or any other asymmetric measure (e.g., APinc from Kotlerman et al. [2010]) and the symmetric 21 Note that, unlike with similarity scores, the score now refers to an asymmetric relation stemming from the question “Is X a type of Y” for the word pair (X, Y). Therefore, the scores for two reverse pairs (X, Y) and (Y, X) should be different; see also Table 8. 806 Vuli´c et al. HyperLex: A Large-Scale Ev"
J17-4004,C04-1146,0,0.49462,"Missing"
J17-4004,Q15-1025,0,0.5743,"l models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we observe interesting di"
J17-4004,P94-1019,0,0.205773,"in three variant WN-based models: (1) WN–B ASIC: fWN returns a score denoting how similar two concepts are, based on the shortest path that connects the concepts in the WN taxonomy. (2) WN–LC H: Leacock–Chodorow similarity function (Leacock and Chodorow 1998) returns a score denoting how similar two concepts are, based on their shortest connecting path (as above) and the maximum depth of the taxonomy in which the concepts occur. The score is then − log(path/2 · depth), where path is the shortest connecting path length and depth the taxonomy depth. (3) WN–W U P: Wu–Palmer similarity function (Wu and Palmer 1994; Pedersen, Patwardhan, and Michelizzi 2004) returns a score denoting how similar two concepts are, based on their depth in the taxonomy and that of their most specific ancestor node. Note that all three WN-based similarity measures are not well-suited for graded LE experiments by their design: For example, they will rank direct co-hyponyms as more similar than distant hyponymy–hypernymy pairs. 6.6 Order Embeddings Following trends in semantic similarity (or graded synonymy computations, see Section 2 again), Vendrov et al. (2016) have recently demonstrated that it is possible to construct a v"
J17-4004,D12-1111,0,0.0202343,"Missing"
J17-4004,Q14-1006,0,0.0151611,"in semantic similarity (or graded synonymy computations, see Section 2 again), Vendrov et al. (2016) have recently demonstrated that it is possible to construct a vector space or a word embedding model that specializes in the lexical entailment relation, rather than in the more popular similarity/synonymy relation. The model is then applied to a variety of tasks including ungraded LE detection and directionality. 809 Computational Linguistics Volume 43, Number 4 Figure 6 A slice of the visual–semantic hierarchy. The toy example is taken from Vendrov et al. (2016), inspired by the resource of Young et al. (2014). The order embedding model exploits the partial order structure of a visual–semantic hierarchy (see Figure 6) by learning a mapping which is not distance-preserving but order-preserving between the visual–semantic hierarchy and a partial order over the embedding space. It learns a mapping from a partially ordered set (U, U ) into a partially ordered embedding space (V, V ): the ordering of a pair in U is then based on the ordering in the embedding space. The chosen embedding space is the reversed product order on RN + , defined by the conjunction of total orders on each coordinate: ~ ~ Y X"
J17-4004,P14-2089,0,0.0202901,"iminary analysis advocates the use of more sophisticated learning algorithms in future work. Another path of research work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic simi"
J17-4004,N13-1120,0,0.0100078,"ther relations. HyperLex, on the other hand, targets a different type of evaluation. The graded entailment function fgraded defines the following mapping: fgraded : (X, Y) → R+ 0 (4) fgraded outputs the strength of the lexical entailment relation s ∈ R+ 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y). From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y. From the relational similarity viewpoint (Jurgens et al. 2012; Zhila et al. 2013), it also measures the prototypicality of the pair (X, Y) for the LE relation. 3.1.2 Evaluation Sets BLESS. Introduced by Baroni and Lenci (2011), the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities. A total of 175 concepts were extracted from the McRae feature norms data set (McRae et al. 2005), and the remaining 25 were selected manually by the authors. These concepts were then paired to 8,625 7 The terms intension and extension assume classical intensional and exten"
J17-4004,J09-3004,0,0.039597,"inguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations such as synonymy, metonymy, meronymy, and so forth.3 Definitions. The classical definition of ungraded lexical entailment is as follows: Given a concept word pair (X, Y), Y is a hypernym"
J17-4004,H05-1079,0,\N,Missing
J17-4004,W07-1401,0,\N,Missing
J17-4004,C98-2122,0,\N,Missing
L18-1269,S12-1051,0,0.447613,"tion capabilities, composing unseen combinations of words and encoding grammatical constructions that are not present in the task-specific training data. Hence, high-quality universal sentence representations are highly desirable for a variety of downstream NLP tasks. The evaluation of general-purpose word and sentence embeddings has been problematic (Chiu et al., 2016; Faruqui et al., 2016), leading to much discussion about the best way to go about it1 . On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judgments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these representations would actually be applied (Ettinger et al., 2016; Nayak et al., 2016). In the case of sentence representations, there is a wide variety of evaluations available, many from before the “embedding era”, that can be used to assess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et"
L18-1269,S13-1004,0,0.061801,"s. Semantic Textual Similarity While semantic relatedness requires training a model on top of the sentence embeddings, we also evaluate embeddings on the unsupervised SemEval tasks. These datasets include pairs of sentences taken from news articles, forum discussions, news conversations, headlines, image and video descriptions labeled with a similarity score between 0 and 5. The goal is to evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spearman correlations. Paraphrase detection The Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) is composed of pairs of sentences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship. We use the same 4 Ant"
L18-1269,S15-2045,0,0.0293789,"s requires training a model on top of the sentence embeddings, we also evaluate embeddings on the unsupervised SemEval tasks. These datasets include pairs of sentences taken from news articles, forum discussions, news conversations, headlines, image and video descriptions labeled with a similarity score between 0 and 5. The goal is to evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spearman correlations. Paraphrase detection The Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) is composed of pairs of sentences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship. We use the same 4 Antonio Rivera - CC BY 2.0 - flickr 1700 Due to License iss"
L18-1269,S16-1081,0,0.270529,"top of the sentence embeddings, we also evaluate embeddings on the unsupervised SemEval tasks. These datasets include pairs of sentences taken from news articles, forum discussions, news conversations, headlines, image and video descriptions labeled with a similarity score between 0 and 5. The goal is to evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spearman correlations. Paraphrase detection The Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) is composed of pairs of sentences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship. We use the same 4 Antonio Rivera - CC BY 2.0 - flickr 1700 Due to License issues, we do not include the SMT"
L18-1269,D15-1075,0,0.038605,"r TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score between 0 and 5 from two input sentences, we follow the approach of Tai et al. (2015a) and learn to predict the probability distribution of relatedness scores. SentEval reports Pearson and Spearman correlation. In addition, we include the SNLI dataset (Bowman et al., 2015), a collection of 570k human-written English supporting the task of natural language inference (NLI), also known as rec3 ognizing textual entailment (RTE) which consists of predicting whether two input sentences are entailed, neutral or contradictory. SNLI was specifically designed to serve as a benchmark for evaluating text representation learning methods. Semantic Textual Similarity While semantic relatedness requires training a model on top of the sentence embeddings, we also evaluate embeddings on the unsupervised SemEval tasks. These datasets include pairs of sentences taken from news art"
L18-1269,S17-2001,0,0.0307753,"orhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score between 0 and 5 from two input sentences, we follow the approach of Tai et al. (2015a) and learn to predict the probability distribution of relatedness scores. SentEval reports Pearson and Spearman correlation. In addition, we include the SNLI dataset (Bowman et al., 2015), a collection of 570k human-written English supporting the task of natural language inference (NLI), also known as rec3 ognizing textual entailment (RTE) which consists of predicting whether two input sentences are entailed, neutral or contradictory. S"
L18-1269,W16-2501,0,0.0277689,"d training data, leading to sparsity and poor vocabulary coverage, which in turn lead to poor generalization capabilities. Similarly, sentence embeddings (which are often built on top of word embeddings) can be used to further increase generalization capabilities, composing unseen combinations of words and encoding grammatical constructions that are not present in the task-specific training data. Hence, high-quality universal sentence representations are highly desirable for a variety of downstream NLP tasks. The evaluation of general-purpose word and sentence embeddings has been problematic (Chiu et al., 2016; Faruqui et al., 2016), leading to much discussion about the best way to go about it1 . On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judgments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these representations would actually be applied (Ettinger et al., 2016; Nayak et al., 2016). In the case of sentence representations, there is a wide variety of evaluations available, many from"
L18-1269,D17-1070,1,0.739102,"ariety of evaluations available, many from before the “embedding era”, that can be used to assess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et al., 2015), concerning what evaluations to use. Recent works in which various alternative sentence encoders are compared use a similar set of tasks ∗ LIUM, Universit´e Le Mans 1 See also recent workshops on evaluating representations for NLP, e.g. RepEval: https://repeval2017.github.io/ (Hill et al., 2016a; Conneau et al., 2017). Implementing pipelines for this large set of evaluations, each with its own peculiarities, is cumbersome and induces unnecessary wheel reinventions. Another wellknown problem with the current status quo, where everyone uses their own evaluation pipeline, is that different preprocessing schemes, evaluation architectures and hyperparameters are used. The datasets are often small, meaning that minor differences in evaluation setup may lead to very different outcomes, which implies that results reported in papers are not always fully comparable. In order to overcome these issues, we introduce Se"
L18-1269,C04-1051,0,0.141667,"en 0 and 5. The goal is to evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations. We include STS tasks from 2012 (Agirre et al., 2012), 20134 (Agirre et al., 2013), 2014 (Agirre et al., 2014), 2015 (Agirre et al., 2015) and 2016 (Agirre et al., 2016). Each of these tasks includes several subtasks. SentEval reports both the average and the weighted average (by number of samples in each subtask) of the Pearson and Spearman correlations. Paraphrase detection The Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004) is composed of pairs of sentences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship. We use the same 4 Antonio Rivera - CC BY 2.0 - flickr 1700 Due to License issues, we do not include the SMT subtask. approach as with SICK-E, except that our classifier has only 2 classes, i.e., the aim is to predict whether the sentences are paraphrases or not. Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Lin et al., 201"
L18-1269,W16-2524,0,0.00676341,"ble for a variety of downstream NLP tasks. The evaluation of general-purpose word and sentence embeddings has been problematic (Chiu et al., 2016; Faruqui et al., 2016), leading to much discussion about the best way to go about it1 . On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judgments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these representations would actually be applied (Ettinger et al., 2016; Nayak et al., 2016). In the case of sentence representations, there is a wide variety of evaluations available, many from before the “embedding era”, that can be used to assess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et al., 2015), concerning what evaluations to use. Recent works in which various alternative sentence encoders are compared use a similar set of tasks ∗ LIUM, Universit´e Le Mans 1 See also recent workshops on evaluating representa"
L18-1269,W16-2506,0,0.0168144,"Missing"
L18-1269,N16-1162,0,0.108444,"mposing unseen combinations of words and encoding grammatical constructions that are not present in the task-specific training data. Hence, high-quality universal sentence representations are highly desirable for a variety of downstream NLP tasks. The evaluation of general-purpose word and sentence embeddings has been problematic (Chiu et al., 2016; Faruqui et al., 2016), leading to much discussion about the best way to go about it1 . On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judgments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these representations would actually be applied (Ettinger et al., 2016; Nayak et al., 2016). In the case of sentence representations, there is a wide variety of evaluations available, many from before the “embedding era”, that can be used to assess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et al., 2015), concern"
L18-1269,D13-1090,0,0.0246471,"Missing"
L18-1269,P07-2045,0,0.0120889,"params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128, 'tenacity': 3, 'epoch_size': 2} You may also pass additional parameters to the params object in order which will further be accessible from the prepare and batcher functions (e.g a pretrained model). Datasets In order to obtain the data and preprocess it so that it can be fed into SentEval, we provide the get transfer data.bash script in the data directory. The script fetches the different datasets from their known locations, unpacks them and preprocesses them. We tokenize each of the datasets with the MOSES tokenizer (Koehn et al., 2007) and convert all files to UTF-8 encoding. Once this script has been executed, the task path parameter can be set to indicate the path of the data directory. Requirements SentEval is written in Python. In order to run the evaluations, the user will need to install numpy, scipy and recent versions of pytorch and scikit-learn. In order to facilitate research where no GPUs are available, we offer for the evaluations to be run on CPU (using scikitlearn) where possible. For the bigger datasets, where more complicated models are often required, for instance STS Benchmark, SNLI, SICK-R and the image-c"
L18-1269,S14-2055,0,0.00967288,"81.1 86.3 92.4 90.2 84.6 46.3 InferSent Supervised methods directly trained for each task (no transfer) SOTA 83.11 86.31 95.51 93.31 89.52 52.42 TREC MRPC SICK-E 83.0 85.2 83.4 85.6 88.4 88.2 72.7/81.0 73.0/80.9 74.4/82.4 74.4/82.3 72.4/81.6 76.2/83.1 78.5 79.0 78.9 80.2 79.5 86.3 96.12 80.4/85.93 84.54 Table 3: Transfer test results for various baseline methods. We include supervised results trained directly on each task (no transfer). Results 1 correspond to AdaSent (Zhao et al., 2015), 2 to BLSTM-2DCNN (Zhou et al., 2016), 3 to TF-KLD (Ji and Eisenstein, 2013) and 4 to Illinois-LH system (Lai and Hockenmaier, 2014). • dropout (float): dropout rate in the case of MLP. • SkipThought vectors (Ba et al., 2016) For use cases where there are multiple calls to SentEval, e.g when evaluating the sentence encoder at every epoch of training, we propose the following prototyping set of parameters, which will lead to slightly worse results but will make the evaluation significantly faster: params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128, 'tenacity': 3, 'epoch_size': 2} You may also pass additional parameters to the params object in order which will further be accessible from the prepare and"
L18-1269,marelli-etal-2014-sick,0,0.234506,"sentiment analysis (MR and both binary and fine-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score between 0 and 5 from two input sentences, we follow the approach of Tai et al. (2015a) and learn to predict the probability distribution of relatedness scores. SentEval reports Pearson and Spearman correlation. In addition, we include the SNLI dataset (Bowman et al., 2015), a collection of 570k human-written English supporting the task of natural language inference (NLI), also known as rec3 ognizin"
L18-1269,W16-2504,0,0.0240212,"wnstream NLP tasks. The evaluation of general-purpose word and sentence embeddings has been problematic (Chiu et al., 2016; Faruqui et al., 2016), leading to much discussion about the best way to go about it1 . On the one hand, people have measured performance on intrinsic evaluations, e.g. of human judgments of word or sentence similarity ratings (Agirre et al., 2012; Hill et al., 2016b) or of word associations (Vuli´c et al., 2017). On the other hand, it has been argued that the focus should be on downstream tasks where these representations would actually be applied (Ettinger et al., 2016; Nayak et al., 2016). In the case of sentence representations, there is a wide variety of evaluations available, many from before the “embedding era”, that can be used to assess representational quality on that particular task. Over the years, something of a consensus has been established, mostly based on the evaluations in seminal papers such as SkipThought (Kiros et al., 2015), concerning what evaluations to use. Recent works in which various alternative sentence encoders are compared use a similar set of tasks ∗ LIUM, Universit´e Le Mans 1 See also recent workshops on evaluating representations for NLP, e.g. R"
L18-1269,P04-1035,0,0.0540204,"bels are scores between 0 and 5. PD=paraphrase detection, ICR=image-caption retrieval. for a broad set of tasks. To evaluate the quality of these representations, we use them as features in various transfer tasks. Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR and both binary and fine-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score between 0 and 5 from two i"
L18-1269,P05-1015,0,0.707122,"the beach.” rank 1.6 4.6 paraphrase Table 2: Natural Language Inference and Semantic Similarity tasks. NLI labels are contradiction, neutral and entailment. STS labels are scores between 0 and 5. PD=paraphrase detection, ICR=image-caption retrieval. for a broad set of tasks. To evaluate the quality of these representations, we use them as features in various transfer tasks. Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR and both binary and fine-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets"
L18-1269,D14-1162,0,0.118184,"Missing"
L18-1269,D16-1159,0,0.00849353,"them highly desirable for downstream NLP tasks. We introduced SentEval as a fair, straightforward and centralized toolkit for evaluating sentence representations. We have aimed to make evaluation as easy as possible: sentence encoders can be evaluated by implementing a simple Python interface, and we provide a script to download the necessary evaluation datasets. In future work, we plan to enrich SentEval with additional tasks as the consensus on the best evaluation for sentence embeddings evolves. In particular, tasks that probe for specific linguistic properties of the sentence embeddings (Shi et al., 2016; Adi et al., 2017) are interesting directions towards understanding how the encoder understands language. We hope that our toolkit will be used by the community in order to ensure that fully comparable results are published in research papers. 1702 Model SST’12 SST’13 SST’14 SST’15 SST’16 Representation learning (transfer) GloVe BoW 52.1 49.6 54.6 56.1 51.4 fastText BoW 58.3 57.9 64.9 67.6 64.3 SkipThought-LN 30.8 24.8 31.4 31.0 59.2 58.9 69.6 71.3 71.5 InferSent Char-phrase 66.1 57.2 74.7 76.1 Supervised methods directly trained for each task (no transfer) PP-Proj 60.01 56.81 71.31 74.81 - S"
L18-1269,D13-1170,0,0.0205232,"6 4.6 paraphrase Table 2: Natural Language Inference and Semantic Similarity tasks. NLI labels are contradiction, neutral and entailment. STS labels are scores between 0 and 5. PD=paraphrase detection, ICR=image-caption retrieval. for a broad set of tasks. To evaluate the quality of these representations, we use them as features in various transfer tasks. Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR and both binary and fine-grained SST) (Pang and Lee, 2005; Socher et al., 2013), question-type (TREC) (Voorhees and Tice, 2000), product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity (SUBJ) (Pang and Lee, 2004) and opinion polarity (MPQA) (Wiebe et al., 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and t"
L18-1269,P15-1150,0,0.400094,", 2005). We generate sentence vectors and classifier on top, either in the form of a Logistic Regression or an MLP. For MR, CR, SUBJ and MPQA, we use nested 10-fold cross-validation, for TREC cross-validation and for SST standard validation. Entailment and semantic relatedness We also include the SICK dataset (Marelli et al., 2014) for entailment (SICK-E), and semantic relatedness datasets including SICK-R and the STS Benchmark dataset (Cer et al., 2017). For semantic relatedness, which consists of predicting a semantic score between 0 and 5 from two input sentences, we follow the approach of Tai et al. (2015a) and learn to predict the probability distribution of relatedness scores. SentEval reports Pearson and Spearman correlation. In addition, we include the SNLI dataset (Bowman et al., 2015), a collection of 570k human-written English supporting the task of natural language inference (NLI), also known as rec3 ognizing textual entailment (RTE) which consists of predicting whether two input sentences are entailed, neutral or contradictory. SNLI was specifically designed to serve as a benchmark for evaluating text representation learning methods. Semantic Textual Similarity While semantic relatedn"
L18-1269,E17-1016,1,0.821259,"Missing"
L18-1269,D16-1157,0,0.0290026,"Missing"
L18-1269,C16-1329,0,0.00812349,"Missing"
L18-1269,S14-2010,0,\N,Missing
N16-1020,R11-1055,0,0.124832,"re 1{·} is the indicator function and we train on D examples with K classes. We obtain image embeddings by doing a forward pass with a given image and taking the 4096-dimensional fully connected layer that precedes the softmax (typically called FC7) as the representation of that image. To construct our embeddings, we used up to 10 images for a given word or phrase, which were obtained through Google Images. It has been shown that images from Google yield higher quality representations than comparable resources such as Flickr and are competitive with hand-crafted datasets (Fergus et al., 2005; Bergsma and Goebel, 2011). We created our final visual representations for words and phrases by taking the average of the extracted image embeddings for a given word or phrase. 3.3 Multimodal fusion strategies While it is desirable to jointly learn representations from different modalities at the same time, this is often not feasible (or may lead to poor performance) due to data sparsity. Instead, we learn uni-modal representations independently, as described above, and then combine them into multi-modal ones. Previous work in multi-modal semantics (Bruni et al., 163 2014) investigated different ways of combining, or"
N16-1020,W13-0901,0,0.495777,"een distinct, and seemingly unrelated, concepts. For instance, when we talk about “the turning wheels of a political regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neu"
N16-1020,W06-3506,0,0.0602043,"and Johnson, 1980; Feldman, 2006). Metaphors arise due to systematic associations between distinct, and seemingly unrelated, concepts. For instance, when we talk about “the turning wheels of a political regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, a"
N16-1020,W13-0908,0,0.0505639,"es of the known ones. Turney et al. (2011) hypothesized that metaphor is commonly used to describe abstract concepts in terms of more concrete or physical experiences. Thus, Turney and colleagues expected that there would be some discrepancy in the level of concreteness of source and target terms in the metaphor. They developed a method to automatically measure concreteness of words and applied it to identify verbal and adjectival metaphors. Neuman et al. (2013) and Gandy et al. (2013) followed in Turney’s steps, extending the models by incorporating information about selectional preferences. Heintz et al. (2013) and Strzalkowski et al. (2013) focused on modeling topical structure of text to identify metaphor. Their main hypothesis was that metaphorical language (coming from a different domain) would represent atypical vocabulary within the topical structure of the text. Strzalkowski et al. (2013) acquired a set of topic chains by linking semantically related words in a given text. They then looked for vocabulary outside the topic chain and yet connected to topic chain words via syntactic dependencies and exhibiting high imageability. Heintz et al. (2013) used LDA topic modelling to identify sets of s"
N16-1020,W13-0907,0,0.119098,"c associations between distinct, and seemingly unrelated, concepts. For instance, when we talk about “the turning wheels of a political regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al"
N16-1020,D14-1005,1,0.748079,"al., 2014), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013) and bilingual lexicon induction (Kiela et al., 2015b). Using visual information is particularly relevant to modelling metaphor, where imagery is ported across domains. In this paper, we present the first metaphor identification method integrating meaning representations learned from linguistic and visual data. We construct our representations using a skip-gram model of Mikolov et al. (2013a) trained on textual data to obtain linguistic embeddings and a deep convolutional neural network (Kiela and Bottou, 2014) trained on image data to obtain visual embeddings. Linguistic word embeddings have been previously successfully used to answer analogy questions (Mikolov et al., 2013b; Levy and Goldberg, 2014). These works have shown that such representations capture the nuances of word meaning needed to recognise relational similarity (e.g. between pairs “king : queen” and “man : woman”), quantified by the respective vector offsets (king – queen ≈ man – woman). In our experiments, we investigate how well these representations can capture information about source and target domains and their interaction in a"
N16-1020,D15-1293,1,0.417472,"ependently, as described above, and then combine them into multi-modal ones. Previous work in multi-modal semantics (Bruni et al., 163 2014) investigated different ways of combining, or fusing, linguistic and perceptual cues. When calculating similarity, for instance, one can either combine the representations first and subsequently compute similarity scores; or compute similarity scores independently per modality and afterwards combine the scores. In contrast with joint learning (which has also been called early fusion), these two possibilities represent middle and late fusion, respectively (Kiela and Clark, 2015). We experiment with middle and late fusion strategies. In middle fusion, we L-2 normalise and concatenate the vectors for linguistic and visual representations and then compute a metaphoricity score for a phrase based on this joint representation. In late fusion, we first compute the metaphoricity scores based on linguistic and visual representations independently, and then combine the metaphoricity scores by taking their average. 3.4 Measuring metaphoricity We investigate a set of arithmetic operations on the linguistic, visual and multimodal embedding vectors to determine whether the two wo"
N16-1020,P15-2020,1,0.678425,"Missing"
N16-1020,D15-1015,1,0.816301,"Missing"
N16-1020,W07-0103,0,0.0213517,"nery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determi"
N16-1020,W14-1618,0,0.0318807,"rticularly relevant to modelling metaphor, where imagery is ported across domains. In this paper, we present the first metaphor identification method integrating meaning representations learned from linguistic and visual data. We construct our representations using a skip-gram model of Mikolov et al. (2013a) trained on textual data to obtain linguistic embeddings and a deep convolutional neural network (Kiela and Bottou, 2014) trained on image data to obtain visual embeddings. Linguistic word embeddings have been previously successfully used to answer analogy questions (Mikolov et al., 2013b; Levy and Goldberg, 2014). These works have shown that such representations capture the nuances of word meaning needed to recognise relational similarity (e.g. between pairs “king : queen” and “man : woman”), quantified by the respective vector offsets (king – queen ≈ man – woman). In our experiments, we investigate how well these representations can capture information about source and target domains and their interaction in a metaphor. We then enrich these representations with visual information. We first acquire linguistic and visual embeddings for individual words and then extend the methods to learn embeddings fo"
N16-1020,Q13-1031,0,0.0263483,"break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determine these properties (such as the MRC concreteness database (Wilson, 1988)). To the best of our knowledge"
N16-1020,P14-5010,0,0.00703906,"w and p(c|w; θ) is a softmax function: evc ·vw , vc0 ·vw c0 ∈C e p(c|w; θ) = P (2) where vc and vw are vector representations of c and w. The parameters we need to set are thus vci and vwi for all words in our word vocabulary V and context vocabulary C, and the set of dimensions i ∈ 1, . . . , d. Given a set D of word-context pairs, embeddings are learned by optimizing the following objective: X arg max log p(c|w) = θ X (w,c)∈D (w,c)∈D (log evc ·vw − log X evc0 ·vw ) (3) c0 ∈C We used a recent dump of Wikipedia1 as our corpus. The text was lemmatized, tagged, and parsed with Stanford CoreNLP (Manning et al., 2014). Words that appeared less than 100 times in their lemmatized form were ignored. The 100-dimensional word and phrase embeddings were learned in two stages: in a first pass, we obtained word-level embeddings (e.g. for white and rabbit) using the standard skip-gram with negative sampling of Eq. (3); we then obtained phrase embeddings (e.g. for white rabbit) through a second pass over the same corpus. In the second pass, the vectors vc and vc0 of Eq. (3) were set to their values from the first pass, and kept fixed. Verb-noun phrases were extracted by finding nsubj and dobj arcs with V B head and"
N16-1020,J04-1002,0,0.0216935,"Strzalkowski et al. (2013) acquired a set of topic chains by linking semantically related words in a given text. They then looked for vocabulary outside the topic chain and yet connected to topic chain words via syntactic dependencies and exhibiting high imageability. Heintz et al. (2013) used LDA topic modelling to identify sets of source and target domain vocabulary. In their system, the acquired topics represented source and target domains, and sentences containing vocabulary from both were tagged as metaphorical. Other approaches addressed automatic identification of conceptual metaphor. Mason (2004) automatically acquired domain-specific selectional preferences of verbs, and then, by mapping their common nominal arguments in different domains, arrived at the corresponding metaphorical mappings. For example, the verb pour has a strong preference for liquids in the LAB domain and for money in the FINANCE domain, suggesting the mapping MONEY is LIQUID. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. For example, the feature vector for politics would contain GAME or MECH"
N16-1020,N13-1090,0,0.20218,"been shown successful in tasks such as modeling semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013) and bilingual lexicon induction (Kiela et al., 2015b). Using visual information is particularly relevant to modelling metaphor, where imagery is ported across domains. In this paper, we present the first metaphor identification method integrating meaning representations learned from linguistic and visual data. We construct our representations using a skip-gram model of Mikolov et al. (2013a) trained on textual data to obtain linguistic embeddings and a deep convolutional neural network (Kiela and Bottou, 2014) trained on image data to obtain visual embeddings. Linguistic word embeddings have been previously successfully used to answer analogy questions (Mikolov et al., 2013b; Levy and Goldberg, 2014). These works have shown that such representations capture the nuances of word meaning needed to recognise relational similarity (e.g. between pairs “king : queen” and “man : woman”), quantified by the respective vector offsets (king – queen ≈ man – woman). In our experiments, we in"
N16-1020,W13-0904,0,0.0949705,"man, 2006). Metaphors arise due to systematic associations between distinct, and seemingly unrelated, concepts. For instance, when we talk about “the turning wheels of a political regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features"
N16-1020,C14-1165,0,0.0451843,"l regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used m"
N16-1020,D13-1115,0,0.02017,"Missing"
N16-1020,N13-1118,1,0.957214,"For instance, when we talk about “the turning wheels of a political regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2"
N16-1020,C10-1113,1,0.601483,"tem, the acquired topics represented source and target domains, and sentences containing vocabulary from both were tagged as metaphorical. Other approaches addressed automatic identification of conceptual metaphor. Mason (2004) automatically acquired domain-specific selectional preferences of verbs, and then, by mapping their common nominal arguments in different domains, arrived at the corresponding metaphorical mappings. For example, the verb pour has a strong preference for liquids in the LAB domain and for money in the FINANCE domain, suggesting the mapping MONEY is LIQUID. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. For example, the feature vector for politics would contain GAME or MECH ANISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain (or sets of source domains). Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. S"
N16-1020,C12-2109,1,0.550266,"Missing"
N16-1020,N10-1147,1,0.267593,"educing the risk of human annotation noise and having a wider coverage and applicability. Since the method relies on automatically acquired lexical knowledge, in the form of linguistic and visual embeddings, and is otherwise resource-independent, it can be applied to unrestricted text in any domain and easily tailored to other metaphor processing tasks. In the future, it would be interesting to apply multimodal word and phrase embeddings to automatically interpret metaphorical language, e.g. by deriving literal or conventional paraphrases for metaphorical expressions (similarly to the task of Shutova (2010)). Multimodal embeddings are also likely to provide useful information for the models of metaphor translation, as they have already proved successful in bilingual lexicon induction more generally (Kiela et al., 2015b). Finally, it would be interesting to further investigate compositional properties of metaphorical language using multimodal phrase embeddings and to apply the embeddings to automatically generalise metaphorical associations between distinct concepts or domains. Acknowledgment We are grateful to the NAACL reviewers for their helpful feedback. Ekaterina Shutova’s research is suppor"
N16-1020,D12-1130,0,0.011038,"yed information learned from both linguistic and visual data. Ample re160 Proceedings of NAACL-HLT 2016, pages 160–170, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics search in cognitive science suggests that human meaning representations are not merely a product of our linguistic exposure, but are also grounded in our perceptual system and sensori-motor experience (Barsalou, 2008; Louwerse, 2011). Semantic models integrating information from multiple modalities have been shown successful in tasks such as modeling semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013) and bilingual lexicon induction (Kiela et al., 2015b). Using visual information is particularly relevant to modelling metaphor, where imagery is ported across domains. In this paper, we present the first metaphor identification method integrating meaning representations learned from linguistic and visual data. We construct our representations using a skip-gram model of Mikolov et al. (2013a) trained on textual data to obtain linguistic embeddings and a deep convolutional neural"
N16-1020,W13-0909,0,0.461432,"Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determine these properties (such as the MRC concreteness database (Wilson, 1988)). To the best of our knowledge, there has not yet been a metaphor processing method that employed information learned from both linguistic and visual data. Ample re160 Proceedings of NAACL-HLT 2016, pages 160–170, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics search in cognitive science suggests that human meaning representations are not merely a product of our linguis"
N16-1020,W13-0906,0,0.137075,"arise due to systematic associations between distinct, and seemingly unrelated, concepts. For instance, when we talk about “the turning wheels of a political regime”, “rebuilding the campaign machinery” or “mending foreign policy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identific"
N16-1020,P14-1024,0,0.185534,"tor space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determine these properties (such as the MRC concreteness database (Wilson, 1988)). To the best of our knowledge, there has not yet been a metaphor processing method that employed information learned from both linguistic and visual data. Ample re160 Proceedings of NAACL-HLT 2016, pages 160–170, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics search in cognitive science suggests that human meaning representations are not merely a product of our linguistic exposure, but are al"
N16-1020,D11-1063,0,0.0869636,"et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determine these properties (such as the MRC concreteness database (Wilson, 1988)). To the best of our knowledge, there has not yet been a metaphor processing method that employed information learned from both linguistic and visual data. Ample re160 Proceedings of NAACL-HLT 2016, pages 160–170, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics search in cognitive science suggests that h"
N16-1020,C08-1119,0,0.0251462,", they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determine these properties (such as the MRC concreteness database (Wilson, 1988)). To the best"
N16-1020,W13-0905,0,0.0391912,"cy”, we view politics and political systems in terms of mechanisms, they can function, break, be mended Metaphor is pervasive in our communication, which makes it important for NLP applications dealing with real-world text. A number of approaches to metaphor processing have thus been proposed, using supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2013; Hovy et al., 2013; Dunn, 2013a), clustering (Shutova et al., 2010; Shutova and Sun, 2013), vector space models (Shutova et al., 2012; Mohler et al., 2014), lexical resources (Krishnakumaran and Zhu, 2007; Wilks et al., 2013) and web search with lexicosyntactic patterns (Veale and Hao, 2008; Li et al., 2013; Bollegala and Shutova, 2013). So far, these and other metaphor processing works relied on textual data to construct their models. Yet, several experiments indicated that perceptual properties of concepts, such as concreteness and imageability, are important features for metaphor identification (Turney et al., 2011; Neuman et al., 2013; Gandy et al., 2013; Strzalkowski et al., 2013; Tsvetkov et al., 2014). However, all of these methods used manually-annotated linguistic resources to determine these properties ("
N16-1020,J15-4002,1,\N,Missing
N16-1020,S16-2003,1,\N,Missing
N16-1020,W15-1402,0,\N,Missing
N16-1071,R11-1055,0,0.192846,"ataset apply to it (e.g. is animal, has 4 legs) and which don’t (e.g. a bird, made of metal). 3.1 Building modality-specific representations We obtain distributed representations of concepts in the property-norm semantic space (henceforth PROPNORM ) by simply treating MCRAE as a bag of 2526 properties, with the production frequencies representing the “co-occurrence counts” (Table 3). Our visual space (henceforth VISUAL) consists of visual representations for all the 541 concepts in MCRAE , built as follows. First, we retrieve 10 images per concept from Google Images,2 following previous work (Bergsma and Goebel, 2011; Kiela and Bottou, 2014). The image representations are then obtained by extracting the pre-softmax layer 2 www.google.com/imghp (images were retrieved on 10 April 2015) from a forward pass in a convolutional neural network that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). We aggregate images associated with a concept into an overall visually grounded representation by taking the mean of the individual image representations. The dimensionality of the visual vectors is 4096. We also build three linguistic spaces (DISTRIB, SVD and EMBED ), along the lines"
N16-1071,P12-1015,0,0.359172,"Missing"
N16-1071,W15-0107,1,0.822125,"al Linguistics one where we take advantage of the abundance of freely available textual corpora. There are two strands of research that attempt to automatically obtain property norm data for new concepts. One approach is to automatically generate feature norms from text corpora by mining text data for a set of generalised property patterns (Kelly et al., 2014; Baroni et al., 2010; Barbu, 2008). Another avenue of research is inspired by Lazaridou et al. (2014) and Mikolov et al. (2013b) and tries to increase the coverage of feature norms through cross-modal mapping from linguistic information (Fagarasan et al., 2015). Here, we follow recent trends in multi-modal semantics and explore automatic property norm extraction from visual, rather than textual, data. Obtaining property norms from visual information makes intuitive sense: information contained in the property norm datasets can often be attributed to extra-linguistic modalities—a large proportion of relevant properties are visual, auditory or tactile, rather than linguistic (e.g. is round, makes noise, is yellow). We show that such conceptual properties can be more accurately predicted through cross-modal mappings from raw perceptual information (i.e"
N16-1071,D14-1032,0,0.0436018,"bridge ltf24,douwe.kiela,stephen.clark@cl.cam.ac.uk Abstract et al., 2013). After having been used to test models of conceptual representation in cognitive science for decades (Randall et al., 2004; Cree et al., 2006), these datasets have proved to be useful in a wide range of semantic NLP tasks as well, including text simplification for limited vocabulary groups. More recently, property norms have been used as a proxy for perceptual information in a number of studies on multi-modal semantics (Andrews et al., 2009; Riordan and Jones, 2011; Silberer and Lapata, 2012; Roller and Im Walde, 2013; Hill and Korhonen, 2014). Such models aim to addres the grounding problem (Harnad, 1990) that distributional semantic models of language (Turney and Pantel, 2010; Clark, 2015) suffer from. Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts. Recent work has focused on text as the main source of information for automatic property extraction. In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm and visual spaces. We also investigate the importance"
N16-1071,D14-1005,1,0.876581,"s animal, has 4 legs) and which don’t (e.g. a bird, made of metal). 3.1 Building modality-specific representations We obtain distributed representations of concepts in the property-norm semantic space (henceforth PROPNORM ) by simply treating MCRAE as a bag of 2526 properties, with the production frequencies representing the “co-occurrence counts” (Table 3). Our visual space (henceforth VISUAL) consists of visual representations for all the 541 concepts in MCRAE , built as follows. First, we retrieve 10 images per concept from Google Images,2 following previous work (Bergsma and Goebel, 2011; Kiela and Bottou, 2014). The image representations are then obtained by extracting the pre-softmax layer 2 www.google.com/imghp (images were retrieved on 10 April 2015) from a forward pass in a convolutional neural network that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). We aggregate images associated with a concept into an overall visually grounded representation by taking the mean of the individual image representations. The dimensionality of the visual vectors is 4096. We also build three linguistic spaces (DISTRIB, SVD and EMBED ), along the lines of Fagarasan et al. (201"
N16-1071,P15-2038,1,0.847704,"or representations from the log-linear skipgram model of Mikolov et al. (2013a). We used the publicly-available3 representations that were trained on part of the Google News dataset (about 100 billion words). We will also employ three multi-modal semantic spaces (VISUAL + DISTRIB , VISUAL + SVD , VI SUAL + EMBED ), in which the visual ( VISUAL) and respective linguistic representations (DISTRIB , SVD , EMBED ) are combined into a multi-modal representation by concatenating their respective L2normalized representations. 3.2 Method and evaluation Following previous work (Fagarasan et al., 2015; Kiela et al., 2015) we use partial least squares regression (PLSR)4 to learn cross-modal maps to the property-norm space (PROPNORM) from the visual (VISUAL), linguistic (DISTRIB , SVD , EMBED) and multi-modal semantic spaces (VISUAL + DISTRIB , VISUAL + SVD , VISUAL + EMBED ). At training time, we take advantage of the fact that we possess both visual/linguistic/multi-modal and property norm information for the concepts in MCRAE. Let’s consider the VISUAL→PROPNORM setting as an example. We use this cross-modal vocabulary to learn a mapping function between VISUAL and PROP 3 https://code.google.com/p/word2vec/ Th"
N16-1071,P14-1132,0,0.079927,"Missing"
N16-1071,D13-1115,0,0.158678,"Missing"
N16-1071,D12-1130,0,0.0931638,"nd Stephen Clark Computer Laboratory University of Cambridge ltf24,douwe.kiela,stephen.clark@cl.cam.ac.uk Abstract et al., 2013). After having been used to test models of conceptual representation in cognitive science for decades (Randall et al., 2004; Cree et al., 2006), these datasets have proved to be useful in a wide range of semantic NLP tasks as well, including text simplification for limited vocabulary groups. More recently, property norms have been used as a proxy for perceptual information in a number of studies on multi-modal semantics (Andrews et al., 2009; Riordan and Jones, 2011; Silberer and Lapata, 2012; Roller and Im Walde, 2013; Hill and Korhonen, 2014). Such models aim to addres the grounding problem (Harnad, 1990) that distributional semantic models of language (Turney and Pantel, 2010; Clark, 2015) suffer from. Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts. Recent work has focused on text as the main source of information for automatic property extraction. In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm"
N16-1071,P13-1056,0,0.0389244,"(those marked with * in Table 6) are highly plausible properties for the given concepts: tastes sweet for BANANA or has legs for TORTOISE. This also means that the model is being unfairly penalised. In order to obtain a complete version of MCRAE, every possible (CONCEPT, property) pair would have to be checked for validity and annotated accordingly depending on whether property is a valid attribute of CONCEPT. 3.5 Importance of complete data We were interested in measuring the impact that a complete dataset of features would have on the performance of the cross-modal zero-shot learning task. Silberer et al. (2013) conducted a study using a subset of the concepts and properties in MCRAE, whereby every property was annotated if it was a plausible attribute of the concept. The published dataset (SILBERER) consists of visual attribute annotations for 512 concepts (that also occur in MCRAE ) and 693 visual properties. The an585 Dataset #concs #props #(conc,prop) pairs SILBERER 512 693 7743 SILB - VIS 512 283 5335 M - VIS 512 283 2140 MCRAE 541 2526 7259 Table 7: Comparison of various datasets, according to the number of concepts and properties covered, as well as the pairs of (CONCEPT, property) contained T"
N16-1071,Q14-1017,0,0.044416,"ED is yellow 29 7 0 a fruit 25 24 0 is edible 13 0 0 is soft 12 0 13 Table 3: Subspace of PROPNORM. Important to note that MCRAE is not complete, meaning that even though some properties are true of a given concept, they have not been produced by the human participants (e.g. the is edible property for APPLE holds the value 0). text-based distributional vectors) (Lazaridou et al., 2014). This represents an extension of the object recognition problem, since we want to associate images with semantic representations of their depicted objects, rather than just with their label (Frome et al., 2013; Socher et al., 2014). The benefit of this approach lies in its generalisation power: once a function between the two semantic spaces is learnt, it can be used to see how an unseen concept relates to other concepts, just by looking at an image of that concept. This is referred to as the zero-shot learning task (Palatucci et al., 2009; Lazaridou et al., 2014). Our task is to increase the coverage of the property norm datasets, meaning that we want to predict properties for new (unseen) concepts. For example, the concept WOLF is not included in MCRAE, but it would be desirable to know which of the properties in the"
N18-1038,D15-1075,0,0.0272993,"s not work as well on this task as the image-only case, probably because interference from the language signal makes the problem harder to optimize. The results indicate that the system has learned to predict image features from captions, and captions from images, at a level exceeding or close to the state-of-the-art on this task. Recent years have seen an increased interest in entailment classification as an appropriate evaluation of sentence representation quality. We evaluate representations on two well-known entailment, or natural language inference, datasets: the largescale SNLI dataset (Bowman et al., 2015) and the SICK dataset (Marelli et al., 2014). 4.2 Implementational details We implement a simple logistic regression on top of the sentence representation. In the cases of SNLI and SICK, as is the standard for these datasets, the representations for the individual sentences u and v are combined by using hu, v, u ∗ v, |u − v|i as the input features. We tune the seed and an l 2 penalty on the validation sets for each, and train using Adam (Kingma and Ba, 2015), with a learning rate of 0.001 and a batch size of 32. 5 5.1 Transfer task performance Having established that we can learn high-quality"
N18-1038,Q17-1002,1,0.856036,"uces biases: aside from the inherent dataset bias in COCO itself, the system will only have coverage for concrete concepts. COCO is also a much smaller dataset than e.g. the Toronto Books Corpus often used in purely text-based methods (Kiros et al., 2015). As such, grounded representations are potentially less “universal” than text-based alternatives, which also cover abstract concepts. There is evidence that meaning is dually coded in the human brain: while abstract concepts are processed in linguistic areas, concrete concepts are processed in both linguistic and visual areas (Paivio, 1990). Anderson et al. (2017) recently corroborated this hypothesis using semantic representations and fMRI studies. In our case, we want to be able to accommodate concrete sentence meanings, for which our vision-centric system is likely to help; as well as abstract sentence meanings, where trying to “imagine” what “democracy is a political system” might look like will probably only introduce noise. Hence, we optionally complement our systems’ representations with more abstract universal sentence representations trained on language-only data (specifically, the Toronto Books Corpus). Although it would be interesting to exa"
N18-1038,L18-1269,1,0.825987,"final layer from a ResNet-101 (He et al., 2016) trained on ImageNet (ILSVRC 2015). 4.1 Transfer tasks We are specifically interested in how well (grounded) universal sentence representations transfer to different tasks. To evaluate this, we perform experiments for a variety of tasks. In all cases, we compare against layer-normalized SkipThought vectors, a well-known high-performing sentence encoding method (Ba et al., 2016). To ensure that we use the exact same evaluations, with identical hyperparameters and settings, we evaluate all systems with the same evaluation pipeline, namely SentEval (Conneau and Kiela, 2018)2. Following previous work in the field, the idea is to take universal sentence representations and to learn a simple classifier on top for each of the transfer tasks—the higher the quality of the sentence representation, the better the performance on these transfer tasks should be. 4.1.1 Semantic classification We evaluate on the following well-known and widely used evaluations: movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), paraphrase identificati"
N18-1038,D17-1070,1,0.829994,"strategy; and explicitly focus on grounded universal sentence representations. Related work Sentence representations Although there appears to be a consensus with regard to the methodology for learning word representations, this is much more of an open problem for sentence representations. Recent work has ranged from trying to learn to compose word embeddings (Le and Mikolov, 2014; Pham et al., 2015; Wieting et al., 2016; Arora et al., 2017), to neural architectures for predicting the previous and next sentences (Kiros et al., 2015) or learning representations via largescale supervised tasks (Conneau et al., 2017). In particular, SkipThought (Kiros et al., 2015) led to an increased interest in learning sentence representations. Hill et al. (2016a) compare a wide selection of unsupervised and supervised methods, including a basic caption prediction system that is similar to ours. That study finds that “different learning methods are preferable for different intended applications”, i.e., that the matter of optimal universal sentence representations is as of yet far from decided. InferSent (Conneau et al., 2017) recently showed that supervised sentence representations can be of very high quality. Here, we"
N18-1038,C04-1051,0,0.0757105,"g previous work in the field, the idea is to take universal sentence representations and to learn a simple classifier on top for each of the transfer tasks—the higher the quality of the sentence representation, the better the performance on these transfer tasks should be. 4.1.1 Semantic classification We evaluate on the following well-known and widely used evaluations: movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), paraphrase identification (MSRP) (Dolan et al., 2004) and sentiment classification (SST, binary version) (Socher et al., 2013). Accuracy is measured in all cases, except for MRPC, which measures accuracy and the F1score. Implementation details We use 300-dimensional GloVe (Pennington et al., 2014) embeddings, trained on WebCrawl, for the initial word representations and optimize using Adam (Kingma and Ba, 2015). We use ELU (Clevert et al., 2016) for the non-linearity in projection layers, set dropout to 0.5 and use a dimensionality 2See https://github.com/facebookresearch/SentEval. The aim of SentEval is to encompass a comprehensive set of bench"
N18-1038,I17-1014,0,0.0426053,"s on studying how these grounded text representations transfer to NLP tasks. Moreover, there has been a lot of work in recent years on the task of image caption generation (Bernardi et al., 2016; Vinyals et al., 2015; Mao et al., 2015; Fang et al., 2015). Here, we do the opposite: we predict the correct image (features) from the caption, rather than the caption from the image (features). Similar ideas were recently successfully applied to multi-modal machine translation 409 Using a standard LSTM (Hochreiter and Schmidhuber, 1997), the hidden state at time t, denoted ht ∈ Rm , is computed via (Elliott and Kádár, 2017; Gella et al., 2017; Lee et al., 2017). Recently, Das et al. (2017) trained dialogue agents to communicate about images, trying to predict image features as well. 3 ht+1, ct+1 = LSTM(xt , ht , ct |Θ) Approach where ct denotes the cell state of the LSTM and where Θ denotes its parameters. To exploit contextual information in both input directions, we process input sentences using a bidirectional LSTM, that reads an input sequence in both normal and reverse order. In particular, for an input sequence x of length T, we compute the hidden state at time t, ht ∈ R2m via In the following, let D = be"
N18-1038,D17-1303,0,0.026566,"rounded text representations transfer to NLP tasks. Moreover, there has been a lot of work in recent years on the task of image caption generation (Bernardi et al., 2016; Vinyals et al., 2015; Mao et al., 2015; Fang et al., 2015). Here, we do the opposite: we predict the correct image (features) from the caption, rather than the caption from the image (features). Similar ideas were recently successfully applied to multi-modal machine translation 409 Using a standard LSTM (Hochreiter and Schmidhuber, 1997), the hidden state at time t, denoted ht ∈ Rm , is computed via (Elliott and Kádár, 2017; Gella et al., 2017; Lee et al., 2017). Recently, Das et al. (2017) trained dialogue agents to communicate about images, trying to predict image features as well. 3 ht+1, ct+1 = LSTM(xt , ht , ct |Θ) Approach where ct denotes the cell state of the LSTM and where Θ denotes its parameters. To exploit contextual information in both input directions, we process input sentences using a bidirectional LSTM, that reads an input sequence in both normal and reverse order. In particular, for an input sequence x of length T, we compute the hidden state at time t, ht ∈ R2m via In the following, let D = be a dataset where eac"
N18-1038,D14-1005,1,0.856818,"s with more abstract universal sentence representations trained on language-only data (specifically, the Toronto Books Corpus). Although it would be interesting to examine multitask scenarios where these representations are jointly learned, we leave this for future work. Here, instead, we combine grounded and language-only representations using simple concatenation, i.e., rgs = rgr ounded ||rling−only . Concatenation has been proven to be a strong and straightforward mid-level multi-modal fusion method, previously explored in multi-modal semantics for word representations (Bruni et al., 2014; Kiela and Bottou, 2014). We call the combined system GroundSent (GS), and distinguish between sentences perceptually grounded in images (GroundSent-Img), weakly grounded in captions (GroundSent-Cap) or grounded in both (GroundSent-Both). 3.6 of 1024 for the LSTM. The network was initialized with orthogonal matrices for the recurrent layers (Saxe et al., 2014) and He initialization (He et al., 2015) for all other layers. The learning rate and margin were tuned on the validation set using grid search. 4 Data, evaluation and comparison We use the same COCO splits as Karpathy and Fei-Fei (2015) for training (113,287 ima"
N18-1038,N16-1162,0,0.337521,"models over non-grounded ones. In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings. 1 Introduction Following the word embedding upheaval of the past few years, one of NLP’s next big challenges has become the hunt for universal sentence representations: generic representations of sentence meaning that can be “plugged into” any kind of system or pipeline. Examples include Paragraph2Vec (Le and Mikolov, 2014), C-Phrase (Pham et al., 2015), SkipThought (Kiros et al., 2015) and FastSent (Hill et al., 2016a). These representations tend to be learned from large corpora in an unsupervised setting, much like word embeddings, and effectively “transferred” to the task at hand. Purely text-based semantic models, which represent word meaning as a distribution over other words (Harris, 1954; Turney and Pantel, 2010; Clark, 2015), suffer from the grounding problem (Harnad, 1990). It has been shown that grounding leads to improved performance on a variety of word-level tasks (Baroni, 2016; Kiela, 2017). Unsupervised sentence representation models are often doubly exposed to the grounding problem, especia"
N18-1038,W13-3512,0,0.061949,"tain a projection layer that maps the GloVe word embeddings that they receive as inputs to a different embedding space. There has been a lot of interest in grounded word representations in recent years, so it is interesting to examine what kind of word representations our models learn. We omit Cap2Cap for reasons of space (it performs similarly to Cap2Both). As shown in Table 5, the grounded word projections that our network learns yield higher-quality word embeddings on four standard lexical semantic similarity benchmarks: MEN (Bruni et al., 2014), SimLex999 (Hill et al., 2016b), Rare Words (Luong et al., 2013) and WordSim-353 (Finkelstein et al., 2001). Discussion There are a few other important questions to investigate. The average abstractness or concreteness of the evaluation datasets may have a large impact on performance. In addition, word embeddings from the learned projection from GloVe input embeddings, which now provides a generic wordembedding grounding method even for words that are not present in the image-caption training data, can be examined. 6.1 Grounded word embeddings Concreteness As we have seen, performance across datasets and models can vary substantially. A dataset’s concreten"
N18-1038,D13-1170,0,0.0152311,"Missing"
N18-1038,marelli-etal-2014-sick,0,0.0265926,"e-only case, probably because interference from the language signal makes the problem harder to optimize. The results indicate that the system has learned to predict image features from captions, and captions from images, at a level exceeding or close to the state-of-the-art on this task. Recent years have seen an increased interest in entailment classification as an appropriate evaluation of sentence representation quality. We evaluate representations on two well-known entailment, or natural language inference, datasets: the largescale SNLI dataset (Bowman et al., 2015) and the SICK dataset (Marelli et al., 2014). 4.2 Implementational details We implement a simple logistic regression on top of the sentence representation. In the cases of SNLI and SICK, as is the standard for these datasets, the representations for the individual sentences u and v are combined by using hu, v, u ∗ v, |u − v|i as the input features. We tune the seed and an l 2 penalty on the validation sets for each, and train using Adam (Kingma and Ba, 2015), with a learning rate of 0.001 and a batch size of 32. 5 5.1 Transfer task performance Having established that we can learn high-quality grounded sentence encodings, the core questi"
N18-1038,P04-1035,0,0.00714083,"e all systems with the same evaluation pipeline, namely SentEval (Conneau and Kiela, 2018)2. Following previous work in the field, the idea is to take universal sentence representations and to learn a simple classifier on top for each of the transfer tasks—the higher the quality of the sentence representation, the better the performance on these transfer tasks should be. 4.1.1 Semantic classification We evaluate on the following well-known and widely used evaluations: movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), paraphrase identification (MSRP) (Dolan et al., 2004) and sentiment classification (SST, binary version) (Socher et al., 2013). Accuracy is measured in all cases, except for MRPC, which measures accuracy and the F1score. Implementation details We use 300-dimensional GloVe (Pennington et al., 2014) embeddings, trained on WebCrawl, for the initial word representations and optimize using Adam (Kingma and Ba, 2015). We use ELU (Clevert et al., 2016) for the non-linearity in projection layers, set dropout to 0.5 and use a dimensionality 2See https://g"
N18-1038,P05-1015,0,0.173425,"re that we use the exact same evaluations, with identical hyperparameters and settings, we evaluate all systems with the same evaluation pipeline, namely SentEval (Conneau and Kiela, 2018)2. Following previous work in the field, the idea is to take universal sentence representations and to learn a simple classifier on top for each of the transfer tasks—the higher the quality of the sentence representation, the better the performance on these transfer tasks should be. 4.1.1 Semantic classification We evaluate on the following well-known and widely used evaluations: movie review sentiment (MR) (Pang and Lee, 2005), product reviews (CR) (Hu and Liu, 2004), subjectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), paraphrase identification (MSRP) (Dolan et al., 2004) and sentiment classification (SST, binary version) (Socher et al., 2013). Accuracy is measured in all cases, except for MRPC, which measures accuracy and the F1score. Implementation details We use 300-dimensional GloVe (Pennington et al., 2014) embeddings, trained on WebCrawl, for the initial word representations and optimize using Adam (Kingma and Ba, 2015). We use ELU (Clevert et al., 2016) for"
N18-1038,D14-1162,0,0.0845175,"Missing"
N18-1038,P15-1094,0,0.140952,"tion quality benchmarks, showing improved performance for grounded models over non-grounded ones. In addition, we thoroughly analyze the extent to which grounding contributes to improved performance, and show that the system also learns improved word embeddings. 1 Introduction Following the word embedding upheaval of the past few years, one of NLP’s next big challenges has become the hunt for universal sentence representations: generic representations of sentence meaning that can be “plugged into” any kind of system or pipeline. Examples include Paragraph2Vec (Le and Mikolov, 2014), C-Phrase (Pham et al., 2015), SkipThought (Kiros et al., 2015) and FastSent (Hill et al., 2016a). These representations tend to be learned from large corpora in an unsupervised setting, much like word embeddings, and effectively “transferred” to the task at hand. Purely text-based semantic models, which represent word meaning as a distribution over other words (Harris, 1954; Turney and Pantel, 2010; Clark, 2015), suffer from the grounding problem (Harnad, 1990). It has been shown that grounding leads to improved performance on a variety of word-level tasks (Baroni, 2016; Kiela, 2017). Unsupervised sentence representation"
N19-1170,D18-1431,0,0.127131,"ributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Section 5) to control attributes at the dialogue level rather than the utterance level. In this work, we require a control method that is both general-purpose (one technique to simultaneously control many attributes) and easily tunable (the control setting is adjustable after training). Given these constraints, we study two control methods: c"
N19-1170,W17-5526,0,0.0372956,"Missing"
N19-1170,W18-2706,0,0.190809,"e automatic metrics designed to capture various conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a ne"
N19-1170,W17-4912,0,0.0572139,"s designed to capture various conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Sect"
N19-1170,P17-4008,0,0.358287,"us conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Section 5) to control attributes"
N19-1170,W18-1505,0,0.147489,"conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Section 5) to control attributes at the dialogue level rather than the utterance level. In"
N19-1170,D14-1162,0,0.0839219,"gment via the question “How much did you enjoy talking to this user?” on a scale of 1–4. 4 Baseline model Our baseline model is a 2-layer LSTM sequenceto-sequence model with attention. On any dialogue turn, the input x to the encoder is the entire dialogue history (separated using unique speakeridentifying tokens), with the model’s own persona prepended. Conditioned on this input sequence x, the decoder generates a response y. Except when stated otherwise, all our models decode using beam search with beam size 20. We initialized the word embedding matrix with 300-dimensional GloVe embeddings (Pennington et al., 2014). Using the ParlAI framework (Miller et al., 2017), we pretrained the model on a dataset of 2.5 million Twitter message-response pairs,1 then fine-tuned it on PersonaChat. On the PersonaChat validation set, the baseline model has a perplexity of 26.83 and F1 of 17.02, which would have placed us 4th out of 26 models in the ConvAI2 competition (Dinan et al., 2019). We attempt to improve over this baseline using control. 5 Controllable text generation methods Suppose we have a sequence-to-sequence model which gives P (y|x) = Πt P (yt |x, y1 , . . . , yt−1 ), the conditional probability of a respo"
N19-1170,E17-1042,0,0.0439964,"Missing"
N19-1170,P18-1102,0,0.242313,"ibutes, we consider two simple but general algorithms: conditional training, in which the neural model is conditioned on additional control features, and weighted decoding, in which control features are added to the decoding scoring function at test time only. One major result of our findings is that existing work has ignored the importance of conversational flow, as standard models (i) repeat or contradict previous statements, (ii) fail to balance specificity with genericness, and (iii) fail to balance asking questions with other dialogue acts. Conducting experiments on the PersonaChat task (Zhang et al., 2018b), we obtain significantly higher engagingness scores than the baseline by optimizing control of repetition, specificity and question-asking over multiple turns. Using these findings, our best model matches the performance of the winning entry in the recent NeurIPS ConvAI2 competition (Dinan et al., 2019), which was trained on much 1702 Proceedings of NAACL-HLT 2019, pages 1702–1723 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics more data but had no control (see Section 8.1). Our code, pretrained models, and full chatlogs, are available at http"
N19-1170,P18-1205,1,0.917759,"ibutes, we consider two simple but general algorithms: conditional training, in which the neural model is conditioned on additional control features, and weighted decoding, in which control features are added to the decoding scoring function at test time only. One major result of our findings is that existing work has ignored the importance of conversational flow, as standard models (i) repeat or contradict previous statements, (ii) fail to balance specificity with genericness, and (iii) fail to balance asking questions with other dialogue acts. Conducting experiments on the PersonaChat task (Zhang et al., 2018b), we obtain significantly higher engagingness scores than the baseline by optimizing control of repetition, specificity and question-asking over multiple turns. Using these findings, our best model matches the performance of the winning entry in the recent NeurIPS ConvAI2 competition (Dinan et al., 2019), which was trained on much 1702 Proceedings of NAACL-HLT 2019, pages 1702–1723 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics more data but had no control (see Section 8.1). Our code, pretrained models, and full chatlogs, are available at http"
P14-2135,N09-1003,0,0.0572959,"Missing"
P14-2135,P12-1092,0,0.0431552,"e average pairwise cosine distance between all the image representations {w~1 . . . w~n } in the set of images for that concept: d(w) = X 1 w ~i · w~j 1− 2n(n − 1) i<j≤n |w ~i ||w~j | (1) We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for concept"
P14-2135,Y08-1023,0,0.0218403,"vements on other tasks in semantic processing and representation. Table 1: Concepts with highest and lowest image dispersion scores in our evaluation set, and concreteness ratings from the USF dataset. the very abstract or very concrete concepts. As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract. It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, S´anchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of"
P14-2135,C94-1103,0,0.022202,"et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 3 Improving Multi-Modal Representations We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included. If not, in accordance with the Dual Coding Theory of human concept pro"
P14-2135,R11-1055,0,0.208132,"ments focus on multi-modal models that extract their perceptual input automatically from images. Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation. They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets. We use Google Images as our image source, and extract the first n image results for each concept word. It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011). Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets. 2.1 Figure 1: Example images for a concrete (elephant – little diversity, low dispersion) and an abstract concept (happiness – greater diversity, high dispersion). Figure 2: Computation of PHOW descriptors using dense SIFT for levels l = 0 to l = 2 and the corresponding histogram representations (Bosch et al., 2007). Image Dispersion-Based Filtering Fol"
P14-2135,P12-1015,0,0.642782,"be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, c Baltimore, Maryland, USA, June"
P14-2135,D13-1115,0,0.470058,"Missing"
P14-2135,N10-1011,0,0.377871,"on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, c Baltimore"
P14-2135,D12-1130,0,0.48871,"lti-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Associatio"
P14-2135,D11-1063,0,0.245846,"s spectrum), we observed a high correlation between abstractness and dispersion (Spearman ρ = 0.61, p < 0.001). On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail. 4.1 Quantifying Concreteness To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2. By classifying con838 Concept shirt bed knife dress car ego nonsense memory potential know Image Dispersion .488 .495 .560 .578 .580 1.000 .999 .999 .997 .996 Conc. (USF) 6.05 5.91 6.08 6.59 6.35 1.93 1.90 1.78 1.90 2.70 racy of 7"
P14-2135,W13-2609,1,\N,Missing
P15-2020,W11-0112,0,0.0323545,"for lexical entailment detection by examining a concept’s generality. We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Gan"
P15-2020,P05-1014,0,0.905176,"Missing"
P15-2020,P13-2078,0,0.234332,"te et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypern"
P15-2020,W98-0718,0,0.195269,"Missing"
P15-2020,D14-1005,1,0.765841,"ailment Detection Douwe Kiela Computer Laboratory University of Cambridge douwe.kiela@cl.cam.ac.uk Laura Rimell Computer Laboratory University of Cambridge laura.rimell@cl.cam.ac.uk Ivan Vuli´c Department of Computer Science KU Leuven ivan.vulic@cs.kuleuven.be Stephen Clark Computer Laboratory University of Cambridge stephen.clark@cl.cam.ac.uk Abstract range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels. We exploit this tendency using sets of images returned by Google’s image search. The intuition is that the set of images returned for animal will consist of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consis"
P15-2020,W11-2501,0,0.154227,"ging dataset BIBLESS . Examples of pairs in the respective datasets can be found in Table 1. Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image"
P15-2020,P14-2135,1,0.908604,"in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1 www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 201"
P15-2020,R11-1055,0,0.077426,"s known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1 www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using C"
P15-2020,I13-1095,0,0.207838,"e unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently"
P15-2020,H05-1079,0,0.0594447,"Missing"
P15-2020,S12-1012,0,0.539959,"Missing"
P15-2020,W09-0215,0,0.65594,"Missing"
P15-2020,N15-1098,0,0.0260618,"Missing"
P15-2020,E14-1054,1,0.310309,"Missing"
P15-2020,E14-4008,0,0.567911,"ion (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypernym pair. Herbelot and"
P15-2020,P14-1068,0,0.140668,"Missing"
P15-2020,D11-1063,0,0.0217261,"Missing"
P15-2020,C04-1146,0,0.81603,"Missing"
P15-2020,C14-1212,0,0.405241,"relations, but does not require detection of directionality, since reversed pairs are grouped with the other negatives. For the combined experiment, we assign reversed hyponym-hypernym pairs a value of -1 instead of 0. We call this more challenging dataset BIBLESS . Examples of pairs in the respective datasets can be found in Table 1. Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For"
P15-2020,J09-3004,0,0.0525782,"Missing"
P15-2020,W13-0904,0,\N,Missing
P15-2038,D14-1005,1,0.811658,"Missing"
P15-2038,P14-2135,1,0.844431,"-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity score. Model performance is evaluated using the Spearman ρs correlation between the ranking produced by the cosine of the model-derived vectors and that produced by the gold-standard similarity scores. Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of a comparison such as lily-rose, the olfactory modality certainly is meaningful, while this is probably not the case for skateboard-swimsuit. Some examples of relevant pairs can be found in Table 1. Hence, we had two annotators rate the two datasets according to whether smell is relevant to the pairwise comparison. The annotation criterion was as follows: if both concepts in a pairwise comparison have a distinctive associated smell, then the comparison is relevant to the olfactory modality. Only if both annotators a"
P15-2038,P14-1132,0,0.0397793,"ection and κ = 0.96 for SimLex-999).1 Olfactory-Relevant Examples MEN sim SimLex-999 sim bakery bread 0.96 steak meat 0.75 grass lawn 0.96 flower violet 0.70 dog terrier 0.90 tree maple 0.55 bacon meat 0.88 grass moss 0.50 oak wood 0.84 beach sea 0.47 daisy violet 0.76 cereal wheat 0.38 daffodil rose 0.74 bread flour 0.33 2.2 Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many o"
P15-2038,I11-1162,0,0.397813,"Missing"
P15-2038,W15-0107,1,0.830114,"partial least squares regression (PLSR) to induce cross-modal mappings from the linguistic to the olfactory space and vice versa.2 Due to the nature of the olfactory data source (see Section 3), it is not possible to build olfactory representations for all concepts in the test sets. However, cross-modal mappings yield an additional benefit: since linguistic representations have full coverage over the datasets, we can project from linguistic space to perceptual space to also obtain full coverage for the perceptual modalities. This technique has been used to increase coverage for feature norms (Fagarasan et al., 2015). Consequently, we are in a position to compare perceptual spaces directly to each other, and to linguistic Table 1: Examples of pairs in the evaluation datasets where olfactory information is relevant, together with the gold-standard similarity score. 2 Tasks Following previous work in grounded semantics, we evaluate performance on two tasks: conceptual similarity and cross-modal zero-shot learning. 2.1 Cross-modal zero-shot learning Conceptual similarity We evaluate the performance of olfactory multimodal representations on two well-known similarity datasets: SimLex-999 (Hill et al., 2014) a"
P15-2038,N10-1011,0,0.048341,"Missing"
P15-2038,D13-1115,0,0.31721,"Missing"
P15-2038,D12-1130,0,0.138536,"ution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; 231 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 231–236, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics MEN test collection MEN (3000 pairs) and its olfactory-relevant subset OMEN (311 pairs); and the SimLex-999 dataset SLex (999 pairs) and i"
P15-2038,Q14-1017,0,0.096799,".50 oak wood 0.84 beach sea 0.47 daisy violet 0.76 cereal wheat 0.38 daffodil rose 0.74 bread flour 0.33 2.2 Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. A chance baseline is obtained by randomly ranking a concept’s nearest neighbors. We use partial least squares regression (PLSR) to"
P15-2038,J15-4004,0,\N,Missing
P15-2038,D14-1032,0,\N,Missing
P16-2031,P14-1006,0,0.112167,"that share a common meaning across different languages. It plays an important role in a variety of fundamental tasks in IR and NLP, e.g. cross-lingual information retrieval and statistical machine translation. The majority of current BLL models aim to learn lexicons from comparable data. These approaches work by (1) mapping language pairs to a shared crosslingual vector space (SCLVS) such that words are close when they have similar meanings; and (2) extracting close lexical items from the induced SCLVS. Bilingual word embedding (BWE) induced models currently hold the state-of-the-art on BLL (Hermann and Blunsom, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016). Although methods for learning SCLVSs are predominantly text-based, this space need not be linguistic in nature: Bergsma and van Durme (2011) and Kiela et al. (2015) used labeled images from 2 2.1 Methodology Linguistic Representations We use three representative linguistic BWE models. Given a source and target vocabulary V S and V T , BWE models learn a representation of each word w ∈ V S ∪ V T as a real-valued vec188 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7"
P16-2031,D14-1005,1,0.413644,"eneral performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set for (linguistic) BLL models from comparable data (Vuli´c and Moens, 2016)5 . It comprises 1, 000 nouns in ES, IT, and NL, along with their oneto-one ground-truth word translations in EN compiled semi-automatically. Translation direction is ES/IT /N L → EN . Multi-Modal Representations We experiment with two ways of fusing information stemming from the linguistic and visual modalities. Following recent work in multi-modal semantics (Bruni et al., 2014; Kiela and Bottou, 2014), we construct representations by concatenating the centered and L2 -normalized linguistic and visual feature vectors: wmm = α × wling ||(1 − α) × wvis Experimental Setup Training Data and Setup We used standard training data and suggested settings to learn M/G/V-EMB model representations. M-EMB and G-EMB were trained on the full cleaned and tokenized Wikipedias from the Polyglot website (AlRfou et al., 2013). V-EMB was trained on the full tokenized document-aligned Wikipedias from (1) where ||denotes concatenation and α is a parameter governing the contributions of each unimodal representatio"
P16-2031,P14-2135,1,0.557318,"setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from Kiela et al. (2015). We also propose a weighting technique based on image dispersion (Kiela et al., 2014) that governs the influence of visual information in fused representations, and show that this technique leads to robust multi-modal models which do not require fine tuning of the fusion parameter. Recent work has revealed the potential of using visual representations for bilingual lexicon learning (BLL). Such image-based BLL methods, however, still fall short of linguistic approaches. In this paper, we propose a simple yet effective multimodal approach that learns bilingual semantic representations that fuse linguistic and visual input. These new bilingual multi-modal embeddings display signi"
P16-2031,D15-1015,1,0.471003,"Missing"
P16-2031,J99-4009,0,0.810588,"image dispersion (ID) (Kiela et al., 2014). ID is defined as the average pairwise cosine distance between all the image representations/vectors {i1 . . . in } in the set of images for a given word w: id(w) = X 2 ij · ik 1− n(n − 1) |ij ||ik | (2) j<k≤n 5 Intuitively, more concrete words display more coherent visual representations and consequently lower ID scores (see Footnote 9 again). The lowest improvements on V ULIC 1000 are reported for the IT-EN language pair, which is incidentally the most abstract test set. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), albeit in a more complex way, since abstract concepts will relate more varied situations (Barsalou and Wiemer-Hastings, 2005). Consequently, uni-modal visual representations are not powerful enough to capture all the semantic intricacies of such abstract concepts, and the linguistic components are more beneficial in such cases. This explains an improved performance with α = 0.7, but also calls for a more intelligent decision mechanism on how much perceptual information to include in the multi-modal models. The decision should be closely related to the degree of a concept’s concreteness, e.g."
P16-2031,P15-1027,0,0.289784,"feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is learned through an L2 -regularized least-squa"
P16-2031,W16-3210,0,0.0149091,"Missing"
P16-2031,E14-1049,0,0.0830889,"l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is le"
P16-2031,W15-1521,0,0.168909,"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tor: wling = [f1ling , . . . , fdling ], where fkling ∈ l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate r"
P16-2031,P04-1067,0,0.0295097,"SCLVS: simvis (w, v) = SF (wvis , vvis ), e.g. cosine. (2) CNN-AVG M AX: An alternative strategy, introduced by Bergsma and van Durme (2011), is to consider the similarities between individual images from the two sets and take the average of the maximum similarity scores as the final similarity simvis (w, v). 2.3 3 Task: Bilingual Lexicon Learning Given a source language word ws , the task is to find a target language word wt closest to ws in the SCLVS, and the resulting pair (ws , wt ) is a bilingual lexicon entry. Performance is measured using the BLL standard Top 1 accuracy (Acc1 ) metric (Gaussier et al., 2004; Gouws et al., 2015). Test Sets We work with three language pairs: English-Spanish/Dutch/Italian (EN-ES/NL/IT), and two benchmarking BLL test sets: (1) B ERGSMA 500: consisting of a set of 500 ground truth noun pairs for the three language pairs, it is considered a benchmarking test set in prior work on BLL using vision (Bergsma and van Durme, 2011)4 . Translation direction in our tests is EN → ES/IT /N L. (2) V ULIC 1000: constructed to measure the general performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set"
P16-2031,N16-1021,0,0.0159524,"le. As future work, we plan to analyse the ability of multi-view representation learning algorithms to yield fused multi-modal representations in bilingual settings (Lazaridou et al., 2015b; Rastogi et al., 2015; Wang et al., 2015), as well as to apply multi-modal bilingual spaces in other tasks such as zero-short learning (Frome et al., 2013) or cross-lingual MM information search and retrieval following paradigms from monolingual settings (Pereira et al., 2014; Vuli´c and Moens, 2015). The inclusion of perceptual data, as this paper reveals, seems especially promising in bilingual settings (Rajendran et al., 2016; Elliott et al., 2016), since the perceptual information demonstrates the ability to transcend linguistic borders. Image Dispersion Weighting The intuition that the inclusion of visual information may lead to negative effects in MM modeling has been exploited by Kiela et al. (2014) in their work on image-dispersion filtering: Although the filtering method displays some clear benefits, its shortcoming lies in the fact that it performs a binary decision which can potentially discard valuable perceptual information for less concrete concepts. Here, we introduce a weighting scheme where the perce"
P16-2031,N15-1058,0,0.0380203,"Missing"
P16-2031,P14-1068,0,0.159578,"ven.be Abstract the Web to learn bilingual lexicons based on visual features, with features derived from deep convolutional neural networks (CNNs) leading to the best results (Kiela et al., 2015). However, vision-based BLL does not yet perform at the same level as state-of-the-art linguistic models. Here, we unify the strengths of both approaches into one single multi-modal vision-language SCLVS. It has been found in multi-modal semantics that linguistic and visual representations are often complementary in terms of the information they encode (Deselaers and Ferrari, 2011; Bruni et al., 2014; Silberer and Lapata, 2014). This is the first work to test the effectiveness of the multi-modal approach in a BLL setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from"
P16-2031,tiedemann-2012-parallel,0,0.0181405,"ting from the BNC word frequency list (Kilgarriff, 1997), the 6, 318 most frequent EN words were translated to the three other languages using Google Translate. The lists were subsequently cleaned, removing all pairs that contain IT/ES/NL words occurring in the test sets and least frequent pairs, to build the final 3×5K training pairs. We trained two monolingual SGNS models, using SGD with a global learning rate of 0.025. For G-EMB, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization was provided in the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We used SGD with a global learning rate 0.15. For V-EMB, monolingual SGNS was trained on pseudo-bilingual documents using SGD with a global learning rate 0.025. All BWEs were trained with d = 300.7 Other parameters are: 15 epochs, 15 negatives, subsampling rate 1e − 4. We report results with two α standard values: 0.5 and 0.7 (more weight assigned to the linguistic part). 4 sions8 . There is a marked difference in performance on B ERGSMA 500 and V ULIC 1000: visual-only BLL models on V ULIC 1000 perform two times worse than linguistic-only BLL models. This is easily explained by the increase"
P16-2031,D13-1168,1,0.367109,"Missing"
P16-2031,W13-3520,0,\N,Missing
P16-4010,D14-1005,1,0.843361,"dvantage that it approximates human auditory perception more closely than e.g. linearly-spaced frequency bands. 2.2 Related work 3 MMFeat Overview The MMFeat toolkit is written in Python. There are two command-line tools (described below) for obtaining files and extracting representations that do not require any knowledge of Python. The Python interface maintains a modular structure and contains the following modules: Convolutional neural networks In computer vision, the BoVW method has been superseded by deep convolutional neural networks (CNNs) (LeCun et al., 1998; Krizhevsky et al., 2012). Kiela and Bottou (2014) showed that such networks learn high-quality representations that can successfully be transfered to natural language processing tasks. Their method works as follows: • • • • mmfeat.miner mmfeat.bow mmfeat.cnn mmfeat.space Source files (images or sounds) can be obtained with the miner module, although this is not a requirement: it is straightforward to build an index of a data directory that matches words or phrases with relevant files. The miner module automatically generates this index, a Python dictionary mapping labels to lists of filenames, which is stored as a Python pickle file index.pk"
P16-4010,W11-2503,0,0.135544,"Missing"
P16-4010,D15-1293,1,0.906087,"subfield called multi-modal semantics. Such multi-modal models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014), improving lexical entailment (Kiela et al., 2015b), predicting compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Bergsma and Van Durme, 2011) and metaphor identification (Shutova et al., 2016). Although most of this work has relied on vision for the perceptual input, recent approaches have also used auditory (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015) and even olfactory (Kiela et al., 2015a) information. 2 Background 2.1 Bag of multi-modal words Although it is possible to ground distributional semantics in perception using e.g. co-occurrence patterns of image tags (Baroni and Lenci, 2008) or surrogates of human semantic knowledge such as feature norms (Andrews et al., 2009), the de facto method for grounding representations in perception has relied on processing raw image data (Baroni, 2016). The traditional method for obtaining visual representations (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2011) has been to apply t"
P16-4010,P12-1015,0,0.0622807,"Missing"
P16-4010,P14-2135,1,0.843021,"entations are clustered and the cluster assignments are reported to the screen, showing similar instruments in similar clusters. miner = GoogleMiner(datadir,  ’/path/to/miner.yaml’) miner.getResults(words, n_images) miner.save() Applying models We then apply both the BoVW and CNN models, in a manner familiar to scikit-learn users, by calling the fit() method: 5-Image dispersion This demo obtains images for the concepts of elephant and happiness and applies BoVW. It then shows that the former has a lower image dispersion score and is consequently more concrete than the latter, as described in Kiela et al. (2014). from mmfeat.bow import * from mmfeat.cnn import * b = BoVW(k=100, subsample=0.1) c = CNN(modelType=’alexnet’, gpu=True) b.load(data_dir) b.fit() c.load(data_dir) c.fit() 7 The field of natural language processing has broadened in scope to address increasingly challenging tasks. While the core NLP tasks will remain predominantly focused on linguistic input, it is important to address the fact that humans acquire and apply language in perceptually rich environments. Moving towards human-level AI will require the integration and modeling of multiple modalities beyond language. Advances in multi"
P16-4010,P13-4032,0,0.0147972,"sociation for Computational Linguistics 2.3 4. quantize the local descriptors by comparing them to the cluster centroids; and 5. combine relevant image representations into an overall visual representation for a word. The process for obtaining perceptual representations thus involves three distinct steps: obtaining files relevant to words or phrases, obtaining representations for the files, and aggregating these into visual or auditory representations. To our knowledge, this is the first toolkit that spans this entire process. There are libraries that cover some of these steps. Notably, VSEM (Bruni et al., 2013) is a Matlab library for visual semantics representation that implements BoVW and useful functionality for manipulating visual representations. DISSECT (Dinu et al., 2013) is a toolkit for distributional compositional semantics that makes it easy to work with (textual) distributional spaces. Lopopolo and van Miltenburg (2015) have also released their code for obtaning BoAW representations1 . The local feature descriptors in step (2) tend to be variants of the dense scale-invariant feature transform (SIFT) algorithm (Lowe, 2004), where an image is laid out as a dense grid and feature descriptor"
P16-4010,P15-2038,1,0.907768,"lies that they suffer from the grounding problem (Harnad, 1990). That is, they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). There has been a lot of interest within the Natural Language Processing community for making use of extra-linguistic perceptual information, much of it in a subfield called multi-modal semantics. Such multi-modal models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014), improving lexical entailment (Kiela et al., 2015b), predicting compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Bergsma and Van Durme, 2011) and metaphor identification (Shutova et al., 2016). Although most of this work has relied on vision for the perceptual input, recent approaches have also used auditory (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015) and even olfactory (Kiela et al., 2015a) information. 2 Background 2.1 Bag of multi-modal words Although it is possible to ground distributional semantics in perception using e.g. co-occurrence patterns of image tags (Baroni and Lenci, 2008) or s"
P16-4010,P15-2020,1,0.715074,"Missing"
P16-4010,N16-1071,1,0.873792,"Missing"
P16-4010,D15-1015,1,0.876122,"Missing"
P16-4010,P13-4006,0,0.0296832,"an overall visual representation for a word. The process for obtaining perceptual representations thus involves three distinct steps: obtaining files relevant to words or phrases, obtaining representations for the files, and aggregating these into visual or auditory representations. To our knowledge, this is the first toolkit that spans this entire process. There are libraries that cover some of these steps. Notably, VSEM (Bruni et al., 2013) is a Matlab library for visual semantics representation that implements BoVW and useful functionality for manipulating visual representations. DISSECT (Dinu et al., 2013) is a toolkit for distributional compositional semantics that makes it easy to work with (textual) distributional spaces. Lopopolo and van Miltenburg (2015) have also released their code for obtaning BoAW representations1 . The local feature descriptors in step (2) tend to be variants of the dense scale-invariant feature transform (SIFT) algorithm (Lowe, 2004), where an image is laid out as a dense grid and feature descriptors are computed for each keypoint. A similar method has recently been applied to the auditory modality (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015), using sou"
P16-4010,N10-1011,0,0.0441248,"aches have also used auditory (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015) and even olfactory (Kiela et al., 2015a) information. 2 Background 2.1 Bag of multi-modal words Although it is possible to ground distributional semantics in perception using e.g. co-occurrence patterns of image tags (Baroni and Lenci, 2008) or surrogates of human semantic knowledge such as feature norms (Andrews et al., 2009), the de facto method for grounding representations in perception has relied on processing raw image data (Baroni, 2016). The traditional method for obtaining visual representations (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2011) has been to apply the bag-of-visual-words (BoVW) approach (Sivic and Zisserman, 2003). The method can be described as follows: 1. obtain relevant images for a word or set of words; 2. for each image, get local feature descriptors; 3. cluster feature descriptors with k-means to find the centroids, a.k.a. the “visual words”; 55 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 55–60, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2.3 4. quantize"
P16-4010,I11-1162,0,0.0131546,"ditory (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015) and even olfactory (Kiela et al., 2015a) information. 2 Background 2.1 Bag of multi-modal words Although it is possible to ground distributional semantics in perception using e.g. co-occurrence patterns of image tags (Baroni and Lenci, 2008) or surrogates of human semantic knowledge such as feature norms (Andrews et al., 2009), the de facto method for grounding representations in perception has relied on processing raw image data (Baroni, 2016). The traditional method for obtaining visual representations (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2011) has been to apply the bag-of-visual-words (BoVW) approach (Sivic and Zisserman, 2003). The method can be described as follows: 1. obtain relevant images for a word or set of words; 2. for each image, get local feature descriptors; 3. cluster feature descriptors with k-means to find the centroids, a.k.a. the “visual words”; 55 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics—System Demonstrations, pages 55–60, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2.3 4. quantize the local descriptors by"
P16-4010,W15-0110,0,0.0221269,"Missing"
P16-4010,D13-1115,0,0.111145,"Missing"
P16-4010,N16-1020,1,0.888127,"(Louwerse, 2008). There has been a lot of interest within the Natural Language Processing community for making use of extra-linguistic perceptual information, much of it in a subfield called multi-modal semantics. Such multi-modal models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014), improving lexical entailment (Kiela et al., 2015b), predicting compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Bergsma and Van Durme, 2011) and metaphor identification (Shutova et al., 2016). Although most of this work has relied on vision for the perceptual input, recent approaches have also used auditory (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015) and even olfactory (Kiela et al., 2015a) information. 2 Background 2.1 Bag of multi-modal words Although it is possible to ground distributional semantics in perception using e.g. co-occurrence patterns of image tags (Baroni and Lenci, 2008) or surrogates of human semantic knowledge such as feature norms (Andrews et al., 2009), the de facto method for grounding representations in perception has relied on processing raw"
P16-4010,P14-1068,0,0.0432824,"r others (Turney and Pantel, 2010; Clark, 2015), which implies that they suffer from the grounding problem (Harnad, 1990). That is, they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). There has been a lot of interest within the Natural Language Processing community for making use of extra-linguistic perceptual information, much of it in a subfield called multi-modal semantics. Such multi-modal models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014), improving lexical entailment (Kiela et al., 2015b), predicting compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Bergsma and Van Durme, 2011) and metaphor identification (Shutova et al., 2016). Although most of this work has relied on vision for the perceptual input, recent approaches have also used auditory (Lopopolo and van Miltenburg, 2015; Kiela and Clark, 2015) and even olfactory (Kiela et al., 2015a) information. 2 Background 2.1 Bag of multi-modal words Although it is possible to ground distributional semantics in perception using e.g. co-occurrence pa"
P16-4010,J15-4004,0,\N,Missing
P17-1016,P10-1106,0,0.0119315,"output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme. (1) We can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010). Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. This means finding the most likely word wt+1 given a 4 Constrained Character-level M"
P17-1016,D16-1126,0,0.509818,"Missing"
P17-1016,C16-1103,0,0.0505521,"Missing"
P17-1016,W09-2005,0,0.0331229,"s phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols. Automatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches. Rule-based poetry generation attempts include case-based reasoning (Gerv´as, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013; Barbieri et al., 2012) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used to generate new poetic variants (Yi et al., 2016; Greene et al., 2010). Neural language models have been increasingly applied to the task of poetry generation. The work of Zhang and Lapata (2014) is one such example, where they were able to outperform all other classical Chinese poetry generati"
P17-1016,D10-1051,0,0.882019,"hes. Rule-based poetry generation attempts include case-based reasoning (Gerv´as, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013; Barbieri et al., 2012) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used to generate new poetic variants (Yi et al., 2016; Greene et al., 2010). Neural language models have been increasingly applied to the task of poetry generation. The work of Zhang and Lapata (2014) is one such example, where they were able to outperform all other classical Chinese poetry generation systems with both manual and automatic evaluation. Ghazvininejad et al. (2016) and Goyal et al. (2016) apply neural language models with regularising finite state machines. However, in the former case the rhythm of the output cannot be defined at sample time, and in the latter case the finite state machine is not trained on rhythm at all, as it is trained on dialogue ac"
P17-1016,N09-1005,0,0.0490867,"Missing"
P17-1016,H05-1026,0,0.0178887,"Missing"
P17-1016,P06-2065,0,0.0134369,"lding a training set of 288,326 words and 1,563,457 characters. An example of the output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme. (1) We can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010). Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words."
P17-1016,W13-3512,0,0.0911945,"Missing"
P17-1016,W16-5508,0,0.0183786,"ural language models have been increasingly applied to the task of poetry generation. The work of Zhang and Lapata (2014) is one such example, where they were able to outperform all other classical Chinese poetry generation systems with both manual and automatic evaluation. Ghazvininejad et al. (2016) and Goyal et al. (2016) apply neural language models with regularising finite state machines. However, in the former case the rhythm of the output cannot be defined at sample time, and in the latter case the finite state machine is not trained on rhythm at all, as it is trained on dialogue acts. McGregor et al. (2016) construct a phonological model for generating prosodic texts, however there is no attempt to embed semantics into this model. 3 Phonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words. These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1 . The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In add"
P17-1016,D14-1074,0,0.488789,"et al., 2012), constraint satisfaction (Toivanen et al., 2013; Barbieri et al., 2012) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used to generate new poetic variants (Yi et al., 2016; Greene et al., 2010). Neural language models have been increasingly applied to the task of poetry generation. The work of Zhang and Lapata (2014) is one such example, where they were able to outperform all other classical Chinese poetry generation systems with both manual and automatic evaluation. Ghazvininejad et al. (2016) and Goyal et al. (2016) apply neural language models with regularising finite state machines. However, in the former case the rhythm of the output cannot be defined at sample time, and in the latter case the finite state machine is not trained on rhythm at all, as it is trained on dialogue acts. McGregor et al. (2016) construct a phonological model for generating prosodic texts, however there is no attempt to embed"
P18-1205,P17-1171,1,0.794153,"sy nor too difficult for the current technology (Voorhees et al., 1999). One issue with conditioning on textual personas is that there is a danger that humans will, even if asked not to, unwittingly repeat profile information either verbatim or with significant word overlap. This may make any subsequent machine learning tasks less challenging, and the solutions will not generalize to more difficult tasks. This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017). To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about “a related characteristic that the same person may have”, hence the revisions could be rephrases, generalizations or specializations. For example “I like basketball” can be revised as “I am a big fan of Michael Jordan” not because they mean the same thing but because the same persona could contain both. In the revision task, workers are instructed not to trivially rephrase the sentence by copying the original words. Ho"
P18-1205,P16-1094,0,0.264567,"Missing"
P18-1205,D16-1127,0,0.802794,"1 Introduction Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy. It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting. Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (Serban et al., 2016; Vinyals and Le, 2015). Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015); 1 Work done while at Facebook AI Research. and (iii) a tendency to produce non-specific answers like “I don’t know” (Li et al., 2015). Those three problems combine to produce an unsatisfying overall experience for a human to engage with. We believe some of those problems are due to there being no good publicly available dataset for general chit-chat. Bec"
P18-1205,D16-1230,0,0.231377,"which are in Tables 5 and 6 in the Appendix. Using “Their persona” has less impact on this dataset. We believe this is because most speakers tend to focus on themselves when it comes to their interests. It would be interesting how often this is the case in other datasets. Certainly this is skewed by the particular instructions one could give to the crowdworkers. For example if we gave the instructions “try not to talk about yourself, but about the other’s interests’ likely these metrics would change. 2210 5.2 Human Evaluation As automated metrics are notoriously poor for evaluating dialogue (Liu et al., 2016) we also perform human evaluation using crowdsourced workers. The procedure is as follows. We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3. In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat. Here, from the Turker’s point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this). In this setting, for both the Turker and the model, the personas come from the test set"
P18-1205,W15-4640,0,0.0999014,"ning agent. Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user. We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both. These scenarios can be tried using either the original personas, or the revised ones. We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following Lowe et al. (2015). The latter consists of choosing N random distractor responses from other dialogues (in our setting, N =19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments). 4 Models We consider two classes of model for next utterance prediction: ranking models and generative models. Ranking models produce a next utterance by considering any utterance in the training set as a possible candidate reply. Generative models generate novel sentences by conditioning on the dialogue histo"
P18-1205,D16-1147,1,0.752999,"rofile, the two models are identical. When the profile is available attention is performed by computing the similarity of the input q with the profile sentences pi , computing the softmax, and taking the weighted sum: X q+ = q + si pi , si = Softmax(sim(q, pi )) P where Softmax(zi ) = ezi / j ezj . One can then rank the candidates c0 using sim(q + , c0 ). One can also perform multiple “hops” of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps. 4.3 Key-Value Profile Memory Network The key-value (KV) memory network (Miller et al., 2016) was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs. Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner. This allows the model 2208 to have a memory of past dialogues that it can directly use to help influence its prediction for the current conver"
P18-1205,D14-1162,0,0.0822456,"ng very slow. In our experiments we simply trained the profile memory network and used the same weights from that model and applied this architecture at test time instead. Training the model directly would presumably give better results, however this heuristic already proved beneficial compared to the original network. Zipf’s law4 . Let F be the set of encoded memories. The decoder now attends over the encoded profile entries, i.e., we compute the mask at , context ct and next input x ˆt as: 4.4 5.1 Seq2Seq The input sequence x is encoded by applying het = LST Menc (xt |het−1 ). We use GloVe (Pennington et al., 2014) for our word embeddings. The final hidden state, het , is fed into the decoder LST Mdec as the initial state hd0 . For each time step t, the decoder then produces the probability of a word j occurring in that place via the softmax, i.e., exp(wj hdt ) p(yt,j = 1 |yt−1 , . . . , y1 ) = PK . d j 0 =1 exp(wj 0 ht ) The model is trained via negative log likelihood. The basic model can be extended to include persona information, in which case we simply prepend it to the input sequence x, i.e., x = ∀p ∈ P ||x, where ||denotes concatenation. For the OpenSubtitles and Twitter datasets trained in Secti"
P18-1205,D16-1264,0,0.0418531,"Missing"
P18-1205,N15-1020,0,0.171556,"Missing"
P18-2057,E12-1004,0,0.0964191,"elative performance of patternbased and distributional models, we apply them to several challenging hypernymy tasks. Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation. For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of BLESS, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns. Negative pairs contain a mixture of co-hyponymy, meronymy, and random pairs. This version contains 14,542 total pairs with 1,337 positive examples. Second, we evaluate on LEDS (Baroni et al., 2012), which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs. We also consider EVAL (Santus et al., 2015), containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations. EVAL is notable for its absence of random pairs. The largest dataset is SHWARTZ (Shwartz et al., 2016), which was collected from a mixture of WordNet, DBPedia, and other resources. We limit ourselves to a 52,578 pair subset excluding multiword expressions. Finally, we evaluate on WBLESS (Weeds et al., 2014), a 1,668 pair subse"
P18-2057,W03-0415,0,0.0709626,"ar values are set to zero). Equation (3) can be interpreted as a smoothed version of the observed PPMI matrix. Due to the truncation of singular values, Equation (3) computes a low-rank embedding of M where similar words (in terms of their Hearst patterns) have similar representations. Since Equation (3) is defined for all pairs (x, y), it allows us to make hypernymy predictions based on the similarity of words. We also consider factorizing a matrix that is constructed from occurrence probabilities as in Equation (1), denoted by sp(x, y). This approach is then closely related to the method of Cederberg and Widdows (2003), which has been proposed to improve precision and recall for hypernymy detection from Hearst patterns. While Equation (2) can correct for different word occurrence probabilities, it cannot handle missing data. However, sparsity is one of the main issues when using Hearst patterns, as a necessarily incomplete set of extraction rules will lead inevitably to missing extractions. For this purpose, we also study low-rank embeddings of the PPMI matrix, which allow us to make predictions for unseen pairs. In particular, let m = |{x : (x, y) ∈ P ∨ (y, x) ∈ P}| denote the number of unique terms in P."
P18-2057,W09-0215,0,0.210955,"defined for large, sparse, positively-valued distributional spaces. First, we consider WeedsPrec (Weeds et al., 2004) which captures the features of 359 x which are included in the set of a broader term’s features, y: Pn x ∗ ✶yi >0 Pni WeedsPrec(x, y) = i=1 i=1 xi Pattern Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term. In particular, let Pn min(xi , yi ) Pn CL(x, y) = i=1 i=1 xi denote the degree of inclusion of x in y as proposed by Clarke (2009). To measure both the inclusion of x in y and the non-inclusion of y in x, invCL is then defined as p invCL(x, y) = CL(x, y) ∗ (1 − CL(y, x)) where H(ci ) is the Shannon entropy of context ci across all terms, and N is chosen in hyperparameter selection. Finally, SLQS is defined using the ratio between the two terms: Ex . Ey Since the SLQS model only compares the relative generality of two terms, but does not make judgment about the terms’ relatedness, we report SLQS-cos, which multiplies the SLQS measure by cosine similarity of x and y (Santus et al., 2014). For completeness, we also include"
P18-2057,C92-2082,0,0.728811,"thods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods. 1 Introduction Hierarchical relationships play a central role in knowledge representation and reasoning. Hypernym detection, i.e., the modeling of word-level hierarchies, has long been an important task in natural language processing. Starting with Hearst (1992), pattern-based methods have been one of the most influential approaches to this problem. Their key idea is to exploit certain lexico-syntactic patterns to detect is-a relations in text. For instance, patterns like “NPy such as NPx ”, or “NPx and other NPy ” often indicate hypernymy relations of the form x is-a y. Such patterns may be predefined, or they may be learned automatically (Snow et al., 2004; Shwartz et al., 2016). However, a well-known problem of Hearst-like patterns is their extreme sparsity: words must co-occur in exactly the right configuration, or else no relation can be detecte"
P18-2057,P15-2020,1,0.920997,"Missing"
P18-2057,S12-1012,0,0.640065,"necessarily incomplete set of extraction rules will lead inevitably to missing extractions. For this purpose, we also study low-rank embeddings of the PPMI matrix, which allow us to make predictions for unseen pairs. In particular, let m = |{x : (x, y) ∈ P ∨ (y, x) ∈ P}| denote the number of unique terms in P. Furthermore, let X ∈ Rm×m be the PPMI matrix with 2.2 Distributional Hypernym Detection Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012; Shwartz et al., 2017). Here, we compare to two methods with strong empirical results. As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces. First, we consider WeedsPrec (Weeds et al., 2004) which captures the features of 359 x which are included in the set of a broader term’s features, y: Pn x ∗ ✶yi >0 Pni WeedsPrec(x, y) = i=1 i=1 xi Pattern Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the nar"
P18-2057,P14-2050,0,0.0201777,"SVD-based models, we select the rank from r ∈ {5, 10, 15, 20, 25, 50, 100, 150, 200, 250, 300, 500, 1000} on the validation set. The other pattern-based models do not have any hyperparameters. Distributional models: For the distributional baselines, we employ the large, sparse distributional space of Shwartz et al. (2017), which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks. The corpus was POS tagged and dependency parsed. Distributional contexts were constructed from adjacent words in dependency parses (Pad´o and Lapata, 2007; Levy and Goldberg, 2014). Targets and contexts which appeared fewer than 100 times in the corpus were filtered, and the resulting co-occurrence matrix was PPMI transformed.1 The resulting space contains representations for 218K words over 732K context dimensions. For the SLQS model, we selected the number of contexts N from the same set of options as the SVD rank in pattern-based models. 3.3 Results Table 2 shows the results from all three experimental settings. In nearly all cases, we find that patternbased approaches substantially outperform all three distributional models. Particularly strong improvements can be o"
P18-2057,N15-1098,0,0.376116,"2005), which states roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear. Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017). Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016). While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora. Here, we revisit the idea of using pattern-based methods for hypernym detection. We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH. We find that simple pattern-based methods consistently outper"
P18-2057,D17-1022,0,0.611294,"Missing"
P18-2057,J07-2002,0,0.170864,"Missing"
P18-2057,D16-1234,1,0.947047,"roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear. Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017). Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016). While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora. Here, we revisit the idea of using pattern-based methods for hypernym detection. We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH. We find that simple pattern-based methods consistently outperform specialized DIH me"
P18-2057,E14-4008,0,0.839466,"Hearst patterns, as a necessarily incomplete set of extraction rules will lead inevitably to missing extractions. For this purpose, we also study low-rank embeddings of the PPMI matrix, which allow us to make predictions for unseen pairs. In particular, let m = |{x : (x, y) ∈ P ∨ (y, x) ∈ P}| denote the number of unique terms in P. Furthermore, let X ∈ Rm×m be the PPMI matrix with 2.2 Distributional Hypernym Detection Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012; Shwartz et al., 2017). Here, we compare to two methods with strong empirical results. As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces. First, we consider WeedsPrec (Weeds et al., 2004) which captures the features of 359 x which are included in the set of a broader term’s features, y: Pn x ∗ ✶yi >0 Pni WeedsPrec(x, y) = i=1 i=1 xi Pattern Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains con"
P18-2057,W15-4208,0,0.149381,"to classify whether pairs of words are in a hypernymy relation. For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of BLESS, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns. Negative pairs contain a mixture of co-hyponymy, meronymy, and random pairs. This version contains 14,542 total pairs with 1,337 positive examples. Second, we evaluate on LEDS (Baroni et al., 2012), which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs. We also consider EVAL (Santus et al., 2015), containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations. EVAL is notable for its absence of random pairs. The largest dataset is SHWARTZ (Shwartz et al., 2016), which was collected from a mixture of WordNet, DBPedia, and other resources. We limit ourselves to a 52,578 pair subset excluding multiword expressions. Finally, we evaluate on WBLESS (Weeds et al., 2014), a 1,668 pair subset of BLESS, with negative pairs being selected from co-hyponymy, random, and hyponymy relations. Previous work has used different metrics for evaluating on BLESS (L"
P18-2057,P16-1226,0,0.291,"Missing"
P18-2057,E17-1007,0,0.640073,"cal relationships. The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH) (Zhitomirsky-Geffet and Dagan, 2005), which states roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear. Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017). Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016). While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora. Here, we revisit the idea of using pattern-based methods for hypernym detection. We evaluate severa"
P18-2057,J17-4004,1,0.90665,"Missing"
P18-2057,N18-1103,0,0.505447,"Missing"
P18-2057,C14-1212,0,0.573264,"evaluate on LEDS (Baroni et al., 2012), which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs. We also consider EVAL (Santus et al., 2015), containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations. EVAL is notable for its absence of random pairs. The largest dataset is SHWARTZ (Shwartz et al., 2016), which was collected from a mixture of WordNet, DBPedia, and other resources. We limit ourselves to a 52,578 pair subset excluding multiword expressions. Finally, we evaluate on WBLESS (Weeds et al., 2014), a 1,668 pair subset of BLESS, with negative pairs being selected from co-hyponymy, random, and hyponymy relations. Previous work has used different metrics for evaluating on BLESS (Lenci and Benotto, 2012; Levy et al., 2015; Roller and Erk, 2016). We chose to evaluate the global ranking using Average Precision. This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in Shwartz et al. (2017). Direction: In direction prediction, the task is to identify which term is broader in a given pair 360 of words. For this task, we evaluate all models on thr"
P18-2057,C04-1146,0,0.842516,"Missing"
P18-2057,P05-1014,0,0.647691,"rds must co-occur in exactly the right configuration, or else no relation can be detected. To alleviate the sparsity issue, the focus in hypernymy detection has recently shifted to distributional representations, wherein words are represented as vectors based on their distribution across large corpora. Such methods offer rich representations of lexical meaning, alleviating the sparsity problem, but require specialized similarity measures to distinguish different lexical relationships. The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH) (Zhitomirsky-Geffet and Dagan, 2005), which states roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear. Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance. An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017). Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy"
P19-1313,D15-1075,0,0.161717,"Missing"
P19-1313,N18-1045,0,0.617794,"the sparsity issue but also require specialized similarity measures to distinguish different lexical relationships. To date, most measures are inspired by the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (2017) showed that such contexts plays an important role for the success of distributional methods. CamachoCollados et al. (2018) proposed a new shared task for hypernym retrieval from text corpora. Recently, Roller et al. (2018) performed a systematic study of unsupervised distributional and pattern-based approaches for hypernym detection. Their results showed that pattern-based methods are able"
P19-1313,W18-1708,0,0.0372028,"|properties) Y such as X1 , X2 , . . . (Unlike |like) (most |all |any |other) Y, X Y including X1 , X2 , . . . 4 3 2 1 0 1 2 3 4 5 Rank (log scale) Figure 2: Frequency distribution of words appearing in the Hearst pattern corpus (on a log-log scale). Table 1: Hearst patterns used in this study. Patterns are lemmatized, but listed as inflected for clarity. embeddings with ones with a natural hierarchical structure. 2014) to hyperbolic space. In addition, works have considered how distributional co-occurrences may be used to augment order-embeddings (Li et al., 2018) and hyperbolic embeddings (Dhingra et al., 2018). Further methods have focused on the often complex overlapping structure of word classes, and induced hierarchies using box-lattice structures (Vilnis et al., 2018) and Gaussian word embeddings (Athiwaratkun and Wilson, 2018). Compared to many of the purely graph-based works, these methods generally require supervision of hierarchical structure, and cannot learn taxonomies using only unstructured noisy data. Taxonomy induction Although detecting hypernymy relationships is an important and difficult task, these systems alone do not produce rich taxonomic graph structures (Camacho-Collados, 201"
P19-1313,P05-1014,0,0.244905,"ler et al., 2018). Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al., 2016); but also come with precision trade-offs. To overcome the sparse extractions of patternbased methods, focus has recently shifted to distributional approaches which provide rich representations of lexical meaning. These methods alleviate the sparsity issue but also require specialized similarity measures to distinguish different lexical relationships. To date, most measures are inspired by the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (201"
P19-1313,C92-2082,0,0.71836,"nd allows us to get additional insights, e.g., about a term’s degree of generality. Figure 1 shows an example of a two-dimensional embedding of the Hearst graph that we use in our experiments. Although we will use higher dimensionalities for our final embedding, the visualization serves as a good illustration of the hierarchical structure that is obtained through the embedding. 2 Related Work Hypernym detection Detecting is-a-relations from text is a long-standing task in natural language processing. A popular approach is to exploit highprecision lexico-syntactic patterns as first proposed by Hearst (1992). These patterns may be predefined or learned automatically (Snow et al., 2005; Shwartz et al., 2016; Nakashole et al., 2012). However, it is well known that such pattern-based methods suffer significantly from missing extractions as terms must occur in exactly the right configuration to be detected (Shwartz et al., 2016; Roller et al., 2018). Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al., 2016); but also come with precision trade-offs. To overcome the sparse extractions of patternbased methods, focus has"
P19-1313,P15-2020,1,0.92823,"Missing"
P19-1313,D10-1108,0,0.152382,"Missing"
P19-1313,S12-1012,0,0.414729,"rich representations of lexical meaning. These methods alleviate the sparsity issue but also require specialized similarity measures to distinguish different lexical relationships. To date, most measures are inspired by the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (2017) showed that such contexts plays an important role for the success of distributional methods. CamachoCollados et al. (2018) proposed a new shared task for hypernym retrieval from text corpora. Recently, Roller et al. (2018) performed a systematic study of unsupervised distributional and pattern-based approaches for hypernym detectio"
P19-1313,N15-1098,0,0.0625533,"tributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (2017) showed that such contexts plays an important role for the success of distributional methods. CamachoCollados et al. (2018) proposed a new shared task for hypernym retrieval from text corpora. Recently, Roller et al. (2018) performed a systematic study of unsupervised distributional and pattern-based approaches for hypernym detection. Their results showed that pattern-based methods are able to outperform DIH-based methods on several challenging hypernymy benchmarks. Key aspects to good performance were the extraction of patterns from large text cor"
P19-1313,D12-1104,0,0.096573,"two-dimensional embedding of the Hearst graph that we use in our experiments. Although we will use higher dimensionalities for our final embedding, the visualization serves as a good illustration of the hierarchical structure that is obtained through the embedding. 2 Related Work Hypernym detection Detecting is-a-relations from text is a long-standing task in natural language processing. A popular approach is to exploit highprecision lexico-syntactic patterns as first proposed by Hearst (1992). These patterns may be predefined or learned automatically (Snow et al., 2005; Shwartz et al., 2016; Nakashole et al., 2012). However, it is well known that such pattern-based methods suffer significantly from missing extractions as terms must occur in exactly the right configuration to be detected (Shwartz et al., 2016; Roller et al., 2018). Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al., 2016); but also come with precision trade-offs. To overcome the sparse extractions of patternbased methods, focus has recently shifted to distributional approaches which provide rich representations of lexical meaning. These methods alleviate"
P19-1313,D14-1162,0,0.0877517,"Missing"
P19-1313,D16-1234,1,0.856853,"on Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (2017) showed that such contexts plays an important role for the success of distributional methods. CamachoCollados et al. (2018) proposed a new shared task for hypernym retrieval from text corpora. Recently, Roller et al. (2018) performed a systematic study of unsupervised distributional and pattern-based approaches for hypernym detection. Their results showed that pattern-based methods are able to outperform DIH-based methods on several challenging hypernymy benchmarks. Key aspects to good performance were the extraction of patterns from large text corpora and using embeddin"
P19-1313,P18-2057,1,0.833365,"ure that is obtained through the embedding. 2 Related Work Hypernym detection Detecting is-a-relations from text is a long-standing task in natural language processing. A popular approach is to exploit highprecision lexico-syntactic patterns as first proposed by Hearst (1992). These patterns may be predefined or learned automatically (Snow et al., 2005; Shwartz et al., 2016; Nakashole et al., 2012). However, it is well known that such pattern-based methods suffer significantly from missing extractions as terms must occur in exactly the right configuration to be detected (Shwartz et al., 2016; Roller et al., 2018). Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al., 2016); but also come with precision trade-offs. To overcome the sparse extractions of patternbased methods, focus has recently shifted to distributional approaches which provide rich representations of lexical meaning. These methods alleviate the sparsity issue but also require specialized similarity measures to distinguish different lexical relationships. To date, most measures are inspired by the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan"
P19-1313,E14-4008,0,0.469038,"meaning. These methods alleviate the sparsity issue but also require specialized similarity measures to distinguish different lexical relationships. To date, most measures are inspired by the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (2017) showed that such contexts plays an important role for the success of distributional methods. CamachoCollados et al. (2018) proposed a new shared task for hypernym retrieval from text corpora. Recently, Roller et al. (2018) performed a systematic study of unsupervised distributional and pattern-based approaches for hypernym detection. Their results showed that"
P19-1313,P14-5010,0,0.00589779,"ributional and pattern-based methods in all settings. WeedsPrec The first distributional model we consider is WeedsPrec (Weeds et al., 2004), which captures the features of x which are included in the set of more general term’s features, y: Pn x ·✶ Pn i yi >0 WeedsPrec(x, y) = i=1 i=1 xi invCL Lenci and Benotto (2012), introduce the idea of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term. The degree of inclusion is denoted as: Pn min(xi , yi ) CL(x, y) = i=1Pn i=1 xi lemmatized, and POS-tagged using CoreNLP 3.8.0 (Manning et al., 2014). The full set of Hearst patterns is provided in Table 1. These include prototypical Hearst patterns, like “animals [such as] big cats”, as well as broader patterns like “New Year [is the most important] holiday.” Noun phrases were allowed to match limited modifiers, and produced additional hits for the head of the noun phrase. The final corpus contains circa 4.5M matched pairs, 431K unique pairs, and 243K unique terms. Hypernymy Tasks We consider three distinct subtasks for evaluating the performance of these models for hypernymy prediction: • Detection: Given a pair of words (u, v), determin"
P19-1313,W15-4208,0,0.0617166,"as: SLQS(x, y) = 1 − Ex /Ey Corpora and Preprocessing We construct our Hearst graph using the same data, patterns, and procedure as described in (Roller et al., 2018): Hearst patterns are extracted from the concatenation of GigaWord and Wikipedia. The corpus is tokenized, • Direction: Given a pair (u, v), determine if u is more general than v or vise versa. • Graded Entailment: Given a pair of words (u, v), determine the degree to which u is a v. For detection, we evaluate all models on five commonly-used benchmark datasets: B LESS (Baroni and Lenci, 2011), L EDS (Baroni et al., 2012), E VAL (Santus et al., 2015), S HWARTZ (Shwartz et al., 2016), and WB LESS (Weeds et al., 2014), In addition to positive hypernymy relations, these datasets include negative samples in the form of random pairs, co-hyponymy, antonymy, meronymy, and adjectival relations. For directionality and graded entailment, we also use B I B LESS (Kiela et al., 2015) and H YPERLEX (Vulic et al., 2016). We refer to Roller et al. (2018) for an in-depth discussion of these datasets. For all models, we use the identical text corpus and tune hyperparameters on the validation sets. 3237 Animals Plants Vehicles All Missing Transitive All Mis"
P19-1313,L16-1056,0,0.0855774,"ural language processing. A popular approach is to exploit highprecision lexico-syntactic patterns as first proposed by Hearst (1992). These patterns may be predefined or learned automatically (Snow et al., 2005; Shwartz et al., 2016; Nakashole et al., 2012). However, it is well known that such pattern-based methods suffer significantly from missing extractions as terms must occur in exactly the right configuration to be detected (Shwartz et al., 2016; Roller et al., 2018). Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al., 2016); but also come with precision trade-offs. To overcome the sparse extractions of patternbased methods, focus has recently shifted to distributional approaches which provide rich representations of lexical meaning. These methods alleviate the sparsity issue but also require specialized similarity measures to distinguish different lexical relationships. To date, most measures are inspired by the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the context"
P19-1313,P16-1226,0,0.699147,"shows an example of a two-dimensional embedding of the Hearst graph that we use in our experiments. Although we will use higher dimensionalities for our final embedding, the visualization serves as a good illustration of the hierarchical structure that is obtained through the embedding. 2 Related Work Hypernym detection Detecting is-a-relations from text is a long-standing task in natural language processing. A popular approach is to exploit highprecision lexico-syntactic patterns as first proposed by Hearst (1992). These patterns may be predefined or learned automatically (Snow et al., 2005; Shwartz et al., 2016; Nakashole et al., 2012). However, it is well known that such pattern-based methods suffer significantly from missing extractions as terms must occur in exactly the right configuration to be detected (Shwartz et al., 2016; Roller et al., 2018). Recent works improve coverage by leveraging search engines (Kozareva and Hovy, 2010) or by exploiting web-scale corpora (Seitner et al., 2016); but also come with precision trade-offs. To overcome the sparse extractions of patternbased methods, focus has recently shifted to distributional approaches which provide rich representations of lexical meaning"
P19-1313,E17-1007,0,0.18787,"fet and Dagan 2005) which hypothesizes that for a subsumption relation (cat, is-a, mammal) the subordinate term (cat) should appear in a subset of the contexts in which the superior term (mammal) occurs. Unsupervised methods for hypernymy detection based on distributional approaches include WeedsPrec (Weeds et al., 2004), invCL (Lenci and Benotto, 2012), SLQS (Santus et al., 2014), and DIVE (Chang et al., 2018). Distributional representations that are based on positional or dependency-based contexts may also capture crude Hearst-pattern-like features (Levy et al., 2015; Roller and Erk, 2016). Shwartz et al. (2017) showed that such contexts plays an important role for the success of distributional methods. CamachoCollados et al. (2018) proposed a new shared task for hypernym retrieval from text corpora. Recently, Roller et al. (2018) performed a systematic study of unsupervised distributional and pattern-based approaches for hypernym detection. Their results showed that pattern-based methods are able to outperform DIH-based methods on several challenging hypernymy benchmarks. Key aspects to good performance were the extraction of patterns from large text corpora and using embedding methods to overcome t"
P19-1313,C14-1212,0,0.346038,"ur Hearst graph using the same data, patterns, and procedure as described in (Roller et al., 2018): Hearst patterns are extracted from the concatenation of GigaWord and Wikipedia. The corpus is tokenized, • Direction: Given a pair (u, v), determine if u is more general than v or vise versa. • Graded Entailment: Given a pair of words (u, v), determine the degree to which u is a v. For detection, we evaluate all models on five commonly-used benchmark datasets: B LESS (Baroni and Lenci, 2011), L EDS (Baroni et al., 2012), E VAL (Santus et al., 2015), S HWARTZ (Shwartz et al., 2016), and WB LESS (Weeds et al., 2014), In addition to positive hypernymy relations, these datasets include negative samples in the form of random pairs, co-hyponymy, antonymy, meronymy, and adjectival relations. For directionality and graded entailment, we also use B I B LESS (Kiela et al., 2015) and H YPERLEX (Vulic et al., 2016). We refer to Roller et al. (2018) for an in-depth discussion of these datasets. For all models, we use the identical text corpus and tune hyperparameters on the validation sets. 3237 Animals Plants Vehicles All Missing Transitive All Missing Transitive All Missing Transitive p(x, y) ppmi(x, y) sp(x, y)"
P19-1313,C04-1146,0,0.752998,"Missing"
P19-1313,P06-1101,0,0.175438,"rely graph-based works, these methods generally require supervision of hierarchical structure, and cannot learn taxonomies using only unstructured noisy data. Taxonomy induction Although detecting hypernymy relationships is an important and difficult task, these systems alone do not produce rich taxonomic graph structures (Camacho-Collados, 2017), and complete taxonomy induction may be seen as a parallel and complementary task. Many works in this area consider a taxonomic graph as the starting point, and consider a variety of methods for growing or discovering areas of the graph. For example, Snow et al. (2006) train a classifier to predict the likelihood of an edge in WordNet, and suggest new undiscovered edges, while Kozareva and Hovy (2010) propose an algorithm which repeatedly crawls for new edges using a web search engine and an initial seed taxonomy. Cimiano et al. (2005) considered learning ontologies using Formal Concept Analysis. Similar works consider noisy graphs discovered from Hearst patterns, and provide algorithms for pruning edges until a strict hierarchy remains (Velardi et al., 2005; Kozareva and Hovy, 2010; Velardi et al., 2013). Maedche and Staab (2001) proposed a method to learn"
P19-1313,J13-3007,0,0.0276963,"growing or discovering areas of the graph. For example, Snow et al. (2006) train a classifier to predict the likelihood of an edge in WordNet, and suggest new undiscovered edges, while Kozareva and Hovy (2010) propose an algorithm which repeatedly crawls for new edges using a web search engine and an initial seed taxonomy. Cimiano et al. (2005) considered learning ontologies using Formal Concept Analysis. Similar works consider noisy graphs discovered from Hearst patterns, and provide algorithms for pruning edges until a strict hierarchy remains (Velardi et al., 2005; Kozareva and Hovy, 2010; Velardi et al., 2013). Maedche and Staab (2001) proposed a method to learn ontologies in a Semantic Web context. Embeddings Recently, works have proposed a variety of graph embedding techniques for representing and recovering hierarchical structure. Order-embeddings (Vendrov et al., 2016) represent text and images with embeddings where the ordering over individual dimensions forms a partially ordered set. Hyperbolic embeddings represent words in hyperbolic manifolds such as the Poincar´e ball and may be viewed as a continuous analogue to tree-like structures (Nickel and Kiela, 2017, 2018). Recently, Tifrea et al."
P19-1313,P18-1025,0,0.0309128,"cy distribution of words appearing in the Hearst pattern corpus (on a log-log scale). Table 1: Hearst patterns used in this study. Patterns are lemmatized, but listed as inflected for clarity. embeddings with ones with a natural hierarchical structure. 2014) to hyperbolic space. In addition, works have considered how distributional co-occurrences may be used to augment order-embeddings (Li et al., 2018) and hyperbolic embeddings (Dhingra et al., 2018). Further methods have focused on the often complex overlapping structure of word classes, and induced hierarchies using box-lattice structures (Vilnis et al., 2018) and Gaussian word embeddings (Athiwaratkun and Wilson, 2018). Compared to many of the purely graph-based works, these methods generally require supervision of hierarchical structure, and cannot learn taxonomies using only unstructured noisy data. Taxonomy induction Although detecting hypernymy relationships is an important and difficult task, these systems alone do not produce rich taxonomic graph structures (Camacho-Collados, 2017), and complete taxonomy induction may be seen as a parallel and complementary task. Many works in this area consider a taxonomic graph as the starting point, and c"
P19-1313,E12-1004,0,\N,Missing
P19-1313,W11-2501,0,\N,Missing
Q17-1002,D13-1202,1,0.763078,"onal modelling there has been increasing importance attributed to grounding semantic models in sensory modalities, e.g., Bruni et al. (2014), Kiela and Bottou (2014). Andrews et al. (2009) demonstrated that multi-modal models formed by combining text-based distributional information with behaviourally generated conceptual properties (as a surrogate for perceptual experience) provide a better proxy for human-like intelligence. However, both the text-based and behaviourallybased components of their model were ultimately derived from linguistic information. Since then, in analyses of brain data, Anderson et al. (2013) have applied multi-modal models incorporating features that are truly grounded in natural image statistics to further support this claim. In addition, Anderson et al. (2015) have demonstrated that visually grounded models describe brain activity associated with internally induced visual features of objects as the ob17 Transactions of the Association for Computational Linguistics, vol. 5, pp. 17–30, 2017. Action Editor: Daichi Mochihashi. Submission batch: 2/2016; Revision batch: 7/2016; Published 1/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license."
Q17-1002,W04-2214,0,0.0431589,"domains (columns), and taxonomic categories (groups of 5 rows). The most concrete half of the words are indicated in bold font. Strike-throughs indicate words for which we did not have semantic model coverage. words in the norms of Barca et al. (2002). They then linked these to WordNet to identify the taxonomic category of the dominant sense of each word. Six taxonomic categories that were heavily populated with abstract words, as well as one unambiguously concrete category, were chosen. All categories supported ample coverage of Law and Music domains (determined according to WordNet Domains (Bentivogli et al., 2004)). Five law words and five music words were selected from each taxonomic category. Taxonomic categories and example stimulus words (translated into English) are as below: Ur-abstract: Anderson et al.’s term for concepts that are classified as abstract in WordNet but do not belong to a clear subcategory, e.g., law or music. At19 tribute: A construct whereby objects or individuals can be distinguished, e.g., legality, tonality. Communication: Something that is communicated by, to or between groups, e.g., accusation, symphony. Event/action: Something that happens at a given place and time, e.g.,"
Q17-1002,P13-1153,0,0.0342749,"on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activity patterns measurable with curren"
Q17-1002,W10-0609,0,0.432764,"sity of Rochester aander41@ur.rochester.edu Douwe Kiela Computer Laboratory University of Cambridge dk427@cam.ac.uk Stephen Clark Massimo Poesio Computer Laboratory School of Computer Science and Electronic Engineering University of Cambridge University of Essex sc609@cam.ac.uk poesio@essex.ac.uk Abstract scanned as participants engage in conceptual tasks. This research has almost exclusively focused on brain activity elicited as participants comprehend concrete nouns as experimental stimuli. Different modelling approaches — predominantly distributional semantic models (Mitchell et al., 2008; Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Carlson et al., 2014) and semantic models based on human behavioural estimation of conceptual features (Palatucci et al., 2009; Sudre et al., 2012; Chang et al., 2010; Bruffaerts et al., 2013; Fernandino et al., 2015) — have elucidated how different brain regions contribute to semantic representation of concrete nouns; however, how these results extend to non-concrete nouns is unknown. Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almo"
Q17-1002,P14-1046,0,0.010655,"computational representations derived from these images in the analysis. Secondary results are that we have exploited rep28 resentational similarity space to build group-level neural representations which better match our inherently group-level computational semantic models. In so doing, this exposes group-level commonalities in neural representation for both concrete and abstract words. Such group-level representations may prove both a useful test-bed for evaluating computational semantic models, as well as a potentially useful information source to incorporate into computational models (see Fyshe et al. (2014) for related work). Finally we have demonstrated that English and Italian text-based models are roughly interchangeable in our neural decoding task. That the English text-based model tended to return marginally higher results on our Italian brain data than the Italian model provides a cautionary note for future studies wishing to use semantic models from different languages to identify culturally specific aspects of neural semantic representation e.g., as a follow up to Zinszer et al. (2016). However we also note that the English Wikipedia data was larger than the corresponding Italian corpus."
Q17-1002,D14-1005,1,0.772322,"er, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current computational models are sufficiently advanced to assist in investigating the representational structure of abstract concepts in the brain. 1 Introduction Since the work of Mitchell et al. (2008), there has been increasing interest in using computational semantic models to interpret neural activity patterns In computational modelling there has been increasing importance attributed to grounding semantic models in sensory modalities, e.g., Bruni et al. (2014), Kiela and Bottou (2014). Andrews et al. (2009) demonstrated that multi-modal models formed by combining text-based distributional information with behaviourally generated conceptual properties (as a surrogate for perceptual experience) provide a better proxy for human-like intelligence. However, both the text-based and behaviourallybased components of their model were ultimately derived from linguistic information. Since then, in analyses of brain data, Anderson et al. (2013) have applied multi-modal models incorporating features that are truly grounded in natural image statistics to further support this claim. In a"
Q17-1002,D15-1293,1,0.410712,"owing two factors. First, the dataset analysed was for a small sample of 67 words, and it is reasonable to conjecture that some of these words are also encoded in modalities other than vision and language. For example, musical words may be encoded in acoustic and motor features (see also Fernandino et al. (2015)). Future work will be necessary to verify that the findings generalise more broadly to words from domains beyond law and music. In work in progress the authors are undertaking more focused analyses on the current dataset, using textual, visual and newly developed audio semantic modes (Kiela and Clark, 2015) to tease apart linguistic, visual and acoustic contributions to semantic representation and how these vary throughout different regions of the brain. A second limitation of the current approach, as pointed out by a reviewer, is that the Google image search algorithm (the workings of which are unknown to the authors) may not perform as well for abstract words as it does for concrete words. Consequently, the visual model may have been handicapped compared to the textual model when decoding neural representations associated with more abstract words. We have no current measure of the degree of th"
Q17-1002,P14-2135,1,0.318578,"-wordout decoding procedure detailed later in Section 4 using the same method as Mitchell et al. (2008): Pearson’s correlation of each voxel’s activity between matched word lists in all scanning run pairs (10 unique run pairs giving 10 correlation coefficients of 68/70 words, where the other 2 words were test words to be decoded) was computed. The mean coefficient was used as stability measure. Voxels associated with the 500 largest stability measures were selected. 3 Semantic Models 3.1 Image-based semantic models Following previous work in multi-modal semantics (Bergsma and Van Durme, 2011; Kiela et al., 2014), we obtain a total of 20 images for each of the stimulus words from Google Images1 . Images from Google have been shown to yield representations that are competitive in quality compared to alternative resources (Bergsma and Van Durme, 2011; Fergus et al., 2005). Image representations are obtained by extracting the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). This approach is similar to e.g., Kriegeskorte (2015), except that we only use the pre-softmax layer, which has bee"
Q17-1002,S12-1019,0,0.0487973,"r41@ur.rochester.edu Douwe Kiela Computer Laboratory University of Cambridge dk427@cam.ac.uk Stephen Clark Massimo Poesio Computer Laboratory School of Computer Science and Electronic Engineering University of Cambridge University of Essex sc609@cam.ac.uk poesio@essex.ac.uk Abstract scanned as participants engage in conceptual tasks. This research has almost exclusively focused on brain activity elicited as participants comprehend concrete nouns as experimental stimuli. Different modelling approaches — predominantly distributional semantic models (Mitchell et al., 2008; Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Carlson et al., 2014) and semantic models based on human behavioural estimation of conceptual features (Palatucci et al., 2009; Sudre et al., 2012; Chang et al., 2010; Bruffaerts et al., 2013; Fernandino et al., 2015) — have elucidated how different brain regions contribute to semantic representation of concrete nouns; however, how these results extend to non-concrete nouns is unknown. Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almost exclusively focuse"
Q17-1002,P08-1001,0,0.0512656,"fMRI experiments were performed in Italian on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activi"
Q17-1002,D10-1103,0,0.0189407,"formed in Italian on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activity patterns measur"
S13-1011,S12-1051,0,0.0212173,"sotropic covariance function (also known as the radial basis function): cov(xi , xj ) = p21 e (xi −xj )T ·(p2 ∗I)−1 ·(xi −xj ) 2 + p23 δij with parameters p1 = 1, p2 = 1, and p3 = 0.01. We found that training for parameters increased overfitting and produced worse results in validation experiments. 3 Submitted Runs We submitted three runs. This is not sufficient for a full evaluation of the new methods we proposed here, but it gives us an inkling of general trends. To choose the composition of the submissions, we used STS 2012 training data for training, and STS 2012 test data for validation (Agirre et al., 2012). The final submitted runs also used some of the STS 2012 test data for training. Basic - With this run we were examining if a simple introduction of syntactic structure can improve over the baseline performance. We trained a GPR combination of the linear and tree kernels (LK-TK) on the MSRpar training data. In validation experiments we found that this data set in general gave the most consistent performance for regression training. Custom - Here we tried to approximate the best training setup for each type of data. We only had training data for OnWN and for this dataset we were able to improv"
S13-1011,S12-1059,0,0.0567482,"Missing"
S13-1011,S12-1088,0,0.0309314,"have been achieved with kernel-based classification algorithms, such as the support vector machine (SVM) (Joachims, 1998), and the methods here can be adapted for use in multiple kernel classification, as in Polajnar et al. (2011). The kernels are combined using Gaussian process regression (GPR) (Rasmussen and Williams, 2006). It is important to note that the combination strategy described here is only a different way of viewing the regressioncombined mixture of similarity measures approach that is already popular in STS systems, including several that participated in previous SemEval tasks (Croce et al., 2012; B¨ar et al., 2012). Likewise, others, such as Croce et al. (2012), have used tree and dependency parse information as part of their systems; however, we use a tree kernel approach based on a novel encoding method introduced by Zanzotto et al. (2011) and from there derive two dependencybased methods. In the rest of this paper we will describe our system, which consists of distributional similarity (Section 2.1), several kernel measures (Section 2.2), and a combination method (Section 2.3). This will be followed by the description of our three submissions (Section 3), and a discussion of the r"
S13-1011,P03-1054,0,0.00431912,"each dependency pair as an ESA vector obtained by searching the ESA collection for the two words in the dependency pair joined by the AND operator. The DGK representation only contains the dependencies that occur in one similarity text or the other, but not in both. 2.3 Regression Each of the kernel measures above is used to calculate a similarity score between a pair of texts. The different similarity scores are then combined using 2 Because many of the datasets contained incomplete or ungrammatical sentences, we had to approximate some parses. The parsing was done using the Stanford parser (Klein and Manning, 2003), which failed on some overly long sentences, which we therefore segmented at conjunctions or commas. Since our methods only compared subtrees of parses, we simply took the union of all the partial parses for a given sentence. 87 Gaussian process regression (GPR) (Rasmussen and Williams, 2006). GPR is a probabilistic regression method where the weights are modelled as Gaussian random variables. GPR is defined by a covariance function, which is akin to the kernel function in the support vector machine. We used the squared exponential isotropic covariance function (also known as the radial basis"
S13-1011,W11-1302,0,0.1931,"Missing"
W13-2609,N09-1003,0,0.131669,"relations between word concepts and have achieved impressive performance in related NLP tasks (Sahlgren, 2006; Turney & Pantel, 2010). In these studies, however, it is not always clear exactly which semantic relation is best reflected by the implemented models. Indeed, research has shown that by changing certain parameter settings in the standard VSM architecture, models can be adapted to better reflect one relation type or another. Specifically, models with smaller context windows are reportedly better at reflecting similarity, whereas models with larger windows better reflect association. (Agirre et al., 2009; Peirsman et al., 2008) Our experiments in this section aim first to corroborate these findings by testing how models of varying context window sizes perform on empirical data of both association and similarity. We then test if this effect differentially affects performance on concrete and abstract words. 3.2 3.3 Results In line with previous studies, we observed that VSMs with smaller window sizes were better able to predict similarity. The model with window size 3 achieves a higher correlation with similarity (Spearman rank rs = -0.29) than the model with window size 9 (rs = -0.25). However"
W13-2609,P94-1019,0,0.102124,"igan (1968) or Toglia and Battig (1978). In both cases contributors were asked to rate words based on a scale of 1 (very abstract) to 7 (very concrete). 1 We extracted the all 2,230 nouns from the USF data for which concreteness scores were known, yielding a total of 15,195 noun-noun pairs together with concreteness and association values. Although some empirical word-similarity datasets are publically available, they contain few if any abstract words (Finkelstein et al., 2002; Rubenstein & Goodenough, 1965). Therefore to evaluate similarity modeling, we use Wu-Palmer Similarity (similarity) (Wu & Palmer, 1994), a word similarity metric based on the position of the senses of two words in the WordNet taxonomy (Felbaum, 1998). similarity can be applied to both abstract and concrete nouns and achieves a high correlation, with human similarity judgments (Wu & Palmer, 1994).2 Motivation VSMs are well established as a method of quantifying relations between word concepts and have achieved impressive performance in related NLP tasks (Sahlgren, 2006; Turney & Pantel, 2010). In these studies, however, it is not always clear exactly which semantic relation is best reflected by the implemented models. Indeed,"
W13-2609,D11-1098,0,\N,Missing
W13-2609,C94-1103,0,\N,Missing
W13-2609,D11-1063,0,\N,Missing
W14-1503,S13-1035,0,0.0271096,"e also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a corpus with 106 word types and 107 tokens. For examining the dependency-based contexts, we include the Google Syntactic N-gram corpus (Goldberg and Orwant, 2013), with 107 types and 1011 tokens. 2.1 Context There are two main approaches to modelling context: window-based and dependencybased. For window-based methods, contexts are determined by word co-occurrences within a window of a given size, where the window simply spans a number of words occurring around instances of a target word. For dependency-based methods, the contexts are determined by word co-occurrences in a particular syntactic relation with a target word (e.g. target word dog is the subject of run, where run subj is the context). We consider different window sizes and compare window-bas"
W14-1503,D12-1050,0,0.0142917,"f parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate"
W14-1503,P12-1015,0,0.025116,"n the literature, but a typical value is in the low thousands. Here we consider vector sizes ranging from 50,000 to 500,000, to see whether larger vectors lead to better performance. Table 1: Datasets for evaluation (Landauer and Dumais, 1997). There is a risk that semantic similarity studies have been overfitting to their idiosyncracies, so in this study we evaluate on a variety of datasets: in addition to WordSim353 (W353) and TOEFL, we also use the Rubenstein & Goodenough (RG) (1965) and Miller & Charles (MC) (1991) data, as well as a much larger set of similarity ratings: the MEN dataset (Bruni et al., 2012). All these datasets consist of human similarity ratings for word pairings, except TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These"
W14-1503,W13-2608,0,0.146911,"al weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. Introduction Vector space models (VSMs) represent the meanings of lexical items as vectors in a “semantic space”. The benefit of"
W14-1503,J07-4004,1,0.5597,"pears to work best (given the small window size). 3.4 Feature granularity Stemming and lemmatisation are standard techniques in NLP and IR to reduce data sparsity. However, with large enough corpora it may be that the loss of information through generalisation hurts performance. In fact, it may be that increased granularity – through the use of grammatical tags – can lead to improved performance. We test these hypotheses by comparing four types of processed context words: lemmatised, stemmed, POS-tagged, and tagged with CCG lexical categories (which can be thought of as fine-grained POS tags (Clark and Curran, 2007)).4 The source corpora are BNC and ukWaC, using a windowbased method with windows of size 5, Positive Mutual Information weighting, vectors of size 50,000 and Cosine similarity. The results are reported in Figure 4. The ukWaC-generated vectors outperform the BNC-generated ones on all but a single instance for each of the granularities. Stemming yields the best overall performance, and increasing the granularity does not lead to better results. Even with a very large corpus like ukWaC, stemming yields signficantly better results than not reducing the feature granularity at all. Conversely, apar"
W14-1503,W02-0908,0,0.307509,"sent a systematic study of parameters used in the construction of semantic vector space models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks."
W14-1503,J07-2002,0,0.919036,"ace models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window si"
W14-1503,P02-1030,0,0.0373706,"sent a systematic study of parameters used in the construction of semantic vector space models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks."
W14-1503,S13-1038,0,0.0100052,"Missing"
W14-1503,C04-1146,0,0.386865,"ent some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vect"
W14-1503,2014.lilt-9.5,0,\N,Missing
W14-1503,W13-2609,1,\N,Missing
W18-3221,W18-3219,0,0.175796,"Missing"
W18-3221,L18-1550,0,0.0418527,"Missing"
W18-3221,N16-1030,0,0.353212,"s “solved”, performance deteriorates proportional to the degree of code-switching in the data. The shared task for the third workshop on Computational Approaches on Linguistic Code-Switching concerned named entity recognition (NER) for two code-switched language pairs (Aguilar et al., 2018): Modern Standard Arabic and Egyptian (MSA-EGY); and English-Spanish (ENG-SPA). Here, we describe our work on the shared task. Traditional NER systems used to rely heavily on hand-crafted features and gazetteers, but have since been replaced by neural architectures that combine bidirectional LSTMs and CRFs (Lample et al., 2016). Equipped with supervised characterlevel representations and pre-trained unsupervised word embeddings, such neural architectures have not only come to dominate named entity recognition, but have also successfully been applied to code-switched language identification (Samih et al., 2016), which makes them highly suitable for the current task as well. 2 Approach The input data consists of noisy user-generated social media text collected from Twitter. Codeswitching can occur between different tweets in the training data, with many tweets being monolingual, but can also occur within tweets (e.g."
W18-3221,P16-1101,0,0.0380217,"er • O: Any other token that is not an NE 154 Proceedings of The Third Workshop on Computational Approaches to Code-Switching, pages 154–158 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics The train/valid/test split for MSA-EGY was 10102/1122/1110. The train/valid/test split for ENG-SPA was 50757/832/15634. The first work to combine CRFs with modern neural representation learning for NER is, to our knowledge, by Collobert et al. (2011). Our architecture is similar to more recent neural architectures for NER, e.g. Huang et al. (2015); Lample et al. (2016); Ma and Hovy (2016). Instead of using a straightforward bidirectional LSTM (BiLSTM), we use several layers and add shortcut connections. Instead of simply feeding in word (and/or character) embeddings, we add a selfattention mechanism. 2.1 is, we combine the language-specific word embeddings wL1 and wL2 with the character-level word representation via a simple self-attention mechanism: αi = softmax(U tanh(V [wL1 , wL2 , wchar ])), wword+char = [α1 wL1 , α2 wL2 , α3 wchar ] 2.2 Capitalization Additionally, we concatenate an embedding to indicate the capitalization of the word, which be either no-capitals, startin"
W18-3221,W17-5308,0,0.0581651,"Missing"
W18-3221,W16-5806,0,0.051127,"8): Modern Standard Arabic and Egyptian (MSA-EGY); and English-Spanish (ENG-SPA). Here, we describe our work on the shared task. Traditional NER systems used to rely heavily on hand-crafted features and gazetteers, but have since been replaced by neural architectures that combine bidirectional LSTMs and CRFs (Lample et al., 2016). Equipped with supervised characterlevel representations and pre-trained unsupervised word embeddings, such neural architectures have not only come to dominate named entity recognition, but have also successfully been applied to code-switched language identification (Samih et al., 2016), which makes them highly suitable for the current task as well. 2 Approach The input data consists of noisy user-generated social media text collected from Twitter. Codeswitching can occur between different tweets in the training data, with many tweets being monolingual, but can also occur within tweets (e.g. “[USER]: en los finales be like [URL]”) or even morphologically within words (e.g. “pero esta twitteando y pitchandome los textos”). The goal is to predict the correct IOB entity type for the following categories: • [BI]-PER: Person • [BI]-LOC: Location • [BI]-ORG: Organization • [BI]-GR"
