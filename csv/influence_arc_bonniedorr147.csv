1994.amta-1.20,P89-1018,0,0.0655957,"his ordering information is encoded in the grammar network by virtue of the relative ordering of integer id&apos;s associated with network links. Other types of parameters encoded in the grammar network are those pertaining to basic categories, pre-terminal categories (e.g., determiner), potential specifiers, and adjuncts for each basic category. 3.2. Trace Theory In general, NP and CP nodes are considered to be barriers to movement. However, Korean allows the head noun of a relative clause to be construed with the empty category across more than one intervening CP node, as shown in the following: (2) [CP [CP t1 t2 kyengyengha-ten] hoysa2-ka manghayperi-n] Bill1-un yocum uykisochimhay issta managed-Rel company-Norn is bankrupt-Rel -Top these days is depressed &apos;Bill is such a person that the company which was managed by him has been bankrupt, and he is depressed these days&apos; The subject NP &apos;Bill&apos; is coindexed with the trace in the more deeply embedded relative clause. If we assume, following Chomsky (1986a), that relative clause formation involves movement from an inner clause into an outer subject position, then the grammaticality of the above example suggests that the Trace theory must be"
1994.amta-1.20,P90-1017,1,0.888883,"Missing"
1994.amta-1.20,1993.tmi-1.14,0,0.0957369,"Missing"
1994.amta-1.20,P90-1015,0,0.0644115,"Missing"
1994.amta-1.20,P93-1016,1,0.872256,"Missing"
1994.amta-1.6,P94-1005,0,0.0567189,"Missing"
1994.amta-1.6,C94-1057,0,0.0285998,"Missing"
1994.amta-1.6,C94-1038,0,0.0525107,"Missing"
1997.mtsummit-workshop.13,A97-1021,1,0.813412,"tersection on the classes described in Palmer, Rosenzweig, and Dang. We use the syntactic properties of the intersective sets to evaluate the polysemy reduction of Palmer, et al. (1997). Our future work will involve an exploration into the mapping of WordNet and Levin’s classes on a large scale in collaboration with Rosenzweig, Palmer and Dang. 2 Experiments In these experiments we draw upon a hand-crafted database of Levin verbs with WordNet sense tags created for automatic lexicon acquisition at the University of Maryland (Dorr and Jones, 1996b; Dorr and Jones, 1996a; Dorr and Jones, 1996c; Dorr, 1997). Levin verbs were tagged by hand with a set of WordNet senses, presented to the user as a set of logical addresses (e.g., 1-7) which are converted internally into WordNet addresses (e.g., 00416048-00416054). This process took a single person-month, with the aid of an interface for typing in human semantic judgments on the words in context. Each verb has between 1 100 and 9 senses, with an average of 2.5 senses per word.2 We ran four experiments which we describe below, detailing examples from a small set of classes and comparing them with the results of Rosenzweig, et al. Experiment 1: We gen"
1997.mtsummit-workshop.13,W96-0306,1,0.893129,"October 28, 1997] Using WordNet to Posit Hierarchical Structure in Levin&apos;s Verb Classes Mari Broman Olsen, Bonnie J. Dorr, and David J. Clark Institute for Advanced Computer Studies University of Maryland College Park, MD, USA 20742 {molsen,bonnie,dav}@umiacs.umd.edu Abstract In this paper we report on experiments using WordNet synset tags to evaluate the semantic properties of the verb classes cataloged by Levin (1993). This paper represents ongoing research begun at the University of Pennsylvania (Rosenzweig and Dang, 1997; Palmer, Rosenzweig, and Dang, 1997) and the University of Maryland (Dorr and Jones, 1996b; Dorr and Jones, 1996a; Dorr and Jones, 1996c). Using WordNet sense tags to constrain the intersection of Levin classes, we avoid spurious class intersections introduced by homonymy and polysemy (run a bath, run a mile). By adding class intersections based on a single shared sense-tagged word, we minimize the impact of the non-exhaustiveness of Levin’s database (Dorr and Olsen, 1996; Dorr, To appear). By examining the syntactic properties of the intersective classes, we provide a clearer picture of the relationship between WordNet/EuroWordNet and the LCS interlingua for machine translation a"
1997.mtsummit-workshop.3,A97-1021,1,0.861671,"ingual database with basic semantic relations between words for several European languages (Dutch, English, French, German, Italian, and Spanish) (Calzolari et al., To appear). Each individual WordNet will be linked to definitions in the English WordNet for English (Miller, 1986; Miller, 1990; Miller and Fellbaum, 1991). One component of this project involves the development of links between verb definitions in English WordNet and Spanish WordNet. Researchers at the University of Barcelona have built these links automatically (Castellón et al., 1997) by importing the English verb database of (Dorr, 1997) into Spanish, using an intermediate Spanish-English bilingual lexicon produced at the University of Maryland. The imported database was hand-checked by a native Spanish speaker; the results of verification are reported herein. One of our objectives is to incorporate Spanish WordNet into an existing interface called “Periscope”, developed by NOVELL, that allows a user to browse through definitions bilingually. A snapshot between the Spanish verb matar and its Dutch equivalents, uitmoorden and kapotmaken, is given in Appendix A. The links in this snapshot (marked by a very faint gray line) indi"
1997.mtsummit-workshop.3,P97-1020,1,0.840812,"nce during this process, perhaps allowing this second sentence to be selected— for reasons of economy—when the manner of motion can be inferred from the prototypical subject or object. 6 Again, the subject of the sentence seems to play a role in the acceptability of this sentence. If él (= he) were used in place of el soldado, this sentence would be perfectly acceptable. See related footnote 5. 7 The LCS given here is based on templates developed in 1996. More recently, we have refined the LCS templates to include activities (ACT), so that the entry for march has changed from GO to ACT. (See (Dorr and Olsen, 1997) for more details.) However, the basic mechanism for lexical selection via WordNet links is still applicable to the modified representation, which assumes ACT to be a degenerate case of GO. 26 Figure 2: Mapping LCS into Target Language by means of Cross-language WordNet Entries 27 These three components are associated with three English words in our LGS lexicon; the English words, in turn, are linked directly to their Spanish WordNet counterparts (march(ar/ando), a través, and atravesar).8 This direct-link method of selecting target-language candidates is more efficient than an earlier LCS-bas"
1997.mtsummit-workshop.4,P97-1020,1,0.891093,"1988; Olsen, 1994; Pustejovsky, 1991; Smith, 1991; Verkuyl, 1972; Verkuyl, 1993), verbs appear to have multiple aspectual types, depending on (and seemingly “coerced” by) the presence of other sentential constituents, such as those introduced by the alternations cataloged by Levin (1993). Olsen constrains this “type-shifting” by allowing only monotonic composition of privative lexical aspect features (+/Ø instead of +/-/Ø); features are added but not deleted. For instance, a state may become an activity (by adding [+dynamic]), and an activity may become an accomplishment (by adding [+telic]). Dorr and Olsen (1997) investigate the relationship between the lexical aspect of verbs, alone and in sentential contexts, and LCSs whose construction is guided by the alternations in Levin (1993). A simple example of the relationship is shown by the Fill Verbs (Class 9.8 (Levin, 1993)), which appear in the atelic state LCS in (4), as well as the telic event LCS in (5).2 (4) Tinsel covered the tree. (be ident (* thing 2) (at ident (thing 2) (!!-ed 9)) (with poss (*head*) (* thing 16))) (5) I covered the tree with tinsel. of lexicons for multiple languages for NLP applications; see, e.g., (Dorr, 1997; Dorr, To appea"
1997.mtsummit-workshop.4,J88-2003,0,0.0798589,"n LCS and lexical aspect. 1 It should also be noted that using the LCS-based approach supports large-scale acquisition 34 Table 1: Privative Featural Identification of Aspectual Classes 3 Lexical Aspect Following Olsen (1997), we use lexical aspect to refer to the situation type denoted by the verb alone, or combined with other sentential constituents. Verbs are assigned to lexical aspect classes, as in Table 1, based on their behavior in a variety of syntactic and semantic frames that focus on these features. (Olsen, 1997, pp. 32-33). As a number of researchers have pointed out (Dowty, 1979; Moens and Steedman, 1988; Olsen, 1994; Pustejovsky, 1991; Smith, 1991; Verkuyl, 1972; Verkuyl, 1993), verbs appear to have multiple aspectual types, depending on (and seemingly “coerced” by) the presence of other sentential constituents, such as those introduced by the alternations cataloged by Levin (1993). Olsen constrains this “type-shifting” by allowing only monotonic composition of privative lexical aspect features (+/Ø instead of +/-/Ø); features are added but not deleted. For instance, a state may become an activity (by adding [+dynamic]), and an activity may become an accomplishment (by adding [+telic]). Dorr"
1997.mtsummit-workshop.4,A97-1021,1,\N,Missing
2003.mtsummit-systems.9,P00-1059,0,0.012259,"., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998). WordNet is the most well-developed and widely used lexical database of English (Fellbaum, 1998). In WordNet, both types of lexical relations are specified among words with the same part of speech (verbs, nouns, adjectives and adverbs). WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al., 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). In the context of these projects, one criticism of WordNet is its lack of cross-categorial links, such as verb-noun or noun-adjective relations. Mel’ˇcuk approaches lexical relations by defining a lexical combinatorial zone that specifies semantically related lexemes through Lexical Functions (LF). These functions define a correspondence between a key lexical item and a set of related lexical items (Mel’ˇcuk, 1988). There are two types of functions: paradigmatic and syntagmatic (Ramos et al., 1994). Paradigmatic LFs associate a lexical item with related lexical items. The relation can be sem"
2003.mtsummit-systems.9,dorr-etal-2000-building,1,0.840179,"(Habash and Dorr, 2003). The CatVar is web-browseable at http://clipdemos.umiacs.umd.edu/catvar/. 2 Background Lexical relations describe relative relationships among different lexemes. Lexical relations are either hierarchical taxonomic relations (such as hypernymy, hyponymy and entailments) or nonhierarchical congruence relations (such as identity, overlap, synonymy and antonymy) (Cruse, 1986). Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) and HowNet (Dong, 2000) (among others) have inspired the work of many researchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998). WordNet is the most well-developed and widely used lexical database of English (Fellbaum, 1998). In WordNet, both types of lexical relations are specified among words with the same part of speech (verbs, nouns, adjectives and adverbs). WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al., 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000"
2003.mtsummit-systems.9,P01-1032,1,0.829252,"2000) (among others) have inspired the work of many researchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resnik, 1999; Hearst, 1998). WordNet is the most well-developed and widely used lexical database of English (Fellbaum, 1998). In WordNet, both types of lexical relations are specified among words with the same part of speech (verbs, nouns, adjectives and adverbs). WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al., 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). In the context of these projects, one criticism of WordNet is its lack of cross-categorial links, such as verb-noun or noun-adjective relations. Mel’ˇcuk approaches lexical relations by defining a lexical combinatorial zone that specifies semantically related lexemes through Lexical Functions (LF). These functions define a correspondence between a key lexical item and a set of related lexical items (Mel’ˇcuk, 1988). There are two types of functions: paradigmatic and syntagmatic (Ramos et al., 1994)."
2003.mtsummit-systems.9,W02-2125,1,0.804117,"o pun˜ aladas a John (literally, ‘Mary gave stabs to John’) being translated into Mary stabbed John. In GHMT, the input SL dependency structure is maintained while all words are translated to TL. Generating a conflated version of the input is conditional upon the existence of a categorial variant of a TL word that satisfies lexical semantic and thematic consistency con&quot; # straints. For example,  0 is a categorial variant   # of  and it maintains   ’s thematic role in the example above as ,   . Details on the databases used to verify the additional constraints are available in (Habash, 2002). 5 Conclusions and Future Work We have presented our approach to constructing a new large-scale database containing categorial vari6 See (Habash and Dorr, 2003) for more details about the other two applications. ations of English words. Future work includes improving the word-cluster ratio and absorbing more of the single-word clusters into existing clusters or other single-word clusters. We are also considering enrichment of the clusters with types of derivational relations such as “nominal-event” or “doer” to complement part-of-speech labels. Other lexical semantic features such telicity, s"
2003.mtsummit-systems.9,N03-1013,1,0.149995,"le to the research community. We expect that the contribution of this resource will become more widely recognized through its future incorporation into additional NLP applications. For example, it is the intention of UMD researchers and WordNet 1.7 developers to use CatVar information for more rapid development and extension of WordNet and mutual validation of both resources. This paper discusses other available resources and how they differ from the CatVar database. We then discuss how and what resources were used to build CatVar. For a more detailed discussion and evaluation of CatVar, see (Habash and Dorr, 2003). The CatVar is web-browseable at http://clipdemos.umiacs.umd.edu/catvar/. 2 Background Lexical relations describe relative relationships among different lexemes. Lexical relations are either hierarchical taxonomic relations (such as hypernymy, hyponymy and entailments) or nonhierarchical congruence relations (such as identity, overlap, synonymy and antonymy) (Cruse, 1986). Resources specifying the relations among lexical items such as WordNet (Fellbaum, 1998) and HowNet (Dong, 2000) (among others) have inspired the work of many researchers in NLP (Carpuat et al., 2002; Dorr et al., 2000; Resn"
2003.mtsummit-systems.9,habash-dorr-2002-handling,1,0.87938,"Missing"
2003.mtsummit-systems.9,P98-1116,0,0.0144195,"Missing"
2003.mtsummit-systems.9,P96-1004,0,0.027651,"Missing"
2003.mtsummit-systems.9,J93-2004,0,0.0239844,"Missing"
2003.mtsummit-systems.9,C92-3145,0,\N,Missing
2003.mtsummit-systems.9,C00-1007,0,\N,Missing
2003.mtsummit-systems.9,P02-1040,0,\N,Missing
2003.mtsummit-systems.9,C98-1112,0,\N,Missing
2003.mtsummit-systems.9,dorr-etal-2002-duster,1,\N,Missing
2006.amta-papers.25,W05-0909,0,0.951185,"atthew Snover and Bonnie Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD 20742 {snover,bonnie}@umiacs.umd.edu Richard Schwartz, Linnea Micciulla, and John Makhoul BBN Technologies 10 Moulton Street Cambridge, MA 02138 {schwartz,lmicciul,makhoul}@bbn.com Abstract most widely used of which is BLEU (Papineni et al., 2002), an evaluation metric that matches ngrams from multiple references. A variant of this metric, typically referred to as the “NIST” metric, was proposed by Doddington (Doddington, 2002). Other proposed methods for MT evaluation include METEOR (Banerjee and Lavie, 2005), which uses unigram matches on the words and their stems, and a linear combination of automatic MT evaluation methods along with meaning-based features for identifying paraphrases (Russo-Lassner et al., 2005).1 We define a new, more intuitive measure of “goodness” of MT output—specifically, the number of edits needed to fix the output so that it semantically matches a correct translation. In a less expensive variant, we attempt to avoid the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. We also seek to achieve higher correlations with"
2006.amta-papers.25,A94-1016,0,0.0221448,"s of our experiments, comparing TER and HTER to BLEU and METEOR (and their human-targeted variants, HBLEU and HMETEOR). We compare these measures against human judgments of the fluency and adequateness of the system output. Finally, we conclude with a summary of results and future work. 2 Related Work The first attempts at MT evaluation relied on purely subjective human judgments (King, 1996). Later work measured MT error by post editing MT output and counting the number of edits, typically measured in the number of keystrokes to convert the system output into a “canonical” human translation (Frederking and Nirenburg, 1994). Attempts have been made to improve MT performance by automatic post-editing techniques (Knight and Chander, 1994). Post editing measures have also been shown effective for text summarization evaluation (Mani et al., 2002) and natural language generation (Sripada et al., 2004). is the definitive MT measure. The authors make no such claim, and have adopted the name Translation Edit Rate for use in this paper and the wider community. When developing MT systems, a purely automatic measure of accuracy is preferred for rapid feedback and reliability. Purely human based evaluation metrics fail in t"
2006.amta-papers.25,niessen-etal-2000-evaluation,0,0.434114,"Missing"
2006.amta-papers.25,P02-1040,0,0.118133,"stems, a purely automatic measure of accuracy is preferred for rapid feedback and reliability. Purely human based evaluation metrics fail in this regard and have largely been replaced by purely automatic MT evaluations. Automatic MT evaluation has traditionally relied upon string comparisons between a set of reference translations and a translation hypothesis. The quality of such automatic measures can only be determined by comparisons to human judgments. One difficulty in using these automatic measures is that their output is not meaningful except to compare one system against another. BLEU (Papineni et al., 2002) calculates the score of a translation by measuring the number of ngrams, of varying length, of the system output that occur within the set of references. This measure has contributed to the recent improvement in MT systems by giving developers a reliable, cheap evaluation measure on which to compare their systems. However, BLEU is relatively unintuitive and relies upon a large number of references and a large number of sentences in order to correlate with human judgments. METEOR (Banerjee and Lavie, 2005) is an evaluation measure that counts the number of exact word matches between the system"
2006.amta-papers.25,2003.mtsummit-papers.51,0,0.150991,"mber of references and a large number of sentences in order to correlate with human judgments. METEOR (Banerjee and Lavie, 2005) is an evaluation measure that counts the number of exact word matches between the system output and reference. Unmatched words are then stemmed and matched. Additional penalities are assessed for reordering the words between the hypothesis and reference. This method has been shown to correlate very well with human judgments. An MT scoring measure that uses the notion of maximum matching string (MMS) has been demonstrated to yield high correlations with human judges (Turian et al., 2003). The MMS method is similar to the approach used by TER, in that it only allows a string to be matched once, and also permits string reordering. The MMS approach explicitly favors long contiguous matches, whereas TER attempts to minimize the number of edits between the reference and the hypothesis. TER assigns a lower cost to phrasal shifts than MMS, and does not explicitly favor longer matching strings. 224 3 Definition of Translation Edit Rate TER is defined as the minimum number of edits needed to change a hypothesis so that it exactly matches one of the references, normalized by the averag"
2006.amta-papers.7,E06-1032,0,0.0204439,"guistic Data Consortium (LDC). We use an Arabic-English parallel corpus of about 5 million words to train the translation model.5 For Arabic preprocessing, the Arabic Treebank scheme is used (Habash and Sadat, 2006). All systems use the same surface trigram language model, trained on approximately 340 million words of English newswire text from the English Gigaword corpus.6 English preprocessing simply included downcasing, separating punctuation from words and splitting off “’s”. Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Both BLEU (Papineni et al., 2002; Callison-Burch et al., 2006) and NIST (Doddington, 2002) metric scores are reported. All scores are computed against four references with n-grams of maximum length four. As a post-processing step, the translations of all systems are true-cased, and all results reported below refer to the case-sensitive BLEU and NIST scores. We conducted three sets of evaluations that explore different aspects of the data sets and the system variants: a full system evaluation, a genre-specific evaluation, and a qualitative evaluation of specific linguistic phenomena. 6.1 Full Evaluation Six system variants are compared: • G IST is a simpl"
2006.amta-papers.7,P05-1066,0,0.16191,"rphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts with preprocessing decisions. Within our approach, working with Arabic morphology is especially challenging. We discuss this issue in more detail in Section 3. 2.2 MT Hybridization More recently a number of statistical MT approaches included syntactic information as part of the preprocessing phase, the decoding phase or the n-best rescoring phase. Collins et al. (2005) incorporated syntactic information as part of preprocessing the parallel corpus. A series of transformations on the source parse trees were applied to make the order of the sourcelanguage words and phrases closer to that of the target language. The same reordering was done for a new source sentence before decoding. They showed a modest statistically significant improvement over basic phrase-based MT. Quirk et al. (2005) used sub-graphs of dependency trees to deal with word-order differences between the source and the target language. During training, dependency graphs on the source side were"
2006.amta-papers.7,P97-1003,0,0.0433478,"and training data size. Symbolic MT approaches tend to capture more abstract generalizations about the languages they translate between compared to statistical MT. This comes at a cost of being more complex than statistical MT, involving more human effort, and depending on already existing resources for morphological analysis and parsing. This dependence on existing resources highlights the problem of variation in morphological representations for Arabic. In a typical situation, the in58 put/output text of an MT system is in simple whitespace tokenization. But, a statistical parser (such as (Collins, 1997) or (Bikel, 2002)) trained out-of-thebox on the Penn Arabic Treebank (Maamouri et al., 2004) assumes the same kind of tokenization it uses (4-way normalized segments into conjunction, particle, word and pronominal clitic). This means a separate tokenizer is needed to convert input text to this representation (Habash and Rambow, 2005; Diab et al., 2004). An additional issue with a treebank-trained statistical parser is that its input/output is in normalized segmentation that does not contain morphological information such as features or lexemes that are important for translation. Arabic-English"
2006.amta-papers.7,N04-4038,0,0.0379168,"endence on existing resources highlights the problem of variation in morphological representations for Arabic. In a typical situation, the in58 put/output text of an MT system is in simple whitespace tokenization. But, a statistical parser (such as (Collins, 1997) or (Bikel, 2002)) trained out-of-thebox on the Penn Arabic Treebank (Maamouri et al., 2004) assumes the same kind of tokenization it uses (4-way normalized segments into conjunction, particle, word and pronominal clitic). This means a separate tokenizer is needed to convert input text to this representation (Habash and Rambow, 2005; Diab et al., 2004). An additional issue with a treebank-trained statistical parser is that its input/output is in normalized segmentation that does not contain morphological information such as features or lexemes that are important for translation. Arabic-English dictionaries use lexemes and proper translation of features, such as number and tense, requires access to these features in both source and target languages. As a result, additional conversion is needed to relate the normalized segmentation to the lexeme and feature level. Of course, in principle, the treebank and parser could be modified to be at the"
2006.amta-papers.7,H05-1085,0,0.0456068,"and to butter (Dorr, 1993). The overgeneration is constrained by multiple statistical targetlanguage models including surface n-grams and structural n-grams. The source-target asymmetry of systems developed in this approach makes them more easily retargetable to new source languages (provided a source-language parser and translation dictionary). In this paper, we describe these two specific extensions for Arabic in detail (Section 4). SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005). These studies examined the effects of various kinds of tokenization, lemmatization and part-of-speech (POS) tagging and showed a positive effect on SMT quality. Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations of Arabic. Her results showed that morphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts wi"
2006.amta-papers.7,habash-dorr-2002-handling,1,0.840173,"T implementations in Section 3. Section 4 describes the Arabic components of our basic GHMT system. Section 5 describes the extensions we made to integrate SMT components into the GHMT system. Section 6 presents three evaluations of multiple MT system variants. 2 Previous Work We discuss research related to our approach in the areas of generation-heavy MT and MT hybridization. 2.1 Generation-Heavy MT GHMT is an asymmetrical hybrid approach that addresses the issue of MT resource poverty in source-poor/target-rich language pairs by exploiting symbolic and statistical target-language resources (Habash and Dorr, 2002; Habash, 2003a; Habash, 56 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 56-65, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas 2003b). Expected source-language resources include a syntactic parser and a simple one-to-many translation dictionary. No transfer rules or complex interlingual representations are used. Rich targetlanguage symbolic resources such as word lexical semantics, categorial variations and subcategorization frames are used to overgenerate multiple structural variations from a target-"
2006.amta-papers.7,P05-1071,1,0.921181,"sis and parsing. This dependence on existing resources highlights the problem of variation in morphological representations for Arabic. In a typical situation, the in58 put/output text of an MT system is in simple whitespace tokenization. But, a statistical parser (such as (Collins, 1997) or (Bikel, 2002)) trained out-of-thebox on the Penn Arabic Treebank (Maamouri et al., 2004) assumes the same kind of tokenization it uses (4-way normalized segments into conjunction, particle, word and pronominal clitic). This means a separate tokenizer is needed to convert input text to this representation (Habash and Rambow, 2005; Diab et al., 2004). An additional issue with a treebank-trained statistical parser is that its input/output is in normalized segmentation that does not contain morphological information such as features or lexemes that are important for translation. Arabic-English dictionaries use lexemes and proper translation of features, such as number and tense, requires access to these features in both source and target languages. As a result, additional conversion is needed to relate the normalized segmentation to the lexeme and feature level. Of course, in principle, the treebank and parser could be m"
2006.amta-papers.7,N06-2013,1,0.900599,"SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005). These studies examined the effects of various kinds of tokenization, lemmatization and part-of-speech (POS) tagging and showed a positive effect on SMT quality. Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations of Arabic. Her results showed that morphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts with preprocessing decisions. Within our approach, working with Arabic morphology is especially challenging. We discuss this issue in more detail in Section 3. 2.2 MT Hybridization More recently a number of statistical MT approaches included syntactic information as part of the preprocessing phase, the decoding phase or the n-best rescoring phase. Collins et al. (2005) incorporated syntactic information as part of preprocessing the"
2006.amta-papers.7,koen-2004-pharaoh,0,0.392152,"ic N/ap N/ap N Nap writer;author clerk authors;writers authors;writers PV IV PV_Pass IV_Pass_yu write write be written;be fated;be destined be written;be fated;be destined kuwfiy˜_1 AJ Kufic/from_Kufa/of_Kufa kAtib_1 N katab-u_1 V author/clerk/writer be_destined/be_fated/ be_written/destine/fate/write 5 Integration of SMT Components into GHMT The main challenge for integrating SMT components into GHMT is that the conception of the phrase (anything beyond a single word) is radically different. Phrase-based SMT systems take a phrase to be a sequence of words with no hidden underlying structure (Koehn, 2004). On the other hand, for systems that use parsers, like GHMT, a phrase has a linguistic structure that defines it and its behavior in a bigger context. Both kinds come with problems. Statistical phrases are created from alignments, which may not be clean. This results in jagged edges to many phrases. For example, the phrase . on the other hand , the (containing seven words starting with a period and ending with “the”) overlaps multiple linguistic phrase boundaries. Another related phenomenon is that of statistical hallucination, e.g., the translation of AlswdAn w (literally, Sudan and) into en"
2006.amta-papers.7,N04-4015,0,0.0389125,"stems developed in this approach makes them more easily retargetable to new source languages (provided a source-language parser and translation dictionary). In this paper, we describe these two specific extensions for Arabic in detail (Section 4). SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005). These studies examined the effects of various kinds of tokenization, lemmatization and part-of-speech (POS) tagging and showed a positive effect on SMT quality. Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations of Arabic. Her results showed that morphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts with preprocessing decisions. Within our approach, working with Arabic morphology is especially challenging. We discuss this issue in more detail in Section 3. 2.2 MT Hybridiz"
2006.amta-papers.7,P03-1021,0,0.016325,"Missing"
2006.amta-papers.7,P02-1040,0,0.0743345,"available from the Linguistic Data Consortium (LDC). We use an Arabic-English parallel corpus of about 5 million words to train the translation model.5 For Arabic preprocessing, the Arabic Treebank scheme is used (Habash and Sadat, 2006). All systems use the same surface trigram language model, trained on approximately 340 million words of English newswire text from the English Gigaword corpus.6 English preprocessing simply included downcasing, separating punctuation from words and splitting off “’s”. Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Both BLEU (Papineni et al., 2002; Callison-Burch et al., 2006) and NIST (Doddington, 2002) metric scores are reported. All scores are computed against four references with n-grams of maximum length four. As a post-processing step, the translations of all systems are true-cased, and all results reported below refer to the case-sensitive BLEU and NIST scores. We conducted three sets of evaluations that explore different aspects of the data sets and the system variants: a full system evaluation, a genre-specific evaluation, and a qualitative evaluation of specific linguistic phenomena. 6.1 Full Evaluation Six system variants ar"
2006.amta-papers.7,popovic-ney-2004-towards,0,0.0433443,"Missing"
2006.amta-papers.7,P05-1034,0,0.104069,"tion More recently a number of statistical MT approaches included syntactic information as part of the preprocessing phase, the decoding phase or the n-best rescoring phase. Collins et al. (2005) incorporated syntactic information as part of preprocessing the parallel corpus. A series of transformations on the source parse trees were applied to make the order of the sourcelanguage words and phrases closer to that of the target language. The same reordering was done for a new source sentence before decoding. They showed a modest statistically significant improvement over basic phrase-based MT. Quirk et al. (2005) used sub-graphs of dependency trees to deal with word-order differences between the source and the target language. During training, dependency graphs on the source side were projected onto the target side by using the alignment links between words in the two languages. The use of syntactic information is the main difference between their approach and phrase-based statistical MT approaches. During decoding, the different subgraphs were combined in order to generate the most likely dependency tree. This approach has been shown to provide significant improvements over a Research into MT hybrids"
2006.amta-papers.7,2006.amta-papers.25,1,0.654799,"more genre-independent than SMT approaches. We believe this is a result of the Arabic linguistic resources we use being biased towards news-genre. For example, the Arabic treebank used for training the parser is only in the news genre. The Buckwalter lexicon potentially also has some internal bias toward news genre because it was developed in tandem with the Arabic treebank. 6.3 Qualitative Evaluation Automatic evaluation systems are often criticized for not capturing linguistic subtleties. This is clearly apparent in the field’s moving back toward using human evaluation metrics such as HTER (Snover et al., 2006). We conducted a small human evaluation of verb and subject realization in eight random documents from MT04. The documents contained 47 sentences and reflect the distribution of genre in the MT04 test set. We compare three systems G HMT, G HMT P HARAOH and P HARAOH. The evaluation was conducted using one bilingual Arabic-English speaker (native Arabic, almost native English). The task is to determine for every verb that appears in the Arabic input whether it is Table 4: Verb and subject realization in eight documents from MT04 Genre News Speech Editorial All Verb Count 46 48 29 123 G HMT Verbs"
2006.amta-papers.7,J04-2003,0,\N,Missing
2006.amta-papers.7,N04-1021,0,\N,Missing
2008.amta-papers.13,J07-2003,0,0.0414222,"oes not depend heavily on such overlap? Answering these questions will make it possible to characterize the utility of paraphrase-based optimization in real-world scenarios, and and how best to leverage it in those scenarios where it does prove useful. 3 Research Questions Paraphrasing Model We generate sentence-level paraphrases via Englishto-English translation using phrase table pivoting, following (Madnani et al., 2007). The translation system we use (for both paraphrase generation and translation) is based on a state-of-the-art hierarchical phrase-based translation model as described in (Chiang, 2007). English-to-English hierarchical phrases are induced using the pivot-based technique proposed in (Bannard and Callison-Burch, 2005) with primary features similar to those used by (Madnani et al., 2007): the joint probability p(e¯1 , e¯2 ), the two conditionals p(e¯1 |e¯2 ) & p(e¯2 |e¯1 ) and the target length. To limit noise during pivoting, we only keep the top 20 paraphrase pairs resulting from each pivot, as determined by the induced fractional counts. Furthermore, we pre-process the source to identify all named entities using BBN IdentiFinder (Bikel et al., 1999) and strongly bias our dec"
2008.amta-papers.13,C04-1051,0,0.191571,"Missing"
2008.amta-papers.13,W04-3250,0,0.0429334,"he paraphraser only on a subset—1 million sentences—instead of the full set. • We use a 1-3 split of the 4 reference translations from the NIST MT02 test set to tune the feature weights for the paraphraser similar to Madnani et al. (2007). • No changes are made to the number of references in any validation set. Only the tuning sets differed in the number of references across different experiments. • BLEU and TER are calculated on lowercased translation output. Brevity penalties for BLEU are indicated if not equal to 1. • For each experiment, BLEU scores shown in bold are significantly better (Koehn, 2004) than the appropriate baselines for that experiment (p &lt; 0.05). 4.1 Table 1: BLEU and TER scores are shown for MT04+05. 1H=Tuning with 1 human reference, 1H+1P=Tuning with the human reference and its paraphrase. Lower TER scores are better. BLEU TER 1H 37.65 56.39 1H+1P 39.32 54.39 Single Reference Datasets In this section, we attempt to gauge the utility of the paraphrase approach in a realistic scenario where only a single reference translation is available for the tuning set. We use the NIST MT03 data, which has four references per development item, to simulate a tuning set in which only a"
2008.amta-papers.13,A00-2023,0,0.0610845,"Missing"
2008.amta-papers.13,W07-0716,1,0.921324,"a for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases. 1 In ou"
2008.amta-papers.13,P08-1023,0,0.0319501,"Missing"
2008.amta-papers.13,P03-1021,0,0.0175466,"n Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al"
2008.amta-papers.13,P02-1040,0,0.0870838,"ference, Hawaii, 21-25 October 2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking"
2008.amta-papers.13,2006.amta-papers.25,1,0.830776,"2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that inv"
2008.amta-papers.13,strassel-etal-2006-integrated,0,0.0152588,"ns. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases"
2010.amta-papers.7,baker-etal-2010-modality,1,0.741353,"es of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now (deontic possibility). Many semanticists (Kratzer, 2009; von Fintel and Iatridou, 2009) define modality as quantification over possible worlds. John might go means that there exist some possible worlds in which John goes. Another view of modality relates more to a speaker’s attitude toward a proposition (Nirenburg and McShane, 2008; McShane et al., 2004). Modality resources built for this purpose have been described previously (Baker et al., 2010). This paper will focus on a tree-grafting mechanism used to enrich the machine-translation output and on the resulting improvements to translation quality when the training process for the machinetranslation systems included tagging of named entities and modality. The next section provides the motivation behind the SIMT approach. Section 3 presents implementation details of the semantically-informed syntactic system. Section 4 describes the tree grafting algorithm. Section 5 provides the results of this work. Standard Hierarchical Rules Syntactic Enhancements Semantically Informed Rules Figur"
2010.amta-papers.7,P05-1033,0,0.172966,"duEnglish test set, and the blind test set was NIST09. Tree-Grafting to refine translation grammars with semantic categories We use synchronous context free grammars (SCFGs) as the underlying formalism for our statistical models of translation. SCFGs provide a convenient and theoretically grounded way of incorporating linguistic information into statistical models of translation, by specifying grammar rules with syntactic non-terminals in the source and target languages. We refine the set of non-terminal symbols so that they not only include syntactic categories, but also semantic categories. Chiang (2005) re-popularized the use of SCFGs for machine translation, with the introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero uses grammars with a single nonterminal symbol “X” rather than using linguistically informed non-terminal symbols. When moving to linguistic grammars, we use the Syntax Augmented Machine Translation (SAMT) developed by Venugopal et al. (2007). In SAMT the “X” symbols in translation grammars are replaced with nonterminal categories derived from parse trees that label the English side of the Urdu-English parallel corpus.1 We refine the syntacti"
2010.amta-papers.7,C96-1079,0,0.0150098,"ut has been accused of assam that the power of this on a large region has taken in the affairs and one Place, vice Center of which he is also Figure 1: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort. Named entities have been the focus of information extraction research since the Message Understanding Conferences of the 1980s (Grishman and Sundheim, 1996). Automatic taggers identify semantic types such as person, organization, location, date, facility, etc. In this research effort we tagged English documents using an HMM-based tagger derived from Identifinder (Bikel et al., 1999). Modality is an extra-propositional component of meaning. In John may go to NY, the basic proposition is John go to NY and the word may indicates modality. Van der Auwera and Amman (2005) define core cases of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now"
2010.amta-papers.7,N06-1031,0,0.0612688,"The semantic annotations were done manually by students following a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP"
2010.amta-papers.7,P03-1054,0,0.00597936,"he training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. In the parsing domain, the work of (Petrov and Klein, 2007) is related to the current work. Petrov and Klein use a technique of rule splitting and rule merging in order to refine parse trees during machine learning. Hierarchical splitting leads to the creation of learned categories that have linguistic relevance, such as a brea"
2010.amta-papers.7,P07-2045,1,0.00864032,"erstand the challenges of translating important semantic entities when working with a lowresource language pair. Figure 1 shows an example taken from the 2008 NIST Urdu-English translation task, and illustrates the translation quality of a stateof-the-art Urdu-English system (prior to the SIMT effort). The small amount of training data for this language pair (see Table 1) results in significantly degraded translation quality compared, e.g., to an Arabic-English system that has more than 100 times the amount of training data. The machine translation output in Figure 1 was produced using Moses (Koehn et al., 2007), a stateof-the-art phrase-based machine translation system that by default does not incorporate any linguistic information (e.g., syntax or morphology or translit3 Figure 3: Workflow for producing semantically-grafted parse trees. The English side of the parallel corpus is automatically parsed, and also tagged with modality and named-entity markers. These tags are then grafted onto the syntactic parse trees. The relation finder was designed for additional tagging but was not implemented in the current work. (Future work will test relations as another component of meaning that may contribute t"
2010.amta-papers.7,W09-0424,1,0.845363,"note that while our framework is general, we focus the discussion here on the particular semantic elements (named entities and modalities) that were incorporated during the SIMT effort. Once the semantically-grafted trees have been produced for the parallel corpus, the trees are presented, along with word alignments (produced by an aligner such as GIZA++), to the rule extraction software to extract synchronous grammar rules that are both syntactically and semantically informed. These grammar rules are used by the decoder to produce translations. In our experiments, we used the Joshua decoder (Li et al., 2009), the SAMT grammar extraction software (Venugopal and Zollmann, 2009), and special purpose-built tree-grafting software. Figure 5 shows example semantic rules that are used by the decoder. The noun-phrase rules are augmented with named entities, and the verb phrase rules are augmented with modalities. The semantic categories are listed in Table 2 and Table 3. Because these get marked on the Urdu source as well as the English translation, semantically enriched grammars also act as very simple named entity or modality taggers for Urdu. However, only entities and modalities that occurred in the p"
2010.amta-papers.7,J93-2004,0,0.0365987,"with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. In the parsi"
2010.amta-papers.7,A00-2030,1,0.678482,"Missing"
2010.amta-papers.7,P08-1001,0,0.0522391,"the Palestinian Authority fired several missiles GPE weapon GPE GPE Figure 4: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with a named entity tagger. The tags are then grafted onto the syntactic parse tree to form new categories like NP-GPE and NP-weapon. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees. context free grammar parser provided by Basis Technology Corporation. 2. The English sentences are named-entity-tagged by the Phoenix tagger (Richman and Schone, 2008) and modality-tagged by the system described in (Baker et al., 2010). 3. The named entities and modalities are grafted onto the syntactic parse trees using a treegrafting procedure. The grafting procedure was implemented as a part of the SIMT effort. Details are spelled out further in Section 4. The workflow for producing semantically-grafted trees is illustrated in Figure 3. Figure 4 illustrates how named-entity tags are grafted onto a parse tree. We note that while our framework is general, we focus the discussion here on the particular semantic elements (named entities and modalities) that"
2010.amta-papers.7,P99-1039,0,0.237079,"Missing"
2020.findings-emnlp.247,P17-1017,0,0.0224341,"find that the approach produces better responses per automated metrics and detailed human evaluations. • We propose the use of LCS-inspired representations based on asks and framings, which in turn are grounded in conversation analysis literature, to generate plans, instead of using dialogue acts. • We release corpora annotated with plans for all utterances, using three planners, including symbolic planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning representations for open-domain dialogue systems based on lexical conceptual structures (explained in Section 3.1). 2 3 R"
2020.findings-emnlp.247,W17-5525,0,0.0184266,"he following contributions: • We investigate the impact of separating planning and realization in open-domain dialogue and find that the approach produces better responses per automated metrics and detailed human evaluations. • We propose the use of LCS-inspired representations based on asks and framings, which in turn are grounded in conversation analysis literature, to generate plans, instead of using dialogue acts. • We release corpora annotated with plans for all utterances, using three planners, including symbolic planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning"
2020.findings-emnlp.247,N18-2012,0,0.0460113,"Missing"
2020.findings-emnlp.247,P02-1040,0,0.107449,"y in a ratio of 80/10/10 for the training, testing, and validation set. 4.1 Planning Phase Evaluation This evaluation focuses on investigating the efficacy of the two automated planners (Context Attention (CTX) and Pseudo-Self Attention (PSA)) in learning to generate response plans. 4.1.1 Automated Metrics Are the automated planners able to faithfully learn how to generate the response utterance plans? To investigate, we compare the performance of the CTX and the PSA planner with the symbolic planner output (which is our silver standard reference) using common automated metrics Table 2: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) on the test set. We use the library by Sharma et al. (2017). We find that PSA was able to achieve higher word overlap metrics with respect to the silver standard. We conducted an indepth analysis of the CTX and PSA planner output on the entire testing set. We found that the PSA model was more likely to produce ask actions that matched the ground truth, resulting in higher scores on the automated metrics. 4.1.2 Human Evaluation Evaluation using automated metrics provides limited evidence for the ability to aut"
2020.findings-emnlp.247,D19-1460,0,0.0215155,"planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning representations for open-domain dialogue systems based on lexical conceptual structures (explained in Section 3.1). 2 3 Related Work Open-Ended Dialogue Systems: Transformer models (Vaswani et al., 2017) and large transformer-based language models such as GPT, GPT-2, XLNet, BERT (Radford et al., 2018, 2019; Yang et al., 2019; Devlin et al., 2019) have helped achieve the SOTA performance across several natural language tasks. However, these models do not achieve the same level of consistent performance on generative mo"
2020.findings-emnlp.247,W94-0319,0,0.558422,"o Self Attention. Introduction Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al., 2019; Wiseman et al., 2017). One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization (Reiter, 1994; Reiter and Dale, 2000). A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith (Dell, 1985; Bock, 1996; Bock et al., 2007; Kennison, 2018). Prior approaches have indeed incorporated content planning into the NLG system, for example data-to-text generation problems (Puduppully et al., 2019; Moryossef et al., 2019) as well as classic works that include planning, based on speech acts (Cohen and Perrault, 1979) (for an in-depth review c.f. (Garoufi, 2014)). Our work closely follows t"
2020.findings-emnlp.247,W19-8610,1,0.842372,"92 0.0065 7.553 0.838 PSG 0.1253 0.0045 15.128 0.847 No Plan Symbolic Planner Table 5: Automated metric results on the responses generated on the test set of both corpora. input utterance but no plan as input; (2) Symbolic Planner based Generation: This model receives the plan from symbolic planner output; (3) CTX Planner-Based Generation: This model receives the CTX plan; (4) PSA Planner-Based Generation: This model receives the PSA plan. 4.2.1 Automated Metrics Prior research has shown that most automated metrics have little to no correlation to human ratings on NLG tasks (Liu et al., 2016; Santhanam and Shaikh, 2019); however, they may provide some standard of reference to evaluate performance. We report the following metrics: (i) BLEU (Papineni et al., 2002) (ii) length of responses, with the understanding that models that are able to generate longer responses are better (iii) following, Mei et al (2017), we report the diversity metric (Li et al., 2016a). Diversity is calculated as the number of distinct unigrams in the generation scaled by the total number of generated tokens (Mei et al., 2017; Li et al., 2016b). (iv) BERT-Score (Zhang* et al., 2020) metric, an embedding-based score which has shown grea"
2020.findings-emnlp.247,P16-1056,0,0.0435254,"Missing"
2020.findings-emnlp.247,J00-3003,0,0.725047,"Missing"
2020.findings-emnlp.247,D19-1062,0,0.0290866,"derstanding tasks (Ziegler et al., 2019; Edunov et al., 2019). Wolf et al. (2019) propose a transfer learning approach that fine tunes large pretrained language models and achieves SOTA scores on the PERSONA-chat dataset (Golovanov et al., 2019) and in the CONVAI2 competition (Dinan et al., 2019; Yusupov and Kuratov, 2018). Keskar et al. (2019) introduce a large-scale conditional transformer model that improves generation based on control codes. Our training paradigm is consistent with existing research that constrains large-scale language models across generation tasks (Rashkin et al., 2019; Urbanek et al., 2019) and yields controllable text generation (Shen et al., 2019; Zhou et al., 2017), with one key difference: we learn to plan and realize separately. Accordingly, we overview planning based approaches next. Planning-Based Approaches: A standard component of traditional NLG systems is a planner (Reiter and Dale, 2000). Prior work leverages intent and meaning representations (MR) to understand the content of the message (Young et al., 2013), but largely in task-oriented as opposed to open-ended 3.1 Approach NLU using Asks and Framing The representation we use to generate plans leverages asks and fr"
2020.findings-emnlp.247,C18-1312,0,0.0208261,", GPT-2, XLNet, BERT (Radford et al., 2018, 2019; Yang et al., 2019; Devlin et al., 2019) have helped achieve the SOTA performance across several natural language tasks. However, these models do not achieve the same level of consistent performance on generative modeling tasks as opposed to language understanding tasks (Ziegler et al., 2019; Edunov et al., 2019). Wolf et al. (2019) propose a transfer learning approach that fine tunes large pretrained language models and achieves SOTA scores on the PERSONA-chat dataset (Golovanov et al., 2019) and in the CONVAI2 competition (Dinan et al., 2019; Yusupov and Kuratov, 2018). Keskar et al. (2019) introduce a large-scale conditional transformer model that improves generation based on control codes. Our training paradigm is consistent with existing research that constrains large-scale language models across generation tasks (Rashkin et al., 2019; Urbanek et al., 2019) and yields controllable text generation (Shen et al., 2019; Zhou et al., 2017), with one key difference: we learn to plan and realize separately. Accordingly, we overview planning based approaches next. Planning-Based Approaches: A standard component of traditional NLG systems is a planner (Reiter and"
2020.findings-emnlp.247,P19-1566,0,0.0198767,"on of responses that are constrained by the response plan. In this phase, we only experiment with the Pseudo Self attention (PSA) model, based on Ziegler et al. (2019), who demonstrate that PSA outperforms other approaches on text generation tasks. We use nucleus sampling to overcome some of the drawbacks of beam search (Holtzman et al., 2020). 3.3 Corpora Our choice of corpora is driven by the presence of information elicitation and persuasive strategies in the utterances (i.e., asks and framings). Accordingly, we experiment with the AntiScam (Li et al., 2019) and Persuasion for Social Good (Wang et al., 2019) corpora. AntiScam contains dialogues about a customer service scenario and is specifically crowdsourced to understand human elicitation strategies. Persuasion for Social Good corpus contains interactions between workers who are assigned the roles of persuader and persuadee, 2738 AntiScam PSG 220 1017 Avg. Conversation Length 12.45 10.43 Avg. Utterance Length 11.13 19.36 Number of GIVE 2192 11587 Number of PERFORM 1681 7335 Number of GAIN 70 399 Number of LOSE 73 588 4376 8078 Number of Dialogues Number of RESPOND pling both in the planning and realization phase. All models are trained on two"
2020.findings-emnlp.247,D17-1239,0,0.0192667,"te that decoupling the process into planning and realization performs better than an end-to-end approach. 1 Figure 1: Example conversation between two speakers A & B where the response for the speaker B is generated based on the response plan from two learned planners: Context Attention and Pseudo Self Attention. Introduction Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al., 2019; Wiseman et al., 2017). One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization (Reiter, 1994; Reiter and Dale, 2000). A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith (Dell, 1985; Bock, 1996; Bock et al., 2007; Kennison, 2018). Prior approaches have indeed incorporated content planni"
2020.stoc-1.1,N03-1013,1,0.118022,"dentify the main action and arguments. For example, click here yields click as the ask and its argument here. Additional constraints are imposed through the use of a lexicon based on Lexical Conceptual Structure (LCS) (Dorr and Olsen, 2018; Dorr and Voss, 2018), derived from a pool of team members’ collected suspected scam/impersonation emails. Verbs from these emails were grouped as follows: • PERFORM: connect, copy, refer • GIVE: administer, contribute, donate • LOSE: deny, forget, surrender • GAIN: accept, earn, grab, win Additional linguistic processing includes: (1) categorial variation (Habash and Dorr, 2003) to map between different parts of speech, e.g., reference(N) → refer(V) enables detection of an explicit ask from you can reference your gift card; and (2) verbal processing to eliminate spurious asks containing verb forms such as sent or signing in sent you this email because you are signing up. 4.2.2 Motive Detection In addition to the use of distinct tools for detecting linguistic knowledge, Panacea extracts the attacker’s intention, or motive. Leveraging the attacker’s demands (asks), goals (framings) and message attack types (from the threat type classifier), the Motive Detection module"
2020.stoc-1.1,W18-3808,1,0.788683,"Missing"
2020.stoc-1.1,W18-1404,1,0.721686,"Missing"
2020.stoc-1.1,D09-1096,0,0.0526194,"Missing"
2020.stoc-1.1,P19-1247,0,0.0282452,"age the attacker to elicit identifying information, is the next advance in this arena. Prior work extracts information from email interactions (Dada et al., 2019), applies supervised learning to identify email signatures and forwarded messages (Carvalho and Cohen, 2004), and classifies email content into different structural sections (Lampert et al., 2009). Statistical and rulebased heuristics extract users’ names and aliases (Yin et al., 2011) and structured script representations determine whether an email resembles a password reset email typically sent from an organization’s IT department (Li and Goldwasser, 2019). Analysis of chatbot responses (Prakhar Gupta and Bigham, 2019) yields human-judgement correlation improvements. Approaches above differ from ours in that they require extensive model training. 1.1.1 Monitoring and Detection Panacea includes an initial protection layer based on the analysis of incoming messages. Conceptual users include end users and IT security professionals. Each message is processed and assigned a label of friend, foe, or unknown, taking into account headers and textual information of each message. The data obtained from this analysis is converted into threat intelligence"
2020.stoc-1.1,P14-5010,0,0.0108818,"d by state-ofthe-art systems in cyber threat intelligence. MISP (Wagner et al., 2016) focuses on information sharing from a community of trusted organizations. MITRE’s Collaborative Research Into Threats (CRITs) (Goffin, 2020) platform is, like Panacea, built on top of the Structured Threat Intelligence eXchange (STIX) specification. Panacea differs from these in that it is part of operational active defenses, rather than solely an analytical tool for incident response and threat reporting. 2 3 System Overview Panacea’s processing workflow is inspired by Stanford’s CoreNLP annotator pipeline (Manning et al., 2014a), but with a focus on using NLP to power active defenses against SE. A F3EAD-inspired phased analysis and engagement cycle is employed to conduct active defense operations. The cycle is triggered when a message arrives and is deconstructed into STIX threat intelligence objects. Object instances for the identities of the sender and all recipients are found or created in the knowledge base. Labeled relationships are created between those identity objects and the message itself. Once a message is ingested, plug-in components process the message in the find phase, yielding a response as a JSON o"
2020.stoc-1.1,W19-5944,0,0.0205723,"Missing"
2020.stoc-1.1,W18-5030,0,0.0283298,"ker, bulwarked by hopes of eventual payoff. Such requests are implemented as a collection of flag seeking strategies built on top of a conversational theory of asks. Flags are collected using information extraction techniques. Future work includes inferential logic and deception detection to unmask an attacker and separate them from feigned identities used to gain trust. 2 Our approach relates to work on conversational agents, e.g., response generation using neural models (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models (Dziri et al., 2018), self-disclosure for targeted responses (Ravichander and Black, 2018), topic models (Bhakta and Harris, 2015), and other NLP analysis (Sawa et al., 2016). All such approaches are limited to a pre-defined set of topics, constrained by the training corpus. Other prior work focuses on persuasion detection/prediction (Hidey and McKeown, 2018) but for judging when a persuasive attempt might be successful, whereas Panacea aims to achieve effective dialogue for countering (rather than adopting) persuasive attempts. Text-based semantic analysis is also used for SE detection (Kim et al., 2018), but not for engaging with an attacker. Whereas a bot might be employed to wa"
2020.stoc-1.2,W18-1404,1,0.779842,"ain through compliance or lack thereof. It should be noted that there is no one-to-one ratio between ask and framing in the ask/framing detection output. Given the content, there may be none, one or more asks and/or framings in the output. Our lexical organization is based on Lexical Conceptual Structure (LCS), a formalism that supports resource construction and extensions to new applications such as SE detection and response generation. Semantic classes of verbs with similar meanings (give, donate) are readily augmented through adoption of the STYLUS variant of LCS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018). We derive LCS+ from asks/framings and employ CATVAR (Habash and Dorr, 2003) to relate word variants (e.g., reference and refer). Table 1 illustrates LCS+ Ask/Framing output for three (presumed) SE emails: two PERFORM asks and one GIVE ask.1 Parentheses () refer to ask arguments, often a link that the potential victim might choose to click. Ask/framing outputs are provided to downstream response generation. For example, a possible response for Table 1(a) is I will contact asap. A comparison of LCS+ to two related resources shows that our lexical organization supports refinements, improves ask"
2020.stoc-1.2,habash-dorr-2002-handling,1,0.360012,"the SE task is to waste the attacker’s time, play along, and possibly extract information that could unveil their identity. GAIN: 13.5.1 Get: You are a winner of 1M Eu. 13.5.2 Obtain: You can recover your credit rating 4. Table 2: Lexical organization of LCS+ relies on Ask Categories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Ou"
2020.stoc-1.2,2006.amta-papers.7,1,0.45115,".2 Obtain: You can recover your credit rating 4. Table 2: Lexical organization of LCS+ relies on Ask Categories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic model"
2020.stoc-1.2,W18-5030,0,0.0251272,"abash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models produce focused responses (Dziri et al., 2018), self-disclosure yields targeted responses (Ravichander and Black, 2018), and SE detection employs topic models (Bhakta and Harris, 2015) and NLP of conversations (Sawa et al., 2016). However, all such approaches are limited to a pre-defined set of topics, constrained by the training corpus. Other prior work focuses on persuasion detection/ prediction (Hidey and McKeown, 2018) by leveraging argument structure, but for the purpose of judging when a persuasive attempt might be successful in subreddit discussions dedicated to changing opinions (ChangeMyView). Our work aims to achieve effective dialogue for countering (rather than adopting) persuasive attempts. Text-b"
2020.stoc-1.2,1985.tmi-1.17,0,0.202151,"Missing"
2020.stoc-1.2,W00-0207,0,0.0954748,"ategories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models produce focused responses (Dziri et al., 2018), self-disclosure yields targeted responses (Ravichander"
2020.stoc-1.2,N03-1013,1,0.592437,"one-to-one ratio between ask and framing in the ask/framing detection output. Given the content, there may be none, one or more asks and/or framings in the output. Our lexical organization is based on Lexical Conceptual Structure (LCS), a formalism that supports resource construction and extensions to new applications such as SE detection and response generation. Semantic classes of verbs with similar meanings (give, donate) are readily augmented through adoption of the STYLUS variant of LCS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018). We derive LCS+ from asks/framings and employ CATVAR (Habash and Dorr, 2003) to relate word variants (e.g., reference and refer). Table 1 illustrates LCS+ Ask/Framing output for three (presumed) SE emails: two PERFORM asks and one GIVE ask.1 Parentheses () refer to ask arguments, often a link that the potential victim might choose to click. Ask/framing outputs are provided to downstream response generation. For example, a possible response for Table 1(a) is I will contact asap. A comparison of LCS+ to two related resources shows that our lexical organization supports refinements, improves ask/framing detection and top ask identification, and yields qualitative improve"
2020.stoc-1.2,2003.mtsummit-systems.9,1,\N,Missing
A97-1021,C96-1055,1,0.793401,"spirit of (Grimshaw, 1993; Levin and R a p p a p o r t Hovav, To appear; Pinker, 1989; Talmy, 1985)). 4 Three inputs are required for acquisition of verb entries: a semantic class, a thematic grid, and a lexeme, which we will henceforth abbreviate as ""class/grid/lexeme."" The output is a Lisp-like expression corresponding to the LCS representation. An example of i n p u t / o u t p u t for our acquisition procedure is shown here: (4) Acquisition of LCS for: touch I n p u t : 47.8: _th_loc; ""touch"" 2Verbs not occurring in Levin&apos;s book are also assigned to classes using techniques described in {Dorr and Jones, 1996; Dorr, To appear). ZAn underscore (_) designates an obligatory role and a comma (,) designates an optional role. 4The ! ! in the Lisp representation corresponds to the angle-bracketed constants ill Table 2, e.g., ! ! - i n g l y corresponds to (MANNER}. 142 4 Language-Specific Annotations In our on-going example (4), the thematic grid _ t h l o c indicates that the t h em e and the location are both obligatory (in English) and should be annotated as such in the instantiated LCS. This is achieved by inserting a *-marker appropriately. Consider the structural divergence between the following En"
A97-1021,J90-1003,0,\N,Missing
A97-1021,J93-2002,0,\N,Missing
ayan-etal-2004-multi,carbonell-etal-2002-automatic,0,\N,Missing
ayan-etal-2004-multi,J93-2004,0,\N,Missing
ayan-etal-2004-multi,J93-2003,0,\N,Missing
ayan-etal-2004-multi,H01-1035,0,\N,Missing
ayan-etal-2004-multi,N01-1026,0,\N,Missing
ayan-etal-2004-multi,E99-1010,0,\N,Missing
ayan-etal-2004-multi,P02-1040,0,\N,Missing
ayan-etal-2004-multi,P02-1033,0,\N,Missing
ayan-etal-2004-multi,P01-1067,0,\N,Missing
ayan-etal-2004-multi,P03-1012,0,\N,Missing
ayan-etal-2004-multi,P02-1050,0,\N,Missing
ayan-etal-2004-multi,J00-2004,0,\N,Missing
ayan-etal-2004-multi,P03-2041,0,\N,Missing
ayan-etal-2004-multi,J97-2004,0,\N,Missing
ayan-etal-2004-multi,N03-1017,0,\N,Missing
ayan-etal-2004-multi,P02-1038,0,\N,Missing
ayan-etal-2004-multi,J03-1002,0,\N,Missing
ayan-etal-2004-multi,N03-1013,1,\N,Missing
ayan-etal-2004-multi,2003.mtsummit-systems.9,1,\N,Missing
ayan-etal-2004-multi,dorr-etal-2002-duster,1,\N,Missing
ayan-etal-2004-multi,P98-1004,0,\N,Missing
ayan-etal-2004-multi,C98-1004,0,\N,Missing
ayan-etal-2004-multi,P98-2162,0,\N,Missing
ayan-etal-2004-multi,C98-2157,0,\N,Missing
ayan-etal-2004-multi,P96-1024,0,\N,Missing
baker-etal-2010-modality,levy-andrew-2006-tregex,0,\N,Missing
baker-etal-2010-modality,W09-0424,0,\N,Missing
baker-etal-2010-modality,P02-1040,0,\N,Missing
bird-etal-2008-acl,D07-1089,0,\N,Missing
bird-etal-2008-acl,W06-1613,0,\N,Missing
bird-etal-2008-acl,N04-1042,0,\N,Missing
bird-etal-2008-acl,radev-etal-2004-mead,1,\N,Missing
C04-1137,A00-2038,1,0.857804,"ns’ Desk Reference (PDR) warns may be “mistaken for each other ... lead[ing]  Orthographic Phonetic Distance EDIT NED SOUNDEX EDITEX Similarity -GRAM LCSR ALINE Measure EDIT NED LCSR BIGRAM TRIGRAM-2B SOUNDEX EDITEX ALINE BI-SIM TRI-SIM PREFIX Table 1: Classification of word distance and similarity measures. to serious medication errors” (24th Ed., 2003). The phonetic transcription of the two names, [zænæks] and [zæntæk], reveals their sound-alike similarity that is not apparent in their orthographic form. For the detection of sound-alike confusion pairs, we apply the ALINE phonetic aligner (Kondrak, 2000), which estimates the similarity between two phonetically-transcribed words. We demonstrate that ALINE outperforms orthographic approaches on a test set containing sound-alike confusion pairs. The next section describes several commonlyused measures of word similarity. After this, we present two new methods for identifying look-alike and sound-alike drug names. We then compare the effectiveness of various measures using our recallbased evaluation methodology on a U.S. pharmacopeial gold standard and on another test set containing sound-alike confusion pairs. We conclude with a discussion of ou"
C04-1137,J99-1003,0,0.492233,"section, we describe a number of measures that have been applied to the problem of identifying confusable drug names. Specific examples of values obtained by the measures are provided in Table 2. String-edit distance (Wagner and Fischer, 1974) (EDIT) (also known as Levenshtein distance) counts up the number of steps it takes to transform one string into another, where the cost of substitution is the same as the cost of insertion or deletion. A normalized edit distance (NED) is calculated by dividing the total edit cost by the length of the longer string. The longest common subsequence ratio (Melamed, 1999) (LCSR) is computed by dividing Zantac/ Xanax 3 0.500 0.500 0.222 0.000 3 5 9.542 0.417 0.333 0.000 Zantac/ Contac 2 0.333 0.667 0.600 0.333 1 2 9.333 0.583 0.500 0.000 Xanax/ Contac 4 0.667 0.333 0.000 0.000 3 7 8.958 0.250 0.167 0.000 Table 2: Examples of values returned by various measures. the length of the longest common subsequence by the length of the longer string. LCSR is closely related to normalized edit distance. If the cost of substitution is at least twice the cost of insertion/deletion and the strings are of equal length, LCSR is equivalent to the normalized edit distance. In -g"
C92-2094,P90-1017,1,0.928596,"getdanguage structure to be realized systematically from lexical items derived from the conceptual form. This work represents a shift away from colnplex, language-specific syutactic translation without entirely abandoning syntax. Furthermore, the work moves toward a model that employs a well-defined lexieal conceptual representation without requiring a &quot;deep&quot; semantic conceptualizatiou. Consider the following example: (1) (i) I stabbed Jnhn (ii) Yo le di pufialadas a Juan &apos;I gave knife-wounds to John&apos; This example illustrates a type of distiuctiou (henceforth called divergence as presented in Dorr (1990a)) that arises in machine translation: the sourcelanguage predicate, stab, is ,napped to more than one target-language word, dar puiialadas a. This divergence type is lezical in that there is a word selection variation between the source language and the target language. Such divergeuees are accounted for by lexical-semantie parameterization, as we will see in section 3. The following section of this paper will provide a catalog of syntactic divergences between the source and target languages. The set of parameters that are used to account for these divergences will be described. In the third"
C92-2094,J90-2002,0,0.0680326,"Missing"
C92-2094,P92-1033,1,0.833076,"Missing"
C96-1055,C92-2070,0,0.0434065,"Missing"
C96-1055,J90-1003,0,\N,Missing
C96-1055,J93-2002,0,\N,Missing
C96-1055,P91-1019,0,\N,Missing
C96-1055,A92-1011,0,\N,Missing
D08-1090,P05-1033,0,0.0322789,"lation model produces the English text to which the 860 language model has been adapted. Bias text that is used by one adaptation but not the other will receive no special treatment by the other model. This could result in new translation rules that produce text to which the language assigns low probability, or it could result in the language model being able to assign a high probability to a good English translation that cannot be produced by the translation model due to a lack of necessary translation rules. While both adaptation methods are integrated into a hierarchical translation model (Chiang, 2005), they are largely implementation independent. Language model adaptation could be integrated into any statistical machine translation that uses a language model over words, while translation model adaptation could be added to any statistical machine translation that can utilize phrasal translation rules. 3.1 Language Model Adaptation For every source document, we estimate a new language model, the bias language model, from the corresponding bias text. Since this bias text is short, the corresponding bias language model is small and specific, giving high probabilities to those phrases that occu"
D08-1090,P98-1069,0,0.0756805,"adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system. 1 Introduction While the amount of parallel data available to train a statistical machine translation system is sharply limited, vast amounts of monolingual data are generally available, especially when translating to languages such as English. Yet monolingual data are generally only used to train the language model of the translation system. Previous work (Fung and Yee, 1998; Richard Schwartz BBN Technologies 10 Moulton Street Cambridge, MA 02138, USA schwartz@bbn.com Rapp, 1999) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations. More recently, Resnik and Smith (2003) and Munteanu and Marcu (2005) have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel. This newly discovered bilingual data can then be used as additio"
D08-1090,2005.eamt-1.19,0,0.41674,"as increasing the probability of existing translation rules. Translation adaptation using the translation system’s own output, known as Self-Training (Ueffing, 2006) has previously shown gains by augmenting the translation model with additional translation rules. In that approach however, the translation model was augmented using parallel data, rather than comparable data, by interpolating a translation model trained using the system output with the original translation model. Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by Hildebrand et al. (2005) to yield significant gains over a baseline system. The translation model was adapted by selecting comparable sentences from parallel corpora for each of the sentences to be translated. In addition to selecting outof-domain data to adapt the translation model, comparable data selection techniques have been used to select and weight portions of the existing training data for the translation model to improve translation performance (Lu et al., 2007). The research presented in this paper utilizes a different approach to translation model adaptation using comparable monolingual text rather than pa"
D08-1090,W03-1003,0,0.0205357,"isit to India. Many phrases and words are shared between the two, including: the name of the movie, the name and relationship of the actress’ character, the name and age of her son and many others. Such a pairing is extremely comparable, although even less related document pairs could easily be considered comparable. We seek to take advantage of these comparable documents to inform the translation of the source document. This can be done by augmenting the major components of the statistical translation system: the Language Model and the Translation Model. This work is in the same tradition as Kim and Khudanpur (2003), Zhao et al. (2004), and Kim (2005). Kim (2005) used large amounts of comparable data to adapt language models on a documentby-document basis, while Zhao et al. (2004) used comparable data to perform sentence level adaptation of the language model. These adapted language models were shown to improve performance 1 This is an actual source document from the tuning set used in our experiments, and the first of a number of similar passages found by the comparable text selection system described in section 2. 858 for both automatic speech recognition as well as machine translation. In addition to"
D08-1090,D07-1036,0,0.0107067,"inal translation model. Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by Hildebrand et al. (2005) to yield significant gains over a baseline system. The translation model was adapted by selecting comparable sentences from parallel corpora for each of the sentences to be translated. In addition to selecting outof-domain data to adapt the translation model, comparable data selection techniques have been used to select and weight portions of the existing training data for the translation model to improve translation performance (Lu et al., 2007). The research presented in this paper utilizes a different approach to translation model adaptation using comparable monolingual text rather than parallel text, exploiting data that would otherwise be unused for estimating the translation model. In addition, this data also informs the translation system by interpolating the original language model with a new language model trained from the same comparable documents. We discuss the selection of comparable text for model adaptation in section 2. In sections 3.1 and 3.2, we describe the model adaptation for the language model and translation mod"
D08-1090,J05-4003,0,0.0718158,"ual data are generally available, especially when translating to languages such as English. Yet monolingual data are generally only used to train the language model of the translation system. Previous work (Fung and Yee, 1998; Richard Schwartz BBN Technologies 10 Moulton Street Cambridge, MA 02138, USA schwartz@bbn.com Rapp, 1999) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations. More recently, Resnik and Smith (2003) and Munteanu and Marcu (2005) have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel. This newly discovered bilingual data can then be used as additional training data for the translation system. Such methods generally have a very low yield leaving vast amounts of data that is only used for language modeling. These methods rely upon comparable corpora, that is, multiple corpora that are of the same general genre. In addition to this, documents can be comparable—two documents that are both on the same event or topic. Comparable documents occur b"
D08-1090,P00-1056,0,0.0454094,"based on a very small number of occurrences in the training data. In other cases, translations may be known for individual words in the source document, but not for longer phrases. Translation model adaptation seeks to generate new phrasal translation rules for these source words and phrases. The bias text for a source document may, if comparable, contain a number of English words and phrases that are the English side of these desired rules. Because the source data and the bias text are not translations of each other and are not sentence aligned, conventional alignment tools, such as GIZA++ (Och and Ney, 2000), cannot be used to align the source and bias text. Because the passages in the bias text are not translations of the source document, it will always be the case that portions of the source document have no translation in the bias text, 861 and portions of the bias text have no translation in the source document. In addition a phrase in one of these texts might have multiple, differing translations in the other text. Unlike language model adaptation, the entirety of the bias text is not used for translation adaptation. We extract those phrases that occur in at least M of the passages in the bi"
D08-1090,P02-1040,0,0.103551,"m, although in practice this is a small expenditure. The most intensive portion is the initial indexing of the monolingual corpus, but this is only required once and can be reused for any subsequent test set that is evaluated. This index can then be quickly searched for comparable passages. When considering research environments, test sets are used repeatedly and bias texts only need to be built once per set, making the building cost negligible. Otherwise, the time required to build the bias text is still small compared to the actual translation time. All conditions were optimized using BLEU (Papineni et al., 2002) and evaluated using both BLEU and Translation Edit Rate (TER) (Snover et al., 2006). BLEU is an accuracy measure, so higher values indicate better performance, while TER is an error metric, so lower values indicate better performance. Optimization was performed on a tuning set of newswire data, comprised of portions of MTEval 2004, MTEval 2005, and GALE 2007 newswire development data, a total of 48921 words of English in 1385 segments and 173 documents. Results were measured on the NIST MTEval 2006 Arabic Evaluation set, which was 55578 words of English in 1797 segments and 104 documents. Fou"
D08-1090,P99-1067,0,0.0328174,"mental results obtained by adapting both the language and translation models show substantial gains over the baseline system. 1 Introduction While the amount of parallel data available to train a statistical machine translation system is sharply limited, vast amounts of monolingual data are generally available, especially when translating to languages such as English. Yet monolingual data are generally only used to train the language model of the translation system. Previous work (Fung and Yee, 1998; Richard Schwartz BBN Technologies 10 Moulton Street Cambridge, MA 02138, USA schwartz@bbn.com Rapp, 1999) has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations. More recently, Resnik and Smith (2003) and Munteanu and Marcu (2005) have exploited monolingual data in both the source and target languages to find document or sentence pairs that appear to be parallel. This newly discovered bilingual data can then be used as additional training data for the translation system. Such methods generally have a very low yield leaving vast amo"
D08-1090,P08-1066,0,0.012193,"t in need of additional translation rules. So, it is under such a condition we would expect the translation model adaptation to be the most beneficial. We evaluate the system’s performance under this condition in section 4.2. The effectiveness of this technique on state-of-the-art systems, and its efficiency when used with a well trained generic translation model is presented in section 4.3. 4.1 Implementation Details Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-todependency rules as described in Shen et al. (2008). While generalized rules are generated from the parallel data, rules generated by the translation model adaptation are not generalized and are used only as phrasal rules. A trigram language model was used during decoding, and a 5-gram language model was used to re-score the n-best list after decoding. In addition to the features described in Shen et al. (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. Bia"
D08-1090,2006.amta-papers.25,1,0.77697,"initial indexing of the monolingual corpus, but this is only required once and can be reused for any subsequent test set that is evaluated. This index can then be quickly searched for comparable passages. When considering research environments, test sets are used repeatedly and bias texts only need to be built once per set, making the building cost negligible. Otherwise, the time required to build the bias text is still small compared to the actual translation time. All conditions were optimized using BLEU (Papineni et al., 2002) and evaluated using both BLEU and Translation Edit Rate (TER) (Snover et al., 2006). BLEU is an accuracy measure, so higher values indicate better performance, while TER is an error metric, so lower values indicate better performance. Optimization was performed on a tuning set of newswire data, comprised of portions of MTEval 2004, MTEval 2005, and GALE 2007 newswire development data, a total of 48921 words of English in 1385 segments and 173 documents. Results were measured on the NIST MTEval 2006 Arabic Evaluation set, which was 55578 words of English in 1797 segments and 104 documents. Four reference translations were used for scoring each translation. Parameter optimizat"
D08-1090,2006.iwslt-papers.3,0,0.0257977,"m the tuning set used in our experiments, and the first of a number of similar passages found by the comparable text selection system described in section 2. 858 for both automatic speech recognition as well as machine translation. In addition to language model adaptation we also modify the translation model, adding additional translation rules that enable the translation of new words and phrases in both the source and target languages, as well as increasing the probability of existing translation rules. Translation adaptation using the translation system’s own output, known as Self-Training (Ueffing, 2006) has previously shown gains by augmenting the translation model with additional translation rules. In that approach however, the translation model was augmented using parallel data, rather than comparable data, by interpolating a translation model trained using the system output with the original translation model. Translation model adaptation using comparable out-of-domain parallel data, rather than monolingual data was shown by Hildebrand et al. (2005) to yield significant gains over a baseline system. The translation model was adapted by selecting comparable sentences from parallel corpora"
D08-1090,C04-1059,0,0.223215,"are shared between the two, including: the name of the movie, the name and relationship of the actress’ character, the name and age of her son and many others. Such a pairing is extremely comparable, although even less related document pairs could easily be considered comparable. We seek to take advantage of these comparable documents to inform the translation of the source document. This can be done by augmenting the major components of the statistical translation system: the Language Model and the Translation Model. This work is in the same tradition as Kim and Khudanpur (2003), Zhao et al. (2004), and Kim (2005). Kim (2005) used large amounts of comparable data to adapt language models on a documentby-document basis, while Zhao et al. (2004) used comparable data to perform sentence level adaptation of the language model. These adapted language models were shown to improve performance 1 This is an actual source document from the tuning set used in our experiments, and the first of a number of similar passages found by the comparable text selection system described in section 2. 858 for both automatic speech recognition as well as machine translation. In addition to language model adapt"
D08-1090,C98-1066,0,\N,Missing
D08-1090,J03-3002,0,\N,Missing
D08-1103,C92-2082,0,0.0701406,"Methods in Natural Language Processing, pages 982–991, c Honolulu, October 2008. 2008 Association for Computational Linguistics Lexicons of pairs of words that native speakers consider antonyms have been created for certain languages, but their coverage has been limited. Further, as each term of an antonymous pair can have many semantically close terms, the contrasting word pairs far outnumber those that are commonly considered antonym pairs, and they remain unrecorded. Even though a number of computational approaches have been proposed for semantic closeness, and some for hypernymy–hyponymy (Hearst, 1992), measures of antonymy have been less successful. To some extent, this is because antonymy is not as well understood as other classical lexical-semantic relations. We first very briefly summarize insights and intuitions about this phenomenon, as proposed by linguists and lexicographers (Section 2). We discuss related work (Section 3). We describe the resources we use (Section 4) and present experiments that examine the manifestation of antonymy in text (Sections 5 and 6). We then propose a new empirical approach to determine the degree of antonymy between two words (Section 7). We compiled a d"
D08-1103,J91-1001,0,0.842435,"rgeon) are also semantically related, but terms that are semantically related may not always be semantically similar (plane–sky, surgeon–scalpel). Antonymy is unique among these relations because it simultaneously conveys both a sense of closeness and of distance (Cruse, 1986). Antonymous concepts are semantically related but not semantically similar. 3 Related work Charles and Miller (1989) proposed that antonyms occur together in a sentence more often than chance. This is known as the co-occurrence hypothesis. They also showed that this was empirically true for four adjective antonym pairs. Justeson and Katz (1991) demonstrated the co-occurrence hypothesis for 35 prototypical antonym pairs (from an original set of 39 antonym pairs compiled by Deese (1965)) and also for an additional 22 frequent antonym pairs. All of these pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb, adjective, and adverb pairs (noun–noun, noun–verb, noun–adjective, verb–adverb and so on) pertaining to 18 concepts (for example, lose(v)–gain(n) and loss(n)–gain(n), where lose(v) and loss(n) pertain to the concept of “failing to have/maintain”). However, non-antonymous semantically related words su"
D08-1103,P98-2127,0,0.292578,"c1  c2  ∑w  T  c1   T  c2  I c1  w  I c2  w   (2) ∑w  T  c1  I c1  w  ∑w  T  c2  I c2  w   Here T c  is the set of all words w that have positive pointwise mutual information with the thesaurus category c (I c  w  0). We adopt this method for use in our approach to determine word-pair antonymy. 5 The co-occurrence hypothesis of antonyms Co-occurrence statistics The distributional hypothesis of closeness states that words that occur in similar contexts tend to be semantically close (Firth, 1957). Distributional measures of distance, such as those proposed by Lin (1998), quantify how similar the two sets of contexts of a target word pair are. Equation 1 is a modified form of Lin’s measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lin w1  w2  ∑w  T  w1   T  w2  I w1  w  I w2  w   (1) ∑w  T  w1  I w1  w  ∑w  T  w2  I w2  w   Here w1 and w2 are the target words; I x  y  is the pointwise mutual information between x and y; and T x  is the set of all words y that have positive pointwise mutual information with the word x (I x  y  0). Mohammad and Hirst (20"
D08-1103,P02-1047,0,0.084301,"ascend–descend, shout–whisper). In its broadest Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automatic creation of a thesaurus it is necessary to distinguish nearsynonyms from word pairs that are semantically contrasting. Measures of distributional similarity fail to do so. Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component"
D08-1103,P08-1118,0,0.078719,"Missing"
D08-1103,H05-1067,0,0.0321323,"ining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automatic creation of a thesaurus it is necessary to distinguish nearsynonyms from word pairs that are semantically contrasting. Measures of distributional similarity fail to do so. Detecting antonymous words is not sufficient to solve most of these problems, but it remains a crucial, and largely unsolved, component. 982 Proceedings of the 2008 Conference on Empirical Methods in Natural La"
D08-1103,W06-1605,1,0.670245,"proposed by Lin (1998), quantify how similar the two sets of contexts of a target word pair are. Equation 1 is a modified form of Lin’s measure that ignores syntactic dependencies and hence it estimates semantic relatedness rather than semantic similarity: Lin w1  w2  ∑w  T  w1   T  w2  I w1  w  I w2  w   (1) ∑w  T  w1  I w1  w  ∑w  T  w2  I w2  w   Here w1 and w2 are the target words; I x  y  is the pointwise mutual information between x and y; and T x  is the set of all words y that have positive pointwise mutual information with the word x (I x  y  0). Mohammad and Hirst (2006) showed that these distributional word-distance measures perform poorly when compared with WordNet-based concept-distance measures. They argued that this is because the word-distance measures clump together the contexts of the different senses of the target words. They proposed a way to obtain distributional distance between word senses, using any of the distributional measures such as cosine or that proposed by Lin, and showed that this approach performed markedly better than the traditional worddistance approach. They used thesaurus categories 985 As a first step towards formulating our appr"
D08-1103,W04-2607,1,0.680847,"ter-thanchance co-occurrence of antonyms in sentences is because together they convey contrast well, which is rhetorically useful, and not really the reason why they are considered antonyms in the first place. 2.2 Are semantic closeness and antonymy opposites? Two words (more precisely, two lexical units) are considered to be close in meaning if there is a lexical-semantic relation between them. Lexicalsemantic relations are of two kinds: classical and non-classical. Examples of classical relations include synonymy, hyponymy, troponymy, and meronymy. Non-classical relations, as pointed out by Morris and Hirst (2004), are much more common and include concepts pertaining to another concept (kind, chivalrous, formal pertaining to gentlemanly), and commonly co-occurring words (for example, problem–solution pairs such as homeless, shelter). Semantic distance (or closeness) in this broad sense is known as semantic relatedness. Two words are considered to be semantically similar if they are associated via the synonymy, hyponymy– hypernymy, or the troponymy relation. So terms that are semantically similar (plane–glider, doctor– surgeon) are also semantically related, but terms that are semantically related may n"
D08-1103,W02-1011,0,0.0141073,"Missing"
D08-1103,C02-1061,0,0.311749,"proposed in this paper is completely unsupervised. Harabagiu et al. (2006) detected antonyms for the purpose of identifying contradictions by using WordNet chains—synsets connected by the hypernymy–hyponymy links and exactly one antonymy link. Lucerto et al. (2002) proposed detecting antonym pairs using the number of words 984 between two words in text and also cue words such as but, from, and and. Unfortunately, they evaluated their method on only 18 word pairs. Neither of these methods determines the degree of antonymy between words and they have not been shown to have substantial coverage. Schwab et al. (2002) create “antonymous vector” for a target word. The closer this vector is to the context vectors of the other target word, the more antonymous the two target words are. However, the antonymous vectors are manually created. Further, the approach is not evaluated beyond a handful of word pairs. Work in sentiment detection and opinion mining aims at determining the polarity of words. For example, Pang, Lee and Vaithyanathan (2002) detect that adjectives such as dazzling, brilliant, and gripping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the n"
D08-1103,C08-1114,0,0.0555083,"such as hypernyms, holonyms, meronyms, and nearsynonyms also tend to occur together more often than chance. Thus, separating antonyms from them has proven to be difficult. Lin et al. (2003) used patterns such as “from X to Y ” and “either X or Y ” to separate antonym word pairs from distributionally similar pairs. They evaluated their method on 80 pairs of antonyms and 80 pairs of synonyms taken from the Webster’s Collegiate Thesaurus (Kay, 1988). In this paper, we propose a method to determine the degree of antonymy between any word pair and not just those that are distributionally similar. Turney (2008) proposed a uniform method to solve word analogy problems that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. However, the Turney method is supervised whereas the method proposed in this paper is completely unsupervised. Harabagiu et al. (2006) detected antonyms for the purpose of identifying contradictions by using WordNet chains—synsets connected by the hypernymy–hyponymy links and exactly one antonymy link. Lucerto et al. (2002) proposed detecting antonym pairs using the number of words 984 between two words in text and also cue w"
D08-1103,P08-1008,0,0.0734216,"from each other in small and large respects. In its strictest sense, antonymy applies to gradable adjectives, such as hot–cold and tall–short, where the two words represent the two ends of a semantic dimension. In a broader sense, it includes other adjectives, nouns, and verbs as well (life–death, ascend–descend, shout–whisper). In its broadest Automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases (The dementors caught Sirius Black / Black could not escape the dementors) and detecting contradictions (Marneffe et al., 2008; Voorhees, 2008) (Kyoto has a predominantly wet climate / It is mostly dry in Kyoto). Of course, such “contradictions” may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements. Antonyms often indicate the discourse relation of contrast (Marcu and Echihabi, 2002). They are also useful for detecting humor (Mihalcea and Strapparava, 2005), as satire and jokes tend to have contradictions and oxymorons. Lastly, it is useful to know which words are semantically contrasting to a target word, even if simply to filter them out. For example, in the automati"
D08-1103,C98-2122,0,\N,Missing
D09-1063,D08-1083,0,0.0214768,"Missing"
D09-1063,esuli-sebastiani-2006-sentiwordnet,0,0.318161,"e GI lexicon has orientation labels for only about 3,600 entries. The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words. Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resources—most of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; AnSentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the"
D09-1063,E06-1027,0,0.560541,"Missing"
D09-1063,strapparava-valitutti-2004-wordnet,0,0.0365172,"or English—the most notable being the General Inquirer (GI) (Stone et al., 1966).1 However, the GI lexicon has orientation labels for only about 3,600 entries. The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words. Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resources—most of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; AnSentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like"
D09-1063,P05-1017,0,0.388491,"l., 1966).1 However, the GI lexicon has orientation labels for only about 3,600 entries. The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words. Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resources—most of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; AnSentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has"
D09-1063,P02-1053,0,0.0167643,"b Institute for Advanced Computer Studies† Department of Computer Science‡ , University of Maryland. Human Language Technology Center of Excellence∗ {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a). A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008). A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion. Some example words that have positive semantic orientation are excellent, happy, honest, and so on. Similarly, a word is said to have negative semantic orientation if it is often used to convey unfavorable sentiment or evaluation of the target. Examples include poor, sad, and dishonest. Certain semantic orientation lexicons have been manually compiled for English—the most notable bein"
D09-1063,J94-2004,0,0.0392561,"gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above. 1 Introduction Sentiment analysis involves determining the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette"
D09-1063,H05-1044,0,0.109497,"or Advanced Computer Studies† Department of Computer Science‡ , University of Maryland. Human Language Technology Center of Excellence∗ {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a). A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008). A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion. Some example words that have positive semantic orientation are excellent, happy, honest, and so on. Similarly, a word is said to have negative semantic orientation if it is often used to convey unfavorable sentiment or evaluation of the target. Examples include poor, sad, and dishonest. Certain semantic orientation lexicons have been manually compiled for English—the most notable being the General Inquire"
D09-1063,W03-1017,0,0.0452558,"ns, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004). Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words). Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis. Here we briefly describe the work closest to ours. Hatzivassiloglou and McKeown (1997) proposed a supervised algorithm to determine the semantic orientation of adjectives. They"
D09-1063,P97-1023,0,0.952146,"the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004). Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words). Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis. Here we briefly describe the work closest to ours. Hatzivassiloglou and McKeown (1997) proposed a supervised algorithm"
D09-1063,kamps-etal-2004-using,0,0.61341,"Missing"
D09-1063,W06-1642,0,0.276944,"entioned above. 1 Introduction Sentiment analysis involves determining the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994). It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen 1 http://www.wjh.harvard.edu/ inquirer 599 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599–608, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004). Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words). Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis. Here we briefly describe the work closest"
D09-1063,W06-1605,1,0.751491,"ly inexpensive method to compile a high-coverage semantic orientation lexicon without the use of any text corpora or manually annotated semantic orientation labels. Both of these resources may be used, if available, to further improve results. The lexicon has about twenty times the number of entries in the GI lexicon, and it includes entries for both individual words and common multiword expressions. The method makes use of a Roget-like thesaurus and a handful of antonymgenerating affix patterns. Whereas thesauri have long been used to estimate semantic distance (Jarmasz and Szpakowicz, 2003; Mohammad and Hirst, 2006), the closest thesaurus-based work on sentiment analysis is by Aman and Szpakowicz (2007) on detecting emotions such as happiness, sadness, and anger. We evaluated our thesaurusbased algorithm both intrinsically and extrinsically and show that the semantic orientation lexicon it generates has significantly more correct entries than the state-of-the-art high-coverage lexicon SentiWordNet, and that it has a significantly higher coverage than the General Inquirer and Turney–Littman lexicons. In Section 2 we examine related work. Section 3 presents our algorithm for creating semantic orientation l"
D09-1063,D08-1103,1,0.416462,"h-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus Saif Mohammadφ†∗ , Cody Dunne‡ , and Bonnie Dorrφ†‡∗ Laboratory for Computational Linguistics and Information Processingφ Human-Computer Interaction Lab Institute for Advanced Computer Studies† Department of Computer Science‡ , University of Maryland. Human Language Technology Center of Excellence∗ {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a). A crucial sub-problem is to determine whether positive or negative sentiment is expressed. Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008). A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion. Some example words that have positive semantic orientation are excellent, happy, honest, and so on. Similarly, a word is said to have negative semantic orientati"
dorr-etal-1998-thematic,W98-1426,0,\N,Missing
dorr-etal-1998-thematic,A97-1021,1,\N,Missing
dorr-etal-1998-thematic,P95-1034,0,\N,Missing
dorr-etal-1998-thematic,P98-1116,0,\N,Missing
dorr-etal-1998-thematic,C98-1112,0,\N,Missing
dorr-etal-2000-building,W98-1426,0,\N,Missing
dorr-etal-2000-building,dorr-etal-2000-chinese,1,\N,Missing
dorr-etal-2000-building,dorr-katsova-1998-lexical,1,\N,Missing
dorr-etal-2000-building,C94-1038,0,\N,Missing
dorr-etal-2000-building,olsen-etal-1998-enhancing,1,\N,Missing
dorr-etal-2000-building,A97-1021,1,\N,Missing
dorr-etal-2000-building,P98-1046,0,\N,Missing
dorr-etal-2000-building,C98-1046,0,\N,Missing
dorr-etal-2000-building,P98-1116,0,\N,Missing
dorr-etal-2000-building,C98-1112,0,\N,Missing
dorr-etal-2000-building,dorr-etal-1998-thematic,1,\N,Missing
dorr-etal-2000-chinese,C94-1038,0,\N,Missing
dorr-etal-2000-chinese,olsen-etal-1998-enhancing,1,\N,Missing
dorr-etal-2000-chinese,A97-1021,1,\N,Missing
dorr-etal-2000-chinese,P98-1046,0,\N,Missing
dorr-etal-2000-chinese,C98-1046,0,\N,Missing
dorr-etal-2002-duster,han-etal-2000-handling,0,\N,Missing
dorr-etal-2002-duster,melamed-1998-empirical,0,\N,Missing
dorr-etal-2002-duster,A00-1009,0,\N,Missing
dorr-etal-2002-duster,J93-2003,0,\N,Missing
dorr-etal-2002-duster,W01-1406,0,\N,Missing
dorr-etal-2002-duster,2001.mtsummit-ebmt.4,0,\N,Missing
dorr-etal-2002-duster,W01-1403,0,\N,Missing
dorr-etal-2002-duster,C00-1078,0,\N,Missing
dorr-etal-2002-duster,C00-2131,0,\N,Missing
dorr-etal-2002-duster,W00-1306,1,\N,Missing
dorr-etal-2002-duster,N01-1026,0,\N,Missing
dorr-etal-2002-duster,J90-2002,0,\N,Missing
dorr-etal-2002-duster,P97-1062,0,\N,Missing
dorr-etal-2002-duster,P01-1067,0,\N,Missing
dorr-etal-2002-duster,P02-1050,1,\N,Missing
dorr-etal-2002-duster,J97-3002,0,\N,Missing
dorr-etal-2002-duster,P00-1056,0,\N,Missing
dorr-katsova-1998-lexical,1997.mtsummit-workshop.3,1,\N,Missing
H05-1009,W99-0606,0,0.024551,"alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to each other by weight"
H05-1009,ayan-etal-2004-multi,1,0.829301,"n Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of c"
H05-1009,P98-1029,0,0.0298457,"roaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to"
H05-1009,P03-1012,0,0.0712536,"ce and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 65–72, Vancouver, October 2005. 2005 Association for Computational Linguistics ai i Output layer wij j Hidden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are PO"
H05-1009,P97-1003,0,0.0189016,"istinguish between instances. One of the strategies in the classification literature is to supply the input data to the set of features as well. While combining word alignments, we use two types of features to describe each instance (i, j): (1) linguistic features and (2) alignment features. Linguistic features include POS tags of both words (ei and fj ) and a dependency relation for one of the words (ei ). We generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996) for English and Chinese, and Connexor for Spanish. Dependency relations are produced using a version of the Collins parser (Collins, 1997) that has been adapted for building dependencies. Alignment features consist of features that are extracted from the outputs of individual alignment systems. For each alignment Ak ∈ A, the following are some of the alignment features that can be used to describe an instance (i, j): 1. 2. 3. 4. 5. 6. It is also possible to use variants, or combinations, of these features to reduce feature space. Figure 2 shows an example of how we transform the outputs of 2 alignment systems, A1 and A2 , for an alignment link (i, j) into data with some of the features above. We use -1 and 1 to represent the abs"
H05-1009,P02-1033,0,0.0183863,"t alignment combination as a classifier ensemble (Hansen and Salamon, 1990; Wolpert, 1992). The ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce a globally optimal solution (Minsky, 1991). Introduction Parallel texts are a valuable resource in natural language processing and essential for projecting knowledge from one language onto another. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al., 2001), and statistical machine translation. Although word-level aligners tend to perform well when there is sufficient training data, the quality decreases as the size of training data decreases. Even with large amounts of training data, statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions (Dorr et al., 2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and We use neural networks to implement the classifier-ensemble approach, as these have previously been shown"
H05-1009,dorr-etal-2002-duster,1,0.844825,"language onto another. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al., 2001), and statistical machine translation. Although word-level aligners tend to perform well when there is sufficient training data, the quality decreases as the size of training data decreases. Even with large amounts of training data, statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions (Dorr et al., 2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and We use neural networks to implement the classifier-ensemble approach, as these have previously been shown to be effective for combining classifiers (Hansen and Salamon, 1990). Neural nets with 2 or more layers and non-linear activation functions are capable of learning any function of the feature space with arbitrarily small error. Neural nets have been shown to be effective with (1) highdimensional input vectors, (2) relatively sparse data, and (3) noisy data with high within-class variability, all of w"
H05-1009,W02-1004,0,0.0211778,"and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to each other by weighted links. As illustrated in Figure 1, an MLP consists 66 of one input layer, one or more hid"
H05-1009,A00-2005,0,0.0117646,"sociation clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to each other by weighted links. As illustrated in Figure 1, an MLP consists 66 of one input layer, one or more hidden layers, and one output layer. The ext"
H05-1009,N03-1017,0,0.0342375,"Missing"
H05-1009,P05-1057,0,0.153569,"Missing"
H05-1009,J00-2004,0,0.0720442,"s a pattern classification problem and treat alignment combination as a classifier ensemble (Hansen and Salamon, 1990; Wolpert, 1992). The ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce a globally optimal solution (Minsky, 1991). Introduction Parallel texts are a valuable resource in natural language processing and essential for projecting knowledge from one language onto another. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al., 2001), and statistical machine translation. Although word-level aligners tend to perform well when there is sufficient training data, the quality decreases as the size of training data decreases. Even with large amounts of training data, statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions (Dorr et al., 2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and We use neural networks to implement the classifier-ense"
H05-1009,P00-1056,0,0.0698415,"links (in the gold standard) for the same set of sentences. Precision (P r), recall (Rc) and alignment error rate (AER) are defined as follows: |A ∩ P | |A ∩ S| Rc = |A| |S| |A ∩ S |+ |A ∩ P | AER = 1 − |A |+ |S| 2. A set of 491 English-Chinese sentence pairs (nearly 13K words on each side) from 2002 NIST MT evaluation test set. We computed precision, recall and error rate on the entire set of sentence pairs for each data set.5 To evaluate NeurAlign, we used GIZA++ in both directions (E-to-F and F -to-E, where F is either Chinese (C) or Spanish (S)) as input and a refined alignment approach (Och and Ney, 2000) that uses a heuristic combination method called grow-diagfinal (Koehn et al., 2003) for comparison. (We henceforth refer to the refined-alignment approach as “RA.”) For the English-Spanish experiments, GIZA++ was trained on 48K sentence pairs from a mixed corpus (UN + Bible + FBIS), with nearly 1.2M of words on each side, using 10 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. For the English-Chinese experiments, we used 107K sentence pairs from FBIS corpus (nearly 4.1M English and 3.3M Chinese words) to train GIZA++, using 5 iterations of Model 1, 5 iterations of HM"
H05-1009,J03-1002,0,0.0194675,"iation for Computational Linguistics ai i Output layer wij j Hidden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yar"
H05-1009,E03-1026,0,0.0202233,"idden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The w"
H05-1009,W02-1012,0,0.043247,"Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 65–72, Vancouver, October 2005. 2005 Association for Computational Linguistics ai i Output layer wij j Hidden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some N"
H05-1009,H01-1035,0,0.0980243,"Missing"
H05-1009,W96-0213,0,\N,Missing
H05-1009,P02-1038,0,\N,Missing
H05-1009,C98-1029,0,\N,Missing
H05-1024,ayan-etal-2004-multi,1,0.815206,"Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al. (2004), where manually-crafted rules are used to correct alignment links related to language divergences. Our approach differs, however, in that the rules are extracted automatically—not manually—by examining an initial alignment and categorizing the errors according to features of the words. 186 Annotated Corpus Initial Annotation Rule Instantiation Rule Application Templates Ground Truth Best Rule Selection Rules Figure 1: TBL Architecture 3 Related Work Corpus Transformation-based Learning As shown in Figure 1, the input to TBL is an unannotated corpus that is first passed to an initial annotator"
H05-1024,J95-4004,0,0.248379,"Missing"
H05-1024,J93-2003,0,0.00596484,"185–192, Vancouver, October 2005. 2005 Association for Computational Linguistics The rest of the paper is organized as follows: In the next section we describe previous work on improving word alignments. Section 3 presents a brief overview of TBL. Section 4 describes the adaptation of TBL to the word alignment problem. Section 5 compares ALP to various alignments and presents results on English-Spanish and EnglishChinese. We show that ALP yields a significant reductions in alignment error rate over that of the best performing alignment system. 2 One of the major problems with the IBM models (Brown et al., 1993) and the HMM models (Vogel et al., 1996) is that they are restricted to the alignment of each source-language word to at most one targetlanguage word. The standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These ap"
H05-1024,P03-1012,0,0.0427099,"rchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al. (2004), where manually-crafted rules are used to correct alignment"
H05-1024,P97-1003,0,0.0429404,"links based on linguistic features of words, rather than the words themselves. Using these features is what sets ALP apart from systems like the RA approach. Specifically, three features are used to instantiate the templates: • • • POS tags on both sides: We assign POS tags using the MXPOST tagger (Ratnaparkhi, 1996) for English and Chinese, and Connexor for Spanish. Dependency relations: ALP utilizes dependencies for a better generalization—if a dependency parser is available in either language. In our experiments, we used a dependency parser only in English (a version of the Collins parser (Collins, 1997) that has been adapted for building dependencies) but not in the other language. A set of closed-class words: We use 16 different classes, 9 of which are different semantic verb classes while the other 7 are function words, prepositions, and complementizers.5 If both POS tags and dependency relations are available, they can be used together to instantiate the templates. That is, a word can be instantiated in a TBL template with: (1) a POS tag (e.g., Noun, Adj); (2) a relation (e.g., Subj, Obj); (3) a parameter class (e.g., Change of State); or (4) different subsets of (1)–(3). We also employ a"
H05-1024,P02-1033,0,0.0509733,"Missing"
H05-1024,dorr-etal-2002-duster,1,0.855239,"(e.g., Noun, Adj); (2) a relation (e.g., Subj, Obj); (3) a parameter class (e.g., Change of State); or (4) different subsets of (1)–(3). We also employ a more generalized form of instantiation, where words in the templates may match the keyword anything. 4.4 Best Rule Selection The rules are selected using two different metrics: The accuracy of the rule or the overall impact of the application of the rule on the entire data. Two different mechanisms may be used for selecting the best rule after generating all possible instantiations of templates: 5 These are based on the parameter classes of (Dorr et al., 2002). 189 1. 2. 5 Rule Accuracy: The goal is to minimize the errors introduced by the application of a transformation rule. To measure accuracy of a rule r, we use good(r)−2×bad(r), where good(r) is the number of alignment links that are corrected by the rule, and bad(r) is the number of incorrect alignment links produced. Overall impact on the training data: The accuracy mechanism (above) is useful for biasing the system toward higher precision. However, if the overall system is evaluated using a metric other than precision (e.g., recall), the accuracy mechanism may not guarantee that the best ru"
H05-1024,N03-1017,0,0.0337481,"Missing"
H05-1024,P05-1057,0,0.108584,"y, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al. (2004), where manually-crafted rules are used to correct alignment links related to language divergences. Our approach differs, however, in that the rules are extracted automatically—not manually—by e"
H05-1024,J00-2004,0,0.0628999,"Missing"
H05-1024,P00-1056,0,0.364521,"ults on English-Spanish and EnglishChinese. We show that ALP yields a significant reductions in alignment error rate over that of the best performing alignment system. 2 One of the major problems with the IBM models (Brown et al., 1993) and the HMM models (Vogel et al., 1996) is that they are restricted to the alignment of each source-language word to at most one targetlanguage word. The standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et"
H05-1024,J03-1002,0,0.0131704,"to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al"
H05-1024,W02-1012,0,0.0364367,"gnment of each source-language word to at most one targetlanguage word. The standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given a"
H05-1024,C96-2141,0,0.546546,"Missing"
H05-1024,H01-1035,0,0.127958,"Missing"
H05-1024,W96-0213,0,\N,Missing
habash-dorr-2002-handling,W98-1426,0,\N,Missing
habash-dorr-2002-handling,han-etal-2000-handling,0,\N,Missing
habash-dorr-2002-handling,habash-2000-oxygen,1,\N,Missing
habash-dorr-2002-handling,A00-1009,0,\N,Missing
habash-dorr-2002-handling,W01-1403,0,\N,Missing
habash-dorr-2002-handling,W00-0207,1,\N,Missing
habash-dorr-2002-handling,C00-1007,0,\N,Missing
habash-dorr-2002-handling,C00-2131,0,\N,Missing
habash-dorr-2002-handling,P95-1034,0,\N,Missing
habash-dorr-2002-handling,1997.mtsummit-workshop.12,0,\N,Missing
habash-dorr-2002-handling,P98-1116,0,\N,Missing
habash-dorr-2002-handling,C98-1112,0,\N,Missing
habash-dorr-2002-handling,dorr-etal-2002-duster,1,\N,Missing
J10-3003,P06-1002,1,0.223176,"rrespondences from the shown sentence pairs. (i1 , j1 ) × (i2 , j2 ) denotes the correspondence  fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown. paraphrases obtained via manually constructed word alignments is signiﬁcantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when"
J10-3003,P00-1059,0,0.00822974,"ujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases"
J10-3003,P05-1074,0,0.455238,"th such a corpus exploits both its parallel and bilingual natures: Align phrases across the two languages and consider all co-aligned phrases in the intended language to be paraphrases. The bilingual phrasal alignments can simply be generated by using the automatic techniques developed for the same task in the SMT literature. Therefore, arguably the most important factor affecting the performance of these techniques is usually the quality of the automatic bilingual phrasal (or word) alignment techniques. One of the most popular methods leveraging bilingual parallel corpora is that proposed by Bannard and Callison-Burch (2005). This technique operates exactly as described above by attempting to infer semantic equivalence between phrases in the same language indirectly with the second language as a bridge. Their approach builds on one of the initial steps used to train a phrase-based statistical machine translation system (Koehn, Och, and Marcu 2003). Such systems rely on phrase tables—a tabulation of correspondences between phrases in the source language and phrases in the target language. These tables are usually extracted by inducing word alignments between sentence pairs in a training corpus and then incremental"
J10-3003,W02-1022,0,0.00836419,"les or empty slots. The clustering is done so as to bring together sentences pertaining to the same topics and having similar structure. The word lattice is the product of an algorithm that computes a multiple-sequence alignment (MSA) for a cluster of sentences (Step 6). A very brief outline of such an algorithm, originally developed to compute an alignment for a set of three or more protein or DNA sequences, is as follows:9 1. Find the most similar pair of sentences in the cluster according to a similarity scoring function. For this approach, a simpliﬁed version of the edit-distance measure (Barzilay and Lee 2002) is used. 2. Align this sentence pair and replace the pair with this single alignment. 3. Repeat until all sentences have been aligned together. The word lattice so generated now needs to be converted into a slotted lattice to allow its use as a paraphrase template. Slotting is performed based on the following intuition: Areas of high variability between backbone nodes, that is, several distinct parallel paths, may correspond to template arguments and can be collapsed into one slot that can be ﬁlled by these arguments. However, multiple parallel paths may also appear in the lattice because of"
J10-3003,N03-1003,0,0.520334,"Missing"
J10-3003,P01-1008,0,0.675717,"ences from a set of sentences that represent the same (or similar) semantic content. We present four techniques in this section that generate paraphrases by ﬁnding such correspondences. The ﬁrst two techniques attempt to do so by relying, again, on the paradigm of distributional similarity: one by positing a bootstrapping distributional similarity algorithm and the other by simply adapting the previously described dependency path similarity algorithm to work with a parallel corpus. The next two techniques rely on more direct, non-distributional methods to compute the required correspondences. Barzilay and McKeown (2001) align phrasal correspondences by attempting to move beyond a single-pass distributional similarity method. They propose a bootstrapping algorithm that allows for the gradual reﬁnement of the features used to determine similarity and yields improved paraphrase pairs. As their input corpus, they use multiple human-written English translations of literary texts such as Madame Bovary and Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic expressions because different translators would use their own words while still preserving the meaning of the original text. The"
J10-3003,J05-3002,0,0.0450185,"el or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraphrase recognition. For a multi-document summarization system, detecting redundancy is a very important concern because two sentences from different documents may convey the same semantic content and it is important not to repeat the same information in the summary. On this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set of sentences by detecting paraphrastic parts and fusing them into a single coherent sentence. Recognizing similar semantic content is also critical for text simpliﬁcation systems (Marsi and Krahmer 2005b). Information extraction enables the detection of regularities of information structure—events which are reported many times, about different individuals and in different forms—and making them explicit so that they can be processed and used in other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together extraction patterns"
J10-3003,P08-1077,0,0.534592,"query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: P1 : ways to live with feline allergy P2 : how to deal with cat allergens Finally, paraphrases have also been used to improve the task of relation extraction (Romano et al. 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphrastic patterns for relation extraction by applying semi-supervised paraphrase induction to a very large monolingual corpus. For example, for the relation of “acquisition,” they collect: Original : X agreed to buy Y Variant 1 : X completed its acquisition of Y Variant 2 : X purchased Y 1.3.2 Expanding Sparse Human Reference Data for Evaluation. A large percentage of NLP applications are evaluated by having human annotators or subjects carry out the same 344 Madnani and Dorr Generating Phrasal and Sentential Paraphrases task for a given set of data and using the output so created"
J10-3003,I05-5001,0,0.00624394,"to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generati"
J10-3003,J93-2003,0,0.0580672,"Missing"
J10-3003,W06-2920,0,0.00560944,"multiple annual community-wide evaluations using Figure 9 An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008). 377 Computational Linguistics Volume 36, Number 3 standard test sets and manual as well as automated metrics, the task of automated paraphrasing does not. An obvious reason for this disparity could be that paraphrasing is not an application in and of itself. However, the existence of similar evaluations for other tasks that are not applications, such as dependency parsing (Buchholz and Marsi 2006; Nivre et al. 2007) and word sense disambiguation (Senseval), suggests otherwise. We believe that the primary reason is that, over the years, paraphrasing has been employed in an extremely fragmented fashion. Paraphrase extraction and generation are used in different forms and with different names in the context of different applications (for example: synonymous collocation extraction, query expansion). This usage pattern does not allow researchers in one community to share the lessons learned with those from other communities. In fact, it may even lead to research being duplicated across com"
J10-3003,D08-1021,0,0.594373,"c syntactic types that occur very rarely can be ignored and a less noisy paraphrase probability estimate can be computed, which may prove more useful in a downstream application than its counterpart computed via the unconstrained approach. We must also note that requiring syntactic constraints for pivot-based paraphrase extraction restricts the approach to those languages where a reasonably good parser is available. An obvious extension of the Callison-Burch style approach is to use the collection of pivoted English-to-English phrase pairs to generate sentential paraphrases for new sentences. Madnani et al. (2008a) combine the pivot-based approach to paraphrase acquisition with a well-deﬁned English-to-English translation model that is then used in an (unmodiﬁed) SMT system, yielding sentential paraphrases by means of “translating” input English sentences. However, instead of fully lexicalized phrasal correspondences as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former by replacing aligned sub-phrases with non-terminal symbols. For example, given the , growth rate has been effectively"
J10-3003,C08-1013,0,0.038034,"Missing"
J10-3003,N06-1003,0,0.42874,"Missing"
J10-3003,P04-1023,0,0.00930114,"Missing"
J10-3003,J07-2003,0,0.0572709,"Missing"
J10-3003,J08-4005,0,0.455743,"Missing"
J10-3003,D07-1008,0,0.00872769,"task is formulated differently. Overall, such a paraphrase corpus with detailed paraphrase annotations is much more informative than a corpus containing binary judgments at the sentence level such as the MSRP corpus. As an example, because the corpus contains paraphrase annotations at the word as well as phrasal levels, it can be used to build systems that can learn from these annotations and generate not only fully lexicalized phrasal paraphrases but also syntactically motivated paraphrastic patterns. To demonstrate the viability of the corpus for this purpose, a grammar induction algorithm (Cohn and Lapata 2007) is applied—originally developed for sentence compression—to the parsed version of their paraphrase corpus and the authors show that they can learn paraphrastic patterns such as those shown in Figure 9. In general, building paraphrase corpora, whether it is done at the sentence level or at the sub-sentential level, is extremely useful for the fostering of further research and development in the area of paraphrase generation. 5. Evaluation of Paraphrase Generation Whereas other language processing tasks such as machine translation and document summarization usually have multiple annual communit"
J10-3003,W05-1203,0,0.0195888,"nd cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3"
J10-3003,P09-1053,0,0.0546388,"stem can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraph"
J10-3003,W09-3102,0,0.0857492,"Missing"
J10-3003,C04-1051,0,0.912229,"Missing"
J10-3003,I05-5002,0,0.0129453,"ate paraphrase generation methods, it is important to examine some recent work that has been done on constructing paraphrase corpora. As part of this work, human subjects are generally asked to judge whether two given sentences are paraphrases of each other. We believe that a detailed examination of this manual evaluation task provides an illuminating insight into the nature of a paraphrase in a practical, rather than a theoretical, context. In addition, it has obvious implications for any method, whether manual or automatic, that is used to evaluate the performance of a paraphrase generator. Dolan and Brockett (2005) were the ﬁrst to attempt to build a paraphrase corpus on a large scale. The Microsoft Research Paraphrase (MSRP) Corpus is a collection of 5, 801 sentence pairs, each manually labeled with a binary judgment as to whether it constitutes a paraphrase or not. As a ﬁrst step, the corpus was created using a heuristic extraction method in conjunction with an SVM-based classiﬁer that was trained to select likely sentential paraphrases from a large monolingual corpus containing news article clusters. However, the more interesting aspects of the task were the subsequent evaluation of these extracted s"
J10-3003,P99-1011,0,0.0380591,"on of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murak"
J10-3003,J02-2001,0,0.0210905,"hata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques that can generate the types of paraphrases under examination in this survey: phrasal and sententi"
J10-3003,W07-1007,0,0.0191401,"adyto-use building blocks and by necessitating development of methods to effectively use such components for the unintended task of paraphrase generation. Domain-Speciﬁc Paraphrasing. Recently, work has been done to generate phrasal paraphrases in specialized domains. For example, in the ﬁeld of health literacy, it is well known that documents for health consumers are not very well-targeted to their purported audience. Recent research has shown how to generate a lexicon of semantically equivalent phrasal (and lexical) pairs of technical and lay medical terms from monolingual parallel corpora (Elhadad and Sutaria 2007) as well as monolingual comparable corpora (Del´eger and Zweigenbaum 2009). Examples include pairs such as myocardial infarction, heart attack and leucospermia, increased white cells in the sperm. In another domain, Max (2008) proposes an adaptation of the pivot-based method to generate rephrasings of short text spans that could help a writer revise a text. Because the goal is to assist a writer in making revisions, the rephrasings do not always need to bear a perfect paraphrastic relationship to the original, a scenario suited for the pivot-based method. Several variants of such adaptatio"
J10-3003,J07-3002,0,0.00594634,". (i1 , j1 ) × (i2 , j2 ) denotes the correspondence  fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown. paraphrases obtained via manually constructed word alignments is signiﬁcantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when using Arabic—a language signiﬁcantly different"
J10-3003,W04-0402,0,0.129018,"ur discussion. In this survey, we will be restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applicatio"
J10-3003,I05-5004,0,0.0371368,"Missing"
J10-3003,W07-1425,0,0.0336995,"e restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Ramb"
J10-3003,C08-1029,0,0.0607718,"Computational Linguistics Volume 36, Number 3 the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monol"
J10-3003,I08-1070,0,0.0671576,"Computational Linguistics Volume 36, Number 3 the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monol"
J10-3003,P91-1023,0,0.264302,"e-pass distributional similarity method. They propose a bootstrapping algorithm that allows for the gradual reﬁnement of the features used to determine similarity and yields improved paraphrase pairs. As their input corpus, they use multiple human-written English translations of literary texts such as Madame Bovary and Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic expressions because different translators would use their own words while still preserving the meaning of the original text. The parallel components are obtained by performing sentence alignment (Gale and Church 1991) on the corpora to obtain sets of parallel sentences that are then lemmatized, part-of-speech tagged and chunked in order to identify all the verb and noun phrases. The bootstrapping algorithm is then employed to incrementally learn better and better contextual features that are then leveraged to generate semantically similar phrasal correspondences. Figure 4 shows the basic steps of the algorithm. To seed the algorithm, some fake paraphrase examples are extracted by using identical words from either side of the aligned sentence pair. For example, given the following sentence pair: S1 : Emma b"
J10-3003,W04-0910,0,0.0447354,"Missing"
J10-3003,I05-5005,0,0.00974313,"g 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those"
J10-3003,P91-1026,0,0.147964,"r the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui a"
J10-3003,W01-0814,0,0.0234691,"e 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note tha"
J10-3003,P99-1044,0,0.0558834,"ssed in more detail in Section 3.1. 343 Computational Linguistics Volume 36, Number 3 In fact, in recent years, the information retrieval community has extensively explored the task of query expansion by applying paraphrasing techniques to generate similar or related queries (Beeferman and Berger 2000; Jones et al. 2006; Sahami and Hellman 2006; Metzler, Dumais, and Meek 2007; Shi and Yang 2007). The generation of paraphrases in these techniques is usually effected by utilizing the query log (a log containing the record of all queries submitted to the system) to determine semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original : simultaneous measurements Variant : concurrent measures Original : development area Variant : area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Original : X was invented by Y Variant 1 : Y’s invention of X Variant 2 : Y, inventor of X Riezler et al. (2007) expand a query b"
J10-3003,N06-1058,0,0.0120224,"acquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the ma"
J10-3003,N03-1017,0,0.0359916,"Missing"
J10-3003,N10-1017,0,0.258199,"oth treat paraphrasing as monolingual translation. However, as outlined in the discussion of that work, Quirk, Brockett, and Dolan use a relatively simplistic translation model and decoder which leads to paraphrases with little or no lexical variety. In contrast, Madnani et al. use a more complex translation model and an unmodiﬁed state-of-the-art SMT decoder to produce paraphrases that are much more diverse. Of course, the reliance of the latter approach on automatic word alignments does inevitably lead to much noisier sentential paraphrases than those produced by Quirk, Brockett, and Dolan. Kok and Brockett (2010) present a novel take on generating phrasal paraphrases with bilingual corpora. As with most approaches based on parallel corpora, they also start with phrase tables extracted from such corpora along with the corresponding phrasal translation probabilities. However, instead of performing the usual pivoting step with the bilingual phrases in the table, they take a graphical approach and represent each phrase in the table as a node, leading to a bipartite graph. Two nodes in the graph are connected to each other if they are aligned to each other. In order to extract paraphrases, they sample rand"
J10-3003,P98-2127,0,0.156252,"in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do"
J10-3003,2006.amta-papers.11,0,0.0050325,"he shown sentence pairs. (i1 , j1 ) × (i2 , j2 ) denotes the correspondence  fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown. paraphrases obtained via manually constructed word alignments is signiﬁcantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when using Arabic—a language"
J10-3003,W07-0716,1,0.805066,"iven source sentence can often be translated into the target language in many valid ways. Because there can be many “correct answers,” almost all models employed by SMT systems require, in addition to a large bitext, a held-out development set comprising multiple high-quality, human-authored reference translations in the target language in order to tune their parameters relative to a translation quality metric. However, given 345 Computational Linguistics Volume 36, Number 3 the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sent"
J10-3003,2008.amta-papers.13,1,0.729331,"c syntactic types that occur very rarely can be ignored and a less noisy paraphrase probability estimate can be computed, which may prove more useful in a downstream application than its counterpart computed via the unconstrained approach. We must also note that requiring syntactic constraints for pivot-based paraphrase extraction restricts the approach to those languages where a reasonably good parser is available. An obvious extension of the Callison-Burch style approach is to use the collection of pivoted English-to-English phrase pairs to generate sentential paraphrases for new sentences. Madnani et al. (2008a) combine the pivot-based approach to paraphrase acquisition with a well-deﬁned English-to-English translation model that is then used in an (unmodiﬁed) SMT system, yielding sentential paraphrases by means of “translating” input English sentences. However, instead of fully lexicalized phrasal correspondences as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former by replacing aligned sub-phrases with non-terminal symbols. For example, given the , growth rate has been effectively"
J10-3003,P09-3004,0,0.0392249,"ter set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraphrase recognition. Fo"
J10-3003,W05-1201,0,0.00805159,"eference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extra"
J10-3003,W05-1612,0,0.00688867,"eference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extra"
J10-3003,P79-1016,0,0.290719,"atic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin"
J10-3003,P97-1063,0,0.0169929,"Missing"
J10-3003,C04-1116,0,0.0150918,"1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the"
J10-3003,W06-3112,0,0.012125,"Missing"
J10-3003,N03-1024,0,0.086167,"Missing"
J10-3003,N07-1071,0,0.00738155,"on the root of the extracted path. For example, whereas verbs frequently tend to have several modiﬁers, nouns tend to have no more than one. However, if a word has any fewer than two modiﬁers, no path can go through it as the root. Therefore, the algorithm tends to perform better for paths with verbal roots. Another issue is that this algorithm, despite the use of more informative distributional features, can generate several incorrect or implausible paraphrase patterns (inference rules). Recent work has shown how to ﬁlter out incorrect inferences when using them in a downstream application (Pantel et al. 2007). Finally, there is no reason for the distributional features to be in the same language as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a 5 A demo of the algorithm is available online at http://demo.patrickpantel.com/Content/LexSem/ paraphrase.htm. 352 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Algorithm 2 (Lin and Pantel 2001). Produce inference rules from a parsed corpus. Summary. Adapt Harris’s (1954) hypothesis of distributional similarity for paths in dependency trees: If two tree paths have similar distributions such that they tend to lin"
J10-3003,P02-1040,0,0.0947883,"ubjects carry out the same 344 Madnani and Dorr Generating Phrasal and Sentential Paraphrases task for a given set of data and using the output so created as a reference against which to measure the performance of the system. The two applications where comparison against human-authored reference output has become the norm are machine translation and document summarization. In machine translation evaluation, the translation hypotheses output by a machine translation system are evaluated against reference translations created by human translators by measuring the n-gram overlap between the two (Papineni et al. 2002). However, it is impossible for a single reference translation to capture all possible verbalizations that can convey the same semantic content. This may unfairly penalize translation hypotheses that have the same meaning but use n-grams that are not present in the reference. For example, the given system output S will not have a high score against the reference R even though it conveys precisely the same semantic content: S: We must consider the entire community. R: We must bear in mind the community as a whole. One solution is to use multiple reference translations, which is expensive. An al"
J10-3003,I05-1011,0,0.656018,"Missing"
J10-3003,I05-5010,0,0.026154,"and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques"
J10-3003,W04-3219,0,0.504647,"Missing"
J10-3003,W95-0107,0,0.0135345,"en computing the alignment similarity score, two lexically matched words across a sentence pair are not considered to fully match unless their score on syntactic features also exceeds a preset threshold. The syntactic features constituting the additional constraints are deﬁned in terms of the output of a chunk parser. Such a parser takes as input the syntactic trees of the sentences in a topic cluster and provides the following information for each word: r r r Part-of-speech tag IOB tag. This is a notation denoting the constituent covering a word and its relative position in that constituent (Ramshaw and Marcus 1995). If a word has the tag I-NP, we can infer that the word is covered by an NP and located inside that NP. Similarly, B denotes that the word is at the beginning and O denotes that the word is not covered by any constituent. IOB chain. A concatenation of all IOB tags going from the root of the tree to the word under consideration. With this information and a heuristic to compute the similarity between two words in terms of their POS and IOB tags, the alignment similarity score can be calculated as the sum of the heuristic similarity value for the given two words and the heuristic similarity valu"
J10-3003,P02-1006,0,0.0166785,"similar or related queries (Beeferman and Berger 2000; Jones et al. 2006; Sahami and Hellman 2006; Metzler, Dumais, and Meek 2007; Shi and Yang 2007). The generation of paraphrases in these techniques is usually effected by utilizing the query log (a log containing the record of all queries submitted to the system) to determine semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original : simultaneous measurements Variant : concurrent measures Original : development area Variant : area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Original : X was invented by Y Variant 1 : Y’s invention of X Variant 2 : Y, inventor of X Riezler et al. (2007) expand a query by generating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query ho"
J10-3003,P05-1077,0,0.060094,"Missing"
J10-3003,P07-1059,0,0.0988993,"semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original : simultaneous measurements Variant : concurrent measures Original : development area Variant : area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Original : X was invented by Y Variant 1 : Y’s invention of X Variant 2 : Y, inventor of X Riezler et al. (2007) expand a query by generating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: P1 : ways to live with feline allergy P2 : how to deal with cat allergens Finally, paraphrases have also been used to improve the task of"
J10-3003,E06-1052,0,0.0204548,"nerating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: P1 : ways to live with feline allergy P2 : how to deal with cat allergens Finally, paraphrases have also been used to improve the task of relation extraction (Romano et al. 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphrastic patterns for relation extraction by applying semi-supervised paraphrase induction to a very large monolingual corpus. For example, for the relation of “acquisition,” they collect: Original : X agreed to buy Y Variant 1 : X completed its acquisition of Y Variant 2 : X purchased Y 1.3.2 Expanding Sparse Human Reference Data for Evaluation. A large percentage of NLP applications are evaluated by having human annotators or subjects carry out the same 344 Madnani and Dorr Generating Phrasal and Sentential Paraphrases task for a gi"
J10-3003,I05-5011,0,0.0157859,"old), again based on the named entities they share. 5. For each pair of similar sentences, compare their respective attached patterns. If the variables in the patterns link to the same or comparable named entities (based on the entity text and type), then consider the patterns to be paraphrases of each other. At the end, the output is a list of generalized paraphrase patterns with named entity types as variables. For example, the algorithm may generate the following two patterns as paraphrases: PERSON is promoted to POST the promotion of PERSON to POST is decided As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords. The idea of enlisting named entities as proxies for detecting semantic equivalence is interesting and has certainly been explored before (see the discussion regarding Pas¸ca and Dienes [2005] in Section 3.2). However, it has some obvious disadvantages. The authors manually evaluate the technique by generating paraphrases for two speciﬁc 8 Although the authors provide motivating examples in Japanese"
J10-3003,P06-2094,0,0.00885352,"not to repeat the same information in the summary. On this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set of sentences by detecting paraphrastic parts and fusing them into a single coherent sentence. Recognizing similar semantic content is also critical for text simpliﬁcation systems (Marsi and Krahmer 2005b). Information extraction enables the detection of regularities of information structure—events which are reported many times, about different individuals and in different forms—and making them explicit so that they can be processed and used in other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together extraction patterns to improve the cohesion of the extracted information. Another recently proposed natural language processing task is that of recognizing textual entailment: A piece of text T is said to entail a hypothesis H if humans reading T will infer that H is most likely true. The observant reader will notice that, in this sense, the task of paraphrase recognition can simply be formulated as bidirectional entailment recognition. The task of recognizing entailment is an application-independent task and has important ramiﬁcatio"
J10-3003,P06-2096,0,0.530022,"gram similarities (sets of shared overlapping word sequences) between a large number of sentences. However, the two approaches are also different in that Pang, Knight, and Marcu use the parse trees of all sentences in a cluster to compute the alignment (and build the lattice), whereas Barzilay and Lee use only surface level information. Furthermore, Barzilay and Lee can use their slotted lattice pairs to generate paraphrases for novel and unseen sentences, whereas Pang, Knight, and Marcu cannot paraphrase new sentences at all. 366 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Shen et al. (2006) attempt to improve Barzilay and Lee’s technique by trying to include syntactic constraints in the cluster alignment algorithm. In that way, it is doing something similar to what Pang, Knight, and Marcu do but with a comparable corpus instead of a parallel one. More precisely, whereas Barzilay and Lee use a relatively simple alignment scoring function based on purely lexical features, Shen et al. try to bring syntactic features into the mix. The motivation is to constrain the relatively free nature of the alignment generated by the MSA algorithm—which may lead to the generation of grammaticall"
J10-3003,I05-1021,0,0.0134205,"urvey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 200"
J10-3003,H90-1004,0,0.239484,"sj and a constant u. This is necessary to handle words from the sentence that do not occur anywhere in the set of paraphrases. Figure 6 shows an example lattice. Once the lattice has been constructed, it is straightforward to extract the 1-best paraphrase by using a dynamic programming algorithm such as Viterbi decoding and extracting the optimal path from the lattice as scored by the product of an n-gram language model and the replacement model. In addition, as with SMT decoding, it is also possible to extract a list of n-best paraphrases from the lattice by using the appropriate algorithms (Soong and Huang 1990; Mohri and Riley 2002). Quirk, Brockett, and Dolan (2004) borrow from the statistical machine translation literature so as to align phrasal equivalences as well as to utilize the aligned phrasal equivalences to rewrite new sentences. The biggest advantage of this method is its SMT inheritance: It is possible to produce multiple sentential paraphrases for any new Figure 6 A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris. Alternate paths between various nodes represent phrasal replacements. The probability values associated with each edge are not shown for the"
J10-3003,P07-1058,0,0.0112734,"Missing"
J10-3003,W05-1205,0,0.0191222,"such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Tex"
J10-3003,P03-1016,0,0.079514,"ord has any fewer than two modiﬁers, no path can go through it as the root. Therefore, the algorithm tends to perform better for paths with verbal roots. Another issue is that this algorithm, despite the use of more informative distributional features, can generate several incorrect or implausible paraphrase patterns (inference rules). Recent work has shown how to ﬁlter out incorrect inferences when using them in a downstream application (Pantel et al. 2007). Finally, there is no reason for the distributional features to be in the same language as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a 5 A demo of the algorithm is available online at http://demo.patrickpantel.com/Content/LexSem/ paraphrase.htm. 352 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Algorithm 2 (Lin and Pantel 2001). Produce inference rules from a parsed corpus. Summary. Adapt Harris’s (1954) hypothesis of distributional similarity for paths in dependency trees: If two tree paths have similar distributions such that they tend to link the same set of words, then they likely mean the same thing and together generate an inference rule. 1: Extract paths of the form described above from the"
J10-3003,P09-1094,0,0.474492,"eries similar to the phrase, (2) deﬁnitions from the Encarta dictionary, (3) a monolingual parallel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and then combined in a log-linear paraphrasing-as-translation model proposed by Madnani et al. (2007). A manual inspection reveals that using multiple sources of information yields paraphrases with much higher accuracy. We believe that such exploitation of multiple types of resources and their combinations is an important development. Zhao et al. (2009) further increase the utility of this combination approach by incorporating application speciﬁc constraints on the pivoted paraphrases. For example, if the output paraphrases need to be simpliﬁed versions of the input sentences, then only those phrasal paraphrase pairs are used where the output is shorter than the input. 380 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Use of SMT Machinery. In theory, statistical machine translation is very closely related to paraphrase generation since the former also relies on ﬁnding semantic equivalence, albeit in a second language. Hence,"
J10-3003,P08-1116,0,0.123234,"nﬂuence of the Web will extend to other avenues of paraphrase generation such as the aforementioned extrinsic evaluation or lack thereof. For example, Fujita and Sato (2008b) propose evaluating phrasal paraphrase pairs, automatically generated from a monolingual corpus, by querying the Web for snippets related to the pairs and using them as features to compute the pair’s paraphrasability. Combining Multiple Sources of Information. Another important trend in paraphrase generation is that of leveraging multiple sources of information to determine whether two units are paraphrastic. For example, Zhao et al. (2008) improve the sentential paraphrases that can be generated via the pivot method by leveraging ﬁve other sources in addition to the bilingual parallel corpus itself: (1) a corpus of Web queries similar to the phrase, (2) deﬁnitions from the Encarta dictionary, (3) a monolingual parallel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and then combined in a log-linear paraphrasing-as-translation model proposed by Madnani et al. (2007). A manual inspection reveals that using multiple"
J10-3003,W06-1610,0,0.00596023,"olution is to use multiple reference translations, which is expensive. An alternative solution, tried in a number of recent approaches, is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference translation so as to award credit to parts of the translation hypothesis that are semantically, even if not lexically, correct (Owczarzak et al. 2006; Zhou, Lin, and Hovy 2006). In evaluation of document summarization, automatically generated summaries (peers) are also evaluated against reference summaries created by human authors (models). Zhou et al. (2006) propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of n-gram overlap between peer summaries and multiple model summaries. 1.3.3 Machine Translation. Besides being used in evaluation of machine translation systems, paraphrasing has also been applied to directly improve the translation process. Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to improve a statistical phrase-based machine translation system. Such a system works by dividing the given sentence into phrases and trans"
J10-3003,N06-1057,0,0.0316833,"olution is to use multiple reference translations, which is expensive. An alternative solution, tried in a number of recent approaches, is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference translation so as to award credit to parts of the translation hypothesis that are semantically, even if not lexically, correct (Owczarzak et al. 2006; Zhou, Lin, and Hovy 2006). In evaluation of document summarization, automatically generated summaries (peers) are also evaluated against reference summaries created by human authors (models). Zhou et al. (2006) propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of n-gram overlap between peer summaries and multiple model summaries. 1.3.3 Machine Translation. Besides being used in evaluation of machine translation systems, paraphrasing has also been applied to directly improve the translation process. Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to improve a statistical phrase-based machine translation system. Such a system works by dividing the given sentence into phrases and trans"
J10-3003,J93-1004,0,\N,Missing
J10-3003,W03-1608,0,\N,Missing
J10-3003,J09-1008,0,\N,Missing
J10-3003,J90-2002,0,\N,Missing
J10-3003,W07-1401,0,\N,Missing
J10-3003,C98-2122,0,\N,Missing
J10-3003,N10-4008,0,\N,Missing
J10-3003,W07-1400,0,\N,Missing
J10-3003,D07-1096,0,\N,Missing
J10-3003,W05-1605,0,\N,Missing
J10-3003,W04-0400,0,\N,Missing
J10-3003,P93-1024,0,\N,Missing
J12-2006,P98-1013,0,0.198951,"Missing"
J12-2006,2010.amta-papers.7,1,0.863358,"s produced by the taggers described here. The resulting system signiﬁcantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu–English test set. This ﬁnding supports the hypothesis that both syntactic and semantic information can improve translation quality. 1. Introduction This article describes the resource- and system-building efforts of an 8-week Johns Hopkins Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT) (Baker et al. 2010a, 2010b, 2010c, 2010d). Speciﬁcally, we describe our modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that were built using the lexicon and annotation scheme. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). Two examples of MN tagging are shown in Figure 1. Note that modality and negation are uniﬁed into single MN tags (e.g., the “Able” modality tag is combin"
J12-2006,baker-etal-2010-modality,1,0.821878,"s produced by the taggers described here. The resulting system signiﬁcantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu–English test set. This ﬁnding supports the hypothesis that both syntactic and semantic information can improve translation quality. 1. Introduction This article describes the resource- and system-building efforts of an 8-week Johns Hopkins Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT) (Baker et al. 2010a, 2010b, 2010c, 2010d). Speciﬁcally, we describe our modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that were built using the lexicon and annotation scheme. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). Two examples of MN tagging are shown in Figure 1. Note that modality and negation are uniﬁed into single MN tags (e.g., the “Able” modality tag is combin"
J12-2006,P05-1033,0,0.0907455,"-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process by adding semantic notations onto the syntactic non-terminals produced by a Penn Treebank trained parser, thus making the categories more informative. In the parsing domain, the work of Petrov and Klein (2007) is related to the current work. In their work, rule splitting and rule merging are applied to reﬁne parse trees during machine learning. Hierarchical splitting leads to the creation of learned categor"
J12-2006,W09-3012,1,0.662316,"ators identify explicit certainty markers and also take into account Perspective, Focus, and Time. Focus separates certainty into facts and opinions, to include attitudes. In our scheme, Focus would be covered by want and belief modality. Also, separating focus and uncertainty can allow the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010) describe a scheme for automatic committed belief tagging. Committed belief indicates the writer believes the proposition. The authors use a previously annotated corpus of committed belief, non-committed belief, and not applicable (Diab et al. 2009), and derive features for machine learning from parse trees. The authors desire to combine their work with FactBank annotation. The CoNLL-2010 shared task (Farkas et al. 2010) was about the detection of cues for uncertainty and their scope. The task was described as “hedge detection,” that is, ﬁnding statements which do not or cannot be backed up with facts. Auxiliary verbs such as may, might, can, and so forth, are one type of hedge cue. The training data for the shared task included the BioScope corpus (Szarvas et al. 2008), which is manually annotated with negation and speculation cues and"
J12-2006,W10-3001,0,0.0447283,"Missing"
J12-2006,N06-1031,0,0.116492,"one manually by students who were provided a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities, modality, and negation automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added beneﬁt of the extracted translation rules is that they are capable of producing semantically tagged Urdu parses, despite the fact that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes that of Huang and Knight (2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making"
J12-2006,P03-1054,0,0.00282903,"ed by only an English parser and tagger. Related work in syntax-based MT includes that of Huang and Knight (2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic p"
J12-2006,W04-3250,0,0.0152888,"dually, each of these made modest improvements over the syntactically informed system alone. Grafting named entities onto the parse trees improved the Bleu score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simultaneously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax alone. This improvement was the largest improvement that we got from anything other than the move from linguistically naive models to syntactically informed models. We used bootstrap resampling to test whether the differences in Bleu scores were statistically signiﬁcant (Koehn 2004). All of the results were a signiﬁcant improvement over Hiero (at p ≤ 0.01). The difference between the syntactic system and the syntactic system with named entities is not signiﬁcant (p = 0.38). The differences between the 5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu words) and four reference translations per sentence. The BLEU score for these experiments is measured on uncased output. 432 Baker et al. Modality and Negation in SIMT syntactic system and the syntactic system with MN, and between the syntactic system and the syntactic system w"
J12-2006,P07-2045,1,0.0084265,"s in this article including the training, development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a split of the NIST08 Urdu–English test set, and the blind test set was NIST09. Urdu set training dev devtest test English lines tokens types tokens types 202k 981 883 1,792 1.7M 21k 22k 42k 56k 4k 4k 6k 1.7M 19k 19–20k 38–41k 51k 4k 4k 5k Table 1) results in signiﬁcantly degraded translation quality compared, for example, to an Arabic–English system that has more than 100 times the amount of training data. The output in Figure 2 was produced using Moses (Koehn et al. 2007), a state-ofthe-art phrase-based MT system that by default does not incorporate any linguistic information (e.g., syntax or morphology or transliteration knowledge). As a result, words that were not directly observed in the bilingual training data were untranslatable. Names, in particular, are problematic. For example, the lack of translation for Nagaland and Nagas induces multiple omissions throughout the translated text, thus producing several instances where the holder of a claim (or belief ) is missing. This is because out-ofvocabulary words are deleted from the Moses output. We use syntac"
J12-2006,W09-0424,1,0.73648,"elements are subsequently used in the translation rules that are extracted from the parallel corpus. The goal of adding them to the translation rules is to constrain the space of possible translations to more grammatical and more semantically coherent output. We explored whether including such semantic elements could improve translation output in the face of sparse training data and few source language annotations. Results were encouraging. Translation quality, as measured by the Bleu metric (Papineni et al. 2002), improved when the training process for the Joshua machine translation system (Li et al. 2009) used in the SCALE workshop included MN annotation. We were particularly interested in identifying modalities and negation because they can be used to characterize events in a variety of automated analytic processes. Modalities and negation can distinguish realized events from unrealized events, beliefs from certainties, and can distinguish positive and negative instances of entities and events. For example, the correct identiﬁcation and retention of negation in a particular language—such as a single instance of the word “not”—is very important for a correct representation of events and likewi"
J12-2006,J93-2004,0,0.0409281,"Missing"
J12-2006,A00-2030,1,0.384287,"Missing"
J12-2006,Y05-1014,0,0.102007,"rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and Karttunen 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modiﬁers. 415 Computational Linguistics Volume 38, Number 2 Earlier work describing the difﬁculty of correctly translating modality using ma´ chine translation includes Sigurd and Gawronska (1994) and Murata et al. (2005). Sigurd ´ and Gawronska (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to speciﬁcally handle modal constructions, whereas our modal annotation approach is a part of a full translation system. We now consider other literature, relating to tree-grafting and machine tra"
J12-2006,W06-3907,0,0.0592182,"Missing"
J12-2006,J05-1004,0,0.125993,"Missing"
J12-2006,P02-1040,0,0.088684,"ed to the source language (in our case, Urdu) during a process of syntactic alignment. These semantic elements are subsequently used in the translation rules that are extracted from the parallel corpus. The goal of adding them to the translation rules is to constrain the space of possible translations to more grammatical and more semantically coherent output. We explored whether including such semantic elements could improve translation output in the face of sparse training data and few source language annotations. Results were encouraging. Translation quality, as measured by the Bleu metric (Papineni et al. 2002), improved when the training process for the Joshua machine translation system (Li et al. 2009) used in the SCALE workshop included MN annotation. We were particularly interested in identifying modalities and negation because they can be used to characterize events in a variety of automated analytic processes. Modalities and negation can distinguish realized events from unrealized events, beliefs from certainties, and can distinguish positive and negative instances of entities and events. For example, the correct identiﬁcation and retention of negation in a particular language—such as a single"
J12-2006,C10-2117,0,0.0335943,"Missing"
J12-2006,prasad-etal-2008-penn,0,0.0139365,"al predicates. ¨ The Prague Dependency Treebank (Hajiˇc et al. 2001; Bohmov´ a, Cinkov´a, and Hajiˇcov´a 2005) (PDT) is a multi-level system of annotation for texts in Czech and other languages, with its roots in the Prague school of linguistics. Besides a morphological layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical layer includes functional relationships, dependency relations, and co-reference. The PDT also integrates propositional and extra-propositional meanings in a single annotation framework. The Penn Discourse Treebank (PDTB) (Webber et al. 2003; Prasad et al. 2008) annotates discourse connectives and their arguments over a portion of the Penn Treebank. Within this framework, senses are annotated for the discourse connectives in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is the Conditional tag, which includes hypothetical, general, unreal present, unreal past, factual present, and factual past arguments. The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for establishing the importance of attributing a belief or assertion expressed in text to its agent (equivalent to the notion of holder in our sc"
J12-2006,P08-1001,0,0.123143,"he rule extraction algorithm with augmented parse trees containing syntactic labels that have semantic annotations grafted onto them so that they additionally express semantic information. Our strategy for producing semantically grafted parse trees involves three steps: 1. The English sentences in the parallel training data are parsed with a syntactic parser. In our work, we used the lexicalized probabilistic context free grammar parser provided by Basis Technology Corporation. 2. The English sentences are MN-tagged by the system described herein and named-entity-tagged by the Phoenix tagger (Richman and Schone 2008). 3. The modality/negation and entity markers are grafted onto the syntactic parse trees using a tree-grafting procedure. The grafting procedure was implemented as part of the SIMT effort. Details are further spelled out in Section 7.2. Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that although we focus the discussion here on the modality and negation, our framework is general and we were able to incorporate other semantic elements (speciﬁcally, named entities) into the SIMT effort. Once the semantically grafted trees have been produced for the parallel corpus, t"
J12-2006,N07-2036,0,0.0114641,"tion is asserted to be real or not real). A major annotation effort for temporal and event expressions is the TimeML speciﬁcation language, which has been developed in the context of reasoning for question answering (Saur´ı, Verhagen, and Pustejovsky 2006). TimeML, which includes modality annotation on events, is the basis for creating the TimeBank and FactBank corpora (Pustejovsky et al. 2006; Saur´ı and Pustejovsky 2009). In FactBank, event mentions are marked with their degree of factuality. Recent work incorporating modality annotation includes work on detecting certainty and uncertainty. Rubin (2007) describes a scheme for ﬁve levels of certainty, referred to as Epistemic modality, in news texts. Annotators identify explicit certainty markers and also take into account Perspective, Focus, and Time. Focus separates certainty into facts and opinions, to include attitudes. In our scheme, Focus would be covered by want and belief modality. Also, separating focus and uncertainty can allow the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010) describe a scheme for automatic committed belief tagging. Committed belief indicates the writer believes the proposition. The a"
J12-2006,C94-1018,0,0.350259,"l. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and Karttunen 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modiﬁers. 415 Computational Linguistics Volume 38, Number 2 Earlier work describing the difﬁculty of correctly translating modality using ma´ chine translation includes Sigurd and Gawronska (1994) and Murata et al. (2005). Sigurd ´ and Gawronska (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to speciﬁcally handle modal constructions, whereas our modal annotation approach is a part of a full translation system. We now consider other literature, relating to tree"
J12-2006,P99-1039,0,0.0211303,"e 10 illustrates how modality tags are grafted onto a parse tree. Note that although we focus the discussion here on the modality and negation, our framework is general and we were able to incorporate other semantic elements (speciﬁcally, named entities) into the SIMT effort. Once the semantically grafted trees have been produced for the parallel corpus, the trees are presented, along with word alignments (produced by the Berkeley aligner), to the rule extraction software to extract synchronous grammar rules that are both 1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999). 428 Baker et al. Modality and Negation in SIMT syntactically and semantically informed. These grammar rules are used by the decoder to produce translations. In our experiments, we used the Joshua decoder (Li et al. 2009), the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special purpose-built tree-grafting software. Figure 11 shows example semantic rules that are used by the decoder. The verb phrase rules are augmented with modality and negation, taken from the semantic categories listed in Table 2. Because these get marked on the Urdu source as well as the English tran"
J12-2006,W08-0606,0,0.0640498,"d corpus of committed belief, non-committed belief, and not applicable (Diab et al. 2009), and derive features for machine learning from parse trees. The authors desire to combine their work with FactBank annotation. The CoNLL-2010 shared task (Farkas et al. 2010) was about the detection of cues for uncertainty and their scope. The task was described as “hedge detection,” that is, ﬁnding statements which do not or cannot be backed up with facts. Auxiliary verbs such as may, might, can, and so forth, are one type of hedge cue. The training data for the shared task included the BioScope corpus (Szarvas et al. 2008), which is manually annotated with negation and speculation cues and their scope, and paragraphs from Wikipedia possibly containing hedge information. Our scheme also identiﬁes cues in the form of triggers, but our desired outcome is to cover the full range of modalities and not just certainty and uncertainty. To identify scope, we use syntactic parse trees, as was allowed in the CoNLL task. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based r"
J12-2006,N07-1063,0,0.0476172,"Missing"
J12-2006,J10-2004,0,0.0138227,"is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process"
J12-2006,J09-3003,0,0.010836,"Missing"
J12-2006,W06-3119,0,0.00897083,"xternal information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process by adding semantic notations onto the syntactic non-terminals produced by a Penn Treebank trained parser, thus making the categories more informative. In the pa"
J12-2006,W08-2225,0,\N,Missing
J12-2006,J03-4002,0,\N,Missing
J12-2006,C98-1013,0,\N,Missing
J13-3004,J90-1003,0,0.118002,"onyms set. We will refer to the common terms (agitation in this example) as the focus words. Because we also wanted to compare occurrence statistics of the high-contrast set with the random pairs set, we created the control set of random pairs by taking each of the focus words and pairing them with another word in WordNet that has a frequency of occurrence in BNC closest to the term contrasting with the focus word. This is to ensure that members of the pairs across the high-contrast set and the control set have similar unigram frequencies. We calculated the pointwise mutual information (PMI) (Church and Hanks 1990) for each of the word pairs in the high-contrast set, the random pairs set, and the synonyms set using unigram and co-occurrence frequencies in the BNC. If two words occurred within a window of five adjacent words in a sentence, they were marked as co-occurring (same window as Church and Hanks [1990] used in their seminal work on word–word associations). Table 7 shows the average and standard deviation in each set. 7 If both members of a pair have WordNet synonyms, then one is chosen at random, and its synonym is taken. 8 WordNet lists synonyms in order of decreasing frequency in the SemCor co"
J13-3004,P08-1118,0,0.0607305,"Missing"
J13-3004,P97-1023,0,0.224847,"tors for each using dictionary definitions. The approach was evaluated on only a handful of word pairs. There is a large amount of work on sentiment analysis and opinion mining aimed at determining the polarity of words (Pang and Lee 2008). For example, Pang, Lee, and Vaithyanathan (2002) detected that adjectives such as dazzling, brilliant, and gripping cast their qualifying nouns positively whereas adjectives such as bad, cliched, and boring portray the noun negatively. Many of these gradable adjectives have opposites, but these approaches, with the exception of that of Hatzivassiloglou and McKeown (1997), did not attempt to determine pairs of positive and negative polarity words that are opposites. Hatzivassiloglou and McKeown proposed a supervised algorithm that uses word usage patterns to generate a graph with adjectives as nodes. An edge between two nodes indicates either that the two adjectives have the same or opposite polarity. A clustering algorithm then partitions the graph into two subgraphs such that the nodes in a subgraph have the same polarity. They used this method to create a lexicon of positive and negative words, and argued that the method could also be used to detect opposit"
J13-3004,C92-2082,0,0.0872931,"in GRE “most contrasting word” questions are not listed as antonyms in WordNet. We should not infer from this that WordNet or any other lexicographic resource is a poor source for detecting opposites, but rather that identifying the large number of contrasting word pairs requires further computation, possibly relying on other semantic relations stored in the lexicographic resource. Even though a number of computational approaches have been proposed for semantic closeness (Curran 2004; Budanitsky and Hirst 2006), and some for hypernymy– 556 Mohammad et al. Computing Lexical Contrast hyponymy (Hearst 1992), measures of lexical contrast have been less successful. To some extent, this is because lexical contrast is not as well understood as other classical lexical– semantic relations. Over the years, many definitions of semantic contrast and opposites have been proposed by linguists (Lehrer and Lehrer 1982; Cruse 1986), cognitive scientists (Kagan 1984), psycholinguists (Deese 1965), and lexicographers (Egan 1984), which differ from each other in various respects. Cruse (1986, page 197) observes that even though people have a robust intuition of opposites, “the overall class is not a well-defined"
J13-3004,J91-1001,0,0.858055,"entary pairs, but poorly on disjoint opposite pairs. Among different parts of speech, the method performs best on noun pairs, and relatively worse on verb pairs. All of the data created and compiled as part of this research are summarized in Table 18 (Section 8), and is available for download.3 2. Related Work Charles and Miller (1989) proposed that opposites occur together in a sentence more often than chance. This is known as the co-occurrence hypothesis. Paradis, Willners, and Jones (2009) describe further experiments to show how canonical opposites tend to have high textual co-occurrence. Justeson and Katz (1991) gave evidence in support of the hypothesis using 35 prototypical opposites (from an original set of 39 opposites compiled by Deese [1965]) and also with an additional 22 frequent opposites. They also showed that opposites tend to occur in parallel syntactic constructions. All of these pairs were adjectives. Fellbaum (1995) conducted similar experiments on 47 noun, verb, adjective, and adverb pairs (noun–noun, noun–verb, noun–adjective, verb–adverb, etc.) pertaining to 18 concepts (for example, lose(v)–gain(n) and loss(n)–gain(n), where lose(v) and loss(n) pertain to the concept of “failing to"
J13-3004,P98-2127,0,0.762129,"Word Pairs in Text As pointed out earlier, there is work on a small set of opposites showing that opposites co-occur more often than chance (Charles and Miller 1989; Fellbaum 1995). Section 5.1 describes experiments on a larger scale to determine whether highly contrasting word pairs (including opposites) occur together more often than randomly chosen word pairs of similar frequency. The section also compares co-occurrence associations with synonyms. Research in distributional similarity has found that entries in distributional thesauri tend to also contain terms that are opposite in meaning (Lin 1998; Lin et al. 2003). Section 5.2 describes experiments to determine whether highly contrasting word pairs (including opposites) occur in similar contexts as often as randomly chosen pairs of words with similar frequencies, and whether highly contrasting words occur in similar contexts as often as synonyms. 5.1 Co-Occurrence In order to compare the tendencies of highly contrasting word pairs, synonyms, and random word pairs to co-occur in text, we created three sets of word pairs: the highcontrast set, the synonyms set, and the control set of random word pairs. The high-contrast set was created"
J13-3004,P02-1047,0,0.0671634,"Missing"
J13-3004,W11-2128,0,0.0852512,"Missing"
J13-3004,H05-1067,0,0.027602,"Missing"
J13-3004,D09-1063,1,0.520022,"Missing"
J13-3004,D07-1060,1,0.643146,"r, that as of February 2012, most of the Mechanical Turk participants are native speakers of English, certain Indian languages, and some European languages. Our future goals include porting this approach to a cross-lingual framework to determine lexical contrast in a resource-poor language by using a bilingual lexicon to connect the words in that language with words in another resource-rich language. We can then use the structure of the thesaurus from the resource-rich language as described in this article to detect contrasting categories of terms. This is similar to the approach described by Mohammad et al. (2007), who compute semantic distance in a resourcepoor language by using a bilingual lexicon and a sense disambiguation algorithm to connect text in the resource-poor language with a thesaurus in a different language. This enables automatic discovery of lexical contrast in a language even if it does not have a Roget-like thesaurus. The cross-lingual method still requires a bilingual lexicon to map words between the target language and the language with the thesaurus, however. 587 Computational Linguistics Volume 39, Number 3 Our method used only one Roget-like published thesaurus, but even more gai"
J13-3004,W02-1011,0,0.021522,"Missing"
J13-3004,C02-1061,0,0.33566,"Missing"
J13-3004,C08-1114,1,0.717504,"sites and synonyms? How does the proposed method perform when compared with other automatic methods? Experiments: We conduct three experiments (described in Sections 7.1, 7.2, and 7.3) involving three different data sets and two tasks to answer 2 Note that though linguists have classified opposites into different kinds, we know of no work doing so for contrasts more generally. Thus this particular analysis must be restricted to opposites alone. 559 Computational Linguistics Volume 39, Number 3 these questions. We compare performance of our method with methods proposed by Lin et al. (2003) and Turney (2008). We automatically generate a new set of 1,296 “most contrasting word” questions to evaluate performance of our method on five different kinds of opposites and across four parts of speech. (The evaluation described in Section 7.1 was first described in Mohammad, Dorr, and Hirst [2008].) Findings: We find that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods. Our method performs best on gradable pairs, antipodal pairs, and complementary pairs, but poorly on disjoint opposite pairs. Among different parts of speech, the method perf"
J13-3004,P08-1008,0,0.0266766,"Missing"
J13-3004,D08-1103,1,\N,Missing
J13-3004,J06-1003,1,\N,Missing
J13-3004,C98-2122,0,\N,Missing
J94-4004,W91-0109,0,0.00859394,"of other researchers. Section 3 formally defines the terms used to classify divergences. Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); an"
J94-4004,1991.mtsummit-papers.4,0,0.0116854,"of other researchers. Section 3 formally defines the terms used to classify divergences. Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); an"
J94-4004,C92-2091,0,0.034676,"3 formally defines the terms used to classify divergences. Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); and translation mismatc"
J94-4004,C90-2007,0,0.0167292,"ake use of a small set of conceptual structures, as a starting point, and then acquire syntactic and semantic information on the basis of these initial representations plus machine-readable definitions from the Longman&apos;s Dictionary of Contemporary English (Proctor 1978). (LDOCE is useful because it includes collocations and sense frequency, thus making it possible to determine the argument structures for different words.) This investigation will benefit from the work of several researchers in the field of automatic lexicon construction, most notably, Brent (1993), Boguraev and Briscoe (1989), Boguraev and Pustejovsky (1990), Briscoe and Copestake (1990), Byrd et al. (1987), Farwell, Guthrie, and Wilks. (1992), Montemagni and Vanderwende (1992), Pustejovsky (1987), Pustejovsky and Bergler (1987), and Pustejovsky, Bergier, and Anick (1993), among others. In particular, it has been argued convincingly by Farwell, Guthrie, and Wilks (1992) that resources such as the LDOCE are useful for constructing dictionary representations for languages other than English, thus paving the way for scaling up interlingual machine translations so that they have broader coverage. Once this extension is complete, we intend to scale up"
J94-4004,J93-2002,0,0.0101756,"matic lexical acquisition procedures that make use of a small set of conceptual structures, as a starting point, and then acquire syntactic and semantic information on the basis of these initial representations plus machine-readable definitions from the Longman&apos;s Dictionary of Contemporary English (Proctor 1978). (LDOCE is useful because it includes collocations and sense frequency, thus making it possible to determine the argument structures for different words.) This investigation will benefit from the work of several researchers in the field of automatic lexicon construction, most notably, Brent (1993), Boguraev and Briscoe (1989), Boguraev and Pustejovsky (1990), Briscoe and Copestake (1990), Byrd et al. (1987), Farwell, Guthrie, and Wilks. (1992), Montemagni and Vanderwende (1992), Pustejovsky (1987), Pustejovsky and Bergler (1987), and Pustejovsky, Bergier, and Anick (1993), among others. In particular, it has been argued convincingly by Farwell, Guthrie, and Wilks (1992) that resources such as the LDOCE are useful for constructing dictionary representations for languages other than English, thus paving the way for scaling up interlingual machine translations so that they have broader co"
J94-4004,J87-3003,0,0.0104702,"point, and then acquire syntactic and semantic information on the basis of these initial representations plus machine-readable definitions from the Longman&apos;s Dictionary of Contemporary English (Proctor 1978). (LDOCE is useful because it includes collocations and sense frequency, thus making it possible to determine the argument structures for different words.) This investigation will benefit from the work of several researchers in the field of automatic lexicon construction, most notably, Brent (1993), Boguraev and Briscoe (1989), Boguraev and Pustejovsky (1990), Briscoe and Copestake (1990), Byrd et al. (1987), Farwell, Guthrie, and Wilks. (1992), Montemagni and Vanderwende (1992), Pustejovsky (1987), Pustejovsky and Bergler (1987), and Pustejovsky, Bergier, and Anick (1993), among others. In particular, it has been argued convincingly by Farwell, Guthrie, and Wilks (1992) that resources such as the LDOCE are useful for constructing dictionary representations for languages other than English, thus paving the way for scaling up interlingual machine translations so that they have broader coverage. Once this extension is complete, we intend to scale up the UNITRAN system and test the LCS approach to l"
J94-4004,P84-1065,0,0.0146489,"Missing"
J94-4004,J90-2002,0,0.543394,"he terms used to classify divergences. Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); and translation mismatches, in which the i"
J94-4004,P90-1017,1,0.909102,"he terms used to classify divergences. Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); and translation mismatches, in which the i"
J94-4004,P92-1033,1,0.662208,"e d g e / Although other translation approaches have attempted to account for divergences, the main innovation of the current approach is that it provides a formalization of these divergences and the techniques by which they are resolved. This is advantageous from a computational point of view in that it facilitates the design and implementation of • Department of Computer Science, University of Maryland, A. V. Williams Building, College Park, MD 20742, USA. 1 The reader is referred to Dorr (1993a) for a discussion of how syntactic divergences are handled. Aspectual divergences are treated by Dorr (1992a). The relatio.nof the current framework to other types of knowledge outside of lexical semantics is discussed by Dorr and Voss (1993b). © 1994 Association for Computational Linguistics Computational Linguistics (1) Volume 20, Number 4 Thematic divergence: E: I like Mary ~ S: Maria me gusta a mi &apos;Mary pleases me&apos; (2) Promotional divergence: E: John usually goes home 4=~S: Juan suele i r a casa &apos;John tends to go home&apos; (3) Demotional divergence: E: I like eating ~ G: Ich esse gem &apos;I eat likingly&apos; (4) Structural divergence: E: John entered the house 4=~S: Juan entr6 en la casa &apos;John entered in t"
J94-4004,1993.tmi-1.14,0,0.510616,"mation, idiomatic usage, aspectual knowledge, discourse knowledge, d o m a i n knowledge, or world k n o w l e d g e / Although other translation approaches have attempted to account for divergences, the main innovation of the current approach is that it provides a formalization of these divergences and the techniques by which they are resolved. This is advantageous from a computational point of view in that it facilitates the design and implementation of • Department of Computer Science, University of Maryland, A. V. Williams Building, College Park, MD 20742, USA. 1 The reader is referred to Dorr (1993a) for a discussion of how syntactic divergences are handled. Aspectual divergences are treated by Dorr (1992a). The relatio.nof the current framework to other types of knowledge outside of lexical semantics is discussed by Dorr and Voss (1993b). © 1994 Association for Computational Linguistics Computational Linguistics (1) Volume 20, Number 4 Thematic divergence: E: I like Mary ~ S: Maria me gusta a mi &apos;Mary pleases me&apos; (2) Promotional divergence: E: John usually goes home 4=~S: Juan suele i r a casa &apos;John tends to go home&apos; (3) Demotional divergence: E: I like eating ~ G: Ich esse gem &apos;I eat"
J94-4004,C92-2081,0,0.0161903,"Missing"
J94-4004,J85-4002,0,0.0400034,"Missing"
J94-4004,J85-2005,0,0.0388603,"Missing"
J94-4004,P91-1025,0,0.0139885,"assify divergences. Section 4 uses this terminology to formalize the divergence classification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); and translation mismatches, in which the information that is conveyed is"
J94-4004,E89-1037,0,0.0743849,"([Location SCHOOL]), and happily ([Manner HAPPILY]), we get the CLCS corresponding to John went happily to school (shown in Figure 3). Each (content) word in the lexicon is associated with a RLCS, whose variable positions may have certain restrictions. The CLCS is a structure that results from combining the lexical items of a source-language sentence into a single underlying pivot form by means of LCS composition. 5 The notion of unification (as used in Definition 3) differs from that of the standard unification frameworks (see, for example, Shieber et al. 1989, 1990; Kaplan and Bresnan 1982; Kaplan et al. 1989; Kay 1984; etc.) in that it is not directly invertible. That is, the generation process operates on the CLCS in a unification-like fashion that roughly mirrors the LCS composition process, but it is not a direct inverse of this process. The notion of unification used here also differs from others in that it is a more ""relaxed"" notion: those words that are mapped in a relaxed way are associated with special lexical information (i.e., the :INT, :EXT, :PROMOTE, :DEMOTE, ,, :CAT, and :CONFLATED parameters, each of which will be formalized shortly). A fundamental component of the mapping between t"
J94-4004,P84-1018,0,0.0428689,"and happily ([Manner HAPPILY]), we get the CLCS corresponding to John went happily to school (shown in Figure 3). Each (content) word in the lexicon is associated with a RLCS, whose variable positions may have certain restrictions. The CLCS is a structure that results from combining the lexical items of a source-language sentence into a single underlying pivot form by means of LCS composition. 5 The notion of unification (as used in Definition 3) differs from that of the standard unification frameworks (see, for example, Shieber et al. 1989, 1990; Kaplan and Bresnan 1982; Kaplan et al. 1989; Kay 1984; etc.) in that it is not directly invertible. That is, the generation process operates on the CLCS in a unification-like fashion that roughly mirrors the LCS composition process, but it is not a direct inverse of this process. The notion of unification used here also differs from others in that it is a more ""relaxed"" notion: those words that are mapped in a relaxed way are associated with special lexical information (i.e., the :INT, :EXT, :PROMOTE, :DEMOTE, ,, :CAT, and :CONFLATED parameters, each of which will be formalized shortly). A fundamental component of the mapping between the interli"
J94-4004,H92-1051,0,0.0315895,"Missing"
J94-4004,1988.tmi-1.5,0,0.0388576,"Missing"
J94-4004,J89-1003,0,0.0181656,"essary to find RLCSs for all three positions W, Y, and Z. Positions W and Y are filled at the leaf level and Z is filled at the TOWARDposs level. Once these positions are filled, the combination of the RLCSs for yo, dar, pu~aladas, a, and Juan covers the entire concept. Thus, the full coverage requirement is satisfied. It should be noted that a number of other systems have attempted to tackle divergences similar to those discussed in this paper without appealing to the notion of full coverage. Three examples of such systems are (1) GETA/ARIANE (Vauquois and Boitet 1985; Boitet 1987); (2) LMT (McCord 1989); and (3) METAL (Alonso 1990; Thurmair 1990). In particular, these approaches address the problem of thematic divergence by means of transfer rules of the following form, respectively: (39) like(SUBJ(ARG2:GN),OBJI(ARGI:GN)) plaire(SUBJ(ARGI:GN),OBJI(ARG2:PREP, GN)) 4~ (40) gverb (like (dat :,, nom: X), g e + f all, • :X) (41) like V ~ gustar V NP ([ROLE SUBJD ~ NP ([ROLE IOBJ]) NP ([ROLE DOBJ]) ~ NP ([ROLE SUBJ]) One problem with these approaches is that surface syntactic decisions are, in a sense, performed off-line by means of lexical entries and transfer rules that specifically encode langu"
J94-4004,C86-1022,0,0.0642358,"Missing"
J94-4004,C92-2083,0,0.0105477,"n on the basis of these initial representations plus machine-readable definitions from the Longman&apos;s Dictionary of Contemporary English (Proctor 1978). (LDOCE is useful because it includes collocations and sense frequency, thus making it possible to determine the argument structures for different words.) This investigation will benefit from the work of several researchers in the field of automatic lexicon construction, most notably, Brent (1993), Boguraev and Briscoe (1989), Boguraev and Pustejovsky (1990), Briscoe and Copestake (1990), Byrd et al. (1987), Farwell, Guthrie, and Wilks. (1992), Montemagni and Vanderwende (1992), Pustejovsky (1987), Pustejovsky and Bergler (1987), and Pustejovsky, Bergier, and Anick (1993), among others. In particular, it has been argued convincingly by Farwell, Guthrie, and Wilks (1992) that resources such as the LDOCE are useful for constructing dictionary representations for languages other than English, thus paving the way for scaling up interlingual machine translations so that they have broader coverage. Once this extension is complete, we intend to scale up the UNITRAN system and test the LCS approach to lexical choice on a broader set of phenomena using a larger lexicon. The"
J94-4004,C88-2100,0,0.0218343,"whether it should be allowed to restate the source-language phrases more verbosely in the target language. This comes up in cases such as the stab example given earlier. It turns out that the word stab can be translated into Spanish as the succinct form apu~alar, or as the more verbose form dar pu~aladas. Similarly, in the reverse direction, the translation of dar pu~aladas is the more succinct form stab; however, one could conceive of a more verbose translation (e.g., inflict knife wounds or even give knife wounds). Currently, there is no preference assignment during lexical selection (as in Nirenburg and Nirenburg 1988; Wilks 1973); instead, the system requires an exact match of the CLCS to the target-language RLCS (or some combination of RLCSs). If there is more than one way of matching the CLCS, multiple forms will be generated (although we have discussed only one target-language form, dar pu~aladas, for the stab example). A firstpass approach to resolving such cases of overgeneration (based on aspectual features) is discussed in Dorr (1992a) and in more detail in Dorr (1993b). In addition, a model 32 The complexityof the lexicalselectionprocess is a well-studiedproblem. See, for example,the work by Reite"
J94-4004,P87-1024,0,0.0222357,"resentations plus machine-readable definitions from the Longman&apos;s Dictionary of Contemporary English (Proctor 1978). (LDOCE is useful because it includes collocations and sense frequency, thus making it possible to determine the argument structures for different words.) This investigation will benefit from the work of several researchers in the field of automatic lexicon construction, most notably, Brent (1993), Boguraev and Briscoe (1989), Boguraev and Pustejovsky (1990), Briscoe and Copestake (1990), Byrd et al. (1987), Farwell, Guthrie, and Wilks. (1992), Montemagni and Vanderwende (1992), Pustejovsky (1987), Pustejovsky and Bergler (1987), and Pustejovsky, Bergier, and Anick (1993), among others. In particular, it has been argued convincingly by Farwell, Guthrie, and Wilks (1992) that resources such as the LDOCE are useful for constructing dictionary representations for languages other than English, thus paving the way for scaling up interlingual machine translations so that they have broader coverage. Once this extension is complete, we intend to scale up the UNITRAN system and test the LCS approach to lexical choice on a broader set of phenomena using a larger lexicon. The current framework pr"
J94-4004,J93-2005,0,0.00981375,"Missing"
J94-4004,E91-1051,0,0.0154659,"DE TENSE INF SUBJ The translation mapping is performed by a transfer equation that relates the source- and target-language f-structures: (44) (T T PRED &apos;JUST((T ARG))&apos;) = VENIR (T T XCOMP) = T (T ARG) This equation identifies venir as the corresponding French predicate, and it maps the argument of just to a complement that is headed by the prepositional complementizer de. Although such a case is handled in the LFG-MT system, there are a number of problems with this approach. A serious flaw concerns the handling of divergences in the context of embedded clauses. (For additional discussion, see Sadler and Thompson 1991.) In particular, if the English sentence in (42) were realized as an embedded complement such as I think that the baby just fell, it would not be possible to generate the French output. The reason for this is that the LFG-MT system is not designed to handle an interaction between a (divergent) matrix clause and a (nondivergent) embedded clause. This sentence is broken down into predicate-argument relations that conform (roughly) to the following logical specification: (45) think(I,fall(baby)) just(fall(baby)) Because the logical constituent fall(baby) is viewed as an argument of two logical h"
J94-4004,P89-1002,0,0.0430481,"Missing"
J94-4004,J90-1004,0,0.020364,"Missing"
J94-4004,1992.tmi-1.2,0,0.0132392,"IMMINGLY])] The solution to this example relies on the assumption that the distinction between the English and French exists by virtue of the fact that the word swim includes the manner component swimmingly, whereas the word traverser does not. Because of this conflational distinction, the manner component is suppressed in English, but is overtly realized (as ~ la nage) in French. 2s The important point is that this entire concept is fully 27 A related, but more general, strategy would be to handle such cases in bilingual lexical entries (see, for example, Beaven 1992a, 1992b; Whitelock 1992; Trujillo 1992). 28 The use of the word traverser (i.e., cross) instead of nager (i.e., swim) is independently determined by the fact that the path component ACROSS is present in the conceptual representation (i.e., there would be no way to realize the path component in conjunction with the word nager). 624 Bonnie J. Dorr Machine Translation Divergences covered (in the sense of [34]) by both the source- and target-language sentences, even though these two sentences do not have the same structural representation. That is, as long as all conceptual components of (49) are somehow retrievable, the suppression/re"
J94-4004,E91-1048,0,0.013036,"sification and to define the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); and translation mismatches, in which the information that is conveyed is different in the source and target languages (as described by Kameyama et al. [1991])"
J94-4004,J85-1003,0,0.0529807,"on is associated with a • marker; thus, it is necessary to find RLCSs for all three positions W, Y, and Z. Positions W and Y are filled at the leaf level and Z is filled at the TOWARDposs level. Once these positions are filled, the combination of the RLCSs for yo, dar, pu~aladas, a, and Juan covers the entire concept. Thus, the full coverage requirement is satisfied. It should be noted that a number of other systems have attempted to tackle divergences similar to those discussed in this paper without appealing to the notion of full coverage. Three examples of such systems are (1) GETA/ARIANE (Vauquois and Boitet 1985; Boitet 1987); (2) LMT (McCord 1989); and (3) METAL (Alonso 1990; Thurmair 1990). In particular, these approaches address the problem of thematic divergence by means of transfer rules of the following form, respectively: (39) like(SUBJ(ARG2:GN),OBJI(ARGI:GN)) plaire(SUBJ(ARGI:GN),OBJI(ARG2:PREP, GN)) 4~ (40) gverb (like (dat :,, nom: X), g e + f all, • :X) (41) like V ~ gustar V NP ([ROLE SUBJD ~ NP ([ROLE IOBJ]) NP ([ROLE DOBJ]) ~ NP ([ROLE SUBJ]) One problem with these approaches is that surface syntactic decisions are, in a sense, performed off-line by means of lexical entries and transfer"
J94-4004,C92-2117,0,0.0515244,"the solution to the divergence problem in the context of detailed examples. Finally, Section 5 discusses certain issues of relevance to the divergence problem including the resolution of several (recursively) interacting divergence types. 2. Classification of Machine Translation Divergences The divergence problem in machine translation has received increasingly greater attention in recent literature (see, for example, Barnett et al. 1991a, 1991b; Beaven 1992a, 1992b; Dorr 1990a, 1990b; Kameyama et al. 1991; Kinoshita, Phillips, and Tsujii 1992; Lindop and Tsujii 1991; Tsujii and Fujita 1991; Whitelock 1992; related discussion can also be found in work by Melby [1986] and Nirenburg and Nirenburg [1988]). In particular, Barnett et al. (1991a) divide distinctions between the source language and the target language into two categories: translation divergences, in which the same information is conveyed in the source and target texts, but the structures of the sentences are different (as in previous work by Dorr [1990a, 1990b]); and translation mismatches, in which the information that is conveyed is different in the source and target languages (as described by Kameyama et al. [1991]).3 Although tran"
J94-4004,C90-3001,0,\N,Missing
J94-4004,C92-2102,0,\N,Missing
J95-2005,J94-4004,1,0.852938,"quently apply filters in order to eliminate those structures that violate GB principles. (See, for example, Abney 1989; Correa 1991; Dorr 1993; Fong 1991.) The current approach provides an alternative to filter-based designs that avoids these difficulties by applying principles to descriptions of structures without actually building the structures themselves. Our approach is similar to that of Lin (1993) in that structure-building is deferred until the descriptions satisfy all principles; however, the current approach differs in that it provides a parameterization mechanism along the lines of Dorr (1994) that allows the system to be ported to languages other than English. We focus particularly on the problem of processing head-final languages such as Korean. We are currently incorporating the parser into a machine translation (MT) system called PRINCITRAN. l In general, parsers of existing principle-based interlingual MT systems are exceedingly inefficient, since they tend to adopt the filter-based paradigm. We combine the benefits of the message-passing paradigm with the benefits of the parameterized approach to build a more efficient, but easily extensible system, that will ultimately be us"
J95-2005,P93-1016,1,0.922789,"-based parsing approaches is that they generally adopt a filter-based paradigm. These approaches typically generate all possible candidate structures of the sentence that satisfy X theory, and then subsequently apply filters in order to eliminate those structures that violate GB principles. (See, for example, Abney 1989; Correa 1991; Dorr 1993; Fong 1991.) The current approach provides an alternative to filter-based designs that avoids these difficulties by applying principles to descriptions of structures without actually building the structures themselves. Our approach is similar to that of Lin (1993) in that structure-building is deferred until the descriptions satisfy all principles; however, the current approach differs in that it provides a parameterization mechanism along the lines of Dorr (1994) that allows the system to be ported to languages other than English. We focus particularly on the problem of processing head-final languages such as Korean. We are currently incorporating the parser into a machine translation (MT) system called PRINCITRAN. l In general, parsers of existing principle-based interlingual MT systems are exceedingly inefficient, since they tend to adopt the filter"
J95-2005,J92-1004,0,0.0191807,"(1993) for more details. 3.1 X Theory The central idea behind X theory is that a phrasal constituent has a layered structure. Every phrasal constituent is considered to have a head (X° = X), which determines the 2 For the purpose of readability, we have omitted integer id's in the graphical representation of the grammar network. Linear ordering is indicated by the starting points of links. For example, C precedes IP in the English network of Figure 1. 3 The idea of constraint application through feature passing among nodes is analogous to techniques applied in the TINA spoken language system (Seneff 1992) except that, in our design, the grammar network is a static data structure; it is not dynamically modified during the parsing process. Thus, we achieve a reduction space requirements. Moreover, our design achieves a reduction in time requirements because we do not retrieve a structure until the resulting parse descriptions satisfy all the network constraints. 257 Computational Linguistics Volume 21, Number 2 properties of the phrase containing it. A phrase potentially contains a complement, resulting in a one-bar level (X = Xbar) projection; it may also contain a specifier (or modifier), resu"
J95-2005,W89-0208,0,\N,Missing
J95-4008,1994.amta-1.13,0,0.0414658,"Rosetta are integrated, which programming language is used, what the extent of user interaction is, etc. 3 The authors simply state that such design decisions are &quot;a matter of taste rather than of principle&quot; (page 417). There is also no information about execution times for the numerous complex phenomena presented in the first 19 chapters. The omission of these details makes it difficult to evaluate the system, especially since so many of Rosetta&apos;s characteristics (e.g., the isomorphism requirement) depart radically from those of fully running and well-tested systems (e.g., the LOGOS system (Gdaniec 1994)). Although the developers describe an experiment where the approach is evaluated using a back-translation technique, it is not clear that this tech3 See H u t c h i n s a n d Somers (1992, p a g e s 279-296) for a m o r e detailed d i s c u s s i o n of this point. 584 Computational Linguistics Volume 21, Number 4 Invariably, interlingua designers (myself included) are forced to select a canonical form that parallels one language or the other in the case of head-swapping phenomena. 4 In the example above, the representation that is chosen corresponds logically to the canonical form &apos;by-chance"
J95-4008,E89-1037,0,0.0265234,"Missing"
J95-4008,E93-1024,0,0.028178,"o Translation Problems. This part, which arrives a bit late on the scene, presents what could be considered the main contribution of the Rosetta project. A wide range of translation problems, and their corresponding solutions, are discussed in great detail. Chapters 12 and 13 present an approach to the divergence problem that is contrasted with that of a number of MT researchers, in particular, work on categorial, promotional, and demotional divergences (Dorr 1993), structural mismatches (Estival, Ballim, Russell, and Warwick 1990), category changes and head switching (Lindop and Tsujii 1991; Kaplan and Wedekind 1993), embedding differences (Sadler and Thompson 1991; Kaplan, Netter, Wedekind, and Zaenen 1989), and complex lexical mismatches (Thurmair 1990). Chapters 14-16 present an approach to handling three additional classes of translation phenomena: temporal expressions, idioms and complex predicates, and scope and negation. These are given a much more comprehensive treatment than that of any other published work I have encountered in the MT field. A number of issues raised in part IV of the book deserve more detailed discussion; see section 2 below. Part V (chapters 17-20): Formal Aspects and Implemen"
J95-4008,E91-1051,0,0.0186188,"bit late on the scene, presents what could be considered the main contribution of the Rosetta project. A wide range of translation problems, and their corresponding solutions, are discussed in great detail. Chapters 12 and 13 present an approach to the divergence problem that is contrasted with that of a number of MT researchers, in particular, work on categorial, promotional, and demotional divergences (Dorr 1993), structural mismatches (Estival, Ballim, Russell, and Warwick 1990), category changes and head switching (Lindop and Tsujii 1991; Kaplan and Wedekind 1993), embedding differences (Sadler and Thompson 1991; Kaplan, Netter, Wedekind, and Zaenen 1989), and complex lexical mismatches (Thurmair 1990). Chapters 14-16 present an approach to handling three additional classes of translation phenomena: temporal expressions, idioms and complex predicates, and scope and negation. These are given a much more comprehensive treatment than that of any other published work I have encountered in the MT field. A number of issues raised in part IV of the book deserve more detailed discussion; see section 2 below. Part V (chapters 17-20): Formal Aspects and Implementation. The first two chapters in this part, whil"
N03-1013,dorr-etal-2000-building,1,0.923464,"Missing"
N03-1013,dorr-etal-2002-duster,1,0.810415,"standard, we asked 8 native speakers to evaluate 400 randomly-selected clusters. Each annotator was given a set of 100 clusters (with two annotators per set). Figure 3 shows a segment of the evaluation interface which was web-browseable. HeadGen with no CatVar filter: 0.1687 This quantitative distinction correlates with humanperceived differences, e.g., between the two headlines Washingtonians fight over drugs and In the nation’s capital (generated for the same story—with and without CatVar, respectively). 4.3 DUSTer DUSTer—Divergence Unraveling for Statistical Translation—was introduced in (Dorr et al., 2002). In this system, common divergence types are systematically identified and English sentences are transformed to bear a closer resemblance to that of another language using a mapping referred to as -to- . The objective is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. The CatVar database has been incorporated into two components of the DUSTer system: (1) In the -tomapping, e.g., the transformation from kick to LightVB kick (corresponding to the English/Spanish divergence"
N03-1013,P01-1032,1,0.877681,"Missing"
N03-1013,habash-dorr-2002-handling,1,0.845734,"to bear a closer resemblance to that of another language using a mapping referred to as -to- . The objective is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. The CatVar database has been incorporated into two components of the DUSTer system: (1) In the -tomapping, e.g., the transformation from kick to LightVB kick (corresponding to the English/Spanish divergence    6  The other conditions on conflatability and some detailed examples are discussed in (Habash, 2002) and (Habash and Dorr, 2002). 7 For details about the Bleu evaluation metric, see (Papineni et al., 2002). Figure 3: Evaluation The annotators were given detailed instructions and many examples to help them with the task. They were asked to classify each word in every cluster as belonging to one of the following categories: Perfect: This word definitely belongs in this cluster. Perfect (except for part of speech problem). Perfect (except for spelling problem). Not Sure: It is not clear whether a word that is derivationally correct belongs in a set or not. Doesn’t Belong: This word doesn’t belong in this cluster. May not"
N03-1013,W02-2125,1,0.902187,"o cluster. The database was inspired by pairs such as cross and across  which are used in Generation-Heavy MT. But since verb-preposition clusters are not typically morphologically related, they are 3 An English Verb-Noun list extracted from LDOCE was provided by Rebecca Green. 4 For example, in a headline generation system (HeadGen), higher Bleu scores were obtained when using the portions of the CatVar database that are most relevant to nominalized events (e.g., NOMLEX). 4.1 Generation-Heavy Machine Translation The Generation-Heavy Hybrid Machine Translation (GHMT) model was introduced in (Habash, 2002) to handle translation divergences between language pairs with asymmetrical (poor-source/rich-target) resources. The approach does not rely on a transfer lexicon or a common interlingual representation to map between divergent structural configurations from source to target language. Instead, different alternative structural configurations are over-generated and these are statistically ranked using a language model. 5 This supplementary database includes 242 clusters for more than 230 verbs and 29 prepositions. Other examples of verb-preposition clusters include: avoid and away from ; enter an"
N03-1013,C92-3145,0,0.0279957,"Missing"
N03-1013,P98-1116,0,0.0139513,"Missing"
N03-1013,P96-1004,0,0.0378019,"Missing"
N03-1013,J93-2004,0,0.0231914,"s in the search algorithm, might even appear in some of the top ranked choices. Given these issues, our goal is to build a database of categorial variations that can be used with both expansionist and reductionist approaches without the cost of over/under-stemming/generation. The research reported herein is relevant to MT, IR, and lexicon construction. 3 Building the CatVar The CatVar database was developed using a combination of resources and algorithms including the Lexical Conceptual Structure (LCS) Verb and Preposition Databases (Dorr, 2001), the Brown Corpus section of the Penn Treebank (Marcus et al., 1993), an English morphological analysis lexicon developed for PC-Kimmo (Englex) (Antworth, 1990), NOMLEX (Macleod et al., 1998), Longman Dictionary of Contemporary English 2 For a deeper discussion and classification of Porter stemmer’s errors, see (Krovetz, 1993). (LDOCE)3 (Procter, 1983), WordNet 1.6 (Fellbaum, 1998), and the Porter stemmer. The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the resource for different applications.4 Some of these resources were used to extract seed links between different word"
N03-1013,P02-1040,0,0.0778672,"Missing"
N03-1013,C00-1007,0,\N,Missing
N03-1013,C98-1112,0,\N,Missing
N03-2026,H01-1033,1,\N,Missing
N04-4040,J95-4004,0,0.232512,"Missing"
N04-4040,N01-1016,0,0.135179,"of less frequently disfluent discourse markers by taking speaker style into account. 1 Introduction Disfluencies in human speech are widespread and cause problems for both downstream processing and human readability of speech transcripts. Recent human studies (Jones et al., 2003) have examined the effect of disfluencies on the readability of speech transcripts. These results suggest that the “cleaning” of text by removing disfluent words can increase the speed at which readers can process text. Recent work on detecting edits for use in parsing of speech transcripts (Core and Schubert, 1999), (Charniak and Johnson, 2001) has shown an improvement in the parser error rate by modeling disfluencies. Many researchers investigating disfluency detection have focused on the use of prosodic cues, as opposed to lexical features (Nakatani and Hirschberg, 1994). There are different approaches to detecting disfluencies. In one approach, one can first try to locate evidence of a general disfluency, e.g., using prosodic features or language model discontinuations. These locations are called interruption points (IPs). Following this, it is generally sufficient to look in the nearby vicinity of the IP to find the dis1 This wo"
N04-4040,J99-4003,0,0.0446307,"wartz@bbn.com fluent words. The most successful approaches so far combine the detection of IPs using prosodic features and language modeling techniques (Liu et al., 2003), (Shriberg et al., 2001), (Stolcke et al., 1998). Our work is based on the premise that the vast majority of disfluencies can be detected using primarily lexical features—specifically the words themselves and part-of-speech (POS) labels—without the use of extensive prosodic cues. Lexical modeling of disfluencies with only minimal acoustic cues has been shown to be successful in the past using strongly statistical techniques (Heeman and Allen, 1999). We shall discuss our algorithm and compare it to two other algorithms that make extensive use of acoustic features. Our algorithm performs comparably on most of the tasks assigned and in some cases outperforms systems that used both prosodic and lexical features. We discuss the task definition in Section 2. In Section 3 we describe our Transformation-Based Learning (TBL) algorithm and its associated features. Section 4 presents results for our system and two other systems that make heavy use of prosodic features to detect disfluencies. We then discuss the errors made by our system, in Sectio"
N04-4040,P83-1019,0,0.175893,"Missing"
N04-4040,W96-0213,0,0.0152166,"are our System A to two other systems that were designed for the same task, System B and System C. System C was only applied to conversational speech, so there are no results for it on broadcast news transcripts. Our system was also given the same speech recognition output as System C for the conversational speech condition, whereas System B used transcripts produced by a different speech recognition system. The rules learned by the system are conditioned on several features of each of the words including the lexeme (the word itself), a POS tag for the word, whether the 2 We use a POS tagger (Ratnaparkhi, 1996) trained on switchboard data with the additional tags of FP (filled pause) and FRAG (word fragment). System B used both prosodic cues and lexical information to detect disfluencies. The prosodic cues were modeled by a decision tree classifier, whereas the lexical information was modeled using a 4-gram language model, separately trained for both CTS and BNEWS. System C first inserts IPs into the text using a decisiontree classifier based on both prosodic and lexical features and then uses TBL. In addition to POS, System C’s feature set also includes whether the word is commonly used as a filler"
N06-1013,H05-1009,1,0.890987,"and Metrics The alignment combination techniques are evaluated in this paper using data from three language pairs, as shown in Table 2. Lang Pair en-ch en-ar en-ro # of Sent’s 491 450 248 # Words (en/fl) 13K/13K 11K/13K 5.5K/5.5K Source NIST MTEval ’022 NIST MTEval ’033 HLT Workshop ’034 Table 2: Data Used for Combination Experiments. Input alignments are generated using two existing word alignment systems: GIZA++ (Och, 2000) 1 In Table 1, NC corresponds to the set of (i, j)’s neighbors that exist in the alignment Ak , and FT represents the set of words that ei (or fj ) is aligned to. 2 From (Ayan et al., 2005). 3 From (Ittycheriah and Roukos, 2005). 4 From (Mihalcea and Pedersen, 2003). 98 and SAHMM (Lopez and Resnik, 2005). Both systems are run in two different directions with default configurations. We indicate the two directions using the notation Aligner(en → f l) and Aligner(f l → en), where en is English, f l is either Chinese (ch), Arabic (ar), or Romanian (ro). To train both systems, additional data was used for the three language pairs: 107K English-Chinese sentence pairs (4.1M/3.3M English/Chinese words); 44K English-Arabic sentence pairs (1.4M/1M English/Arabic words); 48K English-Romani"
N06-1013,J96-1002,0,0.135017,"using a phrase-based MT system. 1 Introduction Word alignment—detection of corresponding words between two sentences that are translations of each other—is usually an intermediate step of statistical machine translation (MT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 2002; Ittycheriah and Roukos, 2005; Liu et al., 2005), but a sentence-level approach to combining knowledge sources is used rather than a word-level approach. This paper describes an approach to combining evidence from alignments generated by existing systems to obtain an alignment that is closer to the true alignment than the individual alignments. The ali"
N06-1013,J93-2003,0,0.0190683,"extracted from linguistic features and input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding significant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system. 1 Introduction Word alignment—detection of corresponding words between two sentences that are translations of each other—is usually an intermediate step of statistical machine translation (MT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garci"
N06-1013,W03-1007,0,0.0228579,"Missing"
N06-1013,C02-1032,0,0.0971474,"1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 2002; Ittycheriah and Roukos, 2005; Liu et al., 2005), but a sentence-level approach to combining knowledge sources is used rather than a word-level approach. This paper describes an approach to combining evidence from alignments generated by existing systems to obtain an alignment that is closer to the true alignment than the individual alignments. The alignment-combination approach (called ACME) operates at the level of alignment links, rather than at the sentence level (as in previous ME approaches). ACME uses ME to decide whether to include/exclude a particular alignment link based on feature"
N06-1013,H05-1012,0,0.65917,"oehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 2002; Ittycheriah and Roukos, 2005; Liu et al., 2005), but a sentence-level approach to combining knowledge sources is used rather than a word-level approach. This paper describes an approach to combining evidence from alignments generated by existing systems to obtain an alignment that is closer to the true alignment than the individual alignments. The alignment-combination approach (called ACME) operates at the level of alignment links, rather than at the sentence level (as in previous ME approaches). ACME uses ME to decide whether to include/exclude a particular alignment link based on feature functions that are extracted f"
N06-1013,N03-1017,0,0.142646,"input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding significant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system. 1 Introduction Word alignment—detection of corresponding words between two sentences that are translations of each other—is usually an intermediate step of statistical machine translation (MT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 2002; Ittycheriah and Ro"
N06-1013,koen-2004-pharaoh,0,0.0578265,"Missing"
N06-1013,P05-1057,0,0.242774,"as been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 2002; Ittycheriah and Roukos, 2005; Liu et al., 2005), but a sentence-level approach to combining knowledge sources is used rather than a word-level approach. This paper describes an approach to combining evidence from alignments generated by existing systems to obtain an alignment that is closer to the true alignment than the individual alignments. The alignment-combination approach (called ACME) operates at the level of alignment links, rather than at the sentence level (as in previous ME approaches). ACME uses ME to decide whether to include/exclude a particular alignment link based on feature functions that are extracted from the input align"
N06-1013,W05-0812,0,0.0365179,"rs, as shown in Table 2. Lang Pair en-ch en-ar en-ro # of Sent’s 491 450 248 # Words (en/fl) 13K/13K 11K/13K 5.5K/5.5K Source NIST MTEval ’022 NIST MTEval ’033 HLT Workshop ’034 Table 2: Data Used for Combination Experiments. Input alignments are generated using two existing word alignment systems: GIZA++ (Och, 2000) 1 In Table 1, NC corresponds to the set of (i, j)’s neighbors that exist in the alignment Ak , and FT represents the set of words that ei (or fj ) is aligned to. 2 From (Ayan et al., 2005). 3 From (Ittycheriah and Roukos, 2005). 4 From (Mihalcea and Pedersen, 2003). 98 and SAHMM (Lopez and Resnik, 2005). Both systems are run in two different directions with default configurations. We indicate the two directions using the notation Aligner(en → f l) and Aligner(f l → en), where en is English, f l is either Chinese (ch), Arabic (ar), or Romanian (ro). To train both systems, additional data was used for the three language pairs: 107K English-Chinese sentence pairs (4.1M/3.3M English/Chinese words); 44K English-Arabic sentence pairs (1.4M/1M English/Arabic words); 48K English-Romanian sentence pairs (1M/1M English/Romanian words).5 POS tags were generated using the MXPOST tagger (Ratnaparkhi, 199"
N06-1013,W03-0301,0,0.0588494,"Missing"
N06-1013,H05-1011,0,0.125122,"glinear combination of the HMM and IBM Model 4 that produces better alignments than either of those. The major advantage of these two methods is that they do not require manually annotated data. The alignment process can be modeled as a product of a transition model and an observation model, where ME models the observations (Ittycheriah and Roukos, 2005). Significant improvements are reported using this approach but the need for large manually aligned data is a bottleneck. An alternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). Moore (2005) and Taskar et al. (2005) represent alignments with several feature functions that are then combined in a weighted sum to model word alignments. Once a confidence score is assigned to all links, a non-trivial search is invoked to find the best alignment using the scores associated with the links. The major difference between these approaches and that of ACME is that we use the ME model to predict the correct class for each alignment link independently using outputs of existing alignment systems, instead of generating them from scratch at the level of the whole sentence, thus eliminating the ne"
N06-1013,P02-1038,0,0.0616772,"two sentences that are translations of each other—is usually an intermediate step of statistical machine translation (MT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 2002; Ittycheriah and Roukos, 2005; Liu et al., 2005), but a sentence-level approach to combining knowledge sources is used rather than a word-level approach. This paper describes an approach to combining evidence from alignments generated by existing systems to obtain an alignment that is closer to the true alignment than the individual alignments. The alignment-combination approach (called ACME) operates at the level of alignment links, rather than at th"
N06-1013,J03-1002,0,0.0586196,"istic features and input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding significant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system. 1 Introduction Word alignment—detection of corresponding words between two sentences that are translations of each other—is usually an intermediate step of statistical machine translation (MT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al., 1996), parsing, POS tagging and PP attachment (Ratnaparkhi, 1998), machine translation (Och and Ney, 2002), and FrameNet classification (Fleischman et al., 2003). They have also been used to solve the word alignment problem (Garcia-Varea et al., 200"
N06-1013,P03-1021,0,0.0202452,"nese (Arabic). ACME (with English POS partitioning) combines alignments using model parameters learned from the corresponding manually aligned data. MT output is evaluated using the standard MT evaluation metric BLEU (Papineni et al., 2002).10 Table 8 presents the BLEU scores on 10 We used the NIST script (version 11a) with its default setFigure 2: Precision and Recall Scores for GIZA++ and ACME Using 2 and 4 Input Alignments. MTEval’03 data for 5 different Pharaoh runs, one for each alignment. The parameters of the MT system were optimized on MTEval’02 data using minimum error rate training (Och, 2003). For the language model, the SRI Language Modeling Toolkit was used to train a trigram model with modified Kneser-Ney smoothing on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. During decoding, the number of English phrases per FL phrase was limited to 100 and the distortion of phrases was limited by 4. Based on the observations in (Koehn et al., 2003), we also limited the phrase length to 3 for computational reasons. Alignment GIZA++(union) GIZA++(gdf) GIZA++(int) ACME[2] ACME[4] Chinese 22.66 23.79 23.97 25.20 25.59 Arabic 41.72 43.82 42.76 44.9"
N06-1013,P02-1040,0,0.0732747,"ication, we examined the improvement in an off-the-shelf phrase-based MT system Pharaoh (Koehn, 2004) on both Chinese and Arabic data. In these experiments, all components of the MT system were kept the same except for the component that generates a phrase table from a given alignment. The input alignments were generated using GIZA++ and SAHMM on 107K (44K) sentence pairs for Chinese (Arabic). ACME (with English POS partitioning) combines alignments using model parameters learned from the corresponding manually aligned data. MT output is evaluated using the standard MT evaluation metric BLEU (Papineni et al., 2002).10 Table 8 presents the BLEU scores on 10 We used the NIST script (version 11a) with its default setFigure 2: Precision and Recall Scores for GIZA++ and ACME Using 2 and 4 Input Alignments. MTEval’03 data for 5 different Pharaoh runs, one for each alignment. The parameters of the MT system were optimized on MTEval’02 data using minimum error rate training (Och, 2003). For the language model, the SRI Language Modeling Toolkit was used to train a trigram model with modified Kneser-Ney smoothing on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. During"
N06-1013,H05-1010,0,0.0383808,"on of the HMM and IBM Model 4 that produces better alignments than either of those. The major advantage of these two methods is that they do not require manually annotated data. The alignment process can be modeled as a product of a transition model and an observation model, where ME models the observations (Ittycheriah and Roukos, 2005). Significant improvements are reported using this approach but the need for large manually aligned data is a bottleneck. An alternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). Moore (2005) and Taskar et al. (2005) represent alignments with several feature functions that are then combined in a weighted sum to model word alignments. Once a confidence score is assigned to all links, a non-trivial search is invoked to find the best alignment using the scores associated with the links. The major difference between these approaches and that of ACME is that we use the ME model to predict the correct class for each alignment link independently using outputs of existing alignment systems, instead of generating them from scratch at the level of the whole sentence, thus eliminating the need for an exhaustive sear"
N06-1013,P05-1069,0,0.0115673,"non-trivial search is invoked to find the best alignment using the scores associated with the links. The major difference between these approaches and that of ACME is that we use the ME model to predict the correct class for each alignment link independently using outputs of existing alignment systems, instead of generating them from scratch at the level of the whole sentence, thus eliminating the need for an exhaustive search over all possible alignments, i.e., previous approaches work globally while ACME is a localized model. A discussion of these two contrasting approaches can be found in (Tillmann and Zhang, 2005). A recent attempt to combine outputs of different alignments views the combination problem as a classifier ensemble in the neural network framework (Ayan et al., 2005). However, this method is subject to the unpredictability of random network initialization, whereas ACME is guaranteed to find the model that maximizes the likelihood of training data. 7 Conclusions We presented a new approach, ACME, to combining the outputs of different word alignment systems by reducing the combination problem to the level of alignment links and using a maximum entropy model to learn whether a particular align"
N06-1013,zhang-etal-2004-interpreting,0,0.0345682,"Missing"
N07-1029,P05-1033,0,0.137728,"through this network. The alignment of speech recognition outputs is fairly straightforward due to the strict constraint in word order. However, machine translation outputs do not have this constraint as the word order may be different between the source and target languages. MT systems employ various re-ordering (distortion) models to take this into account. In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Three MT system combination methods are presented in this paper. They operate on the sentence, phrase and word level. The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sen"
N07-1029,A94-1016,0,0.891074,". The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation s"
N07-1029,P05-3026,0,0.607609,"ion Systems  Antti-Veikko I. Rosti and Necip Fazil Ayan and Bing Xiang and Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr BBN Technologies, 10 Moulton Street, Cambridge, MA 02138   arosti,bxiang,smatsouk,schwartz @bbn.com   Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742  nfa,bonnie @umiacs.umd.edu  Abstract Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting in"
N07-1029,koen-2004-pharaoh,0,0.0363196,"imilarity score weights. The parameters  through L interpolate between the sum, average and maximum of the similarity scores. These interpolation weights and the system weights  are constrained to sum to one. The number of tunable combination weights, in ad  + dition to normal decoder weights, is  where  is the number of systems and is the + number of similarity levels; i.e.,  free system similarity score weights and two free inweights, terpolation weights.       4.2 Phrase-Based Decoding The phrasal decoder used in the phrase-level combination is based on standard beam search (Koehn, 2004). The decoder features are: a trigram language model score, number of target phrases, number of target words, phrase distortion, phrase distortion computed over the original translations and phrase translation confidences estimated in Section 4.1. The total score for a hypothesis is computed as a log-linear combination of these features. The feature weights and combination weights (system and similarity) may be tuned using Powell’s method on -best lists as described in Section 2. The phrase-level combination tuning can be summarized as follows:  1. Estimate sentence posteriors given the total"
N07-1029,E06-1005,0,0.801984,"I. Rosti and Necip Fazil Ayan and Bing Xiang and Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr BBN Technologies, 10 Moulton Street, Cambridge, MA 02138   arosti,bxiang,smatsouk,schwartz @bbn.com   Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742  nfa,bonnie @umiacs.umd.edu  Abstract Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best li"
N07-1029,J03-1002,0,0.00618995,"mber of features; i.e., $  system score scaling factors (  ), three free interpolation weights (Equation 4) for the scaling factor estimation,   GLM weights (  ), three free interpolation weights (Equation 4) for the hypothesis confidence estimation and two free LM re-scoring weights (Equation 6). All parameters may be tuned using Powell’s method on -best lists as described in Section 2. The tuning of the sentence-level combination method may be summarized as follows:     individual systems. If the alignments are not available, they can be automatically generated; e.g., using GIZA++ (Och and Ney, 2003). The phrase translation table is generated for each source sentence using confidence scores derived from sentence posteriors with system-specific total score scaling factors and similarity scores based on the agreement among the phrases from all systems. 4.1 Phrase Confidence Estimation   Each phrase has an initial confidence based on the sentence posterior  estimated from an -best list in the same fashion as in Section 3.2. The confidence of the phrase table entry is increased if several systems agree on the target words. The agreement is measured by four levels of similarity:  1. Same"
N07-1029,P03-1021,0,0.0903169,"een a system output and a targeted reference which preserves the meaning and fluency of the sentence (Snover et al., 2006). The targeted reference is generated by human posteditors who make edits to a reference translation so as to minimize the TER between the reference and 229  the vectors is gradually moved toward a larger positive change in the evaluation metric. To improve the chances of finding a global optimum, the algorithm is repeated with varying initial values. The modified Powell’s method has been previously used in optimizing the weights of a standard feature-based MT decoder in (Och, 2003) where a more efficient algorithm for log-linear models was proposed. However, this is specific to log-linear models and cannot be easily extended for more complicated functions. scaling factors were estimated from the tuning data , + to limit the feature values between   . The same scaling factors have to be applied to the features obtained from the test data. The total confidence score of hypothesis  is obtained from the system confidences  as + @   8      L      BED       &quot;! + @    BED  (4) where  is the number of systems generating the hypo"
N07-1029,P02-1040,0,0.112333,"Missing"
N07-1029,2006.amta-papers.25,1,0.904798,"phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgroup"
N07-1029,P06-1121,0,\N,Missing
N09-1066,kan-etal-2002-using,0,0.273112,"citing article. Elkiss et al. (2008b) conducted several experiments on a set of 2, 497 articles from the free PubMed Central (PMC) repository.1 Results from this experiment confirmed that the cohesion of a citation text of an article is consistently higher than the that of its abstract. They also concluded that citation texts contain additional information are more focused than abstracts. Nakov et al. (2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www"
N09-1066,P03-1054,0,0.00253884,"rization system by generating multiple alternative sentence compressions of the most important sentences in target documents (Zajic et al., 2007). Trimmer compressions are generated by applying linguistically-motivated rules to mask syntactic components of a parse of a source sentence. The rules can be applied iteratively to compress sentences below a configurable length threshold, or can be applied in all combinations to generate the full space of compressions. Trimmer can leverage the output of any constituency parser that uses the Penn Treebank conventions. At present, the Stanford Parser (Klein and Manning, 2003) is used. The set of compressions is ranked according to a set of features that may include metadata about the source sentences, details of the compression process that generated the compression, and externally calculated features of the compression. Summaries are constructed from the highest scoring compressions, using the metadata and maximal marginal relevance (Carbonell and Goldstein, 1998) to avoid redundancy and over-representation of a single source. 587 LexRank We also used LexRank (Erkan and Radev, 2004), a state-of-the-art multidocument summarization system, to generate summaries. Le"
N09-1066,W04-1013,0,0.0666638,"mentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). 4.2 4 The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)). Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR. Trimmer is a syntac"
N09-1066,P08-1093,0,0.371637,"ribing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. 3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation 586 texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the quality of surveys generated from citation summaries— both automatically and manually produced—to surveys generated from other forms of input such as the abstracts or full texts of the source articles. Nanba and Okumura (1999) discuss citation categorization to support a system for writing a survey. Nanba et al. (2004a) automatically categorize citation sentences into three groups using pre-defined phrase-based rules."
N09-1066,N04-1019,0,0.386146,"zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). 4.2 4 The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)). Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR. Trimmer"
N09-1066,C08-1087,1,0.846023,"as, 584 some of which may be unfamiliar to panelists. Thus, they must learn about a new discipline “on the fly” in order to relate their own expertise to the proposal. Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and relations among citations; and (2) summarization techniques that exploit the content of the material in both the citing and cited papers. It is generally agreed upon that manually written abstracts are good summaries of individual papers. More recently, Qazvinian and Radev (2008) argue that citation texts are useful in creating a summary of the important contributions of a research paper. The citation text of a target paper is the set of sentences in other technical papers that explicitly refer to it (Elkiss et al., 2008a). However, Teufel (2005) argues that using citation text directly is not suitable for document summarization. In this paper, we compare and contrast the usefulness of abstracts and of citation text in automatically generating a technical survey on a given topic from multiple research papers. The next section provides the background for this work, inc"
N09-1066,N07-1040,0,0.503915,"e focused than abstracts. Nakov et al. (2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the"
N09-1066,J02-4002,0,0.390372,"ons of the target paper. Our goal is to test the hypothesis that an effective technical survey will reflect information on research not only from the perspective of its authors but also from the perspective of others who use/commend/discredit/add to it. Before describing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. 3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation 586 texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the quality of surveys generated from citation summaries— both automatically and manually produced—to surveys generated from other forms of input such as the abstra"
N09-1066,W06-1613,0,0.814172,"eering and Computer Scienceφ School of Information§ , University of Michigan. {hassanam,mpradeep,vahed,radev}@umich.edu Abstract The number of research publications in various disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material. In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization techniques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role. 1 Introduction In today’s rapidly expanding disciplines, scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their field. In addition, the increasingly interconnected nature of real-world tasks often requires experts in one discipline to rapidly learn about other areas in a short amount of time. Cross-disciplinary research requires scientists in areas such as linguistics, biology, and"
N10-1041,N06-1049,1,0.83045,"a pre-arranged period of time shortly thereafter, each assessor was given five minutes to interact with the participant’s system, live over the web. After this interaction period, participants submitted a final run, which had presumably gained the benefit of user interaction. By comparing initial and final runs, it was possible to quantify the effect of the interaction. The target corpus was AQUAINT-2, which consists of around 970k documents totaling 2.5 GB. System responses consisted of multi-line answers and were evaluated using the “nugget” methodology with the “nugget pyramid” extension (Lin and Demner-Fushman, 2006). 3 Experiment Design This section describes our experiments for the TREC 2007 ciQA task. In summary: the initial run was generated automatically using standard MMR. The web-based interactions consisted of iterations of interactive MMR, where the user selected the best candidate extract at each step. The final run consisted of the output of interactive MMR padded with automatically-generated output. Sentence extracts were used as the basic response unit. For each topic, the top 100 documents were retrieved from the AQUAINT-2 collection with Lucene, using the topic template verbatim as the quer"
N10-1041,N07-1027,1,0.902114,"Missing"
olsen-etal-1998-enhancing,dorr-katsova-1998-lexical,1,\N,Missing
olsen-etal-1998-enhancing,A97-1021,1,\N,Missing
olsen-etal-1998-enhancing,1997.mtsummit-workshop.3,1,\N,Missing
P01-1032,P00-1059,0,\N,Missing
P01-1032,1997.mtsummit-workshop.13,1,\N,Missing
P01-1032,P98-1081,0,\N,Missing
P01-1032,C98-1078,0,\N,Missing
P01-1032,P97-1020,1,\N,Missing
P01-1032,1997.mtsummit-workshop.3,1,\N,Missing
P01-1032,J96-2004,0,\N,Missing
P01-1032,A00-2026,0,\N,Missing
P04-1048,W04-0909,1,0.912344,"e challenges, we have developed SemFrame, a system that induces semantic frames automatically. Overall, the system performs two primary functions: (1) identification of sets of verb senses that evoke a common semantic frame (in the sense that lexical units call forth corresponding conceptual structures); and (2) identification of the conceptual structure of semantic frames. This paper explores the first task of identifying frame semantic verb classes. These classes have several types of uses. First, they are the basis for identifying the internal structure of the frame proper, as set forth in Green and Dorr, 2004. Second, they may be used to extend FrameNet. Third, they support applications needing access to sets of semantically related words, for example, text segmentation and word sense disambiguation, as explored to a limited degree in Green, 2004. Section 2 presents related research efforts on developing semantic verb classes. Section 3 summarizes the features of WordNet (http://www.cogsci.princeton.edu/~wn) and LDOCE (Procter, 1978) that support the automatic induction of semantic verb classes, while Section 4 sets forth the approach taken by SemFrame to accomplish this task. Section 5 presents a"
P04-1048,N03-1013,1,0.843617,"Missing"
P04-1048,W03-1604,0,0.0942637,"Missing"
P04-1048,W03-1601,0,\N,Missing
P04-1048,2003.mtsummit-systems.9,1,\N,Missing
P04-1048,W04-0804,0,\N,Missing
P06-1002,H05-1009,1,0.920862,"odels, such as hidden Markov models (HMM) (Vogel et al., 1996), log-linear models (Och and Ney, 2003), and similarity-based heuristic methods (Melamed, 2000). These methods are unsupervised, i.e., the only input is large parallel corpora. In recent years, researchers have shown that even using a limited amount of manually aligned data improves word alignment significantly (Callison-Burch et al., 2004). Supervised learning techniques, such as perceptron learning, maximum entropy modeling or maximum weighted bipartite matching, have been shown to provide further improvements on word alignments (Ayan et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005). The standard technique for evaluating word alignments is to represent alignments as a set of links (i.e., pairs of words) and to compare the generated alignment against manual alignment of the same data at the level of links. Manual alignments are represented by two sets: Probable (P ) alignments and Sure (S) alignments, where S ⊆ P . Given A, P and S, the most commonly used metrics—precision (Pr), recall (Rc) and alignment error rate (AER)—are defined as follows: Lang Pair en-ch en-ar # of Sent’s 491 450 Test # Words (en/fl) 1"
P06-1002,W05-0909,0,0.0530694,"formance. In recent years, researchers have proposed several algorithms to generate word alignments. However, evaluating word alignments is difficult because even humans have difficulty performing this task. The state-of-the art evaluation metric— alignment error rate (AER)—attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000). Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researcher"
P06-1002,J93-2003,0,0.0188057,"TEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al., 1996), log-linear models (Och and Ney, 2003), and similarity-based heuristic methods (Melamed, 2000). These methods are unsupervised, i.e., the only input is large parallel corpora. In recent years, researchers have shown that even using a limited amount of manually aligned data improves word alignment significantly (Callison-Burch et al., 2004). Supervised learning techniques, such as perceptron learning, maximum entropy modeling or maximum weigh"
P06-1002,P04-1023,0,0.0550565,". However, evaluating word alignments is difficult because even humans have difficulty performing this task. The state-of-the art evaluation metric— alignment error rate (AER)—attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000). Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden"
P06-1002,P05-1033,0,0.0259522,"A which attempts to increase recall without a significant sacrifice in precision. Manually aligned data from two language pairs are used in our intrinsic evaluations using the five combinations above. A summary of the training and test data is presented in Table 1. Our gold standard for each language pair is a manually aligned corpus. English-Chinese an|A ∩ P | |A ∩ S| Rc = |A| |S| |A ∩ S |+ |A ∩ P | AER = 1 − |A |+ |S| Pr = Another approach to evaluating alignments is to measure their impact on an external application, e.g., statistical MT. In recent years, phrase-based systems (Koehn, 2004; Chiang, 2005) have been shown to outperform word-based MT systems; therefore, in this paper, we use a publicly-available phrase-based MT system, Pharaoh (Koehn, 2004), to investigate the impact of different alignments. Although it is possible to estimate phrases directly from a training corpus (Marcu and Wong, 2002), most phrase-based MT systems (Koehn, 2004; Chiang, 2005) start with a word alignment and extract phrases that are consistent with the given alignment. Once the consistent phrases are extracted, they are assigned multiple scores (such 10 notations distinguish between sure and probable alignment"
P06-1002,P04-1064,0,0.0649608,"ignments is difficult because even humans have difficulty performing this task. The state-of-the art evaluation metric— alignment error rate (AER)—attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000). Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM)"
P06-1002,H05-1012,0,0.0627176,"because even humans have difficulty performing this task. The state-of-the art evaluation metric— alignment error rate (AER)—attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000). Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al., 1996), log-linea"
P06-1002,N03-1017,0,0.117624,"es for SU and SG , but improves the scores for the other 3 alignments significantly. Finally, increasing the maximum phrase length to 7 leads to additional improvements in BLEU scores, where SG and SU benefit nearly 2 BLEU points. As in English-Chinese, the worst BLEU scores are obtained by SU while the best scores are produced by SB . As we see from the tables, the relation between intrinsic alignment measures (AER and CPER) Different maximum phrase length, Different sizes of training data, and Different lexical weighting. For maximum phrase length, we used 3 (based on what was suggested by (Koehn et al., 2003) and 7 (the default maximum phrase length in Pharaoh). For lexical weighting, we used the original weighting scheme employed in Pharaoh and a modified version. We realized that the publiclyavailable implementation of Pharaoh computes the lexical weights only for non-NULL alignment links. As a consequence, loose phrases containing NULL-aligned words along their edges receive the same lexical weighting as tight phrases without NULL-aligned words along the edges. We therefore adopted a modified weighting scheme following (Koehn et al., 2003), which incorporates NULL alignments. MT output was eval"
P06-1002,koen-2004-pharaoh,0,0.565639,"nt characteristics on MT performance but also identify several alignment-related factors that impact MT performance regardless of the quality of the initial alignments. In so doing, we begin to answer long-standing questions about the value of alignment in the context of MT. We first evaluate 5 different word alignments intrinsically, using: (1) community-standard metrics—precision, recall and AER; and (2) a new measure called consistent phrase error rate (CPER). Next, we observe the impact of different alignments on MT performance. We present BLEU scores on a phrase-based MT system, Pharaoh (Koehn, 2004), using five different alignments to extract phrases. We investigate the impact of different settings for phrase extraction, lexical weighting, maximum phrase length and training data. Finally, we present a quantitative analysis of which phrases are chosen during the actual decoding process and show how the distribution of the phrases differ from one alignment into another. Our experiments show that precision-oriented alignments yield better phrases for MT than recalloriented alignments. Specifically, they cover a higher percentage of our test sets and result in fewer untranslated words and se"
P06-1002,W02-1018,0,0.0606674,"guage pair is a manually aligned corpus. English-Chinese an|A ∩ P | |A ∩ S| Rc = |A| |S| |A ∩ S |+ |A ∩ P | AER = 1 − |A |+ |S| Pr = Another approach to evaluating alignments is to measure their impact on an external application, e.g., statistical MT. In recent years, phrase-based systems (Koehn, 2004; Chiang, 2005) have been shown to outperform word-based MT systems; therefore, in this paper, we use a publicly-available phrase-based MT system, Pharaoh (Koehn, 2004), to investigate the impact of different alignments. Although it is possible to estimate phrases directly from a training corpus (Marcu and Wong, 2002), most phrase-based MT systems (Koehn, 2004; Chiang, 2005) start with a word alignment and extract phrases that are consistent with the given alignment. Once the consistent phrases are extracted, they are assigned multiple scores (such 10 notations distinguish between sure and probable alignment links, but English-Arabic annotations do not. The details of how the annotations are done can be found in (Ayan et al., 2005) and (Ittycheriah and Roukos, 2005). 3.1 Align. SU SG SI SA SB Phrase Lengths of 3 and 7 Table 2 presents the precision, recall, and AER for 5 different alignments on 2 language"
P06-1002,J00-2004,0,0.0261142,", the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al., 1996), log-linear models (Och and Ney, 2003), and similarity-based heuristic methods (Melamed, 2000). These methods are unsupervised, i.e., the only input is large parallel corpora. In recent years, researchers have shown that even using a limited amount of manually aligned data improves word alignment significantly (Callison-Burch et al., 2004). Supervised learning techniques, such as perceptron learning, maximum entropy modeling or maximum weighted bipartite matching, have been shown to provide further improvements on word alignments (Ayan et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005). The standard technique for evaluating word alignments is to represent ali"
P06-1002,H05-1011,0,0.143828,"Missing"
P06-1002,C00-2163,0,0.127228,"ed alignments yield better MT output (translating more words and using longer phrases) than recalloriented alignments. 1 Introduction Word alignments are a by-product of statistical machine translation (MT) and play a crucial role in MT performance. In recent years, researchers have proposed several algorithms to generate word alignments. However, evaluating word alignments is difficult because even humans have difficulty performing this task. The state-of-the art evaluation metric— alignment error rate (AER)—attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000). Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Comp"
P06-1002,J03-1002,0,0.0576048,"significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM models (Brown et al., 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al., 1996), log-linear models (Och and Ney, 2003), and similarity-based heuristic methods (Melamed, 2000). These methods are unsupervised, i.e., the only input is large parallel corpora. In recent years, researchers have shown that even using a limited amount of manually aligned data improves word alignment significantly (Callison-Burch et al., 2004). Supervised learning techniques, such as perceptron learning, maximum entropy modeling or maximum weighted bipartite matching, have been shown to provide further improvements on word alignments (Ayan et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005). The standard tech"
P06-1002,P03-1021,0,0.0239078,"lementation of Pharaoh computes the lexical weights only for non-NULL alignment links. As a consequence, loose phrases containing NULL-aligned words along their edges receive the same lexical weighting as tight phrases without NULL-aligned words along the edges. We therefore adopted a modified weighting scheme following (Koehn et al., 2003), which incorporates NULL alignments. MT output was evaluated using the standard evaluation metric BLEU (Papineni et al., 2002).2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval’2002 test sets using minimum error rate training (Och, 2003), and the systems were tested on NIST MTEval’2003 test sets for both languages. 2 We used the NIST script (version 11a) for BLEU with its default settings: case-insensitive matching of n-grams up to n = 4, and the shortest reference sentence for the brevity penalty. The words that were not translated during decoding were deleted from the MT output before running the BLEU script. 3 We could not run SB on the larger corpus because of the lack of required inputs. 4 Due to lack of additional training data, we could not do experiments using different sizes of training data on EnglishArabic. 12 Alig"
P06-1002,P02-1040,0,0.112688,"and play a crucial role in MT performance. In recent years, researchers have proposed several algorithms to generate word alignments. However, evaluating word alignments is difficult because even humans have difficulty performing this task. The state-of-the art evaluation metric— alignment error rate (AER)—attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000). Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g., BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005)). However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al., 2004; Goutte et al., 2004; Ittycheriah and Roukos, 2005). Despite significant AER improvements achieved by several researchers, the improvements in BLEU scores are insignificant or, at best, small. 9 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9–16, c Sydney, July 2006. 2006 Association for Computational Linguistics selection in the same MT system. 2 Related Work Starting with the IBM mo"
P06-1002,H05-1010,0,0.0943803,", log-linear models (Och and Ney, 2003), and similarity-based heuristic methods (Melamed, 2000). These methods are unsupervised, i.e., the only input is large parallel corpora. In recent years, researchers have shown that even using a limited amount of manually aligned data improves word alignment significantly (Callison-Burch et al., 2004). Supervised learning techniques, such as perceptron learning, maximum entropy modeling or maximum weighted bipartite matching, have been shown to provide further improvements on word alignments (Ayan et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005). The standard technique for evaluating word alignments is to represent alignments as a set of links (i.e., pairs of words) and to compare the generated alignment against manual alignment of the same data at the level of links. Manual alignments are represented by two sets: Probable (P ) alignments and Sure (S) alignments, where S ⊆ P . Given A, P and S, the most commonly used metrics—precision (Pr), recall (Rc) and alignment error rate (AER)—are defined as follows: Lang Pair en-ch en-ar # of Sent’s 491 450 Test # Words (en/fl) 14K/12K 13K/11K en-ch en-ar 107K 44K Training 4.1M/3.3M FBIS 1.4M/"
P06-1002,C96-2141,0,0.935608,"Missing"
P06-1021,N01-1016,0,0.406874,"rally lack appropriate rules for analyzing these constructions. One possible response to this mismatch between grammatical resources and the brute facts of disfluent speech is to make one look more like the other, for the purpose of parsing. In this separate-processing approach, reparanda are located through a variety of acoustic, lexical or string-based techniques, then excised before submission to a parser (Stolcke and Shriberg, 1996; Heeman and Allen, 1999; Spilker et al., 2000; Johnson and Charniak, 2004). The resulting parse tree then has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in reco"
P06-1021,A00-2018,0,0.0169867,"ation × √ both × √ EDIT F none Parseval F Break index POST (Ratnaparkhi, 1996) which was itself trained on Switchboard. Finally, as described in section 2 these tags were augmented with a special prosodic break symbol if the decision tree rated the probability a ToBI ‘p’ symbol higher than the threshold value of 0.75. Annotation speech repairs. The first two use the CYK algorithm to find the most likely parse tree on a grammar read-off from example trees annotated as in Figures 2 and 4. The third experiment measures the benefit from syntactic indicators alone in Charniak’s lexicalized parser (Charniak, 2000). The tables in subsections 4.1, 4.2, and 4.3 summarize the accuracy of output parse trees on two measures. One is the standard Parseval F-measure, which tracks the precision and recall for all labeled constituents as compared to a gold-standard parse. The other measure, EDIT-finding F, restricts consideration to just constituents that are reparanda. It measures the per-word performance identifying a word as dominated by EDITED or not. As in previous studies, reference transcripts were used in all √ cases. A check ( ) indicates an experiment where prosodic breaks where automatically inferred b"
P06-1021,P99-1053,0,0.591037,"way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs continue a syntactic category that was left unfi"
P06-1021,N04-1011,0,0.0312862,"Missing"
P06-1021,H05-1030,1,0.8458,"ecision and recall trade-off on its detection can be adjusted using a threshold on the posterior probability of predicting “p”, as shown in Figure 3. In essence, the large number of acoustic and prosodic features related to disfluency are encoded via the ToBI label ‘p’, and provided as additional observations to the PCFG. This is unlike previous work on incorporating prosodic information (Gre0.6 0.5 Probability of Miss 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 Probability of False Alarm 0.4 0.5 0.6 Figure 3: DET curve for detecting disfluent breaks from acoustics. gory et al., 2004; Lease et al., 2005; Kahn et al., 2005) as described further in Section 6. 3 Syntactic parallelism The other striking property of speech repairs is their parallel character: subsequent repair regions ‘line up’ with preceding reparandum regions. This property can be harnessed to better estimate the length of the reparandum by considering parallelism from the perspective of syntax. For instance, in Figure 4(a) the unfinished reparandum noun phrase is repaired by another noun phrase – the syntactic categories are parallel. 3.1 Levelt’s WFR and Conjunction The idea that the reparandum is syntactically parallel to the repair can be trac"
P06-1021,J93-2004,0,0.0291288,"ion of Levelt’s WFR can be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements"
P06-1021,C00-2169,0,0.0526183,"Missing"
P06-1021,H94-1020,0,0.0752404,"an be applied to Treebanks by systematically recoding the annotations to indicate which phrases are unfinished and to distinguish matching from nonmatching repairs. (2) If, as schema (2) suggests, conjunction does favor like-categories, and, as Levelt suggests, wellformed repairs are conjoinable with finished versions of their reparanda, then the syntactic categories of repairs ought to match the syntactic categories of (finished versions of) reparanda. 3.2 (3) 3.3.1 Unfinished phrases Some Treebanks already mark unfinished phrases. For instance, the Penn Treebank policy (Marcus et al., 1993; Marcus et al., 1994) is to annotate the lowest node that is unfinished with an -UNF tag as in Figure 4(a). It is straightforward to propagate this mark upwards in the tree from wherever it is annotated to the nearest enclosing EDITED node, just as -BRK is propagated upwards from disjuncture marks on individual words. This percolation simulates the action of McKelvie’s [abort = true]. The resulting PCFG is one in which distributions on phrase structure rules with ‘missing’ daughters are segregated from distributions on ‘complete’ rules. A WFR for grammars Levelt’s WFR imposes two requirements on a grammar • distin"
P06-1021,J83-3003,0,0.415711,"has the reparandum re-attached in a standardized way (Charniak and Johnson, 2001). An alternative strategy, adopted in this paper, is to use the same grammar to model fluent speech, disfluent speech, and their interleaving. Such an integrated approach can use syntactic properties of the reparandum itself. For instance, in example (1) the reparandum is an unfinished noun phrase, the repair a finished noun phrase. This sort of phrasal correspondence, while not absolute, is strong in conversational speech, and cannot be exploited on the separate-processing approach. Section 3 applies metarules (Weischedel and Sondheimer, 1983; McKelvie, 1998a; Core and Schubert, 1999) in recognizing these correspondences using standard context-free grammars. At the same time as it defies parsing, conversational speech offers the possibility of leveraging prosodic cues to speech repairs. SecA grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs conti"
P06-1021,W05-1519,0,0.0261845,"Missing"
P06-1021,H91-1073,0,0.273826,"onstituents labeled EDITED. Such NP NP NP Prosodic disjuncture Everyday experience as well as acoustic analysis suggests that the syntactic interruption in speech repairs is typically accompanied by a change in prosody (Nakatani and Hirschberg, 1994; Shriberg, 1994). For instance, the spectrogram corresponding to example (2), shown in Figure 1, (2) DT NNP the jehovah NNP POS witness EDITED CC NP CC−BRK or NNPS or~+ mormons ’s Figure 2: Propagating BRK, the evidence of disfluent juncture, from acoustics to syntax. disjuncture symbols are identified in the ToBI labeling scheme as break indices (Price et al., 1991; Silverman et al., 1992). The availability of a corpus annotated with ToBI labels makes it possible to design a break index classifier via supervised training. The corpus is a subset of the Switchboard corpus, consisting of sixty-four telephone conversations manually annotated by an experienced linguist according to a simplified ToBI labeling scheme (Ostendorf et al., 2001). In ToBI, degree of disjuncture is indicated by integer values from 0 to 4, where a value of 0 corresponds to clitic and 4 to a major phrase break. In addition, a suffix p denotes perceptually disfluent events reflecting,"
P06-1021,W96-0213,0,\N,Missing
P06-1021,P83-1019,0,\N,Missing
P06-1021,J99-4003,0,\N,Missing
P06-1021,P04-1005,0,\N,Missing
P06-1119,2006.eamt-1.18,1,0.694848,"Missing"
P06-1119,J93-2003,0,0.00721652,"Missing"
P06-1119,cmejrek-etal-2004-prague,1,0.901336,"Missing"
P06-1119,W05-1010,0,0.0143561,"and word order. 5 Related Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation"
P06-1119,C96-1006,0,0.0262257,"Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward pro"
P06-1119,J03-1002,0,0.00292812,"us translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun & Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och & Ney, 2003; Wantanabe & Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical resources. We developed a method for prioritizing keyword phrases from an English thesaurus of concepts and elicited Czech translations for a subset of the keyword phrases. From these, we decomposed phrase elements for reuse in an English-Czech probabilistic dictionary. We then applied the dictionary in machine translation of the rest of the thesaurus. Our results show an overall improvement in machine tra"
P06-1119,P02-1040,0,0.0751386,"rily phrase by phrase/word by word translation. Our evaluation scores below will partially reflect the simplicity of our system. Our system is simple by design. Any improvement or degradation to the input of our system has direct influence on the output. Thus, measures of translation accuracy for our system can be directly interpreted as quality measures for the lexical resources used and the process by which they were developed. 4 Evaluation We performed two different types of evaluation to validate our process. First, we compared our system output to human reference translations using Bleu (Papineni, et al., 2002), a widelyaccepted objective metric for evaluation of machine translations. Second, we showed corrected and uncorrected machine translations to Czech speakers and collected subjective judgments of fluency and accuracy. For evaluation purposes, we selected 418 keyword phrases to be used as target translations. These phrases were selected using a stratified sampling technique so that different levels of thesaurus value would be represented. There was no overlap between these keyword phrases and the 3000 prioritized keyword phrases used to build our lexicon. Prior to machine translation we obtain"
P06-1119,P99-1067,0,0.00918922,"have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domai"
P06-1119,P03-2025,0,0.0476662,"Missing"
P06-1119,C96-2098,0,0.0202396,"knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to sp"
P90-1017,J89-1003,0,0.0793514,"Missing"
P92-1033,P90-1017,1,0.9073,"xical selection and aspectual realization processes that operate during the production of the target-language sentence: there are often a large number of lexical and aspectual possibilities to choose from in the production of a sentence from a lexical semantic representation. Aspectual information from the source-language sentence constrains the choice of target-language terms. In turn, the target-language terms limit the possibilities for generation of aspect. Thus, there is a two-way communication channel between the two processes. Figure 1 shows some of the types of parametric diver9ences (Dorr, 1990a) that can arise cross-linguistically. 257 We will focus primarily on the third type, aspectual distinctions, and show how these may be discovered through the extraction of information in a monolingual corpus. We adopt the viewpoint that the algorithms for extraction of syntactic, lexical-semantic, and aspectual information must be well-grounded in linguistic theory. Once the information is extracted, it may then be used as the basis of parameterized machine translation. Note that we reject the commonly held assumption that the use of corpora necessarily suggests that statistical or exampleba"
P92-1033,J90-2002,0,0.436537,"xical selection and aspectual realization processes that operate during the production of the target-language sentence: there are often a large number of lexical and aspectual possibilities to choose from in the production of a sentence from a lexical semantic representation. Aspectual information from the source-language sentence constrains the choice of target-language terms. In turn, the target-language terms limit the possibilities for generation of aspect. Thus, there is a two-way communication channel between the two processes. Figure 1 shows some of the types of parametric diver9ences (Dorr, 1990a) that can arise cross-linguistically. 257 We will focus primarily on the third type, aspectual distinctions, and show how these may be discovered through the extraction of information in a monolingual corpus. We adopt the viewpoint that the algorithms for extraction of syntactic, lexical-semantic, and aspectual information must be well-grounded in linguistic theory. Once the information is extracted, it may then be used as the basis of parameterized machine translation. Note that we reject the commonly held assumption that the use of corpora necessarily suggests that statistical or exampleba"
P92-1033,W91-0222,1,0.57385,"hun~er) Aepectuah (c) (3) (a) l t e r a t i v e Divergence: E: John stabbed M a r y 4. S: J u a n le dio una puflaJada a M a r l s (John g a v e a knife-wound to M a r y ) S: J u a n le dio p u f i a l a d a s a M a r l s (John gave knife-wounds to M a r y ) (b) Duratlve Divergence, E: John m e t / k n e w M a r y 4* S: J u a n c o a o c i 6 a M a r l s ( J o h n m e t M a r y ) S: J u a n c o n o c i £ a M&rfa (John knew M e r i t ) Figure 1: Three Levels of MT Divergences INTRODUCTION This paper discusses how the two-level knowledge representation model for machine translation presented by Dorr (1991) integrates aspectual information with lexical-semantic information by means of parameterization. The parameter-based approach borrows certain ideas from previous work such as the lexical-semantic model of Jackendoff (1983, 1990) and models of aspectual representation including Bach (1986), Comrie (1976), Dowty (1979), Mourelatos (1981), Passonneau (1988), Pustejovsky (1988, 1989, 1991), and Vendler (1967). However, unlike previous work, the current approach examines aspectual considerations within the context of machine translation. More recently, Bennett *This paper describes research done i"
P92-1033,W91-0214,0,0.0635437,"Missing"
P92-1033,J88-2003,0,0.736686,"aper describes research done in the Institute for Advanced Computer Studies at the University of Maryland. A special thanks goes to Terry Gaasterland and Ki Lee for helping to close the gap between properties of aspectual information and properties of lexical-semantic structure. In addition, useful guidance and commentary during this research were provided by Bruce Dawson, Michael Herweg, Jorge Lobo, Paola Merlo, Norbert Hornstein, Patrick SaintDizier, Clare Voss, and Amy Weinberg. et el. (1990) have examined aspect and verb semantics within the context of machine translation in the spirit of Moens and Steedman (1988). This paper borrows from, and extends, these ideas by demonstrating how this theoretical framework might be adapted for crosslinguistic applicability. The framework has been tested within the context of an interlingual machine translation system and is currently being used as the basis for extraction of aspectual information from corpora. The integration of aspect with lexical-semantics is especially critical in machine translation because of the lexical selection and aspectual realization processes that operate during the production of the target-language sentence: there are often a large nu"
P92-1033,J88-2005,0,0.55512,"r l s ( J o h n m e t M a r y ) S: J u a n c o n o c i £ a M&rfa (John knew M e r i t ) Figure 1: Three Levels of MT Divergences INTRODUCTION This paper discusses how the two-level knowledge representation model for machine translation presented by Dorr (1991) integrates aspectual information with lexical-semantic information by means of parameterization. The parameter-based approach borrows certain ideas from previous work such as the lexical-semantic model of Jackendoff (1983, 1990) and models of aspectual representation including Bach (1986), Comrie (1976), Dowty (1979), Mourelatos (1981), Passonneau (1988), Pustejovsky (1988, 1989, 1991), and Vendler (1967). However, unlike previous work, the current approach examines aspectual considerations within the context of machine translation. More recently, Bennett *This paper describes research done in the Institute for Advanced Computer Studies at the University of Maryland. A special thanks goes to Terry Gaasterland and Ki Lee for helping to close the gap between properties of aspectual information and properties of lexical-semantic structure. In addition, useful guidance and commentary during this research were provided by Bruce Dawson, Michael Her"
P92-1033,P85-1003,0,0.0282417,"formation, which we will describe in the next section. A s p e c t u a l S t r u c t u r e . Aspect is taken to have two components, one comprised of inherent features (i.e., those features that distinguish between states and events) and another comprised of non-inherent features (i. e., those features that define the perspective, e.g., simple, progressive, and perfective). This paper will focus primarily on inherent features, z Previous representational frameworks have omitted aspectual distinctions among verbs, and have typically merged events under the single heading of dynamic (see, e.g., Yip (1985)). However, a number of aspectually oriented lexical-semantic representations have been proposed that more readily accommodate the types of aspectual distinctions discussed here. The current work borrows extends these ideas for the development of an interlingual representation. For example, Dowty (1979) and Vendler (1967) have proposed a four-way aspectual classification system for verbs: states, activities, achievements, and accomplishments, each of which has a different degree of telicity (i.e., culminated vs. nonculmi2The empty location denoted by e corresponds to an unrealized argument of"
P97-1020,C96-1055,1,0.817966,"ment Achievement Telic ÷ + Dynamic + + + Durative Examples + + + know. have march, paint destroy notice, win Table 1: Featurai Identification of Aspectual Classes Aspectual Class Telic Dynamic State Activity Accomplishment Achievement + + + + + Durative + + + Examples know. have march, paint destroy notice, win Table 2: Privative Featural Identification of Aspectual Classes of Levin's Ran kerbs (51.3.2): 3 we assign it the template in (3)(i), with the corresponding Lisp format shown in (3)(ii): (3) (i) [z.... ACTLoc ([xhi,g * 1],[M..... BY MARCH 26])] (ii) (act loc (* thing 1) (by march 26)) (Dorr and Jones, 1996; Dorr, To appear) and assigned LCS templates from a database built as Lisplike structures (Dorr, 1997). The assignment of aspectual features to the classes in Levin was done by hand inspection of the semantic effect of the alternations described in Part I of Levin (Olsen, 1996), with automatic coindexing to the verb classes (see (Dorr and Olsen, 1996)). Although a number of Levin's verb classes were aspectually uniform, many required subdivisions by aspectual class; most of these divided atelic ""manner"" verbs from telic ""result"" verbs, a fundamental linguistic distinction (cf. (Levin and Rapp"
P97-1020,J95-2005,1,0.698514,"(1996) showed that the strength of distributionally derived selectional constraints helps predict whether verbs can participate in a class of diathesis alternations. with aspectual properties of verbs clearly influencing the alternations of interest. He also points out that these properties are difficult to obtain directly from corpora. The ability to determine lexical aspect, on a large scale and in the sentential context, therefore yields an important source of constraints for corpus analysis and psycholinguistic experimentation, as well as for NLP applications such as machine translation (Dorr et al., 1995b) and foreign language tutoring (Dorr et al., 1995a; Sams. 1995; Weinberg et al., 1995). Other researchers have proposed corpusbased approaches to acquiring lexical aspect information with varying data coverage: Klavans and Chodorow (1992) focus on the event-state distinction in verbs and predicates; Light (1996) considers the aspectual properties of verbs and affixes; and McKeown and Siegel (1996) describe an algorithm for classifying sentences according to lexical aspect. properties. Conversely. a number of works in the linguistics literature have proposed lexical semantic templates for rep"
P97-1020,A97-1021,1,0.825683,"le 1: Featurai Identification of Aspectual Classes Aspectual Class Telic Dynamic State Activity Accomplishment Achievement + + + + + Durative + + + Examples know. have march, paint destroy notice, win Table 2: Privative Featural Identification of Aspectual Classes of Levin's Ran kerbs (51.3.2): 3 we assign it the template in (3)(i), with the corresponding Lisp format shown in (3)(ii): (3) (i) [z.... ACTLoc ([xhi,g * 1],[M..... BY MARCH 26])] (ii) (act loc (* thing 1) (by march 26)) (Dorr and Jones, 1996; Dorr, To appear) and assigned LCS templates from a database built as Lisplike structures (Dorr, 1997). The assignment of aspectual features to the classes in Levin was done by hand inspection of the semantic effect of the alternations described in Part I of Levin (Olsen, 1996), with automatic coindexing to the verb classes (see (Dorr and Olsen, 1996)). Although a number of Levin's verb classes were aspectually uniform, many required subdivisions by aspectual class; most of these divided atelic ""manner"" verbs from telic ""result"" verbs, a fundamental linguistic distinction (cf. (Levin and Rappaport Hovav, To appear) and references therein). Examples are discussed below. Following Grimshaw (1993"
P97-1020,P96-1004,0,0.0278468,"ectly from corpora. The ability to determine lexical aspect, on a large scale and in the sentential context, therefore yields an important source of constraints for corpus analysis and psycholinguistic experimentation, as well as for NLP applications such as machine translation (Dorr et al., 1995b) and foreign language tutoring (Dorr et al., 1995a; Sams. 1995; Weinberg et al., 1995). Other researchers have proposed corpusbased approaches to acquiring lexical aspect information with varying data coverage: Klavans and Chodorow (1992) focus on the event-state distinction in verbs and predicates; Light (1996) considers the aspectual properties of verbs and affixes; and McKeown and Siegel (1996) describe an algorithm for classifying sentences according to lexical aspect. properties. Conversely. a number of works in the linguistics literature have proposed lexical semantic templates for representing the aspectual properties of verbs (Dowry, 1979: Hovav and Levin, 1995; Levin and Rappaport Hovav. To appear), although these have not been implemented and tested on a large scale. We show that. it is possible to represent the lexical aspect both of verbs alone and in sentential contexts using Lexical Con"
P97-1020,J88-2003,0,0.606773,"per consideration of these universal pieces of verb meaning may be used to refine lexical representations and derive a range of meanings from combinations of LCS representations. A single algorithm may therefore be used to determine lexical aspect classes and features at both verbal and sentence levels. Finally, we illustrate how knowledge of lexical aspect facilitates the interpretation of events in NLP applications. 1 Introduction Knowledge of lexical aspect--how verbs denote situations as developing or holding in time--is required for interpreting event sequences in discourse (Dowty, 1986; Moens and Steedman, 1988; Passoneau, 1988), interfacing to temporal databases (Androutsopoulos, 1996), processing temporal modifiers (Antonisse, 1994), describing allowable alternations and their semantic effects (Resnik, 1996; Tenny, 1994), and for selecting tense and lexical items for natural language generation ((Dorr and Olsen. 1996: Klavans and Chodorow, 1992), cf. (Slobin and Bocaz, 1988)). In addition, preliminary pyscholinguistic experiments (Antonisse, 1994) indicate that subjects are sensitive to the presence or absence of aspectual features when processing temporal modifiers. Resnik (1996) showed that the"
P97-1020,J88-2005,0,0.861388,"ving Verbal and Compositional Lexical A s p e c t for N L P Applications Bonnie J. Dorr and Marl Broman Olsen U n i v e r s i t y of M a r y l a n d I n s t i t u t e for Ad.vanced C o m p u t e r S t u d i e s A.V. W i l l i a m s B u i l d i n g C o l l e g e P a r k , M D 20742, U S A b o n n i e , m o l s e n © u m i a c s . umd. e d u Abstract Verbal and compositional lexical aspect provide the underlying temporal structure of events. Knowledge of lexical aspect, e.g., (a)telicity, is therefore required for interpreting event sequences in discourse (Dowty, 1986; Moens and Steedman, 1988; Passoneau, 1988), interfacing to temporal databases (Androutsopoulos, 1996), processing temporal modifiers (Antonisse, 1994), describing allowable alternations and their semantic effects (Resnik, 1996; Tenny, 1994), and selecting tense and lexical items for natural language generation ((Dorr and Olsen, 1996; Klavans and Chodorow, 1992), cf. (Slobin and Bocaz, 1988)). We show that it is possible to represent lexical aspect--both verbal and compositional--on a large scale, using Lexical Conceptual Structure (LCS) representations of verbs in the classes cataloged by Levin (1993). We show how proper consideration"
P97-1020,C92-4177,0,\N,Missing
R19-1003,D07-1090,0,0.0155611,"the study of minority languages. Lessons learned from such studies are highly informative to NLP researchers who seek to overcome analogous challenges in the computational processing of these types of languages. Assuming that large monolingual texts are available, an obvious next step is to leverage these texts to augment the NMT systems’ performance. Various approaches have been developed for this purpose. In some approaches, target monolingual texts are employed to train a Language Model (LM) that is then integrated with MT models trained from parallel texts to enhance translation quality (Brants et al., 2007; G¨ulc¸ehre et al., 2015). Although these approaches utilize monolingual text to train a LM, they do not address the shortage of bilingual training datasets. In other approaches, bilingual datasets are automatically generated from monolingual texts by utilizing the Translation Model (TM) trained on aligned bilingual text; the resulting sentence pairs are used to enlarge the initial training dataset for subsequent learning iterations (Ueffing et al., 2008; Sennrich et al., 2016). Although these approaches enlarge the bilingual training dataset, there is no quality control and, thus, the accura"
R19-1003,N16-1102,0,0.0678588,"Missing"
R19-1003,2008.iwslt-papers.6,0,0.0411331,"output labels (whereas we start with neural network based output, i.e., the translation, and artificially produce an input). We expect that this is more robust towards noise in MT. Hoang et al. (2018) showed that the quality of back translation matters and proposed an iterative back translation, in which back translated data are used to build better translation systems in forward and backward directions. These, in turn, are used to reback translate monolingual data. This process is iterated several times. Improving NMT with monolingual source data, following similar work on phrase-based SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for neural language models by (Ter-Sarkisov et al., 2015). Round-tripping has already been utilized in SMT by (Ahmadnia et al., 2019). In this work, forward and backward models produce informative feedback to iteratively update the TMs during the training of the system. 2 NMT consists of an encoder and a decoder. Following (Bahdanau et al., 2015), we adopt an attention-based encoder-decoder model, i.e., one that selectively focuses on sub-parts of the sentence during trans"
R19-1003,P16-1009,0,0.034804,"Language Model (LM) that is then integrated with MT models trained from parallel texts to enhance translation quality (Brants et al., 2007; G¨ulc¸ehre et al., 2015). Although these approaches utilize monolingual text to train a LM, they do not address the shortage of bilingual training datasets. In other approaches, bilingual datasets are automatically generated from monolingual texts by utilizing the Translation Model (TM) trained on aligned bilingual text; the resulting sentence pairs are used to enlarge the initial training dataset for subsequent learning iterations (Ueffing et al., 2008; Sennrich et al., 2016). Although these approaches enlarge the bilingual training dataset, there is no quality control and, thus, the accuracy of the generated bilingual dataset cannot be guaranteed (Ahmadnia et al., 2018). To tackle the issues described above, we apply a new round-tripping approach that incorporates dual learning (He et al., 2016) for automatic learning from unlabeled data, but transcends this prior work through effective leveraging of monolingual text. Specifically, the round-tripping approach takes advantage of the bootstrapping methods including self-training and co-training. These methods start"
R19-1003,W18-2703,0,0.019816,"data augmentation techniques, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1998). A similar avenue of research is self-training (McClosky et al., 2006). The self-training approach as a bootstrapping method typically refers to the scenario where the training dataset is enhanced with training instances with artificially produced output labels (whereas we start with neural network based output, i.e., the translation, and artificially produce an input). We expect that this is more robust towards noise in MT. Hoang et al. (2018) showed that the quality of back translation matters and proposed an iterative back translation, in which back translated data are used to build better translation systems in forward and backward directions. These, in turn, are used to reback translate monolingual data. This process is iterated several times. Improving NMT with monolingual source data, following similar work on phrase-based SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for neural language models by (Ter-Sarkisov et al., 2015). Round"
R19-1003,W15-4006,0,0.0236252,"noise in MT. Hoang et al. (2018) showed that the quality of back translation matters and proposed an iterative back translation, in which back translated data are used to build better translation systems in forward and backward directions. These, in turn, are used to reback translate monolingual data. This process is iterated several times. Improving NMT with monolingual source data, following similar work on phrase-based SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for neural language models by (Ter-Sarkisov et al., 2015). Round-tripping has already been utilized in SMT by (Ahmadnia et al., 2019). In this work, forward and backward models produce informative feedback to iteratively update the TMs during the training of the system. 2 NMT consists of an encoder and a decoder. Following (Bahdanau et al., 2015), we adopt an attention-based encoder-decoder model, i.e., one that selectively focuses on sub-parts of the sentence during translation. Consider a source sentence X = {x1 , x2 , ..., xJ } and a target sentence Y = {y1 , y2 , ..., yI }. The problem of translation from the source language to the target is sol"
R19-1003,P15-1001,0,0.0393761,"conditional probability: 3 Related Work The integration of monolingual data for NMT models was first proposed by (G¨ulc¸ehre et al., 2015), who train monolingual LMs independently, and then integrate them during decoding through rescoring of the beam (adding the recurrent hidden state of the LM to the decoder state of the encoder-decoder network). In this approach, an additional controller mechanism controls the magnitude of the LM signal. The controller parameters and output parameters are tuned on further parallel training data, but the LM parameters are fixed during the fine tuning stage. Jean et al. (2015) also report on experiments with reranking of NMT output with a 5-gram LM, Neural Machine Translation yˆ = arg max P (y|x) (1) y The conditional word probabilities given the source language sentence and preceding target language words compose the conditional probability 19 a stacking LSTM (Hochreiter and Schmidhuber, 1997). The goal is to derive a context vector ci that captures relevant source information to help predict the current target word yi . While these models differ in how the context vector ci is derived, they share the same subsequent steps. ci is calculated as follows: as follows:"
R19-1003,tiedemann-2012-parallel,0,0.0216373,"d segments by comparing them with a data set of reference translations. The scores of each segment, ranging between 0 and 100, are averaged over the entire evaluation dataset to yield an estimate of the overall translation quality (higher is better). The baseline systems for Persian-Spanish are first trained, while our round-trip method conducts joint training. We summarize the overall performances in Table 1: We apply the round-trip training approach to bilingual Persian-Spanish translation, and evaluate the results. We used the Persian-Spanish small bilingual corpora from the Tanzil corpus (Tiedemann, 2012),2 which contains about 50K parallel sentence pairs. We also used 5K and 10K parallel sentences extracted from the OpenSubtitles2018 collection (Tiedemann, 2012),3 as the validation and test datasets, respectively. Finally, we utilized 70K parallel sentences from the KDE4 corpus (Tiedemann, 2012),4 as the monolingual data. We implemented the DyNet-based model architecture (Mi et al., 2016) on top of Mantis (Cohn et al., 2016) which is an implementation of the attention sequence-to-sequence (Seq-to-Seq) NMT. For each language, we constructed a vocabulary with the most common 50K words in the pa"
R19-1003,P15-1002,0,0.0273605,"into a sequence of vectors, and the decoder generates target words one-by-one based on the conditional probability shown in the Equation (2). More specifically, the encoder takes a sequence of source words as inputs → − and returns forward hidden vectors hj (1 ≤ j ≤ J) of the forward-RNN: → − −−→ hj = f (hj−1 , x) (3) ci = (8) where hj is the annotation of source word xj and αt,j is a weight for the j th source vector at time step t to generate yi : αt,j = PJ exp (score (di , hj )) j 0 =1 exp (score (di , hj 0 )) (9) The score function above may be defined in a variety of ways as discussed by Luong et al. (2015). In this paper, we denote all the parameters to be optimized in the neural network as Θ and denote C as the dataset that contains source-target sentence pairs for the training phase. Hence, the learning objective is to seek the optimal parameters Θ∗ : These forward and backward vectors are concatenated to make source vectors hj (1 ≤ j ≤ J) based on Equation (5): h→ − ← −i hj = hj ; hj (5) Θ∗ = arg max Θ X I X log P (yt |y<t , x; Θ) (x,y)∈C (i=1) (10) 4 The decoder takes source vectors as input and returns target words. It starts with the initial hidden vector hJ (concatenated source vector at"
R19-1003,N06-1020,0,0.0188199,"presents the previous related work. In Section 3, we briefly review the relevant mathematical background of NMT paradigm. Section 4 describes the round-trip training approach. The experiments and results are presented in Section 5. Conclusions and future work are discussed in Section 6. but improvements are small. The production of synthetic parallel texts bears resemblance to data augmentation techniques, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1998). A similar avenue of research is self-training (McClosky et al., 2006). The self-training approach as a bootstrapping method typically refers to the scenario where the training dataset is enhanced with training instances with artificially produced output labels (whereas we start with neural network based output, i.e., the translation, and artificially produce an input). We expect that this is more robust towards noise in MT. Hoang et al. (2018) showed that the quality of back translation matters and proposed an iterative back translation, in which back translated data are used to build better translation systems in forward and backward directions. These, in turn"
R19-1003,D16-1249,0,0.0185068,"formances in Table 1: We apply the round-trip training approach to bilingual Persian-Spanish translation, and evaluate the results. We used the Persian-Spanish small bilingual corpora from the Tanzil corpus (Tiedemann, 2012),2 which contains about 50K parallel sentence pairs. We also used 5K and 10K parallel sentences extracted from the OpenSubtitles2018 collection (Tiedemann, 2012),3 as the validation and test datasets, respectively. Finally, we utilized 70K parallel sentences from the KDE4 corpus (Tiedemann, 2012),4 as the monolingual data. We implemented the DyNet-based model architecture (Mi et al., 2016) on top of Mantis (Cohn et al., 2016) which is an implementation of the attention sequence-to-sequence (Seq-to-Seq) NMT. For each language, we constructed a vocabulary with the most common 50K words in the parallel corpora, and OOV words were replace with a special token <UNK>. For monolingual corpora, sentences containing at least one OOV word were removed. Additionally, sentences with more than 80 words were removed from the training set.5 The encoders and decoders make use of Long Short-Term Memory (LSTM) with 500 embedding dimensions, 500 hidden dimensions. For training, we used the SGD al"
R19-1003,2001.mtsummit-papers.68,0,0.0138244,"M. To start the round-trip training approach, the systems are initialized using warm-start TMs trained from initial small bilingual data. The goal is to see whether the round-tripping augments the baseline accuracy. We retrain the baseline systems by enlarging the initial small bilingual corpus: we add the optimized generated bilingual sentences to the initial parallel text. The new enlarged translation system contains both the initial and optimized generated bilingual text. For each translation task, we train the round-trip training approach. We employ Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) (using multibleu.perl script from Moses) as the evaluation metric. BLEU is calculated for individual translated segments by comparing them with a data set of reference translations. The scores of each segment, ranging between 0 and 100, are averaged over the entire evaluation dataset to yield an estimate of the overall translation quality (higher is better). The baseline systems for Persian-Spanish are first trained, while our round-trip method conducts joint training. We summarize the overall performances in Table 1: We apply the round-trip training approach to bilingual Persian-Spanish tran"
R19-1004,ahmadnia-etal-2017-persian,1,0.855613,"(2013) proposed a similar model that uses the concept of an encoder and decoder. They used an n-gram LM for the encoder part and a combination of inverse LM and an RNN for the decoder part. The evaluation of their model was based on rescoring the K-best list of the phrases from the SMT phrase table. 3 From SMT to PBMST Our enhancement to SMT takes a noisy channel model as a starting point, where translation is modeled by decoding a source text, thereby eliminating the noise (e.g., adjusting lexical and syntactic divergences) to uncover the intended translation. However, as in our prior work (Ahmadnia et al., 2017), we adopt a more general, loglinear variant to accommodate an unlimited number of features and to provide a more general framework for controlling each feature’s influence on the overall output. Standard probabilities are scaled to their logarithmic counterparts that are then added together, rather than multiplying, following standard logarithmic rules. The log-linear model is derived via direct modelling of the posterior probability P (y1I |xJ1 ): yˆ = arg max P (y1I |xJ1 ) 4 Following Ahmadnia et al. (2018), we enhance NMT performance by estimating the conditional probability P (yiI |xJj )"
R19-1004,D13-1106,0,0.0511179,"Missing"
R19-1004,P09-5002,0,0.0111426,"ent languages exercise different syntactic ordering. For instance, adjectives in English precede the noun, while they typically follow the noun in Spanish (the cloudy sky versus el cielo nublado); in Persian the verb precedes the subject, and in Chinese the verb comes last. As a result, source language phrases cannot be translated and placed in the same order in the generated translation in the target language, but phrase movements have to be considered. This is the role of the RM. Estimating the exact distance of movement for each phrase is too sparse; therefore, instead, the lexicalized RM (Koehn, 2009) estimates phrase movements using only a few reordering types, such as a monotonous order, where the order is preserved, or a swap, when the order of two consecutive source phrases is inverted when their translations are placed in the target side. Neural Machine Translation (NMT) has been receiving significant attention due to its impressive translation performance (Bahdanau et al., 2015). NMT differs from SMT in its adoption of a large neural network to perform the entire translation process in one shot, for which an encoder-decoder architecture is widely used. In this approach, a source sent"
R19-1004,P07-2045,0,0.013184,"t was an additional annotation to score the phrase pairs of an SMT system. Chandar et al. (2014) trained a feed-forward neural networks to learn the mapping of an inLong Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are an extension of RNNs, capable of learning such longterm dependencies. RNNs are a chain of repeating modules of neural network and, in their simplest form, the repeating module has a single layer. LSTMs also have this chain-like structure, but the repeating modules have four interacting layers. This paper presents a PBSMT model based on the Moses decoder (Koehn et al., 2007) with a LM that is enriched by an external dataset. Scoring of the phrase table generated by Moses is achieved through a LSTM encoder-decoder, and the result is then evaluated in an English-to-Spanish translation task. Specifically, the model is trained to learn the translation probabilities between English phrases and their corresponding Spanish ones. The trained model is then used as a part of a classical PBSMT system, with each phrase pair scored in the phrase table. Our evaluation proves that this approach enhances the translation performance. Although Moses itself is able to score phrases"
R19-1004,N03-1017,0,0.0578622,"observe its positive impact on translation quality. We construct a PBSMT model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set. 1 Introduction The three most essential components of a Statistical Machine Translation (SMT) system are: (1) Translation Model (TM); (2) Language Model (LM); and (3) Reordering Model (RM). Among these models, the RM plays an important role in Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; Koehn et al., 2003), and it still remains a major focus of intense study (Kanouchi et al., 2016; Du and Way, 2017; Chen et al., 2019). 25 Proceedings of Recent Advances in Natural Language Processing, pages 25–32, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_004 Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) are a class of artificial neural network that has recently resurfaced in the field of MT (Schwenk, 2012). Unlike feed-forward networks, RNNs leverage recurrent connections that enable the network to refer to prior states and, thus, to process arbitrary sequences of in"
R19-1004,P19-1174,0,0.027282,"the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set. 1 Introduction The three most essential components of a Statistical Machine Translation (SMT) system are: (1) Translation Model (TM); (2) Language Model (LM); and (3) Reordering Model (RM). Among these models, the RM plays an important role in Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; Koehn et al., 2003), and it still remains a major focus of intense study (Kanouchi et al., 2016; Du and Way, 2017; Chen et al., 2019). 25 Proceedings of Recent Advances in Natural Language Processing, pages 25–32, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_004 Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) are a class of artificial neural network that has recently resurfaced in the field of MT (Schwenk, 2012). Unlike feed-forward networks, RNNs leverage recurrent connections that enable the network to refer to prior states and, thus, to process arbitrary sequences of input. The cornerstone of RNNs is their ability to connect previous information to the present task. For example, gi"
R19-1004,W02-1018,0,0.137511,"the proposed LSTM and observe its positive impact on translation quality. We construct a PBSMT model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set. 1 Introduction The three most essential components of a Statistical Machine Translation (SMT) system are: (1) Translation Model (TM); (2) Language Model (LM); and (3) Reordering Model (RM). Among these models, the RM plays an important role in Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; Koehn et al., 2003), and it still remains a major focus of intense study (Kanouchi et al., 2016; Du and Way, 2017; Chen et al., 2019). 25 Proceedings of Recent Advances in Natural Language Processing, pages 25–32, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_004 Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) are a class of artificial neural network that has recently resurfaced in the field of MT (Schwenk, 2012). Unlike feed-forward networks, RNNs leverage recurrent connections that enable the network to refer to prior states and, thus, to process arbi"
R19-1004,D14-1179,0,0.196562,"Missing"
R19-1004,P03-1021,0,0.0598964,"Missing"
R19-1004,P14-1129,0,0.059085,"Missing"
R19-1004,J03-1002,0,0.00905339,"ent search algorithm finds the highest probability translation among the exponential number of candidates. Training the Moses decoder yields a phrase model as well as a TM which, together, support translation between source and target languages. Moses scores each phrase in the phrase table with respect to a given source sentence and 4 5 The LM is built with the target language (in our case-study, Spanish is the target language) to ensure fluent and well-formed output. KenLM (Heafield, 2011), which comes bundled with the Moses toolkit, is used for building our LM. Also to train the TM, GIZA++ (Och and Ney, 2003) is used for word alignment. Finally, the phrases are extracted and scored as well. The generated phrase table is later used to translate test sentences that are compared to the results of the LSTM encoderdecoder. The following steps are applied to build the NMT system with the LSTM Encoder-Decoder: • Vocabulary building: Generate vocabulary corpus for both source and target sides. • Corpus shuffle: Shuffle the vocabulary corpus of both source and target languages. • Dictionary building: Create dictionary by leveraging an alignment file to replace the &lt;UNK> words. The files mentioned above are"
R19-1004,W17-4123,0,0.0168652,"ecoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set. 1 Introduction The three most essential components of a Statistical Machine Translation (SMT) system are: (1) Translation Model (TM); (2) Language Model (LM); and (3) Reordering Model (RM). Among these models, the RM plays an important role in Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; Koehn et al., 2003), and it still remains a major focus of intense study (Kanouchi et al., 2016; Du and Way, 2017; Chen et al., 2019). 25 Proceedings of Recent Advances in Natural Language Processing, pages 25–32, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_004 Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) are a class of artificial neural network that has recently resurfaced in the field of MT (Schwenk, 2012). Unlike feed-forward networks, RNNs leverage recurrent connections that enable the network to refer to prior states and, thus, to process arbitrary sequences of input. The cornerstone of RNNs is their ability to connect previous information to the present t"
R19-1004,2001.mtsummit-papers.68,0,0.389228,"ese target phrases to yield the target sentence yˆ. This model is considered superior in comparison to the noisy-channel model because of the ability to adjust the importance of individual features, thus controlling each feature’s influence on the overall output. In the PBSMT model, the TM is factored into the translation probabilities of matching phrases in the source and target sentences (Ahmadnia and Serrano, 2015). These are considered additional features in the log-linear model and are weighted accordingly to maximize the performance as measured by Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001). The neural LM Bengio et al. (2003) has become a community standard for SMT system development, i.e., neural networks have been used to rescore translation hypotheses (k-best lists). Recently, however, there has been an emerging interest in training neural networks to score the translated phrase pairs using a source-sentence representation as an additional input (Zou et al., 2013). We adopt this approach for our own PBSMT enrichment, as further detailed below. put phrase to its corresponding output phrase using a bag-of-words approach. This is closely related to the model proposed by Schwenk"
R19-1004,W11-2123,0,0.0528883,"bles automatic training of TMs for any language pair using a large parallel corpus. Once the model is trained, an efficient search algorithm finds the highest probability translation among the exponential number of candidates. Training the Moses decoder yields a phrase model as well as a TM which, together, support translation between source and target languages. Moses scores each phrase in the phrase table with respect to a given source sentence and 4 5 The LM is built with the target language (in our case-study, Spanish is the target language) to ensure fluent and well-formed output. KenLM (Heafield, 2011), which comes bundled with the Moses toolkit, is used for building our LM. Also to train the TM, GIZA++ (Och and Ney, 2003) is used for word alignment. Finally, the phrases are extracted and scored as well. The generated phrase table is later used to translate test sentences that are compared to the results of the LSTM encoderdecoder. The following steps are applied to build the NMT system with the LSTM Encoder-Decoder: • Vocabulary building: Generate vocabulary corpus for both source and target sides. • Corpus shuffle: Shuffle the vocabulary corpus of both source and target languages. • Dicti"
R19-1004,C12-2104,0,0.348161,"(2) Language Model (LM); and (3) Reordering Model (RM). Among these models, the RM plays an important role in Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; Koehn et al., 2003), and it still remains a major focus of intense study (Kanouchi et al., 2016; Du and Way, 2017; Chen et al., 2019). 25 Proceedings of Recent Advances in Natural Language Processing, pages 25–32, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_004 Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) are a class of artificial neural network that has recently resurfaced in the field of MT (Schwenk, 2012). Unlike feed-forward networks, RNNs leverage recurrent connections that enable the network to refer to prior states and, thus, to process arbitrary sequences of input. The cornerstone of RNNs is their ability to connect previous information to the present task. For example, given a LM that predicts the next word based on previous words, no further context is needed to predict the last word in the clouds are in the sky. LSTM-based encoder-decoder, thus yielding improvements in quality. Sentences with the highest scores are selected as the translation output. The rest of this paper is organized"
R19-1004,D13-1176,0,0.0416653,"T enrichment, as further detailed below. put phrase to its corresponding output phrase using a bag-of-words approach. This is closely related to the model proposed by Schwenk (2012), except that their input representation of a phrase was a Bag-Of-Words (BOW). A similar encoderdecoder approach that used two RNNs was proposed by Socher et al. (2011), but their model was restricted to a monolingual setting. More recently, an encoder-decoder model using an RNN was proposed by Auli et al. (2013), where the decoder was conditioned on a representation of either a source sentence or a source context. Kalchbrenner and Blunsom (2013) proposed a similar model that uses the concept of an encoder and decoder. They used an n-gram LM for the encoder part and a combination of inverse LM and an RNN for the decoder part. The evaluation of their model was based on rescoring the K-best list of the phrases from the SMT phrase table. 3 From SMT to PBMST Our enhancement to SMT takes a noisy channel model as a starting point, where translation is modeled by decoding a source text, thereby eliminating the noise (e.g., adjusting lexical and syntactic divergences) to uncover the intended translation. However, as in our prior work (Ahmadni"
R19-1004,W16-4607,0,0.0164871,"model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set. 1 Introduction The three most essential components of a Statistical Machine Translation (SMT) system are: (1) Translation Model (TM); (2) Language Model (LM); and (3) Reordering Model (RM). Among these models, the RM plays an important role in Phrase-Based SMT (PBSMT) (Marcu and Wong, 2002; Koehn et al., 2003), and it still remains a major focus of intense study (Kanouchi et al., 2016; Du and Way, 2017; Chen et al., 2019). 25 Proceedings of Recent Advances in Natural Language Processing, pages 25–32, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_004 Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986) are a class of artificial neural network that has recently resurfaced in the field of MT (Schwenk, 2012). Unlike feed-forward networks, RNNs leverage recurrent connections that enable the network to refer to prior states and, thus, to process arbitrary sequences of input. The cornerstone of RNNs is their ability to connect previous informatio"
R19-1004,D14-1003,0,0.015227,"nd I speak fluent Spanish. The word Spain suggests that the last word is probably the name of a language, but to narrow down that language, access to a larger context is needed. It is entirely possible for the gap between the relevant information and the point where it is needed to become indefinitely large. Unfortunately, as that gap grows, RNNs are increasingly unable to learn to connect the information. 2 Related Work Recently, various neural network models have been applied to MT. However, few approaches have made effective use of neural networks to enhance the translation quality of SMT. Sundermeyer et al. (2014) designed a neural TM that uses LSTM-based RNNs and Bidirectional RNNs, wherein the target word is conditioned not only on the history but also on the future source context. The result was a fully formed source sentence for predicting target words. Feed-forward neural LMs, first proposed by Bengio et al. (2003), were a breakthrough in language modeling. Mikolov et al. (2011) proposed the use of recurrent neural network in language modeling, thus enabling a much longer context history for predicting the next word. Experimental results showed that the RNN-based LM significantly outperforms the s"
R19-1004,D17-1149,0,0.0193084,"he decoder uses that representation to generate the corresponding target translation. NMT’s word-by-word translation generation strategy makes it difficult to translate phrases. This is a significant MT challenge as the meaning of a phrase is not always deducible from the meanings of its individual words or parts. Unfortunately, current NMT systems are word-based or character-based where phrases are not considered as translation units. By contrast, phrases are more effective than words as translation units in SMT. Indeed, leveraging phrases has had a significant impact on translation quality (Wang et al., 2017). Phrases play a key role in Machine Translation (MT). In this paper, we apply a Long Short-Term Memory (LSTM) model over conventional Phrase-Based Statistical MT (PBSMT). The core idea is to use an LSTM encoder-decoder to score the phrase table generated by the PBSMT decoder. Given a source sequence, the encoder and decoder are jointly trained in order to maximize the conditional probability of a target sequence. Analytically, the performance of a PBSMT system is enhanced by using the conditional probabilities of phrase pairs computed by an LSTM encoder-decoder as an additional feature in the"
R19-1004,D13-1141,0,0.263225,"ificantly outperforms the standard feed-forward LM. Schwenk (2012) proposed a feed-forward neural network to score phrase pairs. They employed a feed-forward neural network with fixedsize phrasal inputs consisting of seven words, and with zero padding for shorter phrases. The system also had fixed-size phrasal output consisting of seven words. Similarly, Devlin et al. (2014) utilized a feed-forward neural network to generate translations, but they simultaneously predicted one word in a target phrase. The use of feedforward neural networks demands the use of fixedsize phrases to work properly. Zou et al. (2013) also proposed bilingual learning of word and phrase embeddings, which were used to compute the distance between phrase pairs. The result was an additional annotation to score the phrase pairs of an SMT system. Chandar et al. (2014) trained a feed-forward neural networks to learn the mapping of an inLong Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are an extension of RNNs, capable of learning such longterm dependencies. RNNs are a chain of repeating modules of neural network and, in their simplest form, the repeating module has a single layer. LSTMs also have this chai"
rambow-etal-2006-parallel,W04-2709,1,\N,Missing
rambow-etal-2006-parallel,passonneau-etal-2006-inter,1,\N,Missing
rambow-etal-2006-parallel,J93-2004,0,\N,Missing
rambow-etal-2006-parallel,W00-0204,0,\N,Missing
rambow-etal-2006-parallel,W02-1503,0,\N,Missing
rambow-etal-2006-parallel,rambow-etal-2002-dependency,1,\N,Missing
reeder-etal-2004-interlingual,W04-2709,1,\N,Missing
reeder-etal-2004-interlingual,W03-1601,0,\N,Missing
reeder-etal-2004-interlingual,W03-1604,0,\N,Missing
reeder-etal-2004-interlingual,P98-1013,0,\N,Missing
reeder-etal-2004-interlingual,C98-1013,0,\N,Missing
reeder-etal-2004-interlingual,1991.mtsummit-papers.9,1,\N,Missing
reeder-etal-2004-interlingual,J96-2004,0,\N,Missing
reeder-etal-2004-interlingual,A97-1011,0,\N,Missing
roark-etal-2006-sparseval,A00-2018,1,\N,Missing
roark-etal-2006-sparseval,J93-2004,0,\N,Missing
roark-etal-2006-sparseval,P97-1003,0,\N,Missing
roark-etal-2006-sparseval,N01-1016,1,\N,Missing
roark-etal-2006-sparseval,N04-4032,1,\N,Missing
roark-etal-2006-sparseval,J01-2004,1,\N,Missing
W03-0501,P00-1041,0,0.642724,"structured templates (Paice and Jones, 1993), sentence compression (Hori et al., 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). We focus instead on the construction of headline-style abstracts from a single story. Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization. This approach has been explored in (Zajic et al., 2002) and (Banko et al., 2000). The approach we use in Hedge is most similar to that of (Knight and Marcu, 2001), where a single sentence is shortened using statistical compression. As in this work, we select headline words from story words in the order that they appear in the story—in particular, the first sentence of the story. However, we use linguistically motivated heuristics for shortening the sentence; there is no statistical model, which means we do not require any prior training on a large corpus of story/headline pairs. Linguistically motivated heuristics have been used by (McKeown et al, 2002) to distinguish con"
W03-0501,P97-1003,0,0.00838243,"eadlines, which are often intended only to catch the eye, our approach produces informative abstracts describing the main theme or event of the newspaper article. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task Richard Schwartz BBN schwartz@bbn.com of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated h"
W03-0501,W97-0710,0,0.0392103,"lier work on headline generation, a probabilistic model for automatic headline generation (Zajic et al, 2002). In this paper we will refer to this statistical system as HMM Hedge We demonstrate the effectiveness of our linguistically-motivated approach, Hedge Trimmer, over the probabilistic model, HMM Hedge, using both human evaluation and automatic metrics. 2 Previous Work Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al., 1995; Mann et al., 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al., 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). We focus instead on the construction of headline-style abstracts from a single story. Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization. This approach has been explor"
W03-0501,A00-2030,0,0.00711002,"rticle. We claim that the construction of informative abstracts requires access to deeper linguistic knowledge, in order to make substantial improvements over purely statistical approaches. In this paper, we present our technique for producing headlines using a parse-and-trim approach based on the BBN Parser. As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997). The BBN parser has been used successfully for the task Richard Schwartz BBN schwartz@bbn.com of information extraction in the SIFT system (Miller et al., 2000). The next section presents previous work in the area of automatic generation of abstracts. Following this, we present feasibility tests used to establish the validity of an approach that constructs headlines from words in a story, taken in order and focusing on the earlier part of the story. Next, we describe the application of the parse-and-trim approach to the problem of headline generation. We discuss the linguistically-motivated heuristics we use to produce results that are headlinelike. Finally, we evaluate Hedge Trimmer by comparing it to our earlier work on headline generation, a proba"
W03-0501,J98-3005,0,\N,Missing
W03-0501,X98-1014,1,\N,Missing
W03-0501,P02-1040,0,\N,Missing
W04-0909,W03-1601,0,\N,Missing
W04-0909,W03-1604,0,\N,Missing
W04-0909,H01-1046,0,\N,Missing
W04-0909,J02-3001,0,\N,Missing
W04-0909,P04-1048,1,\N,Missing
W04-0909,P03-1068,0,\N,Missing
W04-0909,C02-1070,0,\N,Missing
W04-2709,P98-1013,0,0.473703,"dependency parser (details in section 6) and is a useful starting point for semantic annotation at IL1, since it allows annotators to see how textual units relate syntactically when making semantic judgments. 4.1.3 IL2 IL2 is intended to be an interlingua, a representation of meaning that is reasonably independent of language. IL2 is intended to capture similarities in meaning across languages and across different lexical/syntactic realizations within a language. For example, IL2 is expected to normalize over conversives (e.g. X bought a book from Y vs. Y sold a book to X) (as does FrameNet (Baker et al 1998)) and non-literal language usage (e.g. X started its business vs. X opened its doors to customers). The exact definition of IL2 will be the major research contribution of this project. 4.2 The Omega Ontology In progressing from IL0 to IL1, annotators have to select semantic terms (concepts) to represent the nouns, verbs, adjectives, and adverbs present in each sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998)"
W04-2709,J96-2004,0,0.0756983,"Missing"
W04-2709,2003.mtsummit-eval.3,1,0.726095,"al content, modality, speech acts, etc. At the same time, while incorporating these items, vagueness and redundancy must be eliminated from the annotation language. Many inter-event relations would need to be captured such as entity reference, time reference, place reference, causal relationships, associative relationships, etc. Finally, to incorporate these, crosssentence phenomena remain a challenge. From an MT perspective, issues include evaluating the consistency in the use of an annotation language given that any source text can result in multiple, different, legitimate translations (see Farwell and Helmreich, 2003) for discussion of evaluation in this light. Along these lines, there is the problem of annotating texts for translation without including in the annotations inferences from the source text. 9 Conclusions This is a radically different annotation project from those that have focused on morphology, syntax or even certain types of semantic content (e.g., for word sense disambiguation competitions). It is most similar to PropBank (Kingsbury et al 2002) and FrameNet (Baker et al 1998). However, it is novel in its emphasis on: (1) a more abstract level of mark-up (interpretation); (2) the assignment"
W04-2709,P03-1001,1,0.790541,"sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998), NMSU’s Mikrokosmos (Mahesh and Nirenburg, 1995), ISI's Upper Model (Bateman et al., 1989) and ISI's SENSUS (Knight and Luk, 1994). After the uppermost region of Omega was created by hand, these various resources’ contents were incorporated and, to some extent, reconciled. After that, several million instances of people, locations, and other facts were added (Fleischman et al., 2003). The ontology, which has been used in several projects in recent years (Hovy et al., 2001), can be browsed using the DINO browser at http://blombos.isi.edu:8000/dino; this browser forms a part of the annotation environment. Omega remains under continued development and extension. 4.1.2 IL1 IL1 is an intermediate semantic representation. It associates semantic concepts with lexical units like nouns, adjectives, adverbs and verbs (details of the ontology in section 4.2). It also replaces the syntactic relations in IL0, like subject and object, with thematic roles, like agent, theme and goal (de"
W04-2709,C18-2019,0,0.0664532,"Missing"
W04-2709,A97-1011,0,0.0452745,"first present the annotation process and tools used with it as well as the annotation manuals. Finally, setup issues relating to negotiating multi-site annotations are discussed. 6.1 Annotation process The annotation process was identical for each text. For the initial testing period, only English texts were annotated, and the process described here is for English text. The process for non-English texts will be, mutatis mutandis, the same. Each sentence of the text is parsed into a dependency tree structure. For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. For the initial testing period, annotators were not permitted to alter these structures. Already at this stage, some of the lexical items are replaced by features (e.g., tense), morphological forms are replaced by features on the citation form, and certain constructions are regularized (e.g., passive) and empty arguments inserted. It is this dependency structure that is loaded into the annotation tool and which each annotator then marks up. The annotator was instructed to annotate all nouns, verbs, adjectives, and adverbs. This involves annotating e"
W04-2709,1994.amta-1.25,0,0.0933693,"Missing"
W04-2709,C98-1013,0,\N,Missing
W05-0901,J96-2004,0,0.0669426,"rd. Unfortunately, a consistent gold standard has not yet been reported. For example, in two previous studies (Mani, 2001; Tombros and Sanderson, 1998), users’ judgments were compared to “gold standard judgments” produced by members of the University of Pennsylvania’s Linguistic Data Consortium. Although these judgments were supposed to represent the correct relevance judgments for each of the documents associated with an event, both studies reported that annotators’ judgments varied greatly and that this was a significant issue for the evaluations. In the SUMMAC experiments, the Kappa score (Carletta, 1996; Eugenio and Glass, 2004) for interannotator agreement was reported to be 0.38 (Mani et al., 2002). In fact, large variations have been found in the initial summary scoring of an individual participant and a subsequent scoring that occurs a few weeks later (Mani, 2001; van Halteren and Teufel, 2003). This paper attempts to overcome the problem of interannotator inconsistency by measuring summary effectiveness in an extrinsic task using a much more consistent form of user judgment instead of a gold standard. Using Relevance-Prediction increases the confidence in our results and strengthens the"
W05-0901,J04-1005,0,0.0262553,"Missing"
W05-0901,N03-1020,0,0.0884371,"-based usefulness. Section 3 presents a novel extrinsic measure called Relevance-Prediction. Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization. We expect these findings to be important for future summarization evaluations. Section 5 presents the results of correlation between task usefulness and the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric (Lin and Hovy, 2003).1 While we show that ROUGE correlates with task usefulness (using our Relevance-Prediction measure), we detect a slight difference between informative, extractive headlines (containing words from the full document) and less informative, non-extractive “eye-catchers” (containing words that might not appear in the full document, and intended to entice a reader to read the entire document). Section 6 further highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries. To evaluate nonextractive summaries reliably, an automatic measur"
W05-0901,C04-1072,0,0.0210096,"t whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearson’s r (Siegel and Castellan, 1988): Pn (ri − r¯)(si − s¯) pPn i=1 pPn (r − r¯)2 ¯)2 i=1 i i=1 (si − s where ri is the ROUGE score of surrogate i, r¯ is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s¯ is the average agreement score. Pearson’s statistics is commonly used in summarization and machine translation evaluation, see e.g. (Lin, 2004; Lin and Och, 2004). As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9 We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1. 6 Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions. However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1). Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement. For Relevance-Prediction, we ob"
W05-0901,W04-1013,0,0.0100221,"tion To test whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearson’s r (Siegel and Castellan, 1988): Pn (ri − r¯)(si − s¯) pPn i=1 pPn (r − r¯)2 ¯)2 i=1 i i=1 (si − s where ri is the ROUGE score of surrogate i, r¯ is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s¯ is the average agreement score. Pearson’s statistics is commonly used in summarization and machine translation evaluation, see e.g. (Lin, 2004; Lin and Och, 2004). As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9 We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1. 6 Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions. However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1). Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement. For Relevan"
W05-0901,N04-1019,0,0.0504141,"her highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries. To evaluate nonextractive summaries reliably, an automatic measure may require knowledge of sophisticated meaning units.2 It is our hope that the conclusions drawn herein will prompt investigation into more sophisticated automatic metrics as researchers shift their focus to non-extractive summaries. 1 ROUGE has been previously used as the primary automatic evaluation metric by NIST in the 2003 and 2004 DUC Evaluations. 2 The content units proposed in recent methods (Nenkova and Passonneau, 2004) are a first step in this direction. 1 Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation c and/or Summarization, pages 1–8, Ann Arbor, June 2005. 2005 Association for Computational Linguistics 2 Background In the past, assessments of usefulness involved a wide range of both intrinsic and extrinsic (task-based) measures (Sparck-Jones and Gallier, 1996). Intrinsic evaluations focus on coherence and informativeness (Jing et al., 1998) and often involve quality comparisons between automatic summaries and reference summaries that are pre-determin"
W05-0901,W00-0407,0,0.0284813,". Using Relevance-Prediction increases the confidence in our results and strengthens the statistical statements we can make about the benefits of summarization. The next section describes an alternative approach to measuring task-based usefulness, where the usage of external judgments as a gold standard is replaced by the 3 A topic is an event or activity, along with all directly related events and activities. An event is something that happens at some specific time and place, and the unavoidable consequences. 2 user’s own decisions on the full text. Following the lead of earlier evaluations (Oka and Ueda, 2000; Mani et al., 2002; Sakai and Sparck-Jones, 2001), we focus on relevance assessment as our extrinsic task. 3 Evaluation of Usefulness of Summaries We define a new extrinsic measure of task-based usefulness called Relevance-Prediction, where we compare a summary-based decision to the subject’s own full-text decision rather than to a different subject’s decision. Our findings differ from that of the SUMMAC results (Mani et al., 2002) in that using Relevance-Prediction as an alternative to comparision to a gold standard is a more realistic agreement measure for assessing usefulness in a relevanc"
W05-0901,J98-3005,0,0.0406616,"used. Definitive conclusions about the usefulness of summaries would provide justification for continued research and development of new summarization methods. To investigate the question of whether text summarization is useful in an extrinsic task, we examined human performance in a relevance assessment task using a human text surrogate (i.e. text intended to stand in the place of a document). We use single-document English summaries as these are sufficient for investigating task-based usefulness, although more elaborate surrogates are possible, e.g., those that span more than one document (Radev and McKeown, 1998; Mani and Bloedorn, 1998). The next section motivates the need for developing a new framework for measuring task-based usefulness. Section 3 presents a novel extrinsic measure called Relevance-Prediction. Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization. We expect these findings to be important for future summarization evaluations. Section 5 presents the results"
W05-0901,W03-0508,0,0.0592052,"Missing"
W05-1007,J02-3001,0,0.0502757,"positive contribution to language-based applications has motivated the development of a number of lexicalsemantic resources. Prominent among them are WordNet,1 PropBank,2 and FrameNet.3 The potential contribution of these resources is constrained by the information they contain and the level of effort involved in their development. For example, semantic annotation tasks (Baker et al., 2004) typically assign semantic roles to the arguments of predicates. The benefit of the semantic annotation is constrained by the presence and quality of semantic roles in the lexical-semantic resource(s) used. Gildea and Jurafsky (2002) suggest that the availability of semantic annotation of this sort is useful for information extraction, word sense disambiguation, machine translation, text summarization, text mining, and speech recognition. Other tasks rely on the identification of semantic relationships to recognize lexical chains (sets of semantically related words that enable a text to be cohesive) (Morris and Hirst, 1991). The success of this work is constrained by the set of semantic relationship types and instantiations underlying the recognition of lexical chains. As Stokes’s dissertation (2004) notes, lexical cohesi"
W05-1007,P04-1048,1,0.831459,"tation (2004) notes, lexical cohesion has been used in discourse analysis, text segmentation, word sense disambiguation, text summarization, topic detection and tracking, and question answering. Unfortunately, most lexical-semantic resources, including those previously mentioned, are the product of considerable ongoing human effort. Given the high development costs associated with these resources, the possibility of enhancing them on the basis of complementary resources that are produced automatically is welcome. This paper demonstrates several of the characteristics and benefits of SemFrame (Green et al., 2004; Green and Dorr, 2004), a system that produces such a resource. 1. SemFrame generates semantic frames in a form like those of FrameNet, the ostensible gold standard for semantic frames. 2. Some SemFrame frames correspond to FrameNet frames. When SemFrame identifies additional lexical units that evoke the frame, it bolsters the use of semantic frames for identifying lexical chains. 1 http://www.cogsci.princeton.edu/˜wn http://www.cis.upenn.edu/˜ace 3 http://framenet.icsi.berkeley.edu 3. Some SemFrame frames cover semantic space not yet investigated in FrameNet, which, be2 57 Proceedings of the"
W05-1007,W04-0909,1,0.830953,"lexical cohesion has been used in discourse analysis, text segmentation, word sense disambiguation, text summarization, topic detection and tracking, and question answering. Unfortunately, most lexical-semantic resources, including those previously mentioned, are the product of considerable ongoing human effort. Given the high development costs associated with these resources, the possibility of enhancing them on the basis of complementary resources that are produced automatically is welcome. This paper demonstrates several of the characteristics and benefits of SemFrame (Green et al., 2004; Green and Dorr, 2004), a system that produces such a resource. 1. SemFrame generates semantic frames in a form like those of FrameNet, the ostensible gold standard for semantic frames. 2. Some SemFrame frames correspond to FrameNet frames. When SemFrame identifies additional lexical units that evoke the frame, it bolsters the use of semantic frames for identifying lexical chains. 1 http://www.cogsci.princeton.edu/˜wn http://www.cis.upenn.edu/˜ace 3 http://framenet.icsi.berkeley.edu 3. Some SemFrame frames cover semantic space not yet investigated in FrameNet, which, be2 57 Proceedings of the ACL-SIGLEX Workshop on"
W05-1007,J91-1002,0,0.0973841,"ly assign semantic roles to the arguments of predicates. The benefit of the semantic annotation is constrained by the presence and quality of semantic roles in the lexical-semantic resource(s) used. Gildea and Jurafsky (2002) suggest that the availability of semantic annotation of this sort is useful for information extraction, word sense disambiguation, machine translation, text summarization, text mining, and speech recognition. Other tasks rely on the identification of semantic relationships to recognize lexical chains (sets of semantically related words that enable a text to be cohesive) (Morris and Hirst, 1991). The success of this work is constrained by the set of semantic relationship types and instantiations underlying the recognition of lexical chains. As Stokes’s dissertation (2004) notes, lexical cohesion has been used in discourse analysis, text segmentation, word sense disambiguation, text summarization, topic detection and tracking, and question answering. Unfortunately, most lexical-semantic resources, including those previously mentioned, are the product of considerable ongoing human effort. Given the high development costs associated with these resources, the possibility of enhancing the"
W07-0716,P02-1038,0,0.0853391,"In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. 1 Introduction Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation quality measure—relies on data sets in which source language sentences a"
W07-0716,W05-0909,0,0.06195,",nfa,resnik,bonnie}@umiacs.umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can b"
W07-0716,P05-1074,0,0.601747,"Missing"
W07-0716,N03-1003,0,0.572191,"Missing"
W07-0716,N06-1003,0,0.538654,"Missing"
W07-0716,P00-1056,0,0.104185,"The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexical re-orderings, but 121 phrasal re-orderings, as well. Each φ(¯ e, f¯, X) denotes a feature function defined on the pair of hierarchical phrases.1 Feature functions represent conditional and joint co-occurrence probabilities over the hierarchical paraphrase pair. The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data. Briefly, training a Hiero model proceeds as follows: • GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence. • Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004). • Grammar rules in the form of equation (2) are induced by “subtracting” out hierarchical phrase pairs from these initial phrase pairs. • Fractional counts are assigned to each produced rule: c(X → h¯ e, f¯i) = m X 1 j=1 njr (3) where m is the number of initial phrase pairs that give rise to"
W07-0716,J04-4002,0,0.0597953,"nt co-occurrence probabilities over the hierarchical paraphrase pair. The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data. Briefly, training a Hiero model proceeds as follows: • GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence. • Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004). • Grammar rules in the form of equation (2) are induced by “subtracting” out hierarchical phrase pairs from these initial phrase pairs. • Fractional counts are assigned to each produced rule: c(X → h¯ e, f¯i) = m X 1 j=1 njr (3) where m is the number of initial phrase pairs that give rise to this grammar rule and njr is the number of grammar rules produced by the j th initial phrase pair. • Feature functions φk1 (f¯, e¯, X) are calculated for each rule using the accumulated counts. Once training has taken place, minimum error rate training (Och, 2003) is used to tune the parameters λi . Fina"
W07-0716,P03-1021,0,0.0625549,"on Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation quality measure—relies on data sets in which source language sentences are paired with (sets of) reference translations. It is widely agreed that, at least for the widely used BLEU criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible. Intuitively this makes sense: if there"
W07-0716,N03-1024,0,0.450807,"Missing"
W07-0716,P02-1040,0,0.118446,"ion Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD, 20742 {nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper,"
W07-0716,W04-3219,0,0.482508,"Missing"
W07-0716,J07-2003,0,0.20005,"ique for paraphrasing, designed with the application to parameter tuning in mind. Section 4 presents evaluation results using a state of the art statistical MT system, demonstrating that half the human reference translations in a standard 4-reference tuning set can be replaced with automatically generated paraphrases, with no significant decrease in MT system performance. In Section 5 we discuss related work, and in Section 6 we summarize the results and discuss plans for future research. 2 Translation Framework The work described in this paper makes use of the Hiero statistical MT framework (Chiang, 2007). Hiero is formally based on a weighted synchronous context-free grammar (CFG), containing synchronous rules of the form X → h¯ e, f¯, φk1 (f¯, e¯, X)i (2) where X is a symbol from the nonterminal alphabet, and e¯ and f¯ can contain both words (terminals) and variables (nonterminals) that serve as placeholders for other phrases. In the context of statistical MT, where phrase-based models are frequently used, these synchronous rules can be interpreted as pairs of hierarchical phrases. The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexic"
W07-0716,2006.amta-papers.25,1,0.912501,".umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically"
W07-0716,W03-1608,0,0.16138,"Missing"
W07-0716,strassel-etal-2006-integrated,0,0.0201083,"count as possible. To do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided. This reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Some 120 Proceedings of the Second Workshop on Statistical Machine Translation, pages 120–127, c Prague, June 2007. 2007 Association for Computational Linguistics efforts have been made to develop alternative processes for eliciting translations, e.g., from users on the Web (Oard, 2003) or from informants in lowdensity languages (Probst et al., 2002). However, reference translations for parameter tuning and evaluation remain a severe data bottleneck for such approaches. Note, however, one crucial property of reference translations: they are paraphrases, i.e., multiple expressions of the same"
W07-0716,N03-1017,0,0.254743,"nslations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. 1 Introduction Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation qualit"
W07-0716,P06-1091,0,0.0340642,"we have an example input and a single labeled, correct output. However, this output is chosen from a space in which the number of possible outputs is exponential in the input size, and in which there are many good outputs in this space (although they are vastly outnumbered by the bad outputs). Various discriminative learning methods have attempted to deal with the first of these issues, often by restricting the space of examples. For instance, some max-margin methods restrict their computations to a set of examples from a “feasible set,” where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). The present approach deals with the second issue: in a learning problem where the use of a single positive example is likely to be highly biased, how can we produce a set of positive examples that is more representative of the space of correct outcomes? Our method exploits alternative sources of information to produce new positive examples that are, we hope, reasonably likely to represent a consensus of good examples. Quite a bit of work has been done on paraphrase, 6 We anticipate doing significance tests for differences in TER in future work. some clearly related to our technique, although"
W07-0716,W04-3250,0,0.0352334,"d in expanding the reference set via paraphrase: • Expanded (4H + 4P): This is the same as Condition 2, but using all four human references. Note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references. Table 4 shows BLEU and TER scores on the test set for all four conditions.5 If only two human references were available (simulated by using only two of the available four), expanding to four using paraphrases would yield a clear improvement. Using bootstrap resampling to compute confidence intervals (Koehn, 2004), we find that the improvement in BLEU score is statistically significant at p < .01. Equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing MERT using all four human reference translations. The difference in BLEU between conditions 2 and 3 is not significant. Finally, our fourth condition asks whether it is possible to improve MT performance given the typical four human reference translations used for MERT in most statistical MT systems, by adding a paraphrase to each one for a"
W07-0716,2006.iwslt-papers.3,0,0.0336729,"Missing"
W07-0716,N06-1057,0,0.159707,"Missing"
W07-2312,P06-1049,0,0.613813,". O’Learya,b Judith D. Schlesingerd a Department of Computer Science b Institute for Advanced Computer Studies University of Maryland, College Park {nmadnani,nfa,bonnie,oleary}@cs.umd.edu c Center for Computational Learning Systems, Columbia University becky@cs.columbia.edu d IDA/Center for Computing Sciences {conroy,judith}@super.org e College of Information Studies, University of Maryland, College Park jklavans@umd.edu While a good ordering is essential for summary comprehension (Barzilay et al., 2002), and recent The issue of sentence ordering is an important one work on sentence ordering (Bollegala et al., 2006) for natural language tasks such as multi-document does show promise, it is important to note that desummarization, yet there has not been a quantitatermining an optimal sentence ordering for a given tive exploration of the range of acceptable sentence summary may not be feasible. The question for orderings for short texts. We present results of a evaluation of ordering is whether there is a single sentence reordering experiment with three experibest ordering that humans will converge on, or that mental conditions. Our findings indicate a very high would lead to maximum reading comprehension,"
W07-2312,W98-1507,0,0.0368907,"timates of the expected values within each cell. Given a confusion matrix where the cells on the matrix diagonal are denoted as nii , the row marginals as ni+ , the column marginals as n+i and the matrix total as n++ , the formula for κ is: Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of"
W07-2312,W05-1621,0,0.241703,"ber is a significant factor in predicting mean τ scores, we can conclude that the 9 summaries differ from each other in terms of the variability among individuals. As in the earlier ANOVA presented in Table 1, we use Tukey’s HSD to determine the magnitude of the difference in means that is necessary for statistical significance, and use this to identify which summaries have significant differences in the amount of similarity among subjects’ reorderings. Applying Tukey’s method to summary number as a factor yields the differences shown in Table 2. 6 Related Work on Evaluating Sentence Ordering Karamanis and Mellish (2005) also measure the amount of variability between human subjects. However, there are several dimensions of contrast between our experiment and theirs: Their experiment operates in a very distinct domain (archaeology) and genre (descriptions of museum artifacts) whereas we use domain-independent multidocument summaries derived from news articles. We use ordinary, English-speaking volunteers as compared to the domain and genre experts that they employ (archaeologists trained in museum labeling). In terms of the experimental design, we use a Latin square design with three experimental condi86 tions"
W07-2312,P03-1069,0,0.678408,"; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that the sum of the distances between adjacent sentences is minimized. The distance (cjk ) between any pair of sentences j and k is computed by first obtaining a similarity score (bjk ) for the pair, and then normalizing this score: bjk , (cjj = 0) cjk = 1 − p"
W07-2312,J06-4002,0,0.267326,"sum to 6, κ ranges from 1 to 0, with 1 indicating that the set of reorderings all reproduce the initial ordering, and 0 indicating that the set of reorderings conforms to chance. 1 2 6 6 2 2 6 4 2 3 1 1 3 3 1 2 3 4 2 2 4 4 2 3 4 5 3 3 5 5 3 4 5 6 4 4 6 6 4 5 6 1 5 5 1 1 5 3 Figure 3: A hypothetical example illustrating Means Vectors compute means vectors for each condition for each summary, giving 27 such vectors. We compare each means vector representing a set of reorderings to each initial ordering O, R and T using three correlation coefficients: Pearson’s r, Spearman’s ρ, and Kendall’s τ (Lapata, 2006). The three correlation coefficients test the closeness of two series of numbers, or two variables x and y, in different ways. Pearson’s r is a parametric test of whether there is a perfect linear relation between the two variables. Spearman’s ρ and Kendall’s τ are non-parametric tests. Spearman’s ρ is computed by replacing the variable values by their rank and computing the correlation. Kendall’s τ is based on counting the number of pairs xi , xi+1 and yi , yi+1 where the deltas of both pairs have the same sign. In sum, the three metrics test whether x and y are in a linear relation, a rank-p"
W07-2312,C04-1108,0,0.578699,"in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that t"
W07-2312,P04-1051,0,0.118959,"Missing"
W07-2312,J98-3005,0,\N,Missing
W07-2312,I05-1055,0,\N,Missing
W08-0209,W08-0208,0,0.329,"orts high-level dynamic data types such as lists and hashes (termed dictionaries in Python), has very readable syntax and, most importantly, ships with an extensive standard library for almost every conceivable task. Although Python already has most of the functionality needed to perform very simple NLP tasks, its still not powerful enough for most standard ones. This is where the Natural Language Toolkit (NLTK) comes in. NLTK1 , written entirely in Python, is a collection of modules and corpora, released under an open-source license, that allows students to learn and conduct research in NLP (Bird et al., 2008). The most important advantage of using NLTK is that it is entirely self-contained. Not only does it provide convenient functions and wrappers that can be used as building blocks for common NLP tasks, it also provides raw and pre-processed versions of standard corpora used frequently in NLP literature. Together, Python and NLTK constitute one of the most potent tools for instruction of NLP (Madnani, 2007) and allow us to develop hands-on assignments that can appeal to a broad audience including both linguistics and computer science students. 1 http://nltk.org Figure 1: An Excerpt from the outp"
W08-0209,J00-4006,0,0.0068115,"tudents to gain insight into this important, but often omitted, idea from computational linguistics. Given the success that we had in our first attempt to re-engineer the introductory NLP course, we plan to continue: (1) our hands-on approach to programming assignments in the NLTK framework and, (2) our practice of adapting ideas from research publications as the bases for assignment and examination problems. Below we describe two concrete ideas for the next iteration of the course. 1. Hands-on Statistical Language Modeling. For this topic, we have so far restricted ourselves to the textbook (Jurafsky and Martin, 2000); the in-class discussion and programming assignments have been missing a hands-on component. We have written a Python interface to the SRI Language Modeling toolkit (Stolcke, 2002) for use in our research work. This interface uses the Simplified Wrapper & Interface Generator (SWIG) to generate a Python wrapper around our C code that does all the heavy lifting via the SRILM libraries. We are currently working on integrating this module into NLTK which would allow all NLTK users, including our students in the next version of the course, to build and query statistical language models directly in"
W08-0209,W02-0109,0,0.0918414,"ramming Framework. The previous version of our introductory course took a more fragmented approach and used different programming languages and tools for different assignments. For example, we used an in-house HMM library written in C for any HMM-based assignments and Perl for some other assignments. As expected, such an approach requires students to familiarize themselves with a different programming interface for each assignment and discourages students to explore on their own. To address this concern, we chose the Python (Python, 2007) programming language and the Natural Language Toolkit (Loper and Bird, 2002), written entirely in Python, for all our assignments and programming tasks. We discuss our use of NLTK in more detail in the next section. • Real-world Data & Corpora. In our previous course, students did not have access to any of the corpora that are used in actual NLP research. We found this to be a serious shortcoming and wanted to ensure that our new curriculum allowed students to use real corpora for evaluating their programming assignments. • Exposure to Research. While we had certainly made it a point to introduce recent research work in our lectures for all topics in the previous cour"
W08-0209,P93-1024,0,0.0168625,"offer such comparisons in the future. 6 Future Plans the same distribution, i.e., the same set of words or in the same context in a corpus, tend to have similar meanings. This is an extremely popular concept in corpus linguistics and forms the basis of a large body of work. We believe that this is an important topic that should be included in the curriculum. We plan to do so in the context of lexical paraphrase acquisition or synonyms automatically from corpora, a task that relies heavily on this notion of distributional similarity. There has been a lot of work in this area in the past years (Pereira et al., 1993; Gasperin et al., 2001; Glickman and Dagan, 2003; Shimohata and Sumita, 2005), much of which can be easily replicated using the Python-NLTK combination. This would allow for a very hands-on treatment and would allow the students to gain insight into this important, but often omitted, idea from computational linguistics. Given the success that we had in our first attempt to re-engineer the introductory NLP course, we plan to continue: (1) our hands-on approach to programming assignments in the NLTK framework and, (2) our practice of adapting ideas from research publications as the bases for as"
W08-0209,I05-1021,0,0.0127146,"ion, i.e., the same set of words or in the same context in a corpus, tend to have similar meanings. This is an extremely popular concept in corpus linguistics and forms the basis of a large body of work. We believe that this is an important topic that should be included in the curriculum. We plan to do so in the context of lexical paraphrase acquisition or synonyms automatically from corpora, a task that relies heavily on this notion of distributional similarity. There has been a lot of work in this area in the past years (Pereira et al., 1993; Gasperin et al., 2001; Glickman and Dagan, 2003; Shimohata and Sumita, 2005), much of which can be easily replicated using the Python-NLTK combination. This would allow for a very hands-on treatment and would allow the students to gain insight into this important, but often omitted, idea from computational linguistics. Given the success that we had in our first attempt to re-engineer the introductory NLP course, we plan to continue: (1) our hands-on approach to programming assignments in the NLTK framework and, (2) our practice of adapting ideas from research publications as the bases for assignment and examination problems. Below we describe two concrete ideas for th"
W08-0209,P06-4018,0,\N,Missing
W09-0441,W05-0909,0,0.098914,"frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human"
W09-0441,P05-1074,0,0.0378241,"ons, its average rank would be 1), although it performed well on average. In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER. 4 Paraphrases TERp uses probabilistic phrasal substitutions to align phrases in the hypothesis with phrases in the reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify English-to-F phrasal correspondences, then map from English to English by following translation units from English to F and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ≈ p(e1|f ) ∗ p(f |e2) 3 System description of metrics are also distributed by AMTA"
W09-0441,D08-1021,0,0.0160003,"s set included pairs that only shared partial semantic content. Most paraphrases extracted by the pivot method are expected to be of this nature. These pairs are not directly beneficial to TERp since they cannot be substituted for each other in all contexts. However, the fact that they share at least some semantic content does suggest that they may not be entirely useless either. Examples include: Varying Paraphrase Pivot Corpora To determine the effect that the pivot language might have on the quality and utility of the extracted paraphrases in TERp, we used paraphrase pairsmade available by Callison-Burch (2008). These paraphrase pairs were extracted from Europarl data using each of 10 European languages (German, Italian, French etc.) as a pivot language separately and then combining the extracted paraphrase pairs. Callison-Burch (2008) also extracted and made available syntactically constrained paraphrase pairs from the same data that are more likely to be semantically related. We used both sets of paraphrases in TERp as alternatives to the paraphrase pairs that we extracted from the Arabic newswire bitext. The results are shown in the last four rows of Table 5 and show that using a pivot language o"
W09-0441,N06-1058,0,0.0246276,"culating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER a"
W09-0441,E06-1031,0,0.0692041,"cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem"
W09-0441,W07-0716,1,0.299608,"ot only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Match"
W09-0441,2008.amta-papers.13,1,0.681799,"exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and"
W09-0441,niessen-etal-2000-evaluation,0,0.0236852,"ce. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference translation divided by the length of the reference translation. Unlike speech recognition, there are many correct translations for any given foreign sentence. These correct translations differ not only in their word choice but also in the order in which the words occur. WER is generally seen as inadequate for evaluation for machine translation as it fails to combine knowledge from multip"
W09-0441,P02-1040,0,0.102317,"n Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. 1 schwartz@bbn.com Introduction Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics. These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality. Numerous methods of judging MT output by humans Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259–268, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 259 or a missing determinator). Different types of translation errors va"
W09-0441,P07-1040,1,0.824501,"an alignment between the hypothesis and the reference, enabling it to be useful beyond general translation evaluation. While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference trans"
W09-0441,2006.amta-papers.25,1,0.965034,"correct meaning, even if the translation is not fully fluent. Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequa"
W09-0441,W06-1610,0,0.0609664,"align the two phrases according to TERp. In effect, the probability of the paraphrase is used to determine how much to discount the alignment of the two phrases. Specifically, the cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all"
W12-3807,P11-2049,0,0.0155415,"tion and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable amount of interest in modality in the biomedical domain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical const"
W12-3807,W09-1324,0,0.0193045,"nd hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the prese"
W12-3807,baker-etal-2010-modality,1,0.685998,"Missing"
W12-3807,J12-2006,1,0.846788,"features used to train our modality tagger and presents experiments and results. Section 5 concludes and discusses future work. 58 2 Related Work Previous related work includes TimeML (Sauri et al., 2006), which involves modality annotation on events, and Factbank (Sauri and Pustejovsky, 2009), where event mentions are marked with degree of factuality. Modality is also important in the detection of uncertainty and hedging. The CoNLL shared task in 2010 (Farkas et al., 2010) deals with automatic detection of uncertainty and hedging in Wikipedia and biomedical sentences. Baker et al. (2010) and Baker et al. (2012) analyze a set of eight modalities which include belief, require and permit, in addition to the five modalities we focus on in this paper. They built a rule-based modality tagger using a semi-automatic approach to create rules. This earlier work differs from the work described in this paper in that the our emphasis is on the creation of an automatic modality tagger using machine learning techniques. Note that the annotation and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable"
W12-3807,W09-3012,1,0.821042,"y of the information. Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagg"
W12-3807,P03-1004,0,0.0266465,"removed the ones which did not in fact have a modality. In the remaining sentences (94 sentences), our expert annotated the target predicate. We refer to this as the Gold dataset in this paper. The MTurk and Gold datasets differ in terms of genres as well as annotators (Turker vs. Expert). The distribution of modalities in both MTurk and Gold annotations are given in Table 2. 4.2 Gold Ability Table 1: For each modality, the number of sentences returned by the simple tagger that we posted on MTurk. 4 MTurk Table 2: Frequency of Modalities modalities in context. For tagging, we used the Yamcha (Kudo and Matsumoto, 2003) sequence labeling system which uses the SVMlight (Joachims, 1999) package for classification. We used One versus All method for multi-class classification on a quadratic kernel with a C value of 1. We report recall and precision on word tokens in our corpus for each modality. We also report Fβ=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall. 4.3 Features We used lexical features at the token level which can be extracted without any parsing with relatively high accuracy. We use the term context width to denote the window of tokens whose features are considered for predictin"
W12-3807,W09-1304,0,0.0189282,"ain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they"
W12-3807,Y05-1014,0,0.0194818,", absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes moda"
W12-3807,W06-3907,0,0.0243548,"of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn et al., 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modifiers. 3 Constructing Modality Training Data In this section, we will discuss the procedure we followed to construct the training data for building the automatic modality tagger. In a pilot study, we obtained and ran the modality tagger described in (Baker et al., 2010) on the English side of the Urdu-English LDC language pack.2 We randomly selected"
W12-3807,C10-2117,1,0.929409,". Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagger by using a semiautomatic"
W12-3807,C94-1018,0,0.029939,"Missing"
W12-3807,W08-0606,0,\N,Missing
W17-2318,W14-3203,0,0.0283974,"ndation for this earlier work. However, in these earlier studies, primitives consisted of properties that were syntactic, lexical, or semantic in nature, whereas the primitives for the current work consist of properties that are phonological in nature. Related Work A number of past studies have investigated the utility of measuring the “voice signal” in order to answer questions about a speaker’s state from their speech (Schuller et al., 2015, 2011). One such study attempts to distinguish classes of individuals with various speech impairments, such as stuttering (N¨oth et al., 2000), aphasia (Fraser et al., 2014), and developmental language disorders (Gorman et al., 2016). The recognition of impaired speech has been employed to detect Alzheimer’s (Rudzicz et al., 2014). Various speech-related features have been employed to detect whether the speech is affected by Parkinson’s Disease (Bocklet et al., 2011). Relatedly, variations in speech properties under intoxicated and sober conditions have also been conducted (Biadsy et al., 2011). Our work differs from prior approaches in that we explore perceivable phonological characteristics through the analysis of language divergences. One of the motivations fo"
W17-2318,habash-dorr-2002-handling,1,0.507291,"nd conclusions in Section 6. 2 device such as a phone, we focused on phonological features on which a machine can be trained to analyze automatically. Our focus on correlations with phonological features—tied to the notion of divergence from a baseline—is a significant contribution beyond what has been investigated previously. The notion of divergence itself is not a new one in natural language processing. The characterization of divergence classes (Dorr, 1994) has been at the heart of solutions to many different problems ranging from word alignment (Dorr et al., 2002) to machine translation (Habash and Dorr, 2002) to acquisition of semantic lexicons (Olsen et al., 1998). Finding the minimal primitive units—and determining their possible combinations—was the foundation for this earlier work. However, in these earlier studies, primitives consisted of properties that were syntactic, lexical, or semantic in nature, whereas the primitives for the current work consist of properties that are phonological in nature. Related Work A number of past studies have investigated the utility of measuring the “voice signal” in order to answer questions about a speaker’s state from their speech (Schuller et al., 2015, 20"
W17-2318,olsen-etal-1998-enhancing,1,0.085868,"cused on phonological features on which a machine can be trained to analyze automatically. Our focus on correlations with phonological features—tied to the notion of divergence from a baseline—is a significant contribution beyond what has been investigated previously. The notion of divergence itself is not a new one in natural language processing. The characterization of divergence classes (Dorr, 1994) has been at the heart of solutions to many different problems ranging from word alignment (Dorr et al., 2002) to machine translation (Habash and Dorr, 2002) to acquisition of semantic lexicons (Olsen et al., 1998). Finding the minimal primitive units—and determining their possible combinations—was the foundation for this earlier work. However, in these earlier studies, primitives consisted of properties that were syntactic, lexical, or semantic in nature, whereas the primitives for the current work consist of properties that are phonological in nature. Related Work A number of past studies have investigated the utility of measuring the “voice signal” in order to answer questions about a speaker’s state from their speech (Schuller et al., 2015, 2011). One such study attempts to distinguish classes of in"
W17-2318,W14-3210,0,0.0234023,"e explore perceivable phonological characteristics through the analysis of language divergences. One of the motivations for using phonological features exclusively rather than also using other features employed in prior studies was that phonological features did not require expensive equipment to collect data from speakers as e.g., a feature like maximum subglottal pressure would require. Since the goal of this work is to develop a measure that is completely based on speech features that can be identified with a simple click on a Beukelman et al. (2011), Duffy (2013), Green et al. (2013), and Orimaye et al. (2014) have established that pronunciation varies systematically within categories of speech impairment. (Silbergleit et al., 1997; Carrow et al., 1974) have shown that ALS speech shows deviant characteristics. For example, (Ball et al., 2001) observe that ALS speakers manifest altered voice quality. A number of speaker-level characteristics associated with impaired speech studied in prior work have been leveraged for our speech-related divergence detection. For example, Duffy (2013) specifically has enumerated speaker-level characteristics, such as monopitch and monoloudness. Rong et al. (2015; 201"
W18-1408,N03-1013,1,0.732872,"ed on the surface. For example, in the LCS Class of Verbs of inherently directed motion (corresponding to Class 51.1.a in (Levin, 1993)), the verb leave can take a NP complement (as in leave the room) and the verb depart can take a PP complement (as in departed from the room). For either case, the spatial component of meaning is uniformly move to a position outside of the room. Whereas the collocations were derived from thematic roles in the original LVD, the spatial components of meaning were derived from verbprepositions pairs associated with a subset of the “Categorial Variation” database (Habash and Dorr, 2003). Representative members of LCS classes were then paired with prepositions that were propagated to other members of the class. Table 1 summarizes the number of LCS classes associated with the lexical notions introduced above (Blocks, Overlaps, Fills-Oblig, Fills-Opt).4 Not all LCS classes are spatial in nature; thus, the second column provides a tally for the full set of LCS classes, and the third column provides a tally for just the spatial subset. The fourth column presents the number of spatial verbs included in the corresponding spatial classes. Representative spatial examples are provided"
W18-1408,habash-dorr-2002-handling,1,0.79309,"take these structures to capture language-bound meanings, that is semantic forms. In our framework, these do not, despite their name, capture language-independent, conceptual knowledge. 63 Proceedings of the First International Workshop on Spatial Language Understanding (SpLU-2018), pages 63–70 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics to its compositional, lexicon-based formalism and its potential for follow-on work in other language processing applications for which cross-lingual LCS mappings have already been devised (e.g., machine translation (Habash and Dorr, 2002)). We assume second, that for human-robot natural-language mediated communication, a number of constraints at the syntax-semantics interface are crucial for interpreting the wide ranging flexibility of real utterances and the context of the system is central to dialogue management. We leverage previously collected dialogue data with naturally occurring spoken Bot Language (Marge et al., 2017) that provides transcripts and dialog analyses (Traum et al., 2018), but without any form of lexical semantics. We assume third, that we will test and validate our approach by augmenting an implemented dia"
W18-1408,2006.amta-papers.7,1,0.65256,"ork and concluding remarks. 2 Approach This section introduces the notion of LCS and describes an LCS-based approach to systematic derivation of usage patterns for understanding and generation. We extend an LCS resource to include constraints (blocks, overlaps, and fills) and present the upshot of these extensions. 2.1 Lexical Conceptual Structure Lexical Conceptual Structure (LCS) (Jackendoff, 1983, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for a range of different applications, including interlingual machine translation (Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). The LCS representation was introduced by Jackendoff as based in the spatial domain and naturally extended to non-spatial domains, as specified by fields.3 For example, the spatial dimension of the LCS representation corresponds to the (Loc)ational field, which underlies the meaning of John traveled from Chicago to Boston in the LCS [John GOLoc [From Chicago] [To Boston]]. This is straightforwardly extended to the (Temp)oral field to represen"
W18-1408,1997.mtsummit-workshop.4,1,0.459919,"Missing"
W18-1408,kordjamshidi-etal-2010-spatial,0,0.0755491,"Missing"
W18-1408,W17-7415,0,0.190644,"th for hearing and for producing utterances for robot-robot communication. However, the position adopted here is one in which generalizations about language structure are assumed and available in natural language generation for both use (“lift up”) and suppression (“elevate”) of spatial prepositions in phrases containing motion and direction verbs, depending on the context. guage resources and standards for capturing spatial information. For example, the ISO 24617 standard provides guidelines for annotating spatial information in English language texts (246177, 2014) that continues to evolve (Pustejovsky and Lee, 2017). This Semantic Annotation Framework (semAF) identifies places, paths, spatial entities, and spatial relations that can be used to associate sequences of processes and events in news articles (Pustejovsky et al., 2011). Spatial prepositions and particles (such as near, off ) and verbs of position and movement (such as lean, swim) in text have corresponding spatial components of meanings, collocations, and classes of spatial verbs in the perspective adopted in this paper. Spatial role labeling using holistic spatial semantics (i.e., analysis at the level of the full utterance) has been used for"
W18-1408,W18-1503,1,0.822678,"cs. We assume third, that we will test and validate our approach by augmenting an implemented dialogue system for understanding and generation of Bot Language. The application of our foundational paradigm to this problem is a future direction outside of the scope of this position paper. The layered lexical representations referred to in the first assumption above form the basis for this discussion. Specifically, we posit that the development of an application such as robot navigation (Bonial et al., 2018; Moolchandani et al., 2018) or generation of narrative explanations (Korpan et al., 2017; Lukin et al., 2018) requires a layered representation scheme to include a set of spatial primitives (the basis for the LCS representation) coupled with a representation of constraints at the syntax-semantics interface. Additional layers include prepositional collocates2 and spatial semantics that are crucial for understanding and production of unconstrained spatial expressions. We describe our extensions to an LCS resource covering 500 semantic classes of verbs, of which 219 fall within a spatial subset. We demonstrate that this resource is designed to systematically account for certain types of spatial expressi"
W18-1408,W17-2808,1,0.717052,"tional, lexicon-based formalism and its potential for follow-on work in other language processing applications for which cross-lingual LCS mappings have already been devised (e.g., machine translation (Habash and Dorr, 2002)). We assume second, that for human-robot natural-language mediated communication, a number of constraints at the syntax-semantics interface are crucial for interpreting the wide ranging flexibility of real utterances and the context of the system is central to dialogue management. We leverage previously collected dialogue data with naturally occurring spoken Bot Language (Marge et al., 2017) that provides transcripts and dialog analyses (Traum et al., 2018), but without any form of lexical semantics. We assume third, that we will test and validate our approach by augmenting an implemented dialogue system for understanding and generation of Bot Language. The application of our foundational paradigm to this problem is a future direction outside of the scope of this position paper. The layered lexical representations referred to in the first assumption above form the basis for this discussion. Specifically, we posit that the development of an application such as robot navigation (Bo"
W18-1408,L18-1017,1,0.706712,"k in other language processing applications for which cross-lingual LCS mappings have already been devised (e.g., machine translation (Habash and Dorr, 2002)). We assume second, that for human-robot natural-language mediated communication, a number of constraints at the syntax-semantics interface are crucial for interpreting the wide ranging flexibility of real utterances and the context of the system is central to dialogue management. We leverage previously collected dialogue data with naturally occurring spoken Bot Language (Marge et al., 2017) that provides transcripts and dialog analyses (Traum et al., 2018), but without any form of lexical semantics. We assume third, that we will test and validate our approach by augmenting an implemented dialogue system for understanding and generation of Bot Language. The application of our foundational paradigm to this problem is a future direction outside of the scope of this position paper. The layered lexical representations referred to in the first assumption above form the basis for this discussion. Specifically, we posit that the development of an application such as robot navigation (Bonial et al., 2018; Moolchandani et al., 2018) or generation of narr"
W18-1408,W00-0207,0,0.556603,"based approach to systematic derivation of usage patterns for understanding and generation. We extend an LCS resource to include constraints (blocks, overlaps, and fills) and present the upshot of these extensions. 2.1 Lexical Conceptual Structure Lexical Conceptual Structure (LCS) (Jackendoff, 1983, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for a range of different applications, including interlingual machine translation (Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). The LCS representation was introduced by Jackendoff as based in the spatial domain and naturally extended to non-spatial domains, as specified by fields.3 For example, the spatial dimension of the LCS representation corresponds to the (Loc)ational field, which underlies the meaning of John traveled from Chicago to Boston in the LCS [John GOLoc [From Chicago] [To Boston]]. This is straightforwardly extended to the (Temp)oral field to represent analogous meanings such as The meeting went from 7pm to 9pm in the LCS [Meeting GOTemp [From 7pm] [To 9"
W18-1408,N15-3006,0,0.0135928,"ructures with layers, and those semantic structures contain primitives that are grounded at a conceptual level (not discussed herein). We leverage Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Dorr, 1993), a logical representation with compositional properties, to guide development of semantics for spatial language in language understanding and generation.1 We note that other logical representations may also be adequate for this study, e.g., Abstract Meaning Representation (Banarescu et al., 2014), Prague Dependency Trees (Hajiˇc et al., 2018), and descendants of such representations (Vanderwende et al., 2015). LCS has been selected due Introduction While prior work in spatial language understanding for tasks such as robot navigation focuses on mapping natural language into deep conceptual or non-linguistic representations—for further reasoning or embodied cognition (Perera et al., 2017; Pastra et al., 2011)—we argue that it is possible to systematically derive regular patterns of language usage from existing lexical-semantic resources (Dorr et al., 2001). Furthermore, even with access to such resources, effective solutions to many application areas such as robot navigation and narrative generation"
W18-3808,W96-0306,1,0.633997,"to Boston in the LCS [John GOLoc [From Chicago] [To Boston]]. This is straightforwardly extended to the (Temp)oral field to represent analogous meanings such as The meeting went from 7pm to 9pm in the LCS [Meeting GOTemp [From 7pm] [To 9pm]]. The LVD developed in prior work (Dorr et al., 2001) includes a set of LCS templates classified according to an extension of Levin (1993)’s 192 classes to a total of 500 classes covering 9525 verb entries (an additional 5500+ verb entries beyond the original 4000+ verb entries). The first 44 classes were added beyond the original set of semantic classes (Dorr and Jones, 1996). The remaining classes were derived through aspectual distinctions to yield a set of LCS classes that were finer-grained than the original Levin classes (Olsen et al., 1997). Each LCS class consists of a set of verbs and, in several cases, the classes included non-Levin words (those not in Levin (1993)), derived semi-automatically (Dorr, 1997). The original LVD provides a mapping of lexical-semantic structures to their surface realization. This mapping serves as a foundation for the enrichments that yield STYLUS. The new resource benefits from decades of prior study that led to the LVD. Speci"
W18-3808,W18-1404,1,0.327075,"lsemantic structures of predicates and their syntactic argument structure. Subsequent work (Dorr and Voss, 2018) argued that regular patterns of language usage can be systematically derived from lexicalsemantic representations and used in applications such as dialogue management for robot navigation. The latter investigation focused on the spatial dimension, e.g., motion and direction. We adopt the view that this systematicity also holds for verbs in the non-spatial dimension, including those that have been metaphorically related to the spatial dimension by a range of corpus-based techniques (Dorr and Olsen, 2018). We consider one example of a non-spatial application: generation of cyber-related textual notifications. We argue that this application requires knowledge at the syntaxsemantics interface that is analogous to spatial knowledge for robot navigation. A recent survey of narrative generation techniques (Kybartas and Bidarra, 2017) highlights several important components of narrative generation (including a story, plot, space, and discourse for telling the story), leaving open the means for surface-realization of arguments from an underlying lexical-semantic structure. STYLUS is designed to accom"
W18-3808,W18-1408,1,0.583813,"ion and for non-spatial applications such as generation of cyber-related notifications. 1 Introduction* This paper presents a derivative resource, called STYLUS (SysTematicallY Derived Language USage), produced through extraction of a set of argument realizations from lexical-semantic representations for a range of different verb classes (Appendix A). Prior work (Jackendoff, 1996; Levin, 1993; Olsen, 1994; Kipper et al., 2008; Palmer et al., 2017) has suggested a close relation between underlying lexicalsemantic structures of predicates and their syntactic argument structure. Subsequent work (Dorr and Voss, 2018) argued that regular patterns of language usage can be systematically derived from lexicalsemantic representations and used in applications such as dialogue management for robot navigation. The latter investigation focused on the spatial dimension, e.g., motion and direction. We adopt the view that this systematicity also holds for verbs in the non-spatial dimension, including those that have been metaphorically related to the spatial dimension by a range of corpus-based techniques (Dorr and Olsen, 2018). We consider one example of a non-spatial application: generation of cyber-related textual"
W18-3808,habash-dorr-2002-handling,1,0.411777,"objects (Kordjamshidi et al., 2011). The association between thematic roles and their corresponding surface realizations has been investigated previously, including in the LCS formalism (described next), but Kordjamshidi et al’s approach also ties into deeper notions such as region of space and frame of reference. 2.2 Lexical-Conceptual Structure Verb Database (LVD) Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Jackendoff, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for wide-ranging applications, including interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), crosslanguage information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). LCS incorporates primitives whose combination captures syntactic generalities, i.e., actions and entities must be systematically related to a syntactic structure: GO, STAY, BE, GO-EXT, ORIENT, and also an ACT primitive developed by Dorr and Olsen (1997). LCS is grounded in the spatial domain and is naturally extended to non-spatial domains, as specified by fields. For example, the spatial dimension o"
W18-3808,N03-1013,1,0.86152,"Components of mean1 Teletype font is used for components of meaning such as UPWARD. Several examples throughout this paper were purposely selected to illustrate the full range of syntactic realizations for the concept of “upwardness.” Other verbs and collocations could easily have been selected (e.g., lower with the collocation down), but a varied selection of lexical distinctions would confound the illustration of more general distinctions at the syntax-semantics interface. 59 ing were derived either from the LCS structure or from verb-prepositions pairs in a “Categorial Variation” database (Habash and Dorr, 2003). For example, the LCS above (from the :LCS slot) includes a sublexical component of meaning, ((* from 3) loc (thing 2) (at loc (thing 2) (thing 4))), that maps optionally to a preposition from in the surface form (as in exit (from) the room). Prepositional collocations (such as from) were derived from the thematic roles (in the :THETA ROLES slot) of the original LVD. These prepositions are specified in parentheses, and are preceded either by a comma (,) for optional collocations or by underscore (_) for obligatory collocations. In the example above, the theme is obligatory (_th), whereas the"
W18-3808,2006.amta-papers.7,1,0.830834,"ociation between thematic roles and their corresponding surface realizations has been investigated previously, including in the LCS formalism (described next), but Kordjamshidi et al’s approach also ties into deeper notions such as region of space and frame of reference. 2.2 Lexical-Conceptual Structure Verb Database (LVD) Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Jackendoff, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for wide-ranging applications, including interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), crosslanguage information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). LCS incorporates primitives whose combination captures syntactic generalities, i.e., actions and entities must be systematically related to a syntactic structure: GO, STAY, BE, GO-EXT, ORIENT, and also an ACT primitive developed by Dorr and Olsen (1997). LCS is grounded in the spatial domain and is naturally extended to non-spatial domains, as specified by fields. For example, the spatial dimension of the LCS representation corresponds to the"
W18-3808,W04-2604,0,0.0622208,"e benefits from decades of prior study that led to the LVD. Specifically, Levin’s classes are based on significant corpus analysis and have been validated in numerous within-language studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998) and cross-language studies (Guerssel et al., 1985; Levin, 2015). Thus, STYLUS is expected to have an important downstream impact, in both depth and breadth, for future linguistic investigations and computational applications. 2.3 Syntax-Semantics Interface Prior work (Jackendoff, 1996; Levin, 1993; Dorr and Voss, 1993; Voss and Dorr, 1995; Kipper et al., 2004; Kipper et al., 2008; Palmer et al., 2017) suggests that there is a close relation between underlying lexical-semantic structures of verbs and nominal predicates and their syntactic argument structure. VerbNet (Kipper et al., 2004) reinforces the view in this resource paper, that prepositions and their relation with verb classes serve as significant predictors of semantic content, but does not leverage an inner structure of events for compositional derivation of argument realizations. FrameNet also sits at the syntax-semantics interface (Fillmore, 2002), with linking generalizations based on"
W18-3808,W17-2808,1,0.826349,"same language usage patterns as their robot navigation counterparts. 5 Conclusion and Future Work STYLUS provides the basis for both understanding and generation in the spatial robot navigation domain and for generation in the non-spatial cyber notification domain. A larger scale application and evaluation of the effectiveness of STYLUS for understanding and generation in these two domains is a 4 An asterisk at the start of a sentence indicates an invalid generated form. Verbs in the designated class are italicized. 61 future area of study. A starting point is an ongoing Bot Language project (Marge et al., 2017) that has heretofore focused on dialogue annotation (Traum et al., 2018) and has not yet incorporated lexiconbased knowledge necessary for automatically detecting incomplete, vague, or implicit navigation commands. Another avenue for exploration is the enhancement of cyber notifications through systematic derivation of mappings to surface realizations for other parts of speech. This work will involve access to a “Categorial Variation” database (CatVar) (Habash and Dorr, 2003) to map verbs in the LCS classes to their nominalized and adjectivalized forms. For example, the CatVar entry for infect"
W18-3808,1997.mtsummit-workshop.4,1,0.494672,"nt from 7pm to 9pm in the LCS [Meeting GOTemp [From 7pm] [To 9pm]]. The LVD developed in prior work (Dorr et al., 2001) includes a set of LCS templates classified according to an extension of Levin (1993)’s 192 classes to a total of 500 classes covering 9525 verb entries (an additional 5500+ verb entries beyond the original 4000+ verb entries). The first 44 classes were added beyond the original set of semantic classes (Dorr and Jones, 1996). The remaining classes were derived through aspectual distinctions to yield a set of LCS classes that were finer-grained than the original Levin classes (Olsen et al., 1997). Each LCS class consists of a set of verbs and, in several cases, the classes included non-Levin words (those not in Levin (1993)), derived semi-automatically (Dorr, 1997). The original LVD provides a mapping of lexical-semantic structures to their surface realization. This mapping serves as a foundation for the enrichments that yield STYLUS. The new resource benefits from decades of prior study that led to the LVD. Specifically, Levin’s classes are based on significant corpus analysis and have been validated in numerous within-language studies (Levin and Rappaport Hovav, 1995; Rappaport Hova"
W18-3808,W17-7415,0,0.0278837,"ext section reviews related work, starting with the spatial underpinnings of the original LVD. Following this, we describe our extensions and present examples for two applications: natural language processing for robot navigation and generation of cyber-related notifications. 2 2.1 Background Spatial Language Understanding Spatial language understanding has made great strides in recent years, with the emergence of language resources and standards for capturing spatial information, e.g., (ISO-24617-7, 2014), which provide guidelines for annotating spatial information in English language texts (Pustejovsky and Lee, 2017; * This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 57 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 57–64 Santa Fe, New Mexico, USA, August 20, 2018. Pustejovsky and Yocum, 2014). This work differs from the perspective adopted for STYLUS in that it provides annotation guidelines for training systems for spatial information extraction, and so it does not focus on generalized mappings at the syntax-semantics interface. The Semantic Annotation Fra"
W18-3808,pustejovsky-yocum-2014-image,0,0.0284658,"atial language understanding has made great strides in recent years, with the emergence of language resources and standards for capturing spatial information, e.g., (ISO-24617-7, 2014), which provide guidelines for annotating spatial information in English language texts (Pustejovsky and Lee, 2017; * This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 57 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 57–64 Santa Fe, New Mexico, USA, August 20, 2018. Pustejovsky and Yocum, 2014). This work differs from the perspective adopted for STYLUS in that it provides annotation guidelines for training systems for spatial information extraction, and so it does not focus on generalized mappings at the syntax-semantics interface. The Semantic Annotation Framework (semAF) identifies places, paths, spatial entities, and spatial relations that can be used to associate sequences of processes and events in news articles (Pustejovsky et al., 2011). Prepositions and particles (near, off) and verbs of position and movement (lean, swim) have corresponding components of meanings and colloca"
W18-3808,W00-0207,0,0.0639704,"eviously, including in the LCS formalism (described next), but Kordjamshidi et al’s approach also ties into deeper notions such as region of space and frame of reference. 2.2 Lexical-Conceptual Structure Verb Database (LVD) Lexical Conceptual Structure (LCS) (Jackendoff, 1983; Jackendoff, 1990; Dorr, 1993; Dowty, 1979; Guerssel et al., 1985) has been used for wide-ranging applications, including interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition (Habash et al., 2006), crosslanguage information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). LCS incorporates primitives whose combination captures syntactic generalities, i.e., actions and entities must be systematically related to a syntactic structure: GO, STAY, BE, GO-EXT, ORIENT, and also an ACT primitive developed by Dorr and Olsen (1997). LCS is grounded in the spatial domain and is naturally extended to non-spatial domains, as specified by fields. For example, the spatial dimension of the LCS representation corresponds to the (Loc)ational field, which underlies the meaning of John traveled from Chicago to Boston in the LCS [Joh"
W18-3808,L18-1017,1,0.831244,"onclusion and Future Work STYLUS provides the basis for both understanding and generation in the spatial robot navigation domain and for generation in the non-spatial cyber notification domain. A larger scale application and evaluation of the effectiveness of STYLUS for understanding and generation in these two domains is a 4 An asterisk at the start of a sentence indicates an invalid generated form. Verbs in the designated class are italicized. 61 future area of study. A starting point is an ongoing Bot Language project (Marge et al., 2017) that has heretofore focused on dialogue annotation (Traum et al., 2018) and has not yet incorporated lexiconbased knowledge necessary for automatically detecting incomplete, vague, or implicit navigation commands. Another avenue for exploration is the enhancement of cyber notifications through systematic derivation of mappings to surface realizations for other parts of speech. This work will involve access to a “Categorial Variation” database (CatVar) (Habash and Dorr, 2003) to map verbs in the LCS classes to their nominalized and adjectivalized forms. For example, the CatVar entry for infect includes the nominalized form infection, which provides additional opti"
W91-0222,P90-1016,0,0.0674787,"Missing"
W91-0222,P90-1017,1,0.89048,"Missing"
W91-0222,J90-2002,0,0.0653942,"Missing"
W91-0222,J88-2002,0,0.0617088,"Missing"
W91-0222,W90-0110,0,0.0282564,"Missing"
W91-0222,J88-2003,0,0.821077,"Missing"
W91-0222,J88-2004,0,0.124696,"Missing"
W91-0222,J88-2005,0,0.306578,"Missing"
W91-0222,P85-1003,0,0.402122,"Missing"
W96-0306,P91-1019,0,0.0304552,"ic relation from machine-readable dictionaries. However, they claim that the semantic classification of verbs based on standard machine-readable dictionaries (e.g., the LDOCE) is ""a hopeless pursuit [since] standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness."" Others have also argued that the task of simplifying lexical entries on the basis of broad semantic class membership is complex and, perhaps, infeasible (see, e.g., Boguraev and Briscoe (1989)). However, a number of researchers (Fillmore, 1968; Grimshaw, 1990; Gruber, 1965; Guthrie et al., 1991; Hearst, 1991; Jackendoff, 1983; Jackendoff, 1990; Levin, 1993; Pinker, 1989; Yarowsky, 1992) have demonstrated conclusively that there is a clear relationship between syntactic context and word senses; it is our aim to exploit this relationship for the acquisition of semantic lexicons. We first describe the LDOCE verb classification resulting from a purely syntactic approach to deriving semantic classes. We then describe a semantic filter designed to reduce the number of incorrect assignments made by the syntactic technique; we show how this filter can be enhanced with a method that accounts"
W96-0306,A92-1011,0,0.0272622,"relation with a pre-defined association between WordNet (Miller, 1985) word senses and Levin's verbs in order to group the full Set of LDOCE verbs into semantic classes. While the LDOCE has been used previously in automatic extraction tasks (Alshawi, 1989; Farwell, Guthrie, and Wilks, 1993; Boguraev and Briscoe, 1989; Wilks et al., 1989; Wilks et al., 1990) these tasks are primarily concerned with the extraction of other types of information including syntactic phrase structure and broad argument restrictions or with the derivation of semantic structures from definition analyses. The work of Sanfilippo and Poznanski (1992) is more closely related to our approach in that it attempts to recover a syntactic-semantic relation from machine-readable dictionaries. However, they claim that the semantic classification of verbs based on standard machine-readable dictionaries (e.g., the LDOCE) is ""a hopeless pursuit [since] standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness."" Others have also argued that the task of simplifying lexical entries on the basis of broad semantic class membership is complex and, perhaps, infeasible (see, e.g., Boguraev and Briscoe"
W96-0306,C92-2070,0,0.0853764,"n of verbs based on standard machine-readable dictionaries (e.g., the LDOCE) is ""a hopeless pursuit [since] standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness."" Others have also argued that the task of simplifying lexical entries on the basis of broad semantic class membership is complex and, perhaps, infeasible (see, e.g., Boguraev and Briscoe (1989)). However, a number of researchers (Fillmore, 1968; Grimshaw, 1990; Gruber, 1965; Guthrie et al., 1991; Hearst, 1991; Jackendoff, 1983; Jackendoff, 1990; Levin, 1993; Pinker, 1989; Yarowsky, 1992) have demonstrated conclusively that there is a clear relationship between syntactic context and word senses; it is our aim to exploit this relationship for the acquisition of semantic lexicons. We first describe the LDOCE verb classification resulting from a purely syntactic approach to deriving semantic classes. We then describe a semantic filter designed to reduce the number of incorrect assignments made by the syntactic technique; we show how this filter can be enhanced with a method that accounts for multiple word senses. Finally we show the results of our classification of unknown verbs,"
