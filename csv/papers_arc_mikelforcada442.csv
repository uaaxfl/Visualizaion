2021.mtsummit-research.8,Surprise Language Challenge: Developing a Neural Machine Translation System between {P}ashto and {E}nglish in Two Months,2021,-1,-1,7,0,5031,alexandra birch,Proceedings of Machine Translation Summit XVIII: Research Track,0,In the media industry and the focus of global reporting can shift overnight. There is a compelling need to be able to develop new machine translation systems in a short period of time and in order to more efficiently cover quickly developing stories. As part of the EU project GoURMET and which focusses on low-resource machine translation and our media partners selected a surprise language for which a machine translation system had to be built and evaluated in two months(February and March 2021). The language selected was Pashto and an Indo-Iranian language spoken in Afghanistan and Pakistan and India. In this period we completed the full pipeline of development of a neural machine translation system: data crawling and cleaning and aligning and creating test sets and developing and testing models and and delivering them to the user partners. In this paperwe describe rapid data creation and experiments with transfer learning and pretraining for this low-resource language pair. We find that starting from an existing large model pre-trained on 50languages leads to far better BLEU scores than pretraining on one high-resource language pair with a smaller model. We also present human evaluation of our systems and which indicates that the resulting systems perform better than a freely available commercial system when translating from English into Pashto direction and and similarly when translating from Pashto into English.
2020.eamt-1.8,A multi-source approach for {B}reton{--}{F}rench hybrid machine translation,2020,-1,-1,2,0,9987,victor sanchezcartagena,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Corpus-based approaches to machine translation (MT) have difficulties when the amount of parallel corpora to use for training is scarce, especially if the languages involved in the translation are highly inflected. This problem can be addressed from different perspectives, including data augmentation, transfer learning, and the use of additional resources, such as those used in rule-based MT. This paper focuses on the hybridisation of rule-based MT and neural MT for the Breton{--}French under-resourced language pair in an attempt to study to what extent the rule-based MT resources help improve the translation quality of the neural MT system for this particular under-resourced language pair. We combine both translation approaches in a multi-source neural MT architecture and find out that, even though the rule-based system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality."
2020.eamt-1.32,An {E}nglish-{S}wahili parallel corpus and its use for neural machine translation in the news domain,2020,-1,-1,4,0,9988,felipe sanchezmartinez,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"This paper describes our approach to create a neural machine translation system to translate between English and Swahili (both directions) in the news domain, as well as the process we followed to crawl the necessary parallel corpora from the Internet. We report the results of a pilot human evaluation performed by the news media organisations participating in the H2020 EU-funded project GoURMET."
2020.acl-main.417,{P}ara{C}rawl: Web-Scale Acquisition of Parallel Corpora,2020,-1,-1,7,0,20851,marta banon,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems."
W19-6721,{P}ara{C}rawl: Web-scale parallel corpora for the languages of the {EU},2019,0,5,2,0,23594,miquel espla,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W19-6723,Global Under-Resourced Media Translation ({G}o{URMET}),2019,-1,-1,7,0,5031,alexandra birch,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W18-6453,Findings of the {WMT} 2018 Shared Task on Parallel Corpus Filtering,2018,0,19,4,0,4417,philipp koehn,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We posed the shared task of assigning sentence-level quality scores for a very noisy corpus of sentence pairs crawled from the web, with the goal of sub-selecting 1{\%} and 10{\%} of high-quality data to be used to train machine translation systems. Seventeen participants from companies, national research labs, and universities participated in this task."
W18-6464,{UA}lacant machine translation quality estimation at {WMT} 2018: a simple approach using phrase tables and feed-forward neural networks,2018,9,0,3,0.898515,9988,felipe sanchezmartinez,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We describe the Universitat d{'}Alacant submissions to the word- and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as \textit{OK} or \textit{BAD}, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them."
W18-6320,Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating machine translation for gisting,2018,0,0,1,1,5037,mikel forcada,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"A popular application of machine translation (MT) is \textit{gisting}: MT is consumed \textit{as is} to make sense of text in a foreign language. Evaluation of the usefulness of MT for gisting is surprisingly uncommon. The classical method uses \textit{reading comprehension questionnaires} (RCQ), in which informants are asked to answer professionally-written questions in their language about a foreign text that has been machine-translated into their language. Recently, \textit{gap-filling} (GF), a form of \textit{cloze} testing, has been proposed as a cheaper alternative to RCQ. In GF, certain words are removed from reference translations and readers are asked to fill the gaps left using the machine-translated text as a hint. This paper reports, for the first time, a comparative evaluation, using both RCQ and GF, of translations from multiple MT systems for the same foreign texts, and a systematic study on the effect of variables such as gap density, gap-selection strategies, and document context in GF. The main findings of the study are: (a) both RCQ and GF clearly identify MT to be useful; (b) global RCQ and GF rankings for the MT systems are mostly in agreement; (c) GF scores vary very widely across informants, making comparisons among MT systems hard, and (d) unlike RCQ, which is framed around documents, GF evaluation can be framed at the sentence level. These findings support the use of GF as a cheaper alternative to RCQ."
W16-3405,Stand-off Annotation of Web Content as a Legally Safer Alternative to Crawling for Distribution,2016,15,0,1,1,5037,mikel forcada,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,Funding from the European Union Seventh Framework Programme FP7/2007-2013 under grant agreement PIAP-GA-2012-324414 (Abu-MaTran) is acknowledged.
W16-2367,Bitextor{'}s participation in {WMT}{'}16: shared task on document alignment,2016,15,7,2,1,5040,miquel esplagomis,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the participation of Prompsit Language Engineering and the Universitat dxe2x80x99Alacant in the shared task on document alignment at the First Conference on Machine Translation (WMT 2016). Two systems have been submitted, corresponding to two different versions of the tool Bitextor: the last stable release, version 4.1, and the newest one, version 5.0. The paper describes the main features of each version of the tool and discusses the results obtained on the data sets published for the shared task."
W16-2383,{UA}lacant word-level and phrase-level machine translation quality estimation systems at {WMT} 2016,2016,6,2,3,1,5040,miquel esplagomis,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the Universitat dxe2x80x99Alacant submissions (labeled as UAlacant) to the machine translation quality estimation (MTQE) shared task at WMT 2016, where we have participated in the word-level and phrase-level MTQE subtasks. Our systems use external sources of bilingual information as a black box to spot sub-segment correspondences between the source segment and the translation hypothesis. For our submissions, two sources of bilingual information have been used: machine translation (Lucy LT KWIK Translator and Google Translate) and the bilingual concordancer Reverso Context. Building upon the word-level approach implemented for WMT 2015, a method for phrase-based MTQE is proposed which builds on the probabilities obtained for word-level MTQE. For each sub-task we have submitted two systems: one using the features produced exclusively based on online sources of bilingual information, and one combining them with the baseline features provided by the organisers of the task."
2016.eamt-2.4,Apertium: a free/open source platform for machine translation and basic language technology,2016,-1,-1,1,1,5037,mikel forcada,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
2016.eamt-2.19,{A}bu-{M}a{T}ran: automatic building of machine translation,2016,-1,-1,3,0.0803574,9426,antonio toral,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
2016.amta-researchers.3,Fuzzy-match repair using black-box machine translation systems: what can be expected?,2016,-1,-1,3,0.952381,5074,john ortega,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"Computer-aided translation (CAT) tools often use a translation memory (TM) as the key resource to assist translators. A TM contains translation units (TU) which are made up of source and target language segments; translators use the target segments in the TU suggested by the CAT tool by converting them into the desired translation. Proposals from TMs could be made more useful by using techniques such as fuzzy-match repair (FMR) which modify words in the target segment corresponding to mismatches identified in the source segment. Modifications in the target segment are done by translating the mismatched source sub-segments using an external source of bilingual information (SBI) and applying the translations to the corresponding positions in the target segment. Several combinations of translated sub-segments can be applied to the target segment which can produce multiple repair candidates. We provide a formal algorithmic description of a method that is capable of using any SBI to generate all possible fuzzy-match repairs and perform an oracle evaluation on three different language pairs to ascertain the potential of the method to improve translation productivity. Using DGT-TM translation memories and the machine system Apertium as the single source to build repair operators in three different language pairs, we show that the best repaired fuzzy matches are consistently closer to reference translations than either machine-translated segments or unrepaired fuzzy matches."
2016.amta-researchers.6,Ranking suggestions for black-box interactive translation prediction systems with multilayer perceptrons,2016,-1,-1,3,1,23599,daniel torregrosa,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"The objective of interactive translation prediction (ITP), a paradigm of computer-aided translation, is to assist professional translators by offering context-based computer-generated suggestions as they type. While most state-of-the-art ITP systems are tightly coupled to a machine translation (MT) system (often created ad-hoc for this purpose), our proposal follows a resourceagnostic approach, one that does not need access to the inner workings of the bilingual resources (MT systems or any other bilingual resources) used to generate the suggestions, thus allowing to include new resources almost seamlessly. As we do not expect the user to tolerate more than a few proposals each time, the set of potential suggestions need to be filtered and ranked; the resource-agnostic approach has been evaluated before using a set of intuitive length-based and position-based heuristics designed to determine which suggestions to show, achieving promising results. In this paper, we propose a more principled suggestion ranking approach using a regressor (a multilayer perceptron) that achieves significantly better results."
W15-4903,Using on-line available sources of bilingual information for word-level machine translation quality estimation,2015,20,8,3,1,5040,miquel esplagomis,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"This paper explores the use of external sources of bilingual information available on-line for word-level machine translation quality estimation (MTQE). These sources of bilingual information are used as a black box to spot sub-segment correspondences between a source-language (SL) sentence S to be translated and a given translation hypothesis T in the target-language (TL). This is done by segmenting both S and T into overlapping sub-segments of variable length and translating them into the TL and the SL, respectively, using the available bilingual sources of information on the fly. A collection of features is then obtained from the resulting sub-segment translations, which is used by a binary classifier to determine which target words in T need to be post-edited. Experiments are conducted based on the data sets published for the word-level MTQE task in the 2014 edition of the Workshop on Statistical Machine Translation (WMT 2014). The sources of bilingual information used are: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. The results obtained confirm that, using less information and fewer features, our approach obtains results comparable to those of state-of-the-art approaches, and even outperform them in some data sets. c xc2xa9 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND."
W15-4904,A general framework for minimizing translation effort: towards a principled combination of translation technologies in computer-aided translation,2015,-1,-1,1,1,5037,mikel forcada,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-4918,Evaluating machine translation for assimilation via a gap-filling task,2015,10,2,2,0,36551,ekaterina ageeva,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-4919,Unsupervised training of maximum-entropy models for lexical selection in rule-based machine translation,2015,21,2,3,0.621719,1394,francis tyers,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system. The training method described is unsupervised; that is, it does not require any annotated corpus. The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model. Using the MT system, the sentences in the sourcelanguage corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system. These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models. We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost."
W15-4944,{A}bu-{M}a{T}ran: Automatic building of Machine Translation,2015,48,0,8,0.0803574,9426,antonio toral,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
W15-3036,{UA}lacant word-level machine translation quality estimation system at {WMT} 2015,2015,8,9,3,1,5040,miquel esplagomis,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the Universitat dxe2x80x99Alacant submissions (labelled as UAlacant) for the machine translation quality estimation (MTQE) shared task in WMT 2015, where we participated in the wordlevel MTQE sub-task. The method we used to produce our submissions uses external sources of bilingual information as a black box to spot sub-segment correspondences between a source segmentS and the translation hypothesisT produced by a machine translation system. This is done by segmenting bothS andT into overlapping subsegments of variable length and translating them in both translation directions, using the available sources of bilingual information on the fly. For our submissions, two sources of bilingual information were used: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. After obtaining the subsegment correspondences, a collection of features is extracted from them, which are then used by a binary classifer to obtain the final xe2x80x9cGOODxe2x80x9d or xe2x80x9cBADxe2x80x9d word-level quality labels. We prepared two submissions for this yearxe2x80x99s edition of WMT 2015: one using the features produced by our system, and one combining them with the baseline features published by the organisers of the task, which were ranked third and first for the sub-task, respectively."
2015.eamt-1.4,Using on-line available sources of bilingual information for word-level machine translation quality estimation,2015,20,8,3,1,5040,miquel esplagomis,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"This paper explores the use of external sources of bilingual information available on-line for word-level machine translation quality estimation (MTQE). These sources of bilingual information are used as a black box to spot sub-segment correspondences between a source-language (SL) sentence S to be translated and a given translation hypothesis T in the target-language (TL). This is done by segmenting both S and T into overlapping sub-segments of variable length and translating them into the TL and the SL, respectively, using the available bilingual sources of information on the fly. A collection of features is then obtained from the resulting sub-segment translations, which is used by a binary classifier to determine which target words in T need to be post-edited. Experiments are conducted based on the data sets published for the word-level MTQE task in the 2014 edition of the Workshop on Statistical Machine Translation (WMT 2014). The sources of bilingual information used are: machine translation (Apertium and Google Translate) and the bilingual concordancer Reverso Context. The results obtained confirm that, using less information and fewer features, our approach obtains results comparable to those of state-of-the-art approaches, and even outperform them in some data sets. c xc2xa9 2015 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND."
2015.eamt-1.5,A general framework for minimizing translation effort: towards a principled combination of translation technologies in computer-aided translation,2015,-1,-1,1,1,5037,mikel forcada,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
2015.eamt-1.19,Evaluating machine translation for assimilation via a gap-filling task,2015,10,2,3,0,36551,ekaterina ageeva,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
2015.eamt-1.20,Unsupervised training of maximum-entropy models for lexical selection i in rule-based machine translation,2015,21,2,3,0.621719,1394,francis tyers,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"This article presents a method of training maximum-entropy models to perform lexical selection in a rule-based machine translation system. The training method described is unsupervised; that is, it does not require any annotated corpus. The method uses source-language monolingual corpora, the machine translation (MT) system in which the models are integrated, and a statistical target-language model. Using the MT system, the sentences in the sourcelanguage corpus are translated in all possible ways according to the different translation equivalents in the bilingual dictionary of the system. These translations are then scored on the target-language model and the scores are normalised to provide fractional counts for training source-language maximum-entropy lexical-selection models. We show that these models can perform equally well, or better, than using the target-language model directly for lexical selection, at a substantially reduced computational cost."
2015.eamt-1.45,{A}bu-{M}a{T}ran: Automatic building of Machine Translation,2015,48,0,8,0.0803574,9426,antonio toral,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W14-0309,Black-box integration of heterogeneous bilingual resources into an interactive translation system,2014,17,6,3,0.7455,5039,juan perezortiz,Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation,0,"The objective of interactive translation prediction (ITP) is to assist human translators in the translation of texts by making context-based computer-generated suggestions as they type. Most of the ITP systems in literature are strongly coupled with a statistical machine translation system that is conveniently adapted to provide the suggestions. In this paper, however, we propose a resource-agnostic approach in which the suggestions are obtained from any bilingual resource (a machine translation system, a translation memory, a bilingual dictionary, etc.) that provides targetlanguage equivalents for source-language segments. These bilingual resources are considered to be black boxes and do not need to be adapted to the peculiarities of the ITP system. Our evaluation shows that savings of up to 85% can be theoretically achieved in the number of keystrokes when using our novel approach. Preliminary user trials indicate that these benefits can be partly transferred to real-world computer-assisted translation interfaces."
forcada-2014-annotation,On the annotation of {TMX} translation memories for advanced leveraging in computer-aided translation,2014,7,2,1,1,5037,mikel forcada,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The term advanced leveraging refers to extensions beyond the current usage of translation memory (TM) in computer-aided translation (CAT). One of these extensions is the ability to identify and use matches on the sub-segment level â for instance, using sub-sentential elements when segments are sentencesâ to help the translator when a reasonable fuzzy-matched proposal is not available; some such functionalities have started to become available in commercial CAT tools. Resources such as statistical word aligners, external machine translation systems, glossaries and term bases could be used to identify and annotate segment-level translation units at the sub-segment level, but there is currently no single, agreed standard supporting the interchange of sub-segmental annotation of translation memories to create a richer translation resource. This paper discusses the capabilities and limitations of some current standards, envisages possible alternatives, and ends with a tentative proposal which slightly abuses (repurposes) the usage of existing elements in the TMX standard."
2014.eamt-1.4,An efficient method to assist non-expert users in extending dictionaries by assigning stems and inflectional paradigms to unknknown words,2014,18,4,5,1,5040,miquel esplagomis,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,A method is presented to assist users with no background in linguistics in adding the unknown words in a text to monolingual dictionaries such as those used in rulebased machine translation systems. Adding a word to these dictionaries requires identifying its stem and the inflection paradigm to be used in order to generate all its word forms. Our method is based on a previous interactive approach in which non-expert users were asked to validate whether some tentative word forms were correct forms of the new word; these validations were then used to determine the most appropriate stem and paradigm. The previous approach was based on a set of intuitive heuristics designed both to obtain an estimate of the eligibility of each candidate stem/paradigm combination and to determine the word form to be validated at each step. Our new approach however uses formal models for both tasks (a hidden Markov model to estimate eligibility and a decision tree to select the word form) and achieves significantly better results.
2014.amta-researchers.4,Using any machine translation source for fuzzy-match repair in a computer-aided translation setting,2014,13,1,3,0.952381,5074,john ortega,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"When a computer-assisted translation (CAT) tool does not find an exact match for the source segment to translate in its translation memory (TM), translators must use fuzzy matches that come from translation units in the translation memory that do not completely match the source segment. We explore the use of a fuzzy-match repair technique called patching to repair translation proposals from a TM in a CAT environment using any available machine translation system, or any external bilingual source, regardless of its internals. Patching attempts to aid CAT tool users by repairing fuzzy matches and proposing improved translations. Our results show that patching improves the quality of translation proposals and reduces the amount of edit operations to perform, especially when a specific set of restrictions is applied."
S12-1065,{UA}lacant: Using Online Machine Translation for Cross-Lingual Textual Entailment,2012,17,8,3,1,5040,miquel esplagomis,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper describes a new method for cross-lingual textual entailment (CLTE) detection based on machine translation (MT). We use sub-segment translations from different MT systems available online as a source of cross-lingual knowledge. In this work we describe and evaluate different features derived from these sub-segment translations, which are used by a support vector machine classifier to detect CLTEs. We presented this system to the SemEval 2012 task 8 obtaining an accuracy up to 59.8% on the English-Spanish test set, the second best performing approach in the contest."
2012.eamt-1.54,Flexible finite-state lexical selection for rule-based machine translation,2012,21,13,3,0.621719,1394,francis tyers,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"In this paper we describe a module (rule formalism, rule compiler and rule processor) designed to provide flexible support for lexical selection in rule-based machine translation. The motivation and implementation for the system is outlined and an efficient algorithm to compute the best coverage of lexical-selection rules over an ambiguous input sentence is described. We provide a demonstration of the module by learning rules for it on a typical training corpus and evaluating against other possible lexicalselection strategies. The inclusion of the module, along with rules learnt from the parallel corpus provides a small, but consistent and statistically-significant improvement over either using the highest-scoring translation according to a target-language model or using the most frequent aligned translation in the parallel corpus which is also found in the systemxe2x80x99s bilingual dictionaries."
2011.mtsummit-papers.18,Using machine translation in computer-aided translation to suggest the target-side words to change,2011,20,9,3,0.895522,5040,miquel esplagomis,Proceedings of Machine Translation Summit XIII: Papers,0,Work supported by Spanish government through TIN2009-14009-C02-01 project. M.L. Forcadaxe2x80x99s sabbatical stay at Dublin City University was supported by Science Foundation Ireland (SFI) through ETS Walton Award 07/W.1/I1802 and by Universitat dxe2x80x99Alacant (Spain).
2011.eamt-1.13,Using word alignments to assist computer-aided translation users by marking which target-side words to change or keep unedited,2011,17,10,3,0,23594,miquel espla,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"This paper explores a new method to improve computer-aided translation (CAT) systems based on translation memory (TM) by using pre-computed word alignments between the source and target segments in the translation units (TUs) of the userxe2x80x99s TM. When a new segment is to be translated by the CAT user, our approach uses the word alignments in the matching TUs to mark the words that should be changed or kept unedited to transform the proposed translation into an adequate translation. In this paper, we evaluate different sets of alignments obtained by using GIZA. Experiments conducted in the translation of Spanish texts into English show that this approach is able to predict which target words have to be changed or kept unedited with an accuracy above 94% for fuzzy-match scores greater or equal to 60%. In an appendix we evaluate our approach when new TUs (not seen during the computation of the word-alignment models) are used."
2011.eamt-1.28,Using Example-Based {MT} to Support Statistical {MT} when Translating Homogeneous Data in a Resource-Poor Setting,2011,16,10,4,0.952381,6023,sandipan dandapat,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"In this paper, we address the issue of applying example-based machine translation (EBMT) methods to overcome some of the difficulties encountered with statistical machine translation (SMT) techniques. We adopt two different EBMT approaches and present an approach to augment output quality by strategically combining both EBMT approaches with the SMT system to handle issues arising from the use of SMT. We use these approaches for English to Turkish translation using the IWSLT09 dataset. Improved evaluation scores (4% relative BLEU improvement) were achieved when EBMT was used to translate sentences for which SMT failed to produce an adequate translation."
W10-1720,{MATREX}: The {DCU} {MT} System for {WMT} 2010,2010,29,40,9,0,38356,sergio penkale,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the DCU machine translation system in the evaluation campaign of the Joint Fifth Workshop on Statistical Machine Translation and Metrics in ACL-2010. We describe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation. We participated in the English--Spanish and English--Czech translation tasks, in which we employed our multi-engine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder."
2009.freeopmt-1.3,The Apertium machine translation platform: Five years on,2009,24,21,1,1,5037,mikel forcada,Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation,0,"This paper describes Apertium: a free/open-source machine translation platform (engine, toolbox and data), its history, its philosophy of design, its technology, the community of developers, the research and business based on it, and its prospects and challenges, now that it is five years old."
2007.tmi-papers.22,Automatic induction of shallow-transfer rules for open-source machine translation,2007,16,9,2,0.952381,9988,felipe sanchezmartinez,Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,Spanish Government through projects TIC2003-08681-C02-01 and TIN2006-15071-C03-01. Spanish Government and the European Social Fund through research grant BES-2004-4711.
W05-0818,{LIHLA}: Shared Task System Description,2005,11,4,3,0,17593,helena caseli,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and language-independent heuristics to find links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English-Inuktitut and Romanian-English parallel sentences, respectively."
2005.mtsummit-osmtw.2,An Open Architecture for Transfer-based Machine Translation between {S}panish and {B}asque,2005,7,10,7,0,28091,inaki alegria,Workshop on open-source machine translation,0,"We present the current status of development of an open architecture for the translation from Spanish into Basque. The machine translation architecture uses an open source analyser for Spanish and new modules mainly based on finite-state transducers. The project is integrated in the OpenTrad initiative, a larger government funded project shared among different universities and small companies, which will also include MT engines for translation among the main languages in Spain. The main objective is the construction of an open, reusable and interoperable framework. This paper describes the design of the engine, the formats it uses for the communication among the modules, the modules reused from other project named Matxin and the new modules we are building."
2005.mtsummit-osmtw.4,An Open-Source Shallow-Transfer Machine Translation Toolbox: Consequences of Its Release and Availability,2005,8,14,3,0,8510,carme armentanooller,Workshop on open-source machine translation,0,"By the time Machine Translation Summit X is held in September 2005, our group will have released an open-source machine translation toolbox as part of a large government-funded project involving four universities and three linguistic technology companies from Spain. The machine translation toolbox, which will most likely be released under a GPL-like license includes (a) the open-source engine itself, a modular shallow-transfer machine translation engine suitable for related languages and largely based upon that of systems we have already developed, such as interNOSTRUM for Spanish{---}Catalan and Traductor Universia for Spanish{---}Portuguese, (b) extensive documentation (including document type declarations) specifying the XML format of all linguistic (dictionaries, rules) and document format management files, (c) compilers converting these data into the high-speed (tens of thousands of words a second) format used by the engine, and (d) pilot linguistic data for Spanish{---}Catalan and Spanish{---}Galician and format management specifications for the HTML, RTF and plain text formats. After describing very briefly this toolbox, this paper aims at exploring possible consequences of the availability of this architecture, including the community-driven development of machine translation systems for languages lacking this kind of linguistic technology."
2005.eamt-1.12,An open-source shallow-transfer machine translation engine for the {R}omance languages of {S}pain,2005,7,44,2,0,51220,antonio corbibellot,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"We present the current status of development of an open-source shallow-transfer machine translation engine for the Romance languages of Spain (the main ones being Spanish, Catalan and Galician) as part of a larger government-funded project which includes non-Romance languages such as Basque and involving both universities and linguistic technology companies. The machine translation architecture uses finite-state transducers for lexical processing, hidden Markov models for part-of-speech tagging, and finite-state based chunking for structural transfer, and is largely based upon that of systems already developed by the Transducens group at the Universitat d'Alacant, such as interNOSTRUM (Spanishxe2x80x94Catalan) and Traductor Universia (Spanishxe2x80x94Portuguese). The possible scope of the project, however, is wider, since it will be possible to use the resulting machine translation system with new pairs of languages; to that end, the project also aims at proposing standard formats to encode the linguistic data needed. This paper briefly describes the machine translation engine, the formats it uses for linguistic data, and the compilers that convert these data into an efficient format used by the engine."
2004.tmi-1.15,Cooperative unsupervised training of the part-of-speech taggers in a bidirectional machine translation system,2004,8,5,3,0.952381,9988,felipe sanchezmartinez,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"When building a machine translation system, the embedded part-of-speech (PoS) tagger deserves special attention, since PoS ambiguities are one of the main sources of mistranslations, specially when related languages are involved. The standard statistical approach for PoS tagging are hidden Markov models (HMM) properly trained by collecting statistics from source-language texts. In the case of bidirectional machine translation systems, this kind of training is often individually performed on each PoS tagger without taking into account the other language, that is, the corresponding target language. But target-language information may help to improve performance. In this paper, a new method is proposed which trains both PoS taggers simultaneously using mutual interaction: at every iteration, the parameters of the HMM corresponding to one of the languages are refined by using the statistical data supplied by the current HMM for the other language. Both models bootstrap by learning cooperatively in an unsupervised manner and require only monolingual texts; no aligned texts are needed. Preliminary results are promising and surpass those of traditional unsupervised approaches."
2003.mtsummit-tttt.2,A 45-hour computers in translation course,2003,-1,-1,1,1,5037,mikel forcada,Workshop on Teaching Translation Technologies and Tools,0,This paper describes how a 45-hour Computers in Translation course is actually taught to 3rd-year translation students at the University of Alacant; the course described started in year 1995{--}1996 and has undergone substantial redesign until its present form. It is hoped that this description may be of use to instructors who are forced to teach a similar subject in such as small slot of time and need some design guidelines.
J02-2004,Incremental Construction and Maintenance of Minimal Finite-State Automata,2002,7,44,2,0,40389,rafael carrasco,Computational Linguistics,0,"Daciuk et al. [Computational Linguistics 26(1):3-16 (2000)] describe a method for constructing incrementally minimal, deterministic, acyclic finite-state automata (dictionaries) from sets of strings. But acyclic finite-state automata have limitations: For instance, if one wants a linguistic application to accept all possible integer numbers or Internet addresses, the corresponding finite-state automaton has to be cyclic. In this article, we describe a simple and equally efficient method for modifying any minimal finite-state automaton (be it acyclic or not) so that a string is added to or removed from the language it accepts; both operations are very important when dictionary maintenance is performed and solve the dictionary construction problem addressed by Daciuk et al. as a special case. The algorithms proposed here may be straightforwardly derived from the customary textbook constructions for the intersection and the complementation of finite-state automata; the algorithms exploit the special properties of the automata resulting from the intersection operation when one of the finite-state automata accepts a single string."
2002.tmi-tmiw.3,Using multilingual content on the web to build fast finite-state direct translation systems,2002,-1,-1,1,1,5037,mikel forcada,Workshop on machine translation roadmap,0,None
2002.tmi-papers.7,Incremental construction and maintenance of morphological analysers based on augmented letter transducers,2002,-1,-1,2,0,53661,alicia garridoalenda,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
2002.eamt-1.16,Explaining real {MT} to translators: between compositional semantics and word-for-word,2002,-1,-1,1,1,5037,mikel forcada,Proceedings of the 6th EAMT Workshop: Teaching Machine Translation,0,None
2001.mtsummit-teach.7,Discovering machine translation strategies beyond word-for-word translation: a laboratory assignment,2001,-1,-1,2,0.695061,5039,juan perezortiz,Workshop on Teaching Machine Translation,0,"It is a common mispreconception to say that machine translation programs translate word-for-word, but real systems follow strategies which are much more complex. This paper proposes a laboratory assignment to study the way in which some commercial machine translation programs translate whole sentences and how the translation differs from a word-for-word translation. Students are expected to infer some of these extra strategies by observing the outcome of real systems when translating a set of sentences designed on purpose. The assignment also makes students aware of the difficulty of constructing such programs while bringing some technological light into the apparent {``}magic{''} of machine translation."
2000.bcs-1.7,Learning machine translation strategies using commercial systems: discovering word reordering rules,2000,-1,-1,1,1,5037,mikel forcada,Proceedings of the International Conference on Machine Translation and Multilingual Applications in the new Millennium: MT 2000,0,None
