2020.acl-main.297,W06-1651,0,0.0926525,"k An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating"
2020.acl-main.297,P17-4017,0,0.0949806,"TL model effectively boosts the F1 score by 9.29 over the syntaxagnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art. 1 $ Cardoso Opinion and sentiment analysis has a wide range of real-world applications like social media monitoring (Bollen et al., 2011), stock market prediction (Nguyen et al., 2015), box office prediction (Yu et al., 2010), and general e-commerce applications (Kim et al., 2013; Hu et al., 2017; Cui et al., 2017). In particular, fine-grained opinion analysis aims to identify users’ opinions in a text, including opinion expressions, holders of the opinions, targets of the opinions, target-dependent attitude, and intensity of opinions (Marasovi´c and Frank, 2018), which is very important for understanding political stance, Corresponding author challenge facing Chavez Target is ... Figure 1: An Example of ORL (bottom) and syntactic dependency tree (top) for “Cardoso says challenge facing Chavezis is reestablishing normalcy.” Introduction ∗ says Holder Expression customers’ reviews, marketing trends, and"
2020.acl-main.297,N19-1423,0,0.237545,"cy arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more information while alleviating parsing errors. For the second barrier, considering that the pipeline methods are notorious for the error propagation problem, we introduce multi-task learning (MTL) frameworks, which have been widely used in many NLP models when predictions at various processing levels are needed (Collobert and Weston, 2008; Ruder, 2017). Apart from the syntactic information, contextualized word representations like BERT (Devlin et al., 2019) are widely used to compensate for the sparsity of task-specific training data. They compress distributional semantics of words from large corpora, making the local context fluent and natural. However, the long-distance dependencies between words are often ignored, which is ideally able to be captured by syntactic analysis. In summary, based on previous studies in using syntax to improve various tasks, this work investigates whether syntax can enhance the neural ORL model. Particularly, we try to answer the following three questions. • How to effectively integrate various syntactic information"
2020.acl-main.297,P19-1024,0,0.0190934,"nd the ORL model. We first obtain the edge-weighted graph from the decoder of a well-trained biaffine parser as a data preprocessing step, and then feed the graph into our D EP GCN in the form of an adjacency matrix A 1 . Then we feed the outputs of the ORL BiLSTM-based encoder as the initial inputs h0 to the D EP GCN. Finally, we feed the output of the D EP GCN to the CRF-based decoder, and update the ORL results under the guidance of the syntactic information. Moreover, we introduce dense connections to the multi-layer D EP GCN for extracting more structural information (Huang et al., 2017; Guo et al., 2019). Instead of only adding connections between adjacent layers, we use dense connections from each layer to all the subsequent layers. Formally, the input of node i at the l-th layer is: (l) 4.1 (0) (1) (l−1) xi = hi ⊕ hi ⊕ · · · ⊕ hi Dependency Graph Convolutional Networks (D EP GCN) (3) (l) In this subsection, we propose dependency graph convolutional networks (D EP GCN) to better encode the syntactic information from the edgeweighted graphs. On the one hand, compared with explicit 1-best parse trees, edge-weighted graphs where hi is the output of node i at the l-th layer. We also make residua"
2020.acl-main.297,P18-1192,0,0.0735118,"human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax prov"
2020.acl-main.297,J13-3002,0,0.0940515,"dency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, conside"
2020.acl-main.297,P16-1087,0,0.537352,"s, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as semantic predicates, and op"
2020.acl-main.297,W06-0301,0,0.113222,"from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issu"
2020.acl-main.297,N18-1054,0,0.470419,"Missing"
2020.acl-main.297,D17-1159,0,0.406534,"ural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representati"
2020.acl-main.297,P16-1105,0,0.0190351,"SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a)"
2020.acl-main.297,P16-1113,0,0.0491495,"res from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved t"
2020.acl-main.297,ruppenhofer-etal-2008-finding,0,0.26963,"ion mining task, opinion role labeling (ORL) aims to identify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Targ"
2020.acl-main.297,D18-1548,0,0.0256489,"ic roles. Marasovi´c and Frank (2018) take SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Z"
2020.acl-main.297,P15-1150,0,0.0563738,"them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved that syntactic knowledge is useful in the neural ORL models. Yang and Cardie"
2020.acl-main.297,D19-1541,1,0.407543,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,N19-1075,0,0.138213,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,P13-1161,0,0.632022,"ts of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as"
2020.acl-main.297,Q14-1039,0,0.218287,"alleviate the error propagation problem; and 3) contributions from syntactic information, especially from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targe"
2020.acl-main.297,N19-1118,1,0.170002,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D14-1162,0,0.083367,"RT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) to obtain deep contextualized word representations as our extra inputs. In particular, we use BERT-base (uncased) model and extract representations from the top-1 hidden layer. Our experiments show that using the top-1 layer representations performs better than the more common use of aggregating top-4 hidden layers.2 Parameters. We follow the previous works of Zhang et al. (2019b) and Marasovi´c and Frank (2018) without much parameter tuning. Specifically, we use the pretrained 100-dimensional glove embeddings (Pennington et al., 2014). The BiLSTM layer number is set to 3, and the hidden output size is 200. We apply 0.33 dropout to word representation and the hidden states of the BiLSTM. We choose Adam (Kingma and Ba, 2014) to optimize model parameters with a learning rate 10−3 . The entire training instances are trained for 30 epochs with the batch size of 50, and the best-epoch model at the peak performance on the dev corpus is chosen. For the MTL, we train the batches of ORL and parsing in turn since this interleaving training can obtain better performance in our experiments. Besides, we use the corpus weighting trick to"
2020.acl-main.297,N19-1066,0,0.110957,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D19-1057,1,0.172248,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.302,D17-1171,0,0.048482,"the DL era, due to 2 Though many recent works report higher performance with extra resources, for example contextualized word representations learned from large-scale unlabeled texts under language model loss, they either adopt the same architecture or achieve similar performance under fair comparison. the unmatched speed of CPU and GPU computation. This leads to the second question: can we batchify the inside-outside algorithm and perform computation directly on GPUs? In that case, we can employ efficient TreeCRF as a built-in component in DL toolkits such as PyTorch for wider applications (Cai et al., 2017; Le and Zuidema, 2014). Overall, targeted at the above two questions, this work makes the following contributions. • We for the first time propose second-order TreeCRF for neural dependency parsing. We also propose an efficient and effective triaffine operation for scoring second-order subtrees. • We propose to batchify the inside algorithm via direct large tensor computation on GPUs, leading to very efficient TreeCRF loss computation. We show that the complex outside algorithm is no longer needed for the computation of gradients and marginal probabilities, and can be replaced by the equally"
2020.acl-main.302,D07-1101,0,0.132825,"Most non-neural graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association fo"
2020.acl-main.302,D14-1082,0,0.596848,"plex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single dependencies in a first-order parser. These three representative works all employ global max-margin training. Dozat and Manning (2017) propose a strong and efficient biaffine parser and obtain state-of-the-art accuracy on a variety of datasets and languages. The biaffine parser is also first-"
2020.acl-main.302,N19-1423,0,0.0536989,"” means p &lt; 0.05 and “‡” means p &lt; 0.005. Biaffine17: Dozat and Manning (2017); F&K19: Falenska and Kuhn (2019); Li19: Li et al. (2019); Ji19: Ji et al. (2019); Zhang19: Zhang et al. (2019). sentences per second, which is able to meet the requirements of a real-time system. More discussions on efficiency are presented in Appendix A. 4.2 Main Results Table 1 lists the main results on the dev and test data. The trends on dev and test are mostly consistent. For a fair comparison with previous works, we only consider those without using extra resources such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). We can see that our baseline L OC achieves the best performance on both PTB and CoNLL09. On PTB, both C RF and C RF 2 O fail to improve 3300 94 86 93 85 C RF 2 O C RF L OC 92 91 72 100 200 300 400 70 C RF 2 O C RF L OC 84 83 100 200 300 400 C RF 2 O C RF L OC 68 66 100 200 300 400 Figure 5: Convergence curves (LAS vs. training epochs) on dev data of PTB, CoNLL09, and NLPCC19. SIB R P F UCM LCM PTB L OC 91.16 90.80 90.98 61.59 50.66 C RF 91.24 90.92 91.08 61.92 50.33 C RF 2 O 91.56 91.11 91.33 63.08 50.99 84 92 82 C RF 2 O (Dep) L OC (Dep) C RF 2 O (Sent) L OC (Sent) 1/4 1/2 full C RF 2 O (De"
2020.acl-main.302,K17-3002,0,0.0623887,"Missing"
2020.acl-main.302,N19-1116,0,0.0182699,"incomplete, complete, and sibling spans in the opposite direction from j to i for brevity. Basically, we first pack the scores of same-width spans at different positions (i, j) for all B sentences in the data batch into large tensors. Then we can do computation and aggregation simultaneously on GPUs via efficient large tensor operation. Similarly, we also batchify the decoding algorithm. Due to space limitation, we omit the details. It is noteworthy that the techniques described here are also applicable to other grammar formulations such as CKY-style constituency parsing (Finkel et al., 2008; Drozdov et al., 2019). 3298 3.3 Outside via Back-propagation Eisner (2016) proposes a theoretical proof on the equivalence between the back-propagation mechanism and the outside algorithm in the case of constituency (phrase-structure) parsing. This work empirically verifies this equivalence for dependency parsing. Moreover, we also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by r"
2020.acl-main.302,W16-5901,0,0.358708,"tion from j to i for brevity. Basically, we first pack the scores of same-width spans at different positions (i, j) for all B sentences in the data batch into large tensors. Then we can do computation and aggregation simultaneously on GPUs via efficient large tensor operation. Similarly, we also batchify the decoding algorithm. Due to space limitation, we omit the details. It is noteworthy that the techniques described here are also applicable to other grammar formulations such as CKY-style constituency parsing (Finkel et al., 2008; Drozdov et al., 2019). 3298 3.3 Outside via Back-propagation Eisner (2016) proposes a theoretical proof on the equivalence between the back-propagation mechanism and the outside algorithm in the case of constituency (phrase-structure) parsing. This work empirically verifies this equivalence for dependency parsing. Moreover, we also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the de"
2020.acl-main.302,P19-1012,0,0.17469,"9 94.16 94.12 96.08 96.04 96.02 96.14 96.11 94.47 94.34 94.33 94.49 94.46 CoNLL09 88.90 85.38 88.68 85.47 88.77 85.58 89.07 89.04 89.12 89.29 89.44 86.10 86.04 86.12 86.24 86.37 89.15 89.14 89.28 89.49 89.63‡ 85.98 86.06 86.18† 86.39 86.52‡ NLPCC19 L OC 77.01 71.14 C RF w/o MBR 77.40 71.65 C RF 77.34 71.62 C RF 2 O w/o MBR 77.58 71.92 C RF 2 O 78.08 72.32 76.92 77.17 77.53‡ 77.89 78.02‡ 71.04 71.58 71.89‡ 72.25 72.33‡ Table 1: Main results. We perform significance test against L OC on the test data, where “†” means p &lt; 0.05 and “‡” means p &lt; 0.005. Biaffine17: Dozat and Manning (2017); F&K19: Falenska and Kuhn (2019); Li19: Li et al. (2019); Ji19: Ji et al. (2019); Zhang19: Zhang et al. (2019). sentences per second, which is able to meet the requirements of a real-time system. More discussions on efficiency are presented in Appendix A. 4.2 Main Results Table 1 lists the main results on the dev and test data. The trends on dev and test are mostly consistent. For a fair comparison with previous works, we only consider those without using extra resources such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). We can see that our baseline L OC achieves the best performance on both PTB and CoNLL09."
2020.acl-main.302,P08-1109,0,0.132583,"Missing"
2020.acl-main.302,N18-1091,0,0.062174,"Missing"
2020.acl-main.302,P99-1010,0,0.129976,"e also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase. 3.4 Handling Partial Annotation As an attractive research direction, studies show that it is more effective to construct or even collect partially labeled data (Nivre et al., 2014; Hwa, 1999; Pereira and Schabes, 1992), where a sentence may correspond to a partial tree |y p |&lt; n in the case of dependency parsing. Partial annotation can be very powerful when combined with active learning, because annotation cost can be greatly reduced if annotators only need to annotate sub-structures that are difficult for models. Li et al. (2016) present a detailed survey on this topic. Moreover, Peng et al. (2019) recently released a partially labeled multi-domain Chinese dependency treebank based on this idea. Then, the question is how to train models on partially labeled data. Li et al. (2016"
2020.acl-main.302,P19-1237,0,0.539301,"th Stanford dependencies (Chen and Manning, 2014), and the Chinese data at the CoNLL09 shared task (Hajiˇc et al., 2009). We also adopt the Chinese dataset released at the NLPCC19 cross-domain dependency parsing shared task (Peng et al., 2019), containing one source domain and three target domains. For simplicity, we directly merge the train/dev/test data of the four domains into larger ones respectively. One characteristic of the data is that most sentences are partially annotated based on active learning. Finally, we conduct experiments on Universal Dependencies (UD) v2.2 and v2.3 following Ji et al. (2019) and Zhang et al. (2019) respectively. We adopt the 300d multilingual pretrained word embeddings used in Zeman et al. (2018) and take the CharLSTM representations as input. For UD2.2, to compare with Ji et al. (2019), we follow the raw text setting of the CoNLL18 shared task (Zeman et al., 2018), and directly use their sentence segmentation and tokenization results. For UD2.3, we also report the results of using gold-standard POS tags to compare with Zhang et al. (2019). Evaluation metrics. We use unlabeled and labeled attachment score (UAS/LAS) as the main metrics. Punctuations are omitted fo"
2020.acl-main.302,P18-1252,1,0.858339,"ely perform the inside algorithm for non-neural parsing, due to the inapplicability of the automatic differentiation mechanism. In order to obtain marginal probabilities and then feature weight gradients, we have to realize the more sophisticated outside algorithm, which is usually at least twice slower than the inside algorithm. This may be the major reason for the less popularity of TreeCRF (vs. max-margin training) before the DL era. As far as we know, all previous works on neural TreeCRF parsing explicitly implement the insideoutside algorithm for gradient computation (Zhang et al., 2019; Jiang et al., 2018). To improve efficiency, computation is transferred from GPUs to CPUs with Cython programming. This work shows that the inside algorithm can be effectively batchified to fully utilize the power of GPUs. Figure 3 and Algorithm 1 together illustrate the batchified version of the second-order inside algorithm, which is a direct extension of the secondorder Eisner algorithm in McDonald and Pereira (2006) by replacing max product with sum product. We omit the generations of incomplete, complete, and sibling spans in the opposite direction from j to i for brevity. Basically, we first pack the scores"
2020.acl-main.302,Q16-1023,0,0.24619,"how little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single dependencies in a first-order parser. These three representative works all employ global max-margin training. Dozat and Manning (2017) propose a strong and efficient biaffine parser and obtain state-of-the-art accuracy on a variety of datasets and languages. The biaffine parser is also first-order and employs simpler and more efficient non-structural training via local head selection for each token (Zhang et al., 2017). Observing such contrasting development, we try to make a connection between pre-DL and"
2020.acl-main.302,P10-1001,0,0.470072,"graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguist"
2020.acl-main.302,N16-1030,0,0.515877,") with two modifications, i.e., using CharLSTM word representation vectors instead of POS tag embeddings, and the first-order Eisner algorithm (Eisner, 2000) for projective decoding instead of the non-projective MST algorithm. Scoring architecture. Figure 2 shows the scoring architecture, consisting of four components. Input vectors. The ith input vector is composed of two parts: the word embedding and the CharLSTM word representation vector of wi . ei = emb(wi ) ⊕ CharLSTM(wi ) (1) where CharLSTM(wi ) is obtained by feeding wi into a BiLSTM and then concatenating the two last hidden vectors (Lample et al., 2016). We find that replacing POS tag embeddings with 3296 s(i, k, j) s(i, j) Biaffine rhi Triaffine rm j MLPh In other words, the model is trained based on simple head selection, without considering the tree structure at all, and losses of all words in a minibatch are accumulated. rhi 0 rsk 0 MLPm MLPh hi MLPs hk rm j MLPm Decoding. Having scores of all dependencies, we adopt the first-order Eisner algorithm with time complexity of O(n3 ) to find the optimal tree.   X y ∗ = arg max s(x, y) ≡ s(i, j) (5) 0 0 hj BiLSTM × 3 y i→j∈y . . . ei . . . ek . . . ej . . . Figure 2: Scoring architecture w"
2020.acl-main.302,D14-1081,0,0.115051,"contrasting development, we try to make a connection between pre-DL and DL techniques for graph-based parsing. Specifically, the first question to be addressed in this work is: can previously useful techniques such as structural learning and high-order modeling further improve the state-of-the-art2 biaffine parser, and if so, in which aspects are they helpful? For structural learning, we focus on the more complex and less popular TreeCRF instead of maxmargin training. The reason is two-fold. First, estimating probability distribution is the core issue in modern data-driven NLP methods (Le and Zuidema, 2014). The probability of a tree, i.e., p(y |x), is potentially more useful than an unbounded score s(x, y) for high-level NLP tasks when utilizing parsing outputs. Second, as a theoretically sound way to measure model confidence of subtrees, marginal probabilities can support Minimum Bayes Risk (MBR) decoding (Smith and Smith, 2007), and are also proven to be crucial for the important research line of token-level active learning based on partial trees (Li et al., 2016). One probable reason for the less popularity of TreeCRF, despite its usefulness, is due to the complexity and inefficiency of the"
2020.acl-main.302,P16-1033,1,0.912898,"ning. The reason is two-fold. First, estimating probability distribution is the core issue in modern data-driven NLP methods (Le and Zuidema, 2014). The probability of a tree, i.e., p(y |x), is potentially more useful than an unbounded score s(x, y) for high-level NLP tasks when utilizing parsing outputs. Second, as a theoretically sound way to measure model confidence of subtrees, marginal probabilities can support Minimum Bayes Risk (MBR) decoding (Smith and Smith, 2007), and are also proven to be crucial for the important research line of token-level active learning based on partial trees (Li et al., 2016). One probable reason for the less popularity of TreeCRF, despite its usefulness, is due to the complexity and inefficiency of the inside-outside algorithm, especially the outside algorithm. As far as we know, all existing works compute the inside and outside algorithms on CPUs. The inefficiency issue becomes more severe in the DL era, due to 2 Though many recent works report higher performance with extra resources, for example contextualized word representations learned from large-scale unlabeled texts under language model loss, they either adopt the same architecture or achieve similar perfo"
2020.acl-main.302,C12-2077,0,0.217529,"nd-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single d"
2020.acl-main.302,P05-1012,0,0.283507,"tures and differs from its neural counterpart in two major aspects. First, structural learning, i.e., explicit awareness of tree structure constraints during training, is indispensable. Most non-neural graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins"
2020.acl-main.302,E06-1011,0,0.854148,"cture constraints during training, is indispensable. Most non-neural graph-based parsers adopt the max-margin training algorithm, which first predicts a highest-scoring tree with the current model, and then updates feature weights so that the correct tree has a higher score than the predicted tree. Second, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c"
2020.acl-main.302,H05-1066,0,0.677681,"Missing"
2020.acl-main.302,J14-2001,0,0.0184305,"parsing. Moreover, we also find that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase. 3.4 Handling Partial Annotation As an attractive research direction, studies show that it is more effective to construct or even collect partially labeled data (Nivre et al., 2014; Hwa, 1999; Pereira and Schabes, 1992), where a sentence may correspond to a partial tree |y p |&lt; n in the case of dependency parsing. Partial annotation can be very powerful when combined with active learning, because annotation cost can be greatly reduced if annotators only need to annotate sub-structures that are difficult for models. Li et al. (2016) present a detailed survey on this topic. Moreover, Peng et al. (2019) recently released a partially labeled multi-domain Chinese dependency treebank based on this idea. Then, the question is how to train models on partially labeled data. Li e"
2020.acl-main.302,P05-1013,0,0.0852854,"a proportion of random dependencies for each sentence (partial trees). Figure 6 shows the results. We can see that the performance gap is quite steady when we gradually reduce the number of training sentences. In contrast, the gap clearly becomes larger when each training sentence has less annotated dependencies. This shows that C RF 2 O is superior to the basic L OC in utilizing partial annotated data for model training. 4.4 Results on Universal Dependencies Table 3 compares different models on UD datasets, which contain a lot of non-projective trees. We adopt the pseudo-projective approach (Nivre and Nilsson, 2005) for handling the ubiquitous nonprojective trees of most languages. Basically, the idea is to transform non-projective trees into projective ones using more complex labels for postprocessing recovery. We can see that for the basic local parsers, the direct non-projective L OC MST and the pseudoprojective L OC achieve very similar performance. More importantly, both C RF and C RF 2 O produce consistent improvements over the baseline in many languages. On both UD2.2 and UD2.3, Our proposed C RF 2 O model achieves the highest accuracy for 10 languages among 12, and obtains significant improvement"
2020.acl-main.302,P15-1031,0,0.0184933,"econd, high-order modeling brings significant accuracy gains. The basic first-order model factors the score of a tree into independent scores of single dependencies (McDonald et al., 2005a). Second-order models were soon propose to incorporate scores of dependency pairs, such as adjacent-siblings (McDonald and Pereira, 2006) and grand-parent-child (Carreras, 2007; Koo and Collins, 2010), showing significant accuracy improvement yet with the cost of lower efficiency and more complex decoding algorithms.1 In contrast, neural graph-based dependency parsing exhibits an opposite development trend. Pei et al. (2015) propose to use feed-forward neural 1 Third-order and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtree"
2020.acl-main.302,P92-1017,0,0.281353,"that marginal probabilities p(i → j |x) directly correspond to gradients after back-propagation with log Z(x) as the loss: ∂ log Z = ∂s(i, j) X p(y |x) = p(i → j |x) y:(i,j)∈y (11) which can be easily proved. For TreeCRF parsers, we perform MBR decoding (Smith and Smith, 2007) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase. 3.4 Handling Partial Annotation As an attractive research direction, studies show that it is more effective to construct or even collect partially labeled data (Nivre et al., 2014; Hwa, 1999; Pereira and Schabes, 1992), where a sentence may correspond to a partial tree |y p |&lt; n in the case of dependency parsing. Partial annotation can be very powerful when combined with active learning, because annotation cost can be greatly reduced if annotators only need to annotate sub-structures that are difficult for models. Li et al. (2016) present a detailed survey on this topic. Moreover, Peng et al. (2019) recently released a partially labeled multi-domain Chinese dependency treebank based on this idea. Then, the question is how to train models on partially labeled data. Li et al. (2016) propose to extend TreeCRF"
2020.acl-main.302,N18-1202,0,0.0277633,"L OC on the test data, where “†” means p &lt; 0.05 and “‡” means p &lt; 0.005. Biaffine17: Dozat and Manning (2017); F&K19: Falenska and Kuhn (2019); Li19: Li et al. (2019); Ji19: Ji et al. (2019); Zhang19: Zhang et al. (2019). sentences per second, which is able to meet the requirements of a real-time system. More discussions on efficiency are presented in Appendix A. 4.2 Main Results Table 1 lists the main results on the dev and test data. The trends on dev and test are mostly consistent. For a fair comparison with previous works, we only consider those without using extra resources such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). We can see that our baseline L OC achieves the best performance on both PTB and CoNLL09. On PTB, both C RF and C RF 2 O fail to improve 3300 94 86 93 85 C RF 2 O C RF L OC 92 91 72 100 200 300 400 70 C RF 2 O C RF L OC 84 83 100 200 300 400 C RF 2 O C RF L OC 68 66 100 200 300 400 Figure 5: Convergence curves (LAS vs. training epochs) on dev data of PTB, CoNLL09, and NLPCC19. SIB R P F UCM LCM PTB L OC 91.16 90.80 90.98 61.59 50.66 C RF 91.24 90.92 91.08 61.92 50.33 C RF 2 O 91.56 91.11 91.33 63.08 50.99 84 92 82 C RF 2 O (Dep) L OC (Dep) C RF 2 O (Sent) L OC ("
2020.acl-main.302,D07-1014,0,0.720599,"f so, in which aspects are they helpful? For structural learning, we focus on the more complex and less popular TreeCRF instead of maxmargin training. The reason is two-fold. First, estimating probability distribution is the core issue in modern data-driven NLP methods (Le and Zuidema, 2014). The probability of a tree, i.e., p(y |x), is potentially more useful than an unbounded score s(x, y) for high-level NLP tasks when utilizing parsing outputs. Second, as a theoretically sound way to measure model confidence of subtrees, marginal probabilities can support Minimum Bayes Risk (MBR) decoding (Smith and Smith, 2007), and are also proven to be crucial for the important research line of token-level active learning based on partial trees (Li et al., 2016). One probable reason for the less popularity of TreeCRF, despite its usefulness, is due to the complexity and inefficiency of the inside-outside algorithm, especially the outside algorithm. As far as we know, all existing works compute the inside and outside algorithms on CPUs. The inefficiency issue becomes more severe in the DL era, due to 2 Though many recent works report higher performance with extra resources, for example contextualized word represent"
2020.acl-main.302,P16-1218,0,0.0315983,"and fourth-order models show little accuracy improvement probably due to the feature sparseness problem (Koo and Collins, 2010; Ma and Zhao, 2012). 3295 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305 c July 5 - 10, 2020. 2020 Association for Computational Linguistics networks for automatically learning combinations of dozens of atomic features similar to Chen and Manning (2014), and for computing subtree scores. They show that incorporating second-order scores of adjacent-sibling subtrees significantly improved performance. Then, both Wang and Chang (2016) and Kiperwasser and Goldberg (2016) propose to utilize BiLSTM as an encoder and use minimal feature sets for scoring single dependencies in a first-order parser. These three representative works all employ global max-margin training. Dozat and Manning (2017) propose a strong and efficient biaffine parser and obtain state-of-the-art accuracy on a variety of datasets and languages. The biaffine parser is also first-order and employs simpler and more efficient non-structural training via local head selection for each token (Zhang et al., 2017). Observing such contrasting development, we try to m"
2020.acl-main.302,P19-1454,0,0.064613,"ure 3 Computing TreeCRF Loss Efficiently The key to TreeCRF loss is how to efficiently compute log Z(x), as shown in Equation 8. This problem has been well solved long before the DL era for non-neural dependency parsing. Straightforwardly, we can directly extend the viterbi decoding algorithm by replacing max product with sum 4 Another way is to use one extra MLP for sibling representation, and re-use head and modifier representation from the basic first-order components, which however leads to inferior performance in our preliminary experiments. 5 We have also tried the approximate method of Wang et al. (2019), which uses three biaffine operations to simulate the interactions of three input vectors, but observed inferior performance. We omit the results due to the space limitation. product, and naturally obtain log Z(x) in the same polynomial time complexity. However, it is not enough to solely perform the inside algorithm for non-neural parsing, due to the inapplicability of the automatic differentiation mechanism. In order to obtain marginal probabilities and then feature weight gradients, we have to realize the more sophisticated outside algorithm, which is usually at least twice slower than the"
2020.acl-main.302,K18-2001,0,0.0745696,"Missing"
2020.acl-main.302,P19-1562,0,0.348224,"is not enough to solely perform the inside algorithm for non-neural parsing, due to the inapplicability of the automatic differentiation mechanism. In order to obtain marginal probabilities and then feature weight gradients, we have to realize the more sophisticated outside algorithm, which is usually at least twice slower than the inside algorithm. This may be the major reason for the less popularity of TreeCRF (vs. max-margin training) before the DL era. As far as we know, all previous works on neural TreeCRF parsing explicitly implement the insideoutside algorithm for gradient computation (Zhang et al., 2019; Jiang et al., 2018). To improve efficiency, computation is transferred from GPUs to CPUs with Cython programming. This work shows that the inside algorithm can be effectively batchified to fully utilize the power of GPUs. Figure 3 and Algorithm 1 together illustrate the batchified version of the second-order inside algorithm, which is a direct extension of the secondorder Eisner algorithm in McDonald and Pereira (2006) by replacing max product with sum product. We omit the generations of incomplete, complete, and sibling spans in the opposite direction from j to i for brevity. Basically, we"
2020.acl-main.302,L16-1262,0,\N,Missing
2020.acl-main.302,E17-1063,0,\N,Missing
2020.coling-main.183,D16-1070,0,0.0188125,"4; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the noisy pseudo MWS training data. SWS with heterogeneous data. In recent years, there has been a surge of interest in improving SWS with heterogeneous SWS data. The basic idea is improving SWS by utilizing multiple manually labeled SWS data for training at the same time. Representative works include Li et al. (2016a), Chen et al. (2016), He et al. (2018), Chen et al. (2017) and Yang et al. (2017). Although MWS results can be 3 4 https://github.com/fxsjy/jieba http://pullword.com 2034 obtained by merging multiple SWS outputs, but many overlapped words may generated due to the lack of proper constraints, leading to low precision. In this work, we alleviate this issue by considering MWS as a constituent tree parsing problem. 7 Conclusions This work advances the state-of-the-art MWS research from three perspectives. First, we manually annotate over 9,000 sentences for better evaluation, consisting of both canonical NEWS and non-"
2020.coling-main.183,P17-1110,0,0.0139382,"e propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the noisy pseudo MWS training data. SWS with heterogeneous data. In recent years, there has been a surge of interest in improving SWS with heterogeneous SWS data. The basic idea is improving SWS by utilizing multiple manually labeled SWS data for training at the same time. Representative works include Li et al. (2016a), Chen et al. (2016), He et al. (2018), Chen et al. (2017) and Yang et al. (2017). Although MWS results can be 3 4 https://github.com/fxsjy/jieba http://pullword.com 2034 obtained by merging multiple SWS outputs, but many overlapped words may generated due to the lack of proper constraints, leading to low precision. In this work, we alleviate this issue by considering MWS as a constituent tree parsing problem. 7 Conclusions This work advances the state-of-the-art MWS research from three perspectives. First, we manually annotate over 9,000 sentences for better evaluation, consisting of both canonical NEWS and non-canonical BAIKE texts. Second, we empl"
2020.coling-main.183,D16-1001,0,0.305701,"ntly improves the state-of-the-art MWS model by 1.12 on NEWS and by 5.97 on BAIKE in F1. We release all the newly annotated data and the codes at https://github.com/gloria0108/ multi-grained-word-seg. 2 Graph-based Model with Local Loss Given an input sentence, the task of MWS is to retrieve all words of different granularities, which can be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in"
2020.coling-main.183,D17-1072,1,0.64964,"xpert).” It is worth emphasizing that even for the same task or application, words of different granularities can be useful due to its potential complementarity: fine-grained words capture local features and help reduce data sparseness, whereas coarse-grained words reserve more semantics to perform exacter matching and analysis. This facilitates researchers to employ multiple SWS outputs at the same time in information retrieval (IR) (Liu et al., 2008) and machine translation (MT) (Su et al., 2017). Motivated by above perspectives, multi-grained word segmentation (MWS) is formally proposed by Gong et al. (2017) as a useful and challenging direction for research on word segmentation. Given an input sentence, MWS aims to accommodate all words of different granularities with a hierarchical tree structure. Figure 1 (right) presents an example, where “W” means the spanning characters compose a word. In this example, “二 (two)”, “老 (elder)”, “二老 (two elders)”, “都 (both)”, “是 (are)”, “都是 (both are)”, “令 (make)”, “人 (people)”, “令人 (make people)”, “尊敬 (respect)”, “令人尊敬 (respectable)”, “ 的 (of)”, “科学 (science)”, “家 (expert)”, “科学家 (scientist)” are all the words of different granularities. To solve the issue of"
2020.coling-main.183,2020.acl-main.275,0,0.0509741,"Missing"
2020.coling-main.183,P99-1010,0,0.427512,"ranularities to help IR. Su et al. (2017) propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representat"
2020.coling-main.183,P13-1075,0,0.0776526,"Missing"
2020.coling-main.183,P18-1110,0,0.0512991,"larities, which can be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in Section 5.3; 2) 2027 Figure 2: Architecture of our MWS model. more importantly, this work aims to conduct in-depth study on a simple, efficient, and effective way to incorporate weakly labeled data for MWS. The graph-based parser with local loss trains the model directly on individual labeled spans, and thus can acc"
2020.coling-main.183,D16-1072,1,0.853501,") propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the"
2020.coling-main.183,P16-1033,1,0.845428,") propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the"
2020.coling-main.183,D08-1111,0,0.212629,"erogeneity of an example sentence (left) with its MWS tree (right): “二 (two) 老 (elder) 都 (both) 是 (are) 令 (make) 人 (people) 尊敬 (respect) 的 (of) 科学 (science) 家 (expert).” It is worth emphasizing that even for the same task or application, words of different granularities can be useful due to its potential complementarity: fine-grained words capture local features and help reduce data sparseness, whereas coarse-grained words reserve more semantics to perform exacter matching and analysis. This facilitates researchers to employ multiple SWS outputs at the same time in information retrieval (IR) (Liu et al., 2008) and machine translation (MT) (Su et al., 2017). Motivated by above perspectives, multi-grained word segmentation (MWS) is formally proposed by Gong et al. (2017) as a useful and challenging direction for research on word segmentation. Given an input sentence, MWS aims to accommodate all words of different granularities with a hierarchical tree structure. Figure 1 (right) presents an example, where “W” means the spanning characters compose a word. In this example, “二 (two)”, “老 (elder)”, “二老 (two elders)”, “都 (both)”, “是 (are)”, “都是 (both are)”, “令 (make)”, “人 (people)”, “令人 (make people)”, “尊"
2020.coling-main.183,D14-1093,0,0.176118,"accumulating training loss, we only consider spans whose gold-standard labels can be directly determined according to the SWS annotation and overlook others, as illustrated in Figure 3, where spans labeled with “W” correspond to the words in the SWS sentence, those labeled with “NW” are definitely non-words since they overlap with existing gold-standard words (i.e., the words annotated in SWS data), and all blank spans without labels are overlooked with no loss. 3.2 DictEx Data as Weakly Labeled Data The use of naturally annotated data has been extensively studied for SWS (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). The basic idea is to derive word boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in sequence labeling models. 2029 In this work, we propose to obtain naturally annotated data with complete word information from the example sentences in dictionary (rather than only boundaries), which are manually constructed by linguistic experts, e.g., Entry: 最佳 (the best) Example sentence: 1: 找到 最佳 途径 2: 这是 最佳 选择 (find the best way) ( this is the best choice) where two DictEx sentences"
2020.coling-main.183,P14-1028,0,0.633797,"y MWS dataset according to our newly compiled annotation guideline, consisting of over 9,000 sentences from two types of texts, i.e., canonical newswire (NEWS) and non-canonical web (BAIKE) data for better evaluation. Detailed evaluation shows that our proposed model with weakly labeled data significantly outperforms the state-of-the-art MWS model by 1.12 and 5.97 on NEWS and BAIKE data in F1. 1 Introduction As a preliminary but critical processing step for Chinese language processing, word segmentation (WS) has been extensively studied for decades and made great progress (Zheng et al., 2013; Pei et al., 2014; Zhang et al., 2016; Yang et al., 2019; He et al., 2020). However, most of previous works adopt the single-grained word segmentation (SWS) formulation, where a sentence corresponds to a single word sequence according to some pre-defined annotation guidelines. As shown in Figure 1 (left), the SWS annotations of the sentence are different according to the guidelines of Penn Chinese Treebank (CTB) (Xue et al., 2005), the People Daily Corpus of the Peking University (PPD) (Yu et al., 2003), and the Microsoft Research WS Corpus (MSR) (Huang et al., 2006). This is largely due to the fact that the b"
2020.coling-main.183,P10-1037,0,0.0403731,"s to help IR. Su et al. (2017) propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices, leading to improved translation performance. Due to the lack of MWS model, they obtain MWS outputs from several SWS models independently trained on heterogeneous SWS datasets. Utilizing weakly labeled data. The use of weakly labeled data has been an interesting research direction in NLP for a long time. On the one hand, it is usually much easier and cheaper to perform partial annotation than complete annotation, especially for complex tasks such as parsing (Hwa, 1999; Sassano and Kurohashi, 2010; Li et al., 2016b; Joshi et al., 2018). On the other hand, it is sometimes feasible to automatically extract naturally annotated data. Several works utilize naturally annotated data with word boundaries for training SWS models, by making use of markup information such as anchor texts in web pages (Jiang et al., 2013; Liu et al., 2014; Zhao et al., 2018). In this work, we propose two types of weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained"
2020.coling-main.183,P17-1076,0,0.236299,"Local Loss Given an input sentence, the task of MWS is to retrieve all words of different granularities, which can be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in Section 5.3; 2) 2027 Figure 2: Architecture of our MWS model. more importantly, this work aims to conduct in-depth study on a simple, efficient, and effective way to incorporate weakly labeled data for MWS. The graph-based"
2020.coling-main.183,C18-1011,0,0.019846,"be naturally organized as a hierarchical tree structure as shown in Figure 1 (right). Gong et al. (2017) propose several MWS approaches and show that treating MWS as constituent parsing leads to the best performance. They adopt the transition-based parser of Cross and Huang (2016), which greedily searches an optimal shift-reduce action sequence to build a tree. In this work, instead of adopting the transition-based parser as Gong et al. (2017), we employ the graph-based parser of Stern et al. (2017) and replace the original global max-margin loss with local span-wise loss (Joshi et al., 2018; Teng and Zhang, 2018) as our basic MWS model due to two considerations: 1) the graph-based parser with local loss gains more efficiency without hurting the performance compared with the transitionbased parser and the graph-based parser with global loss, which will be discussed in Section 5.3; 2) 2027 Figure 2: Architecture of our MWS model. more importantly, this work aims to conduct in-depth study on a simple, efficient, and effective way to incorporate weakly labeled data for MWS. The graph-based parser with local loss trains the model directly on individual labeled spans, and thus can accommodate weakly labeled"
2020.coling-main.183,P16-1218,0,0.0234966,"n (Pei et al., 2014), we use the concatenation of single character embeddings embci and bigram character embeddings embci−1 ci as the input. xi = embci ⊕ embci−1 ci (1) The encoding layer uses two layers of BiLSTM to encode the sentence and produce contextualized representations. We use fi and bi to denote the hidden vector of the top-layer forward and backward LSTMs for the i-th position. The span representation layer constructs a dense representation vector for each possible span ci ...cj−1 denoted as (i, j): ri,j = (fj − fi ) ⊕ (bi − bj ) (2) which is also known as the LSTM-minus features (Wang and Chang, 2016; Cross and Huang, 2016). The classification layer uses an MLP to compute the labeling scores of each span. oi,j = W2 ReLU(W1 ri,j + b1 ) + b2 (3) where W1 , W2 , b1 , and b2 are parameters. In our task, the dimension of oi,j is 2, w.r.t “W” and “NW” respectively (oi,j [0] is the score of labeling span (i, j) as a word, and oi,j [1] as a non-word). 2.2 Training with Local Span-wise Loss During training, we compute a local cross-entropy loss value for each span, and accumulate all loss values of all possible spans in the input sentence. ∗ eoi,j [yi,j ] L=− log o [0] e i,j + eoi,j [1] 0≤i<j≤n X"
2020.coling-main.183,P17-1078,0,0.138669,"weakly labeled data for MWS, i.e., SWS data and naturally annotated data from DictEx sentences, which are shown to be complementary and able to alleviate the under-representation problem of multi-grained phenomena in the noisy pseudo MWS training data. SWS with heterogeneous data. In recent years, there has been a surge of interest in improving SWS with heterogeneous SWS data. The basic idea is improving SWS by utilizing multiple manually labeled SWS data for training at the same time. Representative works include Li et al. (2016a), Chen et al. (2016), He et al. (2018), Chen et al. (2017) and Yang et al. (2017). Although MWS results can be 3 4 https://github.com/fxsjy/jieba http://pullword.com 2034 obtained by merging multiple SWS outputs, but many overlapped words may generated due to the lack of proper constraints, leading to low precision. In this work, we alleviate this issue by considering MWS as a constituent tree parsing problem. 7 Conclusions This work advances the state-of-the-art MWS research from three perspectives. First, we manually annotate over 9,000 sentences for better evaluation, consisting of both canonical NEWS and non-canonical BAIKE texts. Second, we employ a simple graph-based"
2020.coling-main.183,N19-1278,0,0.0224724,"Missing"
2020.coling-main.183,P16-1040,0,0.0283266,"Missing"
2020.coling-main.183,D13-1061,0,0.153603,"Missing"
2020.coling-main.266,C18-1233,0,0.0180067,"kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018)"
2020.coling-main.266,W05-0620,0,0.133682,"Missing"
2020.coling-main.266,N19-1423,0,0.0299973,"cit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing valuable syntactic position infor"
2020.coling-main.266,Q19-1019,0,0.0241109,"racting syntactic knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as th"
2020.coling-main.266,P17-1044,0,0.505726,"ct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata"
2020.coling-main.266,P18-2058,0,0.091085,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,P18-1192,0,0.575347,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,D19-1538,0,0.183322,"cate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic"
2020.coling-main.266,2020.acl-main.744,0,0.187854,"tperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. He et al. (2018a) He et al. (2018a) (w/ ELMo) Baseline HybridHDP Baseline(w/ RoBERTa) HybridHDP (w/ RoBERTa) WSJ Brown P R F1 P R F1 P R F1 88.0 87.24 82.28 83.65 86.87 86.99 86.9 87.26 82.76 84.06 87.89 87.41 87.4 87.25 82.52 83.85 87.38 87.20 89.2 88.05 84.21 85.12 88.11 88.43 87.9 88.00 84.39 85.0 88.64 88.75 83.9 87.4 88.5 88.03 84.30 85.06 88.37 88.59 81.0 80.04 74.37 76.3 82.49 83.05 78.4 79.56 73.59 75.42 83.51 83.28 73.7 80.4 79.6 79.80 73.98 75.86 83.00 83.16 81.53 82.95 85.81 85.93 82.15"
2020.coling-main.266,K17-1041,0,0.102309,"s because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path em"
2020.coling-main.266,H94-1020,0,0.129263,"019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata (2016) propose to use dependency-based embeddings in a neural SRL model for dependency-based SRL. He et al. (2018b) introduce k-order pruning algorithm to prune arguments according to dependency trees. However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994). Our work follows the syntax-aware approach and enhances SRL with heterogeneous syntactic knowledge. We define heterogeneous syntactic treebanks as treebanks that follow different annotation guidelines. All is well known, there exist many published dependency treebanks that follow different annotation guidelines, i.e., English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Internationa"
2020.coling-main.266,D18-1191,0,0.170603,"ate-argument structures. 4.3 Results and Analyses on English SRL Table 5 shows our results on English CoNLL-2005 development and test data, where WSJ is the indomain data and Brown is the out-of-domain data. Our implemented baseline model achieves slightly higher performance than the model (He et al., 2018a) we follow. The proposed methods can further improve our baseline model by +0.76 (p &lt; 1e-4) and +1.88 (p &lt; 1e-4) F1 scores on WSJ and Brown test data, respectively. With the help of RoBERTa representations, our full model achieves 88.59 F1 score on the test WSJ data, slightly outperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) B"
2020.coling-main.266,D14-1162,0,0.0850612,"Missing"
2020.coling-main.266,N18-1202,0,0.0261584,"rspective of explicit and implicit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing va"
2020.coling-main.266,P16-1113,0,0.444205,"tic knowledge brings significant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches"
2020.coling-main.266,D16-1212,0,0.0244291,"commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 7"
2020.coling-main.266,silveira-etal-2014-gold,0,0.054242,"Missing"
2020.coling-main.266,D18-1548,0,0.200322,"ents over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syn"
2020.coling-main.266,P17-1189,0,0.0185216,"ese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 79.01 83.28 86.22 8"
2020.coling-main.266,D19-1541,1,0.823683,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,N19-1075,0,0.556792,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,J08-2004,0,0.0450443,"pendency data completes the forward process. 3.3 Hybrid HDP Our model combines the two representations together, according to our intuition that explicit and implicit syntactic representations are highly complementary, which is denoted as “HybridHDP” (Hybrid Heterogeneous Dependency Parsing) in later sections. In detail, we concatenate the two heterogeneous l isr syntactic representations with the SRL input, formulated as xi = embword ⊕ repchar wi wi ⊕ hv ⊕ hi . 4 Experiments and Analysis 4.1 Experimental Setup We conduct experiments on the commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et a"
2020.coling-main.266,C18-1047,0,0.0252679,"yer would be modified as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP mod"
2020.coling-main.266,D19-1057,1,0.801133,"propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic knowledge. Our work focus on exploiting heterogeneous dependency benchmarks and the results verify our intuition that heterogeneous syntactic knowledge can provide more valid information. 6 Conclusion We propose to encode heterogeneous syntactic knowledge with explicit and implicit methods to help SRL. For the explicit aspect, we propose ExpHD"
2020.coling-main.266,2020.acl-main.297,1,0.824494,"knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as the input of the GCN mo"
2020.coling-main.266,P15-1109,0,0.0199839,"apparently no because integrating syntactic knowledge into pre-trained language models has attracted some attention (Wang et al., 2020). And unitizing heterogeneous syntactic knowledge would be a direct and natural idea, which we leave for future work. 5 Related Work Recently, SRL has achieved significant improvements because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the"
2020.coling-main.338,N19-1009,0,0.0285733,"STM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-domain dependency parsin"
2020.coling-main.338,P16-1231,0,0.0273406,"adiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve the state-of-the-art accuracy on a variety of datasets and languages. However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the left sentence from the product comment (PC) domain i"
2020.coling-main.338,W17-4712,0,0.0244698,"y the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representati"
2020.coling-main.338,D18-1017,0,0.0265257,"n neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further"
2020.coling-main.338,D14-1082,0,0.11834,"ration (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve the state-of-the-art accuracy on a variety of datasets and languages. However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the"
2020.coling-main.338,W03-0407,0,0.0179799,"main adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data (Yarowsky, 1995). As a typical unsupervised approach, self-training has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-training is helpful for unsupervised cross-domain parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training. 5.2 Semi-supervised Domain Adaptation Semi-super"
2020.coling-main.338,D18-1217,0,0.0483108,"enate hinv and hspe as the final contextualized word representation hi , which is used i i for dependency parsing by shared MLP and biaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations. First, we use the released Chinese BERT-Based model as the original BERT model"
2020.coling-main.338,P07-1033,0,0.583474,"Missing"
2020.coling-main.338,N19-1423,0,0.0316635,"iaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations. First, we use the released Chinese BERT-Based model as the original BERT model.2 Then, we finetune BERT on the unlabeled data using the parameters in the original BERT model as the starting point. To save computat"
2020.coling-main.338,P81-1022,0,0.668093,"Missing"
2020.coling-main.338,D18-1498,0,0.0766046,"which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the perfo"
2020.coling-main.338,2020.acl-main.740,0,0.0200745,"mized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain more reliable representations. First, we use the released Chinese BERT-Based model as the original BERT model.2 Then, we finetune BERT on the unlabeled data using the parameters in the original BERT model as the starting point. To save computational resource, we merge all train/unlabeled data of all domains as one unlabeled dataset for fine-tuning BERT once. Thus, the same fine-tuned BERT model is used for all th"
2020.coling-main.338,C16-1038,0,0.613212,"kar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed by Daum´e III (2007). Kim et al. (2016) successfully apply it to a neural model which leverages multiple BiLSTMs to extract shared and private domain features. To learn the differences and commonalities between source and target domains, the DE method uses explicit domain indicators as extra inputs, whereas the FA method employs a shared and two private BiLSTM encoders for the feature separation. This work proposes to improve the contextualized word representation by adversarial learning and fine-tuning BERT, thus further modeling more pure yet effective domain-specific and domain-invariant representations. To alleviate the domain-"
2020.coling-main.338,P17-1119,0,0.0224601,"successfully employ the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effecti"
2020.coling-main.338,Q16-1023,0,0.0443867,"019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016; Andor et al., 2016; Dozat and Manning, 2017). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve the state-of-the-art accuracy on a variety of datasets and languages. However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data. Taking the examples in Figure 1, we can see that as user-generated texts, the left sentence from the product c"
2020.coling-main.338,P19-1229,1,0.638182,"sion structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed by Daum´e III (2007). Kim et al. (2016) successfully apply it to a neural model which leverages multiple BiLSTMs to extract shared and private domain features. To learn the differences and commonalities between source and target domains, the DE method uses explicit domain indicators as extra i"
2020.coling-main.338,P08-2026,0,0.052112,"$ $ 会议 Meeting 在 in 北京 Beijing punc 举行 was held 。 . Figure 1: Examples of dependency trees. The left sentence is from the target-domain PC data and the right one is from the source-domain BC data. differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, i"
2020.coling-main.338,N06-1020,0,0.136708,"e approaches for both unsupervised and semi-supervised domain adaptation. 5.1 Unsupervised Domain Adaptation Due to the lack of target-domain labeled data, previous researches mostly focus on the unsupervised domain adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data (Yarowsky, 1995). As a typical unsupervised approach, self-training has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-traini"
2020.coling-main.338,H05-1066,0,0.157985,"head word, and rDi as a dependent, and MLPH/D both have a single hidden layer with the ReLU activation function. BiAffine layer. The scores of all dependencies are computed via a BiAffine operation,  score(i ← j) = rDi 1 T Wb rHj (4) where score(i ← j) is the score of the dependency (j, i) and the matrix Wb is a BiAffine parameter. The arc-factorization score of a dependency tree is computed with extra MLPs, which can be seen in Dozat and Manning (2017). After obtaining the scores, the highest-scoring tree can be decoded with the dynamic programming algorithm known as maximum spanning tree (McDonald et al., 2005). Parser loss. Assuming wj is the gold-standard head of wi , the BiAffine parser loss for each position i is escore(i←j) P Lparser = − log (5) escore(i←k) 0≤k≤n,k6=i The BiAffine parser treats the classification of dependency labels as a separate task after finding the highest-scoring dependency tree. 3808 3 Approaches In this work, we propose to improve contextualized word representations by adversarial learning and fine-tuning BERT processes to boost the performance of cross-domain dependency parsing. Concretely, we apply adversarial learning to three typical semi-supervised approaches with"
2020.coling-main.338,P08-1108,0,0.034203,"in parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training. 5.2 Semi-supervised Domain Adaptation Semi-supervised domain adaptation assumes the model is trained with all source- and target-domain labeled data. Most recently, Li et al. (2019c) and Yu et al. (2019) reveal that newly generated targetdomain data by self-training or tri-training and model ensemble can improve the cross-domain parsing performance significantly. The model ensemble method is a commonly used strategy to integrate different parsing models in dependency parsing (Nivre and McDonald, 2008). However, all these approaches require to retrain parser repeatedly, making them difficult for practical applications. Daum´e III (2007) for the first time proposes the FA method on sequence labeling task, which distinguishes domain-specific and domain-invariant with different feature extractors. Kim et al. (2016) successfully employ the FA technique on neural network, which uses a shared and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find"
2020.coling-main.338,N18-1202,0,0.0540639,"129 1,300 2,600 291,481 ZX 1,645 500 1,100 33,792 Table 1: Data statistics in sentence number 0 Then, we concatenate hinv and hspe as the final contextualized word representation hi , which is used i i for dependency parsing by shared MLP and biaffine operations. In addition, the orthogonality loss is used to divergent the domain-specific and domain-invariant representations. Finally, the entire model is optimized by a joint loss, which is the same defined as L∗f a . 3.4 Fine-tuning BERT with All Target-domain Unlabeled Data Recently proposed contextualized word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) can further improve parsing performance by a large margin (Clark et al., 2018; Li et al., 2019a). Remarkably, BERT has been proven effective on a variety of natural language processing tasks (Devlin et al., 2019). Recently, researchers pay more attention to updating BERT representations with additional corpus and achieve great progress on BERT applications (Gururangan et al., 2020). Motivated by the successful utilization of BERT and BERT’s strong capability of word representations, we propose to fine-tune BERT model parameters with all unlabeled data to obtain"
2020.coling-main.338,N01-1023,0,0.860666,"xamples of dependency trees. The left sentence is from the target-domain PC data and the right one is from the source-domain BC data. differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed by Daum´e III (2007). Kim et al."
2020.coling-main.338,K17-3007,0,0.0449532,"s another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-domain dependency parsing. 6 Conclusions This work successful"
2020.coling-main.338,E03-1008,0,0.192371,"main constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly labeled data when multiple learners agree on the annotation labels. Sarkar (2001) and Steedman et al. (2003) demonstrate that co-training is helpful for unsupervised cross-domain parsing. However, it still is a challenge to select the appropriate labeled data for self-training and co-training. 5.2 Semi-supervised Domain Adaptation Semi-supervised domain adaptation assumes the model is trained with all source- and target-domain labeled data. Most recently, Li et al. (2019c) and Yu et al. (2019) reveal that newly generated targetdomain data by self-training or tri-training and model ensemble can improve the cross-domain parsing performance significantly. The model ensemble method is a commonly used st"
2020.coling-main.338,N19-1075,0,0.0255217,"ns that benefit for the cross-domain dependency parsing. Experiments on a benchmark dataset show that our proposed adversarial approaches achieve consistent improvements, and fine-tuning BERT further boosts the parsing accuracy by a large margin. Our single model achieves the same state-of-the-art performance as the top submitted system in the NLPCC-2019 shared task, which uses ensemble models and BERT. 1 Introduction Dependency parsing aims to capture syntax with a dependency tree and is proven to be helpful for various natural language processing (NLP) tasks, such as semantic role labeling (Xia et al., 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017). Given an input sentence s = w1 w2 . . . wn , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a dependency from the head word wh to the child word wm with the relation label l ∈ L, and w0 is a pseudo word that points to the root word of the sentence. In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin"
2020.coling-main.338,P95-1026,0,0.749191,"Domain adaptation has been a long-standing yet challenging research topic. Here we try to briefly summarize the representative approaches for both unsupervised and semi-supervised domain adaptation. 5.1 Unsupervised Domain Adaptation Due to the lack of target-domain labeled data, previous researches mostly focus on the unsupervised domain adaptation. Self-training is a simple method to incorporate unlabeled data into the new model, which first annotates the unlabeled data with the existing model, and then train a new model with the combination of newly generated data and actual labeled data (Yarowsky, 1995). As a typical unsupervised approach, self-training has proven effective on cross-domain constituency parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015), but there are also many failed works. Charniak (1997) reports either minor improvements or significant damage for parsing by using self-training. Clark et al. (2003) show the same findings on POS-tagging task. Co-training is another way to utilize the unlabeled data (Blum and Mitchell, 1998). It leverages multiple learners to annotate the unlabeled data respectively, and then arguments the training data with the newly la"
2020.coling-main.338,W15-2201,0,0.61471,"g punc 举行 was held 。 . Figure 1: Examples of dependency trees. The left sentence is from the target-domain PC data and the right one is from the source-domain BC data. differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc. The key for domain adaptation is how to model differences and commonalities between different domains. Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data. Typical methods include self-training (McClosky and Charniak, 2008; Yu et al., 2015) and co-training (Sarkar, 2001). However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow. In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emergence of more labeled data. Particularly, Li et al. (2019b) release large-scale labeled and unlabeled datasets, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method. The feature augmentation (FA) method, as another typical technique for semi-supervised domain adaptation, is first proposed b"
2020.coling-main.338,D18-1041,0,0.0444858,"d and m private BiLSTM encoders for feature separation. As another direction, Li et al. (2019b) propose to utilize an extra domain embedding to indicate the domain information of the input word, and they find that the parsing accuracy of the DE model is obviously higher than other semi-supervised approaches. The adversarial learning is a commonly used strategy to extract pure domain-invariant representations that does not belong to a particular domain as much as possible (Goodfellow et al., 2014; Bousmalis et al., 2016; Kim et al., 2017; Britz et al., 2017; Cao et al., 2018; Guo et al., 2018; Zeng et al., 2018; Adams et al., 2019). Most relevantly, Sato et al. (2017) employ adversarial network to the FA and CON methods, finding that there is little gains and even damage the performance, specially when the scale of target-domain labeled training data is small. Motivated by these works, we apply adversarial learning on 3814 three typical semi-supervised domain adaptation, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representation and orthogonality constraints to detect more pure yet effective word representations, thus further boosting the performance of cross-dom"
2021.acl-long.452,D14-1082,0,0.0607765,"iani and Titov, 2017; Zhang et al., 2018). We follow the implementation of Zhang et al. (2018) and use a two-layer GCN as a more sophisticated way. In order to utilize labels, we extend vanilla GCN to have the same input with LabelCharLSTM, i.e., zk . We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN. 6.2 Experiments The basic parser applys a BiLSTM over character sequence to obtain word representation. In this part, we propose two simple alternative methods to encode internal structure shown in Figure 1-(c). Settings. Following Chen and Manning (2014), we conduct experiments on CTB5 with the same data split (16,091/803/1,910 sentences) and constituent-to-dependency conversion. Both char/label embeddings are randomly initialized and have the same dimension of 50. For the parsers using gold-standard POS tags, we randomly initialized the POS tagging embeddings and set the dimension to 50. For other hyperparameters, we adopt the default configuration of SuPar, including the pre-trained word embeddings. For multi-char words without annotated internal structure, we use the automatic outputs from the trained parser with BERT in Section 5, so that"
2021.acl-long.452,cheng-etal-2014-parsing,0,0.0464199,"Missing"
2021.acl-long.452,P15-2043,0,0.025891,"tribution measurement. Instead of treating a word as a bag of characters, we experiment with two simple ways to obtain structure-aware word representations. Meanwhile, enhancing their approach with explicit wordinternal structure could be also very interesting. Utilizing word-internal structure. Wordinternal structure have been explored in various NLP tasks. Several works propose to learn wordinternal structure, word segmentation, POS tagging and parsing jointly (Zhang et al., 2013, 2014; Li et al., 2018), demonstrating the effectiveness of word-internal structure in helping downstream tasks. Cheng et al. (2015) attempt to convert words into fine-grained subwords according to the 2 Following this direction, studies tried to explore more character information for better Chinese word representation, such as strokes (Cao et al., 2018) and ideographic shape (Sun et al., 2019). 5825 internal structure of words for better dealing with unknown words during word segmentation. Lin et al. (2020) propose to integrate the representation of word-internal structure into the input of neural machine translation model, leading to improved translation performance. 3 Word-internal Structure Annotation In this section,"
2021.acl-long.452,Q16-1026,0,0.0418986,"y, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition (Chiu and Nichols, 2016), dependency parsing (Zhang et al., 2020), and constituent parsing (Gaddy et al., 2018). Another research direction is to leverage character information to obtain better word embeddings. Chen et al. (2015) extended the CBOW model and proposed to jointly learn character and word embeddings. Based on Chen et al. (2015), Yu et al. (2017) proposed to jointly learn embeddings of words, characters, and sub-characters.2 However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. To address this, Xu et al. (2016) extended"
2021.acl-long.452,N19-1423,0,0.0943148,"LabelGCN, and show that using the resulting word representation leads to promising gains on the dependency parsing task. We release WIST at https://github.com/ SUDA-LA/ACL2021-wist, and also provide a demo to parse the internal structure of any input word. 2 Related Work Annotating word-internal structure. In the deep learning (DL) era, pretraining techniques are extremely powerful in handling large-scale unlabeled data, including Skip-Gram or CBOW models (Mikolov et al., 2013) for learning contextindependent word embedding in the beginning, and the recent ELMo (Peters et al., 2018) or BERT (Devlin et al., 2019) for learning context-aware word representations. Conversely, in the pre-DL era, there exist few (if any) effective methods for utilizing unlabeled data, and statistical models rely on discrete one-hot features, leading to severe data sparseness for many NLP tasks. This directly motivates annotation of word-internal structure, especially for dealing with rare words. Annotation of shallow internal structure of Chinese words was first mentioned in Zhao (2009), largely based on heuristic rules. Li (2011); Li and Zhou (2012) found that many multi-char words could be divided into two subwords, i.e."
2021.acl-long.452,N18-1091,0,0.0439491,"Missing"
2021.acl-long.452,D17-1072,1,0.796539,"∗ Chen Gong and Saihao Huang make equal contributions to this work. Zhenghua is the corresponding author. words are the minimal units that express a complete semantic concept or play a grammatical role independently (Xia, 2009; Yu et al., 2003).1 Roles played by characters in word formation can be divided into three types. (1) There is a stable and important set of single-char words, such as “你” (you)”, “的” (of), and most punctuation marks. (2) A character having no specific meaning acts as a part of a single-morpheme word, such as “仿 1 There is still a dispute on the word granularity issue (Gong et al., 2017; Lai et al., 2021). Words are defined as a character sequence that is in tight and steady combination. However, the combination intensity is usually yet vaguely qualified according to co-occurrence frequency. We believe this work may also be potentially useful to this direction. 5823 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5823–5833 August 1–6, 2021. ©2021 Association for Computational Linguistics 佛” (like) and “法(fˇa)老(lˇao)” (Pharaoh, transliteration of foreign w"
2021.acl-long.452,W19-7726,0,0.0686994,"each character, and then determine an unlabeled dependency tree, and finally use a POS tag triple as arc label, corresponding to the POS tags of the modifier/head characters and the whole word. However, we argue POS tag triples are only loosely related with word-formation patterns, not to mention the severe difficulty of annotating char-level POS tags in each word. Recently, Lin et al. (2020) extended Zhang et al. (2014) by using an extra label for marking singlemorpheme words, and annotated hierarchical internal structure of 53K words from a Chinese-English machine translation (MT) dataset. Li et al. (2019a) annotated the internal structure of words with 4 dependency relations. In summary, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word,"
2021.acl-long.452,P19-1229,1,0.923771,"each character, and then determine an unlabeled dependency tree, and finally use a POS tag triple as arc label, corresponding to the POS tags of the modifier/head characters and the whole word. However, we argue POS tag triples are only loosely related with word-formation patterns, not to mention the severe difficulty of annotating char-level POS tags in each word. Recently, Lin et al. (2020) extended Zhang et al. (2014) by using an extra label for marking singlemorpheme words, and annotated hierarchical internal structure of 53K words from a Chinese-English machine translation (MT) dataset. Li et al. (2019a) annotated the internal structure of words with 4 dependency relations. In summary, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word,"
2021.acl-long.452,P11-1141,0,0.0444259,"Missing"
2021.acl-long.452,D12-1132,0,0.0689367,"Missing"
2021.acl-long.452,I17-1007,0,0.0558565,"Missing"
2021.acl-long.452,P18-1130,0,0.0486473,"Missing"
2021.acl-long.452,D17-1159,0,0.0305739,"racter of wi . The final word representation from CharLSTM(wi ) is obtained by concatenating two last-timestamp hidden output vectors of a one-layer BiLSTM. LabelCharLSTM Method. Considering that the word is usually very short and a bare label itself provides rich syntax information, we propose a straightforward extension to CharLSTM, named as LabelCharLSTM, via minor modification. zk = emb(ci,k ) ⊕ emb(li,k ) (3) where li,k represents the label between ci,k and its head in the word-internal structure. LabelGCN method. Previous work show that GCN is very effective in encoding syntactic trees (Marcheggiani and Titov, 2017; Zhang et al., 2018). We follow the implementation of Zhang et al. (2018) and use a two-layer GCN as a more sophisticated way. In order to utilize labels, we extend vanilla GCN to have the same input with LabelCharLSTM, i.e., zk . We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN. 6.2 Experiments The basic parser applys a BiLSTM over character sequence to obtain word representation. In this part, we propose two simple alternative methods to encode internal structure shown in Figure 1-(c). Settings. Following Chen and Manning (20"
2021.acl-long.452,N18-1202,0,0.0749173,"Missing"
2021.acl-long.452,N19-1277,0,0.0443256,"Missing"
2021.acl-long.452,N16-1119,0,0.173699,"ze of characters is much smaller than that of words. In fact, many NLP researchers have tried to utilize charlevel word-internal structures for better Chinese understanding. Most related to ours, previous studies on syntactic parsing have proposed to annotate word-internal structures to alleviate the data sparseness problem (Zhang et al., 2014; Li et al., 2018). However, their annotations mainly consider flat and shallow word-internal structure, as shown in Figure 1-(a) and (b). Meanwhile, researchers try to make use of character information to learn better word embeddings (Chen et al., 2015; Xu et al., 2016). Without explicitly capturing word-internal structures, these studies have to treat a word as a bag of characters. See Section 2 for more discussion. This paper presents an in-depth study on charlevel internal structure of Chinese words. We endeavour to address three questions. (1) What are the word-formation patterns for Chinese words? (2) Can we train a model to predict deep word-internal structures? (3) Is modeling word-internal structures beneficial for word representation learning? For the first question, we propose to use labeled dependency trees to represent word-internal structures, a"
2021.acl-long.452,D17-1027,0,0.023379,"d representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition (Chiu and Nichols, 2016), dependency parsing (Zhang et al., 2020), and constituent parsing (Gaddy et al., 2018). Another research direction is to leverage character information to obtain better word embeddings. Chen et al. (2015) extended the CBOW model and proposed to jointly learn character and word embeddings. Based on Chen et al. (2015), Yu et al. (2017) proposed to jointly learn embeddings of words, characters, and sub-characters.2 However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. To address this, Xu et al. (2016) extended Chen et al. (2015) and proposed a cross-lingual approach to distinguish contribution of characters for a word. The idea is to translate Chinese words and characters into English words, and use similarities between corresponding English word embeddings for contribution measurement. Instead of treating a word as a bag of characters, we"
2021.acl-long.452,P13-1013,0,0.017553,"ranslate Chinese words and characters into English words, and use similarities between corresponding English word embeddings for contribution measurement. Instead of treating a word as a bag of characters, we experiment with two simple ways to obtain structure-aware word representations. Meanwhile, enhancing their approach with explicit wordinternal structure could be also very interesting. Utilizing word-internal structure. Wordinternal structure have been explored in various NLP tasks. Several works propose to learn wordinternal structure, word segmentation, POS tagging and parsing jointly (Zhang et al., 2013, 2014; Li et al., 2018), demonstrating the effectiveness of word-internal structure in helping downstream tasks. Cheng et al. (2015) attempt to convert words into fine-grained subwords according to the 2 Following this direction, studies tried to explore more character information for better Chinese word representation, such as strokes (Cao et al., 2018) and ideographic shape (Sun et al., 2019). 5825 internal structure of words for better dealing with unknown words during word segmentation. Lin et al. (2020) propose to integrate the representation of word-internal structure into the input of"
2021.acl-long.452,P14-1125,0,0.138989,"ed by two annotators and inconsistencies are handled by a third senior annotator. Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation. Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser. Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task. 1 coordinate right left left 想 方 设 think plan design coordinate coordinate 法 婚 姻 method marriage marriage 老 法 法 law Pharaoh (a) Zhang et al. (2014): labels mark head positions. root root root v-v-v v-v-n v-v-n 想 方 设 think plan design 法 n-n-n 婚 n-n-n n-n-n 姻 method marriage marriage 老 法 法 law Pharaoh (b) Li et al. (2018): labels correspond to POS tag triples. root coo obj att coo obj 想 方 设 think plan design 法 婚 姻 method marriage marriage root root frag 法 法 law 老 Pharaoh (c) Ours: fine-grained structure with 11 labels. Figure 1: Three example words with internal structure under different annotation paradigms. “想(think of) 方(plan) 设(design) 法(method)” is a verb and means “find ways or means to do”. “婚(marriage) 姻(marriage) 法(law)” is a noun"
2021.acl-long.452,2020.acl-main.302,1,0.927215,"ted quite shallow hierarchical structure. In contrast, this work presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure 1-(c). Leveraging character information for better word representation. It has already become a standard way in many NLP tasks to obtain charaware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition (Chiu and Nichols, 2016), dependency parsing (Zhang et al., 2020), and constituent parsing (Gaddy et al., 2018). Another research direction is to leverage character information to obtain better word embeddings. Chen et al. (2015) extended the CBOW model and proposed to jointly learn character and word embeddings. Based on Chen et al. (2015), Yu et al. (2017) proposed to jointly learn embeddings of words, characters, and sub-characters.2 However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. To address this, Xu et al. (2016) extended Chen et al. (2015) and proposed a cross-l"
2021.acl-long.452,D18-1244,0,0.0255135,"representation from CharLSTM(wi ) is obtained by concatenating two last-timestamp hidden output vectors of a one-layer BiLSTM. LabelCharLSTM Method. Considering that the word is usually very short and a bare label itself provides rich syntax information, we propose a straightforward extension to CharLSTM, named as LabelCharLSTM, via minor modification. zk = emb(ci,k ) ⊕ emb(li,k ) (3) where li,k represents the label between ci,k and its head in the word-internal structure. LabelGCN method. Previous work show that GCN is very effective in encoding syntactic trees (Marcheggiani and Titov, 2017; Zhang et al., 2018). We follow the implementation of Zhang et al. (2018) and use a two-layer GCN as a more sophisticated way. In order to utilize labels, we extend vanilla GCN to have the same input with LabelCharLSTM, i.e., zk . We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN. 6.2 Experiments The basic parser applys a BiLSTM over character sequence to obtain word representation. In this part, we propose two simple alternative methods to encode internal structure shown in Figure 1-(c). Settings. Following Chen and Manning (2014), we conduct exper"
2021.acl-long.452,E09-1100,0,0.0874791,"Missing"
2021.ccl-1.48,Y96-1018,0,0.306289,"Missing"
2021.ccl-1.48,W16-1702,0,0.0686791,"Missing"
2021.ccl-1.48,P16-1033,1,0.717036,"Missing"
2021.ccl-1.48,J93-2004,0,0.0825203,"Missing"
2021.ccl-1.48,P13-2017,0,0.0894733,"Missing"
2021.ccl-1.48,W03-1507,0,0.320236,"Missing"
2021.ccl-1.48,xia-etal-2000-developing,0,0.717715,"Missing"
2021.ccl-1.48,P19-1426,0,0.0200915,"Missing"
2021.conll-1.23,P05-1022,0,0.363163,"inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond"
2021.conll-1.23,N06-1022,0,0.0673875,"nson (2005) succeed in reranking n-best to aggregate the outputs of three independently parses with a two-stage method. They predict n- trained models for the three tasks. Zhang et al. best parses with a coarse-grained grammar in the (2013) integrate word segmentation and POS tagfirst stage, then, the best parse is selected from the ging information into parse trees, and then use a n-best parses with the finer-grained second-stage transition-based parser to perform joint learning grammar which makes use of more important con- and decoding. They manually annotated the intratextual information. Charniak et al. (2006) extend structures of words on the CTB5 dataset, which is the basic two-stage coarse-to-fine parsing to multi- proven to be very beneficial. 297 Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter- and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on jo"
2021.conll-1.23,P97-1003,0,0.407594,"eraction during inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on product"
2021.conll-1.23,N19-1423,0,0.00635085,"for joint WS-POS-PAR, which can deal with both challenges. In the coarse labeling stage, the joint model outputs a bracketed tree with coarse labels (i.e., phrase, subphrase, word, subword). The constrained CKY algorithm is used to guarantee that the predicted tree contains no illegal production rules. In the fine labeling stage, the model expands each coarse label into a final fine-grained label (such as VP, VP∗ , VV, VV∗ ). Experiments on three Chinese Treebank (CTB) benchmark datasets show the joint framework is superior to the pipeline framework on both settings of without and with BERT (Devlin et al., 2019), and achieves new state-of-the-art performance. We will release our code at https://github.com/ ironsword666/JointParser. 2 2.1 Joint WS-POS-PAR as Char-Level Tree Parsing From Word-level Tree to Char-level Tree As illustrated in Figure 1, a word-level constituent tree with POS tags in 1(a) is converted into a charlevel tree in 1(b). In a word-level tree, a leaf node corresponds to a word with its POS tag. In contrast, in a char-level tree, a leaf node is always a character, and POS tags become non-terminal nodes. We use flat structures for multi-char words, such as “NN1,3 → 巧 克 力”. IP IP ∗ P"
2021.conll-1.23,N18-1091,0,0.0291423,"Missing"
2021.conll-1.23,I11-1136,0,0.0793158,"Missing"
2021.conll-1.23,P13-2110,0,0.0270986,"he model expands each coarse label into a final label (such as VP, VP∗ , VV, VV∗ ). Experiments on Chinese Penn Treebank 5.1 and 7.0 show that our joint model consistently outperforms the pipeline approach on both settings of without and with BERT, and achieves new state-of-the-art performance. IP IP NP ADVP VP PU VA AD NN 。 美味 很 巧克力 Chocolate1 Very2 Delicious3 .4 (a) Word-level tree NN PU VP NP VP ADVP VP AD VA 味 。 美 很 力 克 巧 Cho-1 -co-2 -late3 Very4 Beautiful5 Taste6 .7 (b) Char-level tree Figure 1: Example WS-POS-PAR trees. The English translation is “The chocolate is very delicious.” 2013; Wang et al., 2013). The motivations are three-fold: 1) alleviating error propagation, 2) promoting knowledge sharing and interaction during inference, 3) simplifying system architecture by training a single model. In contrast, there have been few work on joint WS-POS-PAR in the DL era, with the exception of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, const"
2021.conll-1.23,P12-1110,0,0.0197029,"t parses with a coarse-grained grammar in the (2013) integrate word segmentation and POS tagfirst stage, then, the best parse is selected from the ging information into parse trees, and then use a n-best parses with the finer-grained second-stage transition-based parser to perform joint learning grammar which makes use of more important con- and decoding. They manually annotated the intratextual information. Charniak et al. (2006) extend structures of words on the CTB5 dataset, which is the basic two-stage coarse-to-fine parsing to multi- proven to be very beneficial. 297 Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter- and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can brin"
2021.conll-1.23,P82-1020,0,0.778662,"Missing"
2021.conll-1.23,P18-1249,0,0.0844041,"vel parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there era, there had been intensive research interest in exist two severe challenges. (1) The first c"
2021.conll-1.23,P17-1111,0,0.0169357,"al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can bring considerable gains to parsing performance. Compared with their work, we simply use left binarization to decide the intra-word structures, and our model achieves much higher performance by adopting the state-of-the-art BiLSTM-based parsing model. Our main contribution is proposing an elegant way to handle word-vs-phrase conflicts, which unfortunately are not mentioned in their work. Kurita et al. (2017) for the first time apply neural networks to jont WS-POS-DepPAR. They enhance the transition-based parser with char/word embeddings and BiLSTM which alleviate the efforts of feature engineering. Li et al. (2018) annotate a character-level dependency treebank for joint WS-POS-DepPAR. They use a neural characterlevel transition-based parsing model to reduce computational cost. Yan et al. (2020) focus on joint WS-DepPAR. They adopt the character-level graphbased parsing approach. For intra-word dependencies, a character is designed to modify its subsequent character with the label of “app”. Wu an"
2021.conll-1.23,N16-1030,0,0.0296863,"ramework We adopt the typical cascaded pipeline framework as our baseline. In the training phase, three separate models (WS/POS/PAR) are separately trained. In the evaluation phase, the first step is word segmentation; then the word sequence is fed into the POS tagger; finally the word sequence is fed into the constituent parser. Constituent parsing. We directly adopt the competitive two-stage CRF parser of Zhang et al. (2020), which is also backbone of our joint WSPOS-PAR model. As a word-level parser, its input is composed of two parts: 1) word embedding and 2) CharLSTM word representation (Lample et al., 2016). (w) ei = emb(wi ) ⊕ CharLSTM(wi ) (11) Other components, such as encoding and span scoring, are the same with those in Section 3.3. POS tagging. The inputs and encoder of the POS tagging model are the same with the above wordcoarse 501×501 where Wc ∈R is the biaffine param- level constituent parser. We then feed hi into a eter matrix for the label c. MLP layer to directly calculate the scores of differAnalogously, for scoring fine-grained labeled ent POS tags. We directly use the local word-level spans (i, j, f ), two extra MLPs and an extra set cross-entropy loss in the training phase, and"
2021.conll-1.23,D11-1109,1,0.760646,"is already remarkably superior to the tems instead of graph-based. previous state-of-the-art joint framework proposed Joint POS-ConPAR. Wang and Xue (2014) inby Zhang et al. (2013). Compared with it, our tegrate POS tagging and constituent parsing based joint framework achieves an absolute improvement on a transition-based parsing model. They modify of 0.57, 0.84, 3.14 on word segmentation, POS the action to assign a POS tag when the word is tagging, and constituent parsing respectively on shifted into the stack, making POS tagging as a CTB5 test. part of parsing naturally. Joint POS-DepPAR. Li et al. (2011) extend 6 Related Work graph-based dependency parsing to handle POS Coarse-to-fine Parsing. Directly performing dy- tagging simultaneously based on a dynamic programming algorithm for joint decoding. Hatori namic programming parsing with fine-grained et al. (2011) combine POS tagging and dependency grammar is computationally expensive from the parsing into a shift-reduce parsing system. aspects of time and memory size. Coarse-to-fine parsers introduce complexity gradually by coarseJoint WS-POS-ConPAR. Qian and Liu (2012) to-fine utilizing a sequence of grammars. Charniak design a graph-based j"
2021.conll-1.23,2020.tacl-1.6,0,0.0110143,"e by adopting the state-of-the-art BiLSTM-based parsing model. Our main contribution is proposing an elegant way to handle word-vs-phrase conflicts, which unfortunately are not mentioned in their work. Kurita et al. (2017) for the first time apply neural networks to jont WS-POS-DepPAR. They enhance the transition-based parser with char/word embeddings and BiLSTM which alleviate the efforts of feature engineering. Li et al. (2018) annotate a character-level dependency treebank for joint WS-POS-DepPAR. They use a neural characterlevel transition-based parsing model to reduce computational cost. Yan et al. (2020) focus on joint WS-DepPAR. They adopt the character-level graphbased parsing approach. For intra-word dependencies, a character is designed to modify its subsequent character with the label of “app”. Wu and Zhang (2021) split joint WS-POS-DepPAR into joint WS-POS and joint WS-DepPAR by using a shared character-level encoder and two independent decoders. They adopt outputs from joint WSPOS as final word segmentation results. 7 Conclusions In this work, we propose a two-stage coarse-to-fine labeling framework of joint WS-POS-PAR, which is shown to be able to handle both challenges for char-level"
2021.conll-1.23,P13-1013,0,0.232749,"Zhang and Clark, 2009; Liu and Zhang, 2017) to split train/dev/test datasets. Because both dev and test datasets of CTB5 contain only about 350 sentences, which is not enough to conduct robust investigations, we adopt another partition of Chinese Penn Treebank 5.1 (CTB5big), which is proposed by Duan et al. (2007). For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) to redefine the span by the index of its beginning and ending characters. We adopt the standard constituent-level labeled precision, recall, F-score (Par P/R/F) as the evaluation metrics for constituent parsing, where POS tags are discarded. Metrics of labeled precision, recall, and F-score (Tag P/R/F) of POS tags are used to evaluate the POS tagging task. For word segmentation, the unlabeled precision, recall, F-score (Seg P/R/F) of POS tagging are served as the evaluation metrics. 10 102 85.84 103 85.89 85.8 85.71 85.6 93.6 Word Segmentation. The inputs and encoder of the WS model are the s"
2021.conll-1.23,P14-1125,0,0.0271985,"t parse is selected from the ging information into parse trees, and then use a n-best parses with the finer-grained second-stage transition-based parser to perform joint learning grammar which makes use of more important con- and decoding. They manually annotated the intratextual information. Charniak et al. (2006) extend structures of words on the CTB5 dataset, which is the basic two-stage coarse-to-fine parsing to multi- proven to be very beneficial. 297 Joint WS-POS-DepPAR. Hatori et al. (2012) propose a transition-based parser that uses extra actions for word segmentation and POS tagging. Zhang et al. (2014) apply their previous work on intra-word structures (Zhang et al., 2013) to dependency parsing. The idea is to compose characterlevel dependency trees by combing inter- and intraword dependencies. Joint modeling in the DL era. Most closely related to our work, Zheng et al. (2015) also focus on joint WS-POS-ConPAR. They adopt CNN with pooling layers to encode the input character sequence. They show that utilizing the intra-word structures annotated by Zhang et al. (2013) can bring considerable gains to parsing performance. Compared with their work, we simply use left binarization to decide the"
2021.conll-1.23,W09-3825,0,0.0546649,"to better performance. 294 #Train #Dev #Test #Labels Orig. CNF CNF∗ CTB5 18,104 352 348 57 CTB5-big 16,091 803 1,910 57 CTB7 46,572 2,079 2,796 63 323 313 638 183 179 293 Table 2: Numbers of sentences and constituent labels. “Orig.” indicates the number of labels before CNF. “CNF∗ ” represents the number of remained labels after pruning labels with frequency &lt; α = 10. 97.4 638 600 97.2 400 97.0 293 181 200 1 10 102 140 93.8 86.0 93.83 93.81 93.76 93.82 5 5.1 Experiments Settings Data. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5) and 7 (CTB7). For CTB5, we follow previous works (Zhang and Clark, 2009; Liu and Zhang, 2017) to split train/dev/test datasets. Because both dev and test datasets of CTB5 contain only about 350 sentences, which is not enough to conduct robust investigations, we adopt another partition of Chinese Penn Treebank 5.1 (CTB5big), which is proposed by Duan et al. (2007). For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) t"
2021.conll-1.23,Q17-1004,0,0.0153093,"294 #Train #Dev #Test #Labels Orig. CNF CNF∗ CTB5 18,104 352 348 57 CTB5-big 16,091 803 1,910 57 CTB7 46,572 2,079 2,796 63 323 313 638 183 179 293 Table 2: Numbers of sentences and constituent labels. “Orig.” indicates the number of labels before CNF. “CNF∗ ” represents the number of remained labels after pruning labels with frequency &lt; α = 10. 97.4 638 600 97.2 400 97.0 293 181 200 1 10 102 140 93.8 86.0 93.83 93.81 93.76 93.82 5 5.1 Experiments Settings Data. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5) and 7 (CTB7). For CTB5, we follow previous works (Zhang and Clark, 2009; Liu and Zhang, 2017) to split train/dev/test datasets. Because both dev and test datasets of CTB5 contain only about 350 sentences, which is not enough to conduct robust investigations, we adopt another partition of Chinese Penn Treebank 5.1 (CTB5big), which is proposed by Duan et al. (2007). For CTB7, we follow the data split suggested in official guidelines. Table 2 shows the data statistics. Evaluation metrics. Before the evaluation, we convert predicted character-level CNF trees into word-level n-ary trees. Since our joint framework is based on characters, we follow Zhang et al. (2013) to redefine the span by"
2021.conll-1.23,W03-1025,0,0.176447,"e is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there era, there had been intensive research interest in exist two severe challenges. (1) The first challenge jointly modeling of the three tasks, i.e., WS-POSis high model complexity. On the one hand, a PAR (Luo, 2003; Qian and Liu, 2012; Zhang et al., char sequence is almost twice longer than its word ∗ Corresponding author sequence. On the other hand, the size of the la290 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 290–299 November 10–11, 2021. ©2021 Association for Computational Linguistics bel set is very large (e.g., ≥ 600) after transforming char-level trees into Chomsky normal form (CNF), as illustrated in Figure 2(a). Both factors greatly increase time and memory complexity. (2) The second challenge is rule conflict. Since neural graph-based parsers"
2021.conll-1.23,N07-1051,0,0.0954659,"Joint frameworks Qian and Liu (2012) 97.96 93.81 82.85 Zhang et al. (2013) 97.84 94.80 84.43 Wang et al. (2013) 97.86 94.40 83.42 Zheng et al. (2015) – – 84.22 Pipeline Joint 98.05 95.17 87.47 98.41 95.64 87.57 Table 6: Comparisons with previous works on CTB5 test. stage parsing by constructing multiple levels of grammars. They cluster constituent labels of the raw treebank into coarser categories. For example, in the most coarse grammar, there only exists one label “P” which corresponds to the phases. The proposed clustering idea in their work is similar to that used in our work. The work of Petrov and Klein (2007) also builds a multi-stage parser which constructs a sequence of increasingly refined grammars in a automatic fashion. However, the final grammar is finer than the raw treebank grammar. Joint modeling in the pre-DL era. In this part, we try to briefly discuss works on joint modeling Comparisons with previous works. Finally, we of word segmentation, POS tagging, and parsing, list results of all recent related works of the joint which were conducted in the pre-DL era, and most framework on CTB5. Please note that our pipeline of which are transition-based (shift-reduce) sysframework is already re"
2021.conll-1.23,D12-1046,0,0.406671,"osed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there era, there had been intensive research interest in exist two severe challenges. (1) The first challenge jointly modeling of the three tasks, i.e., WS-POSis high model complexity. On the one hand, a PAR (Luo, 2003; Qian and Liu, 2012; Zhang et al., char sequence is almost twice longer than its word ∗ Corresponding author sequence. On the other hand, the size of the la290 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 290–299 November 10–11, 2021. ©2021 Association for Computational Linguistics bel set is very large (e.g., ≥ 600) after transforming char-level trees into Chomsky normal form (CNF), as illustrated in Figure 2(a). Both factors greatly increase time and memory complexity. (2) The second challenge is rule conflict. Since neural graph-based parsers do not consider pro"
2021.conll-1.23,P17-1076,0,0.158471,"of Zheng et al. (2015). Similar to this work, they treated the joint problem as a char-level parsing task, which is obviously the most straightforward way for Chinese, as illustrated in Figure 1(b). However, they showed that the joint model fails to outperform the pipeline approach. In the past few years, constituent parsing has achieved significant progress. Compared with preDL parsers based on discrete features (Collins, 1997; Charniak and Johnson, 2005; Petrov and 1 Introduction Klein, 2007), the major characteristic of graphAs shown in Figure 1(a), the results of word seg- based parsers (Stern et al., 2017; Gaddy et al., 2018; mentation (WS), part-of-speech (POS) tagging, and Kitaev and Klein, 2018; Zhang et al., 2020) is that constituent parsing (PAR) can be organized in a the score of a constituent tree is decomposed into unified hierarchical tree, where leaf nodes include scores of labeled spans (i.e., (2, 3, VP) or VP2,3 ), words and their POS tags, and non-terminal nodes without any consideration on production rules (i.e., correspond to phrases (or constituents) with their VP2,3 → ADVP2,2 VP3,3 ). syntactic label. Before the deep learning (DL) For char-level graph-based WS-POS-PAR, there e"
2021.conll-1.23,P14-1069,0,0.0169979,"ver, the final grammar is finer than the raw treebank grammar. Joint modeling in the pre-DL era. In this part, we try to briefly discuss works on joint modeling Comparisons with previous works. Finally, we of word segmentation, POS tagging, and parsing, list results of all recent related works of the joint which were conducted in the pre-DL era, and most framework on CTB5. Please note that our pipeline of which are transition-based (shift-reduce) sysframework is already remarkably superior to the tems instead of graph-based. previous state-of-the-art joint framework proposed Joint POS-ConPAR. Wang and Xue (2014) inby Zhang et al. (2013). Compared with it, our tegrate POS tagging and constituent parsing based joint framework achieves an absolute improvement on a transition-based parsing model. They modify of 0.57, 0.84, 3.14 on word segmentation, POS the action to assign a POS tag when the word is tagging, and constituent parsing respectively on shifted into the stack, making POS tagging as a CTB5 test. part of parsing naturally. Joint POS-DepPAR. Li et al. (2011) extend 6 Related Work graph-based dependency parsing to handle POS Coarse-to-fine Parsing. Directly performing dy- tagging simultaneously b"
2021.emnlp-main.707,P01-1008,0,0.0422567,"e., ciency with augmented data. In summary, we make data augmentation, which addresses both chal- the following contributions. lenges discussed above in a resource-cheap way. • We present a simple and resource-cheap data augThe idea of data augmentation is automatically mentation framework for cross-domain text-togenerating noisy labeled data using some delibSQL parsing with no human intervention.1 erately designed method, and the technique has • As the key component for our framework, we probeen successfully applied to a wide range of NLP pose a hierarchical SQL-to-question generation tasks (Barzilay and McKeown, 2001; Jia and Liang, model to obtain more reliable NL questions. 2016). In our cross-domain text-to-SQL task, we can directly generate labeled data over unseen DBs • In order to improve training efficiency, we proas extra training data. The key of data augmenpose a simple sampling strategy to utilize genertation is how to improve the quality of generated ated data, which is of relatively larger scale than data. As two prior works, Yu et al. (2018a) manuoriginal training data. ally align question tokens and DB elements in the 1 We release the code at https://github.com/ corresponding SQL query, in"
2021.emnlp-main.707,N19-1423,0,0.017965,"r each dataset, the first major row shows previously reported results, and the second major row gives results of our base parsers without and with data augmentation. To compare previous data augmentation methHyper-parameter settings. For each parser, we ods, we also re-implement the flat one-stage genuse default parameter settings in their released code. eration approach (FLAT) proposed by Guo et al. All these parsers are enhanced with vanilla (in (2018). We do not implement the pattern-based contrast to task-specific) pretraining models, i.e., data augmentation approach (PATTERN) of Yu BERT (Devlin et al., 2019), including IRNet-Ext. et al. (2018a) due to its requirement of human inIn order to avoid the effect of performance vitervention. Moreover, their large performance imbrations9 , we run each model for 5 times with provement is obtained over a very weak baseline. 8 The comparison of V2 and V3 is discussed at https: Performance of our baseline parsers. On Wik//github.com/microsoft/rat-sql/issues/12. iSQL, the averaged performance of our SQLova 9 Please see issues proposed at the github of RATSQL parser is lower than their reported performance by model, such as https://github.com/microsoft/ rat-sq"
2021.emnlp-main.707,P17-2090,0,0.0298472,"ta of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a"
2021.emnlp-main.707,P16-1154,0,0.0353237,"mainly considers as illustrated by the last two examples in Figure 3. the informativeness aspect of generated NL questions. We leave such evaluation and analysis as From the perspective of SQL syntax, HAVING and GROUP_BY, are naturally bundled together, future work, which will certainly help us better understand our proposed approach. and thus are put into one clause, as shown in the third example of Figure 3. LIMIT and ORDER_BY 2.3 Clause-to-subquestion Translation Model are similarly handled. We adopt the standard Seq2Seq model with From the second perspective, some keywords copy mechanism (Gu et al., 2016) for clause-toare not explicitly expressed in NL questions. In subquestion translation, which is also used in our other words, there is a mismatch between intents expressed in NL questions and the implementa- baseline, i.e., flat SQL-to-question translation, with tion details in SQL queries. To better align them, the same hyper-parameter settings. In the input layer, we represent every SQL towe follow IRNet (Guo et al., 2019) and combine GROUP_BY with either SELECT or ORDER_BY. ken by concatenating two embeddings, i.e., word (token as string) embedding, and token type (colFor a nested SQL quer"
2021.emnlp-main.707,D18-1188,0,0.105257,"generated questions, since very complex questions are rare in the training data. Please kindly note that our simple ASTG-based generation procedure can produce a lot of patterns unseen in the original data, because our generation is at production rule level. This is advantageous from the data variety perspective. Moreover, given a DB, we only keep executable SQL queries for correctness check. 2.2 Hierarchical SQL-to-Question Generation Given an SQL query, especially a complex one, it is difficult to generate an NL question that represents exactly same meaning. In their data augmentation work, Guo et al. (2018) use a vanilla Seq2Seq model to translate SQL queries into NL questions and obtain performance boost on WikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adop"
2021.emnlp-main.707,P19-1444,0,0.336098,"ikiSQL consisting of simple queries. However, as shown in Table 2, we find performance consistently drops on all datasets over our strong baselines, which is largely due to the quality issue of generated NL questions, as illustrated in Table 3. This work proposes a hierarchical SQL-toBeing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with"
2021.emnlp-main.707,D19-1394,0,0.0189092,"ing a program language, all SQL queries can be represented as nested tree structures, as depicted in Figure 2-B according to some context-free grammar. In fact, most text-to-SQL parsers proposed recently adopt the abstract syntax tree representation at the decoding stage (Yin and Neubig, 2018; Yu et al., 2018a; Guo et al., 2019; Wang et al., 2020a). Following those works, we design a general ASTG that can cover all SQL patterns in our adopted benchmark datasets. Due to space limita2 As discussed in the logic form-based semantic parsing tion, Figure 2 shows a fraction of the production work of Herzig and Berant (2019), distribution mismatch is rules. mainly caused by insufficient coverage of logical form temAccording to our ASTG, the SQL query in Fig- plates. 8976 question generation model to produce higher- much easier to translate clauses to subquestions quality NL questions. The idea is motivated by compared with direct SQL-to-question translation. our observation that there is a strong segment-level We use a standard copy-based Seq2Seq model (Gu mapping between SQL queries and corresponding et al., 2016) for clause-to-subquestion generation. questions, as shown in Figure 3. For example, the The details"
2021.emnlp-main.707,2020.acl-main.677,0,0.257833,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.emnlp-main.562,1,0.725224,"uSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generalization ability. The second challenge aims t"
2021.emnlp-main.707,2020.acl-main.398,0,0.0547679,"Missing"
2021.emnlp-main.707,D15-1306,0,0.0153771,"sion field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016"
2021.emnlp-main.707,C18-1105,0,0.102154,"ne the corresponding subquestion as the shortest question segment that contains all DB elements in the clause. Finally, we discard low-confidence clause/subquestion pairs to reduce noises, such as subquestions having large overlap with others. We keep overlapping subquestions, unless one subquestion fully contains another. In that case, we only keep the shorter subquestion. We find that a portion of collected clauses have multiple subquestion translations. For example, the clause “ORDER_BY age ASC” are translated as both “in ascending order of the age” and “from youngest to oldest”. We follow Hou et al. (2018) and use them as two independent clause/subquestion pairs for training. 2.4 Three Strategies for Utilizing Generated Data Given a set of DBs, the generated question/SQL pairs are usually of larger scale than the original training data (see Table 1), which may greatly increase training time. In this work, we compare the following three strategies for parser training. • The pre-training strategy first pre-trains the model with only generated data, and then finetunes the model with labeled training data. • The directly merging strategy trains the model with all generated data and labeled training"
2021.emnlp-main.707,D19-1670,0,0.0162761,"ve way the number of augmented pairs affects the accuracy to address the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentat"
2021.emnlp-main.707,P17-1089,0,0.156978,"ion ability. The second challenge aims to produce a legal and executable SQL query is that the scale of labeled data is quite small for to get the correct answer (Date and Darwen, 1997), such a complex task, since it is extremely diffias depicted in Figure 1. A DB usually consists of cult to construct DBs and manually annotate cormultiple tables interconnected via foreign keys. responding question/SQL pairs. For example, the Early research on text-to-SQL parsing mainly Spider dataset has only 200 DBs and 10K quesfocuses on the in-domain setting (Li and Jagadish, tion/SQL pairs in total. 2014; Iyer et al., 2017; Yaghmazadeh et al., 2017), To deal with the first challenge, many previous * Work done during an internship at Baidu Inc. works focus on how to better encode the matching 8974 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8974–8983 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: An overview of our approach containing 3 stages: SQL query generation based on ASTG (Section §2.1), question hierarchical generation according to SQL structure (Section §2.2), model training via data augmentation (Section §2.4). among que"
2021.emnlp-main.707,P16-1002,0,0.0206393,"t al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge base, semantic parsing aims to generate a semantically equivalent formal representation, such as SQL query, logic form (LF), or task-oriented dialogue slots. Based on LF-based representation, Jia and Liang (2016) train a synchronous"
2021.emnlp-main.707,D18-1545,0,0.0126,"ess the sparseness of labeled data, data of parsing models. We conduct this experiment augmentation has been widely and successfully on the Spider dataset using IRNet model based on adopted in the computer vision field (Szegedy et al., the directly merging training strategy. In the ex- 2015). Similarly in the NLP field, a wide range of periment, we randomly sample question/SQL pairs tasks employ data augmentation to accommodate 8981 the capability and need of deep learning models in consuming big data, e.g., text-classification (Wei and Zou, 2019), low-resource dependency parsing (Sahin ¸ and Steedman, 2018), machine translation (Fadaee et al., 2017), etc. Concretely, the first kind of typical techniques tries to generate new data by manipulating the original instance via word/phrase replacement (Wang and Yang, 2015; Jia and Liang, 2016), random deletion (Wei and Zou, 2019), or position swap (Sahin ¸ and Steedman, 2018; Fadaee et al., 2017). The second kind creates completely new instances via generative models (Yoo et al., 2019), while the third kind uses heuristic patterns to construct new instances (Yu et al., 2018a). Data augmentation for semantic parsing. Given an NL question and a knowledge"
2021.emnlp-main.707,2020.acl-main.745,0,0.0314504,"Missing"
2021.emnlp-main.707,D18-1193,0,0.351191,".e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement. Figure 1: An example of the text-to-SQL parsing task. where all question/SQL pairs of train/dev/test sets are generated against the same DB. In order to deal with the more realistic setting where DBs in the evaluation phase are unseen in the training data, researchers propose several cross-domain datasets, such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b) in English, and DuSQL (Wang et al., 2020b) in Chinese. All three datasets adopt the DB-level data splitting, meaning that a DB and all its corresponding question/SQL pairs can appear in only one of the train/dev/test sets. Cross-domain text-to-SQL parsing has two major challenges. First, unseen DBs usually introduce 1 Introduction new schemas, such as new table/column names and unknown semantics of inter-table relationships. Given a natural language (NL) question and a relaTherefore, it is crucial for a parsing model to have tional database (DB), the text-to-SQL parsing task strong generali"
2021.emnlp-main.707,D18-1425,0,0.0470955,"Missing"
2021.findings-emnlp.149,D19-1188,0,0.0211495,", achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. D"
2021.findings-emnlp.149,D14-1082,0,0.188043,"Missing"
2021.findings-emnlp.149,D18-1217,0,0.0295431,"ark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is defined as d = {(h, m, l),"
2021.findings-emnlp.149,P07-1033,0,0.407388,"Missing"
2021.findings-emnlp.149,D18-1498,0,0.02412,"target-domain training data is small. Most recently, Li et al. (2019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the sour"
2021.findings-emnlp.149,W09-1201,0,0.0382346,"rce domain which is a balanced corpus (BC) from news-wire, three target domains which are the product comments (PC) data from Taobao, the product blog (PB) data from Taobao headline, and a web fiction data named “ZhuXian” (ZX). Table 1 shows the detailed illustration of the data statistics. In this work, we pick one target dataset as the target domain, and the rest are the source domains. For example, if the target domain is PC, source domains are BC, PB, and ZX. Evaluation. We use unlabeled attachment score (UAS) and labeled attachment score (LAS) to evaluate the dependency parsing accuracy (Hajic et al., 2009). Each model is trained for at most 1, 000 iterations, and the performance is evaluated on the dev data after each iteration for model selection. We stop the training if the peak performance does not increase in 100 consecutive iterations. Baseline models. To verify the effectiveness and advantage of our proposed model, we select the following approaches as our strong baselines. 1 http://hlt.suda.edu.cn/index.php/ Nlpcc-2019-shared-task • Parameter generation network (PGN). Motivated by Jia et al. (2019), we exploit the PGN based on distributed domain representations to generate domain-related"
2021.findings-emnlp.149,P19-1236,0,0.37234,"n , E) n 0 = BiLSTM (x0 . . . xn , V = W ⊗ E) (6) where ⊗ denotes matrix multiplication; W ∈ RU×D is a parameter matrix to be trained; E ∈ RD is distributed domain-aware sentence representation vector and will be explained later. Distributed domain-aware sentence representation. The distributed domain-aware sentence representation vector can be regarded as a sum of weighted domain embeddings, where higher weights are expected to be assigned to domains that are more similar to the input sentence. First, we compute domain distribution probabilities of each word via simple domain classification. Jia et al. (2019) first propose PGN to generate BiLSTM parameters based on fixed task and domain embeddings for NER domain adaptation, finding  that the PGN can effectively extract domain differzi = softmax MLP hdom (7) i ences. However, the vanilla PGN requires crosswhere hdom is the representation vector of the i-th domain language model task as a bridge to help i word generated by a separated standard BiLSTM. fixed domain embeddings training. Considering the development of pre-training techniques with Then, we compute a distributed domain-aware language model loss and computational complex- word represent"
2021.findings-emnlp.149,2020.acl-main.625,0,0.0360117,"e translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively s"
2021.findings-emnlp.149,P17-1060,0,0.0166215,"te BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (C"
2021.findings-emnlp.149,C16-1038,0,0.0166719,"main NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tr"
2021.findings-emnlp.149,Q16-1023,0,0.0267691,"eveloped (Peng et al., 2019). Intuitively, n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a depen- an effective exploration of all these corpora can dency from the head word wh to the child word give better performance for the target domain comwm with the relation label l ∈ L. pared with the single-source domain adaptation. Recently, supervised neural dependency parsSeparating domain-invariant and domaining models have achieved great success, leading specific features is one popular way for domain to impressive performance (Chen and Manning, adaptation to distinguish the similarity and discrep2014; Kiperwasser and Goldberg, 2016; Dozat and ancy of different domains (Daumé III, 2007; Kim Manning, 2017; Li et al., 2019a). Remarkably, the et al., 2016; Sato et al., 2017). Domain-invariant BiAffine parsing model can obtain a UAS of 96.67 features indicate the shared feature space across ∗ Corresponding author domains, which have been widely-adopted as 1724 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1724–1733 November 7–11, 2021. ©2021 Association for Computational Linguistics knowledge transferring. Domain-specific features imply the differences between domains, which could be helpful if"
2021.findings-emnlp.149,P08-1068,0,0.0956171,"s. and a LAS of 95.03 on standard Penn Treebank benchmark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from diff"
2021.findings-emnlp.149,2020.coling-main.338,1,0.883628,"language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ have bee"
2021.findings-emnlp.149,P19-1229,1,0.815406,"exploration of all these corpora can dency from the head word wh to the child word give better performance for the target domain comwm with the relation label l ∈ L. pared with the single-source domain adaptation. Recently, supervised neural dependency parsSeparating domain-invariant and domaining models have achieved great success, leading specific features is one popular way for domain to impressive performance (Chen and Manning, adaptation to distinguish the similarity and discrep2014; Kiperwasser and Goldberg, 2016; Dozat and ancy of different domains (Daumé III, 2007; Kim Manning, 2017; Li et al., 2019a). Remarkably, the et al., 2016; Sato et al., 2017). Domain-invariant BiAffine parsing model can obtain a UAS of 96.67 features indicate the shared feature space across ∗ Corresponding author domains, which have been widely-adopted as 1724 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1724–1733 November 7–11, 2021. ©2021 Association for Computational Linguistics knowledge transferring. Domain-specific features imply the differences between domains, which could be helpful if the domain gaps could be accurately measured and effectively modeled. The learning of dom"
2021.findings-emnlp.149,P13-2017,0,0.0950635,"Missing"
2021.findings-emnlp.149,D17-1155,0,0.0228326,") propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples"
2021.findings-emnlp.149,2020.emnlp-main.639,0,0.0244174,"l. Most recently, Li et al. (2019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Rece"
2021.findings-emnlp.149,W15-2201,0,0.0135495,") and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1731 baselines, eve"
2021.findings-emnlp.149,D18-1041,0,0.0243399,"019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of wo"
2021.findings-emnlp.149,D18-1039,0,0.028153,"ging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation co"
2021.findings-emnlp.149,P07-1078,0,0.0809956,", 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1"
2021.findings-emnlp.149,N01-1023,0,0.169558,"ng (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1731 baselines, even when all models are enhanc"
2021.findings-emnlp.149,K17-3007,0,0.320371,"enn Treebank benchmark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is define"
2021.findings-emnlp.406,W13-2322,0,0.0531625,"itive results with the SOTA model, and the speed is an order of magnitude faster. Detailed analyses are conducted to gain more insights into our proposed model and the effectiveness of the pre-training technique. 1 face-01 ARG0 nature ARG1 poss really humankind caprice Figure 1: AMR example of the sentence “Facing the caprice of nature, humankind is really insignificant.” Abstract meaning representation (AMR) parsing aims to abstract semantics from a natural language sentence into a rooted, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data"
2021.findings-emnlp.406,D19-1393,0,0.18002,"t the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one encoder-decoder fashion pre-trained language model. They first convert the AMR graph into a text sequence with symbols indicating the concepts’ graph positions. Then, they propose to fine-tune the sentence sequence and AMR graph sequence on BART, achieving large improvements compared with previous works, including"
2021.findings-emnlp.406,2020.acl-main.119,0,0.143065,"The remaining question is how we do concept generation and edge classification with Transformer, which is usually used for encoding sequences. Our answer is giving the Transformer attention mechanisms more meanings. In detail, we try to demonstrate that the self-attention in the decoder captures the semantic relation that can guide establishing the connections for the concepts and the cross-attention implicitly links the concept with its surface word, which is similar to the core of attention-based machine translation. Based on the inspirations, we use the copy mechanism (Zhang et al., 2019b; Cai and Lam, 2020) to copy words or lemmas as candidate concepts for concept prediction, in which we treat the cross-attention between the encoder and decoder as the probability. Another source of candidate concepts is the extracted concept vocabulary from the training data. For edge classification, we directly treat part of the decoder self-attention values as the edge scores between concept nodes. and the speed is an order of magnitude faster. Our contributions are threefold: (I) We propose a simple Transformer-based AMR parser, which only needs to add one external bi-affine scorer for the relation classifica"
2021.findings-emnlp.406,E17-1051,0,0.0189556,"tization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use the evaluation tool of Damonte et al. (2017) to test our model. Training Criterion. We train our models for at most 2,020 epochs and choose the best model to evaluate the test data according to the performance on development data. Methods Zhang et al. (2019a) Zhang et al. (2019b) Cai and Lam (2020) TAMR Smatch Unlabeled-Smatch 76.3 – 77.0 80.0 80.2 82.8 80.3 83.5 Table 2: Smatch scores of our model TAMR and comparison with previous seq2graph-based models on AMR2.0 test data. Silver Data Dev Test JAMR – 67.0? TAMR 80.6 80.3 S PRING – 83.8 Pre-train Dev Test 70.5 70.8 81.5 81.2 83.0 82.7 Fine-tune Dev Test 80.8 81.0 82.4 82.2 83.8 83.7 Ta"
2021.findings-emnlp.406,N19-1423,0,0.0334058,"he model architecture in detail and show how to adapt the AMR parsing process into Transformer in the following sections. 3.2 Input Layer. Encoder Input. The model input of each word wi in the sentence s is composed of its character representation which is generated by a convolutional neural network (CNN) (Kalchbrenner et al., 2014), randomly initialized lemma, part-of-speech tag, named entity tag, and dependency label embeddings (Xia et al., 2019), which is denoted as lem P oS NE fi = repchar wi ⊕ embwi ⊕ embwi ⊕ embwi ⊕ DL embwi , where ⊕ means the concatenation operation. We also use BERT (Devlin et al., 2019) to enhance the word representation. To get the wordbased representations, we make average pooling to sub-word-based representations. And due to the GPU limitation, we fix the BERT model parameters as Zhang et al. (2019b). The final model input representation for wi is computed as xw i = √ wi |s dim ∗ (MLP(fi ) + MLP(repBERT )) + embpi , where dim is the embedding dimension and embpi is the i-th sinusoidal position embedding. Decoder Input. In the decoder, we use the concatenation of the concept character representation and the randomly initialized concept embedding denoted as √ as the concept"
2021.findings-emnlp.406,P14-1134,0,0.0332947,"-based AMR parser, which only needs to add one external bi-affine scorer for the relation classification. (II) We investigate how to ensemble different models via the proposed stack pre-training method. (III) Detailed analyses show more insights into our model and several interesting findings of utilizing the silver data. 2 Related Work AMR parsing approaches can mostly be categorized into four classes: pipeline-based, transitionbased, seq2seq-based, and seq2graph-based approaches. Pipeline-based approaches mainly consist of two steps: 1) concept identification and 2) relation identification. Flanigan et al. (2014) is the first AMR parsing work (JAMR) that treats concept identification as a sequence labeling problem and relation identification as a maximum-scoring connected graph searching problem, in which they also propose an influential rule-based aligner for aligning the concepts and words. Lyu and Titov (2018) treat Second, to achieve competitive performance the alignment as latent variables and propose a joint with the current SOTA model, we seek to use sil- model for AMR parsing. Zhang et al. (2019a) first ver data to enhance the model representative abil- use the attention-based copy mechanism t"
2021.findings-emnlp.406,P14-1062,0,0.0281972,"≤ i ≤ m, 1 ≤ j ≤ m, r ∈ R} is the set of edges in the graph. R is the set of AMR relations. Overall, our Transformer-based model consists of the following modules, i.e., input layer, encoder layer, decoder layer, concept generator, edge generator, and relation classifier. We will describe the model architecture in detail and show how to adapt the AMR parsing process into Transformer in the following sections. 3.2 Input Layer. Encoder Input. The model input of each word wi in the sentence s is composed of its character representation which is generated by a convolutional neural network (CNN) (Kalchbrenner et al., 2014), randomly initialized lemma, part-of-speech tag, named entity tag, and dependency label embeddings (Xia et al., 2019), which is denoted as lem P oS NE fi = repchar wi ⊕ embwi ⊕ embwi ⊕ embwi ⊕ DL embwi , where ⊕ means the concatenation operation. We also use BERT (Devlin et al., 2019) to enhance the word representation. To get the wordbased representations, we make average pooling to sub-word-based representations. And due to the GPU limitation, we fix the BERT model parameters as Zhang et al. (2019b). The final model input representation for wi is computed as xw i = √ wi |s dim ∗ (MLP(fi ) +"
2021.findings-emnlp.406,P17-1014,0,0.292417,"ntics from a natural language sentence into a rooted, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser ba"
2021.findings-emnlp.406,2020.acl-main.703,0,0.537065,"AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one encoder-decoder fashion pre-trained language model. They first convert the AMR graph into a text sequence with symbols indicating the concepts’ graph positions. Then, they propose to fine-tune the sentence sequence and AMR graph sequence on BART, achieving large improvements compared with previous works, including those with BERT. However, it makes the model relatively slower, which parses 31 tokens per second. We think there are two main reasons: 1) the 12-layer Transformer decoder and 2) the longer converted graph sequences that include the added symbols. In this work, we inve"
2021.findings-emnlp.406,2020.emnlp-main.196,1,0.846884,"d, directed, and labeled graph, where the nodes represent concepts and edges represent semantic relations (Banarescu et al., 2013). Figure 1 gives an example. One main challenge of AMR parsing is the lack of large-scale annotated data, which limits the model representative ability. To alleviate the problem and boost the performance, early works propose to use silver (pseudo) data that are generated from some released AMR parsing models Corresponding author. condition degree ARG1 Introduction ∗ significant-02 polarity (van Noord and Bos, 2017; Konstas et al., 2017). Apart from AMR silver data, Xu et al. (2020) try to use other kinds of large-scale silver data to train a pre-trained model, such as constituent parsing data and machine translation data. With the development of pre-trained language models, recent works try to use pre-trained language models to enhance the model input representative ability (Cai and Lam, 2019; Zhou et al., 2021). Most of the them use pre-trained models in the model encoder side since it naturally provides powerful contextualized representations for sentences. Recently, Bevilacqua et al. (2021) propose a seq2seq AMR parser based on BART (Lewis et al., 2020), which is one"
2021.findings-emnlp.406,P19-1009,0,0.0365327,"Missing"
2021.findings-emnlp.406,D19-1392,0,0.0256032,"Missing"
2021.findings-emnlp.406,2020.acl-main.302,1,0.801999,"00 BERT-base-cased 4 8 1024 100 Table 1: Hyper-parameter settings. representation and BERT is fixed in our work due to the GPU memory limitation. The encoder and decoder consist of 4 and 8 Transformer blocks, respectively. Each Transformer block has 8 heads, the feed-forward hidden size is 1024, and the hidden size is 512. Implementation Details. We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use the evaluation tool of Damonte et al. (2017) to test our model. Training Criterion. We train our models for at most 2,020 epochs and choose the best model to evaluate the test data according to the performance on developmen"
2021.findings-emnlp.406,P18-1037,0,0.065492,"he silver data. 2 Related Work AMR parsing approaches can mostly be categorized into four classes: pipeline-based, transitionbased, seq2seq-based, and seq2graph-based approaches. Pipeline-based approaches mainly consist of two steps: 1) concept identification and 2) relation identification. Flanigan et al. (2014) is the first AMR parsing work (JAMR) that treats concept identification as a sequence labeling problem and relation identification as a maximum-scoring connected graph searching problem, in which they also propose an influential rule-based aligner for aligning the concepts and words. Lyu and Titov (2018) treat Second, to achieve competitive performance the alignment as latent variables and propose a joint with the current SOTA model, we seek to use sil- model for AMR parsing. Zhang et al. (2019a) first ver data to enhance the model representative abil- use the attention-based copy mechanism to predict ity. Specifically, we employ three different per- concepts in a BiLSTM encoder-decoder framework formance AMR models (denoted as “father” mod- and then use the bi-affine scorer for edge and relaels) to generate three different performance silver tion prediction based on the predicted concepts. d"
2021.findings-emnlp.406,P14-5010,0,0.00312283,"4733 Input Layer character lemma PoS tag NER tag dependency label concept BERT Encoder Layer Transformer encoder Decoder Layer Transformer decoder Concept Generator hidden size Relation Classifier hidden size 32 300 32 16 64 300 BERT-base-cased 4 8 1024 100 Table 1: Hyper-parameter settings. representation and BERT is fixed in our work due to the GPU memory limitation. The encoder and decoder consist of 4 and 8 Transformer blocks, respectively. Each Transformer block has 8 heads, the feed-forward hidden size is 1024, and the hidden size is 512. Implementation Details. We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech tagging, and named entity tagging. The dependency relations are obtained by the bi-affine dependency parser (Dozat and Manning, 2017) implemented in SuPar (Zhang et al., 2020). Previous works (Zhang et al., 2019b; Cai and Lam, 2020) usually use graph recategorization to reduce the complexity and sparseness of the AMR graph. In this work, we use the same script from Cai and Lam (2020) for preand post-processing. Our models are trained with Adam (Kingma and Ba, 2015) optimizer and learning rate with warm-up same as to Vaswani et al. (2017). We use"
2021.findings-emnlp.406,P19-1451,0,0.0698639,"e bi-affine scorer for edge and relaels) to generate three different performance silver tion prediction based on the predicted concepts. data and try to investigate several questions which Transition-based methods aim to design a seare seldom discussed in previous works: 1) What ries of actions to generate the AMR graph. Wang are the best learning schedules to build pre-trained et al. (2016) propose to transform the sentence’s demodels with silver data and later fine-tune with pendency tree into its AMR graph. Ballesteros and the gold-standard data, respectively? 2) Are all Al-Onaizan (2017); Naseem et al. (2019) use Stackthe different performance silver data beneficial for LSTM transition-based AMR parser that transour model, even its father model lags behind our forms the sentence into the AMR graph, which model? and 3) Whether using multiple different is different from Wang et al. (2016). With the performance silver data can provide more informa- rise of Transformer, Astudillo et al. (2020); Zhou tion than the best performance one or not, i.e., can et al. (2021) propose to use Stack-Transformer for the higher performance silver data benefits from transition-based AMR parsing. lower performance silv"
2021.findings-emnlp.406,S16-1181,0,0.0190748,"of actions to generate the AMR graph. Wang are the best learning schedules to build pre-trained et al. (2016) propose to transform the sentence’s demodels with silver data and later fine-tune with pendency tree into its AMR graph. Ballesteros and the gold-standard data, respectively? 2) Are all Al-Onaizan (2017); Naseem et al. (2019) use Stackthe different performance silver data beneficial for LSTM transition-based AMR parser that transour model, even its father model lags behind our forms the sentence into the AMR graph, which model? and 3) Whether using multiple different is different from Wang et al. (2016). With the performance silver data can provide more informa- rise of Transformer, Astudillo et al. (2020); Zhou tion than the best performance one or not, i.e., can et al. (2021) propose to use Stack-Transformer for the higher performance silver data benefits from transition-based AMR parsing. lower performance silver data? Based on the anSeq2seq-based approaches convert the AMR swers to these questions, which are shown in Secgraph generation problem into a symbolic sequence tion 6.2, we propose a stack pre-training technique generation problem, where the hierarchy structure for effectively us"
2021.naacl-main.144,P17-4017,0,0.0467976,"Missing"
2021.naacl-main.144,N19-1423,0,0.126883,"ng et al. (2019a) extract the semanthat our proposed unified model achieves superior tic representations from a pre-trained SRL model performance compared with previously proposed BMESO-based works. Our contributions are: (i) and feed them into the opinion mining model, achieving substantial improvements. Zhang et al. we propose a unified span-based model for opinion (2020) incorporate the powerful contextual repremining in the end-to-end fashion that also supports sentations of bi-directional encoder representations the given-expression setting, (ii) we successfully from Transformers (BERT) (Devlin et al., 2019) integrate syntactic constituents knowledge into our and external dependency syntactic knowledge. model with MTL and GCN, achieving promising improvements, (iii) detailed analyses demonstrate To solve or alleviate the weaknesses of the prethe effectiveness of our unified model and the use- viously proposed BMESO-based models, we profulness of integrating constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classifica"
2021.naacl-main.144,Q19-1019,0,0.0464782,"Missing"
2021.naacl-main.144,P18-2058,0,0.0219271,"edly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion roles , and R is the set of opinion relations (holder"
2021.naacl-main.144,P14-1062,0,0.0186777,"char representation, and contextual word representation to compose the model input, denoted as: xi = embword ⊕ repchar ⊕ repcontext , (1) wi wi wi |s Encoder Layer. + M LPbexp (hb ) + M LPeexp (he ), srol = M LP rol (spanrol b,e ) (5) + M LPbrol (hb ) + M LPerol (he ). We can observe that for a sentence with n words, the numbers of candidate spans for expressions and roles are both n∗(n+1) , while the number of 2 gold expressions and roles are much fewer. To alleviate the unbalanced number of gold samples where ⊕ means the concatenate operation. We use the convolutional neural networks (CNN) (Kalchbrenner et al., 2014) to generate the character representations over the characters of words. 1797 1 We omit the process of span boundary module in Figure 2 for clarity. O Classification Layer MLP Holder OM Target MLP Encoder MLP Representation Layer OM Constituent MTL Input seriously needs equipment for detecting drugs GCN Constituent Encoder Input Layer GCN Input OM Encoder Layer Encoder Input MTL+GCN GCN Figure 2: The model architecture of our unified span-based opinion mining model (left) and syntactic constituent integration methods (right). and negative samples, we adapt the focal loss that is widely used in"
2021.naacl-main.144,P16-1087,0,0.342622,"rporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinio"
2021.naacl-main.144,D14-1162,0,0.0862699,"62.04 53.27 57.76 Proportional F1 Holder Target Overall 46.62 34.29 55.62 41.65 48.90 61.20 49.88 55.68 Table 1: Experimental results of our span-based opinion mining model and comparison with previous works on the MPQA2.0 dataset in the end-to-end setting. “-” means results are not reported in their paper. Exact P R F1 Zhang et al. (2019b) 60.21 48.52 53.04 S PAN OM 64.85 52.60 58.06 S PAN OM+BERT 67.15 60.63 63.71 Models Table 2: Results and comparison of the expression prediction on the exact metric in the end-to-end setting. 5.2 Hyper-parameters. We employ the 300-dimension GloVe vector (Pennington et al., 2014) as our pre-trained word embeddings. The character embeddings are randomly initialized and a CNN with kernel sizes of 3, 4, 5 is used to capture the character representations. For the contextual representations, we extract the representations from the base BERT by making a weighted summation over the last four layer outputs. The hidden size of the BiLSTM layer is set to 300 and we employ 2-layer BiLSTMs to encode the input representations. The dimension of opinion expression and role representations is 300 and the hidden size of expression, role, and relation classifiers is 150. We use 3-layer"
2021.naacl-main.144,P13-1161,0,0.210861,"thod. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap"
2021.naacl-main.144,Q14-1039,0,0.0244727,"n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795–1804 June 6–11, 2021. ©2021 Association for Computational Linguistics belongs to an expression, 0 otherwise), thus one sample is expanded n times if one sentence has n expressions, which is inefficient (Marasovi´c and Frank, 2018; Zhang et al., 2020). 2) The BMESObased method is weak to capture long-range dependencies and prefers to predict shorter opinion role spans (Zhang et al., 2020). 2 Related Work There are several task settings for opinion mining in the community: 1) Breck et al. (2007); Yang and Cardie (2014) focus on labeling the expressions. 2) Katiyar and Cardie (2016); Zhang et al. (2019b); Quan et al. (2019) discover the opinion structures in the end-to-end setting, i.e, based on the systemMotivated by the span-based representations of atic expressions. 3) Marasovi´c and Frank (2018); opinion expressions and roles, we propose a unified Zhang et al. (2019a, 2020) identify the opinion span-based opinion mining model (S PAN OM) that roles based on the given expressions. Our work can solve or alleviate the aforementioned weak- follows the end-to-end setting and also supports nesses. First, we tre"
2021.naacl-main.144,P18-1249,0,0.388081,"ing constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion ro"
2021.naacl-main.144,2020.acl-main.297,1,0.617524,"promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpi"
2021.naacl-main.144,N19-1066,0,0.12942,"etting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpinion mining (OM), which aims to find the opin- sions in one sentence. Figure 1 gives an example, ion structures of “Who expressed what opinions in which some overlapped opinion relations have towards what.” in one sentence, has achieved much been discarded by previous works (Katiyar and attention in recent years (Katiyar and Cardie, 2016; Cardie, 2016), such as [happy, he loves being EnMarasovi´c and Frank, 2018; Zhang et al., 2019b, derly Park, Target] and [loves, he, Holder]. There 2020). The opinion analysis has many NLP appli- are also other works which focus only on predicting cations, such as social media monitoring (Bollen opinions roles based on the gold-standard expreset al., 2011) and e-commerce applications (Cui sions, which also follow the BMESO-based method et al., 2017). The commonly used benchmark (Marasovi´c and Frank, 2018; Zhang et al., 2020). However, they also suffer from some weaknesses: ∗ Rui Wang’s contributions were carried out while at 1) the expressions are usually fed into the model inAlibaba"
2021.naacl-main.144,N18-1054,0,0.041995,"Missing"
2021.naacl-main.144,J93-2004,0,0.0741147,"F1 score of 67.66. Finally, we try to combine the two kinds of methods and the results are shown in the last major row. It is clear that combining the MTL method with OntoNotes and the GCN method with ParserPTB achieves better results than the reversed one. Therefore, our constituent-enhanced opinion mining model follows this combination. Besides, we can also see the relative lower results of “OntoNotes+PTB” in “+MTL” and “+GCN” settings, which is strange Which source of constituent knowledge is better? There are two main constituent syntax corpus in the community, i.e., Penn Treebank (PTB) (Marcus et al., 1993) and OntoNotes5.0 (Weischedel et al., 2013). The PTB corpus contains about 39k training data and mainly focuses on news data, while the OntoNotes5.0 corpus contains about 75k training data and focuses on multi-domain data (news, web, telephone conversation, and etc.). It is a worthy question to explore which is better for our span-based OM model, or what kind of combination is better. We compare them with various combinations on the BERT-based model, whose results are shown in Table 5. First, the sec7 We use the code of Kitaev and Klein (2018) to train the OntoNotes conond major row shows the"
2021.naacl-main.144,D18-1244,0,0.0229945,"information to expressions and roles. 4.2 The GCN Method. The MTL method enhances our OM model from the aspect of model representative ability by jointly modeling opinion mining and partial constituency parsing. We argue that modeling the syntactic constituent structure is also beneficial for OM because it provides valuable syntactic information for a sentence. Therefore, we try to employ the recently popular GCN (Kipf and Welling, 2016) to encode the constituent structure. However, the conventional GCN is not suitable for constituency trees, because it usually works on the dependency trees (Zhang et al., 2018, 2020) where the nodes are the surface words in a sentence. While, in constituent trees, there exists a certain number of non-terminal nodes3 , such as “NP”, “VP”, “SBAR” and so on. So it is hard to directly apply conventional GCN on the constituent trees. In the following, we first introduce the definition and workflow of typical GCN and then describe our modification. Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as: ! X l hlv = ρ Wl hl−1 (11) u +b , u∈N (v) 3"
C10-3004,W09-1207,1,0.237403,"nnotated with lexical tags, including word segmentation, part-of-speech tagging, and named entity recognition tags2 . 2.2 NER WSD Parser SRL 3 Speed 185KB/s 56.3KB/s 14.4KB/s 7.2KB/s 0.2KB/s 1.3KB/s Table 1: The performance and speed for each module. the dependency syntactic parsing subtask in the CoNLL-2009 Syntactic and Semantic Dependencies in Multiple Languages Shared Task (Hajiˇc et al., 2009). 6. Semantic Role Labeling (SRL): SRL is to identify the relations between predicates in a sentence and their associated arguments. The module is based on syntactic parser. A maximum entropy model (Che et al., 2009) is adopted here which achieved the ﬁrst place in the joint task of syntactic and semantic dependencies of the CoNLL2009 Shared Task. Table 1 shows the performance and speed of each module in detail. The performances are obtained with n-fold cross-validation method. The speed is gotten on a machine with Xeon 2.0GHz CPU and 4G Memory. At present, LTP processes these modules with a cascaded mechanism, i.e., some higher-level processing modules depend on other lower-level modules. For example, WSD needs to take the output of POSTag as input; while before POSTag, the document must be processed wit"
C10-3004,S07-1034,1,0.697039,"Missing"
C10-3004,J96-1002,0,\N,Missing
C12-1103,P11-1048,0,0.0128095,"cient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating two separately-trained sub-models, and they find that training the integrated model on LBP leads to large improvement drops compared with separately-trained models. 3 Pipeline POS tagging and dependency parsing The pipeline method treats POS tagging and dependency parsing as two cascaded problems. First, an optimal POS tag sequence ˆt is de"
C12-1103,C10-1011,0,0.178642,"Dependency features fdep (x, t, h, m, l) Sibling features fsib (x, t, h, m, l, s) Grandchild features fgrd (x, t, h, m, l, g) Atomic features incorporated l, wh, w m , t h , t m , t h±1 , t m±1 , t b , d i r(h, m), d ist(h, m) l, wh, ws , w m , t h , t m , t s , t h±1 , t m±1 , t s±1 , d i r(h, m), d ist(h, m) l, wh, w m , w g , t h , t m , t g , t h±1 , t m±1 , t g±1 , d i r(h, m), d i r(m, g) Table 1: Brief illustration of the syntactic features. b is an index between h and m. d i r(i, j) and d ist(i, j) denote the direction and distance of the dependency (i, j). Please refer to Table 4 of (Bohnet, 2010) for the complete feature list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt"
C12-1103,D12-1133,0,0.229202,"peline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as laten"
C12-1103,D07-1101,0,0.509725,"t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corres"
C12-1103,W08-2102,0,0.0186802,"Missing"
C12-1103,P05-1022,0,0.0567146,"Missing"
C12-1103,P12-2003,1,0.88498,"Missing"
C12-1103,W02-1001,0,0.0361869,"ted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMOD VMOD  1 SUB AMOD á 2 193 gang man just turned 19 AD VV CD JJ 19 P VMOD  4 VMOD  &apos;"
C12-1103,W09-1201,0,0.0527682,"Missing"
C12-1103,I11-1136,0,0.318758,"is most closely related to (Li et al., 2011) who present the first work on joint models for Chinese POS tagging and unlabeled dependency parsing. Similar to us, their joint models are based on graph-based dependency parsing. They find that the joint models largely outperform the pipeline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy."
C12-1103,P10-1110,0,0.190632,"the discriminative power of the POS features in resolving such syntaxinsensitive POS ambiguities are suppressed in the joint models when trained with AP or PA. Compared with AP and PA, SPA raises the weight of the POS features and can better utilize the disambiguation power of both the POS and syntactic features, leading to large tagging accuracy boost. On the other hand, better tagging results can further help parsing. 6 Experiments Data. We conduct experiments on CTB5 (Xue et al., 2005). Following the standard practice, we adopt the data split of (Duan et al., 2007; Zhang and Clark, 2008b; Huang and Sagae, 2010) and adopt Penn2Malt2 for constituent-to-dependency conversion with the head-finding rules of (Zhang and Clark, 2008b). We also evaluate our models on another version of CTB5 used in (Bohnet and Nivre, 2012) to compare with their joint model. We thank Bernd Bohnet for sharing their dataset. We refer to their dataset as CTB5-Bohnet. We carefully compare CTB5 with CTB5-Bohnet and find that except for the mismatch of about 30 sentence, the datasets differ in both dependency structures and dependency labels. After discussions with Bernd Bohnet, we find out that they adopt Yue Zhang’s constituent-t"
C12-1103,D08-1008,0,0.0157962,") , ˆt, d) AP wjoint (7) PA computes the update step τjoint by considering the loss of the best result, the score distance, and the feature vector distance.  ˆ − Scorejoint (x( j) , t( j) , d( j) ) + ρpos (t( j) , ˆt) + ρsyn (d( j) , d) ˆ Scorejoint (x( j) , ˆt, d)  τjoint = ( j) ( j) ( j) ( j) ˆ ˆ 2 kfjoint (x , t , d ) − fjoint (x , t, d)k (8) PA  (k+1) (k) ˆ w =w + τjoint (fjoint (x( j) , t( j) , d( j) ) − fjoint (x( j) , ˆt, d)) joint joint ( j) ˆ is the where ρpos (t , ˆt) is the incorrect POS tag number in ˆt according to t( j) , and ρsyn (d( j) , d) ˆ according to d( j) . Following (Johansson and Nugues, 2008), dependency error number in d ˆ increases by 1 for an incorrect dependency and by 0.5 for a correct dependency ρsyn (d( j) , d) with a wrong label. Theoretically, Eq. 8 computes the smallest update that makes the correct hypothesis outscores the returned highest-scoring hypothesis by the overall error. We can see that AP and PA use the same update step for the POS features fpos (.) and syntactic features fsyn (.). Therefore, the weights of the POS features and the syntactic features are of the same scale after training is completed. We argue that this is problematic since the number of the sy"
C12-1103,P08-1068,0,0.333682,"Missing"
C12-1103,P10-1001,0,0.392309,"igram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corresponding to the three typ"
C12-1103,P11-1089,0,0.0153191,"ctive dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one inte"
C12-1103,D11-1109,1,0.93535,"score that are previously defined in the pipeline models. Scorejoint (x, t, d) = Scorepos (x, t) + Scoresyn (x, t, d) = wpos · fpos (x, t) + wsyn · fsyn (x, t, d) (6) = wpos⊕syn · fpos⊕syn (x, t, d) = wjoint · fjoint (x, t, d) where ⊕ denotes vector concatenation. Note that our joint model incorporates the same POS and syntactic features with the pipeline models. Under the joint model, the weights of POS and syntactic features, denoted by wpos⊕syn or wjoint , are simultaneously learned. Therefore, they can interact with each other to determine an optimal joint result. 4.1 Decoding Similar to (Li et al., 2011), we extend the parsing algorithm of (Carreras, 2007) using the idea of (Eisner, 2000) and propose a dynamic programming (DP) based decoding algorithm for our joint model. Figure 3 illustrates the basic DP structures and operations. The key idea is to augment the basic DP structures in the parsing algorithm (namely spans) with a few POS tags. A span means a partially built structure spanning a sub-sentence. For example, the leftside span in Figure 3(a), which is called an incomplete span and is denoted by I(h,m,l)(t h ,t m ) , represents a partial tree spanning wh...w m with wh being tagged as"
C12-1103,D10-1004,0,0.0469191,"Missing"
C12-1103,P05-1012,0,0.236675,"ctors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three c"
C12-1103,N07-1051,0,0.0863404,"Missing"
C12-1103,W96-0213,0,0.889178,"NG 2012, Mumbai, December 2012. 1681 1 Introduction Given an input sentence of n words, denoted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMO"
C12-1103,D10-1001,0,0.012255,"nts out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating tw"
C12-1103,D08-1016,0,0.0105209,"e parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction"
C12-1103,D09-1058,0,0.154484,"Missing"
C12-1103,P08-1101,0,0.469583,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,D08-1059,0,0.230453,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,J11-1005,0,0.00551549,"Missing"
C12-1103,P11-2033,0,0.150722,"Missing"
C12-1188,J04-4004,0,0.0330972,"rithm that is first proposed by (Wolpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Depen"
C12-1188,D12-1133,0,0.0193385,"out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS taggin"
C12-1188,D07-1101,0,0.172376,"nd the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error pattern"
C12-1188,A00-2018,0,0.0563471,"olpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Dependency parsing A dependency tree for a"
C12-1188,P05-1022,0,0.0310453,"specially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat"
C12-1188,P12-2003,1,0.910792,"stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing acc"
C12-1188,W02-1001,0,0.0331342,"odifier word (or child) is w m . The task of dependency parsing is to find an optimum dependency tree d for the input sentence x. Generally, the POS tag sequence of the sentence t = t 1 · · · t n (where t i ∈ T, 1 ≤ i ≤ n, T is the POS tag set) is taken as an input for dependency parsing, which is determined by the task of POS tagging, thus forming a pipeline model of the two tasks. POS tagging is a typical sequence labeling problems which can be resolved by algorithms such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and averaged perceptron (Collins, 2002). The goal of joint models of the two tasks is to find an optimum dependency tree and an optimum POS ˆ for x concurrently. tag sequence (ˆt, d) 3.1 Graph-based Joint Model The graph-based joint model is first proposed by (Li et al., 2011b). Such a model is extended from a graph-based model for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). In the model, the score of a dependency tree along with POS tags on each node is factored into scores of small parts. (Li et al., 2011b) have introduced several different graph-based joint model"
C12-1188,W03-0433,0,0.0480722,"Missing"
C12-1188,I11-1136,0,0.107354,"tem: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependen"
C12-1188,P10-1110,0,0.0722255,"cy Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagg"
C12-1188,P10-1001,0,0.0624319,"ansition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different"
C12-1188,I11-1171,1,0.250948,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D11-1109,1,0.320009,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D08-1017,0,0.11201,"he performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic con"
C12-1188,P05-1012,0,0.376473,"dels including the guided graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al.,"
C12-1188,J11-1007,0,0.0608408,"ey propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parse"
C12-1188,E06-1011,0,0.241065,"ed graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certa"
C12-1188,J08-4003,0,0.161113,"and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG pars"
C12-1188,P08-1108,0,0.37237,"odel can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that,"
C12-1188,P06-1055,0,0.370553,"that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent"
C12-1188,N07-1051,0,0.546016,"of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can als"
C12-1188,C10-1120,0,0.0226267,"ndency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) p"
C12-1188,P11-1139,0,0.0211513,"ing accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from grap"
C12-1188,P12-1026,0,0.110473,"uent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing accuracy by a PCFG parser. Similarly, (Sun and Uszkoreit, 2012) exploited a PCFG parser to enhance Chinese POS tagging. Thus it is reasonable to investigate the performance of constituent-based joint models and to improve the performance of joint Chinese POS tagging and dependency parsing by a constituent-based joint model. In this paper, first we study the integration of a graph-based joint model (JGraph) and a transition-based joint model (JTrans) by stacked learning. The stacked learning is implemented using a two-level architecture, where the level-0 consists of one or more predictors of which the results are exploited as input to enhance the level-1"
C12-1188,W03-3023,0,0.373203,"er. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 201"
C12-1188,D08-1059,0,0.0212821,"odels, the probability should be much lower with a negative impact. We name the features related with the consistency of the two level-0 models as guided consistent features. Table 2 lists the guided consistent features used in this work. Both the guided graph-based joint model and the guided transition-based joint model are considered. 6 6.1 Experiments Experimental Settings We use CTB5.1 to conduct our experiments. Following the works of (Li et al., 2011b) and (Hatori et al., 2011), we use the standard split of CTB5.1 described in (Duan et al., 2007) and the conversion rules of CS-to-DS in (Zhang and Clark, 2008). We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy or UAS), root accuracy and complete 3078 The Guided Graph-based Joint Model: JGraph(JTrans, JConst) pos dep JTrans JConst JTrans JTrans {Whether ˆt m is identical to ˆt m ?} ⊗{ˆt m ◦ t m , ˆt m ◦ wm ◦ t m } ˆ JTrans and d ˆ JConst ?} ⊗ {Whether hx m is in {Whether the heads of m are identical in d ˆ JTrans ?} ⊗{t h , t m , t h ◦ t m } d The Guided Transition-based Joint Model: JTrans(JGraph, JConst) pos syn JGraph JConst JGraph JGraph {Whether ˆt m is"
C12-1188,P11-2033,0,0.0805884,"2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagging, (Sun and Uszkoreit,"
C12-1188,J03-4003,0,\N,Missing
C14-1075,C10-1011,0,0.0510777,"small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1 which defines the score of a dependency tree as: X X Score(x, d; w) = wdep · fdep (x, h, m) + wsib · fsib (x, h, s, m) (2) {(h,m)}⊆d {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are feature vectors corresponding to two kinds of subtree; wdep/sib are the feature weight vectors; the dot product gives the scores contributed by the corresponding subtrees. We adopt the state-of-the-art syntactic features proposed in Bohnet (2010). 3.2 Probabilistic CRF-based GParser Previous work on dependency parsing mostly adopts linear models and online perceptron training, which lack probabilistic explanations of dependency trees and likelihood of the training data. Instead, we build a log-linear CRF-based probabilistic dependency parser, which defines the probability of a dependency tree as: X exp{Score(x, d; w)} p(d|x; w) = ; Z(x; w) = exp{Score(x, d′ ; w)} (3) Z(x; w) ′ d ∈Y(x) where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x. 3.3 Likelihood and Gradient of Training Data with Ambigu"
C14-1075,D08-1092,0,0.0257035,"anually construct treebanks. Therefore, lots of recent work has been devoted to get help from bilingual constraints. The motivation behind are two-fold. First, a difficult syntactic ambiguity in one language may be very easy to resolve in another language. Second, a more accurate parser on one language may help an inferior parser on another language, where the performance difference may be due to the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard"
C14-1075,W10-2906,0,0.0518478,"ginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected dat"
C14-1075,D07-1101,0,0.304917,"re 1(c). We introduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into s"
C14-1075,N13-1006,0,0.0145899,"ojections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected data with in previous subsection for si"
C14-1075,P10-1003,1,0.918629,"the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard dependency trees. All above work shows significant performance gain ∗ Correspondence author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 783 Proceedings of COLING 2014, the 25th International Conference on Computational Lingu"
C14-1075,P10-1065,0,0.130343,"the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard dependency trees. All above work shows significant performance gain ∗ Correspondence author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 783 Proceedings of COLING 2014, the 25th International Conference on Computational Lingu"
C14-1075,D11-1005,0,0.191529,"Missing"
C14-1075,P11-1061,0,0.0488018,"data to learn model parameters. Second, their work measures the reliability of a projected dependencies only from the perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side marginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters fr"
C14-1075,D07-1098,0,0.0508734,"Missing"
C14-1075,P08-1109,0,0.0570156,"e second term within O(n3 ) time complexity, where n is the length of the input sentence. Similarly, the first term can be solved by running the InsideOutside algorithm in the constrained search space Fi . 3.4 Stochastic Gradient Descent (SGD) Training With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature weights w for our CRF-based baseline and bitext-enhanced parsers. We follow the implementation in CRFsuite.2 At each step, the algorithm approximates a gradient with a small subset of training examples, and then updates the feature weights. Finkel et al. (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. Moreover, it is very convenient to parallel SGD since computation among examples in the same batch is mutually independent. Once the feature weights w are learnt, we can parse the test data and try to find the optimal parse tree with the Viterbi decoding algorithm in O(n3 ) parsing time (Eisner, 2000; McDonald and Pereira, 2006). d∗ = arg max p(d|x; w) d∈Y(x) (8) 4 Experiments and Analysis To verify the effectiveness of our proposed method, we carry out experiments on Eng"
C14-1075,I11-1030,0,0.0271017,"s the reliability of a projected dependencies only from the perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side marginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Nase"
C14-1075,P09-1042,0,0.0398834,"ks as follows. First, we train a parser on sourcelanguage treebank, called a source parser. Then, we use the source parser to produce automatic syntactic structures on the source side of bitext. Next, with the help of automatic word alignments, we project the source-side syntactic structures into the target side. Finally, the target-side structures are used as goldstandard to train new parsing models of target language. Previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al., 2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to explore projected structures to further improve the performance of statistical parsers trained on full-scale monolingual treebanks (see Section 4.4 for performance comparison). The major issues for syntax projection are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isomorphism between langua"
C14-1075,D09-1127,0,0.0214918,"parser on one language may help an inferior parser on another language, where the performance difference may be due to the intrinsic complexity of languages or the scale of accessible labeled resources. Following the above research line, much effort has been done recently to explore bilingual constraints for parsing. Burkett and Klein (2008) propose a reranking based method for joint constituent parsing of bitext, which can make use of structural correspondence features in both languages. Their method needs bilingual treebanks with manually labeled syntactic trees on both sides for training. Huang et al. (2009) compose useful parsing features based on word reordering information in source-language sentences. Chen et al. (2010a) derive bilingual subtree constraints with auto-parsed source-language sentences. During training, both Huang et al. (2009) and Chen et al. (2010a) require bilingual text with target-language gold-standard dependency trees. All above work shows significant performance gain ∗ Correspondence author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creative"
C14-1075,P10-1002,0,0.205903,"Missing"
C14-1075,P10-1001,0,0.149505,"oduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees"
C14-1075,C12-1103,1,0.877484,"throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected data with in previou"
C14-1075,N06-1014,0,0.120943,"Missing"
C14-1075,P13-1105,0,0.0280801,"Missing"
C14-1075,E06-1011,0,0.785748,"cies, as illustrated in Figure 1(c). We introduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a depend"
C14-1075,P05-1012,0,0.13372,"any projected dependencies, as illustrated in Figure 1(c). We introduce three new dependencies to compose candidate heads for the unattached word “ 3 ”. Note that it is illegal to add the dependency “ 1 y 3 ” since it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method f"
C14-1075,D11-1006,0,0.166925,"Missing"
C14-1075,P12-1066,0,0.0414152,"Missing"
C14-1075,W03-3017,0,0.129615,"ected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1 which"
C14-1075,D09-1086,0,0.0212304,"n sourcelanguage treebank, called a source parser. Then, we use the source parser to produce automatic syntactic structures on the source side of bitext. Next, with the help of automatic word alignments, we project the source-side syntactic structures into the target side. Finally, the target-side structures are used as goldstandard to train new parsing models of target language. Previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al., 2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to explore projected structures to further improve the performance of statistical parsers trained on full-scale monolingual treebanks (see Section 4.4 for performance comparison). The major issues for syntax projection are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isomorphism between languages; 3) incomplete parse trees after project"
C14-1075,P11-2120,0,0.18399,"Missing"
C14-1075,W09-1104,0,0.530339,"projection typically works as follows. First, we train a parser on sourcelanguage treebank, called a source parser. Then, we use the source parser to produce automatic syntactic structures on the source side of bitext. Next, with the help of automatic word alignments, we project the source-side syntactic structures into the target side. Finally, the target-side structures are used as goldstandard to train new parsing models of target language. Previous work on syntax projection mostly focuses on unsupervised grammar induction where no labeled data exists for target language (Hwa et al., 2005; Spreyer and Kuhn, 2009; Ganchev et al., 2009; Liu et al., 2013). Smith and Eisner (2009) propose quasi-synchronous grammar for cross-lingual parser projection and assume the existence of hundreds of target language annotated sentences. Similar to our work in this paper, Jiang et al. (2010) try to explore projected structures to further improve the performance of statistical parsers trained on full-scale monolingual treebanks (see Section 4.4 for performance comparison). The major issues for syntax projection are 1) errors from the source-language parser and unsupervised word aligner; 2) intrinsic syntactic non-isom"
C14-1075,N12-1052,0,0.0614031,"Missing"
C14-1075,N13-1126,0,0.262539,"Missing"
C14-1075,P13-1106,0,0.0167994,"urns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om et al. (2012) derive crosslingual clusters from bitext to help delexicalized parser transfer. Naseem et al. (2012) propose selectively sharing to better explore multi-source transfer information. 6 In the previous draft of this paper, we directly use the projected data with in previous subsection for simplicity, and find t"
C14-1075,W03-3023,0,0.113252,"ince it would cross the projected dependency “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algor"
C14-1075,N01-1026,0,0.040213,"se these partial structures as extra training data to learn model parameters. Second, their work measures the reliability of a projected dependencies only from the perspective of alignment probability, while we adopt a probabilistic parsing model and use target-side marginal probabilities to throw away bad projections, which turns out effective in handling syntactic non-isomorphism and errors in word alignments and source-side parses. 5 Related work Cross-lingual annotation projection has been applied to many different NLP tasks to help processing resource-poor languages, such as POS tagging (Yarowsky and Ngai, 2001; Naseem et al., 2009; Das and Petrov, 2011) and named entity recognition (NER) (Fu et al., 2011). In another direction, much previous work explores bitext to improve monolingual NER performance based on bilingual constraints (Chen et al., 2010b; Burkett et al., 2010; Li et al., 2012a; Che et al., 2013; Wang et al., 2013). Based on a universal POS tag set (Petrov et al., 2011), McDonald et al. (2011) propose to train delexicalized parsers on resource-rich language for parsing resource-poor language without use of bitext (Zeman and Resnik, 2008; Cohen et al., 2011; Søgaard, 2011). T¨ackstr¨om e"
C14-1075,I08-3008,0,0.358131,"Missing"
C14-1075,P11-2033,0,0.0847711,"ncy “ 2 x 5 ”. Ç  Ç 785 Z  3 Dependency Parsing with Ambiguous Labelings In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. Graph-based methods view the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while transition-based methods try to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 3.1 Graph-based Dependency Parser (GParser) We adopt the graph-based paradigm because it allows us to elegantly derive our CRF-based probabilistic parser, which is required to compute the marginal probabilities of dependencies and likelihood of both manually labeled data and unannotated bitext with ambiguous labelings. The graph-based method factors the score of a dependency tree into scores of small subtrees p. X Score(x, d; w) = w · f (x, d) = Score(x, p; w) (1) p⊆d We adopt the second-order model of McDonald and Pereira (2006) as our core parsing algorithm,1 which defines the score of a d"
C14-1075,petrov-etal-2012-universal,0,\N,Missing
C16-1202,P15-1098,0,0.0299232,"Figure 4: MAE vs number of training reviews per user (Amazon) identify the feature words directly extracted from comments. Recently some researchers try to learn latent aspects from comments (Wu and Ester, 2015). Wang et al. (2011) builds a regression model over the topic results from LDA. Their approach does not try to identify user profiles, and thus only applicable to score prediction on the text comments. Wang and Blei (2011) combine the merits of traditional collaborative filtering and topic modeling which provides a latent structure to recommend scientific articles in citation networks. Tang et al. (2015) learn representations of users and item for sentiment classification. Although similar concepts of user and item representations are used in (Tang et al., 2015), there are two major differences. Firstly, the problem we try to solve is completely different: they work on review sentiment classification, while our work focuses on score prediction without text. Secondly, the models are different: their approach includes user/product representation in CNN to enhance score prediction accuracy over text review, while out proposal attempts to maximize the likelihood of individual words. McAuley and L"
C18-1183,P16-1039,0,0.0252383,"utilize partial annotation learning and reinforcement learning to perform new-type named entity recognition in new domains. Several previous studies relevant to our approach have been conducted. NER. Most early studies treated NER task as the sequence labeling problem based on a large annotated corpus with supervised methods, such as HMM, MEMM (Hai and Ng, 2002) and CRF (Lafferty et al., 2001). Recently, neural networks have been explored by researchers (Collobert et al., 2011; Lample et al., 2016), and applied to reduce the weakness of feature sparsity problem and heavy feature engineering (Cai and Zhao, 2016). Those models have the similar architecture for decoding and feature extraction, which is chosen as our baseline model. In order to overcome the challenge of data deficient, some approaches based on weakly supervised learning (Nadeau et al., 2006; Riloff and Jones, 1999) have been proposed and successfully expand training data and feature space. However, it is difficult to implement these methods on Chinese tasks because of the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166"
C18-1183,J81-4005,0,0.737702,"Missing"
C18-1183,W10-4204,0,0.0186873,"or the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE data. Reinforcement Learning. In recent years, reinforcement learning has become an issue in research, and applied successfully to many tasks. In text generation community, a deep Q-learning is served by Guo (2015) as generative model to improve the seq2seq model, which completes the process of decoding by Iteration. Li et al. (2016a) show how to apply deep reinforcement learning to model future reward in chatbot dialogue and capture the impact of this conversation in the future. Dethlefs (2010) aim to optimize the integration of NLG tasks that are inherently different in nature by learning a generation policy with reinforcement learning. For computer vision, Yeung et al. (2017) propose a reinforcement learning-based formulation select the right examples for training a classifier from noisy web search results. To more fine-grainedly select high-quality training sentences from noisy data, Feng et al. (2018) train an instance selector based on a policy function with reinforcement learning, which is inspirational to our model. 6 Conclusion This paper presents a new approach to utilize t"
C18-1183,N09-1037,0,0.0293189,"ficient, some approaches based on weakly supervised learning (Nadeau et al., 2006; Riloff and Jones, 1999) have been proposed and successfully expand training data and feature space. However, it is difficult to implement these methods on Chinese tasks because of the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166 Under reasonable assumptions, OOV features should not be forced into certain tag. What’s more, joint models have also obtained great performance (Qian and Liu, 2013; Finkel and Manning, 2009). Learning from PAs. Learning from PAs has always been an attractive idea, since it usually requires much less or even none human annotation effort to obtain partially annotated data than fully annotated data, especially for complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et"
C18-1183,C02-1025,0,0.205429,"Missing"
C18-1183,P13-1075,0,0.0718746,"Missing"
C18-1183,N16-1030,0,0.625965,"-generated annotations. In experiments, we create two datasets for Chinese named entity recognition in two domains with the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets. 1 Introduction In recent years, deep learning approaches have achieved great progress in the task of named entity recognition (NER) (Collobert et al., 2011; Chiu and Nichols, 2015). The standardized approach is that using BiLSTMs for encoding and then applying CRF for jointly label decoding (Huang et al., 2015; Lample et al., 2016). In addition, BiLSTMs and CNNs are employed to model character- or word-level representations (Ma and Hovy, 2016). Most previous studies on NER focus on a certain set of predefined NER types, such as organization, location, person, date, and so on, where a certain amount of labeled data is provided to train the models. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new N"
C18-1183,W06-0115,0,0.0172484,"ontains 2,400 sentences tagged by annotators. We split the data into three sets: 1,200 sentences for training, 400 for dev, and 800 for testing. We collect a list of entities to construct dictionary from the training data. To reduce the effect of ambiguities, we remove the entry that belongs to more than one type, or it is a number or single character. Finally, the dictionary has 927 entries (included as EC.dic in supplementary materials). We perform distant supervision on raw data to obtain 2,500 sentences. NEWS: For news domain, we use a NER data from MSRA, which was used in Sighan-bakeoff (Levow, 2006). We only test our systems on the type of PERSON. We randomly select 3,000 sentences as training dataset, 3,328 as dev data, and 3,186 as testing data. The rest set is used as raw data, having 36,602 sentences. We collect a list of person names from the training data. To increase the coverage, we add an additional names to the list. Finally, the list has 71,664 entries (included as NEWS.dic in supplementary materials). We perform distant supervision on raw data to obtain 3,722 sentences. Embedding: In our approach, we need to map Chinese characters into vector representations by the lookup tab"
C18-1183,C12-2067,0,0.0237183,"variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166 Under reasonable assumptions, OOV features should not be forced into certain tag. What’s more, joint models have also obtained great performance (Qian and Liu, 2013; Finkel and Manning, 2009). Learning from PAs. Learning from PAs has always been an attractive idea, since it usually requires much less or even none human annotation effort to obtain partially annotated data than fully annotated data, especially for complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further im"
C18-1183,D16-1127,0,0.0115603,"e labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE data. Reinforcement Learning. In recent years, reinforcement learning has become an issue in research, and applied successfully to many tasks. In text generation community, a deep Q-learning is served by Guo (2015) as generative model to improve the seq2seq model, which completes"
C18-1183,D16-1072,1,0.845342,"e labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE data. Reinforcement Learning. In recent years, reinforcement learning has become an issue in research, and applied successfully to many tasks. In text generation community, a deep Q-learning is served by Guo (2015) as generative model to improve the seq2seq model, which completes"
C18-1183,D14-1093,0,0.0748401,"complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produc"
C18-1183,P16-1101,0,0.146503,"ith the help of distant supervision. The experimental results show that the proposed approach obtains better performance than the comparison systems on both two datasets. 1 Introduction In recent years, deep learning approaches have achieved great progress in the task of named entity recognition (NER) (Collobert et al., 2011; Chiu and Nichols, 2015). The standardized approach is that using BiLSTMs for encoding and then applying CRF for jointly label decoding (Huang et al., 2015; Lample et al., 2016). In addition, BiLSTMs and CNNs are employed to model character- or word-level representations (Ma and Hovy, 2016). Most previous studies on NER focus on a certain set of predefined NER types, such as organization, location, person, date, and so on, where a certain amount of labeled data is provided to train the models. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new NER type, but small-scale data is available at some time. As an alternative solution, distant supervision can autom"
C18-1183,D14-1097,0,0.0461429,"Missing"
C18-1183,P09-1113,0,0.228853,"beled data is provided to train the models. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new NER type, but small-scale data is available at some time. As an alternative solution, distant supervision can automatically generate large-scale labeled data for new-type NER without human-cost. The idea of distant supervision has widely used in the task of relation extraction (Mintz et al., 2009; Riedel et al., 2010; Zeng et al., 2015). For relation extraction, at first we have a knowledge base. If two entities e1 and e2 have relation r according to the knowledge base, then we populate this knowledge and assume the relation between e1 and e2 is r in the sentences that contain the both entities. In this way, we can produce a lot of labeled data for model training. Similarly, in our task, we first acquire a dictionary containing a list of the new-type entities. Then, we automatically generate large-scale labeled data by assuming that each entity mention in a sentence is a positive inst"
C18-1183,D15-1064,0,0.511038,"where “XX” is the type of entities. 2.2 The Baseline LSTM-CRF Given a sentence x = c1 c2 · · · cn , the goal is to assign an unique tag yi for Chinese character ci in the sentence. In general, the model predicts the entities in the sentence x by estimating the probability p(y|x), where y is a possible label sequence for sentence x. The final output ymax of the system for one sentence is the label sequence with the maximum probability. Here, we present a new NE tagger based on the LSTM-CRF model of Lample et al. (2016), which achieves the state-of-the-art performance in the NER task. Following Peng and Dredze (2015a), we represent Chinese characters as vectors and feed them into BiLSTM layer in the Chinese NER task. The left part of Figure 2 shows the framework of our baseline LSTM-CRF based NE tagger. The input layer. For each input sentence x = c1 c2 · · · cn , we map serialized characters into a list of vectors x1 x2 · · · xn with an embedding layer including a lookup table as its key parameter. Following Lample et al. (2016), the lookup table is initialized with embeddings pre-trained on a large-scale raw corpus and is further fine-tuned during our training process. The BiLSTM layer. With vector seq"
C18-1183,P13-2110,0,0.0244274,"challenge of data deficient, some approaches based on weakly supervised learning (Nadeau et al., 2006; Riloff and Jones, 1999) have been proposed and successfully expand training data and feature space. However, it is difficult to implement these methods on Chinese tasks because of the lack of morphological variations such as capitalization and in particular the uncertainty in word segmentation, and it may cause large number of matching errors. 2166 Under reasonable assumptions, OOV features should not be forced into certain tag. What’s more, joint models have also obtained great performance (Qian and Liu, 2013; Finkel and Manning, 2009). Learning from PAs. Learning from PAs has always been an attractive idea, since it usually requires much less or even none human annotation effort to obtain partially annotated data than fully annotated data, especially for complex tasks like sequence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to thei"
C18-1183,C08-1113,0,0.375003,"ork shoes)” is a product, but only the first two characters “工装(fatigue clothes)” are matched by the dictionary because “工装鞋(work shoes)” is not included in the dictionary. Obviously, such false labeled examples certainly provide wrong supervision during model training if we directly use the automatically generated data. In this paper, we propose an approach to handle the two problems of distantly supervised NER data. As for the incomplete annotation problem, we treat the data as partially annotated data based on the extended CRF-PA model that can directly learn from partial annotations (PA) (Tsuboi et al., 2008). The noisy annotation problem is also ubiquitous in distantly supervised for relation extraction, and researchers try to address this issue by using reinforcement learning (RL) technology to select positive instances (Feng et al., 2018). Inspired of their work, we design an instance selector to obtain clean instances from distantly supervised NER data. In summary, we make the following contributions: • We propose a novel approach for new-type named entity recognition, which firstly combines the advantages of both partial annotation learning and reinforcement learning, to handle the problems o"
C18-1183,D14-1010,0,0.0703814,"quence labeling. Li et al. (2012) propose to only manually annotate the most uncertain word boundaries in a sentence for Chinese word segmentation in order to reduce annotation cost. Tsuboi et al. (2008) extend the standard CRF to directly learn from incomplete annotations for sequence labeling tasks. This work refers to their model as CRF-PA. Jiang et al. (2013) propose to derive segmentation boundaries from implicit information encoded in web texts, such as anchor texts and punctuation marks, and use them as partially labeled training data in Chinese word segmentation. Liu et al. (2014) and Yang and Vozila (2014) further improve the work of Jiang et al. (2013) by employing the more sophisticated CRF-PA model. Marcheggiani and Arti`eres (2014) systematically compare a dozen uncertainty metrics in token-wise active learning with CRF-PA for several sequence labeling tasks. Li et al. (2016b) propose a coupled sequence labeling approach for exploiting heterogeneous data by treating the single-sided annotations as PAs for the task of joint word segmentation and POS tagging. In this work, we for the first time apply the CRF-PA model to NER, and employ distance supervision to produce partially annotated NE da"
C18-1183,D15-1203,0,0.0506862,"s. However, different applications require particular entity types, such as “Brand” and “Product” in Ecommerce domain, and “Company” for finance industry. Considering the high cost of human annotation, it may not be feasible to annotate large amounts of labeled data for each new NER type, but small-scale data is available at some time. As an alternative solution, distant supervision can automatically generate large-scale labeled data for new-type NER without human-cost. The idea of distant supervision has widely used in the task of relation extraction (Mintz et al., 2009; Riedel et al., 2010; Zeng et al., 2015). For relation extraction, at first we have a knowledge base. If two entities e1 and e2 have relation r according to the knowledge base, then we populate this knowledge and assume the relation between e1 and e2 is r in the sentences that contain the both entities. In this way, we can produce a lot of labeled data for model training. Similarly, in our task, we first acquire a dictionary containing a list of the new-type entities. Then, we automatically generate large-scale labeled data by assuming that each entity mention in a sentence is a positive instance of the corresponding type according"
D11-1109,D07-1101,0,0.442565,"d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts. Their experiments on English and Czech show that Model 1 and Model 2 obtain nearly the same parsing ac"
D11-1109,P05-1022,0,0.355797,"Missing"
D11-1109,C10-1019,1,0.878284,"Missing"
D11-1109,D07-1022,0,0.0164053,"Missing"
D11-1109,W02-1001,0,0.285248,"et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently"
D11-1109,N09-1046,0,0.0525872,"Missing"
D11-1109,C96-1058,0,0.797918,"me with Model 1 in Koo and Collins (2010), but without using grand-sibling features.2 • The third-order model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for"
D11-1109,N09-1037,0,0.0607158,"Missing"
D11-1109,P08-1043,0,0.0977694,"Missing"
D11-1109,P10-1110,0,0.400604,"tructures. On the contrary, joint models of version 2 can incorporate both aforementioned feature sets, but have higher complexity. These two versions of models will be thoroughly compared in the experiments. 1185 We then define the allowable candidate POS tags of the word wi to be Ti (x) = {t : t ∈ T , P (ti = t|x) ≥ λt × pmaxi (x)} where λt is the pruning threshold. Ti (x) is used to constrain the POS search space by replacing T in Algorithm 1. 5 Experiments We use the Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005). Following the setup of Duan et al. (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 11371147) sets. We use the head-finding rules of Zhang and Clark (2008b) to turn the bracketed sentences into dependency structures. We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy), root accuracy and complete match rate (all excluding punctuation) . For the averaged training, we train each model for 15 iterations and select the parameters that perform best on the development"
D11-1109,P08-1102,0,0.111307,"Missing"
D11-1109,P10-1001,0,0.270992,"2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming decoding, it can efficiently find an optimal tree in a huge search space. In a graph-based model, the score"
D11-1109,P09-1058,0,0.227854,"Missing"
D11-1109,P03-1056,0,0.0427835,"where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 20"
D11-1109,P10-1113,0,0.0383995,"Missing"
D11-1109,C10-1080,0,0.0394632,"Missing"
D11-1109,E06-1011,0,0.689881,"the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts"
D11-1109,P05-1012,0,0.930686,"der model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 ac"
D11-1109,J08-4003,0,0.155196,"Missing"
D11-1109,N07-1051,0,0.116748,"Missing"
D11-1109,W96-0213,0,0.524192,"d as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score o"
D11-1109,D10-1001,0,0.0739428,"POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002"
D11-1109,P07-1096,0,0.0207597,"s determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming de"
D11-1109,W08-2121,0,0.0983419,"Missing"
D11-1109,P09-1055,0,0.0150803,"Missing"
D11-1109,C10-1135,0,0.0414692,"Missing"
D11-1109,P08-1101,0,0.518379,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,D08-1059,0,0.828377,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,W09-1201,0,\N,Missing
D16-1072,P09-1059,0,0.123204,", and syntactic structures (Xue et al., 2005; Xia, 2000), whereas People’s Daily corpus (PD)1 is a large-scale corpus annotated with words and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s Daily newspaper (Yu et al., 2003). Table 1 gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). Li et al. (2015) propose a coupled sequence labeling approach that can directly learn and predict two heterogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag 1 http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 753 Proceedings of the 2016 Conference on Empi"
D16-1072,P13-1075,0,0.31356,"Missing"
D16-1072,P10-1001,0,0.055785,"Missing"
D16-1072,P12-1071,1,0.860867,"where such heterogeneous resources are ubiquitous due to historical reasons. Jiang et al. (2009) first propose the guide-feature approach, which is similar to stacked learning (Nivre and McDonald, 2008), for joint WS&POS tagging on CTB and PD. Sun and Wan (2012) further extend the guide-feature method and propose a more complex sub-word stacking approach. Qiu et al. (2013) propose a linear coupled model similar to that of Li et al. (2015). The key difference is that the model of Qiu et al. (2013) only uses separate features, while Li et al. (2015) and this work explore joint features as well. Li et al. (2012a) apply the guide-feature idea to dependency parsing on CTB and PD. Zhang et al. (2014a) extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however assumes the existence of training data with both-side annotations. Our context-aware pruning approach is similar to coarse-to-fine pruning in parsing community (Koo and Collins, 2010; Rush and Petrov, 2012), which is a useful technique that allows us to use very complex parsing models without too much efficiency cost. The idea is first to use a simple and basic off-shelf"
D16-1072,C12-1103,1,0.830618,"where such heterogeneous resources are ubiquitous due to historical reasons. Jiang et al. (2009) first propose the guide-feature approach, which is similar to stacked learning (Nivre and McDonald, 2008), for joint WS&POS tagging on CTB and PD. Sun and Wan (2012) further extend the guide-feature method and propose a more complex sub-word stacking approach. Qiu et al. (2013) propose a linear coupled model similar to that of Li et al. (2015). The key difference is that the model of Qiu et al. (2013) only uses separate features, while Li et al. (2015) and this work explore joint features as well. Li et al. (2012a) apply the guide-feature idea to dependency parsing on CTB and PD. Zhang et al. (2014a) extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however assumes the existence of training data with both-side annotations. Our context-aware pruning approach is similar to coarse-to-fine pruning in parsing community (Koo and Collins, 2010; Rush and Petrov, 2012), which is a useful technique that allows us to use very complex parsing models without too much efficiency cost. The idea is first to use a simple and basic off-shelf"
D16-1072,P15-1172,1,0.724149,"Missing"
D16-1072,D14-1093,0,0.302157,"Missing"
D16-1072,P08-1108,0,0.0427893,"ly corpus (PD)1 is a large-scale corpus annotated with words and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s Daily newspaper (Yu et al., 2003). Table 1 gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). Li et al. (2015) propose a coupled sequence labeling approach that can directly learn and predict two heterogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag 1 http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 753 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 753–762, c Austin, Texas, Novembe"
D16-1072,D13-1062,0,0.2279,"d stacking approach of Sun and Wan (2012) can be understood as a more complex 760 A lot of research has been devoted to design an effective way to exploit non-overlapping heterogeneous labeled data, especially in Chinese language processing, where such heterogeneous resources are ubiquitous due to historical reasons. Jiang et al. (2009) first propose the guide-feature approach, which is similar to stacked learning (Nivre and McDonald, 2008), for joint WS&POS tagging on CTB and PD. Sun and Wan (2012) further extend the guide-feature method and propose a more complex sub-word stacking approach. Qiu et al. (2013) propose a linear coupled model similar to that of Li et al. (2015). The key difference is that the model of Qiu et al. (2013) only uses separate features, while Li et al. (2015) and this work explore joint features as well. Li et al. (2012a) apply the guide-feature idea to dependency parsing on CTB and PD. Zhang et al. (2014a) extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however assumes the existence of training data with both-side annotations. Our context-aware pruning approach is similar to coarse-to-fine pr"
D16-1072,P02-1035,0,0.214026,"Missing"
D16-1072,N12-1054,0,0.0575587,"Missing"
D16-1072,P12-1025,0,0.0799453,"ctures (Xue et al., 2005; Xia, 2000), whereas People’s Daily corpus (PD)1 is a large-scale corpus annotated with words and POS tags, containing about 300 thousand sentences from the first half of 1998 of People’s Daily newspaper (Yu et al., 2003). Table 1 gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource (Jiang et al., 2009; Sun and Wan, 2012), which is similar to stacked learning (Nivre and McDonald, 2008). Li et al. (2015) propose a coupled sequence labeling approach that can directly learn and predict two heterogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag 1 http://icl.pku.edu.cn/icl_groups/ corpustagging.asp 753 Proceedings of the 2016 Conference on Empirical Methods in Nat"
D16-1072,N13-1126,0,0.0214313,"Missing"
D16-1072,D14-1010,0,0.214771,"Missing"
D16-1072,P08-1101,0,0.0348463,"plates, and return local feature vectors for tagging wi−1 as t′ and wi as t. Traditional single-side tagging models can only exploit a single set of separate features fsep_a (.) or fsep_b (.). In contrast, the coupled model makes use of all three sets of features. Li et al. (2015) demonstrate that the joint features fjoint (.) capture the implicit mappings between heterogeneous annotations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features. For the feature templates, we follow Li et al. (2015) and adopt those described in Zhang and Clark (2008) for POS tagging, and use those described in Zhang et al. (2014b) for joint WS&POS tagging. derivations are as follows: ∂log Z(x, S; θ) ∂θ ∑ ∂ t∈S eScore(x,t;θ) 1 = × Z(x, S; θ) ∂θ ( ) ∑ eScore(x,t;θ) ∂Score(x, t; θ) = × Z(x, S; θ) ∂θ t∈S ∑ = p(t|x, S; θ) × f(x, t) (5) t∈S 2.2 =Et|x,S;θ [f(x, t)] Learn from Incomplete Data The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, Li et al. (2015) first concatenate a single-side tag with many possible second-side tags, and the"
D16-1072,C14-1051,0,0.0481396,"wi as t. Traditional single-side tagging models can only exploit a single set of separate features fsep_a (.) or fsep_b (.). In contrast, the coupled model makes use of all three sets of features. Li et al. (2015) demonstrate that the joint features fjoint (.) capture the implicit mappings between heterogeneous annotations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features. For the feature templates, we follow Li et al. (2015) and adopt those described in Zhang and Clark (2008) for POS tagging, and use those described in Zhang et al. (2014b) for joint WS&POS tagging. derivations are as follows: ∂log Z(x, S; θ) ∂θ ∑ ∂ t∈S eScore(x,t;θ) 1 = × Z(x, S; θ) ∂θ ( ) ∑ eScore(x,t;θ) ∂Score(x, t; θ) = × Z(x, S; θ) ∂θ t∈S ∑ = p(t|x, S; θ) × f(x, t) (5) t∈S 2.2 =Et|x,S;θ [f(x, t)] Learn from Incomplete Data The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, Li et al. (2015) first concatenate a single-side tag with many possible second-side tags, and then use the set of bundled tags as possibly-correct references du"
D16-1072,P14-1125,0,0.062006,"wi as t. Traditional single-side tagging models can only exploit a single set of separate features fsep_a (.) or fsep_b (.). In contrast, the coupled model makes use of all three sets of features. Li et al. (2015) demonstrate that the joint features fjoint (.) capture the implicit mappings between heterogeneous annotations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features. For the feature templates, we follow Li et al. (2015) and adopt those described in Zhang and Clark (2008) for POS tagging, and use those described in Zhang et al. (2014b) for joint WS&POS tagging. derivations are as follows: ∂log Z(x, S; θ) ∂θ ∑ ∂ t∈S eScore(x,t;θ) 1 = × Z(x, S; θ) ∂θ ( ) ∑ eScore(x,t;θ) ∂Score(x, t; θ) = × Z(x, S; θ) ∂θ t∈S ∑ = p(t|x, S; θ) × f(x, t) (5) t∈S 2.2 =Et|x,S;θ [f(x, t)] Learn from Incomplete Data The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, Li et al. (2015) first concatenate a single-side tag with many possible second-side tags, and then use the set of bundled tags as possibly-correct references du"
D17-1072,D15-1141,0,0.0233762,"1 gives an example sentence segmented in different guidelines. Meanwhile, WS approaches gradually evolve from maximum matching based on lexicon dictionaries (Liu and Liang, 1986), to path searching from segmentation graphs based on language modeling scores and other statistics (Zhang and Liu, 2002), to character-based sequence labeling (Xue, 2003), to shift-reduce incremental parsing (Zhang and Clark, 2007). Recently, neural network models have also achieved success by effectively learning representation of characters and contexts (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015; Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016). This work proposes and addresses multi-grained WS (MWS). First, we build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings. 1 Introduction To date, all the labeled datasets adopt the single-granularity formalization, an"
D17-1072,D13-1061,0,0.0243576,"3), and Penn Chinese Treebank (CTB) (Xue et al., 2005). Table 1 gives an example sentence segmented in different guidelines. Meanwhile, WS approaches gradually evolve from maximum matching based on lexicon dictionaries (Liu and Liang, 1986), to path searching from segmentation graphs based on language modeling scores and other statistics (Zhang and Liu, 2002), to character-based sequence labeling (Xue, 2003), to shift-reduce incremental parsing (Zhang and Clark, 2007). Recently, neural network models have also achieved success by effectively learning representation of characters and contexts (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015; Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016). This work proposes and addresses multi-grained WS (MWS). First, we build a large-scale pseudo MWS dataset for model training and tuning by leveraging the annotation heterogeneity of three SWS datasets. Then we manually annotate 1,500 test sentences with true MWS annotations. Finally, we propose three benchmark approaches by casting MWS as constituent parsing and sequence labeling. Experiments and analysis lead to many interesting findings. 1 Introduction To date, all the lab"
D19-1541,W13-3820,0,0.0306716,"rser as external inputs for the basic SRL model. Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually an"
D19-1541,C10-3009,0,0.053222,"Missing"
D19-1541,C18-1233,0,0.257748,"Missing"
D19-1541,N19-1423,0,0.212989,"itecture of He et al. (2018a) as our basic SRL model with a modification on the argument representation. The architecture of the basic SRL module is shown in the right part of Figure 2, and we will describe it in the following subsections. 2.1 Input Layer Following He et al. (2018a); Li et al. (2019), we employ CNNs to encode Chinese characters for each word wi into its character representation, denoted as repchar . Then, we concatenate repchar i i with the word embedding embword to represent i the word-level features as our basic model input. In addition, we also employ BERT representations (Devlin et al., 2019) to boost the performance of our baseline model, which we denote as repBERT . Formally, the input representation of i wi is: xi = repchar ⊕ embword ⊕ repBERT i i i (2) , where ⊕ is the concatenation operation. Our basic SRL model and BERT-enhanced baseline depend on whether including the BERT representation repBERT or not. i 2.2 Motivated by the recently presented span-based models (He et al., 2018a; Li et al., 2019) for Y BiLSTM Encoder Over the input layer, we employ the BiLSTMs with highway connections (Srivastava et al., 2015; 5383 Argument Representation MLP Scorer X Predicate Representat"
D19-1541,P11-2051,0,0.0287699,"nchmarks of Chinese Proposition Bank 1.0 and CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in"
D19-1541,P18-2058,0,0.193907,"NLP tasks, such as information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the p"
D19-1541,P17-1044,0,0.273409,"买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identification sub-tasks of SRL. Compared with the larg"
D19-1541,P18-1192,0,0.416408,"NLP tasks, such as information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the p"
D19-1541,C16-1038,0,0.0207963,"gs. Xia et al. (2017) propose a progressive model to learn and transfer knowledge from heterogeneous SRL data. The above works are all focus on the span-based Chinese SRL, and we compare with their results in Table 2. Different from them, we propose a MTL framework to integrate implicit syntactic representations into a simple unified model on both span-based and wordbased SRL, achieving substantial improvements. In addition to the hard parameter sharing strategy that we discuss in Section 3.2, partial parameter sharing strategy is also a commonly studied approach in MTL and domain adaptation. Kim et al. (2016) introduce simple neural extensions of feature argumentation by employing a global LSTM used across all domains and independent LSTMs used within individual domains. Peng 5389 et al. (2017) explore a multitask learning approach which shares parameters across formalisms for semantic dependency parsing. In addition, Peng et al. (2018) present a multi-task approach for frame-semantic parsing and semantic dependency parsing with latent structured variables. 7 Conclusion This paper proposes a syntax-aware MTL framework to integrate implicit syntactic representations into a simple unified SRL model."
D19-1541,D17-1018,0,0.0257275,"e development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achieving significant improvements. Inspired by the span-based model proposed by Lee et al. (2017) for coreference resolution, He et al. (2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani et al. (2017) propose a simple and fast syntax-agnostic model with rich input representations. Cai et al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based mode"
D19-1541,C10-1081,0,0.0887575,"Experiments on the benchmarks of Chinese Proposition Bank 1.0 and CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an exampl"
D19-1541,K17-1041,0,0.331544,"nese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018) mainly focus on building more powerful models or exploring the usage of external knowledge on this dataset. Inspired by the development of neural models and exploration of syntactic information, this paper proposes a MTL framework to extract syntactic representations as the external input features for the simple unified SRL model. The contributions of our paper are three-folds: 1. We introduce a simple unified model for span-based and word-based Chinese SRL. 2. We propose a MTL framework to extract implicit syntactic representations for SRL model, which si"
D19-1541,D17-1159,0,0.0627452,"2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani et al. (2017) propose a simple and fast syntax-agnostic model with rich input representations. Cai et al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements. Marcheggiani and Titov (2017) employ the graph convolutional neural networks on top of the BiLSTM encoder to encode syntactic information. He et al. (2018b) propose a k-th order argument pruning algorithm based on systematic dependency trees. Strubell et al. (2018) propose a self-attention based neural MTL model which incorporate dependency parsing as a auxiliary task for SRL. Swayamdipta et al. (2018) propose a MTL framework using hard parameter strategy to incorporate constituent parsing loss into semantic tasks, i.e. SRL and coreference resolution, which outperforms their baseline by +0.8 F1 score. Xia et al. (2019) in"
D19-1541,D09-1153,0,0.17987,"fectively improve the performance (p &lt; 0.0001), no matter whether employ the BERT representations or not. Especially, our proposed framework (IIR) consistently outperforms the hard parameter sharing strategy. So we only report the results of our proposed framework in later experiments. Our final results outperforms the best previous model (Xia et al., 2017) by 7.87 and 4.24 F1 scores with BERT representations or not, respectively. Table 3 shows the results of our framework in the end-to-end setting. To our best knowledge, we are the first to present the results of end-toMethods Previous Works Sun et al. (2009) Wang et al. (2015b) Sha et al. (2016) Xia et al. (2017) Ours Baseline Baseline + Dep (HPS) Baseline + Dep (IIR) Baseline + BERT Baseline + BERT + Dep (HPS) Baseline + BERT + Dep (IIR) F1 74.12 77.59 77.69 79.67 80.48 83.51 83.91 86.62 87.03 87.54 Table 2: Results and comparison with previous works on CPB1.0 test set. end on the CPB1.0 dataset. We achieve the result of 85.57 in F1 score, which is a strong baseline for later works. It is clear that our framework can still achieve better results compared with the strong baseline, which employs BERT representations as the external input. Results"
D19-1541,W04-2705,0,0.0823318,"ucted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role labeling (SRL) is a fundamental and important task in natural language processing (NLP), which aims to identify the semantic structure (Who did what to whom, when and where, etc.) of each given predicate in a sentence. Semantic knowledge has been widely exploited in many down-stream NLP tasks, such"
D19-1541,W08-2121,0,0.176317,"Missing"
D19-1541,D18-1191,0,0.0754045,"ity, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achieving significant improvements. Inspired by the span-based model proposed by Lee et al. (2017) for coreference resolution, He et al. (2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani et al. (2017) propose a simple and fast syntax-agnostic model with rich input representations. Cai et al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements. Marcheggiani and Titov (201"
D19-1541,J05-1004,0,0.6859,"tively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role labeling (SRL) is a fundamental and important task in natural language processing (NLP), which aims to identify the semantic structure (Who did what to whom, when and where, etc.) of each given predicate in a sentence. Semantic knowledge has been widely exploited in"
D19-1541,P17-1186,0,0.0863687,"Missing"
D19-1541,N18-1135,0,0.0174564,"ed model on both span-based and wordbased SRL, achieving substantial improvements. In addition to the hard parameter sharing strategy that we discuss in Section 3.2, partial parameter sharing strategy is also a commonly studied approach in MTL and domain adaptation. Kim et al. (2016) introduce simple neural extensions of feature argumentation by employing a global LSTM used across all domains and independent LSTMs used within individual domains. Peng 5389 et al. (2017) explore a multitask learning approach which shares parameters across formalisms for semantic dependency parsing. In addition, Peng et al. (2018) present a multi-task approach for frame-semantic parsing and semantic dependency parsing with latent structured variables. 7 Conclusion This paper proposes a syntax-aware MTL framework to integrate implicit syntactic representations into a simple unified SRL model. The experimental results show that our proposed framework can effectively improve the basic SRL model, even when the basic model is enhanced with BERT representations. Especially, our proposed framework is more effective at utilizing syntactic information, compared with the hard parameter sharing strategy of MTL. By utilizing BERT"
D19-1541,P16-1113,0,0.558864,"ed in many down-stream NLP tasks, such as information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to joi"
D19-1541,P06-2104,0,0.102644,"Missing"
D19-1541,D18-1412,0,0.0798863,"information exCorresponding author. AM-TMP 买 Tom bought A0 买.01 汤姆 obj Figure 1: Example of span-based (blue blocks) and word-based (red blocks) SRL formulations in a sentence, where the top part is its dependency tree. Introduction ∗ subj punc root adjct Intuitively, syntax and semantics are strongly correlative. For example, the semantic A0 and A1 roles are usually the syntactic subject and object, as shown in Figure 1. Inspired by the correlation, researchers try to improve SRL performance by exploring various ways to integrate syntactic knowledge (Roth and Lapata, 2016; He et al., 2018b; Swayamdipta et al., 2018). In contrast, some recent works (He et al., 2017; Tan et al., 2018; Cai et al., 2018) propose deep neural models for SRL without considering any syntactic in5382 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identi"
D19-1541,P15-2115,0,0.408103,"ets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role"
D19-1541,D15-1186,0,0.33176,"ets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predicate. Semantic role"
D19-1541,S15-1027,0,0.0220554,"t al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identification sub-tasks of SRL. Compared with the large amount of research for English SRL, Chinese SRL works are rare, mainly because of the limited amount of data and lack of attention of Chinese researchers. For Chinese, the commonly used datasets are Chinese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018) mainly focus on building more powerful models or exploring the usage of external knowledge on this dataset. Inspired by the development of neural models and exploration of syntactic informatio"
D19-1541,P17-1189,0,0.453027,"d with the large amount of research for English SRL, Chinese SRL works are rare, mainly because of the limited amount of data and lack of attention of Chinese researchers. For Chinese, the commonly used datasets are Chinese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018) mainly focus on building more powerful models or exploring the usage of external knowledge on this dataset. Inspired by the development of neural models and exploration of syntactic information, this paper proposes a MTL framework to extract syntactic representations as the external input features for the simple unified SRL model. Th"
D19-1541,D16-1212,0,0.302848,"Missing"
D19-1541,N19-1075,0,0.327315,"on. We adopt the official scripts pro3 https://github.com/google-research/bert# pre-trained-models 4 https://catalog.ldc.upenn.edu/LDC2003T09 vided by CoNLL-20055 and CoNLL-20096 for span-based and word-based SRL evaluation, respectively. We conduct significant tests using the Dan Bikel’s randomized parsing evaluation comparer. 4.2 Syntax-aware Methods To illustrate the effectiveness and advantage of our proposed framework7 (Integration of Implicit Representations, IIR), we conduct several experiments with the recently employed syntax-aware methods on CPB1.0 dataset for comparison: • Tree-GRU Xia et al. (2019) investigate several syntax-aware methods for the English span-based SRL, showing the effectiveness of introducing syntactic knowledge into the SRL task. We only compare with the TreeGRU method, since the other methods are all predicate-specific and hence not fit into our basic SRL model. • FIR Following Yu et al. (2018) and Zhang et al. (2019), we extract the outputs of BiLSTMs as the fixed implicit representations (FIR) from a pre-trained biaffine parser. In detail, we train the biaffine parser with the same training data used in our framework, and employ the combination of development data"
D19-1541,D07-1002,0,0.0552069,"CoNLL2009 Chinese datasets show that our proposed framework can effectively improve the performance over the strong baselines. With the external BERT representations, our framework achieves new state-of-the-art 87.54 and 88.5 F1 scores on the two test data of the two benchmarks, respectively. In-depth analysis are conducted to gain more insights on the proposed framework and the effectiveness of syntax. 1 ， 昨天 Yesterday , AM-TMP A0 V adv 了 一双 鞋 a pair of shoes 。 . A1 A1 traction (Bastianelli et al., 2013), machine translation (Liu and Gildea, 2010; Gao and Vogel, 2011) and question answering (Shen and Lapata, 2007; Wang et al., 2015a). There are two formulations of SRL in the community according to the definition of semantic roles. The first is called span-based SRL, which employs a continuous word span as a semantic role and follows the manual annotations in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The second is word-based SRL (Surdeanu et al., 2008), also called dependencybased SRL, whose semantic role is usually syntactic or semantic head word of the manually annotated word span. Figure 1 gives an example of the two forms in a sentence, where “bought” is the given predic"
D19-1541,J08-2004,0,0.260693,"l Language Processing, pages 5382–5392, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics formation, achieving promising results. Most recently, He et al. (2018a); Li et al. (2019) extend the span-based models to jointly tackle the predicate and argument identification sub-tasks of SRL. Compared with the large amount of research for English SRL, Chinese SRL works are rare, mainly because of the limited amount of data and lack of attention of Chinese researchers. For Chinese, the commonly used datasets are Chinese Proposition Bank 1.0 (CPB1.0) (span-based) (Xue, 2008) and CoNLL-2009 Chinese (word-based) (Hajiˇc et al., 2009). The CPB1.0 dataset follows the same annotation guideline with the English PropBank benchmark (Palmer et al., 2005). Wu and Palmer (2015) present a top model based selection preference approach to improve Chinese SRL. Since the amount of CPB1.0 dataset is small, Xia et al. (2017) exploit heterogeneous SRL data to improve the performance via a progressive learning approach. The CoNLL-2009 benchmark is released by the CoNLL-2009 shared task (Hajiˇc et al., 2009). Previous works (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 20"
D19-1541,C18-1047,0,0.102196,"Missing"
D19-1541,D18-1548,0,0.104419,"al. (2018) present an endto-end model with BiLSTMs and biaffine scorer Apart from the above syntax-free works, researchers also pay much attention on improving the neural-based SRL approaches by introducing syntactic knowledge. Roth and Lapata (2016) introduce the dependency path embeddings to the neural-based model and achieve substantial improvements. Marcheggiani and Titov (2017) employ the graph convolutional neural networks on top of the BiLSTM encoder to encode syntactic information. He et al. (2018b) propose a k-th order argument pruning algorithm based on systematic dependency trees. Strubell et al. (2018) propose a self-attention based neural MTL model which incorporate dependency parsing as a auxiliary task for SRL. Swayamdipta et al. (2018) propose a MTL framework using hard parameter strategy to incorporate constituent parsing loss into semantic tasks, i.e. SRL and coreference resolution, which outperforms their baseline by +0.8 F1 score. Xia et al. (2019) investigate and compare several syntax-aware methods on span-based SRL, showing the effectiveness of integrating syntactic information. Compared with the large amount of works on English SRL, Chinese SRL works are rare, mainly because of"
D19-1541,N19-1118,1,0.818401,"Missing"
D19-1541,P16-1040,0,0.0132588,"epend on whether including the BERT representation repBERT or not. i 2.2 Motivated by the recently presented span-based models (He et al., 2018a; Li et al., 2019) for Y BiLSTM Encoder Over the input layer, we employ the BiLSTMs with highway connections (Srivastava et al., 2015; 5383 Argument Representation MLP Scorer X Predicate Representation Biaffine Scorer BiLSTMs Input representation 买 bought 买 bought 了 了 一双 a pair of 鞋 shoes Figure 2: The detailed architecture of our proposed framework, where the left part is the dependency parser and the right part is the basic SRL module, respectively. Zhang et al., 2016b) to encode long-range dependencies and obtain rich representations denoted as hi for time stamp i. The highway connections are used to alleviate the gradient vanishing problem when training deep neural networks. ment can compose a semantic relation. 2.3 3 Predicate and Argument Representations We directly employ the output of the top BiLSTM as the predicate representation at each time stamp. For all the candidate arguments, we simplify the representations by employing the mean operation over the BiLSTM outputs within the corresponding argument spans, which achieves similar results compared w"
D19-1541,D09-1004,0,0.213027,"gure 6: Sentence F1 scores comparison on CoNLL2009 Chinese test data, where the x axis presents the F1 scores of Baseline + BERT and y axis shows the F1 scores of Baseline + BERT + Dep (IIR), respectively. ure 6, we can see that most of the scatter points are off the diagonal line, demonstrating strong differences between the two models. Based on this finding, how to better integrate syntactic knowledge and BERT representations becomes an interesting and meaningful question, and we leave it for future work. 6 Related Work Traditional discrete-feature-based SRL works (Swanson and Gordon, 2006; Zhao et al., 2009) mainly make heavy use of syntactic information. Along with the impressive development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achievin"
D19-1541,P15-1109,0,0.0289841,"nal line, demonstrating strong differences between the two models. Based on this finding, how to better integrate syntactic knowledge and BERT representations becomes an interesting and meaningful question, and we leave it for future work. 6 Related Work Traditional discrete-feature-based SRL works (Swanson and Gordon, 2006; Zhao et al., 2009) mainly make heavy use of syntactic information. Along with the impressive development of neuralnetwork-based approaches in the NLP community, much attention has been paid to build more powerful neural model without considering any syntactic information. Zhou and Xu (2015) employ deep stacked BiLSTMs and achieve strong performance for span-based English SRL. He et al. (2017) extend their work (Zhou and Xu, 2015) by employing several advanced practices in recent deep learning literature, leading to significant improvements. Tan et al. (2018) present a strong self-attention based model, achieving significant improvements. Inspired by the span-based model proposed by Lee et al. (2017) for coreference resolution, He et al. (2018a); Ouchi et al. (2018) present similar span-based models for SRL which can exploit span-level features. For word-based SRL, Marcheggiani e"
I11-1171,A00-2018,0,0.0775912,"Missing"
I11-1171,E06-1011,0,0.174495,"Missing"
I11-1171,J05-1003,0,0.0537492,"Missing"
I11-1171,P08-1108,0,0.105721,"ndard averaged perceptron to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features fs (x, d, t) explore features related with the head and children of the focus word. Syntactic features with guide POS tags 1448 Guide POS Features: fg (x, t′ , t) t′i ti t′i−1 t′i ti t′i−1 ti t′i t′i+1 ti t′i+1 ti t′i−1 t′i+1 ti ′ ti ∼ fb (x) t′i−1 t′i t′i+1 ti Syntactic Features: fs (x, d, t) Syntactic Features with Guide POS: fsg (x, t′ , d, t) #lc(i) ti wh(i) ti #lc(i) t′i ti #rc(i) t′i ti t′h(i) ti t′i d(i) ti wlc(i,k) ti d(i) ti t′lc(i,k) t′i ti t′rc(i,k) t′i ti t′h(i) t′i ti t′h(i) d(i) ti #rc(i) ti wh(i) d(i) ti t′lc(i,k) ti t′rc(i,k) ti t′h(i"
I11-1171,W02-1001,0,0.027099,"e to CRF in tagging accuracy but requires much less training time. During training phase, we adopt the 10-fold cross validation strategy to produce both tC and dA for the training set. Input sentence ˆt = arg max µ(x, t) CRF-based tagger tC Dependency Parser dA t We implement two baseline taggers, i.e., a Perceptron-based tagger and a CRF-based tagger. As a linear model, Perceptron defines the score of a tag sequence to be µ(x, t) = w · f (x, t) where f (x, t) refers to the feature vector and w is the corresponding weight vector. We use standard averaged perceptron to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features"
I11-1171,W96-0213,0,0.369745,"Missing"
I11-1171,I05-3005,0,0.0485469,"Missing"
I11-1171,D07-1117,0,0.0424302,"Missing"
I11-1171,N09-2054,0,0.0424392,"Missing"
I11-1171,P08-1101,0,0.0183868,"ce information can be explored. Experimental results show that this effort is rewarding, and the tagging accuracy is significantly improved. Detailed error analysis confirms the usefulness of these syntactic features. 2 Baseline POS Taggers Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. The goal of POS tagging is to find the highest-scoring sequence: We adopt the exponentiated gradient algorithm to learn the weight vector (Collins et al., 2008). For POS tagging features f (x, t), we follow the work of Zhang and Clark (2008a). Besides standard POS unigram (wi ti ), bigram (ti−1 ti ) and trigram (ti−2 ti−1 ti ) features, they explore many features composed of Chinese characters, such as ci,0 ti and ci,−1 ti , where ci,0 and ci,−1 denote the start and end characters of wi . These characterbased features are very helpful for tagging accuracy. Due to space limitation, we refer to Zhang and Clark (2008a) for the complete feature description. In order to distinguish these features from our proposed syntactic features, we refer to them as the basic features and denote them as fb (x, t). Given w, we adopt the Viterbi al"
I11-1171,D08-1059,0,0.10179,"ce information can be explored. Experimental results show that this effort is rewarding, and the tagging accuracy is significantly improved. Detailed error analysis confirms the usefulness of these syntactic features. 2 Baseline POS Taggers Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. The goal of POS tagging is to find the highest-scoring sequence: We adopt the exponentiated gradient algorithm to learn the weight vector (Collins et al., 2008). For POS tagging features f (x, t), we follow the work of Zhang and Clark (2008a). Besides standard POS unigram (wi ti ), bigram (ti−1 ti ) and trigram (ti−2 ti−1 ti ) features, they explore many features composed of Chinese characters, such as ci,0 ti and ci,−1 ti , where ci,0 and ci,−1 denote the start and end characters of wi . These characterbased features are very helpful for tagging accuracy. Due to space limitation, we refer to Zhang and Clark (2008a) for the complete feature description. In order to distinguish these features from our proposed syntactic features, we refer to them as the basic features and denote them as fb (x, t). Given w, we adopt the Viterbi al"
I11-1171,P08-1102,0,0.0604386,"Missing"
I11-1171,P09-1058,0,0.123831,"Missing"
I11-1171,D08-1017,0,0.0653507,"to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features fs (x, d, t) explore features related with the head and children of the focus word. Syntactic features with guide POS tags 1448 Guide POS Features: fg (x, t′ , t) t′i ti t′i−1 t′i ti t′i−1 ti t′i t′i+1 ti t′i+1 ti t′i−1 t′i+1 ti ′ ti ∼ fb (x) t′i−1 t′i t′i+1 ti Syntactic Features: fs (x, d, t) Syntactic Features with Guide POS: fsg (x, t′ , d, t) #lc(i) ti wh(i) ti #lc(i) t′i ti #rc(i) t′i ti t′h(i) ti t′i d(i) ti wlc(i,k) ti d(i) ti t′lc(i,k) t′i ti t′rc(i,k) t′i ti t′h(i) t′i ti t′h(i) d(i) ti #rc(i) ti wh(i) d(i) ti t′lc(i,k) ti t′rc(i,k) ti t′h(i) wi ti t′h(i) t′i d(i)"
I17-1006,C10-1011,0,0.0510393,"Missing"
I17-1006,P16-1231,0,0.0713207,"dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency parsers, such as the traditional linear graph-based parser (LGPar) and transition-based parser (LTPar), and the newly proposed biaffine neural network graph-based parser (Biaffine) (Dozat and Manning, 2017) and globally normalized neural network transition-based parser (GN3Par) (Andor et al., 2016). Introduction Traditional supervised approaches for structural classification assume full annotation (FA), meaning that the training instances have complete manually-labeled structures. In the case of dependency parsing, FA means a complete parse tree is provided for each training sentence. However, recent studies suggest that it is more economic and effective to construct labeled data with partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on differ"
I17-1006,W02-1001,0,0.150548,"d we only need to disable some illegal combination operations during dynamic programming. LTPar can also directly learn from PA in a similar way, as shown in Algorithm 1. Constrained decoding is performed to find a pseudo gold-standard reference (line 8). It is more complicate to design constrained decoding for transition-based parsing Directly training parsers with PA As described in Li et al. (2014), CRF parsers such as LLGPar and Biaffine can naturally learn 51 train-1K train-39K #Sentence #Token 1,000 24,358 dev the beam size is 64 and the standard early update is adopted during training (Collins, 2002). For LGPar and LTPar, averaged perceptron is adopted (Collins, 2002). For Biaffine, we directly adopt most hyperparameters of the released code of Dozat and Manning (2017), only removing the components related with dependency labels, since we focus on unlabeled dependency parsing in this work. The LSTM (two forward plus two backward) layers all use 300-dimension hidden cells. Dropout with ratio of 0.75 is applied to most layers before output. The two MLPs both have 100-dimension outputs without hidden layer. Adam optimization is adopted with α1 = α2 = 0.9. For GN3Par, we follow Daniel et al."
I17-1006,P08-1109,0,0.105003,"Missing"
I17-1006,W15-2202,0,0.019127,"conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert pa"
I17-1006,P09-1042,0,0.0314207,", most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar)"
I17-1006,P10-1002,0,0.0598798,"Missing"
I17-1006,P13-1075,0,0.0606609,"Missing"
I17-1006,P10-1001,0,0.026383,"h effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al. (2014). In other words, we extend the or"
I17-1006,P92-1017,0,0.548641,"Missing"
I17-1006,C12-2067,0,0.0303579,"oding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency"
I17-1006,N07-1051,0,0.0542854,"Missing"
I17-1006,C14-1075,1,0.641626,"om the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objec"
I17-1006,P02-1035,0,0.274531,"w) // Unconstrained decoding: LGPar 6: a− = arg maxa→d∈Y(xj ) Score(xj , a → d; w) // Unconstrained decoding: LTPar 7: d+ = arg maxd∈Y(xj ,dp ) Score(xj , d; w) // Constrained decoding: LGPar 8: j a+ = arg maxa→d∈Y(xj ,dp ) Score(xj , a → d; w) // Constrained decoding: LTPar j 9: wk+1 = wk + f (x, d+ ) − f (x, d− ) // Update: LGPar 10: wk+1 = wk + f (x, a+ ) − f (x, a− ) // Update: LTPar 11: k =k+1 12: end for 13: end for 2.2 Transition-based Approach from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al., 2002; Dredze et al., 2009; T¨ackstr¨om et al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each actio"
I17-1006,P16-1033,1,0.853955,"enn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into f"
I17-1006,P10-1037,0,0.22981,"-based parser (LTPar). For the test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc"
I17-1006,D14-1093,1,0.936317,"d other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-st"
I17-1006,W09-1104,0,0.197058,"ulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased depe"
I17-1006,I17-1007,0,0.0144737,"ic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al. (2014). In other words, we extend the original Biaffine Parser described in Dozat and Manning (2017) by adding a CRF layer. Under the CRF model, the conditional probability of d given x is: • We present a general framework for directly training GN3Par, LGPar and LTPar with PA based on constrained decoding. The basic idea is to use the current feature weights to parse the sentence under the PA-constrained search space, and use the best parse as a pseudo gold-standard reference for feat"
I17-1006,W13-5711,0,0.0354672,"Missing"
I17-1006,N13-1126,0,0.0556355,"Missing"
I17-1006,D14-1097,0,0.0698373,"Missing"
I17-1006,W03-3023,0,0.070973,"et al., 2009; T¨ackstr¨om et al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike"
I17-1006,P13-2109,0,0.0494961,"Missing"
I17-1006,D14-1010,0,0.149707,"rmance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency parsers, such as the traditi"
I17-1006,P05-1012,0,0.118936,"nstruct labeled data with partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the pro"
I17-1006,J11-1005,1,0.858367,"Missing"
I17-1006,E06-1011,0,0.0639672,"th partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al."
I17-1006,P11-2033,1,0.820633,"First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike the traditional perceptron-like train"
I17-1006,P15-1134,0,0.0363309,"Missing"
I17-1006,W11-2917,0,0.165393,"e test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only"
I17-1006,W03-3017,0,0.110861,"t al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike the tradition"
I17-1006,J14-2001,0,0.0967859,"n achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency pars"
I17-1006,P99-1010,0,\N,Missing
I17-1006,D14-1082,0,\N,Missing
I17-1006,D07-1101,0,\N,Missing
K16-2021,P09-2004,0,0.065353,"re “0” means that Arg1 locates at the same sentence containing CP, “1” means that Arg1 is in the previous sentence of the sentence containing CP, and so on. We throw instances in which Arg1 or Arg2 locates at multiple sentences. 4 Train 14231 (0) 364 (44) 70 (18) 57(22) and perform dynamic programming based search from right to left. For simplicity, we set the window size to 6, meaning that the model considers at most six sentences, from the 0th sentence containing CP, to the 5th sentence in front. For the features, we directly adopt those described in Lin et al. (2014), Pitler et al. (2009), Pitler and Nenkova (2009), and Knott (1996). Especially, we design a three-tag label set in order to enforce the model to return exactly one sentence with Arg1. Explicit-Arg1 Sentence Locator: Sequence Labeling As far as we know, most previous participating systems last year assume that Arg1 lies in the same sentence or the previous sentence of CP. However, we find that there exist many cases that Arg1 locates at longer-distance sentences from the CP. Table 3 shows data statistics regarding the sentencelevel distance of Arg1 and CP. We also find that there are cases that Arg1 locates at more than one sentences, and th"
K16-2021,P09-1077,0,0.0629195,"nce containing CP, where “0” means that Arg1 locates at the same sentence containing CP, “1” means that Arg1 is in the previous sentence of the sentence containing CP, and so on. We throw instances in which Arg1 or Arg2 locates at multiple sentences. 4 Train 14231 (0) 364 (44) 70 (18) 57(22) and perform dynamic programming based search from right to left. For simplicity, we set the window size to 6, meaning that the model considers at most six sentences, from the 0th sentence containing CP, to the 5th sentence in front. For the features, we directly adopt those described in Lin et al. (2014), Pitler et al. (2009), Pitler and Nenkova (2009), and Knott (1996). Especially, we design a three-tag label set in order to enforce the model to return exactly one sentence with Arg1. Explicit-Arg1 Sentence Locator: Sequence Labeling As far as we know, most previous participating systems last year assume that Arg1 lies in the same sentence or the previous sentence of CP. However, we find that there exist many cases that Arg1 locates at longer-distance sentences from the CP. Table 3 shows data statistics regarding the sentencelevel distance of Arg1 and CP. We also find that there are cases that Arg1 locates at more"
K16-2021,W02-1001,0,0.233326,"lows previous practice and employs a cas∗ Train 2000 17619 38967 14722 17813 Dev 100 783 1675 680 756 Table 1: Data statistics of English. caded framework and comprises 9 components, as shown in Figure 2. In the following, we will introduce each component in detail. The codes are released at http://hlt.suda.edu.cn/ ˜zhli for future research study. 2 Classification and Sequence Labeling Based on Linear Model In this work, we implement our classification and sequence labeling models based on linear model due to its simplicity and good performance on variety of natural language processing tasks (Collins, 2002). Given an input instance x and a label y, a linear model defines the score of labeling x as y: Score(x, y) = w · f (x, y) Correspondence author. 150 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 150–157, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Figure 3: Example of a parse tree from which we extract features. Figure 2: Framework of our system. 3 where f (.) is a feature vector constructed according to a hand-crafted feature template list and w is the corresponding feature weight vector. The decoding t"
K16-2021,prasad-etal-2008-penn,0,0.218986,"s paper descirbes our participating system for CoNLL-2016 discourse parsing shared task (Xue et al., 2016). We participate in the closed track, and due to the time limitation, we focus on English. Given an document, which contains several paragraphs and each paragraph is composed of a few sentences, discourse parsing aims to identify explicit and non-explict discourse relations, including explicit connnective phrases (CP), explicit/non-explicit arguments and senses. Figure 1 presents a graphical illustration of the task. Following the official requirement, we use Section 2-21 of the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014) as the training data, Section 22 as the development data, and Section 23 as the test data. A blind test is also used for evaluation. Table 1 presents the data statistics. Due to the complexity of the task, our system follows previous practice and employs a cas∗ Train 2000 17619 38967 14722 17813 Dev 100 783 1675 680 756 Table 1: Data statistics of English. caded framework and comprises 9 components, as shown in Figure 2. In the following, we will introduce each component in detail. The codes are released at http://hlt.suda.edu.cn/ ˜zhli for future research study. 2 Class"
K16-2021,J14-4007,0,0.0532068,"participating system for CoNLL-2016 discourse parsing shared task (Xue et al., 2016). We participate in the closed track, and due to the time limitation, we focus on English. Given an document, which contains several paragraphs and each paragraph is composed of a few sentences, discourse parsing aims to identify explicit and non-explict discourse relations, including explicit connnective phrases (CP), explicit/non-explicit arguments and senses. Figure 1 presents a graphical illustration of the task. Following the official requirement, we use Section 2-21 of the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014) as the training data, Section 22 as the development data, and Section 23 as the test data. A blind test is also used for evaluation. Table 1 presents the data statistics. Due to the complexity of the task, our system follows previous practice and employs a cas∗ Train 2000 17619 38967 14722 17813 Dev 100 783 1675 680 756 Table 1: Data statistics of English. caded framework and comprises 9 components, as shown in Figure 2. In the following, we will introduce each component in detail. The codes are released at http://hlt.suda.edu.cn/ ˜zhli for future research study. 2 Classification and Sequence"
K16-2021,E14-1068,0,0.100065,"Missing"
K16-2021,K15-2004,0,0.12151,"utational Linguistics Figure 3: Example of a parse tree from which we extract features. Figure 2: Framework of our system. 3 where f (.) is a feature vector constructed according to a hand-crafted feature template list and w is the corresponding feature weight vector. The decoding task in the linear model is to find the maximum-scoring label: CP Identification Given an input document, the first task is to extract all connective phrases (CPs) (e.g., “so that”) in the document,1 which we refer to as CP identification. We directly adopt the method described in previous works (Wang and Lan, 2015; Kong et al., 2015), and take two steps for this task. yˆ = arg max Score(x, y) y 1. Candidate CP extraction. We extract all candidate CPs in the input document by exact matching with a phrase dictionary. If a string in a sentence exactly matchs a phrase in the dictionary, it then is considered as a candidate CP and will be verified in the second step. The dictionary is provided by the official organizer and contains 100 phrases. To learn w, we use the standard online training procedure, which use one instance for feature weight update at a time: w(t+1) = w(t) + f (x, y ∗ ) − f (x, yˆ) where t is the global time"
K16-2021,K15-2003,0,0.0145208,"Arg2 locates at the the same sentence with CP. Therefore, based on the results of Arg1 sentence locator, we have two cases to handle: Arg1 and Arg2 locate at the same sentence with CP (SS), or Arg1 locates at a previous sentence of CP (PS). Then, we use three sequence labeling models to locate the exact words of Arg1/2. All three models perform at the level of words, and each time assign a “Arg1/Arg2/None” tag to a word. Many systems in CoNLL-2015 (Xue et al., 2015) evaluation also treat Arg1/2 word location as a sequence labeling problem, and uses conditional random filed (CRF) based models (Stepanov et al., 2015; Nguyen et al., 2015; Lalitha Devi et al., 2015) or recurrent neural networks (RNN) (Wang et al., 2015). 5.1 True Positive 16940 718 7 Non-explicit Sense Classification After processing the explicit relations, we then turn to the problem of non-explicit relation parsing. As suggested by the official organizer, if two adjacent sentences do not have explicit relation after previous processing, we consider them as a candidate sentence pair having non-explicit relation. Please note that we only consider sentence pairs that are in the same paragraph. As far as we know, most previous work directly"
K16-2021,K15-2007,0,0.0348038,"Missing"
K16-2021,K15-2002,0,0.500051,"Association for Computational Linguistics Figure 3: Example of a parse tree from which we extract features. Figure 2: Framework of our system. 3 where f (.) is a feature vector constructed according to a hand-crafted feature template list and w is the corresponding feature weight vector. The decoding task in the linear model is to find the maximum-scoring label: CP Identification Given an input document, the first task is to extract all connective phrases (CPs) (e.g., “so that”) in the document,1 which we refer to as CP identification. We directly adopt the method described in previous works (Wang and Lan, 2015; Kong et al., 2015), and take two steps for this task. yˆ = arg max Score(x, y) y 1. Candidate CP extraction. We extract all candidate CPs in the input document by exact matching with a phrase dictionary. If a string in a sentence exactly matchs a phrase in the dictionary, it then is considered as a candidate CP and will be verified in the second step. The dictionary is provided by the official organizer and contains 100 phrases. To learn w, we use the standard online training procedure, which use one instance for feature weight update at a time: w(t+1) = w(t) + f (x, y ∗ ) − f (x, yˆ) where"
K16-2021,K15-2014,0,0.149924,"Missing"
K16-2021,K15-2001,0,0.134588,"ve 4850 200 Table 7: Distribution of adjacent sentences having non-explicit relation. Data statistics show that for explicit relations, nearly all Arg2 locates at the the same sentence with CP. Therefore, based on the results of Arg1 sentence locator, we have two cases to handle: Arg1 and Arg2 locate at the same sentence with CP (SS), or Arg1 locates at a previous sentence of CP (PS). Then, we use three sequence labeling models to locate the exact words of Arg1/2. All three models perform at the level of words, and each time assign a “Arg1/Arg2/None” tag to a word. Many systems in CoNLL-2015 (Xue et al., 2015) evaluation also treat Arg1/2 word location as a sequence labeling problem, and uses conditional random filed (CRF) based models (Stepanov et al., 2015; Nguyen et al., 2015; Lalitha Devi et al., 2015) or recurrent neural networks (RNN) (Wang et al., 2015). 5.1 True Positive 16940 718 7 Non-explicit Sense Classification After processing the explicit relations, we then turn to the problem of non-explicit relation parsing. As suggested by the official organizer, if two adjacent sentences do not have explicit relation after previous processing, we consider them as a candidate sentence pair having"
K16-2021,K16-2001,0,0.0403783,"ee are treated as classification problems. All our sequence labeling and classification models are implemented based on linear models with averaged perceptron training. Our feature sets are mostly borrowed from previous works. The main focus of our effort is to recall cases when Arg1 locates at sentences far before the connective phrase, with some yet limited success. 1 Figure 1: Illustration of discourse parsing. Document Paragraph Sentence Explicit relations Non-explicit relations General Description This paper descirbes our participating system for CoNLL-2016 discourse parsing shared task (Xue et al., 2016). We participate in the closed track, and due to the time limitation, we focus on English. Given an document, which contains several paragraphs and each paragraph is composed of a few sentences, discourse parsing aims to identify explicit and non-explict discourse relations, including explicit connnective phrases (CP), explicit/non-explicit arguments and senses. Figure 1 presents a graphical illustration of the task. Following the official requirement, we use Section 2-21 of the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014) as the training data, Section 22 as the development data, and Se"
K16-2021,miltsakaki-etal-2004-penn,0,\N,Missing
K16-2021,K15-2010,0,\N,Missing
K19-2014,hajic-etal-2012-announcing,0,0.40014,"Missing"
K19-2014,W12-3602,0,0.222709,"a Li2∗, Min Zhang2 1 Alibaba Group, China 2 School of Computer Science and Technology, Soochow University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the"
K19-2014,S19-2002,1,0.758372,"istic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing and remote edge recovery under the MTL framework. In this paper, we describe our participating systems in the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). The task includes five frameworks for graph-based meaning representations, i.e., DM, PSD, EDS, UCCA, and AMR. One common characteristic of our systems is that we employ graph-based methods instead of transition-based methods when predicting edges between nodes. For SDP, we jointly perform edge predict"
K19-2014,N16-1030,0,0.0442481,"ds SDP We construct our SDP parser based on the ideas of Dozat and Manning (2017) and Dozat and Manning (2018). Note that lemmas, POS tags and frames are also included in the MRP evaluation metrics, so our method is a bit different from Dozat and Manning (2018). Edge Prediction. Our basic edge prediction model is similar to the Dozat and Manning (2017) and Dozat and Manning (2018). The input words are first mapped into a dense vector composed by pretrained word embeddings and character-level features. xi = eword ⊕ echar i i where echar is extracted by the bidirectional i character-level LSTM (Lample et al., 2016). They are then fed into a multilayer bidirectional wordlevel LSTM to get contextualized representations. Finally, two modules are applied to predict edges. One is to predict whether or not a directed edge exists between two words (keeping the edges between pairs of words with positive scores); and the other is to predict the most probable label for each potential edge (choosing the label with maximum score). Each of them has two seperate MLPs for head and dependent representations and a biaffine layer for scoring. The training loss is the sum of sigmoid cross-entropy loss for edges and softma"
K19-2014,P13-1023,0,0.187952,"entations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing a"
K19-2014,P18-1037,0,0.437775,"ency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL framework and it is adopted by the DM, PSD, and UCCA models. For both EDS and AMR, we first produce nodes and then predict edges in a pipeline architecture. We have not attempted to jointly solve multiple semantic frameworks via MTL yet. • BERT. We observe that using BERT as our extra inputs is effective for all the models, except AMR. It is also interesting that BERTlarge does not produce more improvements over BERT-base ba"
K19-2014,W13-2322,0,0.464322,"rings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in dependency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL"
K19-2014,S14-2056,0,0.253914,"how University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguis"
K19-2014,P17-1112,0,0.0478012,"amework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with"
K19-2014,K19-2001,0,0.157991,"Missing"
K19-2014,P18-1038,0,0.0536348,"ared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in"
K19-2014,L16-1630,0,0.0887716,"Missing"
K19-2014,S15-2153,0,0.651634,"Missing"
K19-2014,P18-2077,0,0.232099,"ency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and othe"
K19-2014,S14-2008,0,0.350225,"Missing"
K19-2014,oepen-lonning-2006-discriminant,0,0.184117,"ef introduction of each framework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence"
K19-2014,D14-1162,0,0.096733,"rs. Concept Identification Model. The concept identification model chooses a concept c conditioned on the aligned word k based on the BiLSTM state hk , which is defined as Pθ (c|hk , wk ). For more details about the re-categorization and candidate concept, please refer to Lyu and Titov (2018). Relation Identification Model. The relation identification model is arc-factored as: Pφ (R|a, c, w) = m Y 3 Experiments This section describes model parameters used in our models, and the overall results of all the five tasks. 3.1 Model Parameters In both SDP and UCCA tasks, we use 100dimensional GloVe (Pennington et al., 2014) as pretrained embedding and random initialized 50dimensional char embedding. The char lstm output is 100-dimensional. We also utilize the BERT embeddings extracted from the last four transformer layers. The final BERT representation is their normalized weighted sum, which is concatenated with the word embeddings. The other parameters are the same with the previous works (Dozat and Manning, 2018; Jiang et al., 2019). In EDS task, external resources we use are: 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese; and 2) BERT 10 (Devlin et al.,"
K19-2014,P17-1076,0,0.0287443,"MLP MLP Shared BiLSTMs ... xi ... Figure 2: An example of newest version of UCCA. Figure 1: The framework of our SDP Parser. To handle discontinuous node, we trace bottomup from a discontinuous leaf node until we find the specific node whose parent is the lowest common ancestor (LCA) of the discontinuous node and leaf node. Finally we move the edge to make the specific node become the child of the discontinuous, with “-ancestor” added behind the edge label. Please refer to Jiang et al. (2019) for more conversion details. Constituent Parsing. We directly adopt the minimal span-based parser of Stern et al. (2017). Given an input sentence X = {x0 , x1 , · · · , xn }, each word xi is mapped into a dense vector xi and fed into bidirectional LSTM layers. The top-layer output of each position are used to represent the span as cross-entropy loss for labels. 0 ` = `label + `edge Lexical Taggers. This SDP task is more difficult than the ealier 2014 and 2015 SDP tasks (Oepen et al., 2014, 2015), since the gold tokenization result, lemmas, and POS tags are not available in the parser input data and the predictions are parts of the MRP evaluation metrics. We use automatic tokenization result and lemmas provided"
L18-1706,H05-1091,0,0.308427,"Missing"
L18-1706,J81-4005,0,0.735931,"Missing"
L18-1706,P82-1020,0,0.832646,"Missing"
L18-1706,D15-1064,0,0.0503344,"Missing"
L18-1706,P15-1033,0,0.0548731,"ime t. ? , ? , ? , ? denote the weight matrices of different gates for input ? , and ? , ? , ? , ? are the weight matrices for hidden state ℎ . ? , ? , ? , ? denote the bias vectors. It should be noted that we do not include peephole connections (Gers et al., 2003) in the our LSTM formulation. 3.2.2 Bi-LSTM For many sequence labeling tasks it is beneficial to have access to both past (left) and future (right) contexts. However, the LSTM’s hidden state ℎ takes information only from past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Dyer et al., 2015) is bi-directional LSTM (Bi-LSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output. We treat NER as a classification problem in the final stage. 3.3 Bi-LSTM-CRF Then, we can add a CRF layer to the Bi-LSTM model as shown in Figure 1. That is Bi-LSTM-CRFs (Huang, Xu, and Kai 2015) which are well-suited for sequence labeling. Bi-LSTM-CRF can be regarded as a combination of bidirectional LSTM and CRF. By contrast to the local cla"
L18-1706,I08-4017,0,0.10243,"Missing"
N19-1118,P17-2021,0,0.0240852,"e structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its b"
N19-1118,D17-1209,0,0.232385,"mework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential"
N19-1118,P17-1177,0,0.407881,"propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classica"
N19-1118,D17-1304,0,0.0354014,"Missing"
N19-1118,P18-1163,0,0.07328,"drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019"
N19-1118,W14-4012,0,0.16658,"Missing"
N19-1118,D14-1179,0,0.0643719,"Missing"
N19-1118,W06-1628,0,0.1279,"Missing"
N19-1118,W17-3203,0,0.04525,"Missing"
N19-1118,P81-1022,0,0.687283,"Missing"
N19-1118,P16-1078,0,0.150303,"al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integra"
N19-1118,D17-1012,0,0.247924,"e source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate ba"
N19-1118,W15-3014,0,0.0253779,"ne Input Encoder Decoder 教育 o1 o2 o3 o4 o5 o6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence"
N19-1118,W04-3250,0,0.132416,"h et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively: • Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to the model proposed by Chen et al. (2017a). • Tree-Linearization: We first convert dependency trees into constituent trees (Sun and Wan, 2013), and then feed it into the NMT model propose"
N19-1118,W17-3204,0,0.0210465,"ore overlapping with other network components. This further demonstrates that pretrained syntax-aware word representations are helpful for NMT. 4.4.2 Alignment Study Alignment quality is an important metric to illustrate and evaluate machine translation outputs. Here we study how syntax features influence the alignment results for NMT. We approximate the alignment scores by the attention probabilities as shown in Equation 4.8 For better understanding 8 We aim to offer an intuitive interpretation by a carefullyselected example. In fact, the alignment computation method here may be problematic (Koehn and Knowles, 2017). 1156 System Baseline×3 SAWR×3 Tree-RNN×3 Tree-Linearization×3 Hybrid MT03 40.90 41.94 42.03 41.74 42.72 MT04 43.25 44.59 44.15 44.23 45.14 MT05 40.64 41.91 41.50 41.32 42.38 MT06 40.16 41.97 41.41 41.44 42.15 Average/∆ 41.24 42.60/+1.36 42.27/+1.03 42.18/+0.94 43.10/+1.86 Table 4: Ensemble performances, where the Hybrid model denotes SAWR + Tree-RNN + Tree-Linearization. System ... 现代 (modern) ... 的 (’s) ... Baseline Tree-RNN Baseline SAWR Tree-Linearization 42 BLEU SAWR Tree-RNN 38 Tree-Linearization 34 Figure 3: Alignments for the baseline and syntaxintegrated systems, where the same examp"
N19-1118,E17-2093,0,0.0238477,"ecent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn1158 tactic tags. The method is as effective as TreeRNN approaches yet"
N19-1118,D15-1278,0,0.0190868,"network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbo"
N19-1118,N13-1060,0,0.0581759,"Missing"
N19-1118,P17-1064,1,0.696123,"of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system. To solve the problem, Tree-Linearization is a good alternative for syntax encoding. The main idea is to linearize syntax trees into sequential symbols, and then exploit the resulting sequences as inputs for NMT. Li et al. (2017) propose a depth-first method to traverse a constituent tree, converting it into a sequence of symbols mixed with sentential words and syntax labels. Similarly, Wu et al. (2017b) combine several strategies of tree traversing for dependency syntax integration. In this work, we present an implicit syntax encoding method for NMT, enhancing NMT models by syntax-aware word representations (SAWRs). Figure 1 illustrates the basic idea, where trees are modeled indirectly by sequential vectors extracted from an encoder-decoder dependency parser. On the one hand, the method avoids the structural heterog"
N19-1118,P02-1040,0,0.105406,"t datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE) (Sennrich et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively: • Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to t"
N19-1118,N18-1202,0,0.0504328,"rocess can be formalized as follows:  h = Bi-RNN ex1 ⊕ s1 , · · · , exn ⊕ sn . (6) Noticeably, the SAWR method can be regarded as an adaption of joint learning as well. We can train both dependency parsing and machine translation model parameters concurrently. In this work, we focus on the machine translation task and do not involve the training objective of dependency parsing. However, we can still finetune model parameters of the encoder part of dependency parsing by back-propagating the training losses of NMT into this part as well. Actually, SAWRs are also similar to the ELMO embeddings (Peters et al., 2018). ELMO learns context word representations by using language model as objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem. 4 the TED tst2012 and tst2013 as the development and test datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, whil"
N19-1118,W16-2209,0,0.0414485,"see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present th"
N19-1118,P16-1162,0,0.0740184,"objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem. 4 the TED tst2012 and tst2013 as the development and test datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE) (Sennrich et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). A"
N19-1118,P16-1159,0,0.0239934,"明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predic"
N19-1118,D16-1159,0,0.226869,"the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151"
N19-1118,Q13-1025,0,0.0621683,"Missing"
N19-1118,P06-1077,0,0.132793,"on. We implement Tree-RNN and TreeLinearization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because"
N19-1118,2015.iwslt-evaluation.11,0,0.117502,"Missing"
N19-1118,P15-1150,0,0.104527,"1 o2 o3 o4 o5 o6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden"
N19-1118,D15-1166,0,0.266251,"6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then i"
N19-1118,P08-1114,0,0.0513682,"arization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network"
N19-1118,D16-1096,0,0.0201671,", neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapol"
N19-1118,P16-1105,0,0.0360613,"14; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn1158 tactic tags. The method is as eff"
N19-1118,P17-1065,0,0.296249,"ariety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that i"
N19-1118,1983.tc-1.13,0,0.712779,"Missing"
N19-1118,D11-1020,0,0.0301346,"in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing a"
N19-1118,D17-1150,0,0.0770327,"esting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system. To solv"
N19-1118,N16-1035,0,0.02831,"es such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words"
N19-1118,Q18-1011,0,0.0337858,"Missing"
N19-1118,P17-2092,0,0.0126484,"lated Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tre"
N19-1118,P17-2060,0,0.0194939,"lated Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tre"
P12-1071,P11-1070,0,0.0244387,"Missing"
P12-1071,W09-1210,0,0.0236959,"Missing"
P12-1071,D08-1092,0,0.0423625,"ect comparison indicates that our approach also outperforms previous work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Corresp"
P12-1071,W10-2906,0,0.0128754,"so outperforms previous work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorpo"
P12-1071,P05-1022,0,0.228193,"Missing"
P12-1071,A00-2018,0,0.417901,"Missing"
P12-1071,W09-1207,1,0.900322,"Missing"
P12-1071,D09-1060,0,0.078901,"Missing"
P12-1071,P10-1003,0,0.0547958,"s work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased"
P12-1071,P99-1065,0,0.119653,"In contrast, we experiment with two real large-scale treebanks, and boost the stateof-the-art parsing accuracy using QG features. Second, we explore much richer QG features to fully exploit the knowledge of the source treebank. These features are tailored to the dependency parsing problem. In summary, the present work makes substantial progress in modeling structural annotation inconsistencies with QG features for parsing. Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008). The work by Niu et al. (2009) is, to our knowledge, the only study to date that combines the converted treebank with the existing target treebank. They automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a statistical constituency parser trained on CTB5. Their experiments show that the combined treebank can significantly improve the performance of constituency parsers. However, their method requires several sophisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. I"
P12-1071,W02-1001,0,0.0341419,"Missing"
P12-1071,W09-1205,0,0.0613185,"Missing"
P12-1071,D11-1044,0,0.0868094,"ions, and the score of a target dependency tree becomes Source Treebank S={(xi, di)}i Train Target Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features"
P12-1071,D09-1127,0,0.176739,"Missing"
P12-1071,P09-1059,0,0.148157,". We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated exper"
P12-1071,P10-1001,0,0.129047,"en models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). 1 The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which w"
P12-1071,P08-1068,0,0.10649,"Missing"
P12-1071,D11-1109,1,0.951214,"ral classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). 1 The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future"
P12-1071,D08-1017,0,0.185678,"ch as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGenhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0 w1 ...wn and its POS tag sequence t = t0 t1 ...tn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 ≤ h ≤ n, 0 < m ≤ n, l ∈ L}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the label set. We omit the l"
P12-1071,E06-1011,0,0.0258575,"the highest scoring tree from a directed graph. To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees). Scorebs (x, t, d) = wbs · fbs (x, t, d) X = wpart · fpart (x, t, p) h g h h m s dependency m m grandparent sibling Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3 ) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3 ) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4 ).4 For the O2 model, the score function is rewritten as: X Scorebs (x, t, d) = wdep · fdep (x, t, h, m) {(h,m)}⊆d + X wsib · fsib (x, t, h, s, m) {(h,s),(h,m)}⊆d + X wgrd · fgrd (x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep (.), fsib (.) and fgrd (.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula"
P12-1071,P05-1012,0,0.141192,"earch, we adopt the graph-based parsing models for their state-of-the-art performance in a variety of languages.3 Graph-based models view the problem as finding the highest scoring tree from a directed graph. To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees). Scorebs (x, t, d) = wbs · fbs (x, t, d) X = wpart · fpart (x, t, p) h g h h m s dependency m m grandparent sibling Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3 ) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3 ) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4 ).4 For the O2 model, the score function is rewritten as: X Scorebs (x, t, d) = wdep · fdep (x, t, h, m) {(h,m)}⊆d + X wsib · fsib (x, t, h, s, m) {(h,s),(h,m)}⊆d + X wgrd · fgrd (x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep (.), fsib (.) and fgrd (.) corre"
P12-1071,P09-1006,0,0.330964,"these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground. Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy. Figure 1 gives an example with different an675 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675–684, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics OBJ NMOD NMOD ROOT w0 VV NN CC NN 促进1 贸易2 和3 工业4 promote v trade n and c industry n ROOT VOB proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work LAD COO Figure 1: Example with annotations from CTB5 (upper) and CDT (under). notations from CTB5 and CDT.2 This example illustrates that the two treebanks annotate coordination constructions differently. In CTB5, the last noun is the head, whereas the first noun is the head in CDT. One natural idea for multiple treebank exploitation is treebank conversion. First, the annotations in the source treebank are converted into the style of the target treebank. Then, both the converted treebank and the target treebank are combined. Finally, the combined treebank are used to train a b"
P12-1071,P08-1108,0,0.119102,"phisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGenhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0 w1 ...wn and its POS tag sequence t = t0 t1 ...tn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 ≤ h ≤ n, 0 < m ≤ n, l ∈ L}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the l"
P12-1071,W03-3017,0,0.0228695,"2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure ("
P12-1071,W06-3104,0,0.253692,"by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure (Koo and Collins, 2010). Target Side h Syntactic Struct"
P12-1071,D09-1086,0,0.108437,"xperiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated experiments on one treebank by ma"
P12-1071,D07-1003,0,0.0514962,"ee becomes Source Treebank S={(xi, di)}i Train Target Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features to encourage or penalize certain scorin"
P12-1071,D11-1038,0,0.0160561,"rget Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features to encourage or penalize certain scoring parts in the target side according to the source tree d"
P12-1071,W03-3023,0,0.0296712,"al. (2011). For the O1 and O2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decodi"
P12-1071,P08-1101,0,0.261243,"ling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of pa"
P12-1071,D08-1059,0,0.394174,"ling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of pa"
P12-1071,P11-2033,0,0.0579417,"m) ◦ dist(h, m) to form new features. Corpus Train Dev Test PD 281,311 5,000 10,000 CDT 55,500 1,500 3,000 CTB5 16,091 803 1,910 352 348 CTB5X 18,104 CTB6 22,277 1,762 2,556 Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p < 10−5 ) Li11 86.18 — Z&N11 86.00 — Table 3: Data used in this work (in sentence number). Table 4: Parsing accuracy (UAS) comparison on CTB5test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). We adopt unlabeled attachment score (UAS) as the primary evaluation metric. We also use Root accuracy (RA) and complete match rate (CM) to give more insights. All metrics exclude punctuation. We adopt Dan Bikel’s randomized parsing evaluation comparator for significance test (Noreen, 1989).7 For all models used in current work (POS tagging and parsing), we adopt averaged perceptron to train the feature weights (Collins, 2002). We train each model for 10 iterations and select the parameters that perform best on the development set. 5.1 Preliminaries This subsection describes how we project th"
P12-1071,P11-1156,0,0.0529369,"Missing"
P12-1071,W09-1201,0,\N,Missing
P14-1043,D09-1087,0,0.0224704,"results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two par"
P14-1043,P11-1070,0,0.0123477,"chen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated f"
P14-1043,D12-1133,0,0.025955,"Missing"
P14-1043,C10-1011,0,0.0365972,"al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations to compose rich feature sets. Please refer to Table 4 of Bohnet (2010) for the complete feature list. Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled"
P14-1043,D07-1101,0,0.552209,"of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories correspondi"
P14-1043,P10-1001,0,0.198994,"arsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of s"
P14-1043,P08-1068,0,0.409193,"g∗, Wenliang Chen Provincial Key Laboratory for Computer Information Processing Technology Soochow University {zhli13,minzhang,wlchen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse tr"
P14-1043,C12-1103,1,0.917666,"Missing"
P14-1043,P05-1022,0,0.0577924,"iveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data. For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (Goldberg and Elhadad, 2010) and other constituent parsers (Collins and Koo, 2005; Charniak and Johnson, 2005; Finkel et al., 2008). 93.2 93.1 93 UAS 92.9 92.8 92.7 92.6 92.5 92.4 B+Z Parser 92.3 0 50K 100K 200K 500K 1M 1.7M Unlabeled Data Size Figure 3: Performance of GParser with different sizes of “Unlabeled ← B+Z” on English test set. 5 Related Work Our work is originally inspired by the work of T¨ackstr¨om et al. (2013). They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by making use of sourcelanguage treebanks. Different from their work, we explore the ide"
P14-1043,N06-1020,0,0.116258,"w parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best p"
P14-1043,E06-1011,0,0.470919,"h , ws , wm , th , tm , ts , th±1 , tm±1 , ts±1 dir(h, m), dist(h, m) 3. We build the first state-of-the-art CRF-based dependency parser. Using the probabilistic parser, we benchmark and conduct systematic comparisons among ours and all previous bootstrapping methods, including self/co/tritraining. Table 1: Brief illustration of the syntactic features. ti denotes the POS tag of wi . b is an index between h and m. dir(i, j) and dist(i, j) denote the direction and distance of the dependency (i, j). 2 Supervised Dependency Parsing We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. 2.1 Then the score of a dependency tree is: X Score(x, d; w) = wdep · fdep (x, h, m) Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspe"
P14-1043,D09-1060,1,0.844222,"ory for Computer Information Processing Technology Soochow University {zhli13,minzhang,wlchen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is t"
P14-1043,D13-1129,1,0.895096,"Missing"
P14-1043,J05-1003,0,0.0347821,"demonstrates the effectiveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data. For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (Goldberg and Elhadad, 2010) and other constituent parsers (Collins and Koo, 2005; Charniak and Johnson, 2005; Finkel et al., 2008). 93.2 93.1 93 UAS 92.9 92.8 92.7 92.6 92.5 92.4 B+Z Parser 92.3 0 50K 100K 200K 500K 1M 1.7M Unlabeled Data Size Figure 3: Performance of GParser with different sizes of “Unlabeled ← B+Z” on English test set. 5 Related Work Our work is originally inspired by the work of T¨ackstr¨om et al. (2013). They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by making use of sourcelanguage treebanks. Different from th"
P14-1043,P05-1012,0,0.196144,"Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 < m ≤ n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (20"
P14-1043,P08-1108,0,0.0613393,"ws produce divergent structures. Impact of unlabeled data size: To understand how our approach performs with regards to the unlabeled data size, we train semi-supervised GParser with different sizes of unlabeled data. Fig. 3 shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data. uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguou"
P14-1043,W03-3017,0,0.0361248,"modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations"
P14-1043,N07-1051,0,0.170069,"Figure 1 contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (Zhang and Nivre, 2011), and a generative constituent parser (Petrov and Klein, 2007). The 1-best parse trees of these three parsers are aggregated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ensemble training outperforms other entire-tree based methods suc"
P14-1043,N06-2033,0,0.0303478,"fferent sizes of unlabeled data. Fig. 3 shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data. uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training obj"
P14-1043,P08-1109,0,0.0368128,"Missing"
P14-1043,D07-1111,0,0.0936021,"up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable. 1 Introduction Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗ Cor"
P14-1043,N10-1115,0,0.0132809,"parsers, than traditional tri-training. Detailed analysis demonstrates the effectiveness of our approach. Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data. For future work, among other possible extensions, we would like to see how our approach performs when employing more diverse parsers to compose the parse forest of higher quality for the unlabeled data, such as the easyfirst non-directional dependency parser (Goldberg and Elhadad, 2010) and other constituent parsers (Collins and Koo, 2005; Charniak and Johnson, 2005; Finkel et al., 2008). 93.2 93.1 93 UAS 92.9 92.8 92.7 92.6 92.5 92.4 B+Z Parser 92.3 0 50K 100K 200K 500K 1M 1.7M Unlabeled Data Size Figure 3: Performance of GParser with different sizes of “Unlabeled ← B+Z” on English test set. 5 Related Work Our work is originally inspired by the work of T¨ackstr¨om et al. (2013). They first apply the idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by maki"
P14-1043,D07-1070,0,0.0173488,"eatures into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed"
P14-1043,C10-1120,0,0.166841,"owsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable. 1 Introduction Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗ Correspondence author 457 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457–467, c B"
P14-1043,W09-1104,0,0.135652,"supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable."
P14-1043,N10-1091,0,0.0241289,"led data. Fig. 3 shows the accuracy curve on the test set. We can see that the parser consistently achieves higher accuracy with more unlabeled data, demonstrating the effectiveness of our approach. We expect that our approach has potential to achieve higher accuracy with more additional data. uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training objective is to maximize the mix"
P14-1043,D09-1058,0,0.150625,"s to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-pars"
P14-1043,N13-1126,0,0.0453798,"Missing"
P14-1043,D08-1017,0,0.0995804,"Missing"
P14-1043,P08-1061,0,0.01868,"ed parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labe"
P14-1043,W03-3023,0,0.134172,"rom the head word wh to the modifier wm , and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 h s m (b) adjacent sibling {(h,m)}⊆d + X wsib · fsib (x, h, s, m) {(h,s),(h,m)}⊆d where fdep (x, h, m) and fsib (x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different"
P14-1043,P95-1026,0,0.203753,"referred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and"
P14-1043,D12-1030,0,0.024625,"hey adopt the higher-order model of Carreras (2007). Again, our method may be combined with their work to achieve higher performance. Comparison with Previous Work We adopt the best settings on development data for semi-supervised GParser with our proposed approach, and make comparison with previous results on test data. Table 4 shows the results. The first major row lists several state-of-theart supervised methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achie"
P14-1043,P11-2033,0,0.406452,"with high oracle score. Please note that the parse forest in Figure 1 contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (Zhang and Nivre, 2011), and a generative constituent parser (Petrov and Klein, 2007). The 1-best parse trees of these three parsers are aggregated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ens"
P14-1043,P11-1156,0,0.0131227,"{zhli13,minzhang,wlchen}@suda.edu.cn Abstract of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-bes"
P15-1172,N09-2054,0,0.0755111,"Missing"
P15-1172,P09-1059,0,0.320863,"aseline model. Particularly, when M ′ = 1K, the model converges very slowly. However, from the trend of the curves, we expect that the accuracy gap between our coupled model with M ′ = 5K/20K and the baseline model should be much smaller when reaching convergence. Based on the above observation, we adopt N ′ = 5K and M ′ = 5K in the following experiments. Moreover, we select the best iteration on the development data, and use the corresponding model to parse the test data. 5.2 Final Results Table 3 shows the final results on the CTB test data. We re-implement the guide-feature based method of Jiang et al. (2009), referred to as twostage CRF. Li et al. (2012) jointly models Chinese POS tagging and dependency parsing, and report the best tagging accuracy on CTB. The results show that our coupled model outperforms the baseline model by large margin, and also achieves slightly higher accuracy than the guide-feature based method. 5.3 Accuracy 94.10 94.81 (+0.71) † 95.00 (+0.90) †‡ 94.60 Table 3: Final results on CTB test data. † means the corresponding approach significantly outperforms the baseline at confidence level of p < 10−5 ; whereas ‡ means the accuracy difference between the two-stage CRF and the"
P15-1172,P13-1075,0,0.0367226,"Missing"
P15-1172,C12-1103,1,0.827616,"Missing"
P15-1172,P14-1043,1,0.750353,"this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comm"
P15-1172,C14-1075,1,0.842938,"this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comm"
P15-1172,D14-1093,0,0.0197553,"contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comments from our anonymous reviewers. This work was supported by National Natural Science Foundation of China (Grant No. 61432013, 61203314) and Jiangsu Planned Projects for Postdoctoral Research Funds (No. 14010"
P15-1172,P08-1108,0,0.285245,"Missing"
P15-1172,C14-1145,0,0.0497809,"Missing"
P15-1172,C12-2093,0,0.0181146,"otations. They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side. In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling. The use of both joint and separate features is proven to be crucial for the success of our coupled model. In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings. The naming of the “coupled” CRF is borrowed from the work of Qiu et al. (2012), which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interacti"
P15-1172,D13-1062,0,0.545921,"ed tag set. In practice, we usually only need results following one annotation style. Therefore, we employ our coupled model to convert PD into the style of CTB, and train our baseline model with two training data with homogeneous annotations. Again, Algorithm 1 is used to merge the two data with N ′ = 5K and M ′ = 5K. The results are shown in the bottom row in Table 6. We can see that with the extra converted data, the baseline model can achieve slightly lower accuracy with the coupled model and avoid the inefficiency problem at the meantime. 6 Related Work This work is partially inspired by Qiu et al. (2013), who propose a model that performs heterogeneous Chinese word segmentation and POS tagging and produces two sets of results following CTB and PD styles respectively. Different from our CRFbased coupled model, their approach adopts a linear model, which directly combines two separate sets of features based on single-side tags, without considering the interacting joint features between the two annotations. They adopt an approximate decoding algorithm which tries to find the best single-side tag sequence with reference to tags at the other side. In contrast, our approach is a direct extension of"
P15-1172,N13-1126,0,0.0283562,"Missing"
P15-1172,D14-1010,0,0.0482597,"labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation system was built. We also thank the helpful comments from our anonymous reviewers. This work was supported by National Natural Science Foundation of China (Grant No. 61432013, 61203314) and Jiangsu Planned Projects for Postdoctoral Research Funds (No. 1401075B), and was also parti"
P15-1172,P08-1101,0,0.0356683,"are based on single-side tags. The advantages of our coupled model over the traditional model are to provide us with the flexibility of using both kinds of features, which significantly contributes to the accuracy improvement as shown in the following experiments. 3.1 Mapping Functions defines the probability of a tag sequence as: exp(Score(x, t; θ)) P (t|x; θ) = P ′ t′ exp(Score(x, t ; θ)) X (1) Score(x, t; θ) = θ · f (x, i, ti−1 , ti ) 1≤i≤n where f (x, i, ti−1 , ti ) is the feature vector at the ith word and θ is the weight vector. We adopt the state-of-the-art tagging features in Table 1 (Zhang and Clark, 2008). 3 Coupled POS Tagging (TaggerCTB&PD ) In this section, we introduce our coupled model, which is able to learn and predict two heterogeneous annotations simultaneously. The idea is to bundle two sets of POS tags together and let the CRF-based model work in the enlarged tag space. For example, a CTB tag “NN” and a PD tag “n” would be bundled into “[NN,n]”. Figure 2 shows the graphical structure of our model. Different from the traditional model in Eq. (1), our coupled model defines the score of a bundled tag sequence as follows: Score(x, [ta , tb ]; θ) =   f (x, i, [tai−1 , tbi−1 ], [tai , t"
P15-1172,C14-1051,0,0.143791,"the other side. In contrast, our approach is a direct extension of traditional CRF, and is more theoretically simple from the perspective of modelling. The use of both joint and separate features is proven to be crucial for the success of our coupled model. In addition, their work indicates that their model relies on a hand-crafted loose mapping between annotations, which is opposite to our findings. The naming of the “coupled” CRF is borrowed from the work of Qiu et al. (2012), which treats the joint task of Chinese word segmentation and POS tagging as two coupled sequence labeling problems. Zhang et al. (2014) propose a shift-reduce dependency parsing model which can simultaneously learn and produce two heterogeneous parse trees. However, their approach assumes the existence of data with annotations at both sides, which is obtained by converting phrase-structure trees into dependency trees with different heuristic rules. This work is also closely related with multitask learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation (BenDavid and Schuller, 2003; Ando and Zhang, 2005; Parameswaran and Weinberger, 2010). However,"
P15-1172,P02-1035,0,0.0333588,"ling tasks, such as POS tagging and chunking. In this work, our coupled CRF jointly models two same tasks which have different annotation schemes. Moreover, this work provides a natural way to 1790 learn from incomplete annotations where one sentence only contains one-side labels. The reviewer also suggests that our objective can be optimized with the latent variable structured perceptron of Sun et al. (2009), which we leave as future work. Learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002), sequence labeling (Dredze et al., 2009), parsing (Riezler et al., 2002; T¨ackstr¨om et al., 2013; Li et al., 2014a; Li et al., 2014b). Recently, researchers derive natural annotations from web data, transform them into ambiguous labelings to supervise Chinese word segmentation models (Jiang et al., 2013; Liu et al., 2014; Yang and Vozila, 2014). Acknowledgments The authors would like to thank the undergraduate students Fangli Lu and Xiaojing Wang for building our annotation system, and Le Lu, Die Hu, Yue Zhang, Jian Zhang, Qiuyi Yan, Xinzhou Jiang for data annotation. We are also grateful that Yu Ding kindly shared her earlier codes on which our annotation syste"
P15-1172,P11-2009,0,0.0609344,"Missing"
P15-1172,P12-1026,0,0.0513514,"Missing"
P15-1172,P12-1025,0,0.592973,"Missing"
P16-1033,D14-1108,0,0.0232974,"nnotated sentence, where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial"
P16-1033,C10-1011,0,0.131693,"Missing"
P16-1033,W09-2307,0,0.0617144,"Missing"
P16-1033,P10-1001,0,0.0312588,"lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based pa"
P16-1033,D07-1015,0,0.0416854,"Missing"
P16-1033,N06-1019,0,0.0701527,"Missing"
P16-1033,W08-1301,0,0.158099,"Missing"
P16-1033,C12-2067,0,0.0659251,"Missing"
P16-1033,C14-1075,1,0.946592,"certain metric for AL for sequence labeling problems. In the case of dependency parsing, the marginal probability of a dependency is the sum of probabilities of all legal trees that contain the dependency. ∑ p(h ↷ m|x; w) = p(d|x; w) (4) d∈Y(x):h↷m∈d Intuitively, marginal probability is a more principled metric for measuring reliability of a dependency since it considers all legal parses in the search space, compared to previous methods based on scores of local classifiers (Sassano and Kurohashi, 2010; Flannery and Mori, 2015) or votes of n-best parses (Mirroshandel and Nasr, 2011). Moreover, Li et al. (2014) find strong correlation between marginal probability and correctness of a dependency in cross-lingual syntax projection. Score(x, d∗ ) (5) n1.5 Normalized tree probability. The CRF-based parser allows us, for the first time in AL for dependency parsing, to directly use tree probabilities for uncertainty measurement. Unlike previous approximate methods based on k-best parses (Mirroshandel and Nasr, 2011), tree probabilities globally consider all parse trees in the search space, and thus are intuitively more consistent and proper for measuring the reliability of a tree. Our initial assumption i"
P16-1033,U12-1005,0,0.0219821,"2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They f"
P16-1033,P14-1126,0,0.0479024,"Missing"
P16-1033,W15-2202,0,0.24441,"notation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting"
P16-1033,I11-1087,0,0.0377813,"Missing"
P16-1033,W13-5711,0,0.102764,"arsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences pro"
P16-1033,D14-1097,0,0.0830651,"Missing"
P16-1033,I11-1100,0,0.0281049,"Missing"
P16-1033,P15-1119,1,0.88885,"Missing"
P16-1033,E06-1011,0,0.0810264,"4th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for dependency parsing. We adopt the second-order graphbased model of McDonald and Pereira (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies. (1) This is the first work that applies a stateof-the-art probabilistic parsing model to AL for dependency parsing. The CRF-based dependency parser on the one hand allows us to use probabilities of trees or marginal probabilities of single dependencies for uncertainty measurement, and on the other hand can directly learn parameters from partially annotated trees. Using probabilistic models may be ubiquitous in AL for r"
P16-1033,P99-1010,0,0.894454,"Missing"
P16-1033,W07-2216,0,0.0829738,"Missing"
P16-1033,J04-3001,0,0.119634,"d McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and"
P16-1033,N12-1053,0,0.0241059,"omain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing perf"
P16-1033,C08-1113,0,0.0347094,"Missing"
P16-1033,P15-1134,0,0.075254,"Missing"
P16-1033,D14-1122,0,0.0228308,"where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which"
P16-1033,W11-2917,0,0.448477,"es full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units r"
P16-1033,P11-2033,1,0.841112,"ng trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for de"
P16-1033,P92-1017,0,0.8556,"Missing"
P16-1033,D15-1039,0,0.0453421,"Missing"
P16-1033,P02-1035,0,0.254628,"e, and may be asked to annotate another selected word in the same sentence in next AL iteration. Obviously, frequently switching sentences incurs great waste of cognitive effort, 3.4 Learning from PA A major challenge for AL with PA is how to learn from partially labeled sentences, as depicted in Figure 1. Li et al. (2014) show that a probabilistic CRF-based parser can naturally and effectively learn from PA. The basic idea is converting a partial tree into a forest as shown in Figure 2, 347 and using the forest as the gold-standard reference during training, also known as ambiguous labeling (Riezler et al., 2002; T¨ackstr¨om et al., 2013). For each remaining word without head, we add all dependencies linking to it as long as the new dependency does not violate the existing dependencies. We denote the resulting forest as Fj, whose probability is naturally the sum of probabilities of each tree d in F. ∑ p(F|x; w) = p(d|x; w) d∈F ∑ eScore(x,d;w) = ∑ d∈F Score(x,d′ ;w) d′ ∈Y(x) e Train Chinese English ∑N i=1 log p(Fi |xi ; w) #Sentences 14,304 803 1,910 #Tokens 318,408 20,454 50,319 #Sentences 39,115 1,700 2,416 #Tokens 908,154 40,117 56,684 are selected and annotated at each iteration. In the case of si"
P16-1033,P10-1037,0,0.419473,"09). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-1033,D08-1112,0,0.139903,"Missing"
P16-1033,D07-1014,0,0.220985,"Missing"
P16-1033,N13-1126,0,0.0792451,"Missing"
P16-1033,P02-1016,0,0.0777853,"l., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; F"
P18-1252,P16-1231,0,0.0506344,"Missing"
P18-1252,D12-1133,0,0.0569982,"Missing"
P18-1252,P12-1071,1,0.896274,"or boosting parsing performance. Though under different linguistic theories or annotation guidelines, the treebanks are painstakingly developed to capture the syntactic structures of the same language, thereby having a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treeb"
P18-1252,D14-1082,0,0.0998588,"we propose two simple yet effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre"
P18-1252,P16-1033,1,0.654377,"iSeqLSTM at wk , denoted as hseq k , is fed into two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task"
P18-1252,P07-1033,0,0.462705,"Missing"
P18-1252,P15-1033,0,0.104829,"t effective treebank conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multiple treebank exploitation and leads to significantly higher parsing accuracy. 1 Introduction During the past few years, neural network based dependency parsing has achieved significant progress and outperformed the traditional discrete-feature based parsing (Chen and Manning, 2014; Dyer et al., 2015; Zhou ∗ #Tok Grammar 0.36M Case grammar 1.62M Phrase structure 1.00M Phrase structure 0.90M Phrase structure 0.90M Dependency structure 1.40M Dependency structure The first two (student) authors make equal contributions to this work. Zhenghua is the correspondence author. et al., 2015; Andor et al., 2016). Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep biaffine parser that further advances the state-of-the-art accuracy by large margin. As reported, their parser outperforms the state-of-the-art discrete-feature based parser of Bohnet and Nivre (2012) by 0.97 (93."
P18-1252,C16-1002,0,0.486022,"ing a great deal of common grounds. Previous researchers have proposed two approaches for multi-treebank exploitation. On the one hand, the guiding-feature method projects the knowledge of the source-side treebank into the target-side treebank, and utilizes extra pattern-based features as guidance for the target-side parsing, mainly for the traditional discrete-feature based parsing (Li et al., 2012). On the other hand, the multi-task learning method simultaneously trains two parsers on two treebanks and uses shared neural network parameters for representing common-ground syntactic knowledge (Guo et al., 2016).2 Regardless of their effectiveness, while the guiding-feature method fails to directly use the source-side treebank as extra training data, the multi-task learning method is incapable of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1"
P18-1252,N13-1013,0,0.0187158,"of explicitly capturing the structural correspondences between two guidelines. In this sense, we consider both of them as indirect exploitation approaches. Compared with the indirect approaches, treebank conversion aims to directly convert a source-side treebank into the target-side guideline, and uses the converted treebank as extra labeled data for training the targetside model. Taking the example in Figure 1, the goal of this work is to convert the under tree that follows the HIT-CDT guideline (Che et al., 2012) into the upper one that follows our new guideline. However, due to the lack 2 Johansson (2013) applies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion"
P18-1252,P13-2105,0,0.0200075,"treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li et al. (2012) to th"
P18-1252,I17-1007,0,0.0141555,"two separate MLPs to get two lowerdimensional representation vectors. ( seq ) H rH hk k = MLP (1) ( seq ) D D rk = MLP hk where rH k is the representation vector of wk as a head word, and rD k as a dependent. Finally, the score of the dependency i ← j is computed via a biaffine operation. [ score(i ← j) = rD i 1 ]T Wb rH j (2) During training, the original biaffine parser uses the local softmax loss. For each wi and its head wj , its loss is defined as score(i←j) − log ∑e escore(i←k) . Since our training data is k partially annotated, we follow Li et al. (2016) and employ the global CRF loss (Ma and Hovy, 2017) for better utilization of the data, leading to consistent accuracy gain. Multi-task learning aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) apply multi-task learning to multi-treebank exploitation based on the neural transition-based parser of Dyer et al. (2015), and achieve higher improvement than the guiding-feature approach of Li et al. (2012). Based on the state-of-the-art biaffine parser, this work makes a straightforward extension to realize multi-task learning. We treat the source-side and target-side"
P18-1252,P16-1105,0,0.141086,"is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, which is consistent with Miwa and Bansal. We did not implement the in-between subtree treeLSTM. 2710 score(i ← j) consistent: i ← j h↑a wa grand: i ← k ← j sibling: i ← k → j reverse: i → j Biaffine wi h↓i reverse grand: i → k → j e"
P18-1252,P09-1006,0,0.036991,"pplies the feature-sharing approach of Daumé III (2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofi"
P18-1252,D14-1162,0,0.079859,"Missing"
P18-1252,C14-1026,0,0.0706607,"Missing"
P18-1252,P15-1150,0,0.0608474,"←j rpat ⊕ eli ⊕ elj ⊕ ela i←j = e (3) Through rpat i←j , the extended word representaH tions, i.e., rD i,i←j and rj,i←j , now contain the structural information of wi and wj in dsrc . The remaining parts of the biaffine parser is H unchanged. The extended rD i,i←j and rj,i←j are fed into the biaffine layer to compute a more reliable score of the dependency i ← j, with the help of the guidance of dsrc . 4.2 The TreeLSTM Approach Compared with the pattern embedding approach, our second conversion approach employs treeLSTM to obtain a deeper representation of i ← j in the source-side tree dsrc . Tai et al. (2015) first propose treeLSTM as a generalization of seqLSTM for encoding treestructured inputs, and show that treeLSTM is more effective than seqLSTM on the semantic relatedness and sentiment classification tasks. Miwa and Bansal (2016) compare three treeLSTM variants on the relation extraction task and show that the SP-tree (shortest path) treeLSTM is superior to the full-tree and subtree treeLSTMs. In this work, we employ the SP-tree treeLSTM of Miwa and Bansal (2016) for our treebank conversion task. Our preliminary experiments also show the SP-tree treeLSTM outperforms the full-tree treeLSTM, w"
P18-1252,telljohann-etal-2004-tuba,0,0.122222,"Missing"
P18-1252,L16-1034,0,0.0229828,"Missing"
P18-1252,P15-1117,0,0.081965,"Missing"
P18-1252,P11-2126,0,0.0209418,"2007) for multiple treebank exploitation, which can be regarded as a simple discrete-feature variant of multi-task learning. pred root adv subj $ 奶奶 obj 叫 Grandma SBV asks 我 快 上学 me quickly go to school DBL ADV HED VOB Figure 1: Example of treebank conversion from the source-side HIT-CDT tree (under) to the target-side our-CDT tree (upper). of bi-tree aligned data, in which each sentence has two syntactic trees following the sourceside and target-side guidelines respectively, most previous studies are based on unsupervised treebank conversion (Niu et al., 2009) or pseudo bi-tree aligned data (Zhu et al., 2011; Li et al., 2013), making very limited progress. In this work, we for the first time propose the task of supervised treebank conversion. The key motivation is to better utilize a largescale source-side treebank by constructing a small-scale bi-tree aligned data. In summary, we make the following contributions. (1) We have manually annotated a highquality bi-tree aligned data containing over ten thousand sentences, by reannotating the HIT-CDT treebank according to a new guideline. (2) We propose a pattern embedding conversion approach by retrofitting the indirect guiding-feature method of Li e"
P19-1229,P16-1231,0,0.0859338,"Missing"
P19-1229,K18-2005,0,0.0589861,"1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of"
P19-1229,D14-1082,0,0.145105,"ency from the head wh to the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the"
P19-1229,D18-1217,0,0.105638,"Missing"
P19-1229,P07-1033,0,0.607207,"Missing"
P19-1229,N19-1423,0,0.066859,"Missing"
P19-1229,P15-1033,0,0.0218671,"the modifier wm ∗ ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation"
P19-1229,N09-1068,0,0.0306646,"meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially lab"
P19-1229,W15-2202,0,0.0307514,"shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-domain constituent parsing. Both results report large improvement and show the usefulness of even small amount of target-domain annotation, showing the great potential of semi-supervised domain adaptation for parsing. 6 Conclusions This work addresses the task of semi-supervised domain adaptation for Chinese dependency parsing, based on our two newl"
P19-1229,I11-1100,0,0.0717653,"Missing"
P19-1229,C16-1002,0,0.0777974,"erse annotation guideline) for a language. Inspired by their work, we propose to concatenate each word position with an extra domain embedding to indicate which domain this training sentence comes from, as illustrated in Figure 3. In this way, we expect the model can fully utilize both training datasets, since most parameters are shared except the two domain embedding vectors, and learn to distinguish the domain-specific and general features as well. (3) Multi-task learning (MTL) aims to incorporate labeled data of multiple related tasks for improving performance (Collobert and Weston, 2008). Guo et al. (2016) first employ MTL to improve parsing performance by utilizing multiple heterogeneous treebanks and treating each treebank as a separate task. As shown in Figure 4, we make a straightforward extension to the biaffine parser to realize multi-task learning. The sourcedomain and target-domain parsing are treated as xi ... Figure 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the traini"
P19-1229,D18-1498,0,0.0496972,"ubset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labeled target-domain data, which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labe"
P19-1229,P18-1252,1,0.921905,"this work, we choose two typical domain-aware web texts for annotation, i.e., product blogs and web fictions. This section introduces the details about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guidelin"
P19-1229,P18-1110,0,0.0770222,"Missing"
P19-1229,R13-1046,0,0.0528327,"Missing"
P19-1229,P17-1060,0,0.0488226,", which can be used to directly learn the domain-specific distributions or features. Daum´e III (2007) propose a simple yet effective feature augmentation approach that performs well on a number of sequence labeling tasks. The idea is to distinguish domain-specific and general features by making a copy of each feature for each domain plus a shared (general) pseudo domain. Finkel and Manning (2009) further propose a hierarchical Bayesian extension of this idea. As pointed by Finkel and Manning (2009), those two works can be understood as MTL under the traditional discrete-feature ML framework. Kim et al. (2017) propose a neural mixture of experts approach for cross-domain intent classification and slot tagging. Different from the unsupervised method of Guo et al. (2018), they use a small amount of target-domain labeled data to train an attention module for the computation of example-to-domain distances. In the parsing community, Flannery and Mori (2015) propose to annotate partially labeled target-domain data with active learning for cross-domain Japanese dependency parsing. Similarly, Joshi et al. (2018) annotate a few dozen partially labeled target-domain sentences with a few brackets for cross-do"
P19-1229,Q16-1023,0,0.122391,"Missing"
P19-1229,P18-1249,0,0.0262471,"g has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to attract more research attention (Nivre et al., 2007; Hajiˇc et al., 2009; Petrov and McDonald, 2012). 2386 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
P19-1229,D14-1108,0,0.0607182,"Missing"
P19-1229,P14-1043,1,0.843831,"e 4: The framework of MTL. two individual tasks with shared parameters for word/tag embeddings and BiLSTMs. The main weakness of MTL is that the model cannot make full use of the source-domain labeled data, since the source-domain training data only contributes to the training of the shared parameters. The corpus weighting strategy. For all above three approaches, the target-domain labeled data would be overwhelmed by the source-domain data during training if directly combined, since there usually exists a very big gap in their scale. Therefore, we employ the simple corpus weighting strategy (Li et al., 2014) as a useful trick. Before each iteration, we randomly sample training sentences separately from the target- and sourcedomain training data in the proportion of 1 : M . Then we merge and randomly shuffle the sampled data for one-iteration training. We treat M ≥ 1 as a hyper-parameter tuned on the dev data. 3.3 Utilizing Unlabeled Data Besides labeled data, how to exploit unlabeled data, both target- and source-domain, has been an interesting and important direction for crossdomain parsing for a long time, as discussed in Section 5. Recently, Peters et al. (2018) introduce embeddings from langu"
P19-1229,P18-1130,0,0.0228167,"product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the majo"
P19-1229,P06-1043,0,0.131945,"arch, we try to give a brief (and far from complete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting researc"
P19-1229,N10-1004,0,0.035199,"s from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of using all the labeled data (Plank and van Noord, 2011; Khan et al., 2013). The multi-source domain adaptation problem assumes there are labeled datasets for multiple source domains. Given a target domain, the challenge is how to effectively combine knowledge in the source domains. McClosky et al. (2010) first raise this scenario for constituent parsing. They employ a regression model to predict crossdomain performance, and then use the values to combine parsing models independently trained on each source domain. Guo et al. (2018) employ a similar idea of mixture of experts under the neural MTL framework, and conduct experiments on sentiment classification and POS tagging tasks. They employ meta-training to learn to compute the point-to-set distance between a target-domain example and a source domain. Semi-supervised domain adaptation assumes there exist some (usually very small-scale) labele"
P19-1229,D07-1111,0,0.070654,"the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the idea is to automatically select a most relevant subset from the source-domain training data to train the parsing model, instead of usin"
P19-1229,D14-1122,0,0.0233107,"sume there is no labeled target-domain training data and thus focus on unsupervised domain adaptation. So far, approaches in this direction have made limited progress, due to the intrinsic difficulty of both domain adaptation and parsing (see discussions in Section 5). On the other hand, due to the extreme complexity and heavy cost, progress on syntactic data annotation on new-domain texts has been very slow, and only several small-scale datasets on web texts have been built, mostly as evaluation data for cross-domain parsing (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). To meet the above challenges, this paper presents two newly-annotated large-scale domainaware datasets (over 12K sentences), and try to tackle the task of semi-supervised domain adaptation for Chinese dependency parsing. With the access of both labeled and unlabeled targetdomain data, we propose and evaluate several simple approaches and conduct error analysis in order to investigate the following three questions: Q1: How to effectively combine the source- and target-domain labeled training data? Q2: How to utilize the target-domain unlabeled data for further improvements? Q3: Given a certai"
P19-1229,W15-2201,0,0.286589,"lete) review on some representative approaches of high relevance with syntactic parsing. Unsupervised domain adaptation. Due to the lack of sufficient labeled data, most previous works focuses on unsupervised domain adapta2392 tion, assuming there is only labeled data for the source domain. Researchers make great effort to learn useful features from large-scale unlabeled target-domain data, which is usually much easier to collect. As a typical semi-supervised approach, self-training is shown to be very useful for cross-domain constituent parsing (McClosky et al., 2006) and dependency parsing (Yu et al., 2015). There are also many failed works on applying self-training for in-domain and crossdomain dependency parsing. Sagae and Tsujii (2007) apply co-training to the CoNLL-2007 cross-domain dependency parsing task and report positive gains (Nivre et al., 2007). In contrast, Dredze et al. (2007) experiment with many domain adaptation approaches with no success on the same datasets and suggest the major obstacle comes from the divergent annotation guideline adopted by the target-domain evaluation data. Source-domain data selection is another interesting research direction. Given a target domain, the i"
P19-1229,E14-1062,0,0.0554062,"about the data annotation procedure. Data selection. The product blog (PB) texts are crawled from the Taobao headline website, which contains articles written by users mainly on description and comparison of different commercial products. After data cleaning and automatic word segmentation, we have collected about 340K sentences. Then, we select 10 thousand sentences with [5, 25] words for manual annotation following the active learning workflow of Jiang et al. (2018). The remaining sentences are used as unlabeled data. For web fictions, we follow the work on cross-domain word segmentation of Zhang et al. (2014), and adopt the popular novel named as “Zhuxian” (ZX, also known as “Jade dynasty”). Among their annotated 4,555 sentences, we select about 3,400 sentences with [5, 45] words for annotation. The remaining 32K sentences of ZX are used as unlabeled data in this work. Annotation guideline. After comparing several publicly available guidelines for dependency parsing including the universal dependencies (UD) (McDonald et al., 2013), we adopt the guideline released by Jiang et al. (2018) based on three considerations. First, their guideline contains 20 relations specifically designed to capture Chin"
P19-1229,P15-1117,0,0.0182641,"ł this Introduction Corresponding author The two domain-specific datasets, plus another one for product comment texts, are also used in the NLPCC-2019 shared task (http://hlt.suda.edu.cn/index. php/Nlpcc-2019-shared-task) on cross-domain Chinese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, w"
P19-1229,N18-1202,0,0.26759,"inese dependency parsing. Please note that the settings for the source-domain training data are different between this work and NLPCC-2019 shared task. 1 adv att Recently, dependency parsing has achieved tremendous progress thanks to the strong capability of deep neural networks in capturing long-distance contexts (Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018). Furthermore, contextualized word representations learned from large-scale unlabeled texts under language model training loss (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), are proven to be extensively helpful for many NLP tasks including dependency parsing (Che et al., 2018; Clark et al., 2018; Kitaev and Klein, 2018). However, parsing performance drops dramatically when processing texts that are different from the training data, known as the domain adaptation problem. In fact, with the surge of web data (or user generated content), cross-domain parsing has become the major challenge for applying syntactic analysis in realistic NLP systems. To meet this challenge, the community has organized several shared tasks to a"
P19-1229,P11-1157,0,0.0874358,"Missing"
S19-2002,P05-1013,0,0.438798,"Missing"
S19-2002,N18-1202,0,0.049253,"he embeddings of the named entity tags and the dependency labels, but find limited performance gains. Then, the parser employs two cascaded bidirectional LSTM layers as the encoder, and use the top-layer outputs as the word representations. Afterwards, the parser represents each span wi ...wj as 2.4 Use of BERT For the open tracks, we use the contextualized word representations produced by BERT (Devlin et al., 2018) as extra input features.2 Following previous works, we use the weighted summation of the last four transformer layers and then multiply a task-specific weight parameter following (Peters et al., 2018). 2 We use the multilingual cased BERT from https:// github.com/google-research/bert. ri,j = (fj − fi ) ⊕ (bi − bj ) 13 3 Cross-lingual Parsing Because of little training data for French, we borrow the treebank embedding approach of Stymne et al. (2018) for exploiting multiple heterogeneous treebanks for the same language, and propose a language embedding approach to utilize English and German training data. The training datasets of the three languages are merged to train a single UCCA parsing model. The only modification is to concatenate each word position with an extra language embedding (o"
S19-2002,P18-2098,0,0.0642438,"Missing"
S19-2002,P13-1023,0,0.359727,"t al. (2017) first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task. Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs. Hershcovich et al. (2018) then extend their previous approach and propose to utilize the annotated data with other Introduction Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). Figure 1 shows an example sentence and its UCCA graph. Words are represented as terminal nodes. Circles denote non-terminal nodes, and the semantic relation 1 The full UCCA scheme also has implicit and linkage relations, which are overlooked in the community so far. ∗ Corresponding author, hlt.suda.edu.cn/zhenghua 11 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 11–15 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics ROOT semantic formalisms such as abstract meaning representation (AMR), universal depend"
S19-2002,N19-1423,0,0.0680309,"Missing"
S19-2002,P17-1104,0,0.0904906,"lustration. between two non-terminal nodes is represented by the label on the edge. One node may have multiple parents, among which one is annotated as the primary parent, marked by solid line edges, and others as remote parents, marked by dashed line edges. The primary edges form a tree structure, whereas the remote edges enable reentrancy, forming directed acyclic graphs (DAGs).1 The second feature of UCCA is the existence of nodes with discontinuous leaves, known as discontinuity. For example, node 3 in Figure 1 is discontinuous because some terminal nodes it spans are not its descendants. Hershcovich et al. (2017) first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task. Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs. Hershcovich et al. (2018) then extend their previous approach and propose to utilize the annotated data with other Introduction Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by Abend and Ra"
S19-2002,P18-1035,0,0.105592,"cyclic graphs (DAGs).1 The second feature of UCCA is the existence of nodes with discontinuous leaves, known as discontinuity. For example, node 3 in Figure 1 is discontinuous because some terminal nodes it spans are not its descendants. Hershcovich et al. (2017) first propose a transition-based UCCA Parser, which is used as the baseline in the closed tracks of this shared task. Based on the recent progress on transitionbased parsing techniques, they propose a novel set of transition actions to handle both discontinuous and remote nodes and design useful features based on bidirectional LSTMs. Hershcovich et al. (2018) then extend their previous approach and propose to utilize the annotated data with other Introduction Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). Figure 1 shows an example sentence and its UCCA graph. Words are represented as terminal nodes. Circles denote non-terminal nodes, and the semantic relation 1 The full UCCA scheme also has implicit and linkage relations, which are overlooked in the community so far. ∗ Corresponding author, hlt.suda.edu.cn/zhenghua 11 Proceedings of the 13th Int"
S19-2002,P17-1076,0,\N,Missing
W08-2134,W05-0627,1,0.833091,"o the predicate (negative for being left to the predicate and positive for right), another feature is formed, namely “Bag of POS (Numbered)”. WIND5 BIGRAM (b3): 5 closest words from both left and right plus the predicate itself, in total 11 words form a “window”, within which bigrams are enumerated. The final optimized feature set for the task of predicate classification is (a1, a21, a23, a71, a72, a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3, a71+a9). 3.4 Semantic Role Classification In our system, the identification and classification of semantic roles are achieved in a single stage (Liu et al., 2005) through one single classifier (actually two, one for noun predicates, and the other for verb predicates). Each word in a sentence is given probabilities to be each semantic role (including none of the these roles) for a predicate. Features introduced in addition to those of the previous subsections are the following: POS PATH (c11), REL PATH (c12): The “POS Path” feature consists of POS tags of the words along the path from a word to the predicate. Other than “Up” and “Down”, the “Left” and “Right” direction of the path is added. Similarly, the “Relation Path” feature consists of the relation"
W08-2134,C04-1197,0,0.0272563,"ost Inference. During the Predicate Identification stage we examine each word in a sentence to discover target predicates, including both noun predicates (from NomBank) and verb predicates (from PropBank). In the Predicate Classification stage, each predicate is assigned a certain sense number. For each predicate, the probabilities of a word in the sentence to be each semantic role are predicted in the Semantic Role Classification stage. Maximum entropy model is selected as our classifiers in these stages. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al., 2004). 3.2 Predicate Identification The predicate identification is treated as a binary classification problem. Each word in a sentence is predicted to be a predicate or not to be. A set of features are extracted for each word, and an optimized subset of them are adopted in our final system. The following is a full list of the features: DEPREL (a1): Type of relation to the parent. WORD (a21), POS (a22), LEMMA (a23), HEAD (a31), HEAD POS (a32), HEAD LEMMA (a33): The forms, POS tags and lemmas of a word and it’s headword (parent) . FIRST WORD (a41), FIRST POS (a42), FIRST LEMMA (a43), LAST WORD (a51)"
W08-2134,W08-2121,0,0.0848569,"Missing"
W09-1207,W09-1201,0,0.118581,"Missing"
W09-1207,D08-1008,0,0.0936805,"Missing"
W09-1207,kawahara-etal-2002-construction,0,0.0239566,"ntropy classifier is implemented with Maximum Entropy Modeling Toolkit1 . The classifier parameters are tuned with the development data for different languages respectively. lp solve 5.52 is chosen as our ILP problem solver. 1 2 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html http://sourceforge.net/projects/lpsolve 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Compute Cloud)3 was used, too. Our system requires 15G memory at most and the longest training time is about 36 hours. During training the"
W09-1207,W02-1006,0,0.0162974,"dicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a"
W09-1207,P05-1013,0,0.0387666,"role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 49 For each arc, we firstly use unigram features to choose the K1 -best labels. The second parameter of f1lbl (·) indicates whether the node is the head of the arc, and the third parameter indicates the direction. L denotes the whole label set. Then we re-rank the labels by combining the bigram features, and choose K2 -best labels. During decoding, we only use the K2 labels chosen for each arc (K2 ¿ K1 < |L|). 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A s"
W09-1207,C04-1197,0,0.029135,"tage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles’ duplication for each single predicate. T"
W09-1207,W08-2121,0,0.165854,"Missing"
W09-1207,taule-etal-2008-ancora,0,0.0539562,"Missing"
W09-1207,burchardt-etal-2006-salsa,0,\N,Missing
W09-1207,S07-1034,1,\N,Missing
W09-1207,J96-1002,0,\N,Missing
W09-1207,W08-2134,1,\N,Missing
W09-1207,D07-1101,0,\N,Missing
W14-6835,W13-4407,0,0.0405858,"pe Junjie Yu and Zhenghua Li Provincial Key Laboratory for Computer Information Processing Technology Soochow University, China 20144227010@stu.suda.edu.cn; zhli13@suda.edu.cn Abstract 2005). Different input methods lead to different types of spelling errors. For example, input methods based on pinyin which usually lead to spelling errors of characters sharing similar pronunciations; while input methods based on radical methods usually lead to errors related to character shapes. Huang et al. (2007) proposed a learning model based on Chinese phonemic alphabet to detect Chinese spelling errors. Yeh et al. (2013) presented a method based on Ngram ranked inverted index list to deal with this problem. Spelling check is an important preprocessing task when dealing with user generated texts such as tweets and product comments. Compared with some western languages such as English, Chinese spelling check is more complex because there is no word delimiter in Chinese written texts and misspelled characters can only be determined in word level. Our system works as follows. First, we use character-level n-gram language models to detect potential misspelled characters with low probabilities below some predefined"
W14-6835,P00-1032,0,0.0900889,"lling check is a traditional and important preprocessing task for natural language processing, since spelling errors happen in written texts, such as short messages, emails, and so on. Lots of research has been devoted to English spelling error detection and correction. In English spelling error detection and correction, the errors can be classified into “nonword” error and “real-word” error (Kukich, 1992). Unlike English, Chinese words are not separated by space and all characters in Chinese are “real-word”. Therefore, automatic word segmentation need to be applied in order to produce words (Zhang et al., 2000). There are many Chinese input methods (Zhang et al., 2.2 Spelling Error Detection In spelling error detection phase, we propose two methods to deal with this problem. One is to gather the characters which get a low score 220 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 220–223, Wuhan, China, 20-21 October 2014 Figure 1: Framework of our proposed system under language model. Another is to record any by the language model. After filtering, the independent characters after automatic word number of candidates has been reduced which segmentation. Howe"
