2021.starsem-1.23,Modeling Sense Structure in Word Usage Graphs with the Weighted Stochastic Block Model,2021,-1,-1,3,0.637255,630,dominik schlechtweg,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"We suggest to model human-annotated Word Usage Graphs capturing fine-grained semantic proximity distinctions between word uses with a Bayesian formulation of the Weighted Stochastic Block Model, a generative model for random graphs popular in biology, physics and social sciences. By providing a probabilistic model of graded word meaning we aim to approach the slippery and yet widely used notion of word sense in a novel way. The proposed framework enables us to rigorously compare models of word senses with respect to their fit to the data. We perform extensive experiments and select the empirically most adequate model."
2021.starsem-1.24,Compound or Term Features? Analyzing Salience in Predicting the Difficulty of {G}erman Noun Compounds across Domains,2021,-1,-1,4,0,1000,anna hatty,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Predicting the difficulty of domain-specific vocabulary is an important task towards a better understanding of a domain, and to enhance the communication between lay people and experts. We investigate German closed noun compounds and focus on the interaction of compound-based lexical features (such as frequency and productivity) and terminology-based features (contrasting domain-specific and general language) across word representations and classifiers. Our prediction experiments complement insights from classification using (a) manually designed features to characterise termhood and compound formation and (b) compound and constituent word embeddings. We find that for a broad binary distinction into {`}easy{'} vs. {`}difficult{'} general-language compound frequency is sufficient, but for a more fine-grained four-class distinction it is crucial to include contrastive termhood features and compound and constituent features."
2021.spnlp-1.6,Using Hierarchical Class Structure to Improve Fine-Grained Claim Classification,2021,-1,-1,5,0,413,erenay dayanik,Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021),0,"The analysis of public debates crucially requires the classification of political demands according to hierarchical \textit{claim ontologies} (e.g. for immigration, a supercategory {``}Controlling Migration{''} might have subcategories {``}Asylum limit{''} or {``}Border installations{''}). A major challenge for automatic claim classification is the large number and low frequency of such subclasses. We address it by jointly predicting pairs of matching super- and subcategories. We operationalize this idea by (a) encoding soft constraints in the claim classifier and (b) imposing hard constraints via Integer Linear Programming. Our experiments with different claim classifiers on a German immigration newspaper corpus show consistent performance increases for joint prediction, in particular for infrequent categories and discuss the complementarity of the two approaches."
2021.konvens-1.24,"{W}ord{G}uess: Using Associations for Guessing, Learning and Exploring Related Words",2021,-1,-1,3,0,5582,cennet oguz,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.iwpt-1.13,"Applying Occam{'}s Razor to Transformer-Based Dependency Parsing: What Works, What Doesn{'}t, and What is Really Necessary",2021,-1,-1,3,0,5443,stefan grunewald,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"The introduction of pre-trained transformer-based contextualized word embeddings has led to considerable improvements in the accuracy of graph-based parsers for frameworks such as Universal Dependencies (UD). However, previous works differ in various dimensions, including their choice of pre-trained language models and whether they use LSTM layers. With the aims of disentangling the effects of these choices and identifying a simple yet widely applicable architecture, we introduce STEPS, a new modular graph-based dependency parser. Using STEPS, we perform a series of analyses on the UD corpora of a diverse set of languages. We find that the choice of pre-trained embeddings has by far the greatest impact on parser performance and identify XLM-R as a robust choice across the languages in our study. Adding LSTM layers provides no benefits when using transformer-based embeddings. A multi-task training setup outputting additional UD features may contort results. Taking these insights together, we propose a simple but widely applicable parser architecture and configuration, achieving new state-of-the-art results (in terms of LAS) for 10 out of 12 diverse languages."
2021.eacl-srw.25,Explaining and Improving {BERT} Performance on Lexical Semantic Change Detection,2021,-1,-1,4,0,10513,severin laicher,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Type- and token-based embedding architectures are still competing in lexical semantic change detection. The recent success of type-based models in SemEval-2020 Task 1 has raised the question why the success of token-based models on a variety of other NLP tasks does not translate to our field. We investigate the influence of a range of variables on clusterings of BERT vectors and show that its low performance is largely due to orthographic information on the target word, which is encoded even in the higher layers of BERT representations. By reducing the influence of orthography we considerably improve BERT{'}s performance."
2021.conll-1.41,Negation-Instance Based Evaluation of End-to-End Negation Resolution,2021,-1,-1,4,0,11395,elizaveta sineva,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"In this paper, we revisit the task of negation resolution, which includes the subtasks of cue detection (e.g. {``}not{''}, {``}never{''}) and scope resolution. In the context of previous shared tasks, a variety of evaluation metrics have been proposed. Subsequent works usually use different subsets of these, including variations and custom implementations, rendering meaningful comparisons between systems difficult. Examining the problem both from a linguistic perspective and from a downstream viewpoint, we here argue for a negation-instance based approach to evaluating negation resolution. Our proposed metrics correspond to expectations over per-instance scores and hence are intuitively interpretable. To render research comparable and to foster future work, we provide results for a set of current state-of-the-art systems for negation resolution on three English corpora, and make our implementation of the evaluation scripts publicly available."
2021.acl-long.543,Lexical Semantic Change Discovery,2021,-1,-1,4,0,10514,sinan kurtyigit,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change. Furthermore, we provide an almost fully automated framework for both evaluation and discovery."
2020.udw-1.8,Identifying and Handling Cross-Treebank Inconsistencies in {UD}: A Pilot Study,2020,-1,-1,3,0,11203,tillmann donicke,Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),0,"The Universal Dependencies treebanks are a still-growing collection of treebanks for a wide range of languages, all annotated with a common inventory of dependency relations. Yet, the usages of the relations can be categorically different even for treebanks of the same language. We present a pilot study on identifying such inconsistencies in a language-independent way and conduct an experiment which illustrates that a proper handling of inconsistencies can improve parsing performance by several percentage points."
2020.sigmorphon-1.5,Ensemble Self-Training for Low-Resource Languages: Grapheme-to-Phoneme Conversion and Morphological Inflection,2020,-1,-1,3,1,14273,xiang yu,"Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"We present an iterative data augmentation framework, which trains and searches for an optimal ensemble and simultaneously annotates new training data in a self-training style. We apply this framework on two SIGMORPHON 2020 shared tasks: grapheme-to-phoneme conversion and morphological inflection. With very simple base models in the ensemble, we rank the first and the fourth in these two tasks. We show in the analysis that our system works especially well on low-resource languages."
2020.msr-1.4,{IMS}ur{R}eal Too: {IMS} in the Surface Realization Shared Task 2020,2020,-1,-1,4,1,14273,xiang yu,Proceedings of the Third Workshop on Multilingual Surface Realisation,0,"We introduce the IMS contribution to the Surface Realization Shared Task 2020. The new system achieves substantial improvement over the state-of-the-art system from last year, mainly due to a better token representation and a better linearizer, as well as a simple ensembling approach. We also experiment with data augmentation, which brings some additional performance gain. The system is available at https://github.com/EggplantElf/IMSurReal."
2020.lrec-1.115,{DE}bate{N}et-mig15:Tracing the 2015 Immigration Debate in {G}ermany Over Time,2020,-1,-1,6,0,628,gabriella lapesa,Proceedings of the 12th Language Resources and Evaluation Conference,0,"DEbateNet-migr15 is a manually annotated dataset for German which covers the public debate on immigration in 2015. The building block of our annotation is the political science notion of a claim, i.e., a statement made by a political actor (a politician, a party, or a group of citizens) that a specific action should be taken (e.g., vacant flats should be assigned to refugees). We identify claims in newspaper articles, assign them to actors and fine-grained categories and annotate their polarity and date. The aim of this paper is two-fold: first, we release the full DEbateNet-mig15 corpus and document it by means of a quantitative and qualitative analysis; second, we demonstrate its application in a discourse network analysis framework, which enables us to capture the temporal dynamics of the political debate"
2020.lrec-1.636,{GRAIN}-{S}: Manually Annotated Syntax for {G}erman Interviews,2020,-1,-1,6,1,6297,agnieszka falenska,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present GRAIN-S, a set of manually created syntactic annotations for radio interviews in German. The dataset extends an existing corpus GRAIN and comes with constituency and dependency trees for six interviews. The rare combination of gold- and silver-standard annotation layers coming from GRAIN with high-quality syntax trees can serve as a useful resource for speech- and text-based research. Moreover, since interviews can be put between carefully prepared speech and spontaneous conversational speech, they cover phenomena not seen in traditional newspaper-based treebanks. Therefore, GRAIN-S can contribute to research into techniques for model adaptation and for building more corpus-independent tools. GRAIN-S follows TIGER, one of the established syntactic treebanks of German. We describe the annotation process and discuss decisions necessary to adapt the original TIGER guidelines to the interviews domain. Next, we give details on the conversion from TIGER-style trees to dependency trees. We provide data statistics and demonstrate differences between the new dataset and existing out-of-domain test sets annotated with TIGER syntactic structures. Finally, we provide baseline parsing results for further comparison."
2020.lrec-1.859,{CCOHA}: Clean Corpus of Historical {A}merican {E}nglish,2020,-1,-1,3,0,629,reem alatrash,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Modelling language change is an increasingly important area of interest within the fields of sociolinguistics and historical linguistics. In recent years, there has been a growing number of publications whose main concern is studying changes that have occurred within the past centuries. The Corpus of Historical American English (COHA) is one of the most commonly used large corpora in diachronic studies in English. This paper describes methods applied to the downloadable version of the COHA corpus in order to overcome its main limitations, such as inconsistent lemmas and malformed tokens, without compromising its qualitative and distributional properties. The resulting corpus CCOHA contains a larger number of cleaned word tokens which can offer better insights into language change and allow for a larger variety of tasks to be performed."
2020.iwpt-1.4,Integrating Graph-Based and Transition-Based Dependency Parsers in the Deep Contextualized Era,2020,-1,-1,3,1,6297,agnieszka falenska,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"Graph-based and transition-based dependency parsers used to have different strengths and weaknesses. Therefore, combining the outputs of parsers from both paradigms used to be the standard approach to improve or analyze their performance. However, with the recent adoption of deep contextualized word representations, the chief weakness of graph-based models, i.e., their limited scope of features, has been mitigated. Through two popular combination techniques {--} blending and stacking {--} we demonstrate that the remaining diversity in the parsing models is reduced below the level of models trained with different random seeds. Thus, an integration no longer leads to increased accuracy. When both parsers depend on BiLSTMs, the graph-based architecture has a consistent advantage. This advantage stems from globally-trained BiLSTM representations, which capture more distant look-ahead syntactic relations. Such representations can be exploited through multi-task learning, which improves the transition-based parser, especially on treebanks with a high ratio of right-headed dependencies."
2020.coling-main.353,Real-Valued Logics for Typological Universals: Framework and Application,2020,-1,-1,3,0,11203,tillmann donicke,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper proposes a framework for the expression of typological statements which uses real-valued logics to capture the empirical truth value (truth degree) of a formula on a given data source, e.g. a collection of multilingual treebanks with comparable annotation. The formulae can be arbitrarily complex expressions of propositional logic. To illustrate the usefulness of such a framework, we present experiments on the Universal Dependencies treebanks for two use cases: (i) empirical (re-)evaluation of established formulae against the spectrum of available treebanks and (ii) evaluating new formulae (i.e. potential candidates for universals) generated by a search algorithm."
2020.acl-main.134,Fast and Accurate Non-Projective Dependency Tree Linearization,2020,-1,-1,4,1,14273,xiang yu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding."
W19-8636,Head-First Linearization with Tree-Structured Representation,2019,0,1,4,1,14273,xiang yu,Proceedings of the 12th International Conference on Natural Language Generation,0,"We present a dependency tree linearization model with two novel components: (1) a tree-structured encoder based on bidirectional Tree-LSTM that propagates information first bottom-up then top-down, which allows each token to access information from the entire tree; and (2) a linguistically motivated head-first decoder that emphasizes the central role of the head and linearizes the subtree by incrementally attaching the dependents on both sides of the head. With the new encoder and decoder, we reach state-of-the-art performance on the Surface Realization Shared Task 2018 dataset, outperforming not only the shared tasks participants, but also previous state-of-the-art systems (Bohnet et al., 2011; Puduppully et al., 2016). Furthermore, we analyze the power of the tree-structured encoder with a probing task and show that it is able to recognize the topological relation between any pair of tokens in a tree."
W19-7911,Dependency Length Minimization vs. Word Order Constraints: An Empirical Study On 55 Treebanks,2019,-1,-1,3,1,14273,xiang yu,"Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)",0,None
W19-4815,Learning the {D}yck Language with Attention-based {S}eq2{S}eq Models,2019,0,2,3,1,14273,xiang yu,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"The generalized Dyck language has been used to analyze the ability of Recurrent Neural Networks (RNNs) to learn context-free grammars (CFGs). Recent studies draw conflicting conclusions on their performance, especially regarding the generalizability of the models with respect to the depth of recursion. In this paper, we revisit several common models and experimental settings, discuss the potential problems of the tasks and analyses. Furthermore, we explore the use of attention mechanisms within the seq2seq framework to learn the Dyck language, which could compensate for the limited encoding ability of RNNs. Our findings reveal that attention mechanisms still cannot truly generalize over the recursion depth, although they perform much better than other models on the closing bracket tagging task. Moreover, this also suggests that this commonly used task is not sufficient to test a model{'}s understanding of CFGs."
P19-3018,An Environment for Relational Annotation of Political Debates,2019,0,0,4,1,1033,andre blessing,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper describes the MARDY corpus annotation environment developed for a collaboration between political science and computational linguistics. The tool realizes the complete workflow necessary for annotating a large newspaper text collection with rich information about claims (demands) raised by politicians and other actors, including claim and actor spans, relations, and polarities. In addition to the annotation GUI, the tool supports the identification of relevant documents, text pre-processing, user management, integration of external knowledge bases, annotation comparison and merging, statistical analysis, and the incorporation of machine learning models as {``}pseudo-annotators{''}."
P19-1012,The (Non-)Utility of Structural Features in {B}i{LSTM}-based Dependency Parsers,2019,33,0,2,1,6297,agnieszka falenska,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Classical non-neural dependency parsers put considerable effort on the design of feature functions. Especially, they benefit from information coming from structural features, such as features drawn from neighboring tokens in the dependency tree. In contrast, their BiLSTM-based successors achieve state-of-the-art performance without explicit information about the structural context. In this paper we aim to answer the question: How much structural context are the BiLSTM representations able to capture implicitly? We show that features drawn from partial subtrees become redundant when the BiLSTMs are used. We provide a deep insight into information flow in transition- and graph-based neural architectures to demonstrate where the implicit information comes from when the parsers make their decisions. Finally, with model ablations we demonstrate that the structural context is not only present in the models, but it significantly influences their performance."
P19-1273,Who Sides with Whom? Towards Computational Construction of Discourse Networks for Political Debates,2019,0,0,6,0,411,sebastian pado,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Understanding the structures of political debates (which actors make what claims) is essential for understanding democratic political decision making. The vision of computational construction of such discourse networks from newspaper reports brings together political science and natural language processing. This paper presents three contributions towards this goal: (a) a requirements analysis, linking the task to knowledge base population; (b) an annotated pilot corpus of migration claims based on German newspaper reports; (c) initial modeling results."
D19-6306,{IMS}ur{R}eal: {IMS} at the Surface Realization Shared Task 2019,2019,0,0,5,1,14273,xiang yu,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"We introduce the IMS contribution to the Surface Realization Shared Task 2019. Our submission achieves the state-of-the-art performance without using any external resources. The system takes a pipeline approach consisting of five steps: linearization, completion, inflection, contraction, and detokenization. We compare the performance of our linearization algorithm with two external baselines and report results for each step in the pipeline. Furthermore, we perform detailed error analysis revealing correlation between word order freedom and difficulty of the linearization task."
W18-6021,Approximate Dynamic Oracle for Dependency Parsing with Reinforcement Learning,2018,0,0,3,1,14273,xiang yu,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"We present a general approach with reinforcement learning (RL) to approximate dynamic oracles for transition systems where exact dynamic oracles are difficult to derive. We treat oracle parsing as a reinforcement learning problem, design the reward function inspired by the classical dynamic oracle, and use Deep Q-Learning (DQN) techniques to train the oracle with gold trees as features. The combination of a priori knowledge and data-driven methods enables an efficient dynamic oracle, which improves the parser performance over static oracles in several transition systems."
W18-4509,Supervised Rhyme Detection with {S}iamese Recurrent Networks,2018,-1,-1,2,0,5482,thomas haider,"Proceedings of the Second Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"We present the first supervised approach to rhyme detection with Siamese Recurrent Networks (SRN) that offer near perfect performance (97{\%} accuracy) with a single model on rhyme pairs for German, English and French, allowing future large scale analyses. SRNs learn a similarity metric on variable length character sequences that can be used as judgement on the distance of imperfect rhyme pairs and for binary classification. For training, we construct a diachronically balanced rhyme goldstandard of New High German (NHG) poetry. For further testing, we sample a second collection of NHG poetry and set of contemporary Hip-Hop lyrics, annotated for rhyme and assonance. We train several high-performing SRN models and evaluate them qualitatively on selected sonnetts."
N18-1066,Polyglot Semantic Parsing in {API}s,2018,49,6,3,1,3542,kyle richardson,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Traditional approaches to semantic parsing (SP) work by training individual models for each available parallel dataset of text-meaning pairs. In this paper, we explore the idea of polyglot semantic translation, or learning semantic parsing models that are trained on multiple datasets and natural languages. In particular, we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn (2017b,a). The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages, or mixed input languages, using a single unified model. To facilitate modeling of this type, we develop a novel graph-based decoding framework that achieves state-of-the-art performance on the above datasets, and apply this method to two other benchmark SP tasks."
L18-1176,A Lightweight Modeling Middleware for Corpus Processing,2018,0,0,2,1,21818,markus gartner,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1348,Moving {TIGER} beyond Sentence-Level,2018,0,0,3,1,6297,agnieszka falenska,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1457,{G}erman Radio Interviews: The {GRAIN} Release of the {SFB}732 Silver Standard Collection,2018,0,1,9,1,30021,katrin schweitzer,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-2026,{NLAT}ool: an Application for Enhanced Deep Text Understanding,2018,0,1,9,1,21818,markus gartner,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"Today, we see an ever growing number of tools supporting text annotation. Each of these tools is optimized for specific use-cases such as named entity recognition. However, we see large growing knowledge bases such as Wikipedia or the Google Knowledge Graph. In this paper, we introduce NLATool, a web application developed using a human-centered design process. The application combines supporting text annotation and enriching the text with additional information from a number of sources directly within the application. The tool assists users to efficiently recognize named entities, annotate text, and automatically provide users additional information while solving deep text understanding tasks."
C18-1298,"Bridging resolution: Task definition, corpus resources and rule-based experiments",2018,0,2,3,0.392157,28149,ina roesiger,Proceedings of the 27th International Conference on Computational Linguistics,0,"Recent work on bridging resolution has so far been based on the corpus ISNotes (Markert et al. 2012), as this was the only corpus available with unrestricted bridging annotation. Hou et al. 2014{'}s rule-based system currently achieves state-of-the-art performance on this corpus, as learning-based approaches suffer from the lack of available training data. Recently, a number of new corpora with bridging annotations have become available. To test the generalisability of the approach by Hou et al. 2014, we apply a slightly extended rule-based system to these corpora. Besides the expected out-of-domain effects, we also observe low performance on some of the in-domain corpora. Our analysis shows that this is the result of two very different phenomena being defined as bridging, namely referential and lexical bridging. We also report that filtering out gold or predicted coreferent anaphors before applying the bridging resolution system helps improve bridging resolution."
W17-3516,The {C}ode2{T}ext Challenge: Text Generation in Source Libraries,2017,21,1,3,1,3542,kyle richardson,Proceedings of the 10th International Conference on Natural Language Generation,0,"We propose a new shared task for tactical data-to-text generation in the domain of source code libraries. Specifically, we focus on text generation of function descriptions from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction, and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets."
P17-1148,Learning Semantic Correspondences in Technical Documentation,2017,37,0,2,1,3542,kyle richardson,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals."
K17-3004,{IMS} at the {C}o{NLL} 2017 {UD} Shared Task: {CRF}s and Perceptrons Meet Neural Networks,2017,0,7,4,0.88559,18856,anders bjorkelund,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This paper presents the IMS contribution to the CoNLL 2017 Shared Task. In the preprocessing step we employed a CRF POS/morphological tagger and a neural tagger predicting supertags. On some languages, we also applied word segmentation with the CRF tagger and sentence segmentation with a perceptron-based parser. For parsing we took an ensemble approach by blending multiple instances of three parsers with very different architectures. Our system achieved the third place overall and the second place for the surprise languages."
D17-2012,Function Assistant: A Tool for {NL} Querying of {API}s,2017,5,5,2,1,3542,kyle richardson,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries, or descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of (Richardson and Kuhn, 2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github."
D17-1288,Multi-modular domain-tailored {OCR} post-correction,2017,18,4,2,0.952381,17771,sarah schulz,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"One of the main obstacles for many Digital Humanities projects is the low data availability. Texts have to be digitized in an expensive and time consuming process whereas Optical Character Recognition (OCR) post-correction is one of the time-critical factors. At the example of OCR post-correction, we show the adaptation of a generic system to solve a specific problem with little data. The system accounts for a diversity of errors encountered in OCRed texts coming from different time periods in the domain of literature. We show that the combination of different approaches, such as e.g. Statistical Machine Translation and spell checking, with the help of a ranking mechanism tremendously improves over single-handed approaches. Since we consider the accessibility of the resulting tool as a crucial part of Digital Humanities collaborations, we describe the workflow we suggest for efficient text recognition and subsequent automatic and manual post-correction"
W16-4001,Flexible and Reliable Text Analytics in the Digital Humanities {--} Some Methodological Considerations,2016,0,0,1,1,999,jonas kuhn,Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities ({LT}4{DH}),0,"The availability of Language Technology Resources and Tools generates a considerable methodological potential in the Digital Humanities: aspects of research questions from the Humanities and Social Sciences can be addressed on text collections in ways that were unavailable to traditional approaches. I start this talk by sketching some sample scenarios of Digital Humanities projects which involve various Humanities and Social Science disciplines, noting that the potential for a meaningful contribution to higher-level questions is highest when the employed language technological models are carefully tailored both (a) to characteristics of the given target corpus, and (b) to relevant analytical subtasks feeding the discipline-specific research questions. Keeping up a multidisciplinary perspective, I then point out a recurrent dilemma in Digital Humanities projects that follow the conventional set-up of collaboration: to build high-quality computational models for the data, fixed analytical targets should be specified as early as possible {--} but to be able to respond to Humanities questions as they evolve over the course of analysis, the analytical machinery should be kept maximally flexible. To reach both, I argue for a novel collaborative culture that rests on a more interleaved, continuous dialogue. (Re-)Specification of analytical targets should be an ongoing process in which the Humanities Scholars and Social Scientists play a role that is as important as the Computational Scientists{'} role. A promising approach lies in the identification of re-occurring types of analytical subtasks, beyond linguistic standard tasks, which can form building blocks for text analysis across disciplines, and for which corpus-based characterizations (viz. annotations) can be collected, compared and revised. On such grounds, computational modeling is more directly tied to the evolving research questions, and hence the seemingly opposing needs of reliable target specifications vs. {``}malleable{''} frameworks of analysis can be reconciled. Experimental work following this approach is under way in the Center for Reflected Text Analytics (CRETA) in Stuttgart."
Q16-1012,Learning to Make Inferences in a Semantic Parsing Task,2016,38,2,2,1,3542,kyle richardson,Transactions of the Association for Computational Linguistics,0,"We introduce a new approach to training a semantic parser that uses textual entailment judgements as supervision. These judgements are based on high-level inferences about whether the meaning of one sentence follows from another. When applied to an existing semantic parsing task, they prove to be a useful tool for revealing semantic distinctions and background knowledge not captured in the target representations. This information is used to improve the quality of the semantic representations being learned and to acquire generic knowledge for reasoning. Experiments are done on the benchmark Sportscaster corpus (Chen and Mooney, 2008), and a novel RTE-inspired inference dataset is introduced. On this new dataset our method strongly outperforms several strong baselines. Separately, we obtain state-of-the-art results on the original Sportscaster semantic parsing task."
P16-1181,How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents,2016,34,1,4,0.88559,18856,anders bjorkelund,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We cast sentence boundary detection and syntactic parsing as a joint problem, so an entire text document forms a training instance for transition-based dependency parsing. When trained with an early update or max-violation strategy for inexact search, we observe that only a tiny part of these very long training instances is ever exploited. We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task. When we use an alternative update strategy, our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation. A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection."
L16-1024,{IMS} {H}ot{C}oref {DE}: A Data-driven Co-reference Resolver for {G}erman,2016,18,1,2,0.392157,28149,ina roesiger,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a data-driven co-reference resolution system for German that has been adapted from IMS HotCoref, a co-reference resolver for English. It describes the difficulties when resolving co-reference in German text, the adaptation process and the features designed to address linguistic challenges brought forth by German. We report performance on the reference dataset T{\""u}Ba-D/Z and include a post-task SemEval 2010 evaluation, showing that the resolver achieves state-of-the-art performance. We also include ablation experiments that indicate that integrating linguistic features increases results. The paper also describes the steps and the format necessary to use the resolver on new texts. The tool is freely available for download."
L16-1684,Learning from Within? Comparing {P}o{S} Tagging Approaches for Historical Text,2016,0,1,2,0.952381,17771,sarah schulz,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we investigate unsupervised and semi-supervised methods for part-of-speech (PoS) tagging in the context of historical German text. We locate our research in the context of Digital Humanities where the non-canonical nature of text causes issues facing an Natural Language Processing world in which tools are mainly trained on standard data. Data deviating from the norm requires tools adjusted to this data. We explore to which extend the availability of such training material and resources related to it influences the accuracy of PoS tagging. We investigate a variety of algorithms including neural nets, conditional random fields and self-learning techniques in order to find the best-fitted approach to tackle data sparsity. Although methods using resources from related languages outperform weakly supervised methods using just a few training examples, we can still reach a promising accuracy with methods abstaining additional resources."
C16-1140,Named Entity Disambiguation for little known referents: a topic-based approach,2016,26,1,2,1,35760,andrea glaser,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We propose an approach to Named Entity Disambiguation that avoids a problem of standard work on the task (likewise affecting fully supervised, weakly supervised, or distantly supervised machine learning techniques): the treatment of name mentions referring to people with no (or very little) coverage in the textual training data is systematically incorrect. We propose to indirectly take into account the property information for the {``}non-prominent{''} name bearers, such as nationality and profession (e.g., for a Canadian law professor named Michael Jackson, with no Wikipedia article, it is very hard to obtain reliable textual training data). The target property information for the entities is directly available from name authority files, or inferrable, e.g., from listings of sportspeople etc. Our proposed approach employs topic modeling to exploit textual training data based on entities sharing the relevant properties. In experiments with a pilot implementation of the general approach, we show that the approach does indeed work well for name/referent pairs with limited textual coverage in the training data."
W15-2908,Towards Opinion Mining from Reviews for the Prediction of Product Rankings,2015,21,2,3,1,36876,wiltrud kessler,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Opinion mining aims at summarizing the content of reviews for a specific brand, product, or manufacturer. However, the actual desire of a user is often one step further: Produce a ranking corresponding to specific needs such that a selection process is supported. In this work, we aim towards closing this gap. We present the task to rank products based on sentiment information and discuss necessary steps towards addressing this task. This includes, on the one hand, the identification of gold rankings as a fundament for an objective function and evaluation and, on the other hand, methods to rank products based on review information. To demonstrate early results on that task, we employ real world examples of rankings as gold standard that are of interest to potential customers as well as product managers, in our case the sales ranking provided by Amazon.com and the quality ranking by Snapsort.com. As baseline methods, we use the average star ratings and review frequencies. Our best textbased approximation of the sales ranking achieves a Spearmanxe2x80x99s correlation coefficient of = 0.23. On the Snapsort data, a ranking based on extracting comparisons leads to = 0.51. In addition, we show that aspect-specific rankings can be used to measure the impact of specific aspects on the ranking."
W15-0706,A Pilot Experiment on Exploiting Translations for Literary Studies on Kafka{'}s {``}Verwandlung{''},2015,15,0,3,0,30805,fabienne cap,Proceedings of the Fourth Workshop on Computational Linguistics for Literature,0,"We present a manually annotated word alignment of Franz Kafkaxe2x80x99s xe2x80x9cVerwandlungxe2x80x9d and use this as a controlled test case to assess the principled usefulness of word alignment as an additional information source for the (monolingually motivated) identification of literary characters, focusing on the technically wellexplored task of co-reference resolution. This pilot set-up allows us to illustrate a number of methodological components interacting in a modular architecture. In general, co-reference resolution is a relatively hard task, but the availability of word-aligned translations can provide additional indications, as there is a tendency for translations to explicate underspecified or vague passages."
R15-1037,Structural Alignment for Comparison Detection,2015,16,1,2,1,36876,wiltrud kessler,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"There tends to be a substantial proportion of reviews that include explicit textual comparisons between the reviewed item and another product. To the extent that such comparisons can be captured reliably by automatic means, they can provide an extremely helpful input to support a process of choice. As the small amount of available training data limits the development of robust systems to automatically detect comparisons, this paper investigates how to use semi-supervised strategies to expand a small set of labeled sentences. Specifically, we use structural alignment, a method that starts out from a seed set of manually annotated data and finds similar unlabeled sentences to which the labels can be projected. We present several adaptations of the method to our task of comparison detection and show that adding the found expansion sentences slightly improves over a non-expanded baseline in low-resource settings, i.e., when a very small amount of training data is available."
P15-4005,Multi-modal Visualization and Search for Text and Prosody Annotations,2015,14,0,4,1,21818,markus gartner,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"We present ICARUS for intonation, an interactive tool to browse and search automatically derived descriptions of fundamental frequency contours. It offers access to tonal features in combination with other annotation layers like part-ofspeech, syntax or coreference and visualizes them in a highly customizable graphical interface with various playback functions. The built-in search allows multilevel queries, the construction of which can be done graphically or textually, and includes the ability to search F0 contours based on various similarity measures."
P14-5002,"Visualization, Search, and Error Analysis for Coreference Annotations",2014,15,9,5,1,21818,markus gartner,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present the ICARUS Coreference Explorer, an interactive tool to browse and search coreference-annotated data. It can display coreference annotations as a tree, as an entity grid, or in a standard textbased display mode, and lets the user switch freely between the different modes. The tool can compare two different annotations on the same document, allowing system developers to evaluate errors in automatic system predictions. It features a flexible search engine, which enables the user to graphically construct search queries over sets of documents annotated with coreference."
P14-1005,Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features,2014,37,50,2,0.833333,18856,anders bjorkelund,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We investigate different ways of learning structured perceptron models for coreference resolution when using non-local features and beam search. Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features. By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline. Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English."
kessler-kuhn-2014-corpus,A Corpus of Comparisons in Product Reviews,2014,14,13,2,1,36876,wiltrud kessler,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Sentiment analysis (or opinion mining) deals with the task of determining the polarity of an opinionated document or sentence. Users often express sentiment about one product by comparing it to a different product. In this work, we present a corpus of comparison sentences from English camera reviews. For our purposes we define a comparison to be any statement about the similarity or difference of two entities. For each sentence we have annotated detailed information about the comparisons it contains: The comparative predicate that expresses the comparison, the type of the comparison, the two entities that are being compared, and the aspect they are compared in. The results of our agreement study show that the decision whether a sentence contains a comparison is difficult to make even for trained human annotators. Once that decision is made, we can achieve consistent results for the very detailed annotations. In total, we have annotated 2108 comparisons in 1707 sentences from camera reviews which makes our corpus the largest resource currently available. The corpus and the annotation guidelines are publicly available on our website."
blessing-kuhn-2014-textual,Textual Emigration Analysis ({TEA}),2014,5,2,2,1,1033,andre blessing,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a web-based application which is called TEA (Textual Emigration Analysis) as a showcase that applies textual analysis for the humanities. The TEA tool is used to transform raw text input into a graphical display of emigration source and target countries (under a global or an individual perspective). It provides emigration-related frequency information, and gives access to individual textual sources, which can be downloaded by the user. Our application is built on top of the CLARIN infrastructure which targets researchers of the humanities. In our scenario, we focus on historians, literary scientists, and other social scientists that are interested in the semantic interpretation of text. Our application processes a large set of documents to extract information about people who emigrated. The current implementation integrates two data sets: A data set from the Global Migrant Origin Database, which does not need additional processing, and a data set which was extracted from the German Wikipedia edition. The TEA tool can be accessed by using the following URL: http://clarin01.ims.uni-stuttgart.de/geovis/showcase.html"
ghayoomi-kuhn-2014-converting,Converting an {HPSG}-based Treebank into its Parallel Dependency-based Treebank,2014,26,3,2,0,39502,masood ghayoomi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"A treebank is an important language resource for supervised statistical parsers. The parser induces the grammatical properties of a language from this language resource and uses the model to parse unseen data automatically. Since developing such a resource is very time-consuming and tedious, one can take advantage of already extant resources by adapting them to a particular application. This reduces the amount of human effort required to develop a new language resource. In this paper, we introduce an algorithm to convert an HPSG-based treebank into its parallel dependency-based treebank. With this converter, we can automatically create a new language resource from an existing treebank developed based on a grammar formalism. Our proposed algorithm is able to create both projective and non-projective dependency trees."
seeker-kuhn-2014-domain,An Out-of-Domain Test Suite for Dependency Parsing of {G}erman,2014,20,2,2,1,17921,wolfgang seeker,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a dependency conversion of five German test sets from five different genres. The dependency representation is made as similar as possible to the dependency representation of TiGer, one of the two big syntactic treebanks of German. The purpose of these test sets is to enable researchers to test dependency parsing models on several different data sets from different text genres. We discuss some easy to compute statistics to demonstrate the variation and differences in the test sets and provide some baseline experiments where we test the effect of additional lexical knowledge on the out-of-domain performance of two state-of-the-art dependency parsers. Finally, we demonstrate with three small experiments that text normalization may be an important step in the standard processing pipeline when applied in an out-of-domain setting."
richardson-kuhn-2014-unixman,{U}nix{M}an Corpus: A Resource for Language Learning in the {U}nix Domain,2014,13,1,2,1,3542,kyle richardson,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a new resource, the UnixMan Corpus, for studying language learning it the domain of Unix utility manuals. The corpus is built by mining Unix (and other Unix related) man pages for parallel example entries, consisting of English textual descriptions with corresponding command examples. The commands provide a grounded and ambiguous semantics for the textual descriptions, making the corpus of interest to work on Semantic Parsing and Grounded Language Learning. In contrast to standard resources for Semantic Parsing, which tend to be restricted to a small number of concepts and relations, the UnixMan Corpus spans a wide variety of utility genres and topics, and consists of hundreds of command and domain entity types. The semi-structured nature of the manuals also makes it easy to exploit other types of relevant information for Grounded Language Learning. We describe the details of the corpus and provide preliminary classification results."
glaser-kuhn-2014-exploring,Exploring the utility of coreference chains for improved identification of personal names,2014,19,0,2,1,35760,andrea glaser,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Identifying the real world entity that a proper name refers to is an important task in many NLP applications. Context plays an important role in disambiguating entities with the same names. In this paper, we discuss a dataset and experimental set-up that allows us to systematically explore the effects of different sizes and types of context in this disambiguation task. We create context by first identifying coreferent expressions in the document and then combining sentences these expressions occur in to one informative context. We apply different filters to obtain different levels of coreference-based context. Since hand-labeling a dataset of a decent size is expensive, we investigate the usefulness of an automatically created pseudo-ambiguity dataset. The results on this pseudo-ambiguity dataset show that using coreference-based context performs better than using a fixed window of context around the entity. The insights taken from the pseudo data experiments can be used to predict how the method works with real data. In our experiments on real data we obtain comparable results."
E14-2015,A Graphical Interface for Automatic Error Mining in Corpora,2014,8,3,5,0,39090,gregor thiele,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present an error mining tool that is designed to help human annotators to find errors and inconsistencies in their annotation. The output of the underlying algorithm is accessible via a graphical user interface, which provides two aggregate views: a list of potential errors in context and a distribution over labels. The user can always directly access the actual sentence containing the potential error, thus enabling annotators to quickly judge whether the found candidate is indeed incorrectly labeled."
W13-3704,Towards Joint Morphological Analysis and Dependency Parsing of {T}urkish,2013,28,7,2,0,6298,ozlem ccetinouglu,Proceedings of the Second International Conference on Dependency Linguistics ({D}ep{L}ing 2013),0,"Turkish is an agglutinative language with rich morphology-syntax interactions. As an extension of this property, the Turkish Treebank is designed to represent sublexical dependencies, which brings extra challenges to parsing raw text. In this work, we use a joint POS tagging and parsing approach to parse Turkish raw text, and we show it outperforms a pipeline approach. Then we experiment with incorporating morphological feature prediction into the joint system. Our results show statistically significant improvements with the joint systems and achieve the state-ofthe-art accuracy for Turkish dependency parsing."
W13-2708,Towards a Tool for Interactive Concept Building for Large Scale Analysis in the Humanities,2013,22,7,5,1,1033,andre blessing,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We develop a pipeline consisting of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. Our main focus is the interaction between the political scientists and the natural language processing groups to ensure a beneficial assistance for the political scientists and new application challenges for NLP. It is of particular importance to find a xe2x80x9ccommon languagexe2x80x9d between the different disciplines. Therefore, we use an interactive web-interface which is easily usable by non-experts. It interfaces an active learning algorithm which is complemented by the NLP pipeline to provide a rich feature selection. Political scientists are thus enabled to use their own intuitions to find custom concepts."
P13-4010,{ICARUS} {--} An Extensible Graphical Search Tool for Dependency Treebanks,2013,9,18,5,1,21818,markus gartner,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present ICARUS, a versatile graphical search tool to query dependency treebanks. Search results can be inspected both quantitatively and qualitatively bymeans of frequency lists, tables, or dependency graphs. ICARUS also ships with plugins that enable it to interface with tool chains running either locally or remotely."
P13-1152,Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures,2013,39,8,2,1,1567,sina zarriess,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture."
J13-1004,Morphological and Syntactic Case in Statistical Dependency Parsing,2013,47,18,2,1,17921,wolfgang seeker,Computational Linguistics,0,"Most morphologically rich languages with free word order use case systems to mark the grammatical function of nominal elements, especially for the core argument functions of a verb. The standard pipeline approach in syntactic dependency parsing assumes a complete disambiguation of morphological case information prior to automatic syntactic analysis. Parsing experiments on Czech, German, and Hungarian show that this approach is susceptible to propagating morphological annotation errors when parsing languages displaying syncretism in their morphological case paradigms. We develop a different architecture where we use case as a possibly underspecified filtering device restricting the options for syntactic analysis. Carefully designed morpho-syntactic constraints can delimit the search space of a statistical dependency parser and exclude solutions that would violate the restrictions overtly marked in the morphology of the words in a given sentence. The constrained system outperforms a state-of-the-art data-driven pipeline architecture, as we show experimentally, and, in addition, the parser output comes with guarantees about local and global morpho-syntactic wellformedness, which can be useful for downstream applications."
D13-1033,The Effects of Syntactic Features in Automatic Prediction of Morphology,2013,32,2,2,1,17921,wolfgang seeker,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.
D13-1194,Detection of Product Comparisons - How Far Does an Out-of-the-Box Semantic Role Labeling System Take You?,2013,17,17,2,1,36876,wiltrud kessler,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons. An (opinionated) comparison consists of a comparative xe2x80x9cpredicatexe2x80x9d and up to three xe2x80x9cargumentsxe2x80x9d: the entity evaluated positively, the entity evaluated negatively, and the aspect under which the comparison is made. In user-generated product reviews, the xe2x80x9cpredicatexe2x80x9d and xe2x80x9cargumentsxe2x80x9d are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable. We address the interesting question how well training an outof-the-box SRL model works for English data. We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets."
seeker-kuhn-2012-making,Making Ellipses Explicit in Dependency Conversion for a {G}erman Treebank,2012,21,42,2,1,17921,wolfgang seeker,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present a carefully designed dependency conversion of the German phrase-structure treebank TiGer that explicitly represents verb ellipses by introducing empty nodes into the tree. Although the conversion process uses heuristics like many other conversion tools we designed them to fail if no reasonable solution can be found. The failing of the conversion process makes it possible to detect elliptical constructions where the head is missing, but it also allows us to find errors in the original annotation. We discuss the conversion process and the heuristics, and describe some design decisions and error corrections that we applied to the corpus. Since most of today's data-driven dependency parsers are not able to handle empty nodes directly during parsing, our conversion tool also derives a canonical dependency format without empty nodes. It is shown experimentally to be well suited for training statistical dependency parsers by comparing the performance of two parsers from different parsing paradigms on the data set of the CoNLL 2009 Shared Task data and our corpus."
ziering-etal-2012-corpus,A Corpus-based Study of the {G}erman Recipient Passive,2012,15,0,3,0,32558,patrick ziering,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we investigate the usage of a non-canonical German passive alternation for ditransitive verbs, the recipient passive, in naturally occuring corpus data. We propose a classifier that predicts the voice of a ditransitive verb based on the contextually determined properties its arguments. As the recipient passive is a low frequent phenomenon, we first create a special data set focussing on German ditransitive verbs which are frequently used in the recipient passive. We use a broad-coverage grammar-based parser, the German LFG parser, to automatically annotate our data set for the morpho-syntactic properties of the involved predicate arguments. We train a Maximum Entropy classifier on the automatically annotated sentences and achieve an accuracy of 98.05{\%}, clearly outperforming the baseline that always predicts active voice baseline (94.6{\%})."
E12-1009,The Best of Both Worlds {--} A Graph-based Completion Model for Transition-based Parsers,2012,40,51,2,0.169483,16528,bernd bohnet,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Transition-based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available. In this paper, we describe a model that takes into account complete structures as they become available to rescore the elements of a beam, combining the advantages of transition-based and graph-based approaches. We also propose an efficient implementation that allows for the use of sophisticated features and show that the completion model leads to a substantial increase in accuracy. We apply the new transition-based parser on typologically different languages such as English, Chinese, Czech, and German and report competitive labeled and unlabeled attachment scores."
E12-1078,To what extent does sentence-internal realisation reflect discourse context? A study on word order,2012,24,2,3,1,1567,sina zarriess,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We compare the impact of sentence-internal vs. sentence-external features on word order prediction in two generation settings: starting out from a discriminative surface realisation ranking model for an LFG grammar of German, we enrich the feature set with lexical chain features from the discourse context which can be robustly detected and reflect rough grammatical correlates of notions from theoretical approaches to discourse coherence. In a more controlled setting, we develop a constituent ordering classifier that is trained on a German treebank with gold coreference annotation. Surprisingly, in both settings, the sentence-external features perform poorly compared to the sentence-internal ones, and do not improve over a baseline model capturing the syntactic functions of the constituents."
D12-1085,Generating Non-Projective Word Order in Statistical Linearization,2012,33,5,3,0.169483,16528,bernd bohnet,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We propose a technique to generate non-projective word orders in an efficient statistical linearization system. Our approach predicts liftings of edges in an unordered syntactic tree by means of a classifier, and uses a projective algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech."
C12-2014,Comparing Non-projective Strategies for Labeled Graph-Based Dependency Parsing,2012,16,1,2,0.833333,18856,anders bjorkelund,Proceedings of {COLING} 2012: Posters,0,"We fill a gap in the systematically analyzed space of available techniques for state-of-the-art dependency parsing by comparing non-projective strategies for graph-based parsers. Using three languages with varying frequency of non-projective constructions, we compare the non-projective approximation algorithm with pseudo-projective parsing. We also analyze the differences between different encoding schemes for pseudo-projective parsing. We find only minor differences between the encoding schemes for pseudo-projective parsing, and that the non-projective approximation algorithm is superior to pseudo-projective parsing."
C12-2015,Phrase Structures and Dependencies for End-to-End Coreference Resolution,2012,15,5,2,0.833333,18856,anders bjorkelund,Proceedings of {COLING} 2012: Posters,0,"We present experiments in data-driven coreference resolution comparing the effect of different syntactic representations provided as features in the coreference classification step: no syntax, phrase structure representations, dependency representations, and combinations of the representation types. We compare the end-to-end performance of a parametrized state-of-the-art coreference resolution system on the English data from the CoNLL 2012 shared task. On their own, phrase structures are more useful than dependencies, but the combinations yield highest performance and a significant improvement on the resolution of pronouns. Enriching phrase structure with dependency trees obtained from an independent parser is most helpful, but an extension of the predicted phrase structure using just pattern-based phraseto-dependency conversion seems to provide signals for the machine learning that cannot be distilled from phrase structure alone (despite intense feature selection). This is an interesting result for a highly configurational language: It is easier to learn generalizations over grammatical constraints on coreference when grammatical relations are explicitly provided."
C12-2098,Light Textual Inference for Semantic Parsing,2012,29,1,2,1,3542,kyle richardson,Proceedings of {COLING} 2012: Posters,0,"There has been a lot of recent interest in Semantic Parsing, centering on using data-driven techniques for mapping natural language to full semantic representations (Mooney, 2007). One particular focus has been on learning with ambiguous supervision (Chen and Mooney, 2008; Kim and Mooney, 2012), where the goal is to model language learning within broader perceptual contexts (Mooney, 2008). We look at learning light inference patterns for Semantic Parsing within this paradigm, focusing on detecting speaker commitments about events under discussion (Nairn et al., 2006; Karttunen, 2012). We adapt PCFG induction techniques (Borschinger et al., 2011; Johnson et al., 2012) for learning inference using event polarity and context as supervision, and demonstrate the effectiveness of our approach on a modified portion of the Grounded World corpus (Bordes et al., 2010)."
C12-2105,Data-driven Dependency Parsing With Empty Heads,2012,24,8,5,1,17921,wolfgang seeker,Proceedings of {COLING} 2012: Posters,0,"Syntactic dependency structures are based on the assumption that there is exactly one node in the structure for each word in the sentence. However representing elliptical constructions (e.g. missing verbs) is problematic as the question where the dependents of the elided material should be attached to has to be solved. In this paper, we present an in-depth study into the challenges of introducing empty heads into dependency structures during automatic parsing. Structurally, empty heads provide an attachment site for the dependents of the non-overt material and thus preserve the linguistically plausible structure of the sentence. We compare three different (computational) approaches to the introduction of empty heads and evaluate them against German and Hungarian data. We then conduct a fine-grained error analysis on the output of one of the approaches to highlight some of the difficulties of the task. We find that while a clearly defined part of the phenomena can be learned by the parser, more involved elliptical structures are still mostly out of reach of the automatic tools."
W11-2908,On the Role of Explicit Morphological Feature Representation in Syntactic Dependency Parsing for {G}erman,2011,11,3,2,1,17921,wolfgang seeker,Proceedings of the 12th International Conference on Parsing Technologies,0,"We investigate the question whether an explicit feature representation for morphological features is necessary when parsing German with a fully lexicalized, statistical dependency parser. We use two morphosyntactic phenomena of German to show that while lexicalization does indeed suffice to a large extent when recovering the internal structure of noun phrases, an accurate explicit representation can support the correct selection of its grammatical function."
P11-1101,Underspecifying and Predicting Voice for Surface Realisation Ranking,2011,29,6,3,1,1567,sina zarriess,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice."
W10-2106,A Cross-Lingual Induction Technique for {G}erman Adverbial Participles,2010,10,2,3,1,1567,sina zarriess,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"We provide a detailed comparison of strategies for implementing medium-to-low frequency phenomena such as German adverbial participles in a broad-coverage, rule-based parsing system. We show that allowing for general adverb conversion of participles in the German LFG grammar seriously affects its overall performance, due to increased spurious ambiguity. As a solution, we present a corpus-based cross-lingual induction technique that detects adverbially used participles in parallel text. In a grammar-based evaluation, we show that the automatically induced resource appropriately restricts the adverb conversion to a limited class of participles, and improves parsing quantitatively as well as qualitatively."
P10-1111,Hard Constraints for Grammatical Function Labelling,2010,33,8,3,1,17921,wolfgang seeker,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebank-based (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation."
bouma-etal-2010-towards,Towards a Large Parallel Corpus of Cleft Constructions,2010,16,3,3,0,2746,gerlof bouma,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present our efforts to create a large-scale, semi-automatically annotated parallel corpus of cleft constructions. The corpus is intended to reduce or make more effective the manual task of finding examples of clefts in a corpus. The corpus is being developed in the context of the Collaborative Research Centre SFB 632, which is a large, interdisciplinary research initiative to study information structure, at the University of Potsdam and the Humboldt University in Berlin. The corpus is based on the Europarl corpus (version 3). We show how state-of-the-art NLP tools, like POS taggers and statistical dependency parsers, may facilitate powerful and precise searches. We argue that identifying clefts using automatically added syntactic structure annotation is ultimately to be preferred over using lower level, though more robust, extraction methods like regular expression matching. An evaluation of the extraction method for one of the languages also offers some support for this method. We end the paper by discussing the resulting corpus itself. We present some examples of interesting clefts and translational counterparts from the corpus and suggest ways of exploiting our newly created resource in the cross-linguistic study of clefts."
dione-etal-2010-design,"Design and Development of Part-of-Speech-Tagging Resources for {W}olof ({N}iger-{C}ongo, spoken in {S}enegal)",2010,-1,-1,2,0,5821,cheikh dione,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we report on the design of a part-of-speech-tagset for Wolof and on the creation of a semi-automatically annotated gold standard. In order to achieve high-quality annotation relatively fast, we first generated an accurate lexicon that draws on existing word and name lists and takes into account inflectional and derivational morphology. The main motivation for the tagged corpus is to obtain data for training automatic taggers with machine learning approaches. Hence, we took machine learning considerations into account during tagset design and we present training experiments as part of this paper. The best automatic tagger achieves an accuracy of 95.2{\%} in cross-validation experiments. We also wanted to create a basis for experimenting with annotation projection techniques, which exploit parallel corpora. For this reason, it was useful to use a part of the Bible as the gold standard corpus, for which sentence-aligned parallel versions in many languages are easy to obtain. We also report on preliminary experiments exploiting a statistical word alignment of the parallel text."
spreyer-etal-2010-training,Training Parsers on Partial Trees: A Cross-language Comparison,2010,15,12,3,1,46263,kathrin spreyer,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a study that compares data-driven dependency parsers obtained by means of annotation projection between language pairs of varying structural similarity. We show how the partial dependency trees projected from English to Dutch, Italian and German can be exploited to train parsers for the target languages. We evaluate the parsers against manual gold standard annotations and find that the projected parsers substantially outperform our heuristic baselines by 925{\%} UAS, which corresponds to a 2143{\%} reduction in error rate. A comparative error analysis focuses on how the projected target language parsers handle subjects, which is especially interesting for Italian as an instance of a pro-drop language. For Dutch, we further present experiments with German as an alternative source language. In both source languages, we contrast standard baseline parsers with parsers that are enhanced with the predictions from large-scale LFG grammars through a technique of parser stacking, and show that improvements of the source language parser can directly lead to similar improvements of the projected target language parser."
C10-2129,Informed ways of improving data-driven dependency parsing for {G}erman,2010,28,8,4,1,17921,wolfgang seeker,Coling 2010: Posters,0,"We investigate a series of targeted modifications to a data-driven dependency parser of German and show that these can be highly effective even for a relatively well studied language like German if they are made on a (linguistically and methodologically) informed basis and with a parser implementation that allows for fast and robust training and application. Making relatively small changes to a range of very different system components, we were able to increase labeled accuracy on a standard test set (from the CoNLL 2009 shared task), ignoring gold standard part-of-speech tags, from 87.64% to 89.40%. The study was conducted in less than five weeks and as a secondary project of all four authors. Effective modifications include the quality and combination of auto-assigned morphosyntactic features entering machine learning, the internal feature handling as well as the inclusion of global constraints and a combination of different parsing strategies."
C10-2163,Cross-Lingual Induction for Deep Broad-Coverage Syntax: A Case Study on {G}erman Participles,2010,11,0,3,1,1567,sina zarriess,Coling 2010: Posters,0,"This paper is a case study on cross-lingual induction of lexical resources for deep, broad-coverage syntactic analysis of German. We use a parallel corpus to induce a classifier for German participles which can predict their syntactic category. By means of this classifier, we induce a resource of adverbial participles from a huge monolingual corpus of German. We integrate the resource into a German LFG grammar and show that it improves parsing coverage while maintaining accuracy."
W09-3831,Using a maximum entropy-based tagger to improve a very fast vine parser,2009,11,3,2,0,143,anders sogaard,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"In this short paper, an off-the-shelf maximum entropy-based POS-tagger is used as a partial parser to improve the accuracy of an extremely fast linear time dependency parser that provides state-of-the-art results in multilingual unlabeled POS sequence parsing."
W09-2904,Exploiting Translational Correspondences for Pattern-Independent {MWE} Identification,2009,14,20,2,1,1567,sina zarriess,"Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications ({MWE} 2009)",0,"Based on a study of verb translations in the Europarl corpus, we argue that a wide range of MWE patterns can be identified in translations that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language. We show that these correspondences can be reliably detected on dependency-parsed, word-aligned sentences. We propose an extraction method that combines word alignment with syntactic filters and is independent of the structural pattern of the translation."
W09-2303,Empirical Lower Bounds on Aligment Error Rates in Syntax-Based Machine Translation,2009,27,21,2,0,143,anders sogaard,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax-based machine translation systems such as Wu (1997), Zhang et al. (2006) and Chiang (2007), in terms of what alignments they induce, has been discussed in Wu (1997) and Wellington et al. (2006), but with a one-sided focus on so-called inside-out alignments. Other alignment configurations that cannot be induced by 2-SCFGs are identified in this paper, and their frequencies across a wide collection of hand-aligned parallel corpora are examined. Empirical lower bounds on two measures of alignment error rate, i.e. the one introduced in Och and Ney (2000) and one where only complete translation units are considered, are derived for 2-SCFGs and related formalisms."
W09-1104,Data-Driven Dependency Parsing of New Languages Using Incomplete and Noisy Training Data,2009,24,31,2,1,46263,kathrin spreyer,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL}-2009),0,"We present a simple but very effective approach to identifying high-quality data in noisy data sets for structured problems like parsing, by greedily exploiting partial structures. We analyze our approach in an annotation projection framework for dependency trees, and show how dependency parsers from two different paradigms (graph-based and transition-based) can be trained on the resulting tree fragments. We train parsers for Dutch to evaluate our method and to investigate to which degree graph-based and transition-based parsers can benefit from incomplete training data. We find that partial correspondence projection gives rise to parsers that out-perform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006)."
P09-2010,Improving data-driven dependency parsing using large-scale {LFG} grammars,2009,11,17,2,0.666667,2622,lilja ovrelid,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"This paper presents experiments which combine a grammar-driven and a data-driven parser. We show how the conversion of LFG output to dependency representation allows for a technique of parser stacking, whereby the output of the grammar-driven parser supplies features for a data-driven dependency parser. We evaluate on English and German and show significant improvements stemming from the proposed dependency structure as well as various other, deep linguistic features derived from the respective grammars."
spreyer-etal-2008-identification,Identification of Comparable Argument-Head Relations in Parallel Corpora,2008,17,0,2,1,46263,kathrin spreyer,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present the machine learning framework that we are developing, in order to support explorative search for non-trivial linguistic configurations in low-density languages (languages with no or few NLP tools). The approach exploits advanced existing analysis tools for high-density languages and word-aligned multi-parallel corpora to bridge across languages. The goal is to find a methodology that minimizes the amount of human expert intervention needed, while producing high-quality search and annotation tools. One of the main challenges is the susceptibility of a complex system combining various automatic analysis components to hard-to-control noise from a number of sources. We present systematic experiments investigating to what degree the noise issue can be overcome by (i) exploiting more than one perspective on the target language data by considering multiple translations in the parallel corpus, and (ii) using minimally supervised learning techniques such as co-training and self-training to take advantage of a larger pool of data for generalization. We observe that while (i) does help in the training individual machine learning models, a cyclic bootstrapping process seems to suffer too much from noise. A preliminary conclusion is that in a practical approach, one has to rely on a higher degree of supervision or on noise detection heuristics."
W07-1205,Deep Grammars in a Tree Labeling Approach to Syntax-based Statistical Machine Translation,2007,13,0,2,1,15249,mark hopkins,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"In this paper, we propose a new syntaxbased machine translation (MT) approach based on reducing the MT task to a tree-labeling task, which is further decomposed into a sequence of simple decisions for which discriminative classifiers can be trained. The approach is very flexible and we believe that it is particularly well-suited for exploiting the linguistic knowledge encoded in deep grammars whenever possible, while at the same time taking advantage of data-based techniques that have proven a powerful basis for MT, as recent advances in statistical MT show.n n A full system using the Lexical-Functional Grammar (LFG) parsing system XLE and the grammars from the Parallel Grammar development project (ParGram; (Butt et al., 2002)) has been implemented, and we present preliminary results on English-to-German translation with a tree-labeling system trained on a small subsection of the Europarl corpus."
W07-0406,Machine Translation as Tree Labeling,2007,12,7,2,1,15249,mark hopkins,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"We present the main ideas behind a new syntax-based machine translation system, based on reducing the machine translation task to a tree-labeling task. This tree labeling is further reduced to a sequence of decisions (of four varieties), which can be discriminatively trained. The optimal tree labeling (i.e. translation) is then found through a simple depth-first branch-andbound search. An early system founded on these ideas has been shown to be competitive with Pharaoh when both are trained on a small subsection of the Europarl corpus."
W06-2002,A Framework for Incorporating Alignment Information in Parsing,2006,8,1,2,1,15249,mark hopkins,Proceedings of the Cross-Language Knowledge Induction Workshop,0,"The standard PCFG approach to parsing is quite successful on certain domains, but is relatively inflexible in the type of feature information we can include in its probabilistic model. In this work, we discuss preliminary work in developing a new probabilistic parsing model that allows us to easily incorporate many different types of features, including crosslingual information. We show how this model can be used to build a successful parser for a small handmade gold-standard corpus of 188 sentences (in 3 languages) from the Europarl corpus."
P06-2048,Exploring the Potential of Intractable Parsers,2006,11,0,2,1,15249,mark hopkins,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible. We also provide a decoder for this probability model that is linear-space, optimal, and anytime. A parser based on this framework, when evaluated on Section 23 of the Penn Tree-bank, compares favorably with other state-of-the-art approaches, in terms of both accuracy and speed."
kuhn-jellinghaus-2006-multilingual,Multilingual parallel treebanking: a lean and flexible approach,2006,0,3,1,1,999,jonas kuhn,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We propose a bootstrapping approach to creating a phrase-level alignment over a sentence-aligned parallel corpus, reporting concrete treebank annotation work performed on a sample of sentence tuples from the Europarl corpus, currently for English, French, German, and Spanish. The manually annotated seed data will be used as the basis for automatically labelling the rest of the corpus. Some preliminary experiments addressing the bootstrapping aspects are presented.The representation format for syntactic correspondence across parallel text that we propose as the starting point for a process of successive refinement emphasizes correspondences of major constituents that realize semantic arguments or modifiers; language-particular details of morphosyntactic realization are intentionally left largely unlabelled. We believe this format is a good basis for training NLPtools for multilingual application contexts in which consistency across languages is more central than fine-grained details in specific languages (in particular, syntax-based statistical Machine Translation)."
W05-0803,Parsing Word-Aligned Parallel Corpora in a Grammar Induction Context,2005,14,2,1,1,999,jonas kuhn,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"We present an Earley-style dynamic programming algorithm for parsing sentence pairs from a parallel corpus simultaneously, building up two phrase structure trees and a correspondence mapping between the nodes. The intended use of the algorithm is in bootstrapping grammars for less studied languages by using implicit grammatical information in parallel corpora. Therefore, we presuppose a given (statistical) word alignment underlying in the synchronous parsing task; this leads to a significant reduction of the parsing complexity. The theoretical complexity results are corroborated by a quantitative evaluation in which we ran an implementation of the algorithm on a suite of test sentences from the Europarl parallel corpus."
P04-1060,Experiments in parallel-text based grammar induction,2004,10,49,1,1,999,jonas kuhn,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper discusses the use of statistical word alignment over multiple parallel texts for the identification of string spans that cannot be constituents in one of the languages. This information is exploited in monolingual PCFG grammar induction for that language, within an augmented version of the inside-outside algorithm. Besides the aligned corpus, no other resources are required. We discuss an implemented system and present experimental results with an evaluation against the Penn Tree-bank."
palmer-etal-2004-utilization,Utilization of Multiple Language Resources for Robust Grammar-Based Tense and Aspect Classification,2004,7,1,2,0,1316,alexis palmer,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper reports on an ongoing project that uses varied language resources and advanced NLP tools for a linguistic classification task in discourse semantics. The system we present is designed to assign a ``situation entity'' class label to each predicator in English text. The project goal is to achieve the best-possible identification of situation entities in naturally-occurring written texts by implementing a robust system that will deal with real corpus material, rather than just with constructed textbook examples of discourse. In this paper we focus on the combination of multiple information sources, which we see as being vital for a robust classification system. We use a deep syntactic grammar of English to identify morphological, syntactic, and discourse clues, and we use various lexical databases for fine-grained semantic properties of the predicators. Experiments performed to date show that enhancing the output of the grammar with information from lexical resources improves recall but lowers precision in the situation entity classification task."
kuhn-mateo-toledo-2004-applying,"Applying Computational Linguistic Techniques in a Documentary Project for {Q}{'}anjob{'}al ({M}ayan, {G}uatemala)",2004,1,2,1,1,999,jonas kuhn,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper reports on a number of experiments in which we applied standard techniques from NLP in the context of documentation of endangered languages. We concentrated on the use of existing, freely available toolkits. Specifically, we explore the use of Finite-State Morphological Analysis, Maximum Entropy Part-of-Speech Tagging, and N-Gram Language Modeling."
P03-1025,Compounding and Derivational Morphology in a Finite-State Setting,2003,9,0,1,1,999,jonas kuhn,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes the application of finite-state approximation techniques on a unification-based grammar of word formation for a language like German. A refinement of an RTN-based approximation algorithm is proposed, which extends the state space of the automaton by selectively adding distinctions based on the parsing history at the point of entering a context-free rule. The selection of history items exploits the specific linguistic nature of word formation. As experiments show, this algorithm avoids an explosion of the size of the automaton in the approximation construction."
P02-1007,{OT} Syntax {--} Decidability of Generation-based Optimization,2002,14,3,1,1,999,jonas kuhn,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"In Optimality-Theoretic Syntax, optimization with unrestricted expressive power on the side of the OT constraints is undecidable. This paper provides a proof for the decidability of optimization based on constraints expressed with reference to local subtrees (which is in the spirit of OT theory). The proof builds on Kaplan and Wedekind's (2000) construction showing that LFG generation produces context-free languages."
P00-1046,Processing Optimality-theoretic Syntax by Interleaved Chart Parsing and Generation,2000,8,12,1,1,999,jonas kuhn,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"The Earley deduction algorithm is extended for the processing of OT syntax based on feature grammars. Due to faithfulness violations, infinitely many candidates must be compared. With the (reasonable) assumptions (i) that OT constraints are descriptions denoting bounded structures and (ii) that every rule recursion in the base grammar incurs some constraint violation, a chart algorithm can be devised. Interleaving parsing and generation permits the application of generation-based optimization even in the parsing task, i.e., for a string input."
P00-1061,Lexicalized Stochastic Modeling of Constraint-Based Grammars using Log-Linear Measures and {EM} Training,2000,17,62,3,0,1028,stefan riezler,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"We present a new approach to stochastic modeling of constraint-based grammars that is based on loglinear models and uses EM for estimation from unannotated data. The techniques are applied to an LFG grammar for German. Evaluation on an exact match task yields 86% precision for an ambiguity rate of 5.4, and 90% precision on a subcat frame match for an ambiguity rate of 25. Experimental comparison to training from a parsebank shows a 10% gain from EM training. Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models."
C96-2113,An Underspecified {HPSG} Representation for Information Structure,1996,4,8,1,1,999,jonas kuhn,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"Information structure can be of great use in linguistic applications, especially in those involving a speech component. However, focus marking by prosody is often ambiguous. Existing theories capture this by rules that produce alternative focus structures. This disjunction is hard to handle computationally. In this paper, a compact, graphically underspecified representation is proposed, along with composition principles and a resolution routine based on context information."
