2007.mtsummit-ucnlg.18,C92-1038,0,0.0869102,"Missing"
2007.mtsummit-ucnlg.18,P04-1052,1,0.848516,"omains System IDs: CAM-B, CAM-T, CAM-BU and CAM-TU Advaith Siddharthan & Ann Copestake University of Cambridge {as372,aac10}@cl.cam.ac.uk Abstract We present four variations of our 2004 incremental algorithm (Siddharthan and Copestake, 2004), and present results on both the Furniture and People datasets. discriminating power as only one criteria for selecting attributes and allowed for the easy incorporation of other considerations such as reference modification. 2.1 Quantifying Discriminating Power For each attribute of the referent, we define the following three quotients. 1 Introduction In Siddharthan and Copestake (2004), we presented an algorithm for generating referring expressions in open domains. Our algorithm was novel in that it was intended for open domains where attribute classification in infeasible, and in that it provided the first incremental algorithm that could handle relations as well as attributes. In that paper we evaluated our algorithm by trying to reproduce referrring expressions in the Penn WSJ Treebank. Here, we describe four variations of the general method described there and evaluate it on both the furniture and People datasets. 2 Overview of 2004 algorithm GRE algorithms make the fol"
2021.eacl-main.273,P19-1305,0,0.0176637,"design pipelines which learn to translate and summarise separately (Leuski et al., 2003; Or˘asan and Chiorean, 2008). However, such paradigms suffer from the error propagation problem, i.e., errors produced by upstream modules may accumulate and degrade the output quality (Zhu et al., 2020). In addition, parallel data to train effective translators is not always accessible (Cao et al., 2020). Recently, end-to-end methods have been applied to alleviate this issue. The main challenge for this 3124 research direction is the lack of direct corpora, leading to attempts such as zero-shot learning (Duan et al., 2019), multi-task learning (Zhu et al., 2019), and transfer learning (Cao et al., 2020). Although training requirements have been relaxed by these methods, our extreme setup with summarisation data only available for the target language and very limited parallel data, has never been visited before. zh 1600 Year 1700 1750 19 9 Literature 2 3 H IST S UMM Corpus 10 13 3.1 Dataset Construction Politics 41 1 ZH Detailed references are included in the ‘source’ entries of H IST S UMM’s metadata. Sovereign Military 31 In history and digital humanities research, summarisation is most needed when analysing d"
2021.eacl-main.273,D15-1229,0,0.209961,"able versions of Wanli Gazette available; worse still, the calligraphy copies (see Appendix B) are unrecognisable even for non-expert humans, making the OCR technique inapplicable. Therefore, we performed a thorough literature search on over 200 related academic papers and manually retrieved 100 news texts1 . To generate summaries in the respective modern language for these historical news stories, we recruited two experts with degrees in Germanistik and Ancient Chinese Literature, respectively. They were asked to produce summaries in the style of DE MLSUM (Scialom et al., 2020) and ZH LCSTS (Hu et al., 2015), whose news stories and summaries are crawled from the S¨uddeutsche Zeitung website and posts by professional media on the Sina Weibo platform, respectively. The annotation process turned out to be very effort-intensive: for both languages, the experts spent at least 20 minutes in reading and composing a summary for one single news story. The accomplished corpus of 100 news stories and expert summaries in each language, namely H ISTS UMM (see examples in Tab. 1), were further examined by six other experts for quality control (see details in § 6.2). 1800 Figure 1: Publication time of H IST S U"
2021.eacl-main.273,E17-2068,0,0.252668,"we know the final outcome (i.e., the authority’s seal had been saved from fire). 4 Methodology Based on the popular cross-lingual transfer learning framework of (Ruder et al., 2019), we propose 3126 a simple historical text summarisation framework (see Fig. 3), which can be trained even without supervision (i.e., parallel historical-modern signals). Step 1. For both DE and ZH, we begin with respectively training modern and historical monolingual word embeddings. Specially, for DE, following the suggestions of Wang et al. (2019), we selected subword-based embedding algorithms (e.g., FastText (Joulin et al., 2017)) as they yield competitive results. In addition to training word embeddings on the raw text, for historical DE we also consider performing text normalisation (NORM) to enhance model performance. This orthographic technique aims to convert words from their historical spellings to modern ones, and has been widely adopted as a standard step by NLP applications for historical alphabetic languages (Bollmann, 2019). Although training a normalisation model in a fully unsupervised setup is not yet realistic, it can get bootstrapped with a single lexicon table to yield satisfactory performance (Ljubeˇ"
2021.eacl-main.273,D19-1117,0,0.0145182,"fer corresponding information, e.g., only by adding ‘居正’ (Juzheng, a minister’s name) to the context can we interpret the sentence ‘已， 又为私书安之云’ (№20) as ‘after that, (Juzheng) wrote a private letter to comfort him’. This adds extra difficulty to the generation of summaries. Writing style. To inform readers, a popular practice adopted by modern news writers is to introduce key points in the first one or two sentences (White, 1998). Many machine summarisation algorithms leverage this pattern to enhance summarisation quality by incorporating positional signals (Edmundson, 1969; See et al., 2017; Gui et al., 2019). However, this rhetorical technique was not widely used in H IST S UMM, where crucial information may appear in the middle or even the end of stories. For instance, the keyword ‘T¨urck’ (Turkish) (№33) first occurs in the second half of the story; in article №7 of ZH H IST S UMM (see Tab. 1), only after reading the last sentence can we know the final outcome (i.e., the authority’s seal had been saved from fire). 4 Methodology Based on the popular cross-lingual transfer learning framework of (Ruder et al., 2019), we propose 3126 a simple historical text summarisation framework (see Fig. 3), wh"
2021.eacl-main.273,D19-1302,0,0.0167318,"e and summarise separately (Leuski et al., 2003; Or˘asan and Chiorean, 2008). However, such paradigms suffer from the error propagation problem, i.e., errors produced by upstream modules may accumulate and degrade the output quality (Zhu et al., 2020). In addition, parallel data to train effective translators is not always accessible (Cao et al., 2020). Recently, end-to-end methods have been applied to alleviate this issue. The main challenge for this 3124 research direction is the lack of direct corpora, leading to attempts such as zero-shot learning (Duan et al., 2019), multi-task learning (Zhu et al., 2019), and transfer learning (Cao et al., 2020). Although training requirements have been relaxed by these methods, our extreme setup with summarisation data only available for the target language and very limited parallel data, has never been visited before. zh 1600 Year 1700 1750 19 9 Literature 2 3 H IST S UMM Corpus 10 13 3.1 Dataset Construction Politics 41 1 ZH Detailed references are included in the ‘source’ entries of H IST S UMM’s metadata. Sovereign Military 31 In history and digital humanities research, summarisation is most needed when analysing documentary and narrative text such as ne"
2021.eacl-main.273,2020.acl-main.121,0,0.143341,"historical resources is a key reason for the former, and for the latter, resources do not exist in any language. We hope to spur research on historical text summarisation and in particular for non-EN languages through this work. Cross-lingual summarisation. The traditional strands of cross-lingual text summarisation systems design pipelines which learn to translate and summarise separately (Leuski et al., 2003; Or˘asan and Chiorean, 2008). However, such paradigms suffer from the error propagation problem, i.e., errors produced by upstream modules may accumulate and degrade the output quality (Zhu et al., 2020). In addition, parallel data to train effective translators is not always accessible (Cao et al., 2020). Recently, end-to-end methods have been applied to alleviate this issue. The main challenge for this 3124 research direction is the lack of direct corpora, leading to attempts such as zero-shot learning (Duan et al., 2019), multi-task learning (Zhu et al., 2019), and transfer learning (Cao et al., 2020). Although training requirements have been relaxed by these methods, our extreme setup with summarisation data only available for the target language and very limited parallel data, has never"
C04-1129,E99-1042,0,0.0328042,"our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge. 1 Introduction Syntactic simplification is an NLP task, the goal of which is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content. Text simplification is a useful task for varied reasons. Chandrasekar et al. (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. The PSET project (Carroll et al., 1999), on the other hand, focused its research on simplifying newspaper text for aphasics, who have trouble with long sentences and complicated grammatical constructs. We have previously (Siddharthan, 2002; Siddharthan, 2003) developed a shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction. In this paper, we explore the use of syntactic simplification in multi-document summarization. 1.1 Sentence Shortening for Summarization It is interesting to survey the literature in sentence shortening, a task related to syntactic simp"
C04-1129,C96-2183,0,0.412753,"summary is a reference-generation task rather than a content-selection one, and implement a baseline reference rewriting module. We perform our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge. 1 Introduction Syntactic simplification is an NLP task, the goal of which is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content. Text simplification is a useful task for varied reasons. Chandrasekar et al. (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. The PSET project (Carroll et al., 1999), on the other hand, focused its research on simplifying newspaper text for aphasics, who have trouble with long sentences and complicated grammatical constructs. We have previously (Siddharthan, 2002; Siddharthan, 2003) developed a shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction. In this paper, we explore the use of syntactic simplification in multi-document summarization. 1.1 Se"
C04-1129,grover-etal-2000-lt,0,0.0107854,"o improved content selection in summaries. We therefore also need to evaluate our summarizer. We do this in $ 3, but first we describe the summarizer in more detail. 2.2 Description of our Summarizer Our summarizer has four stages—preprocessing of original documents to remove parentheticals, clustering of the simplified sentences, selecting of one representative sentence from each cluster and deciding which of these selected sentences to incorporate in the summary. We use our syntactic simplification software (Siddharthan, 2002; Siddharthan, 2003) to remove parentheticals. It uses the LT TTT (Grover et al., 2000) for POS-tagging and simple noun-chunking. It then performs apposition and relative clause identification and attachment using shallow techniques based on local context and animacy information obtained from WordNet (Miller et al., 1993). We then cluster the simplified sentences with SimFinder (Hatzivassiloglou et al., 1999). To further tighten the clusters and ensure that their size is representative of their importance, we post-process them as follows. SimFinder implements an incremental approach to clustering. At each incremental step, the similarity of a new sentence to an existing cluster"
C04-1129,W99-0625,0,0.0695217,"at the clustering is not always accurate. Clusters can contain spurious sentences, and a cluster’s size might then exaggerate its importance. Improving the quality of the clustering can thus be expected to improve the content of the summary. We now describe our experiments on syntactic simplification and sentence clustering. Our hypothesis is that simplifying parenthetical units (relative clauses and appositives) will improve the performance of our clustering algorithm, by preventing it from clustering on the basis of background information. 2.1 Simplification and Clustering We use SimFinder (Hatzivassiloglou et al., 1999) for sentence clustering and its similarity metric to evaluate cluster quality; SimFinder outputs similarity values (simvals) between 0 and 1 for pairs of sentences, based on word overlap, synonymy and n-gram matches. We use the average of the simvals for each pair of sentences in a cluster to evaluate a quality-score for the cluster. Table 1 below shows the quality-scores averaged over all clusters when the original document set is and is not preprocessed using our syntactic simplification software (described in $ 2.2). We use 30 document sets from the 2003 Document Understanding Conference ("
C04-1129,A00-1043,0,0.0157875,", the sentence: Former Democratic National Committee finance director Richard Sullivan faced more pointed questioning from Republicans during his second day on the witness stand in the Senate’s fund-raising investigation. got shortened (with different levels of reduction) to: # Richard Sullivan Republicans Senate. # Richard Sullivan faced pointed questioning. # Richard Sullivan faced pointed questioning from Republicans during day on stand in Senate fundraising investigation. Grefenstette (1998) provided a rule based approach to telegraphic reduction of the kind illustrated above. Since then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that"
C04-1129,N03-1020,0,0.045946,"evaluation methods and also started providing participants with multiple human-written models needed for reliable evaluation. Participating generic multidocument summarizers were tested on 30 eventbased sets in 2003 and 50 sets in 2004, all 80 containing roughly 10 newswire articles each. There were four human-written summaries for each set, created for evaluation purposes. In DUC’03, the task was to generate 100 word summaries, while in DUC’04, the limit was changed to 665 bytes. 3.2 Evaluation Metric We evaluated our summarizer on the DUC test sets using the Rouge automatic scoring metric (Lin and Hovy, 2003). The experiments in Lin and Hovy (2003) show that among n-gram approaches to scoring, Rouge-1 (based on unigrams) has the highest correlation with human scores. In 2004, an additional automatic metric based on longest common subsequence was included (Rouge-L), that aims to overcome some deficiencies of Rouge-1, such as its susceptibility to ungrammatical keyword packing by dishonest summarizers2 . For our evaluations, we use the Rouge settings from DUC’04: stop words are included, words are Porter-stemmed, and all four human model summaries are used. 3.3 DUC’04 Evaluation We entered our syste"
C04-1129,W03-1101,0,0.0123843,"ince then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that statistical sentence-shortening approaches like Knight and Marcu (2000) do not improve content selection in summaries. Indeed he reported that syntax-based sentence-shortening resulted in significantly worse content selection by their extractive summarizer NeATS. Lin (2003) concluded that pure syntaxbased compression does not improve overall summarizer performance, even though the compression algorithm performs well at the sentence level. 1.2 Simplifying Syntax for Summarization A problem with using statistical sentenceshortening for summarization is that syntactic form does not a"
C04-1129,N03-1026,0,0.014123,"e: Former Democratic National Committee finance director Richard Sullivan faced more pointed questioning from Republicans during his second day on the witness stand in the Senate’s fund-raising investigation. got shortened (with different levels of reduction) to: # Richard Sullivan Republicans Senate. # Richard Sullivan faced pointed questioning. # Richard Sullivan faced pointed questioning from Republicans during day on stand in Senate fundraising investigation. Grefenstette (1998) provided a rule based approach to telegraphic reduction of the kind illustrated above. Since then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that statistical sentence-s"
C04-1129,N03-2024,1,\N,Missing
C04-1129,A97-1030,0,\N,Missing
C04-1129,E99-1029,0,\N,Missing
C12-1020,W10-1301,0,0.025265,"ngineers or nurses (Goldberg et al., 1994; Theune et al., 2001; Portet et al., 2009). These are capable of generating high quality texts; e.g., offshore oil rig workers preferred weather forecasts generated by the SumTime system to texts written by professional human forecasters (Sripada et al., 2003). There is some previous work on the use of data-to-text for lay audiences; e.g., generating narratives from sensor data for automotive (Reddington et al., 2011) and environmental (Molina et al., 2011) applications, generating personal narratives to help children with complex communication needs (Black et al., 2010), and summarising neonatal intensive care data for parents of premature babies (Mahamood et al., 2008). There are some notable examples of NLG systems that make use of structured textual records rather than numeric data. Peba-II (Milosavljevic, 1997) was an online animal encyclopedia that provided descriptions and comparison of animals using HTML pages. The Power (Daley et al., 1998) and ILEX (O’Donnell et al., 2001) systems in the virtual museum domain dynamically generated descriptions of museum objects based on the user’s discourse history and user model. Dial Your Disc (Van Deemter and Odi"
C12-1020,W09-0613,0,0.44043,"ssed as similar species then the similar species group will not be present at all. The schema loops through the entire message list and decides what groups are needed based on the type of messages present. 3.3 Microplanner The microplanner is the second stage of the generation architecture. Here, the document plan produced by the first stage, the document planner, is refined to produce a text specification. This includes phrase specifications and their aggregation into sentences. The phrase specification is structured in a way that it can be realised by the surface realiser. We use SIMPLENLG (Gatt and Reiter, 2009), and the design of these structures is therefore influenced by the functionalities of the SIMPLENLG library. To allow for the generation of more complex sentence structures and sentences that are easier to understand, the microplanner also carries out aggregation. The aggregation performed in this system focuses on the formation of sentences. For example, in the paragraph that deals with comparisons (see Fig. 2), the system knows that it will be comparing different features. This means that the phrase specifications can be aggregated through subordination, using conjunctions such as “whereas”"
C12-1020,karasimos-isard-2004-multi,0,0.183843,"m of generating engaging texts, attempting to keep its users amused by 314 (a) (b) Figure 4: Bumblebee Model: (a) Visual bumblebee features, (b) List of Thorax types focusing on the expression of unusual content. Our work shares commonalities with these systems, in that it targets non-expert audiences and has educational goals. The main mechanism employed by us in the current work is the use of comparisons for generating feedback. This builds on a body of previous work. Milosavljevic (1997) described comparisons as a useful tool for augmenting the user’s existing knowledge with new knowledge. Karasimos and Isard (2004) showed that texts which contained comparisons and aggregations helped readers retain more information and perform better on factual recall while also finding these texts more interesting and pleasant to read. Later, Marge et al. (2008) performed a similar experiment to isolate the effects of comparison from those of aggregation. These two experiments provide evidence that comparisons can help to improve the knowledge of users on a given domain, and are a basis for our work. 3 Implementation of the NLG module Our NLG system uses the architecture proposed by Reiter and Dale (2000) and is compat"
C12-1020,W08-1123,0,0.0612935,"with these systems, in that it targets non-expert audiences and has educational goals. The main mechanism employed by us in the current work is the use of comparisons for generating feedback. This builds on a body of previous work. Milosavljevic (1997) described comparisons as a useful tool for augmenting the user’s existing knowledge with new knowledge. Karasimos and Isard (2004) showed that texts which contained comparisons and aggregations helped readers retain more information and perform better on factual recall while also finding these texts more interesting and pleasant to read. Later, Marge et al. (2008) performed a similar experiment to isolate the effects of comparison from those of aggregation. These two experiments provide evidence that comparisons can help to improve the knowledge of users on a given domain, and are a basis for our work. 3 Implementation of the NLG module Our NLG system uses the architecture proposed by Reiter and Dale (2000) and is compatible with a wide range of work within the field. There are three main components in this architecture; a document planner, a micro planner and a surface realiser. Additionally, the document planning makes use of a domain model. We descr"
C14-1188,W11-1601,0,0.47312,"text simplification systems have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilitate poor readers? Our goals in this paper are twofold. First, we want to evaluate text simplification systems more sy"
C14-1188,P05-1067,0,0.0149348,"ucts (Siddharthan and Mandya, 2014; Mandya and Siddharthan, 2014). In this paper we combine this work (summarised in §3) with a new method for sentence compression (described in §4). 3 Text Simplification with Synchronous Dependency Grammars We use the RegenT text simplification (Siddharthan, 2011), augmented with automatically acquired rules, as described in detail elsewhere (Mandya and Siddharthan, 2014; Siddharthan and Mandya, 2014). In this section, we will restrict ourselves to summarising the key features of the system. Our text simplification system follows the architecture proposed in Ding and Palmer (2005) for Synchronous Dependency Insertion Grammars, reproduced in Fig. 1. It uses the same dataset1 as Woodsend and Lapata (2011) for learning lexicalised rules. The rules are acquired in the format required by the RegenT text simplification system (Siddharthan, 2011), which is used to implement the simplification. This 1 consisting of ∼140K aligned simplified and original sentence pairs obtained from Simple English Wikipedia and English Wikipedia. Input Sentence −→ Dependency Parse −→ Source ETs Target ETs −→ Generation −→ Output Sentences ↓ ↑ ET Transfer Figure 1: System Architecture 1997 RULE 1"
C14-1188,N07-1023,0,0.0350413,"simplifications to enumerate manually. Some contemporary work in text simplification has evolved from research in sentence compression, a related research area that aims to shorten sentences for the purpose of summarising the main content. Sentence compression has historically been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions, using ideas adapted from statistical machine translation. The compression rules learnt are typically syntactic tree-to-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003; Cohn and Lapata, 2009; Nomoto, 2008) of some variety. Indeed, Woodsend and Lapata (2011) develop this line of research. Their model is based on quasi-synchronous tree substitution grammar (QTSG) (Smith and Eisner, 2006) and integer linear programming. Quasi-synchronous grammars aim to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. Woodsend and Lapata (2011) use QTSG to generate all possible rewrite operations for a source tree, and then integer linear programming to select the most appropriate"
C14-1188,W14-4404,1,0.617462,", 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilitate poor readers? Our goals in this paper are twofold. First, we want to evaluate text simplification systems more systematically than has been attempted before, using both human judgements on a larger scale, and directly testing comprehension on longer passages for target reader populations. Second, we want to compare two different approaches to text simplification. In this paper, we present a text simplification system that can perform lexical and synta"
C14-1188,P14-1041,0,0.652381,"with reduced literacy, motivated by a large body of evidence that manual text simplification is an effective intervention (Anderson and Freebody, 1981; L’Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008). However automatic text simplification systems have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya"
C14-1188,P08-1035,1,0.930512,"simplification has evolved from research in sentence compression, a related research area that aims to shorten sentences for the purpose of summarising the main content. Sentence compression has historically been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions, using ideas adapted from statistical machine translation. The compression rules learnt are typically syntactic tree-to-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003; Cohn and Lapata, 2009; Nomoto, 2008) of some variety. Indeed, Woodsend and Lapata (2011) develop this line of research. Their model is based on quasi-synchronous tree substitution grammar (QTSG) (Smith and Eisner, 2006) and integer linear programming. Quasi-synchronous grammars aim to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. Woodsend and Lapata (2011) use QTSG to generate all possible rewrite operations for a source tree, and then integer linear programming to select the most appropriate simplification. Their system performs lexical and syntactic"
C14-1188,W13-4813,0,0.155083,"heds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilitate poor readers? Our goals in this paper are twofold. First, we want to evaluate text simplification systems more systematically than has been attempted before, using both human judgements on"
C14-1188,N03-1026,0,0.23706,"ate manually. Some contemporary work in text simplification has evolved from research in sentence compression, a related research area that aims to shorten sentences for the purpose of summarising the main content. Sentence compression has historically been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions, using ideas adapted from statistical machine translation. The compression rules learnt are typically syntactic tree-to-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003; Cohn and Lapata, 2009; Nomoto, 2008) of some variety. Indeed, Woodsend and Lapata (2011) develop this line of research. Their model is based on quasi-synchronous tree substitution grammar (QTSG) (Smith and Eisner, 2006) and integer linear programming. Quasi-synchronous grammars aim to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. Woodsend and Lapata (2011) use QTSG to generate all possible rewrite operations for a source tree, and then integer linear programming to select the most appropriate simplification. Their"
C14-1188,E14-1076,1,0.918381,"to the large numbers of people with reduced literacy, motivated by a large body of evidence that manual text simplification is an effective intervention (Anderson and Freebody, 1981; L’Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008). However automatic text simplification systems have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Naraya"
C14-1188,W03-2314,1,0.927759,"k et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008). However automatic text simplification systems have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilit"
C14-1188,W03-2602,1,0.840496,"k et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008). However automatic text simplification systems have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilit"
C14-1188,W10-4213,1,0.858287,"ems do not offer a treatment of morphology (Zhu et al., 2010; Woodsend and Lapata, 2011; Paetzold and Specia, 2013). This means that while some syntactic reordering operations can be performed well, others requiring morphological changes cannot. Consider converting passive to active voice (e.g., from “trains are liked by John” to “John likes trains”). Besides deleting auxiliaries and reordering the arguments of the verb, there is also a requirement to modify the verb to make it agree in number with the new subject “John”, and take the tense of the auxiliary “are”. Hand crafted systems such as Siddharthan (2010) and Siddharthan (2011) use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting, and can handle voice change correctly. However, hand crafted systems are limited in scope to syntactic simplification as there are too many lexico-syntactic and lexical simplifications to enumerate manually. Some contemporary work in text simplification has evolved from research in sentence compression, a related research area that aims to shorten sentences for the purpose of summarising the main content. Sentence compression has historica"
C14-1188,W11-2802,1,0.960125,"tment of morphology (Zhu et al., 2010; Woodsend and Lapata, 2011; Paetzold and Specia, 2013). This means that while some syntactic reordering operations can be performed well, others requiring morphological changes cannot. Consider converting passive to active voice (e.g., from “trains are liked by John” to “John likes trains”). Besides deleting auxiliaries and reordering the arguments of the verb, there is also a requirement to modify the verb to make it agree in number with the new subject “John”, and take the tense of the auxiliary “are”. Hand crafted systems such as Siddharthan (2010) and Siddharthan (2011) use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting, and can handle voice change correctly. However, hand crafted systems are limited in scope to syntactic simplification as there are too many lexico-syntactic and lexical simplifications to enumerate manually. Some contemporary work in text simplification has evolved from research in sentence compression, a related research area that aims to shorten sentences for the purpose of summarising the main content. Sentence compression has historically been addressed in a"
C14-1188,W06-3104,0,0.072967,"tence compression has historically been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions, using ideas adapted from statistical machine translation. The compression rules learnt are typically syntactic tree-to-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003; Cohn and Lapata, 2009; Nomoto, 2008) of some variety. Indeed, Woodsend and Lapata (2011) develop this line of research. Their model is based on quasi-synchronous tree substitution grammar (QTSG) (Smith and Eisner, 2006) and integer linear programming. Quasi-synchronous grammars aim to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. Woodsend and Lapata (2011) use QTSG to generate all possible rewrite operations for a source tree, and then integer linear programming to select the most appropriate simplification. Their system performs lexical and syntactic simplification as well as compression. Recently, there have been attempts to combine approaches. Narayan and Gardent (2014) use an approach based on semantics to perform syntactic si"
C14-1188,D11-1038,0,0.406712,"ms have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilitate poor readers? Our goals in this paper are twofold. First, we want to evaluate text simplification systems more systematically than has been"
C14-1188,P12-1107,0,0.336116,"Missing"
C14-1188,C10-1152,0,0.645466,"However automatic text simplification systems have rarely been evaluated in a manner that sheds light on whether they can facilitate target users. To date, evaluations of automatic text simplification have been (a) performed on a small scale, as few as 20–25 sentences in some cases (Wubben et al., 2012; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014), (b) performed on sentences in isolation, thus not measuring incoherence caused at the inter-sentential level that can make text more difficult (Siddharthan (2003a) being the exception), and (c) performed using either automatic metrics (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013) or using ratings by fluent readers for fluency, simplicity and meaning preservation (Siddharthan, 2006; Woodsend and Lapata, 2011; Wubben et al., 2012; Paetzold and Specia, 2013; Siddharthan and Mandya, 2014; Narayan and Gardent, 2014; Mandya and Siddharthan, 2014). As such, none of these evaluations can help us answer the basic question: How good is automatic text simplification; i.e., would it facilitate poor readers? Our goals in this paper are twofold. First, we want to evaluate text simp"
D09-1155,J08-4004,0,0.0294529,"Missing"
D09-1155,J02-4002,1,0.838057,"an expert, a semi-expert, and a non-expert) is acceptable overall vindicate our task definition as domain-knowledge free (using the tools of justification and domain-specific generic knowledge). However, the agreements involving the semi-expert are higher than the agreement between expert and non-expert. This probably means that the chemistry generics were not fully adequate to ensure that the non-expert understood enough of the chemistry to achieve the highest-possible agreement. 1500 The automation of AZ-annotation is underway. This requires adaptation of the high-level features used in AZ (Teufel and Moens, 2002) to chemistry. We are also preparing an annotation experiment with naive annotators. Another research avenue is the expansion of the guidelines to other disciplines such as bio-medicine, and to longer journal articles, e.g., in computational linguistics. OWN FAIL Initial attempts to improve the dehydration of 4 via chemical or thermal means were unsuccessful; similarly, attempts to couple the chlorosilane (Me3Si)2 (Me2ClSi)CH with Ag2O failed. (b510692c) OWN FAIL When the ABL algorithms try to learn with two completely distinct sentences, nothing can be learned. (0104006) OWN R ES While the ac"
D09-1155,E99-1015,1,0.914116,"r so far been restricted to one discipline, computational linguistics (CL). Here, we present a more informative AZ scheme with 15 categories in place of the original 7, and show that it can be applied to the life sciences as well as to CL. We use a domain expert to encode basic knowledge about the subject (such as terminology and domain specific rules for individual categories) as part of the annotation guidelines. Our results show that non-expert human coders can then use these guidelines to reliably annotate this scheme in two domains, chemistry and computational linguistics. 1 Introduction Teufel et al. (1999) define the task of Argumentative Zoning (AZ) as a sentence-by-sentence classification with mutually exclusive categories from the annotation scheme given in Fig. 1. The reasoning behind the categories is inspired by the notion of a knowledge claim (Myers, 1992; Luukkonen, 1992): the act of writing a paper corresponds to an attempt of claiming ownership for a new piece of knowledge, which is to be integrated into the repository of scientific knowledge in the authors’ field by the process of peer review and publication. In the cause of this process, the authors have to convince the reviewers th"
D09-1155,W06-1613,1,0.871442,"@cl.cam.ac.uk batchelorc@rsc.org Abstract divide the paper into zones, OTHER , OWN and BACKGROUND. These are defined on the basis of who owns the knowledge claim in the corresponding segment. There are also two categories which are defined by their relationship to existing work, BASIS and C ONTRAST . That means that parts of the AZ scheme are similar to citation function classification schemes from the area of citation content analysis (Garfield, 1965; Weinstock, 1971; Spiegel-R¨using, 1977), and to automatic citation function classification (Nanba and Okumura, 1999; Garzone and Mercer, 2000; Teufel et al., 2006). The remaining categories, A IM and T EXTUAL , fulfil different rhetorical functions for the presentation of the paper. A IM points out the paper’s main knowledge claim, a rhetorical move which may be repeated in the conclusion and the introduction. T EXTUAL explains the physical location of information, e.g., by giving a section overview or presenting a summary of a subsection. On the basis of human-annotated training material, AZ can be automatically classified using supervised machine learning. Argumentative Zoning (AZ) is an analysis of the argumentative and rhetorical structure of a scie"
D09-1155,mizuta-collier-2004-annotation,0,0.0640423,"must have a way of distinguishing, for instance, between methods and results. Note that several of the applications based on AZ and AZ-II in general rely on the rare categories much more than they rely on the more frequent categories. OWN FAIL is an example of a rare but important category, and so is A IM , which is central to summarisation applications. The comparative and contrastive categories C O D I A NTISUPP and G AP W EAK , on the other hand, are particularly useful to citation-based search applications. Other AZ-like schemes for scientific discourse created for the biomedical domain (Mizuta and Collier, 2004) and for computer science (Feltrim et al., 2005) also made the decision to subdivide OWN , in similar ways to how we propose here. The current work, however, is the first experimental proof that humans can make this distinction – and the others encoded in AZ-II – reliably, and in two quite distinct disciplines. 3 Discipline-Independent Non-Expert Annotation An important principle of AZ is that its categories can be decided without domain knowledge. This rule is anchored in the guidelines: when choosing a category, no reasoning about the scientific facts is allowed. The avoidance of domain-know"
D09-1155,J96-2004,0,\N,Missing
E14-1076,P05-1067,0,0.139848,"es. They define elementary trees (ETs) to be sub-sentential dependency structures containing one or more lexical items. The SDIG formalism assumes that the isomorphism of the two syntactic structures is at the ET level, thus allowing for non-isomorphic tree to tree mapping at the sentence level. We base our approach to text simplification on SDIGs, but the formalism is adapted for the monolingual task, and the rules are written in a formalism that is suited to writing rules by hand as well as automatically acquiring rules from aligned sentences. Our system follows the architecture proposed in Ding and Palmer (2005), reproduced in Fig. 1. In this paper, we will present the ET Transfer component as a set of transformation rules. The rest of Section 3 will focus on the linguistic knowledge we need to encode in these rules, the method for automatic acquisition of rules from a corpus of aligned sentences, and the generation process. tired. John went to sleep.” and “Susan was tired. Susan went to sleep.” Systems such as Siddharthan (2011) use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting, and are well suited to handle the voice"
E14-1076,J08-1001,0,0.00813102,"ed in this paper, with manual and automatically acquired rules. SEW is the human generated simplification from Simple English Wikipedia. All differences in means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, H YBRID and SEW are significantly better than QTSG (p &lt; 0.001; t-test). Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). large part because it is the only version that does not delete information through sentence compression). Table 2 shows some examples of simplifications from the evaluation dataset, along with their average scores for fluency, simplicity and meaning preservation. These examples have been selected to help interpret the results in Table 1. QTSG frequently generates fragments (“Komiyama is a.”, etc.), likely through incorrect splitting rules in the grammar; this is penalise"
E14-1076,P03-1054,0,0.00320307,"further develop this line of research. Their model is based on quasi-synchronous grammar (Smith and Eisner, 2006) and integer linear programming. Quasi2.3 Formalisms and linguistic coverage The systems summarised above differ primarily in the level of linguistic knowledge they encode. PBMT systems use the least knowledge, and as such are ill equipped to to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Syntax based approaches use syntactic knowledge. However, both Zhu et al. (2010) and Woodsend and Lapata (2011) use the Stanford Parser (Klein and Manning, 2003) for syntactic structure, and this representation lacks morphological information. This means that some simplification operations such as voice conversion are not handled well. For example, to simplify “trains are liked by John” to “John likes trains”, besides deleting auxiliaries and reordering the arguments of the verb “like”, the verb also needs to agree in number with the new subject (“John”), and take the tense of the auxiliary verb (“are”). The grammar acquisition process leads to further problems. From an aligned pair “John, who was tired, went to sleep.” and “John was tired. He went to"
E14-1076,bott-etal-2012-text,0,0.0895616,"Missing"
E14-1076,W09-2105,0,0.0189525,"Missing"
E14-1076,C96-2183,0,0.494738,"dimentary level of literacy, and even among those who have studied for 8 years, only a quarter can be considered fully literate. While there is a large 2 Related work There are two largely distinct bodies of work on automatic text simplification – those that use handcrafted rules, and those that apply machine translation approaches. 2.1 Hand-crafted text simplification systems The first body of work uses hand-crafted rules to perform syntactic simplification operations (e.g., splitting coordinated and subordinated clauses, and disembedding apposition and relative clauses). Some early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). F"
E14-1076,W11-1601,0,0.343821,"evaluated using BLEU, and on human evaluations of simplicity, grammaticality and meaning preservation. We will directly compare our approach to Woodsend and Lapata (2011), as this is the best performing contemporary system that has the same linguistic scope as ours. Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only perform a small set of simplification operations, such as lexical substitution, deletion and simple paraphrase. They are not well suited for reordering or splitting operations. Specifically, the syntactic simplification operations that handcrafted systems focus on are out of scope. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input se"
E14-1076,H05-1031,1,0.599747,"ion from Simple English Wikipedia. All differences in means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, H YBRID and SEW are significantly better than QTSG (p &lt; 0.001; t-test). Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). large part because it is the only version that does not delete information through sentence compression). Table 2 shows some examples of simplifications from the evaluation dataset, along with their average scores for fluency, simplicity and meaning preservation. These examples have been selected to help interpret the results in Table 1. QTSG frequently generates fragments (“Komiyama is a.”, etc.), likely through incorrect splitting rules in the grammar; this is penalised heavily by the raters. The H YBRID system uses manually written rules for sentence splitting a"
E14-1076,N10-1144,1,0.85022,"Missing"
E14-1076,J11-4007,1,0.226891,"h Wikipedia. All differences in means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, H YBRID and SEW are significantly better than QTSG (p &lt; 0.001; t-test). Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). large part because it is the only version that does not delete information through sentence compression). Table 2 shows some examples of simplifications from the evaluation dataset, along with their average scores for fluency, simplicity and meaning preservation. These examples have been selected to help interpret the results in Table 1. QTSG frequently generates fragments (“Komiyama is a.”, etc.), likely through incorrect splitting rules in the grammar; this is penalised heavily by the raters. The H YBRID system uses manually written rules for sentence splitting and is more robust in this r"
E14-1076,W03-2314,1,0.924548,"s the system described in this paper, with manual and automatically acquired rules. SEW is the human generated simplification from Simple English Wikipedia. All differences in means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, H YBRID and SEW are significantly better than QTSG (p &lt; 0.001; t-test). Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). large part because it is the only version that does not delete information through sentence compression). Table 2 shows some examples of simplifications from the evaluation dataset, along with their average scores for fluency, simplicity and meaning preservation. These examples have been selected to help interpret the results in Table 1. QTSG frequently generates fragments (“Komiyama is a.”, etc.), likely through incorrect splitting rules in"
E14-1076,W03-2602,1,0.948865,"s the system described in this paper, with manual and automatically acquired rules. SEW is the human generated simplification from Simple English Wikipedia. All differences in means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, H YBRID and SEW are significantly better than QTSG (p &lt; 0.001; t-test). Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). large part because it is the only version that does not delete information through sentence compression). Table 2 shows some examples of simplifications from the evaluation dataset, along with their average scores for fluency, simplicity and meaning preservation. These examples have been selected to help interpret the results in Table 1. QTSG frequently generates fragments (“Komiyama is a.”, etc.), likely through incorrect splitting rules in"
E14-1076,W10-4213,1,0.841865,"early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 synchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple Wikiped"
E14-1076,W11-2802,1,0.888111,"drasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 synchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple Wikipedia. The latter perfo"
E14-1076,W06-3104,0,0.0275533,"rning phrase to be inserted to complete the sentence. This allows the translation model to handle constructs such as relative clauses and apposition. Dras (1999) was the first to apply synchronous grammars to monolingual tasks. His approach is to map between two TAG grammars using a Generalised Synchronous TAG formalism, and to use Integer Programming to generate a text that satisfies the externally imposed constraints (such as length or readability) using minimal paraphrasing. Woodsend and Lapata (2011) further develop this line of research. Their model is based on quasi-synchronous grammar (Smith and Eisner, 2006) and integer linear programming. Quasi2.3 Formalisms and linguistic coverage The systems summarised above differ primarily in the level of linguistic knowledge they encode. PBMT systems use the least knowledge, and as such are ill equipped to to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Syntax based approaches use syntactic knowledge. However, both Zhu et al. (2010) and Woodsend and Lapata (2011) use the Stanford Parser (Klein and Manning, 2003) for syntactic structure, and this representation lacks morphological information. This me"
E14-1076,D11-1038,0,0.34406,"onal Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 synchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple Wikipedia. The latter performs best in their study, and also achieves better scores than the Zhu et al. (2010) system, both when evaluated using BLEU, and on human evaluations of simplicity, grammaticality and meaning preservation. We will directly compare our approach to Woodsend and Lapata (2011), as this is the best performing contemporary system that has the same linguistic scope as"
E14-1076,P12-1107,0,0.496012,"Missing"
E14-1076,P01-1067,0,0.199841,"the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only perform a small set of simplification operations, such as lexical substitution, deletion and simple paraphrase. They are not well suited for reordering or splitting operations. Specifically, the syntactic simplification operations that handcrafted systems focus on are out of scope. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Splitting is encoded as two probabilities: A segmentation table stores probabilities of sentence splitting at particular words (e.g., which). A completion table stores probabilities of the splitting word to be deleted from the translation, and for the governing phrase to be inserted to complete the sentence. This allows the translation model to handle constructs such as relative clauses and apposition. Dras (1999) was"
E14-1076,C10-1152,0,0.653721,"systems are typically the only available alternative. 2.2 synchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple Wikipedia. The latter performs best in their study, and also achieves better scores than the Zhu et al. (2010) system, both when evaluated using BLEU, and on human evaluations of simplicity, grammaticality and meaning preservation. We will directly compare our approach to Woodsend and Lapata (2011), as this is the best performing contemporary system that has the same linguistic scope as ours. Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben"
H05-1005,W99-0625,0,\N,Missing
H05-1005,J90-1003,0,\N,Missing
H05-1005,P02-1040,0,\N,Missing
H05-1005,N04-1019,0,\N,Missing
H05-1005,W97-0704,0,\N,Missing
H05-1005,X98-1026,0,\N,Missing
H05-1005,grover-etal-2000-lt,0,\N,Missing
H05-1005,N03-1020,0,\N,Missing
H05-1031,J98-3005,1,\N,Missing
H05-1031,J98-2001,0,\N,Missing
H05-1031,N03-2024,1,\N,Missing
H05-1031,C04-1129,1,\N,Missing
H05-1031,J95-2003,0,\N,Missing
H05-1031,grover-etal-2000-lt,0,\N,Missing
H05-1031,J96-2004,0,\N,Missing
H05-1031,J86-3001,0,\N,Missing
J11-4007,W08-1107,0,0.0255227,"Missing"
J11-4007,J05-3002,1,0.864994,"eport an evaluation of the effect of reference rewriting on summary quality in Section 6, including a discussion of its scope and limitations in Section 6.2. 812 Siddharthan, Nenkova, and McKeown Information Status and References to People 2. Related Work Related research into summarization, information status distinctions, and generating referring expressions is reviewed here. 2.1 Extractive and Abstractive Summarization Multi-document summarization has been an active area of research over the past two decades and yet, barring a few exceptions (Radev and McKeown 1998; Daum´e III et al. 2002; Barzilay and McKeown 2005), most systems still use shallow features to produce an extractive summary, an age-old technique (Luhn 1958) that has well-known problems. Extractive summaries may contain phrases that the reader cannot understand out of context (Paice 1990) and irrelevant phrases that happen to occur in a relevant sentence (Knight and Marcu 2000; Barzilay 2003). Referring expressions in extractive summaries illustrate this, as sentences compiled from different documents might contain too little, too much, or repeated information about the referent. In a study of how summary revisions could be used to improve"
J11-4007,W09-2817,0,0.0534453,"Missing"
J11-4007,W07-2302,0,0.0297396,"stance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff 2005) have not been used for generation or summarization tasks. Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, and Viethen 2009). The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both ﬁrst and subsequent mentions of the main subject of the article, consists of 2,000 texts in ﬁve different domains (cities, countries, rivers, people, and mountains). The more recent GREC-People corpus consists of 1,000 texts in just one domain (people) but references to all people mentioned in a text have been annotated. The GREC challenges require systems to pick the most appropriate reference in context from a list of all references in"
J11-4007,J96-2004,0,0.191249,"Missing"
J11-4007,A00-2018,0,0.0190877,"containing 651,000 words and coming from 876 news reports from six different news agencies. The variety of sources is important because working with text from a single source could lead to the learning of paper-speciﬁc editorial rules. The reference characteristics we were interested in were number of pre-modiﬁers, presence and type of post-modiﬁers, and the form of name used to refer to people. The corpus was automatically annotated for person name occurrence and co-reference using Nominator (Wacholder, Ravin, and Choi 1997). Syntactic form of references was obtained using Charniak’s parser (Charniak 2000). This automatically annotated corpus contains references to 6,240 distinct people. The distribution of forms for discourse-new and discourse-old references are shown in Table 1. For discourse-old references, computing the probability of a syntactic realization is not as straightforward as for discourse-new references, because the form of the reference is inﬂuenced by the form of previous references, among other factors. To capture this relationship, we used the data from discourse-old mentions to form a Markov chain, which captures exactly the probability of transitioning from one form of ref"
J11-4007,W09-0609,0,0.0146935,"uations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evaluation exercises such as the TUNA challenge (Gatt, Belz, and Kow 2008) therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent. In a similar vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, an"
J11-4007,P02-1057,0,0.0118805,"rse-old is the only information status distinction 815 Computational Linguistics Volume 37, Number 4 that participating systems model, with other features derived from lexical and syntactic context; for instance, Greenbacker and McCoy (2009) consider subjecthood, parallelism, recency, and ambiguity. 2.2.4 Applications of Information Status Distinctions to GRE. The main application of theories of information status has been in anaphora resolution. Information status distinctions are not normally used in work on generating referring expressions, with a few notable exceptions. Krahmer and Theune (2002) show that the relative salience of discourse entities can be taken into account to produce less-informative descriptions (including fewer attributes than those necessary to uniquely identify the referent using a discourse model that does not incorporate salience). In contrast, Jordan and Walker (2005) show that, in task-oriented dialogs, over-speciﬁed references (including more attributes than needed to uniquely identify the intended referent) are more likely for certain dialog states and communicative goals. Some participating teams in the GREC challenges use the discourse-new vs. discourse-"
J11-4007,W10-4203,0,0.0181468,"tractors are added to the referring expression until its interpretation contains only the intended referent. Subsequent work on referring expression generation has (a) expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter 2002; Gatt and Van Deemter 2007), (b) explored different search algorithms for ﬁnding a minimal description (e.g., Horacek 2003), and (c) offered different representation frameworks such as graph theory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) as alternatives for representing referring characteristics. This body of research assumes a limited domain where the semantics of attributes and their allowed values can be formalized, though semantic representations and inference mechanisms are getting increasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz 2008; Ren, van Deemter, and Pan 2010). In contrast, Siddharthan and Copestake (2004) consider open-domain generation of referring expressions in a regeneration task (text simpliﬁcation); they take a different approach, approximating the hand-coded do"
J11-4007,D08-1019,0,0.0261054,"linguistic rules (e.g., Zajic et al. 2007) often combined with corpus-based information (Jing and McKeown 2000), whereas other approaches use statistical compression applied to news (Knight and Marcu 2000; Daum´e III and Marcu 2002) and to spoken dialogue (Galley and McKeown 2007). Other researchers addressed the problem of generating new sentences to include in a summary. Information fusion, which uses bottom–up multi-sequence alignment of the parse trees of similar sentences, generates new summary sentences from phrases extracted from different document sentences (Barzilay and McKeown 2005; Filippova and Strube 2008). 2.1.2 Summary Revision. Research in single-document summarization on improving summaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work. Three types of ad hoc revision rules are deﬁned—elimination (removing parentheticals, sentence initial prepositional phrases, and adverbial phrases), aggregation (combining constituents from two sentences), and smoothing. The smoothing operators cover some reference editing operations. They include substitution of a proper name with a name alias if the name is mentioned earlier, expansion of a pronoun with co-referential proper name"
J11-4007,N07-1023,1,0.491252,"h that unclear references in summaries pose serious problems for users (Paice 1990). 2.1.1 Sentence Compression and Fusion. Research in abstractive summarization has largely focused on the problem of compression, developing techniques to edit sentences by removing information that is not salient from extracted sentences. Some approaches use linguistic rules (e.g., Zajic et al. 2007) often combined with corpus-based information (Jing and McKeown 2000), whereas other approaches use statistical compression applied to news (Knight and Marcu 2000; Daum´e III and Marcu 2002) and to spoken dialogue (Galley and McKeown 2007). Other researchers addressed the problem of generating new sentences to include in a summary. Information fusion, which uses bottom–up multi-sequence alignment of the parse trees of similar sentences, generates new summary sentences from phrases extracted from different document sentences (Barzilay and McKeown 2005; Filippova and Strube 2008). 2.1.2 Summary Revision. Research in single-document summarization on improving summaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work. Three types of ad hoc revision rules are deﬁned—elimination (removing parentheticals, sente"
J11-4007,W08-1131,0,0.160759,"Missing"
J11-4007,J95-2003,0,0.277775,"Missing"
J11-4007,J86-3001,0,0.385105,"48, with W EKA parameters: “J48 -U -M 4”). We now discuss what features we used for our two classiﬁcation tasks (see the list of features in Table 2). Our hypothesis is that features capturing the frequency and syntactic and lexical forms of references are sufﬁcient to infer the desired distinctions. The frequency features are likely to give a good indication of the global salience of a person in the document set. Pronominalization indicates that an entity was particularly salient at a speciﬁc point of the discourse, as has been widely discussed in attentional status and centering literature (Grosz and Sidner 1986; Gordon, Grosz, and Gilliom 1993). Modiﬁed noun phrases (with apposition, relative clauses, or pre-modiﬁcation) can also signal different information status; for instance, we expect post-modiﬁcation to be more prevalent for characters who are less familiar. For our lexical features, we used two months worth of news articles collected over the Web (and independent of the DUC collection) to collect unigram and bigram lexical models of discourse-new references of people. The names themselves were removed from the discourse-new reference noun phrases and the counts were collected over the pre-mod"
J11-4007,C00-1045,0,0.728214,"Missing"
J11-4007,A00-2024,1,0.290386,"Missing"
J11-4007,P03-1054,0,0.0123544,"cision Section Prediction Accuracy Discourse-new references Include Name Section 5.1 Include Role & temporal mods Include Afﬁliation Include Post-Modiﬁcation Section 5.3.1 Section 5.3.2 Section 5.2 .74 (rising to .92 when there is unanimity among human summarizers) .79 .75 to .79 (depending on rule) .72 (rising to 1.00 when there is unanimity among human summarizers) Discourse-old references Include Only Surname Section 3 .70 833 Computational Linguistics Volume 37, Number 4 Implementation of Algorithm 4. Our reference rewrite module operates on parse trees obtained using the Stanford Parser (Klein and Manning 2003). For each person automatically identiﬁed using the techniques described in Section 4.1.1, we matched every mention of their surname in the parse trees of MEAD summary sentences. We then replaced the enclosing NP (includes all pre- and post-modifying constituents) with a new NP generated using Algorithm 4. The regenerated summary was produced automatically, without any manual correction of parses, semantic analyses, or information status classiﬁcations. We now enumerate implementation details not covered in Section 4.1.1: 1. Although our algorithm determines when to include role and afﬁliation"
J11-4007,J03-1003,0,0.0161679,"Missing"
J11-4007,P99-1072,0,0.248696,"Missing"
J11-4007,W99-0108,0,0.237814,"identify the referent using a discourse model that does not incorporate salience). In contrast, Jordan and Walker (2005) show that, in task-oriented dialogs, over-speciﬁed references (including more attributes than needed to uniquely identify the intended referent) are more likely for certain dialog states and communicative goals. Some participating teams in the GREC challenges use the discourse-new vs. discourse-old distinction as a feature to help select the most likely reference in context. Different interpretations of Centering Theory have also been used to generate pronominal references (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000). Our research is substantially different in that we model a much richer set of information status distinctions. Also, our choice of the news genre makes our studies complementary to the GREC challenges, which use Wikipedia articles about people or other named entities. News stories tend to be about events, not people, and the choice of initial references to participants is particularly important to help the reader understand the news. Our research is thus largely focused on the generation of initial references. Due to their short length, summaries do not gen"
J11-4007,I08-1016,1,0.86528,"f pronoun replacement), is meant to work for all entities, not just mentions to people, and does not incorporate distinctions inferred from the input to the summarizer. Although the rules and the overall approach are based on reasonable intuitions, in practice entity rewrites for summarization do introduce errors, some due to the rewrite rules themselves, others due to problems with co-reference resolution and parsing. 813 Computational Linguistics Volume 37, Number 4 Readers are very sensitive to these errors and prefer extractive summaries to summaries where all references have been edited (Nenkova 2008). Automatic anaphora resolution for all entities mentioned in the input and summary text is also errorful, with about one third of all substitutions in the summary being incorrect (Steinberger et al. 2007). In contrast, when editing references is restricted to references to people alone, as we do in the work presented here, there are fewer edits per summary but the overall result is perceived as better than the original by readers (Nenkova and McKeown 2003). 2.1.3 Reference in Summaries. There has been little investigation of the phenomenon of reference in news summaries. In addition to the re"
J11-4007,N03-2024,1,0.898914,"Missing"
J11-4007,H05-1031,1,0.891249,"Missing"
J11-4007,W06-1612,0,0.0862228,"r vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff 2005) have not been used for generation or summarization tasks. Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, and Viethen 2009). The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both ﬁrst and subsequent mentions of the main subject of t"
J11-4007,W02-0404,0,0.147936,"Missing"
J11-4007,H05-1002,0,0.0614896,"Missing"
J11-4007,A97-1033,1,0.647061,"o errorful, with about one third of all substitutions in the summary being incorrect (Steinberger et al. 2007). In contrast, when editing references is restricted to references to people alone, as we do in the work presented here, there are fewer edits per summary but the overall result is perceived as better than the original by readers (Nenkova and McKeown 2003). 2.1.3 Reference in Summaries. There has been little investigation of the phenomenon of reference in news summaries. In addition to the revision of subsequent references described in Mani, Gates, and Bloedorn (1999), we are aware of Radev and McKeown (1997), who built a prototype system called PROFILE that extracted references to people from news, merging and recording information about people mentioned in various news articles. The idea behind the system was that the rich proﬁles collected for people could be used in summaries of later news in order to generate informative descriptions. However, the collection of information about entities from different contexts and different points in time leads to complications in description generation; for example, past news can refer to Bill Clinton as Clinton, an Arkansas native, the democratic president"
J11-4007,J98-3005,1,0.379455,"m was that the rich proﬁles collected for people could be used in summaries of later news in order to generate informative descriptions. However, the collection of information about entities from different contexts and different points in time leads to complications in description generation; for example, past news can refer to Bill Clinton as Clinton, an Arkansas native, the democratic presidential candidate Bill Clinton, U.S. President Clinton, or former president Clinton and it is not clear which of these descriptions are appropriate to use in a summary of a novel news item. In later work, Radev and McKeown (1998) developed an approach to learn correlations between linguistic indicators and semantic constraints to address such problems, but this line of research has not been pursued further. Next, we review related work on reference outside the ﬁeld of summarization. 2.2 Information Status and Generating Referring Expressions Research on information status distinctions closely relates to work on generating referring expressions. We now overview the two ﬁelds and how they interact. 2.2.1 Information Status Distinctions. Information status distinctions depend on two parameters related to the referent’s p"
J11-4007,W10-4212,0,0.079416,"Missing"
J11-4007,W03-2602,1,0.897915,"Missing"
J11-4007,P04-1052,1,0.822018,"nding a minimal description (e.g., Horacek 2003), and (c) offered different representation frameworks such as graph theory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) as alternatives for representing referring characteristics. This body of research assumes a limited domain where the semantics of attributes and their allowed values can be formalized, though semantic representations and inference mechanisms are getting increasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz 2008; Ren, van Deemter, and Pan 2010). In contrast, Siddharthan and Copestake (2004) consider open-domain generation of referring expressions in a regeneration task (text simpliﬁcation); they take a different approach, approximating the hand-coded domainknowledge of earlier systems with a measure of relatedness for attribute-values that is derived from WordNet synonym and antonym links. 2.2.3 Recent Trends: Data Collection and Evaluations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evalu"
J11-4007,C04-1129,1,0.871401,"Missing"
J11-4007,P98-2204,0,0.0630138,"hearer-old O R the person’s organization (country/ state/ afﬁliation) has been already mentioned A ND is the most salient organization in the discourse at the point where the reference needs to be generated T HEN the afﬁliation of a person can be omitted in the discourse-new reference. Algorithm 2: Omitting the afﬁliation in a discourse-new reference. Based on our intuitions about discourse salience and information status, we initially postulated the decision procedure in Algorithm 2. We described how we make the hearer-new/hearer-old judgment in Section 4.2. We used a salience-list (S-List) (Strube 1998) to determine the salience of organizations. This is a shallow attentional-state model and works as follows: 1. Within a sentence, entities are added to the salience-list from left to right. 2. Within the discourse, sentences are considered from right to left. In other words, entities in more recent sentences are more salient than those in previous ones, and within a sentence, earlier references are more salient than later ones. Results. To make the evaluation meaningful, we only considered examples where there was an afﬁliation mentioned for the person in the input documents, ruling out the t"
J11-4007,J02-1003,0,0.0174239,"Missing"
J11-4007,W03-0508,0,0.0286631,"Missing"
J11-4007,W06-1410,0,0.0243741,"ness for attribute-values that is derived from WordNet synonym and antonym links. 2.2.3 Recent Trends: Data Collection and Evaluations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evaluation exercises such as the TUNA challenge (Gatt, Belz, and Kow 2008) therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent. In a similar vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 200"
J11-4007,A97-1030,0,\N,Missing
J11-4007,W09-0629,0,\N,Missing
J11-4007,grover-etal-2000-lt,0,\N,Missing
J11-4007,C98-2199,0,\N,Missing
J11-4007,radev-etal-2004-mead,0,\N,Missing
J11-4007,E03-1017,0,\N,Missing
liakata-etal-2010-corpora,W08-0606,0,\N,Missing
liakata-etal-2010-corpora,W09-1325,1,\N,Missing
liakata-etal-2010-corpora,D09-1155,1,\N,Missing
liakata-etal-2010-corpora,P07-1125,0,\N,Missing
liakata-etal-2010-corpora,I08-1050,0,\N,Missing
N07-1040,1995.iwpt-1.8,0,0.0250204,"Missing"
N07-1040,grover-etal-2000-lt,0,0.099544,"Missing"
N07-1040,passonneau-2004-computing,0,0.0121771,"the model. In other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference class S in the model. Recall error is then RE(S) = m(S)/c(S) and Recall is . Recall for the enR(S) = 1 − RE = c(S)−m(S) c(S) tire file (or set of files) is calculated by summing over all co-reference classes in the model: R= P i c(Si ) P − m(Si ) i c(Si ) Precision (P ) is calculated by swapping the model and system and the f-measure (F = 2R × P/(R + P )) is symmetric with respect to both annotations. 5.2 Krippendorff ’s Alpha We follow Passonneau (2004) and Poesio and Artstein (2005) in using Krippendorff (1980)’s α metric to compute agreement between annotations. The advantage of α over the more commonly used κ metric is that α allows for partial agreement when annotators assign multiple labels to the same markable; in this case calculating agreement on a markable requires a more graded agreement calculation than the “1 if sets are identical and 0 otherwise” provided for by κ. Krippendorff’s α measures disagreement, and allows for the use of distance metrics to calculate partial disagreement. Following Passonneau, we present results using f"
N07-1040,J02-4002,1,0.738894,"just to some subpart of it. Kim and Webber (2006) solve the problem of distinguishing between these relations for one case. They decide whether the pronoun “they” anaphorically refers to the authors of a cited paper, or whether it refers to some entity that is discussed in (a subpart of) a paper (e.g., “galaxies”). In this paper, we tackle the other problem of scientific attribution. We do not distinguish between the two types of links stated above, but only identify which citation(s) a linguistic expression is attributable 1 We use a list of around 40 research methodology related nouns from Teufel and Moens (2002), such as e.g., “study, account, investigation, result” etc. These are nouns we are particularly interested in. 317 to. For tasks of interest to us, it is not enough to only consider anaphoric references to entire papers; authors often make statements comparing/using/criticising aspects or subparts of cited work. We therefore consider a far wider range of markables than Kim and Webber’s single pronoun “they”. Our attribution task differs from the traditional anaphora resolution task in that we have a fixed list of possible referents (the reference list items, Current-Paper or No-SpecificPaper)"
N07-1040,W06-1613,1,0.757387,"c data about the frequency with which particular papers are cited. The success of citation indexers such as CiteSeer (Giles et al., 1998) and Google Scholar relies on the robust detection of formal citations in arbitrary text. In bibliographic information retrieval, anchor text, i.e., the context of a citation can be used to characterise (index) the cited paper using terms outside of that paper (Bradshaw, 2003); O’Connor (1982) presents an approach for identifying the area around citations where the text focuses on that citation. And automatic citation classification (Nanba and Okumura, 1999; Teufel et al., 2006) determines the function that a citation plays in the discourse. For such information access and retrieval purposes, the relevance of a citation within a paper is often crucial. One can estimate how important a citation is by simply counting how often it occurs in the paper. But as Kim and Webber (2006) argue, this ignores many expressions in text which refer to the cited author’s work but which are not as easy to recognise as citations. They address the resolution of instances of the third person personal pronoun “they” in astronomy papers: it can either refer to a citation or to some entitie"
N07-1040,M95-1005,0,0.0685961,"arkable, rather than a citation and the markable; similarly, in 3(b,c) we count instances of first person pronoun/“this paper”; for 2(e), we now calculate the distance of the closest citation instance. In short, the same features are used, but current work and citations are swapped. 5 Evaluation Metrics We consider two evaluation metrics. The first is the scoring system used for the co-reference task in the Message Understanding Conferences MUC-6 and MUC-7. The second is Krippendorff’s α. We briefly discuss both below. 5.1 The MUC-6/MUC-7 Metric The MUC-6/MUC-7 Co-reference evaluation metric (Vilain et al., 1995) works by comparing co-reference classes across two annotated files. Calling one annotation the “model” and the other the “system”, for each co-reference class S in the model, c(S) is the minimal number of co-reference links needed to generate the class (this is one less than the cardinality of the class; c(S) = |S |− 1). m(S) is the number of “missing” links in the system annotation relative to the co-reference class as marked up in the model. In other words, this is the minimum number of co-reference links that need to be added to the system annotation to fully generate the co-reference clas"
N10-1144,N03-1003,0,0.0445821,"on (replacing difficult words with easier ones), but more recently, there has been work on syntactic simplification and, in particular, the way syntactic rewrites interact with discourse structure and text cohesion (Siddharthan, 2006). Elsewhere, there has been renewed interest in paraphrasing, including the replacement of words (especially verbs) with their dictionary definitions (Kaji et al., 2002) and the replacement of idiomatic or otherwise troublesome expressions with simpler ones. The current research emphasis is on automatically learning paraphrases from comparable or aligned corpora (Barzilay and Lee, 2003; Ibrahim et al., 2003). The text simplification and paraphrasing literature does not address paraphrasing that requires syntactic alterations such as those in example 1 or the question of appropriateness of different formulations of a discourse relation. Some natural language generation systems incorporate results from psycholinguistic studies to make principled choices between alternative formulations. For example, SkillSum (Williams and Reiter, 2008) and ICONOCLAST (Power et al., 2003) are two contemporary generation systems that allow for specifying aspects of style such as choice of disco"
N10-1144,briscoe-carroll-2002-robust,0,0.0364955,"Missing"
N10-1144,J96-2004,0,0.362064,"Missing"
N10-1144,J93-3003,0,0.125122,"rpora that mark up discourse relations – the RST Discourse Treebank based on Rhetorical Structure Theory (Mann and Thompson, 1988), and the Penn Discourse Treebank (Webber et al., 2005). Neither is suitable for studies on the felicity of specific formulations of a discourse relation. As part of this research, we have created a corpus of 144 real text examples, reformulated in 8 ways, giving 1152 sentences in total. There have been numerous corpus studies of discourse connectives, such as studies on the discourserole disambiguation of individual cue-phrases in spoken and written corpora (e.g., Hirschberg and Litman (1993)), the substitutability of discourse connectives (e.g., Hutchinson (2005)), and indeed corpus studies as a means of informing the choice of discourse relations to consider in a theory (e.g., Knott and Dale (1994); Knott (1996)). A distinguishing feature of our approach relative to previous ones is an in-depth study of syntactic variations; in contrast, for example, Knott’s taxonomy of discourse relations is based on the use of a substitution text that precludes variants of the same relation having different syntax. 3 Linguistic acceptability study 3.1 Dataset creation We have constructed a dat"
N10-1144,P05-1019,0,0.0183557,"cal Structure Theory (Mann and Thompson, 1988), and the Penn Discourse Treebank (Webber et al., 2005). Neither is suitable for studies on the felicity of specific formulations of a discourse relation. As part of this research, we have created a corpus of 144 real text examples, reformulated in 8 ways, giving 1152 sentences in total. There have been numerous corpus studies of discourse connectives, such as studies on the discourserole disambiguation of individual cue-phrases in spoken and written corpora (e.g., Hirschberg and Litman (1993)), the substitutability of discourse connectives (e.g., Hutchinson (2005)), and indeed corpus studies as a means of informing the choice of discourse relations to consider in a theory (e.g., Knott and Dale (1994); Knott (1996)). A distinguishing feature of our approach relative to previous ones is an in-depth study of syntactic variations; in contrast, for example, Knott’s taxonomy of discourse relations is based on the use of a substitution text that precludes variants of the same relation having different syntax. 3 Linguistic acceptability study 3.1 Dataset creation We have constructed a dataset that can be used to gain insights into differences between different"
N10-1144,W03-1608,0,0.0598594,"words with easier ones), but more recently, there has been work on syntactic simplification and, in particular, the way syntactic rewrites interact with discourse structure and text cohesion (Siddharthan, 2006). Elsewhere, there has been renewed interest in paraphrasing, including the replacement of words (especially verbs) with their dictionary definitions (Kaji et al., 2002) and the replacement of idiomatic or otherwise troublesome expressions with simpler ones. The current research emphasis is on automatically learning paraphrases from comparable or aligned corpora (Barzilay and Lee, 2003; Ibrahim et al., 2003). The text simplification and paraphrasing literature does not address paraphrasing that requires syntactic alterations such as those in example 1 or the question of appropriateness of different formulations of a discourse relation. Some natural language generation systems incorporate results from psycholinguistic studies to make principled choices between alternative formulations. For example, SkillSum (Williams and Reiter, 2008) and ICONOCLAST (Power et al., 2003) are two contemporary generation systems that allow for specifying aspects of style such as choice of discourse marker, clause ord"
N10-1144,P02-1028,0,0.179813,"in ways, to make it more accessible to particular classes of readers. The PSET project (Carroll et al., 1998) considered simplifying news reports for aphasics. The PSET project focused mainly on lexical simplification (replacing difficult words with easier ones), but more recently, there has been work on syntactic simplification and, in particular, the way syntactic rewrites interact with discourse structure and text cohesion (Siddharthan, 2006). Elsewhere, there has been renewed interest in paraphrasing, including the replacement of words (especially verbs) with their dictionary definitions (Kaji et al., 2002) and the replacement of idiomatic or otherwise troublesome expressions with simpler ones. The current research emphasis is on automatically learning paraphrases from comparable or aligned corpora (Barzilay and Lee, 2003; Ibrahim et al., 2003). The text simplification and paraphrasing literature does not address paraphrasing that requires syntactic alterations such as those in example 1 or the question of appropriateness of different formulations of a discourse relation. Some natural language generation systems incorporate results from psycholinguistic studies to make principled choices between"
N10-2001,P06-4020,1,0.739372,"search. Named Entity Recognition NER in the biomedical domain was implemented as described in Vlachos (2007). Gene Mention tagging was performed using Conditional Random Fields and syntactic parsing, using features derived from grammatical relations to augment the tagging. We also use a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model focuses on biomedical entities and seeks to find the antecedents of anaphora, both coreferent and associative ones, and also to identify discoursenew expressions (Gasperin and Briscoe, 2008). Parsing The RASP toolkit (Briscoe et al., 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text. GRs are triplets consisting of a relation-type and two arguments and also encode morphology, word position and part-of-speech; for example, parsing “John likes Mary.” gives us a subject relation and a direct object relation: (|ncsubj ||like+s:2 VVZ ||John:1 NP1|) (|dobj ||like+s:2 VVZ ||Mary:3 NP1|) Representing a parse as a set of flat triplets allows us to index on grammatical relations, thus enabling complex relational queries. 4.3 Image Processing We build a"
N10-2001,C08-1033,1,0.822796,"put of these systems is then indexed, enabling semantic search. Named Entity Recognition NER in the biomedical domain was implemented as described in Vlachos (2007). Gene Mention tagging was performed using Conditional Random Fields and syntactic parsing, using features derived from grammatical relations to augment the tagging. We also use a probabilistic model for resolution of non-pronominal anaphora in biomedical texts. The model focuses on biomedical entities and seeks to find the antecedents of anaphora, both coreferent and associative ones, and also to identify discoursenew expressions (Gasperin and Briscoe, 2008). Parsing The RASP toolkit (Briscoe et al., 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text. GRs are triplets consisting of a relation-type and two arguments and also encode morphology, word position and part-of-speech; for example, parsing “John likes Mary.” gives us a subject relation and a direct object relation: (|ncsubj ||like+s:2 VVZ ||John:1 NP1|) (|dobj ||like+s:2 VVZ ||Mary:3 NP1|) Representing a parse as a set of flat triplets allows us to index on grammatical relations, thus enabling complex re"
N10-2001,E99-1015,0,0.0516161,"re (such as Ariadne Genomics, Temis or Linguamatics). This option is only available to a tiny minority of researchers working for large wellfunded corporations. 4 Summary of Technology 4.1 PDF to SciXML The PDF format represents a document in a manner designed to facilitate printing. In short, it provides information on font and position for textual and graphical units. To enable information retrieval and extraction, we need to convert this typographic representation into a logical one that reflects the structure of scientific documents. We use an XML schema called SciXML (first introduced in Teufel et al. (1999)) that we extend to include images. We linearise the textual elements in the PDF, representing these as <div&gt; elements in XML and classify these divisions as {Title|Author|Affiliation|Abstract|Footnote|Caption| 2 Heading|Citation |References|Text} in a constraint satisfaction framework. In addition, we identify all graphics in the PDF, including lines and images. We then identify tables by looking for specific patterns of text and lines. A bounding box is identified for a table and an image is generated that overlays the text on the lines. Similarly we overlay text onto images that have been i"
P04-1052,E91-1028,0,0.108148,"tractor, it is quadratic in n. The IA compares each attribute of the discourse referent to only one attribute per distractor and is linear in n. Note, however, that values for n of over 4 are rare. 3.3 Relations Semantically, attributes describe an entity (e.g., the small grey dog) and relations relate an entity to other entities (e.g., the dog in the bin). Relations are troublesome because in relating an entity e o to e1 , we need to recursively generate a referring expression for e1 . The IA does not consider relations and the referring expression is constructed out of attributes alone. The Dale and Haddock (1991) algorithm allows for relational descriptions but involves exponential global search, or a greedy search approximation. To incorporate relational descriptions in the incremental framework would require a classification system which somehow takes into account the relations themselves and the secondary entities e1 etc. This again suggests that the existing algorithms force the incrementality at the wrong stage in the generation process. Our approach computes the order in which attributes are incorporated after observing the context, by quantifying their utility through the quotient DQ. This make"
P04-1052,E03-1017,0,0.0322441,"ge  colour black Assuming that the *preferred-attributes* list is [size, colour, ...], the algorithm would first compare the values of the size attribute (both large), disregard that attribute as not being discriminating, compare the values of the colour attribute and return the brown dog. Subsequent work on referring expression generation has expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter, 2002), explored different search algorithms for finding the minimal description (e.g., Horacek (2003)) and offered different representation frameworks like graph theory (Krahmer et al., 2003) as alternatives to AVMs. However, all these approaches are based on very similar formalisations of the problem, and all make the following assumptions: 1. 2. 3. 4. A semantic representation exists. A classification scheme for attributes exists. The linguistic realisations are unambiguous. Attributes cannot be reference modifying. All these assumptions are violated when we move from generation in a very restricted domain to regeneration in an open domain. In regeneration tasks such as summarisation, open-"
P04-1052,J03-1003,0,0.0672025,"Missing"
P04-1052,C92-1038,0,0.17427,"ther entities in the discourse domain. For example, if there were a small brown dog (small1(x) ∧ brown1(x) ∧ dog1(x)) in context, the minimal description for the big brown dog would be big1(x) ∧ dog1(x)1 . This semantic framework makes it difficult to apply existing referring expression generation algorithms to the many regeneration tasks that are important today; for example, summarisation, openended question answering and text simplification. Unlike in traditional generation, the starting point in 1 The predicate dog1 is selected because it has a distinguished status, referred to as type in Reiter and Dale (1992). One such predicate has to to be present in the description. these tasks is unrestricted text, rather than a semantic representation of a small domain. It is difficult to extract the required semantics from unrestricted text (this task would require sense disambiguation, among other issues) and even harder to construct a classification for the extracted predicates in the manner that existing approaches require (cf., §2). In this paper, we present an algorithm for generating referring expressions in open domains. We discuss the literature and detail the problems in applying existing approaches"
P04-1052,W00-1424,0,0.0326096,"Missing"
P04-1052,J02-1003,0,0.606438,"Missing"
P04-1052,J93-2004,0,\N,Missing
P04-1052,P90-1013,0,\N,Missing
P08-4002,briscoe-carroll-2002-robust,0,0.0259199,"Missing"
P08-4002,N03-2024,0,0.0623531,"Missing"
P08-4002,C04-1129,1,0.843101,"Missing"
P13-4029,C12-1020,1,0.699404,"Missing"
P13-4029,W12-1520,1,0.842727,"Missing"
P13-4029,W09-0613,0,0.0343657,"ging”) as an issue. Our focus will now be on improving the language, to address some of the readability and engagingness concerns. Surface realiser The role of the surface realiser is to convert the text specification received from the microplanner into text that the user can read and understand. This includes linguistic realisation (converting the sentence specifications into sentences) and structural realisation (structuring the sentences inside the document). Both the linguistic and structural realisations are performed by using functionalities provided by the S IMPLE NLG realiser library (Gatt and Reiter, 2009). 4 Utility of blogs in this domain Until recently, our partner charity was publishing hand-written blogs based on the journeys of these satellite tagged red kites. They have had to close down the site due to resource constraints: Such blogs are difficult, monotonous and time consuming to produce by hand. Tag2Blog will allow the charity to restart this form of public engagement. We have earlier studied the use of ecological blogs based on satellite tag data (Siddharthan et al., 2012). Using hand-written blogs in a toy domain, we found that readers were willing to anthromorphise the bird, and g"
rambow-etal-2006-parallel,W04-2709,1,\N,Missing
rambow-etal-2006-parallel,passonneau-etal-2006-inter,1,\N,Missing
rambow-etal-2006-parallel,J93-2004,0,\N,Missing
rambow-etal-2006-parallel,W00-0204,0,\N,Missing
rambow-etal-2006-parallel,W02-1503,0,\N,Missing
rambow-etal-2006-parallel,rambow-etal-2002-dependency,1,\N,Missing
reeder-etal-2004-interlingual,W04-2709,1,\N,Missing
reeder-etal-2004-interlingual,W03-1601,0,\N,Missing
reeder-etal-2004-interlingual,W03-1604,0,\N,Missing
reeder-etal-2004-interlingual,P98-1013,0,\N,Missing
reeder-etal-2004-interlingual,C98-1013,0,\N,Missing
reeder-etal-2004-interlingual,1991.mtsummit-papers.9,1,\N,Missing
reeder-etal-2004-interlingual,J96-2004,0,\N,Missing
reeder-etal-2004-interlingual,A97-1011,0,\N,Missing
rupp-etal-2008-language,W07-1008,1,\N,Missing
rupp-etal-2008-language,W06-1613,1,\N,Missing
rupp-etal-2008-language,P07-2012,1,\N,Missing
rupp-etal-2008-language,P06-4020,0,\N,Missing
rupp-etal-2008-language,J96-2004,0,\N,Missing
rupp-etal-2008-language,N07-1040,1,\N,Missing
rupp-etal-2008-language,W06-2718,1,\N,Missing
W04-2709,P98-1013,0,0.473703,"dependency parser (details in section 6) and is a useful starting point for semantic annotation at IL1, since it allows annotators to see how textual units relate syntactically when making semantic judgments. 4.1.3 IL2 IL2 is intended to be an interlingua, a representation of meaning that is reasonably independent of language. IL2 is intended to capture similarities in meaning across languages and across different lexical/syntactic realizations within a language. For example, IL2 is expected to normalize over conversives (e.g. X bought a book from Y vs. Y sold a book to X) (as does FrameNet (Baker et al 1998)) and non-literal language usage (e.g. X started its business vs. X opened its doors to customers). The exact definition of IL2 will be the major research contribution of this project. 4.2 The Omega Ontology In progressing from IL0 to IL1, annotators have to select semantic terms (concepts) to represent the nouns, verbs, adjectives, and adverbs present in each sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998)"
W04-2709,J96-2004,0,0.0756983,"Missing"
W04-2709,2003.mtsummit-eval.3,1,0.726095,"al content, modality, speech acts, etc. At the same time, while incorporating these items, vagueness and redundancy must be eliminated from the annotation language. Many inter-event relations would need to be captured such as entity reference, time reference, place reference, causal relationships, associative relationships, etc. Finally, to incorporate these, crosssentence phenomena remain a challenge. From an MT perspective, issues include evaluating the consistency in the use of an annotation language given that any source text can result in multiple, different, legitimate translations (see Farwell and Helmreich, 2003) for discussion of evaluation in this light. Along these lines, there is the problem of annotating texts for translation without including in the annotations inferences from the source text. 9 Conclusions This is a radically different annotation project from those that have focused on morphology, syntax or even certain types of semantic content (e.g., for word sense disambiguation competitions). It is most similar to PropBank (Kingsbury et al 2002) and FrameNet (Baker et al 1998). However, it is novel in its emphasis on: (1) a more abstract level of mark-up (interpretation); (2) the assignment"
W04-2709,P03-1001,1,0.790541,"sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998), NMSU’s Mikrokosmos (Mahesh and Nirenburg, 1995), ISI's Upper Model (Bateman et al., 1989) and ISI's SENSUS (Knight and Luk, 1994). After the uppermost region of Omega was created by hand, these various resources’ contents were incorporated and, to some extent, reconciled. After that, several million instances of people, locations, and other facts were added (Fleischman et al., 2003). The ontology, which has been used in several projects in recent years (Hovy et al., 2001), can be browsed using the DINO browser at http://blombos.isi.edu:8000/dino; this browser forms a part of the annotation environment. Omega remains under continued development and extension. 4.1.2 IL1 IL1 is an intermediate semantic representation. It associates semantic concepts with lexical units like nouns, adjectives, adverbs and verbs (details of the ontology in section 4.2). It also replaces the syntactic relations in IL0, like subject and object, with thematic roles, like agent, theme and goal (de"
W04-2709,C18-2019,0,0.0664532,"Missing"
W04-2709,A97-1011,0,0.0452745,"first present the annotation process and tools used with it as well as the annotation manuals. Finally, setup issues relating to negotiating multi-site annotations are discussed. 6.1 Annotation process The annotation process was identical for each text. For the initial testing period, only English texts were annotated, and the process described here is for English text. The process for non-English texts will be, mutatis mutandis, the same. Each sentence of the text is parsed into a dependency tree structure. For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. For the initial testing period, annotators were not permitted to alter these structures. Already at this stage, some of the lexical items are replaced by features (e.g., tense), morphological forms are replaced by features on the citation form, and certain constructions are regularized (e.g., passive) and empty arguments inserted. It is this dependency structure that is loaded into the annotation tool and which each annotator then marks up. The annotator was instructed to annotate all nouns, verbs, adjectives, and adverbs. This involves annotating e"
W04-2709,1994.amta-1.25,0,0.0933693,"Missing"
W04-2709,C98-1013,0,\N,Missing
W06-1312,N06-1050,1,0.85381,"Missing"
W06-1312,E99-1015,1,0.884703,"Missing"
W06-1312,W06-1613,1,0.855471,"Missing"
W06-1312,P84-1044,0,0.676122,"Missing"
W06-1613,P06-1116,1,0.62025,"Missing"
W06-1613,J96-2004,0,0.0803218,"constrained to the paragraph boundary. In rare cases, paper-wide information is required (e.g., for PMot, we need to know that a praised approach is used by the authors, information which may not be local in the paragraph). Annotators are thus asked to skim-read the paper before annotation. One possible view on this annotation scheme could consider the first two sets of categories as “negative” and the third set of categories “positive”, in the sense of Pang et al. (2002) and Turney (2002). Authors need to make a point (namely, 3 As opposed to reference list items, which are fewer. Following Carletta (1996), we measure agreement in (E) Kappa, which follows the formula K = P (A)−P where 1−P (E) P(A) is observed, and P(E) expected agreement. Kappa ranges between -1 and 1. K=0 means agreement is only as expected by chance. Generally, Kappas of 0.8 are considered stable, and Kappas of .69 as marginally stable, according to the strictest scheme applied in the field. 4 2 Our citation processor can recognise these after parsing the citation list. 106 authors of the paper, and everybody else) are modelled by 185 patterns. For instance, in a paragraph describing related work, we expect to find references"
W06-1613,J02-4002,1,0.433414,": Distribution of citation categories Weak CoCoGM CoCoR0 P .78 .81 .77 R .49 .52 .46 F .60 .64 .57 Percentage Accuracy 0.77 Kappa (n=12; N=2829; k=2) 0.57 Macro-F 0.57 CoCo.56 .19 .28 CoCoXY .72 .54 .62 PBas .76 .46 .58 PUse .66 .61 .63 PModi .60 .27 .37 PMot .75 .64 .69 PSim .68 .38 .48 PSup .83 .32 .47 Neut .80 .92 .86 Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3). that recorded the presence of cues that our annotators associated with a particular class. 3.3 P R F Other features There are other features which we use for this task. We know from Teufel and Moens (2002) that verb tense and voice should be useful for recognizing statements of previous work, future work and work performed in the paper. We also recognise modality (whether or not a main verb is modified by an auxiliary, and which auxiliary it is). The overall location of a sentence containing a reference should be relevant. We observe that more PMot categories appear towards the beginning of the paper, as do Weak citations, whereas comparative results (CoCoR0, CoCoR-) appear towards the end of articles. More fine-grained location features, such as the location within the paragraph and the sectio"
W06-1613,W06-1312,1,0.843214,"Missing"
W06-1613,P02-1053,0,0.00484051,"Missing"
W06-1613,P84-1044,0,0.052741,"ve been created over the years, and the question has been studied in detail, even to the level of in-depth interviews with writers about each individual citation (Hodges, 1972). Part of this sustained interest in citations can be explained by the fact that bibliometric metrics are commonly used to measure the impact of a researcher’s work by how often they are cited (Borgman, 1990; Luukkonen, 1992). However, researchers from the field of discourse studies have long criticised purely quantitative citation analysis, pointing out that many citations are done out of “politeness, policy or piety” (Ziman, 1968), and that criticising citations or citations in pass103 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103–110, c Sydney, July 2006. 2006 Association for Computational Linguistics Li and Abe 96 Brown et al. 90a Resnik 95 Church and Gale 91 Rose et al. 90 Dagan et al. 94 Hindle 93 Hindle 90 Nitta and Niwa 94 Dagan et al 93 Pereira et al. 93 His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. Followi"
W06-1613,W02-1011,0,0.0180982,"Missing"
W06-1613,N06-1050,1,0.718252,"Missing"
W06-1613,W06-0804,1,0.742995,"Missing"
W06-1613,H91-1061,0,\N,Missing
W10-4213,N03-1003,0,0.0581404,"al., 1991), stuing, including the replacement of words (espedents’ reading comprehension shows significant cially verbs) with their dictionary definitions (Kaji improvements. An example of a revision choice et al., 2002) and the replacement of idiomatic or that might be applied differentially depending on otherwise troublesome expressions with simpler the literacy skills of the reader involves connecones. The emphasis has been on automatically tives such as because. Connectives that permit learning paraphrases from comparable or aligned pre-posed adverbial clauses have been found to corpora (Barzilay and Lee, 2003; Ibrahim et al., be difficult for third to fifth grade readers, even 2003). The text simplification and paraphrasing when the order of mention coincides with the literature does not address paraphrasing that recausal (and temporal) order (Anderson and Daviquires syntactic alterations such as those in Examson, 1988); this experimental result is consistent ple 1 or the question of appropriateness of differwith the observed order of emergence of connecent formulations of a discourse relation. tives in children’s narratives (Levy, 2003). Thus the b) version of the following example Some natural l"
W10-4213,P06-4020,0,0.181096,"res: PubMed Abstracts1 (technical writing from the Biomedical domain), and articles from the British National Corpus2 tagged as World News or Natural Science (popular science writing in the mainstream media). 3.2 Reformulation using Phrasal Parse Trees As described above, we have access to a corpus that contains aligned sentences for each pair of types (a type is a combination of a discourse marker and an information order; thus we have 8 types). In principle it should be easy to learn transfer rules between parse trees of aligned sentences. Figure 1 shows parse trees ( using the RASP parser (Briscoe et al., 2006)) for the active and the passive voice with “cause” as a verb. A transfer rule is derived by aligning nodes between two parse trees so that the rule only contains the differences in structure between the trees. In the representation in Figure 1, the variable ??X0[NP] maps 1 PubMed URL: http://www.ncbi.nlm.nih.gov/pubmed/ The British National Corpus, version 3 (BNC XML Edition). 2007. http://www.natcorp.ox.ac.uk 2 The explosion was caused by an incendiary device. (S (NP (AT The) (NN1 explosion)) (VP (VBDZ be+ed) (VP (VVN cause+ed) (PP (II by) (NP (AT1 an) (JJ incendiary) (NN1 device)))))) An in"
W10-4213,W03-1608,0,0.131632,"Missing"
W10-4213,P02-1028,0,0.149087,"Missing"
W10-4213,N03-1026,0,0.164746,"Missing"
W10-4213,1995.tmi-1.2,0,0.303742,"Missing"
W10-4213,N10-1144,1,0.90225,"expressing the discourse relation of causation using four lexico-syntactic discourse markers – “cause” as a verb and as a noun, “because” as a conjunction and “because of” as a preposition. 1 Introduction There are many reasons why a writer might want to choose one formulation of a discourse relation over another; for example, maintaining thread of discourse, avoiding shifts in focus and issues of salience and end weight. There are also reasons to use different formulations for different audiences; for example, to account for differences in reading skills and domain knowledge. In recent work, Siddharthan and Katsos (2010) demonstrated through psycholinguistic experiments that domain experts and lay readers show significant differences in which formulations of causation they find acceptable. They further showed that the most appropriate formulation depends both on the domain expertise of the user and the propositional content of the sentence, and that these preferences can be learnt in a supervised machine learning framework. That work, as does much of the related comprehension and literacy literature, used manually reformulated sentences. In this paper, we present an approach to automate such complex reformula"
W10-4213,N07-1023,0,0.0350625,"of such user-driven generation research to text regeneration areas. 2.3 Sentence Compression Sentence compression is a research area that aims to shorten sentences for the purpose of summarising the main content. There are similarities between our interest in reformulation and existing work in sentence compression. Sentence compression has usually been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions. The compression rules learnt are therefore tree-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003) of some variety. These approaches focus on deletion operations, mostly performed low down in the parse tree to remove modifiers. Further they make assumptions about isomorphism between the aligned tree, which means they cannot be readily applied to more complex reformulation operations such as insertion and reordering that are essential to perform reformulations such as those in Example 1. Cohn and Lapata (2009) provide an approach based on Synchronous Tree Substitution Grammar (STSG) that in principle can handle the range of reformulation operations. However, given the"
W10-4213,W03-2317,0,0.0381491,"m mulations. For example, SkillSum (Williams and Anderson and Davison (1988), p. 35): Reiter, 2008) and ICONOCLAST (Power et al., 2003) are two contemporary generation systems that allow for specifying aspects of style such as choice of discourse marker, clause order, repetition and sentence and paragraph lengths in the form of constraints that can be optimised. However, to date, these systems do not consider syntactic reformulations of the type we are interested in. Our research is directly relevant to such generation systems as it can help such systems make decisions in a principled manner. Williams et al. (2003) examined the impact of discourse level choices on readability in the domain of reporting the results of literacy assessment tests, using the results of the test to control both the content and the realisation of the generated report. Our research aims to facilitate the transfer of such user-driven generation research to text regeneration areas. 2.3 Sentence Compression Sentence compression is a research area that aims to shorten sentences for the purpose of summarising the main content. There are similarities between our interest in reformulation and existing work in sentence compression. Sen"
W11-2802,N03-1003,0,0.0705118,"Missing"
W11-2802,J05-3002,0,0.0132212,"mmar (STSG) that in principle can handle the range of reformulation operations. However, given their focus on sentence compression, they restricted themselves to local transformations near the bottom of the parse tree. Siddharthan (2010) compared different representations and concluded that phrasal parse trees were inadequate for learning complex lexicosyntactic transformation rules and that dependency structures were more suited. Indeed dependency structures are now increasingly popular for other text regeneration tasks, such as sentence fusion (Krahmer et al., 2008; Marsi and Krahmer, 2005; Barzilay and McKeown, 2005). 3 Simplification using typed dependencies We now summarise R EGEN T, our system for regenerating text, including two approached to generation: gen-light (§3.1) and gen-heavy (§3.2). As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence. These are triplets consisting of a relation-type and two arguments. We will use the following sentence to illustrate the process (note that the parser provides word position and part-of-speech tags in addition to dependency relations): The/DT cat/NN was/VBD chased/VBN by/IN"
W11-2802,C96-2183,0,0.94898,"Missing"
W11-2802,J08-4005,0,0.0198641,"Missing"
W11-2802,de-marneffe-etal-2006-generating,0,0.019615,"Missing"
W11-2802,N07-1023,0,0.0232185,"bustness in the face of incorrect parser analyses. 2.3 Other text regeneration tasks Sentence compression is a related research area that aims to shorten sentences for the purpose of summarising the main content. There are similarities between our interest in reformulation and existing work in sentence compression. Sentence compression has usually been addressed in a generative framework, where transformation rules are learnt from parsed corpora of sentences aligned with manually compressed versions. The compression rules learnt are therefore tree-tree transformations (Knight and Marcu, 2000; Galley and McKeown, 2007; Riezler et al., 2003) of some variety. These approaches focus on deletion operations, mostly performed low down in the parse tree to remove modi4 fiers. Further they make assumptions about isomorphism between the aligned tree, which means they cannot be readily applied to more complex reformulation operations such as insertion and reordering. Cohn and Lapata (2009) provide an approach based on Synchronous Tree Substitution Grammar (STSG) that in principle can handle the range of reformulation operations. However, given their focus on sentence compression, they restricted themselves to local"
W11-2802,W03-1608,0,0.0496601,"Missing"
W11-2802,P02-1028,0,0.0397516,"Missing"
W11-2802,P08-2049,0,0.0251073,"Missing"
W11-2802,A97-1039,0,0.390259,"sing errors cause the realiser in the genheavy approach to order words and phrases in ways that are disliked by our evaluators. to: The original police inquiry led to Mulcaire being jailed in 2007. The police inquiry also discovered evidence that he has successfully intercepted voicemail messages belonging to Rebekah Brooks. Rebekah Brooks was editor of the Sun. This was when Mulcaire was working exclusively for its Sunday stablemate. The main aim of this paper is to describe and compare two methods for generating sentences from the transformed dependency graphs: 1. gen-heavy: We use RealPro (Lavoie and Rambow, 1997), a statistical realiser to generate, making all decisions related to morphology and word ordering. 1 Introduction In this paper, we present a system, R EGEN T, for text regeneration tasks such as text simplification, style modification or paraphrase. Our system applies transformation rules specified in XML files, to a typed dependency representation obtained from the Stanford Parser (De Marneffe et al., 2006). There are currently rule files for simplifying coordination (of verb phrases and full clauses), subordination, apposition and relative clauses, as well as conversion of passive to activ"
W11-2802,W05-1612,0,0.00797821,"ous Tree Substitution Grammar (STSG) that in principle can handle the range of reformulation operations. However, given their focus on sentence compression, they restricted themselves to local transformations near the bottom of the parse tree. Siddharthan (2010) compared different representations and concluded that phrasal parse trees were inadequate for learning complex lexicosyntactic transformation rules and that dependency structures were more suited. Indeed dependency structures are now increasingly popular for other text regeneration tasks, such as sentence fusion (Krahmer et al., 2008; Marsi and Krahmer, 2005; Barzilay and McKeown, 2005). 3 Simplification using typed dependencies We now summarise R EGEN T, our system for regenerating text, including two approached to generation: gen-light (§3.1) and gen-heavy (§3.2). As mentioned before, we use the Stanford parser (De Marneffe et al., 2006) to obtain a typed dependency representation of the input sentence. These are triplets consisting of a relation-type and two arguments. We will use the following sentence to illustrate the process (note that the parser provides word position and part-of-speech tags in addition to dependency relations): The/DT ca"
W11-2802,N10-1144,1,0.882009,"Missing"
W11-2802,W03-2314,1,0.826255,"Missing"
W11-2802,W10-4213,1,0.84868,"plies transformation rules specified in XML files, to a typed dependency representation obtained from the Stanford Parser (De Marneffe et al., 2006). There are currently rule files for simplifying coordination (of verb phrases and full clauses), subordination, apposition and relative clauses, as well as conversion of passive to active voice; for instance, simplifying: 2. gen-light: We reuse word order and morphology from the original sentence, and specify any changes to these as part of each transformation rule. Both options have pros and cons. In the gen-light approach described in detail in Siddharthan (2010) and summarised in §3.1, we can reuse information from the input sentence as much as possible, leading to very efficient generation. The downside is 2 Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 2–11, c Nancy, France, September 2011. 2011 Association for Computational Linguistics that we need to encode some generation decisions within transfer rules, making them cumbersome to write and difficult to learn automatically. A case can be made, particularly for the issue of subject-verb agreement, for such issues to be handled by a generator. This would mak"
W11-2802,N03-1026,0,\N,Missing
W11-2824,W03-1602,0,0.0248768,"context, and ask volunteers to rank their preferences between two common but ambiguous lexical substitutions, and two uncommon but also unambiguous ones. Preliminary results suggest a slight preference towards the unambiguous. 1 2 Introduction Paraphrasing is a sub-field of natural language processing (NLP) which aims to modify utterances from one form into another, without changing their meaning. One particular application of paraphrase is text modification to improve information access for low-level readers; e.g., syntactic simplification (Siddharthan, 2006; Siddharthan, 2003), paraphrase (Inui et al., 2003) and lexical simplification (Devlin and Tait, 1998). Lexical simplification is typically defined as the task of replacing difficult words with simpler ones. However, there are many open question about when one word would be a good substitute for another in context. Our analysis of WordNet 3.0 entries (Miller, 1995) demonstrates an inverse correlation between word frequency rank in the BNC1 and num1 Andrew Starkey University of Aberdeen School of Engineering and Physical Sciences a.starkey@abdn.ac.uk The British National Corpus, version 3, 2007. Distributed 176 Related work Hayes (1988) found c"
W11-2824,P02-1028,0,0.0159138,"logy of the time. The PSET project (Devlin and Tait, 1998; Carroll et al., 1998) looked at simplifying news reports for aphasics and was perhaps the first computational work to focus on lexical simplification (replacing difficult words with easier ones). The PSET project used WordNet (Miller, 1995) to identify synonyms and the Oxford Psycholinguistic Database (Quinlan, 1992) to determine the relative difficulty of words (Devlin and Tait, 1998). Elsewhere, there has been interest in paraphrasing, including the replacement of difficult words (especially verbs) with their dictionary definitions (Kaji et al., 2002). The tradeoff between brevity (and perhaps fluency) and clarity (or ambiguity) was studied by Khan et al. (2008) in the context of generating referby Oxford University Computing Services on behalf of the BNC Consortium. http://www.natcorp.ox.ac.uk Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 176–180, c Nancy, France, September 2011. 2011 Association for Computational Linguistics ring expressions with the specific form “Adj Noun and Noun” (e.g., old men and women) where the scope of the adjective is ambiguous. They found that hearers prefer to read cle"
W11-2824,C08-1055,0,0.0341892,"Missing"
W11-2824,W03-2314,1,\N,Missing
W12-1520,W10-1301,0,0.0308918,"ges 120–124, c Utica, May 2012. 2012 Association for Computational Linguistics (a) (b) Figure 1: Plot of (a) distance from nest as a function of time, and (b) clusters of visited locations. ically been used to generate summaries of technical data for professionals, such as engineers, nurses and oil rig workers. There is some work on the use of data-to-text for lay audiences; e.g., generating narratives from sensor data for automotive (Reddington et al., 2011) and environmental (Molina et al., 2011) applications, generating personal narratives to help children with complex communication needs (Black et al., 2010), and summarising neonatal intensive care data for parents (Mahamood et al., 2008). Our application differs from the above-mentioned data-to-text applications, in that we aim to generate inspiring as well as informative texts. It bears some resemblance to NLG systems that offer “infotainment”, such as Dial Your Disc (Van Deemter and Odijk, 1997) and Ilex (O’Donnell et al., 2001). In fact, Dial Your Disc, which generates spoken monologues about classical music, focused emphatically on generating engaging texts, and achieved linguistic variation through the use of recursive, syntactically struct"
W12-2203,W09-0629,0,0.0745978,"e, but allows us to tease apart the fluency and comprehension issues that arise. 1 Introduction In Natural Language Generation, recent approaches to evaluation tend to consider either “naturalness” or “usefulness”. Following evaluation methodologies commonly used for machine translation and summarisation, there have been attempts to measure naturalness in NLG by comparison to human generated gold standards. This has particularly been the case in evaluating referring expressions, where the generated expression can be treated as a set of attributes and compared with human generated expressions (Gatt et al., 2009; Viethen and Dale, 2006), but there have also been attempts at evaluating sentences this way. For instance, Langkilde-Geary (2002) generate sentences from a parsed analysis of an existing sentence, and evaluate by comparison to the original. However, this approach has been criticised at many levels (see for example, Gatt et al. (2009) or Sripada et al. (2003)); for instance, because there are many good ways to realise a sentence, because typical NLG tasks do not come with reference sentences, and because fluency judgements in the monolingual case are more subtle than for machine translation."
W12-2203,W02-2103,0,0.0799671,", recent approaches to evaluation tend to consider either “naturalness” or “usefulness”. Following evaluation methodologies commonly used for machine translation and summarisation, there have been attempts to measure naturalness in NLG by comparison to human generated gold standards. This has particularly been the case in evaluating referring expressions, where the generated expression can be treated as a set of attributes and compared with human generated expressions (Gatt et al., 2009; Viethen and Dale, 2006), but there have also been attempts at evaluating sentences this way. For instance, Langkilde-Geary (2002) generate sentences from a parsed analysis of an existing sentence, and evaluate by comparison to the original. However, this approach has been criticised at many levels (see for example, Gatt et al. (2009) or Sripada et al. (2003)); for instance, because there are many good ways to realise a sentence, because typical NLG tasks do not come with reference sentences, and because fluency judgements in the monolingual case are more subtle than for machine translation. Readability metrics, by comparison, do not rely on reference texts, and try to model the linguistic quality of a text based on feat"
W12-2203,P10-1056,0,0.0306626,"cause there are many good ways to realise a sentence, because typical NLG tasks do not come with reference sentences, and because fluency judgements in the monolingual case are more subtle than for machine translation. Readability metrics, by comparison, do not rely on reference texts, and try to model the linguistic quality of a text based on features derived from the text. This body of work ranges from the Flesch Metric (Flesch, 1951), which is based on average word and sentence length, to more systematic evaluations of various lexical, syntactic and discourse characteristics of a text (cf. Pitler et al. (2010), who assess readability of textual summaries). Some researchers have also suggested measuring edit distance by using a human to revise a system generated text and quantifying the revisions made (Sripada et al., 2003). This does away with the need for reference texts and is quite suited to expert domains such as medicine or weather forecasting, where a domain expert can easily correct system output. Analysis of these corrections can provide feedback on problematic content and style. We have previously evaluated text reformulation applications by asking readers which version they prefer (Siddha"
W12-2203,N10-1144,1,0.927013,"iments (Sections 3.1 and 3.2). We finish with a discussion of their suitability for more general evaluation of NLG with target readers. 2 Data We use a dataset created to explore generation choices in the context of expressing causal relations; specifically, the choice of periphrastic causative (Wolff et al., 2005) and information order. The dataset considers four periphrastic causatives (henceforth referred to as discourse markers): “because”, “because of ”, the verb “cause” and the noun 18 “cause” with different lexico-syntactic properties. We present an example from this dataset below (cf. Siddharthan and Katsos (2010) for details): (1) a. b. c. d. e. f. g. h. Fructose-induced hypertension is caused by increased salt absorption by the intestine and kidney. [b caused-by a] Increased salt absorption by the intestine and kidney causes fructose-induced hypertension. [a caused b] Fructose-induced hypertension occurs because of increased salt absorption by the intestine and kidney. [b because-of a] Because of increased salt absorption by the intestine and kidney, fructose-induced hypertension occurs. [because-of ab] Fructose-induced hypertension occurs because there is increased salt absorption by the intestine a"
W12-2203,J11-4007,1,0.799728,"(2010), who assess readability of textual summaries). Some researchers have also suggested measuring edit distance by using a human to revise a system generated text and quantifying the revisions made (Sripada et al., 2003). This does away with the need for reference texts and is quite suited to expert domains such as medicine or weather forecasting, where a domain expert can easily correct system output. Analysis of these corrections can provide feedback on problematic content and style. We have previously evaluated text reformulation applications by asking readers which version they prefer (Siddharthan et al., 2011), or through the use of Likert scales (Likert, 1932) for measuring meaning preservation and grammaticality (Siddharthan, 2006). However, none of these approaches tell us very much about the comprehensibility of a text for an end reader. To address this, there has been recent interest in task based evaluations. Task based evaluations directly evaluate generated utterances for their utility 17 NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 17–24, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguisti"
W12-2203,W06-1410,0,0.0295599,"tease apart the fluency and comprehension issues that arise. 1 Introduction In Natural Language Generation, recent approaches to evaluation tend to consider either “naturalness” or “usefulness”. Following evaluation methodologies commonly used for machine translation and summarisation, there have been attempts to measure naturalness in NLG by comparison to human generated gold standards. This has particularly been the case in evaluating referring expressions, where the generated expression can be treated as a set of attributes and compared with human generated expressions (Gatt et al., 2009; Viethen and Dale, 2006), but there have also been attempts at evaluating sentences this way. For instance, Langkilde-Geary (2002) generate sentences from a parsed analysis of an existing sentence, and evaluate by comparison to the original. However, this approach has been criticised at many levels (see for example, Gatt et al. (2009) or Sripada et al. (2003)); for instance, because there are many good ways to realise a sentence, because typical NLG tasks do not come with reference sentences, and because fluency judgements in the monolingual case are more subtle than for machine translation. Readability metrics, by c"
W14-4404,P14-1041,0,0.0391546,"proach based on syntax-based SMT (Yamada and 16 Proceedings of the 8th International Natural Language Generation Conference, pages 16–25, c Philadelphia, Pennsylvania, 19-21 June 2014. 2014 Association for Computational Linguistics Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Woodsend and Lapata (2011) propose quasi-synchronous tree substitution grammars (QTSG) for a similarly wide range of simplification operations as well as lexical substitution. Narayan and Gardent (2014) combine PMBT for local paraphrase with a syntactic splitting component based on a deep semantic representation. None of these systems model morphological information, which means some simplification operations such as voice conversion cannot be handled correctly. Against this limitation, hand-crafted systems have an advantage here, as they tend to encode the maximum linguistic information. We have previously described systems (Siddharthan, 2010; Siddharthan, 2011) that can perform voice conversion accurately and use transformation rules that encode morphological changes as well as deletions,"
W14-4404,P11-2087,0,0.022421,"uman simplifications. 1 Introduction Text simplification is the process of reducing the linguistic complexity of a text, while still retaining the original information content and meaning. Text Simplification is often thought of as consisting of two components - syntactic simplification and lexical simplification. While syntactic simplification aims at reducing the grammatical complexity of a sentence, lexical simplification focuses on replacing difficult words or short phrases by simpler variants. Traditionally, entirely different approaches have been used for lexical (Devlin and Tait, 1998; Biran et al., 2011; Yatskar et al., 2010; Specia et al., 2012) and syntactic simplification (Canning, 2002; Chandrasekar et al., 1996; Siddharthan, 2011; De Belder and Moens, 2010; Candido Jr et al., 2009). Recent years have seen the application of machine translation inspired approaches to text simplification. These approaches learn from aligned English and Simplified English sentences extracted from the Simple English Wikipedia (SEW) corpus (simple.wikipedia.org). However, even these approaches (Woodsend and Lapata, 2 Related work Text simplification systems are characterised by the level of linguistic knowle"
W14-4404,E14-1076,1,0.639874,"described systems (Siddharthan, 2010; Siddharthan, 2011) that can perform voice conversion accurately and use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting. On the other hand, such hand-crafted systems are limited in scope to syntactic simplificatio as there are too many lexico-syntactic and lexical simplifications to enumerate by hand. We have also previously described how to construct a hybrid system that combines automatically derived lexical rules with hand-crafted syntactic rules within a single framework (Siddharthan and Mandya, 2014). We extend that work here by describing how such automatically learnt rules can be generalised. storm advmod intensive storm advmod strongest advmod most Figure 2: Transduction of Elementary Trees (ETs) (2011) for learning rules. These datasets consist of ∼140K aligned simplified and original sentence pairs obtained from Simple English Wikipedia and English Wikipedia. The rules are acquired in the format required by the RegenT text simplification system (Siddharthan, 2011), which is used to implement the simplification. This requires dependency parses from the Stanford Parser (De Marneffe et"
W14-4404,J06-1003,0,0.011974,"following rule shown in 6(a). This is the original rule extracted from the training data (cf. Tab. 2). RULE 6( A ): 3 both the DELETE and INSERT lists are removed entirely from the rule: in expanding lexical context. The idea is that the lexical specification of context variables in rules can be expanded by identifying related words in WordNet. We propose to use Lin’s similarity measure (Lin, 1998), an information content based similarity measure for our experiments as information content based measures are observed to perform better in deriving similar terms, in comparison to other methods (Budanitsky and Hirst, 2006). Lin’s formula is based on Resnik’s. Let IC(c) = −log p(c) be the information content of a concept (synset) in WordNet, where p(c) is the likelihood of seeing the concept (or any of its hyponyms) in a corpus. Resnik defines the similarity of two concepts c1 and c2 as simres (c1 , c2 ) = maxc∈S(c1 ,c2) IC(c), the IC of the most specific class c that subsumes both c1 and c2. Lin’s formula normalises this by the IC of each class: 3.5 2 Table 2: Details of rules derived with different length in DELETE and INSERT relations Table 1: Number of extracted rules where the INSERT and DELETE lists contai"
W14-4404,W03-2314,1,0.852722,"ions: 26 hand-crafted rules for apposition, relative clauses, and combinations of the two; a further 85 rules handle subordination and coordination (these are greater in number because they are lexicalised on the conjunction); 11 further rules cover voice conversion from passive to active; 38 rules for light verbs and various cleft constructions; 99 rules to handle common verbose constructions described in the old GNU diction utility; 14 rules to standardise quotations. The RegenT system does not have a decoder or a planner. It also does not address discourse issues such as those described in Siddharthan (2003a), though it includes a component that improves relative clause attachment based on Siddharthan (2003b). It applies the simplification rules exhaustively to the dependency parse; i.e., every rule for which the DELETE list is matched is applied iteratively (see Siddharthan (2011) for details). We have created a hybrid text simplification system by integrating our automatically acquired rules (lexical context extended using WordNet for single change rules, and lexical context removed for longer rules) with the existing RegenT system as described above. This is sensible, as the existing manually"
W14-4404,W09-2105,0,0.0763782,"Missing"
W14-4404,W03-2602,1,0.908825,"ions: 26 hand-crafted rules for apposition, relative clauses, and combinations of the two; a further 85 rules handle subordination and coordination (these are greater in number because they are lexicalised on the conjunction); 11 further rules cover voice conversion from passive to active; 38 rules for light verbs and various cleft constructions; 99 rules to handle common verbose constructions described in the old GNU diction utility; 14 rules to standardise quotations. The RegenT system does not have a decoder or a planner. It also does not address discourse issues such as those described in Siddharthan (2003a), though it includes a component that improves relative clause attachment based on Siddharthan (2003b). It applies the simplification rules exhaustively to the dependency parse; i.e., every rule for which the DELETE list is matched is applied iteratively (see Siddharthan (2011) for details). We have created a hybrid text simplification system by integrating our automatically acquired rules (lexical context extended using WordNet for single change rules, and lexical context removed for longer rules) with the existing RegenT system as described above. This is sensible, as the existing manually"
W14-4404,C96-2183,0,0.206381,"y of a text, while still retaining the original information content and meaning. Text Simplification is often thought of as consisting of two components - syntactic simplification and lexical simplification. While syntactic simplification aims at reducing the grammatical complexity of a sentence, lexical simplification focuses on replacing difficult words or short phrases by simpler variants. Traditionally, entirely different approaches have been used for lexical (Devlin and Tait, 1998; Biran et al., 2011; Yatskar et al., 2010; Specia et al., 2012) and syntactic simplification (Canning, 2002; Chandrasekar et al., 1996; Siddharthan, 2011; De Belder and Moens, 2010; Candido Jr et al., 2009). Recent years have seen the application of machine translation inspired approaches to text simplification. These approaches learn from aligned English and Simplified English sentences extracted from the Simple English Wikipedia (SEW) corpus (simple.wikipedia.org). However, even these approaches (Woodsend and Lapata, 2 Related work Text simplification systems are characterised by the level of linguistic knowledge they encode, and by whether their simplification rules are handcrafted or automatically acquired from a corpus."
W14-4404,W10-4213,1,0.843917,"quasi-synchronous tree substitution grammars (QTSG) for a similarly wide range of simplification operations as well as lexical substitution. Narayan and Gardent (2014) combine PMBT for local paraphrase with a syntactic splitting component based on a deep semantic representation. None of these systems model morphological information, which means some simplification operations such as voice conversion cannot be handled correctly. Against this limitation, hand-crafted systems have an advantage here, as they tend to encode the maximum linguistic information. We have previously described systems (Siddharthan, 2010; Siddharthan, 2011) that can perform voice conversion accurately and use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting. On the other hand, such hand-crafted systems are limited in scope to syntactic simplificatio as there are too many lexico-syntactic and lexical simplifications to enumerate by hand. We have also previously described how to construct a hybrid system that combines automatically derived lexical rules with hand-crafted syntactic rules within a single framework (Siddharthan and Mandya, 2014). We ext"
W14-4404,W11-1601,0,0.558145,"h Wikipedia (SEW) corpus (simple.wikipedia.org). However, even these approaches (Woodsend and Lapata, 2 Related work Text simplification systems are characterised by the level of linguistic knowledge they encode, and by whether their simplification rules are handcrafted or automatically acquired from a corpus. In recent times, the availability of a corpus of aligned English Wikipedia (EW) and Simple English Wikipedia (SEW) sentences has lead to the application of various “monolingual translation” approaches to text simplification. Phrase Based Machine Translation (PBMT) systems (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012) use the least linguistic knowledge (only word sequences), and as such are ill equipped to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and 16 Proceedings of the 8th International Natural Language Generation Conference, pages 16–25, c Philadelphia, Pennsylvania, 19-21 June 2014. 2014 Association for Computational Linguistics Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the pars"
W14-4404,W11-2802,1,0.157254,"taining the original information content and meaning. Text Simplification is often thought of as consisting of two components - syntactic simplification and lexical simplification. While syntactic simplification aims at reducing the grammatical complexity of a sentence, lexical simplification focuses on replacing difficult words or short phrases by simpler variants. Traditionally, entirely different approaches have been used for lexical (Devlin and Tait, 1998; Biran et al., 2011; Yatskar et al., 2010; Specia et al., 2012) and syntactic simplification (Canning, 2002; Chandrasekar et al., 1996; Siddharthan, 2011; De Belder and Moens, 2010; Candido Jr et al., 2009). Recent years have seen the application of machine translation inspired approaches to text simplification. These approaches learn from aligned English and Simplified English sentences extracted from the Simple English Wikipedia (SEW) corpus (simple.wikipedia.org). However, even these approaches (Woodsend and Lapata, 2 Related work Text simplification systems are characterised by the level of linguistic knowledge they encode, and by whether their simplification rules are handcrafted or automatically acquired from a corpus. In recent times, t"
W14-4404,S12-1046,0,0.0135039,"simplification is the process of reducing the linguistic complexity of a text, while still retaining the original information content and meaning. Text Simplification is often thought of as consisting of two components - syntactic simplification and lexical simplification. While syntactic simplification aims at reducing the grammatical complexity of a sentence, lexical simplification focuses on replacing difficult words or short phrases by simpler variants. Traditionally, entirely different approaches have been used for lexical (Devlin and Tait, 1998; Biran et al., 2011; Yatskar et al., 2010; Specia et al., 2012) and syntactic simplification (Canning, 2002; Chandrasekar et al., 1996; Siddharthan, 2011; De Belder and Moens, 2010; Candido Jr et al., 2009). Recent years have seen the application of machine translation inspired approaches to text simplification. These approaches learn from aligned English and Simplified English sentences extracted from the Simple English Wikipedia (SEW) corpus (simple.wikipedia.org). However, even these approaches (Woodsend and Lapata, 2 Related work Text simplification systems are characterised by the level of linguistic knowledge they encode, and by whether their simpli"
W14-4404,de-marneffe-etal-2006-generating,0,0.0163881,"Missing"
W14-4404,P05-1067,0,0.0985796,"on the planet in 1989. (b) It was the second strongest storm on the planet in 1989. An automatic comparison of the dependency parses for the two sentences (using the Stanford Parser) reveals that there are two typed dependencies that occur only in the parse of the first sentence, and one that occur only in the parse of the second sentence. Thus, to convert the first sentence into the second, we need to delete two dependencies and introduce one other. From this example, we extract the following rule: 3 Simplification using synchronous dependency grammars We follow the architecture proposed in Ding and Palmer (2005) for Synchronous Dependency Insertion Grammars, reproduced in Fig. 1. In this paper, we focus on the decomposition of a dependency parse into Elementary Trees (ETs), and the learning of rules to transduce a source ET to a target ET. We use the datasets of Coster and Kauchak (2011) and Woodsend and Lapata RULE 1: MOST _ INTENSIVE 2 STRONGEST 1. DELETE Input Sentence −→ Dependency Parse −→ Source ETs (a) advmod(?X0[intensive], ?X1[most]) (b) advmod(?X2[storm], ?X0[intensive]) ↓ ET Transfer 2. INSERT ↓ Output Sentences ←− Generation ←− Target ETs (a) advmod(?X2, ?X3[strongest]) The rule contains"
W14-4404,D11-1038,0,0.41267,"d as such are ill equipped to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and 16 Proceedings of the 8th International Natural Language Generation Conference, pages 16–25, c Philadelphia, Pennsylvania, 19-21 June 2014. 2014 Association for Computational Linguistics Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Woodsend and Lapata (2011) propose quasi-synchronous tree substitution grammars (QTSG) for a similarly wide range of simplification operations as well as lexical substitution. Narayan and Gardent (2014) combine PMBT for local paraphrase with a syntactic splitting component based on a deep semantic representation. None of these systems model morphological information, which means some simplification operations such as voice conversion cannot be handled correctly. Against this limitation, hand-crafted systems have an advantage here, as they tend to encode the maximum linguistic information. We have previously described s"
W14-4404,P12-1107,0,0.116547,"Missing"
W14-4404,P01-1067,0,0.109935,"Missing"
W14-4404,N10-1056,0,0.0323287,". 1 Introduction Text simplification is the process of reducing the linguistic complexity of a text, while still retaining the original information content and meaning. Text Simplification is often thought of as consisting of two components - syntactic simplification and lexical simplification. While syntactic simplification aims at reducing the grammatical complexity of a sentence, lexical simplification focuses on replacing difficult words or short phrases by simpler variants. Traditionally, entirely different approaches have been used for lexical (Devlin and Tait, 1998; Biran et al., 2011; Yatskar et al., 2010; Specia et al., 2012) and syntactic simplification (Canning, 2002; Chandrasekar et al., 1996; Siddharthan, 2011; De Belder and Moens, 2010; Candido Jr et al., 2009). Recent years have seen the application of machine translation inspired approaches to text simplification. These approaches learn from aligned English and Simplified English sentences extracted from the Simple English Wikipedia (SEW) corpus (simple.wikipedia.org). However, even these approaches (Woodsend and Lapata, 2 Related work Text simplification systems are characterised by the level of linguistic knowledge they encode, and b"
W14-4404,C10-1152,0,0.399167,"fication rules are handcrafted or automatically acquired from a corpus. In recent times, the availability of a corpus of aligned English Wikipedia (EW) and Simple English Wikipedia (SEW) sentences has lead to the application of various “monolingual translation” approaches to text simplification. Phrase Based Machine Translation (PBMT) systems (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012) use the least linguistic knowledge (only word sequences), and as such are ill equipped to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and 16 Proceedings of the 8th International Natural Language Generation Conference, pages 16–25, c Philadelphia, Pennsylvania, 19-21 June 2014. 2014 Association for Computational Linguistics Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Woodsend and Lapata (2011) propose quasi-synchronous tree substitution grammars (QTSG) for a similarly wide range of simplification operations as well"
W15-4726,W07-2315,1,0.717634,"of all incidents, we use an agglomerative clustering algorithm, where the distance between two incidents is defined by the weighted similarity of all above mentioned features. The algorithm also has a minimal cluster size, which is influenced by the total number of incidents, and a maximum distance, which are used to decide, when to stop the agglomeration and which clusters are irrelevant. In this way we try to balance the interest between greatest possible and tightest possible clus3.3 NLG The Data-2-Text module of our prototype follows the three-stage pipelined architecture, as described by Reiter (2007), and uses simpleNLG (Gatt and Reiter, 2009) as surface realiser. 3.3.1 Psychological Background Since we try to achieve a behaviour change, we use different psychological techniques for the verbalisation of feedback, which have been shown to be useful in the literature (cf. Section 2.1) to maximize the likelihood of achieving this goal. This is reflected particularly in the document plan, which follows mainly the three techniques described in Section 2.2. Another psychological aspect was already taken into account during the specification of relevant behaviour. We try to avoid unnecessary fru"
W15-4726,W09-0613,1,0.883422,"rative clustering algorithm, where the distance between two incidents is defined by the weighted similarity of all above mentioned features. The algorithm also has a minimal cluster size, which is influenced by the total number of incidents, and a maximum distance, which are used to decide, when to stop the agglomeration and which clusters are irrelevant. In this way we try to balance the interest between greatest possible and tightest possible clus3.3 NLG The Data-2-Text module of our prototype follows the three-stage pipelined architecture, as described by Reiter (2007), and uses simpleNLG (Gatt and Reiter, 2009) as surface realiser. 3.3.1 Psychological Background Since we try to achieve a behaviour change, we use different psychological techniques for the verbalisation of feedback, which have been shown to be useful in the literature (cf. Section 2.1) to maximize the likelihood of achieving this goal. This is reflected particularly in the document plan, which follows mainly the three techniques described in Section 2.2. Another psychological aspect was already taken into account during the specification of relevant behaviour. We try to avoid unnecessary frustration by only reporting behaviour that ca"
W15-4726,W13-2115,0,0.0306267,"g spatio-temporal data in other domains (Turner et al., 2008; Ponnamperuma et al., 2013). Hattie and Timperley (2007) pointed out, that “specific goals are more effective than general or nonspecific ones” (emphasis added). Ye and Johnson (1995), Teach and Shortliffe (1987), Weiner (1980) and many others pointed out, that it is crucial for the acceptance of feedback from computer systems, that the feedback is justified in a way that allows the user to reconstruct how conclusions were drawn. 2 2.3 Related Work NLG systems that generate feedback have proven to be helpful in many different areas. Gkatzia et al. (2013) for example showed that an NLG system can provide students with feedback that is perceived as helpful as feedback from lecturers, using reinforcement learning. The SkillSum system (Williams and Reiter, 2008), which generates feedback about basic reading skills and performed significantly better than a comparable system that used canned texts. In the context of citizen science, automatically generated feedback has been shown to improve both skill levels and motivation levels among participants (Blake et al., 2012; van der Wal et al., 2016). As Eugenio et al. (2005) have shown, aggregation is o"
W15-4726,P13-4029,1,0.828847,"Missing"
W15-4726,C12-1020,1,\N,Missing
W15-4726,P05-1007,0,\N,Missing
W16-2807,W11-1701,0,0.10389,"stance of the text towards the issue being debated is then learnt in a supervised framework as a function of these features. The main advantage of our feature set is that it is scrutable: The reasons for a classification can be explained to a human user in natural language. We also report that our method outperforms previous approaches to stance classification as well as a range of baselines based on sentiment analysis and topic-sentiment analysis. 1 Introduction In recent years, stance classification for online debates has received increasing research interest (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Ranade et al., 2013; Sridhar et al., 2014). Given a post belonging to a two-sided debate on an issue (e.g. abortion rights; see Table 1), the task is classify the post as for or against the issue. The argumentative nature of such posts makes stance classification difficult; for example, one • topic-stance features – a set of automatically extracted ‘topic terms’ (for abortion rights, these would include, for example, ‘fetus’, ‘baby’, ‘woman’ and ‘life’), where each topic term is associated with a distributional lexical model (DLM) that captures the writer’s stance toward"
W16-2807,D13-1170,0,0.00317315,"ained in the sentence, we can associate a negative sentiment with topic terms ‘fetus’ and ‘human’. Similarly for sentence 2, a negative sentiment can be associated with topic terms such as ‘fetus’, ‘woman’ and ‘pregnancy’. This model has the advantage over the sentiment analysis baseline that sentiment is assoIn addition to the features proposed above, we experimented with a variety of baselines for comparison. 3.4.1 Sentiment model Our first baseline involved treating stance (‘for’ or ‘against’) as sentiment (‘positive’ or ‘negative’). For this purpose, we used the Stanford sentiment tool 1 (Socher et al., 2013) to obtain sentence-level sentiment labels and provide these as features for stance classification of posts. 3.4.2 Topic-sentiment model However, we do not expect a direct equivalence between sentiment and stance; for example, in Table 3, a negative sentiment is expressed in sentences arguing for abortion and 1 http://nlp.stanford.edu:8080/sentiment/ 64 Sentences arguing for abortion rights 1. A fetus is no more a human than an acorn is a tree. 2. The fetus causes sickness, discomfort, and and extreme pain to a woman during her pregnancy and labor. For abortion rights The fetus causes physical"
W16-2807,W08-1301,0,0.163707,"Missing"
W16-2807,W10-0214,0,0.0664245,"Missing"
W16-2807,N09-1057,0,0.0907856,"Missing"
W16-2807,D14-1083,0,0.0520293,"Missing"
W16-2807,N12-1072,0,0.109017,"towards the issue being debated is then learnt in a supervised framework as a function of these features. The main advantage of our feature set is that it is scrutable: The reasons for a classification can be explained to a human user in natural language. We also report that our method outperforms previous approaches to stance classification as well as a range of baselines based on sentiment analysis and topic-sentiment analysis. 1 Introduction In recent years, stance classification for online debates has received increasing research interest (Somasundaran and Wiebe, 2010; Anand et al., 2011; Walker et al., 2012; Ranade et al., 2013; Sridhar et al., 2014). Given a post belonging to a two-sided debate on an issue (e.g. abortion rights; see Table 1), the task is classify the post as for or against the issue. The argumentative nature of such posts makes stance classification difficult; for example, one • topic-stance features – a set of automatically extracted ‘topic terms’ (for abortion rights, these would include, for example, ‘fetus’, ‘baby’, ‘woman’ and ‘life’), where each topic term is associated with a distributional lexical model (DLM) that captures the writer’s stance towards that topic. • stanc"
W16-2807,P09-2079,0,0.0561167,"Missing"
W16-2807,W05-0308,0,0.0575128,"Missing"
W16-2807,W15-0503,0,0.0697928,"Missing"
W16-2816,W15-0514,0,0.0790909,"Missing"
W16-2816,P12-2041,0,0.0762942,"Missing"
W16-2816,de-marneffe-etal-2014-universal,0,0.0234305,"Missing"
W16-2816,fillmore-etal-2002-framenet,0,0.0398715,"Missing"
W16-2816,P04-1085,0,0.112847,"Missing"
W16-2816,W14-2106,0,0.0553047,"Missing"
W16-2816,C14-1141,0,0.0455328,"Missing"
W16-2816,N09-3013,0,0.0719873,"Missing"
W16-2816,H05-1005,1,0.787401,"Missing"
W16-2816,J11-4007,1,0.841697,"Missing"
W16-2816,J02-4002,0,0.284961,"Missing"
W16-2816,walker-etal-2012-corpus,0,0.0621424,"Missing"
W16-2816,W01-0100,0,\N,Missing
W16-6601,P04-1035,0,0.0296983,"s against a large collection of names (Ward, 1993): Scorelex (Sj ) = X 1 i ∈Sj }| |{wi |wiw ∈N / ames abilities for each class (Pos and Neg), calculated as: p(P os|w1..n ) = p(P os) p(N eg|w1..n ) = p(N eg) 2.3 Scorelex (Sj ) |{wi |wi ∈ Sj }| Sentiment score We implemented hybrid of a statistical and a rule based sentiment analysis component. Supervised sentiment classifier: The statistical component was implemented as a supervised Na¨ıve Bayes classifier with unigram, bigram and trigram features. We first experimented with training it on a large corpus of positive and negative movie reviews (Pang and Lee, 2004). We were however not satisfied with the quality of classifications for news stories. The key issue was the difference in vocabulary usage in the two genres; e.g. a word such as “terrifying” features prominently in positive movie reviews, but should no predict positive sentiment in a news story. For genre adaptation, a new dataset was created specifically for our purpose by taking a pre-existent dataset of 2,225 BBC articles assembled for topic classification (Greene and Cunningham, 2006). These articles were then manually labelled as positive, negative or neutral based just on the topic of th"
W16-6601,J12-1004,0,0.0165281,"nd in a single document summarisation context. The expert also made various spe8 cific observations about vocabulary, highlighting words and phrases such as ‘blaze’, ‘flash floods’, ‘arson’ and ‘aid agencies’ as examples that may be difficult for a child to understand, and approving of Newsround defining terms like ‘arson’ clearly within the text. The solution it would appear is to combine the purely extractive approach described in this paper with more abstractive approaches used in research on text simplification. This will be explored in future work. For instance, numerical simplification (Power and Williams, 2012; Bautista et al., 2011), accurate conversion of passive to active voice (Siddharthan, 2010), sentence shortening to preferentially remove difficult words (Angrosh et al., 2014), lexical simplification (De Belder and Moens, 2010; Yatskar et al., 2010), explanatory descriptions of named entities (Siddharthan et al., 2011), simplifying causality and discourse connectives (Siddharthan, 2003; Siddharthan and Katsos, 2010) and defining terminology (Elhadad, 2006) have all been demonstrated for text simplification systems. 5 Conclusions Our goal was to create an automatic news summarisation system c"
W16-6601,N10-1144,1,0.855493,"Missing"
W16-6601,J11-4007,1,0.823141,"on’ clearly within the text. The solution it would appear is to combine the purely extractive approach described in this paper with more abstractive approaches used in research on text simplification. This will be explored in future work. For instance, numerical simplification (Power and Williams, 2012; Bautista et al., 2011), accurate conversion of passive to active voice (Siddharthan, 2010), sentence shortening to preferentially remove difficult words (Angrosh et al., 2014), lexical simplification (De Belder and Moens, 2010; Yatskar et al., 2010), explanatory descriptions of named entities (Siddharthan et al., 2011), simplifying causality and discourse connectives (Siddharthan, 2003; Siddharthan and Katsos, 2010) and defining terminology (Elhadad, 2006) have all been demonstrated for text simplification systems. 5 Conclusions Our goal was to create an automatic news summarisation system capable of producing summaries suitable for children by combining scores for sentence informativeness, sentiment and difficulty. Our evaluation confirmed that our summariser outperforms a generic summariser focused only on informativeness in each of the aspects of informativeness, positivity and simplicity. Additionally,"
W16-6601,W03-2314,1,0.671441,"purely extractive approach described in this paper with more abstractive approaches used in research on text simplification. This will be explored in future work. For instance, numerical simplification (Power and Williams, 2012; Bautista et al., 2011), accurate conversion of passive to active voice (Siddharthan, 2010), sentence shortening to preferentially remove difficult words (Angrosh et al., 2014), lexical simplification (De Belder and Moens, 2010; Yatskar et al., 2010), explanatory descriptions of named entities (Siddharthan et al., 2011), simplifying causality and discourse connectives (Siddharthan, 2003; Siddharthan and Katsos, 2010) and defining terminology (Elhadad, 2006) have all been demonstrated for text simplification systems. 5 Conclusions Our goal was to create an automatic news summarisation system capable of producing summaries suitable for children by combining scores for sentence informativeness, sentiment and difficulty. Our evaluation confirmed that our summariser outperforms a generic summariser focused only on informativeness in each of the aspects of informativeness, positivity and simplicity. Additionally, an overwhelming majority of experimental participants rated the summ"
W16-6601,W10-4213,1,0.8394,"s about vocabulary, highlighting words and phrases such as ‘blaze’, ‘flash floods’, ‘arson’ and ‘aid agencies’ as examples that may be difficult for a child to understand, and approving of Newsround defining terms like ‘arson’ clearly within the text. The solution it would appear is to combine the purely extractive approach described in this paper with more abstractive approaches used in research on text simplification. This will be explored in future work. For instance, numerical simplification (Power and Williams, 2012; Bautista et al., 2011), accurate conversion of passive to active voice (Siddharthan, 2010), sentence shortening to preferentially remove difficult words (Angrosh et al., 2014), lexical simplification (De Belder and Moens, 2010; Yatskar et al., 2010), explanatory descriptions of named entities (Siddharthan et al., 2011), simplifying causality and discourse connectives (Siddharthan, 2003; Siddharthan and Katsos, 2010) and defining terminology (Elhadad, 2006) have all been demonstrated for text simplification systems. 5 Conclusions Our goal was to create an automatic news summarisation system capable of producing summaries suitable for children by combining scores for sentence informa"
W16-6601,N10-1056,0,0.0226438,"to understand, and approving of Newsround defining terms like ‘arson’ clearly within the text. The solution it would appear is to combine the purely extractive approach described in this paper with more abstractive approaches used in research on text simplification. This will be explored in future work. For instance, numerical simplification (Power and Williams, 2012; Bautista et al., 2011), accurate conversion of passive to active voice (Siddharthan, 2010), sentence shortening to preferentially remove difficult words (Angrosh et al., 2014), lexical simplification (De Belder and Moens, 2010; Yatskar et al., 2010), explanatory descriptions of named entities (Siddharthan et al., 2011), simplifying causality and discourse connectives (Siddharthan, 2003; Siddharthan and Katsos, 2010) and defining terminology (Elhadad, 2006) have all been demonstrated for text simplification systems. 5 Conclusions Our goal was to create an automatic news summarisation system capable of producing summaries suitable for children by combining scores for sentence informativeness, sentiment and difficulty. Our evaluation confirmed that our summariser outperforms a generic summariser focused only on informativeness in each of th"
W18-6548,W17-3535,1,0.890675,"Missing"
W18-6548,W17-3525,0,0.0277796,"n, 1996) of “Overview first, zoom and filter, then details-ondemand”. One of the main ideas presented there is that it is beneficial for a reader to be exposed to an overview of the information before diving into specific details of interest. There have been related NLG research about sets of objects, although with different goals or focuses. For example, to refer to or identify a set of objects within a larger set (Van Deemter, 2002), to perform a data-to-text analysis of tabularized data by records1 , to generate a page title for set items with shared characteristics from existing metadata (Mathur et al., 2017), or to address the issue of missing data encountered in summarisation (Inglis et al., 2017). In contrast, our work explores summaries that describe commonalities and differences within a set in order to help a user make informed decisions in selecting an object from the set. Our work focuses particularly on Content Determination step in the NLG pipe-line (Reiter and Dale, 2000), including selecting features and values to be presented. We explored the task of creating a textual summary describing a large set of objects characterised by a small number of features using an e-commerce dataset. Wh"
W18-6548,J02-1003,1,0.573446,"Missing"
