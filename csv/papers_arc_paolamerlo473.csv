2021.naacl-main.39,Multi-Adversarial Learning for Cross-Lingual Word Embeddings,2021,-1,-1,3,1,3346,haozhou wang,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings - maps of matching words across languages - without supervision. Despite these successes, GANs{'} performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs{'} incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages."
2020.iwpt-1.1,Syntactic Parsing in Humans and Machines,2020,-1,-1,1,1,3347,paola merlo,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"To process the syntactic structures of a language in ways that are compatible with human expectations, we need computational representations of lexical and syntactic properties that form the basis of human knowledge of words and sentences. Recent neural-network-based and distributed semantics techniques have developed systems of considerable practical success and impressive performance. As has been advocated by many, however, such systems still lack human-like properties. In particular, linguistic, psycholinguistic and neuroscientific investigations have shown that human processing of sentences is sensitive to structure and unbounded relations. In the spirit of better understanding the structure building and long-distance properties of neural networks, I will present an overview of recent results on agreement and island effects in syntax in several languages. While certain sets of results in the literature indicate that neural language models exhibit long-distance agreement abilities, other finer-grained investigation of how these effects are calculated indicates that that the similarity spaces they define do not correlate with human experimental results on intervention similarity in long-distance dependencies. This opens the way to reflections on how to better match the syntactic properties of natural languages in the representations of neural models."
2020.conll-1.30,Word associations and the distance properties of context-aware word embeddings,2020,-1,-1,2,0,20976,maria rodriguez,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"What do people know when they know the meaning of words? Word associations have been widely used to tap into lexical repre- sentations and their structure, as a way of probing semantic knowledge in humans. We investigate whether current word embedding spaces (contextualized and uncontextualized) can be considered good models of human lexi- cal knowledge by studying whether they have comparable characteristics to human associa- tion spaces. We study the three properties of association rank, asymmetry of similarity and triangle inequality. We find that word embeddings are good mod- els of some word associations properties. They replicate well human associations between words, and, like humans, their context-aware variants show violations of the triangle in- equality. While they do show asymmetry of similarities, their asymmetries do not map those of human association norms."
W19-7906,Intervention effects in object relatives in {E}nglish and {I}talian: a study in quantitative computational syntax,2019,-1,-1,2,0,23419,giuseppe samo,"Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)",0,None
W19-7801,"{S}yntax{F}est 2019 Invited talk - Quantitative Computational Syntax: dependencies, intervention effects and word embeddings",2019,0,0,1,1,3347,paola merlo,"Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)",0,None
W19-4817,Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in {F}rench and {E}nglish,2019,0,0,1,1,3347,paola merlo,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages. Results are at present inconclusive. In this paper, we extend previous work on long-distance dependencies in three ways. We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties under study. We extend the work to sentence embeddings and to new languages. We confirm previous negative results: word embeddings and sentence embeddings do not unequivocally encode fine-grained linguistic properties of long-distance dependencies."
K19-1011,Cross-Lingual Word Embeddings and the Structure of the Human Bilingual Lexicon,2019,0,0,1,1,3347,paola merlo,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Research on the bilingual lexicon has uncovered fascinating interactions between the lexicons of the native language and of the second language in bilingual speakers. In particular, it has been found that the lexicon of the underlying native language affects the organisation of the second language. In the spirit of interpreting current distributed representations, this paper investigates two models of cross-lingual word embeddings, comparing them to the shared-translation effect and the cross-lingual coactivation effects of false and true friends (cognates) found in humans. We find that the similarity structure of the cross-lingual word embeddings space yields the same effects as the human bilingual lexicon."
D19-1450,Weakly-Supervised Concept-based Adversarial Learning for Cross-lingual Word Embeddings,2019,37,0,3,1,3346,haozhou wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information not only in a single language but also across different languages. Current unsupervised adversarial approaches show that it is possible to build a mapping matrix that aligns two sets of monolingual word embeddings without high quality parallel data, such as a dictionary or a sentence-aligned corpus. However, without an additional step of refinement, the preliminary mapping learnt by these methods is unsatisfactory, leading to poor performance for typologically distant languages. In this paper, we propose a weakly-supervised adversarial training method to overcome this limitation, based on the intuition that mapping across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs."
K18-1038,Vectorial Semantic Spaces Do Not Encode Human Judgments of Intervention Similarity,2018,0,0,1,1,3347,paola merlo,Proceedings of the 22nd Conference on Computational Natural Language Learning,0,"Despite their practical success and impressive performances, neural-network-based and distributed semantics techniques have often been criticized as they remain fundamentally opaque and difficult to interpret. In a vein similar to recent pieces of work investigating the linguistic abilities of these representations, we study another core, defining property of language: the property of long-distance dependencies. Human languages exhibit the ability to interpret discontinuous elements distant from each other in the string as if they were adjacent. This ability is blocked if a similar, but extraneous, element intervenes between the discontinuous components. We present results that show, under exhaustive and precise conditions, that one kind of word embeddings and the similarity spaces they define do not encode the properties of intervention similarity in long-distance dependencies, and that therefore they fail to represent this core linguistic notion."
J18-2007,Festina Lente: A Farewell from the Editor,2018,4,0,1,1,3347,paola merlo,Computational Linguistics,0,None
K17-3024,{CLCL} (Geneva) {DINN} Parser: a Neural Network Dependency Parser Ten Years Later,2017,1,0,2,0,32761,christophe moor,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This paper describes the University of Geneva{'}s submission to the CoNLL 2017 shared task Multilingual Parsing from Raw Text to Universal Dependencies (listed as the CLCL (Geneva) entry). Our submitted parsing system is the grandchild of the first transition-based neural network dependency parser, which was the University of Geneva{'}s entry in the CoNLL 2007 multilingual dependency parsing shared task, with some improvements to speed and portability. These results provide a baseline for investigating how far we have come in the past ten years of work on neural network dependency parsing."
W16-4505,Modifications of Machine Translation Evaluation Metrics by Using Word Embeddings,2016,0,1,2,1,3346,haozhou wang,Proceedings of the Sixth Workshop on Hybrid Approaches to Translation ({H}y{T}ra6),0,"Traditional machine translation evaluation metrics such as BLEU and WER have been widely used, but these metrics have poor correlations with human judgements because they badly represent word similarity and impose strict identity matching. In this paper, we propose some modifications to the traditional measures based on word embeddings for these two metrics. The evaluation results show that our modifications significantly improve their correlation with human judgements."
Q16-1025,Multi-lingual Dependency Parsing Evaluation: a Large-scale Analysis of Word Order Properties using Artificial Data,2016,39,3,2,1,22877,kristina gulordava,Transactions of the Association for Computational Linguistics,0,"The growing work in multi-lingual parsing faces the challenge of fair comparative evaluation and performance analysis across languages and their treebanks. The difficulty lies in teasing apart the properties of treebanks, such as their size or average sentence length, from those of the annotation scheme, and from the linguistic properties of languages. We propose a method to evaluate the effects of word order of a language on dependency parsing performance, while controlling for confounding treebank properties. The method uses artificially-generated treebanks that are minimal permutations of actual treebanks with respect to two word order properties: word order variation and dependency lengths. Based on these artificial data on twelve languages, we show that longer dependencies and higher word order variability degrade parsing performance. Our method also extends to minimal pairs of individual sentences, leading to a finer-grained understanding of parsing errors."
J16-2007,{O}bituary: In Memoriam: Susan Armstrong,2016,0,0,2,0,2866,pierrette bouillon,Computational Linguistics,0,None
W15-2115,Diachronic Trends in Word Order Freedom and Dependency Length in Dependency-Annotated Corpora of {L}atin and {A}ncient {G}reek,2015,23,8,2,1,22877,kristina gulordava,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"One easily observable aspect of language variation is the order of words. In human and machine natural language processing, it is often claimed that parsing freeorder languages is more difficult than parsing fixed-order languages. In this study on Latin and Ancient Greek, two wellknown and well-documented free-order languages, we propose syntactic correlates of word order freedom. We apply our indicators to a collection of dependencyannotated texts of different time periods. On the one hand, we confirm a trend towards more fixed-order patterns in time. On the other hand, we show that a dependency-based measure of the flexibility of word order is correlated with the parsing performance on these languages."
W15-2125,Evaluation of Two-level Dependency Representations of Argument Structure in Long-Distance Dependencies,2015,20,0,1,1,3347,paola merlo,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"Full recovery of argument structure information for question answering or information extraction requires that parsers can analyse long-distance dependencies. Previous work on statistical dependency parsing has used post-processing or additional training data to tackle this complex problem. We evaluate an alternative approach to recovering long-distance dependencies. This approach uses a two-level parsing model to recover both grammatical dependencies, such as subject and object, and full argument structure. We show that this two-level approach is competitive, while also providing useful semantic role information."
P15-2078,Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases,2015,16,9,2,1,22877,kristina gulordava,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efficiency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies."
K15-1025,Structural and lexical factors in adjective placement in complex noun phrases across {R}omance languages,2015,18,3,2,1,22877,kristina gulordava,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"One of the most common features across all known languages is their variability in word order. We show that differences in the prenominal and postnominal placement of adjectives in the noun phrase across five main Romance languages is not only subject to heaviness effects, as previously reported, but also to subtler structural interactions among dependencies that are better explained as effects of the principle of dependency length minimisation. These effects are almost purely structural and show lexical conditioning only in highly frequent collocations."
W14-0706,Likelihood of External Causation in the Structure of Events,2014,30,0,2,1,17323,tanja samardvzic,Proceedings of the {EACL} 2014 Workshop on Computational Approaches to Causality in Language ({CA}to{CL}),0,"This article addresses the causal structure of events described by verbs: whether an event happens spontaneously or it is caused by an external causer. We automatically estimate the likelihood of external causation of events based on the distribution of causative and anticausative uses of verbs in the causative alternation. We train a Bayesian model and test it on a monolingual and on a bilingual input. The performance is evaluated against an independent scale of likelihood of external causation based on typological data. The accuracy of a two-way classification is 85% in both monolingual and bilingual setting. On the task of a three-way classification, the score is 61% in the monolingual setting and 69% in the bilingual setting."
J13-4006,Multilingual Joint Parsing of Syntactic and Semantic Dependencies with a Latent Variable Model,2013,115,41,2,0,856,james henderson,Computational Linguistics,0,"Current investigations in data-driven models of parsing have shifted from purely syntactic analysis to richer semantic representations, showing that the successful recovery of the meaning of text requires structured analyses of both its grammar and its semantics. In this article, we report on a joint generative history-based model to predict the most likely derivation of a dependency parser for both syntactic and semantic dependencies, in multiple languages. Because these two dependency structures are not isomorphic, we propose a weak synchronization at the level of meaningful subsequences of the two derivations. These synchronized subsequences encompass decisions about the left side of each individual word. We also propose novel derivations for semantic dependency structures, which are appropriate for the relatively unconstrained nature of these graphs. To train a joint model of these synchronized derivations, we make use of a latent variable model of parsing, the Incremental Sigmoid Belief Network ISBN architecture. This architecture induces latent feature representations of the derivations, which are used to discover correlations both within and between the two derivations, providing the first application of ISBNs to a multi-task learning problem. This joint model achieves competitive performance on both syntactic and semantic dependency parsing for several languages. Because of the general nature of the approach, this extension of the ISBN architecture to weakly synchronized syntactic-semantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned."
P11-2052,Scaling up Automatic Cross-Lingual Semantic Role Annotation,2011,26,43,2,0.9965,12096,lonneke plas,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Broad-coverage semantic annotations for training statistical learners are only available for a handful of languages. Previous approaches to cross-lingual transfer of semantic annotations have addressed this problem with encouraging results on a small scale. In this paper, we scale up previous efforts by using an automatic approach to semantic annotation that does not rely on a semantic ontology for the target language. Moreover, we improve the quality of the transferred semantic annotations by using a joint syntactic-semantic parser that learns the correlations between syntax and semantics of the target language and smooths out the errors from automatic transfer. We reach a labelled F-measure for predicates and arguments of only 4% and 9% points, respectively, lower than the upper bound from manual annotations."
W10-2108,Cross-Lingual Variation of Light Verb Constructions: Using Parallel Corpora and Automatic Alignment for Linguistic Research,2010,10,9,2,1,17323,tanja samardvzic,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"Cross-lingual parallelism and small-scale language variation have recently become subject of research in both computational and theoretical linguistics. In this article, we use a parallel corpus and an automatic aligner to study English light verb constructions and their German translations. We show that parallel corpus data can provide new empirical evidence for better understanding the properties of light verbs. We also study the influence that the identified properties of light verb constructions have on the quality of their automatic alignment in a parallel corpus. We show that, even though characterised by limited compositionality, these constructions can be aligned better than fully compositional phrases, due to an interaction between the type of light verb construction and its frequency."
W10-1814,Cross-Lingual Validity of {P}rop{B}ank in the Manual Annotation of {F}rench,2010,14,22,3,0.9965,12096,lonneke plas,Proceedings of the Fourth Linguistic Annotation Workshop,0,Methods that re-use existing mono-lingual semantic annotation resources to annotate a new language rely on the hypothesis that the semantic annotation scheme used is cross-lingually valid. We test this hypothesis in an annotation agreement study. We show that the annotation scheme can be applied cross-lingually.
W09-1205,A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for Multiple Languages,2009,14,37,3,0,40042,andrea gesmundo,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"Motivated by the large number of languages (seven) and the short development time (two months) of the 2009 CoNLL shared task, we exploited latent variables to avoid the costly process of hand-crafted feature engineering, allowing the latent variables to induce features from the data. We took a pre-existing generative latent variable model of joint syntactic-semantic dependency parsing, developed for English, and applied it to six new languages with minimal adjustments. The parser's robustness across languages indicates that this parser has a very general feature set. The parser's high performance indicates that its latent variables succeeded in inducing effective features. This system was ranked third overall with a macro averaged F1 score of 82.14%, only 0.5% worse than the best system."
P09-1033,"Abstraction and Generalisation in Semantic Role Labels: {P}rop{B}ank, {V}erb{N}et or both?",2009,19,28,1,1,3347,paola merlo,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Semantic role labels are the representation of the grammatically relevant aspects of a sentence meaning. Capturing the nature and the number of semantic roles in a sentence is therefore fundamental to correctly describing the interface between grammar and meaning. In this paper, we compare two annotation schemes, Prop-Bank and VerbNet, in a task-independent, general way, analysing how well they fare in capturing the linguistic generalisations that are known to hold for semantic role labels, and consequently how well they grammaticalise aspects of meaning. We show that VerbNet is more verb-specific and better able to generalise to new semantic role instances, while PropBank better captures some of the structural constraints among roles. We conclude that these two resources should be used together, as they are complementary."
N09-2032,Domain Adaptation with Artificial Data for Semantic Parsing of Speech,2009,13,8,3,0.846576,12096,lonneke plas,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,We adapt a semantic role parser to the domain of goal-directed speech by creating an artificial treebank from an existing text tree-bank. We use a three-component model that includes distributional models from both target and source domains. We show that we improve the parser's performance on utterances collected from human-machine dialogues by training on the artificially created data without loss of performance on the text treebank.
W08-2101,Semantic Parsing for High-Precision Semantic Role Labelling,2008,25,23,1,1,3347,paola merlo,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"In this paper, we report experiments that explore learning of syntactic and semantic representations. First, we extend a state-of-the-art statistical parser to produce a richly annotated tree that identifies and labels nodes with semantic role labels as well as syntactic labels. Secondly, we explore rule-based and learning techniques to extract predicate-argument structures from this enriched output. The learning method is competitive with previous single-system proposals for semantic role labelling, yields the best reported precision, and produces a rich output. In combination with other high recall systems it yields an F-measure of 81%."
W08-2122,A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies,2008,10,45,2,0,856,james henderson,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. The submitted model yields 79.1% macro-average F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1. A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1."
P08-2054,Unlexicalised Hidden Variable Models of Split Dependency Grammars,2008,13,10,2,1,41635,gabriele musillo,"Proceedings of ACL-08: HLT, Short Papers",0,"This paper investigates transforms of split dependency grammars into unlexicalised context-free grammars annotated with hidden symbols. Our best unlexicalised grammar achieves an accuracy of 88% on the Penn Treebank data set, that represents a 50% reduction in error over previously published results on unlexicalised dependency parsing."
W06-2303,Robust Parsing of the {P}roposition {B}ank,2006,19,14,2,1,41635,gabriele musillo,Proceedings of the Workshop on {ROMAND} 2006:Robust Methods in Analysis of Natural language Data,0,We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output. We show conclusive results on joint learning and inference of syntactic and semantic representations.
N06-2026,Accurate Parsing of the {P}roposition {B}ank,2006,19,14,2,1,41635,gabriele musillo,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output. We show conclusive results on joint learning and inference of syntactic and semantic representations.
J06-3002,The Notion of Argument in Prepositional Phrase Attachment,2006,43,45,1,1,3347,paola merlo,Computational Linguistics,0,"In this article we refine the formulation of the problem of prepositional phrase (PP) attachment as a four-way disambiguation problem. We argue that, in interpreting PPs, both knowledge about the site of the attachment (the traditional noun-verb attachment distinction) and the nature of the attachment (the distinction of arguments from adjuncts) are needed. We introduce a method to learn arguments and adjuncts based on a definition of arguments as a vector of features. In a series of supervised classification experiments, first we explore the features that enable us to learn the distinction between arguments and adjuncts. We find that both linguistic diagnostics of argumenthood and lexical semantic classes are useful. Second, we investigate the best method to reach the four-way classification of potentially ambiguous prepositional phrases. We find that whereas it is overall better to solve the problem as a single four-way classification task, verb arguments are sometimes more precisely identified if the classification is done as a two-step process, first choosing the attachment site and then labeling it as argument or adjunct."
W05-1509,Lexical and Structural Biases for Function Parsing,2005,28,10,2,1,41635,gabriele musillo,Proceedings of the Ninth International Workshop on Parsing Technology,0,"In this paper, we explore two extensions to an existing statistical parsing model to produce richer parse trees, annotated with function labels. We achieve significant improvements in parsing by modelling directly the specific nature of function labels, as both expressions of the lexical semantics properties of a constituent and as syntactic elements whose distribution is subject to structural locality constraints. We also reach state-of-the-art accuracy on function labelling. Our results suggest that current statistical parsing methods are sufficiently robust to produce accurate shallow functional or semantic annotation, if appropriately biased."
H05-1078,Accurate Function Parsing,2005,19,17,1,1,3347,paola merlo,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we extend an existing parser to produce richer output annotated with function labels. We obtain state-of-the-art results both in function labelling and in parsing, by automatically relabelling the Penn Treebank trees. In particular, we obtain the best published results on semantic function labels. This suggests that current statistical parsing methods are sufficiently general to produce accurate shallow semantic annotation."
E03-1079,Generalised {PP}-attachment Disambiguation Using Corpus-based Linguistic Diagnostics,2003,19,9,1,1,3347,paola merlo,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose a new formulation of the PP attachment problem as a 4-way classification which takes into account the argument or adjunct status of the PP. Based on linguistic diagnostics, we train a 4-way classifier that reaches an average accuracy of 73.9% (baseline 66.2%). Compared to a sequence of binary classifiers, the 4-way classifier reaches better performance and individuates a verb's arguments more accurately, thus improving the acquisition of a crucial piece of information for many NLP applications."
P02-1027,A Multilingual Paradigm for Automatic Verb Classification,2002,21,37,1,1,3347,paola merlo,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We demonstrate the benefits of a multilingual approach to automatic lexical semantic verb classification based on statistical analysis of corpora in multiple languages. Our research incorporates two interrelated threads. In one, we exploit the similarities in the crosslinguistic classification of verbs, to extend work on English verb classification to a new language (Italian), and to new classes within that language, achieving an accuracy of 86.4% (baseline 33.9%). Our second strand of research exploits the differences across languages in the syntactic expression of semantic properties, to show that complementary information about English verbs can be extracted from their translations in a second language (Chinese). The use of multilingual features improves classification performance of the English verbs, achieving an accuracy of 83.5% (baseline 33.3%)."
C02-1091,Using Syntactic Analysis to Increase Efficiency in Visualizing Text Collections,2002,11,7,2,0,856,james henderson,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Self-Organizing Maps (SOMs) are a good method to cluster and visualize large collections of documents, but they are computationally expensive. In this paper, we investigate linguistically motivated reductions on the usual bag-of-words representation, to improve efficiency. We find that reducing the document representation to heads of verb and noun phrases reduces the heavy computational cost without degrading the quality of the map, especially in combination with term reduction techniques. More severe reductions which focus on subject and object nominal phrases are not advantageous."
C02-1146,Crosslinguistic Transfer in Automatic Verb Classification,2002,17,8,3,0,37093,vivian tsang,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We investigate the use of multilingual data in the automatic classification of English verbs, and show that there is a useful transfer of information across languages. Specifically, we experiment with three lexical semantic classes of English verbs. We collect statistical features over a sample of English verbs from each of the classes, as well as over Chinese translations of those verbs. We use the English and Chinese data, alone and in combination, as training data for a machine learning algorithm whose output is an automatic verb classifier. We demonstrate that Chinese data is indeed useful in helping to classify the English verbs (at 82% accuracy), and furthermore that a multilingual combination of data outperforms the English data alone (85% accuracy). Moreover, our results using monolingual corpora show that it is not necessary to use a parallel corpus to extract the translations in order for this technique to be successful."
W01-0715,Automatic distinction of arguments and modifiers: the case of prepositional phrases,2001,12,17,1,1,3347,paola merlo,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"The automatic distinction of arguments and modifiers is a necessary step for the automatic acquisition of subcategorisation frames and argument structure. In this work, we report on supervised learning experiments to learn this distinction for the difficult case of prepositional phrases attached to the verb. We develop statistical indicators of linguistic diagnostics for argumenthood, and we approximate them with counts extracted from an annotated corpus. We reach an accuracy of 86.5%, over a baseline of 74%, showing that this novel method is promising in solving this difficult problem."
J01-3003,Automatic Verb Classification Based on Statistical Distributions of Argument Structure,2001,48,170,1,1,3347,paola merlo,Computational Linguistics,0,"Automatic acquisition of lexical knowledge is critical to a wide range of natural language processing tasks. Especially important is knowledge about verbs, which are the primary source of relational information in a sentence---the predicate-argument structure that relates an action or state to its participants (i.e., who did what to whom). In this work, we report on supervised learning experiments to automatically classify three major types of English verbs, based on their argument structure--specifically, the thematic roles they assign to participants. We use linguistically-motivated statistical indicators extracted from large annotated corpora to train the classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based upper bound we calculate at 86.5%. A detailed analysis of the performance of the algorithm and of its errors confirms that the proposed features capture properties related to the argument structure of the verbs. Our results validate our hypotheses that knowledge about thematic relations is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. We thus demonstrate an effective combination of deeper linguistic knowledge with the robustness and scalability of statistical techniques."
merlo-stevenson-2000-establishing,Establishing the Upper Bound and Inter-judge Agreement of a Verb Classification Task,2000,21,2,1,1,3347,paola merlo,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Detailed knowledge about verbs is critical in many NLP and IR tasks, yet manual determination of such knowledge for large numbers of verbs is difficult, time-consuming and resource intensive. Recent responses to this problem have attempted to classify verbs automatically, as a first step to automatically build lexical resources. In order to estimate the upper bound of a verb classification task, which appears to be difficult and subject to variability among experts, we investigated the performance of human experts in controlled classification experiments. We report here the results of two experimentsxe2x80x94using a forced-choice task and a non-forced choice taskxe2x80x94which measure human expert accuracy (compared to a gold standard) in classifying verbs into three pre-defined classes, as well as inter-expert agreement. To preview, we find that the highest expert accuracy is 86.5% agreement with the gold standard, and that inter-expert agreement is not very high (K between .53 and .66). The two experiments show comparable results."
C00-2118,Automatic Lexical Acquisition Based on Statistical Distributions,2000,14,10,2,1,8359,suzanne stevenson,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We automatically classify verbs into lexical semantic classes, based on distributions of indicators of verb alternations, extracted from a very large annotated corpus. We address a problem which is particularly difficult because the verb classes, although semantically different, show similar surface syntactic behavior. Five grammatical features are sufficient to reduce error rate by more than 50% over chance: we achieve almost 70% accuracy in a task whose baseline performance is 34%, and whose expert-based upper bound we calculated at 86.5%. We conclude that corpus-driven extraction of grammatical features is a promising methodology for find-grained verb classification."
W99-0503,Supervised Learning of Lexical Semantic Verb Classes Using Frequency Distributions,1999,0,15,2,1,8359,suzanne stevenson,{SIGLEX}99: Standardizing Lexical Resources,0,"Vve zeport a number of computatmnal experiments m supervised learning whose goal Is to automatmally classify a set of verbs into lexmal semanUc classes, based on frequency dls tnbutmn approxlmatmns of grammatical features extracted from a very large annotated corpus DlstnbuUons of five syntactic features that approximate tranmUvlty alternatmns and thematic role assignments are sufficient to reduce error rate by 56% over chance We conclude that corpus da ta is a usable repository of verb class mformatmn, and that corpusdriven extraction of grammaUcal features Is a promising methodology for automatm lexmal acqum,Uon"
E99-1007,Automatic Verb Classification Using Distributions of Grammatical Features,1999,20,32,2,1,8359,suzanne stevenson,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes, based on distributional approximations of diatheses, extracted from a very large annotated corpus. Distributions of four grammatical features are sufficient to reduce error rate by 50% over chance. We conclude that corpus data is a usable repository of verb class information, and that corpus-driven extraction of grammatical features is a promising methodology for automatic lexical acquisition."
W98-1116,What grammars tell us about corpora: the case of reduced relative clauses,1998,10,3,1,1,3347,paola merlo,Sixth Workshop on Very Large Corpora,0,None
W97-0317,Attaching Multiple Prepositional Phrases: Backed-off Estimation Generalized,1997,7,30,1,1,3347,paola merlo,Second Conference on Empirical Methods in Natural Language Processing,0,"There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments. To our knowledge, however, these investigations have only considered the problem of attaching the first PP, i.e., in a [V NP PP] configuration. In this paper, we consider one technique which has been successfully applied to this problem, backed-off estimation, and demonstrate how it can be extended to deal with the problem of multiple PP attachment. The multiple PP attachment introduces two related problems: sparser data (since multiple PPs are naturally rarer), and greater syntactic ambiguity (more attachment configurations which must be distinguished). We present and algorithm which solves this problem through re-use of the relatively rich data obtained from first PP training, in resolving subsequent PP attachments."
J95-4003,Modularity and Information Content Classes in Principle-Based Parsing,1995,52,6,1,1,3347,paola merlo,Computational Linguistics,0,"In recent years models of parsing that are isomorphic to a principle-based theory of grammar (most notably Government and Binding (GB) Theory) have been proposed (Berwick et al. 1991). These models are natural and direct implementations of the grammar, but they are not efficient, because GB is not a computationally modular theory. This paper investigates one problem related to the tension between building linguistically based parsers and building efficient ones. In particular, the issue of what is a linguistically motivated way of deriving a parser from principle-based theories of grammar is explored. It is argued that an efficient and faithful parser can be built by taking advantage of the way in which principles are stated. To support this claim, two features of an implemented parser are discussed. First, configurations and lexical information are precompiled separately into two tables (an X; table and a table of lexical co-occurrence) which gives rise to more compact data structures. Secondly, precomputation of syntactic features (xcexb8-roles, case, etc.) results in efficient computation of chains, because it reduces several problems of chain formation to a local computation, thus avoiding extensive search of the tree for an antecedent or extensive backtracking. It is also shown that this method of building long-distance dependencies can be computed incrementally."
1993.iwpt-1.8,A Principle-based Parser for Foreign Language Training in {G}erman and {A}rabic,1993,-1,-1,3,0,56900,joe garman,Proceedings of the Third International Workshop on Parsing Technologies,0,"In this paper we discuss the design and implementation of a parser for German and Arabic, which is currently being used in a tutoring system for foreign language training. Computer-aided language tutoring is a good application for testing the robustness and flexibility of a parsing system, since the input is usually ungrammatical in some way. Efficiency is also a concern, as tutoring applications typically run on personal computers, with the parser sharing memory with other components of the system. Our system is principle-based, which ensures a compact representation, and improves portability, needed in order to extend the initial design from German to Arabic and (eventually) Spanish. Currently, the parser diagnoses agreement errors, case errors, selection errors, and some word order errors. The parser can handle simple and complex declaratives and questions, topicalisations, verb movement, relative clauses {---} broad enough coverage to be useful in the design of real exercises and dialogues."
P92-1040,An {LR} Category-Neutral Parser With Left Corner Prediction,1992,14,0,1,1,3347,paola merlo,30th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present a new parsing model of linguistic and computational interest. Linguistically, the relation between the parser and the theory of grammar adopted (Government and Binding (GB) theory as presented in Chomsky (1981, 1986a, b) is clearly specified. Computationally, this model adopts a mixed parsing procedure, by using left corner prediction in a modified LR parser."
