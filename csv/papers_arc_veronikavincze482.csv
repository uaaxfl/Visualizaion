2020.rdsm-1.6,Automatic Detection of {H}ungarian Clickbait and Entertaining Fake News,2020,-1,-1,1,1,15691,veronika vincze,Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM),0,"Online news do not always come from reliable sources and they are not always even realistic. The constantly growing number of online textual data has raised the need for detecting deception and bias in texts from different domains recently. In this paper, we identify different types of unrealistic news (clickbait and fake news written for entertainment purposes) written in Hungarian on the basis of a rich feature set and with the help of machine learning methods. Our tool achieves competitive scores: it is able to classify clickbait, fake news written for entertainment purposes and real news with an accuracy of over 80{\%}. It is also highlighted that morphological features perform the best in this classification task."
2020.lrec-1.290,{P}{\\'a}rt{\\'e}let: A {H}ungarian Corpus of Propaganda Texts from the {H}ungarian Socialist Era,2020,-1,-1,2,0,17249,zoltan kmetty,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we present P{\'a}rt{\'e}let, a digitized Hungarian corpus of Communist propaganda texts. P{\'a}rt{\'e}let was the official journal of the governing party during the Hungarian socialism from 1956 to 1989, hence it represents the direct political agitation and propaganda of the dictatorial system in question. The paper has a dual purpose: first, to present a general review of the corpus compilation process and the basic statistical data of the corpus, and second, to demonstrate through two case studies what the dataset can be used for. We show that our corpus provides a unique opportunity for conducting research on Hungarian propaganda discourse, as well as analyzing changes of this discourse over a 35-year period of time with computer-assisted methods."
2020.iwclul-1.7,ap{PIL}cation: an Android-based Tool for Learning {M}ansi,2020,-1,-1,3,0,18926,gabor bobaly,Proceedings of the Sixth International Workshop on Computational Linguistics of Uralic Languages,0,None
W18-4925,Edition 1.1 of the {PARSEME} Shared Task on Automatic Identification of Verbal Multiword Expressions,2018,0,2,4,0.213919,12002,carlos ramisch,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"This paper describes the PARSEME Shared Task 1.1 on automatic identification of verbal multiword expressions. We present the annotation methodology, focusing on changes from last year{'}s shared task. Novel aspects include enhanced annotation guidelines, additional annotated data for most languages, corpora for some new languages, and new evaluation settings. Corpora were created for 20 languages, which are also briefly discussed. We report organizational principles behind the shared task and the evaluation metrics employed for ranking. The 17 participating systems, their methods and obtained results are also presented and analysed."
L18-1061,{S}zeged{K}oref: A {H}ungarian Coreference Corpus,2018,0,0,1,1,15691,veronika vincze,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1208,{E}-magyar {--} A Digital Language Processing System,2018,0,0,8,0,17457,tamas varadi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-6527,{H}ungarian Copula Constructions in Dependency Syntax and Parsing,2017,6,1,2,1,31421,katalin simko,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
W17-1704,The {PARSEME} Shared Task on Automatic Identification of Verbal Multiword Expressions,2017,5,8,5,0.407925,16493,agata savary,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"Multiword expressions (MWEs) are known as a {``}pain in the neck{''} for NLP due to their idiosyncratic behaviour. While some categories of MWEs have been addressed by many studies, verbal MWEs (VMWEs), such as to take a decision, to break one{'}s heart or to turn off, have been rarely modelled. This is notably due to their syntactic variability, which hinders treating them as {``}words with spaces{''}. We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs. It is a joint effort, carried out within a European research network, to elaborate universal terminologies and annotation guidelines for 18 languages. Its main outcome is a multilingual 5-million-word annotated corpus which underlies a shared task on automatic identification of VMWEs. This paper presents the corpus annotation methodology and outcome, the shared task organisation and the results of the participating systems."
W17-1705,{US}zeged: Identifying Verbal Multiword Expressions with {POS} Tagging and Parsing Techniques,2017,6,2,3,1,31421,katalin simko,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,The paper describes our system submitted for the Workshop on Multiword Expressions{'} shared task on automatic identification of verbal multiword expressions. It uses POS tagging and dependency parsing to identify single- and multi-token verbal MWEs in text. Our system is language independent and competed on nine of the eighteen languages. Our paper describes how our system works and gives its error analysis for the languages it was submitted for.
W17-1721,Verb-Particle Constructions in Questions,2017,8,0,1,1,15691,veronika vincze,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),0,"In this paper, we investigate the behavior of verb-particle constructions in English questions. We present a small dataset that contains questions and verb-particle construction candidates. We demonstrate that there are significant differences in the distribution of WH-words, verbs and prepositions/particles in sentences that contain VPCs and sentences that contain only verb + prepositional phrase combinations both by statistical means and in machine learning experiments. Hence, VPCs and non-VPCs can be effectively separated from each other by using a rich feature set, containing several novel features."
W17-0606,Language technology resources and tools for {M}ansi: an overview,2017,-1,-1,3,1,18927,csilla horvath,Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages,0,None
E17-1034,{U}niversal {D}ependencies and Morphology for {H}ungarian - and on the Price of Universality,2017,14,0,1,1,15691,veronika vincze,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"In this paper, we present how the principles of universal dependencies and morphology have been adapted to Hungarian. We report the most challenging grammatical phenomena and our solutions to those. On the basis of the adapted guidelines, we have converted and manually corrected 1,800 sentences from the Szeged Treebank to universal dependency format. We also introduce experiments on this manually annotated corpus for evaluating automatic conversion and the added value of language-specific, i.e. non-universal, annotations. Our results reveal that converting to universal dependencies is not necessarily trivial, moreover, using language-specific morphological features may have an impact on overall performance."
W16-5002,Detecting Uncertainty Cues in {H}ungarian Social Media Texts,2016,-1,-1,1,1,15691,veronika vincze,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics ({E}x{P}ro{M}),0,"In this paper, we aim at identifying uncertainty cues in Hungarian social media texts. We present our machine learning based uncertainty detector which is based on a rich features set including lexical, morphological, syntactic, semantic and discourse-based features, and we evaluate our system on a small set of manually annotated social media texts. We also carry out cross-domain and domain adaptation experiments using an annotated corpus of standard Hungarian texts and show that domain differences significantly affect machine learning. Furthermore, we argue that differences among uncertainty cue types may also affect the efficiency of uncertainty detection."
W16-2115,{U}niversal {M}orphology for {O}ld {H}ungarian,2016,12,1,2,0.735184,24311,eszter simon,"Proceedings of the 10th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"This paper provides a description of the automatic conversion of the morphologically annotated part of the Old Hungarian Corpus. These texts are in the format of the Humor analyzer, which does not follow any international standards. Since standardization always facilitates future research, even for researchers who do not know the Old Hungarian language, we opted for mapping the Humor formalism to a widely used universal tagset, namely the Universal Dependencies framework. The benefits of using a shared tagset across languages enable interlingual comparisons from a theoretical point of view and also multilingual NLP applications can profit from a unified annotation scheme. In this paper, we report the adaptation of the Universal Dependencies morphological annotation scheme to Old Hungarian, and we discuss the most important theoretical linguistic issues that had to be resolved during the process. We focus on the linguistic phenomena typical of Old Hungarian that required special treatment and we offer solutions to them."
P16-2030,Detecting Mild Cognitive Impairment by Exploiting Linguistic Information from Transcripts,2016,21,2,1,1,15691,veronika vincze,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,None
L16-1459,A {H}ungarian Sentiment Corpus Manually Annotated at Aspect Level,2016,14,3,2,1,15692,martina szabo,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we present a Hungarian sentiment corpus manually annotated at aspect level. Our corpus consists of Hungarian opinion texts written about different types of products. The main aim of creating the corpus was to produce an appropriate database providing possibilities for developing text mining software tools. The corpus is a unique Hungarian database: to the best of our knowledge, no digitized Hungarian sentiment corpus that is annotated on the level of fragments and targets has been made so far. In addition, many language elements of the corpus, relevant from the point of view of sentiment analysis, got distinct types of tags in the annotation. In this paper, on the one hand, we present the method of annotation, and we discuss the difficulties concerning text annotation process. On the other hand, we provide some quantitative and qualitative data on the corpus. We conclude with a description of the applicability of the corpus."
2016.gwc-1.20,Where Bears Have the Eyes of Currant: Towards a {M}ansi {W}ord{N}et,2016,-1,-1,4,1,18927,csilla horvath,Proceedings of the 8th Global WordNet Conference (GWC),0,"Here we report the construction of a wordnet for Mansi, an endangered minority language spoken in Russia. We will pay special attention to challenges that we encountered during the building process, among which the most important ones are the low number of native speakers, the lack of thesauri and the bear language. We will discuss our solutions to these issues, which might have some theoretical implications for the methodology of wordnet building in general."
W14-4909,Annotating Uncertainty in {H}ungarian Webtext,2014,25,1,1,1,15691,veronika vincze,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"Uncertainty detection has been a popular topic in natural language processing, which manifested in the creation of several corpora for English. Here we show how the annotation guidelines originally developed for English standard texts can be adapted to Hungarian webtext. We annotated a small corpus of Facebook posts for uncertainty phenomena and we illustrate the main characteristics of such texts, with special regard to uncertainty annotation. Our results may be exploited in adapting the guidelines to other languages or domains and later on, in the construction of automatic uncertainty detectors."
W14-0803,{VPCT}agger: Detecting Verb-Particle Constructions With Syntax-Based Methods,2014,18,6,2,1,38803,istvan nagy,Proceedings of the 10th Workshop on Multiword Expressions ({MWE}),0,"Verb-particle combinations (VPCs) con- sist of a verbal and a preposition/particle component, which often have some addi- tional meaning compared to the meaning of their parts. If a data-driven morpholog- ical parser or a syntactic parser is trained on a dataset annotated with extra informa- tion for VPCs, they will be able to iden- tify VPCs in raw texts. In this paper, we examine how syntactic parsers perform on this task and we introduce VPCTag- ger, a machine learning-based tool that is able to identify English VPCs in context. Our method consists of two steps: it first selects VPC candidates on the basis of syntactic information and then selects gen- uine VPCs among them by exploiting new features like semantic and contextual ones. Based on our results, we see that VPC- Tagger outperforms state-of-the-art meth- ods in the VPC detection task."
W14-0116,Non-Lexicalized Concepts in Wordnets: A Case Study of {E}nglish and {H}ungarian,2014,15,2,1,1,15691,veronika vincze,Proceedings of the Seventh Global {W}ordnet Conference,0,"Here, we investigate non-lexicalized synsets found in the Hungarian wordnet, and compare them to the English one, in the context of wordnet building principles. We propose some strategies that may be used to overcome difficulties concerning non-lexicalized synsets in wordnets constructed using the expand method. It is shown that the merge model could also have been applied to Hungarian, and with the help of the above-mentioned strategies, a wordnet based on the expand model can be transformed into a wordnet similar to that constructed with the merge model."
vincze-etal-2014-szeged,{S}zeged Corpus 2.5: Morphological Modifications in a Manually {POS}-tagged {H}ungarian Corpus,2014,4,0,1,1,15691,veronika vincze,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Szeged Corpus is the largest manually annotated database containing the possible morphological analyses and lemmas for each word form. In this work, we present its latest version, Szeged Corpus 2.5, in which the new harmonized morphological coding system of Hungarian has been employed and, on the other hand, the majority of misspelled words have been corrected and tagged with the proper morphological code. New morphological codes are introduced for participles, causative / modal / frequentative verbs, adverbial pronouns and punctuation marks, moreover, the distinction between common and proper nouns is eliminated. We also report some statistical data on the frequency of the new morphological codes. The new version of the corpus made it possible to train magyarlanc, a data-driven POS-tagger of Hungarian on a dataset with the new harmonized codes. According to the results, magyarlanc is able to achieve a state-of-the-art accuracy score on the 2.5 version as well."
racz-etal-2014-4fx,4{FX}: Light Verb Constructions in a Multilingual Parallel Corpus,2014,11,3,3,0,39558,anita racz,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe 4FX, a quadrilingual (English-Spanish-German-Hungarian) parallel corpus annotated for light verb constructions. We present the annotation process, and report statistical data on the frequency of LVCs in each language. We also offer inter-annotator agreement rates and we highlight some interesting facts and tendencies on the basis of comparing multilingual data from the four corpora. According to the frequency of LVC categories and the calculated KendallÂs coefficient for the four corpora, we found that Spanish and German are very similar to each other, Hungarian is also similar to both, but German differs from all these three. The qualitative and quantitative data analysis might prove useful in theoretical linguistic research for all the four languages. Moreover, the corpus will be an excellent testbed for the development and evaluation of machine learning based methods aiming at extracting or identifying light verb constructions in these four languages."
vincze-etal-2014-automatic,Automatic Error Detection concerning the Definite and Indefinite Conjugation in the {H}un{L}earner Corpus,2014,4,1,1,1,15691,veronika vincze,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present the results of automatic error detection, concerning the definite and indefinite conjugation in the extended version of the HunLearner corpus, the learnersÂ corpus of the Hungarian language. We present the most typical structures that trigger definite or indefinite conjugation in Hungarian and we also discuss the most frequent types of errors made by language learners in the corpus texts. We also illustrate the error types with sentences taken from the corpus. Our results highlight grammatical structures that might pose problems for learners of Hungarian, which can be fruitfully applied in the teaching and practicing of such constructions from the language teacherÂs or learnersÂ point of view. On the other hand, these results may be exploited in extending the functionalities of a grammar checker, concerning the definiteness of the verb. Our automatic system was able to achieve perfect recall, i.e. it could find all the mismatches between the type of the object and the conjugation of the verb, which is promising for future studies in this area."
C14-1132,An Empirical Evaluation of Automatic Conversion from Constituency to Dependency in {H}ungarian,2014,12,0,2,1,31421,katalin simko,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we investigate the differences between Hungarian sentence parses based on automatically converted and manually annotated dependency trees. We also train constituency parsers on the manually annotated constituency treebank and then convert their output to dependency trees. We argue for the importance of training on gold standard corpora, and we also demonstrate that although the results obtained by training on the constituency treebank and converting the output to dependency format and those obtained by training on the automatically converted dependency treebank are similar in terms of accuracy scores, the typical errors made by different systems differ from each other."
C14-1174,Uncertainty Detection in {H}ungarian Texts,2014,27,10,1,1,15691,veronika vincze,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Uncertainty detection is essential for many NLP applications. For instance, in information retrieval, it is of primary importance to distinguish among factual, negated and uncertain information. Current research on uncertainty detection has mostly focused on the English language, in contrast, here we present the first machine learning algorithm that aims at identifying linguistic markers of uncertainty in Hungarian texts from two domains: Wikipedia and news media. The system is based on sequence labeling and makes use of a rich feature set including orthographic, lexical, morphological, syntactic and semantic features as well. Having access to annotated data from two domains, we also focus on the domain specificities of uncertainty detection by comparing results obtained in indomain and cross-domain settings. Our results show that the domain of the text has significant influence on uncertainty detection."
W13-4917,Overview of the {SPMRL} 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages,2013,110,38,20,0,167,djame seddah,Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"This paper reports on the first shared task on statistical parsing of morphologically rich languages (MRLs). The task features data sets from nine languages, each available both in constituency and dependency annotation. We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types. We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios."
W13-3608,{LFG}-based Features for Noun Number and Article Grammatical Errors,2013,10,6,2,0.952381,1666,gabor berend,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,We introduce here a participating system of the CoNLL-2013 Shared Task xe2x80x9cGrammatical Error Correctionxe2x80x9d. We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. We carried out feature engineering and we found that (among others) the f-structure of an LFG parser can provide very informative features for the machine learning system.
R13-1099,magyarlanc: A Tool for Morphological and Dependency Parsing of {H}ungarian,2013,16,33,2,0,39508,janos zsibrita,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Hungarian is the stereotype of morphologically rich and free word order languages. Here, we introduce magyarlanc, a natural language toolkit developed for the linguistic preprocessing xe2x80x90 segmentation, morphological analysis, POS-tagging and dependency parsing xe2x80x90 of Hungarian texts. We hope that the free availability of the toolkit fosters the research not just on the Hungarian language but on all the morphologically rich languages in general. The main novelties of the tool are the application of a new harmonized morphological coding system of Hungarian, the datadriven approach and the integration of a dependency parser. The system is implemented in JAVA, hence it can be used in a platform-independent way."
P13-2046,Identifying {E}nglish and {H}ungarian Light Verb Constructions: A Contrastive Approach,2013,27,6,1,1,15691,veronika vincze,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Here, we introduce a machine learningbased approach that allows us to identify light verb constructions (LVCs) in Hungarian and English free texts. We also present the results of our experiments on the SzegedParalellFX Englishxe2x80x90Hungarian parallel corpus where LVCs were manually annotated in both languages. With our approach, we were able to contrast the performance of our method and define language-specific features for these typologically different languages. Our presented method proved to be sufficiently robust as it achieved approximately the same scores on the two typologically different languages."
I13-1024,Dependency Parsing for Identifying {H}ungarian Light Verb Constructions,2013,26,17,1,1,15691,veronika vincze,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Light verb constructions (LVCs) are verb and noun combinations in which the verb has lost its meaning to some degree and the noun is used in one of its original senses. They often share their syntactic pattern with other constructions (e.g. verbobject pairs) thus LVC detection can be viewed as classifying certain syntactic patterns as light verb constructions or not. In this paper, we explore a novel way to detect LVCs in texts: we apply a dependency parser to carry out the task. We present our experiments on a Hungarian treebank, which has been manually annotated for dependency relations and light verb constructions. Our results outperformed those achieved by state-of-the-art techniques for Hungarian LVC detection, especially due to the high precision and the treatment of long-distance dependencies."
I13-1038,Full-coverage Identification of {E}nglish Light Verb Constructions,2013,22,4,2,1,38803,istvan nagy,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"The identification of light verb constructions (LVC) is an important task for several applications. Previous studies focused on some limited set of light verb constructions. Here, we address the full coverage of LVCs. We investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use of an extended and rich feature set to select LVCs among extracted candidates."
I13-1044,"Weasels, Hedges and Peacocks: Discourse-level Uncertainty in {W}ikipedia Articles",2013,28,11,1,1,15691,veronika vincze,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Uncertainty is an important linguistic phenomenon that is relevant in many areas of language processing. While earlier research mostly concentrated on the semantic aspects of uncertainty, here we focus on discourse- and pragmaticsrelated aspects of uncertainty. We present a classification of such linguistic phenomena and introduce a corpus of Wikipedia articles in which the presented types of discourse-level uncertainty xe2x80x90 weasel, hedge and peacock xe2x80x90 have been manually annotated. We also discuss some experimental results on discourse-level uncertainty detection."
W12-3715,How to Evaluate Opinionated Keyphrase Extraction?,2012,13,1,2,1,1666,gabor berend,Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,0,Evaluation often denotes a key issue in semantics- or subjectivity-related tasks. Here we discuss the difficulties of evaluating opinionated keyphrase extraction. We present our method to reduce the subjectivity of the task and to alleviate the evaluation process and we also compare the results of human and machine-based evaluation.
vincze-2012-light,Light Verb Constructions in the {S}zeged{P}aralell{FX} {E}nglish{--}{H}ungarian Parallel Corpus,2012,28,13,1,1,15691,veronika vincze,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we describe the first English-Hungarian parallel corpus annotated for light verb constructions, which contains 14,261 sentence alignment units. Annotation principles and statistical data on the corpus are also provided, and English and Hungarian data are contrasted. On the basis of corpus data, a database containing pairs of English-Hungarian light verb constructions has been created as well. The corpus and the database can contribute to the automatic detection of light verb constructions and it is also shown how they can enhance performance in several fields of NLP (e.g. parsing, information extraction/retrieval and machine translation)."
szabo-etal-2012-hunor,{H}un{O}r: A {H}ungarian{---}{R}ussian Parallel Corpus,2012,-1,-1,2,1,15692,martina szabo,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present HunOr, the first multi-domain HungarianâRussian parallel corpus. Some of the corpus texts have been manually aligned and split into sentences, besides, named entities also have been annotated while the other parts are automatically aligned at the sentence level and they are POS-tagged as well. The corpus contains texts from the domains literature, official language use and science, however, we would like to add texts from the news domain to the corpus. In the future, we are planning to carry out a syntactic annotation of the HunOr corpus, which will further enhance the usability of the corpus in various NLP fields such as transfer-based machine translation or cross lingual information retrieval."
J12-2004,Cross-Genre and Cross-Domain Detection of Semantic Uncertainty,2012,59,34,2,0.689655,20401,gyorgy szarvas,Computational Linguistics,0,"Uncertainty is an important linguistic phenomenon that is relevant in various Natural Language Processing applications, in diverse genres from medical to community generated, newswire or scientific discourse, and domains from science to humanities. The semantic uncertainty of a proposition can be identified in most cases by using a finite dictionary (i.e., lexical cues) and the key steps of uncertainty detection in an application include the steps of locating the (genre-and domain-specific) lexical cues, disambiguating them, and linking them with the units of interest for the particular application (e.g., identified events in information extraction). In this study, we focus on the genre and domain differences of the context-dependent semantic uncertainty cue recognition task.n n We introduce a unified subcategorization of semantic uncertainty as different domain applications can apply different uncertainty categories. Based on this categorization, we normalized the annotation of three corpora and present results with a state-of-the-art uncertainty cue recognition model for four fine-grained categories of semantic uncertainty.n n Our results reveal the domain and genre dependence of the problem; nevertheless, we also show that even a distant source domain data set can contribute to the recognition and disambiguation of uncertainty cues, efficiently reducing the annotation costs needed to cover a new domain. Thus, the unified subcategorization and domain adaptation for training the models offer an efficient solution for cross-domain and cross-genre semantic uncertainty recognition."
E12-1007,Dependency Parsing of {H}ungarian: Baseline Results and Challenges,2012,24,15,2,0.381638,29565,richard farkas,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Hungarian is a stereotype of morphologically rich and non-configurational languages. Here, we introduce results on dependency parsing of Hungarian that employ a 80K, multi-domain, fully manually annotated corpus, the Szeged Dependency Treebank. We show that the results achieved by state-of-the-art data-driven parsers on Hungarian and English (which is at the other end of the configurational-non-configurational spectrum) are quite similar to each other in terms of attachment scores. We reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages. This analysis highlights that addressing the language-specific phenomena is required for a further remarkable error reduction."
W11-0817,Detecting Noun Compounds and Light Verb Constructions: a Contrastive Study,2011,20,18,1,1,15691,veronika vincze,Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,0,"In this paper, we describe our methods to detect noun compounds and light verb constructions in running texts. For noun compounds, dictionary-based methods and POS-tagging seem to contribute most to the performance of the system whereas for light verb constructions, the combination of POS-tagging, syntactic information and restrictions on the nominal and verbal component yield the best result. However, focusing on deverbal nouns proves to be beneficial for both types of MWEs. The effect of syntax is negligible on noun compound detection whereas it is unambiguously helpful for identifying light verb constructions."
R11-2001,Domain-Dependent Detection of Light Verb Constructions,2011,18,1,4,1,38803,istvan nagy,Proceedings of the Second Student Research Workshop associated with {RANLP} 2011,0,"In this paper, we show how our methods developed for identifying light verb constructions can be adapted to different domains and different types of texts. We both experiment with rule-based methods and machine learning approaches. Our results indicate that existing solutions for detecting light verb constructions can be successfully applied to other domains as well and we conclude that even a little amount of annotated target data can notably contribute to performance if a bigger corpus from another domain is also exploited when training."
R11-2006,Inter-domain Opinion Phrase Extraction Based on Feature Augmentation,2011,18,1,4,1,1666,gabor berend,Proceedings of the Second Student Research Workshop associated with {RANLP} 2011,0,"In this paper, a system for the extraction of key argument phrases xe2x80x93 which make the opinion holder feel negative or positive towards a particular product xe2x80x93 from product reviews is introduced. Since the necessary amount of training examples from any arbitrary product type (target domain) is not always available, the possible usage of domain adaptation in the task of opinion phrase extraction is also examined. Experimental results show that models relying on training examples mainly from a different domain can still yield results that are comparable to that of the intra-domain settings."
R11-1023,Noun Compound and Named Entity Recognition and their Usability in Keyphrase Extraction,2011,16,3,3,1,38803,istvan nagy,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"We investigate how the automatic identification of noun compounds and named entities can contribute to keyphrase extraction and we also show how previously identified noun compounds affect named entity recognition and vice versa, how noun compound detection is supported by identified named entities. Our experiments demonstrate that already known noun compounds yield better performance in named entity recognition and already known named entities enhance noun compound detection. The integration of noun compound and named entity related features into a keyphrase extractor also proves to be more effective than the model not including them. Our results indicate that the above features tend to be beneficial in several NLP-related tasks."
R11-1040,Multiword Expressions and Named Entities in the Wiki50 Corpus,2011,14,35,1,1,15691,veronika vincze,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Multiword expressions (MWEs) and named entities (NEs) exhibit unique and idiosyncratic features, thus, they often pose a problem to NLP systems. In order to facilitate their identification we developed the first corpus of Wikipedia articles in which several types of multiword expressions and named entities are manually annotated at the same time. The corpus can be used for training or testing MWE-detectors or NER systems, which we illustrate with experiments and it also makes it possible to investigate the co-occurrences of different types of MWEs and NEs within the same domain."
R11-1089,Domain-Dependent Identification of Multiword Expressions,2011,16,8,2,1,38803,istvan nagy,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"The identification of different kinds of multiword expressions require different solutions, on the other hand, there might be domain-related differences in their frequency and typology. In this paper, we show how our methods developed for identifying noun compounds and light verb constructions can be adapted to different domains and different types of texts. Our results indicate that with little effort, existing solutions for detecting multiword expressions can be successfully applied to other domains as well."
W10-3105,Speculation and negation annotation in natural language texts: what the case of {B}io{S}cope might (not) reveal,2010,8,10,1,1,15691,veronika vincze,Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,0,"In information extraction, it is of key importance to distinguish between facts and uncertain or negated information. In other words, IE applications have to treat sentences / clauses containing uncertain or negated information differently from factual information that is why the development of hedge and negation detection systems has received much interest -- e.g. the objective of the CoNLL-2010 Shared Task was also to develop hedge detection systems (Farkas et al., 2010). For the training and evaluation of such systems, corpora annotated for negation and speculation are necessary."
W10-3001,The {C}o{NLL}-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text,2010,26,195,2,0.381638,29565,richard farkas,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results. The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task."
vincze-etal-2010-hungarian,{H}ungarian Dependency Treebank,2010,11,48,1,1,15691,veronika vincze,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Herein, we present the process of developing the first Hungarian Dependency TreeBank. First, short references are made to dependency grammars we considered important in the development of our Treebank. Second, mention is made of existing dependency corpora for other languages. Third, we present the steps of converting the Szeged Treebank into dependency-tree format: from the originally phrase-structured treebank, we produced dependency trees by automatic conversion, checked and corrected them thereby creating the first manually annotated dependency corpus for Hungarian. We also go into detail about the two major sets of problems, i.e. coordination and predicative nouns and adjectives. Fourth, we give statistics on the treebank: by now, we have completed the annotation of business news, newspaper articles, legal texts and texts in informatics, at the same time, we are planning to convert the entire corpus into dependency tree format. Finally, we give some hints on the applicability of the system: the present database may be utilized â among others â in information extraction and machine translation as well."
C10-1125,{H}ungarian Corpus of Light Verb Constructions,2010,21,12,1,1,15691,veronika vincze,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"The precise identification of light verb constructions is crucial for the successful functioning of several NLP applications. In order to facilitate the development of an algorithm that is capable of recognizing them, a manually annotated corpus of light verb constructions has been built for Hungarian. Basic annotation guidelines and statistical data on the corpus are also presented in the paper. It is also shown how applications in the fields of machine translation and information extraction can make use of such a corpus and an algorithm."
W08-0606,"The {B}io{S}cope corpus: annotation for negation, uncertainty and their scope in biomedical texts",2008,12,248,2,0,20401,gyorgy szarvas,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"Backgroundn Detecting uncertain and negative assertions is essential in most BioMedical Text Mining tasks where, in general, the aim is to derive factual knowledge from textual data. This article reports on a corpus annotation project that has produced a freely available resource for research on handling negation and uncertainty in biomedical texts (we call this corpus the BioScope corpus)."
vincze-etal-2008-hungarian,{H}ungarian Word-Sense Disambiguated Corpus,2008,5,4,1,1,15691,veronika vincze,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"To create the first Hungarian WSD corpus, 39 suitable word form samples were selected for the purpose of word sense disambiguation. Among others, selection criteria required the given word form to be frequent in Hungarian language usage, and to have more than one sense considered frequent in usage. HNC and its Heti Vil{\'a}ggazdas{\'a}g subcorpus provided the basis for corpus text selection. This way, each sample has a relevant context (whole article), and information on the lemma, POS-tagging and automatic tokenization is also available. When planning the corpus, 300-500 samples of each word form were to be annotated. This size makes it possible that the subcorpora prepared for the individual word forms can be compared to data available for other languages. However, the finalized database also contains unannotated samples and samples with single annotation, which were annotated only by one of the linguists. The corpus follows the ACLÂs SensEval/SemEval WSD tasks format. The first version of the corpus was developed within the scope of the project titled The construction Hungarian WordNet Ontology and its application in Information Extraction Systems (Hatvani et al., 2007). The corpus Â for research and educational purposesÂ is available and can be downloaded free of charge."
