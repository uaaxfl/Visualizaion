2006.bcs-1.4,W02-0504,0,0.339727,"Missing"
2006.bcs-1.4,P05-1071,0,0.0484791,"ve perfective verb), and 33 as PV (perfective verb). As a NOUN, it has the vowels “kutub”, as a PV_PASS it has “kutib”, and as PV it has “katab”. Therefore, in this case the vowelization does not add much beyond the Part of Speech tags. It is possible therefore that the full benefits of vocalization can only be seen in the context of a wider NLP pipeline than just the parser, including in particular the Part-of-Speech tagger. Related to this, Habash (2005) reports a drop in ambiguity when considering tokens only within the same Part of Speech tag. Also notable in the connection is the work of Habash & Rambow (2005), who describe an integrated approach to tokenization, Part of Speech tagging and morphological disambiguation. Of particular interest is the close connection between POS tagging and morphological disambiguation. While this connection is clearly related to the concerns expressed here, they do not include a step of diacritazation. In our view, the categorization of the ambiguities resolved by diacritic restoration discussed in Section 2 deserves detailed empirical analysis based on the data the parser is using. As just discussed, there is a close connection between POS tags and some diacritic r"
2006.bcs-1.4,2006.bcs-1.4,1,0.0530913,"Missing"
2006.bcs-1.4,W05-0711,0,0.546586,"above, used the bare text, there has been very little work examining whether a parser can make use of vocalized text. The Arabic Treebank now gives us a corpus to carry out such experiments, as we discuss in the following subsection. However, in addition to exploring which diacritic information is useful for the parser, we must also be concerned with what might be available to the parser outside the context of these experiments and outside the context of Treebank research. As discussed in Section 3, there is a growing body of work on diacritic restoration. However, Zitouni, et al. (2006) and Nelken & Shieber (2005) both report that it is much harder to restore the diacritics representing case information. This is not surprising, since as Nelken & Shieber write, “including case information naturally yields proportionally worse accuracy. Since case markings encode higher-order grammatical information, they would require a more powerful grammatical model than offered by finite state methods”. While noting these concerns and issues, for the experiments reported here we continue to train and test the data as it is tokenized following the stage of POS annotation, and we continue to use the gold tags, while va"
2006.bcs-1.4,W04-1612,0,0.663917,"of Pennsylvania, USA {maamouri,bies,skulick}@ldc.upenn.edu Arabic diacritization (referred to sometimes as vocalization or vowelling), defined as the full or partial representation of short vowels, shadda (consonantal length or germination), tanween (nunation or definiteness), and hamza (the glottal stop and its support letters), is still largely understudied in the current NLP literature. In this paper, the lack of diacritics in standard Arabic texts is presented as a major challenge to most Arabic natural language processing tasks, including parsing. Recent studies (Messaoudi, et al. 2004; Vergyri & Kirchhoff 2004; Zitouni, et al. 2006 and Maamouri, et al. forthcoming) about the place and impact of diacritization in text-based NLP research are presented along with an analysis of the weight of the missing diacritics on Treebank morphological and syntactic analyses and the impact on parser development. Keywords: Arabic NLP, Arabic diacritics, Diacritization, Modern Standard Arabic (MSA), Treebanks, Linguistic Annotation, Parsing 1 INTRODUCTION Arabic NLP research, focusing mainly on Modern Standard Arabic (MSA) in this paper, faces two major challenges, not necessarily shared with many other natural lang"
2006.bcs-1.4,P06-1073,0,0.743734,"mouri,bies,skulick}@ldc.upenn.edu Arabic diacritization (referred to sometimes as vocalization or vowelling), defined as the full or partial representation of short vowels, shadda (consonantal length or germination), tanween (nunation or definiteness), and hamza (the glottal stop and its support letters), is still largely understudied in the current NLP literature. In this paper, the lack of diacritics in standard Arabic texts is presented as a major challenge to most Arabic natural language processing tasks, including parsing. Recent studies (Messaoudi, et al. 2004; Vergyri & Kirchhoff 2004; Zitouni, et al. 2006 and Maamouri, et al. forthcoming) about the place and impact of diacritization in text-based NLP research are presented along with an analysis of the weight of the missing diacritics on Treebank morphological and syntactic analyses and the impact on parser development. Keywords: Arabic NLP, Arabic diacritics, Diacritization, Modern Standard Arabic (MSA), Treebanks, Linguistic Annotation, Parsing 1 INTRODUCTION Arabic NLP research, focusing mainly on Modern Standard Arabic (MSA) in this paper, faces two major challenges, not necessarily shared with many other natural languages: the first is it"
2020.lrec-1.493,W10-2211,0,0.0911768,"Missing"
2020.lrec-1.493,J93-2004,1,0.0968492,"Missing"
2020.lrec-1.493,N15-1186,0,0.0615565,"xist, such as Morfessor CatMAP (Creutz and Lagus, 2007), Morfessor FlatCat (Grönroos et al., 2014), etc. As discussed above, most of these systems are only designed for identifying morpheme boundaries. The annotation discussed here is intended primarily to move this stream of work forward. Some more recent systems focus on identifying morphologically related word pairs, such as (stop, stopped), and then transform the output to morpheme segmentations so they are compatible with the available segmentation based evaluation. Such work includes Schone and Jurafsky (2001); Narasimhan et al. (2015); Soricut and Och (2015); Luo et al. (2017) and Xu et al. (2018). These systems have the advantage of finding morphologically related word pairs, which is equivalent to finding roots for complex words. Another stream of completely unsupervised work primarily aims at discovering morphological paradigms (Parkes et al., 1998; Goldsmith, 2001; Chan, 2006). Xu et al. (2018) discover such paradigms, but primarily use them for improving a probabilistic segmentation model. The annotation we report on here does not attempt to determine paradigm information for a wide set of word roots, which is a somewhat different task. Whil"
2020.lrec-1.493,L16-1521,1,0.86281,"tion and so include some related higher-resource languages. Representative Language Packs have been created for Akan, Amharic, Arabic, Bengali, Farsi, Hindi, Hungarian, Indonesian, Mandarin, Russian, Somali, Spanish, Swahili, Tagalog, Tamil, Thai, Ukrainian, Vietnamese, Wolof, Yoruba, Zulu, plus partial resource packs for Hausa, Turkish, Uzbek and English. Incident Language Packs contain manually labeled evaluation data designed to test system performance on tasks related to situational awareness for one or more surprise languages that remain unknown until the start of each annual evaluation (Strassel and Tracey, 2016). Incident Language Packs have been created for Ilocano, Kinyarwanda, Odia, Oromo, Sinhala, Tigrinya, and Uyghur. 1.2 Overall Research Goals Morphology analysis is useful as an underlying task for various natural language processing applications. Supervised methods for such analysis require significant training data and suffer from difficulty in transferring tools from one language to another. Unsupervised methods do not suffer from these problems and are therefore better suited for the LORELEI program’s research goals of rapidly functioning in a new unanticipated language. In general, unsuper"
2020.lrec-1.493,C18-1005,1,0.744915,"he nine languages in our corpus cover five primary language families (Austronesian: Indonesian, Tagalog; Dravidian: Tamil; Indo-European: Hindi, Russian, Spanish; Niger-Congo: Akan (Twi), Swahili; Uralic: Hungarian), and cover a range of morphological phenomena including suffixation, prefixation, infixation, circumfixation, full and partial reduplication, and vowel harmony. Further, seven of the nine languages are annotated with root information as well, which can be used to test existing systems that are designed for identifying roots, such as Narasimhan et al. (2015), Luo et al. (2017), and Xu et al. (2018). Although this data set does not include languages in which templatic morphology plays a sizeable role (primarily of Semitic and Afro-Asiatic language families), it is sufficient to provide a real challenge for the current state-of-the-art in unsupervised morphology. 2. 2.1 Related Work Related Morphological Annotation The Penn Treebank developed a part-of-speech tagset for English that encoded some morphological distinctions but that did not provide information on morphemes or morphological segments (Marcus et al., 1993). The Penn Arabic Treebank moved to more complete morphological annotati"
bies-etal-2006-linguistic,A00-2018,0,\N,Missing
bies-etal-2006-linguistic,graff-bird-2000-many,0,\N,Missing
bies-etal-2006-linguistic,N01-1016,0,\N,Missing
bies-etal-2006-linguistic,W02-1007,0,\N,Missing
bies-etal-2006-linguistic,N04-4032,0,\N,Missing
bies-etal-2006-linguistic,J03-4003,0,\N,Missing
bies-etal-2006-linguistic,N06-1024,1,\N,Missing
bies-etal-2006-linguistic,P04-1005,0,\N,Missing
bies-etal-2006-linguistic,cieri-etal-2004-fisher,0,\N,Missing
H94-1020,H92-1026,0,0.0176213,"Missing"
H94-1020,H92-1030,1,0.192685,"Missing"
H94-1020,P93-1035,0,0.0323163,"Missing"
H94-1020,E91-1004,1,0.125876,"Missing"
H94-1020,J93-2004,1,0.0729341,"cial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as &quot;underlying&quot; position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles. 1. I N T R O D U C T I O N During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this material also annotated with a skeletal syntactic bracketing. All of this material has been hand corrected after processing by automatic tools. The largest component of the corpus consists of materials from the Dow-Jones News Service; over 1.6 million words of this material has been hand parsed, with an additional 1 million words tagged for part of speech. Also included is a skeletally parsed version of the Brown corpus, the classic million word balanced corpus of American English"
H94-1020,H91-1060,0,0.00776414,"be of much more use if it explicitly provided some form of predicate-argument structure. The desired level of representation would make explicit at least the logical subject and logical object of the verb, and indicate, at least in clear cases, how subconstituents are semantically related to their predicates. Such a representation could serve as both a starting point for the kinds of SEMEVAL representations now being discussed as a basis for evaluation of human language technology within the ARPA HLT program, and as a basis for &quot;glass box&quot; evaluation of parsing technology. The ongoing effort [1] to develop a standard objective methodology to compare parser outputs across widely divergent grammatical frameworks has now resulted in a widely supported standard for parser comparison. On the other hand, many existing parsers cannot be evaluated by this metric because they directly produce a level of representation closer to predicate-argument structure than to classical surface grammatical analysis. Hand-in-hand with this limitation of the existing Penn Treebank for parser testing is a parallel limitation for automatic methods for parser training for parsers based on deeper representation"
H94-1020,J93-1005,0,\N,Missing
H94-1020,H93-1047,0,\N,Missing
H94-1020,P93-1005,0,\N,Missing
H94-1020,1991.iwpt-1.22,1,\N,Missing
kulick-etal-2010-consistent,maamouri-etal-2010-speech,1,\N,Missing
kulick-etal-2010-consistent,W04-1606,0,\N,Missing
kulick-etal-2010-consistent,maamouri-etal-2008-diacritic,1,\N,Missing
kulick-etal-2012-developments,E03-1068,0,\N,Missing
kulick-etal-2012-developments,P00-1058,0,\N,Missing
kulick-etal-2012-developments,P03-2041,0,\N,Missing
kulick-etal-2012-developments,P10-2014,0,\N,Missing
kulick-etal-2012-developments,C10-1045,0,\N,Missing
kulick-etal-2012-developments,P11-2122,1,\N,Missing
L16-1145,W13-2322,1,0.860349,"Missing"
L16-1145,bies-etal-2014-incorporating,1,0.844507,"ish Treebank within BOLT improved annotation quality by adding several rounds of Quality Control (QC) to the annotation process. The first QC process consists of a series of specific searches for approximately 200 types of potential inconsistencies and parser or annotation errors. Any errors found in these searches were hand corrected. An additional QC process then identifies repeated text and structures, and flags non-matching annotations. Identified annotation errors are also manually corrected. A special effort for English translation Treebank is the annotation of alternative translations (Bies et al. 2014). Both literal and fluent translation alternates are annotated for word-level tokenization and part-of-speech, whereas only the fluent translation alternates are annotated as part of the syntactic structure of the tree. 3.1.2 Arabic Treebank Arabic Treebank started with Penn Arabic Treebank guidelines, enhanced first with the GALE (Global Autonomous Language Exploitation) project and now with the BOLT project (Maamouri et al. 2011). The two efforts of Arabic Treebank within the GALE project are enhancement of Penn style Treebank annotation (Maamouri & Bies, 2010) and the creation of CATiB (Col"
L16-1145,bonial-etal-2014-propbank,1,0.859958,"or challenge for Arabic was with special features of the Egyptian dialect. It was sometimes very difficult to decide if an MSA word form in the dialect had an equivalent meaning or a slightly different meaning. Additionally, as a pro-drop language, Arabic poses special issues for coreference. Co-reference chains creation problems were initially solved by manual annotation but are now able to be created during post-processing. The English PropBank has focused on expanding predicate annotation beyond the verb and is now annotating verbs, eventive nouns, adjectives, and light verb constructions (Bonial et al. 2014). In English, light verbs are semantically bleached verbs. The argument structure for light verb and non-light verb instances are different. In light verb constructions, the real predicate is generally the nominalized predicate that the light verb supports. A major focus for English PropBank has been to unify FrameFiles across these different parts of speech. This means that the frame used for 'bathe' is always identical to that used for 'bath'. The goal of this expansion is to provide event semantic representations for the entire sentence, specifically pieces most often missed when looking so"
L16-1145,cotterell-callison-burch-2014-multi,0,0.0685218,"Missing"
L16-1145,W10-0702,0,0.0308468,"nd facilitating linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2010-enriching,1,0.823206,"ting linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2012-parallel,1,0.828457,"-9 (-NONE- *T*))))))) (. .)) ) POS annotation is stored in .pos files, where POS tags are attached to tokenized/segmented word units, each sentence per line, as shown in the following example. 好_VA 的_SP 呗_SP ，_PU 来_VV 的话_SP 打_VV 我 _PN 电话_NN 就_AD 可以_VV ，_PU 或者_CC 报_VV 我_PN 名字_NN ，_PU 我_PN 定_VV 了_AS 包厢_NN 3.2 Word Alignment 3.2.1 Alignment Approach Word alignment guidelines are developed based on guidelines for the Blinker and ARCADE projects, enriched during GALE by adding tagging guidelines (Li et al. 2010), and further enhanced to tackle new genres and language features for the BOLT project (Li et al. 2012). The alignment annotation involves a process of 2-pass annotation plus one round of cross-file checking. The initial alignment by junior annotators goes through quality control by senior annotators, and is then followed by a cross-file check by lead annotators for consistency. The word alignment tool is developed by LDC (Figure 3), allowing annotators to align source and translation words as well as to label both alignment links and individual words. Alignment is performed on two pairs of languages: Chinese-English and Egyptian-English. Chinese alignment is performed at two levels: character-"
L16-1145,maamouri-etal-2014-developing,1,0.87281,"Missing"
L16-1145,N13-1039,0,0.101377,"Missing"
L16-1405,W13-2301,1,0.860938,"y in order to handle OOV items, on the assumption that whatever lexicon we used would necessarily be lacking in coverage for the project annotation, even if the suffix sequences could be made close to complete. This framework builds upon our experience with development of Arabic analyzers starting with the Arabic Treebank (Kulick et al., 2010) and moving to the Egyptian Arabic Treebank (Maamouri et al., 2014), in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was greatly increased in order to handle OOV items, as the CALIMA analyzer (Habash et al., 2012; Maamouri et al., 2014) was simultaneously being developed. However, even then this handling was suboptimal because there was no general classification of what suffix combinations could 3 A"
L16-1405,W12-2301,0,0.0292489,"in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was greatly increased in order to handle OOV items, as the CALIMA analyzer (Habash et al., 2012; Maamouri et al., 2014) was simultaneously being developed. However, even then this handling was suboptimal because there was no general classification of what suffix combinations could 3 Another possible analysis is millet/NOUN+ler/3P, due to the ambiguity of ler, which we have not discussed here. 2555 follow stems with particular morphological properties. In contrast, the work reported here integrates this annotation need into the development of the morphological specification, instead of being added on after the analyzer development. 5. Lexicon-Analyzer Tradeoffs, and Integration of Existi"
L16-1405,E09-2008,0,0.223454,"ixes in the analyzer. Also, the interaction between the lexicon and the morphological specification could be, and indeed was, different for each language, depending on the morphological characteristics of the language and the available resources. For example, since Hausa has far less suffixation than Turkish, for Hausa the lexicon held multiple entries for a stem containing the effects of the derivational morphology, with much less information in the morphological specification than for Turkish. While morphological analyzer development tools such as XFST (Beesley and Karttunen, 2003) or Foma (Hulden, 2009) can be extremely powerful and very useful for grammar organization, they can also be quite fragile and complex when dealing with various phenomena such as longdistance dependencies between morphemes, so they could not be used exclusively for this project. We therefore adopted a hybrid approach, described in Section 3. 3. STATE_NOUN ; STATE_NOUN ; STATE_NOUN ; STATE_NOUN ; LEXICON STATE_NOUN +STATE-NOUN:x NOUN_HYPOCORISTIC ; +STATE-NOUN:x NOUN_PLURAL ; LEXICON NOUN_HYPOCORISTIC +CIk/DIM:x # ; LEXICON NOUN_PLURAL NOUN_POSS_FROM_STEM ; +lEr/PLURAL:x NOUN_POSS_FROM_PLURAL ; LEXICON NOUN_POSS_FROM"
L16-1405,kulick-etal-2010-consistent,1,0.828918,"based on flexible key/value entries in a map. This makes it more convenient than in SAMA to classify the possible stems for a suffix, including in terms of the morphological properties that would be needed by a stem to take a particular suffix sequence. We used this property in order to handle OOV items, on the assumption that whatever lexicon we used would necessarily be lacking in coverage for the project annotation, even if the suffix sequences could be made close to complete. This framework builds upon our experience with development of Arabic analyzers starting with the Arabic Treebank (Kulick et al., 2010) and moving to the Egyptian Arabic Treebank (Maamouri et al., 2014), in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was grea"
L16-1405,maamouri-etal-2014-developing,1,0.850053,"convenient than in SAMA to classify the possible stems for a suffix, including in terms of the morphological properties that would be needed by a stem to take a particular suffix sequence. We used this property in order to handle OOV items, on the assumption that whatever lexicon we used would necessarily be lacking in coverage for the project annotation, even if the suffix sequences could be made close to complete. This framework builds upon our experience with development of Arabic analyzers starting with the Arabic Treebank (Kulick et al., 2010) and moving to the Egyptian Arabic Treebank (Maamouri et al., 2014), in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was greatly increased in order to handle OOV items, as the CALIMA analyzer"
L16-1405,L16-1521,0,0.216201,"; +STATE-NOUN:x NOUN_PLURAL ; LEXICON NOUN_HYPOCORISTIC +CIk/DIM:x # ; LEXICON NOUN_PLURAL NOUN_POSS_FROM_STEM ; +lEr/PLURAL:x NOUN_POSS_FROM_PLURAL ; LEXICON NOUN_POSS_FROM_STEM NOUN_CASE +(I)m/POSS_1S NOUN_CASE ... LEXICON NOUN_POSS_FROM_PLURAL NOUN_CASE +(I)m/POSS_1S NOUN_CASE ... ; ; ; ; Figure 1: Foma specification for sequences of abstract morphemes in Turkish Development of the Morphological Analysis Specification The starting point of the morphological specification was the information in the more general grammatical sketch that was developed for each language as part of the project (Strassel and Tracey, 2016). The grammatical sketch is simply a description of the main characteristics of a given language - morphology, syntax, etc., similar to the information that could be found in any overview of a language. It was written from the perspective of being used by a NLP system developer, not by a linguist. 3.1. LEXICON NOUN_STEM ... NOUN_STEM_%02/NOUN:x .... NOUN_STEM_%05/NOUN:x .... NOUN_STEM_%08/NOUN:x .... NOUN_STEM_%11/NOUN:x .... Abstract Morpheme Organization While we do not use Foma (Hulden, 2009) for the full analyzer specification, as mentioned above, we do utilize it as a convenient way to im"
L16-1405,W05-1106,0,0.0915031,"Missing"
L16-1589,bies-etal-2014-incorporating,1,0.837176,"Table 4: Light and Rich ERE tagged items 3720 5. Annotation Decisions for Translation Artifacts As mentioned in Section 3 above, ERE data used in this corpus was collected and translated under DARPA’s BOLT program, which focused on machine translation. The emphasis on maximizing meaning fidelity over fluency in the translations resulted in some features that ERE had to accommodate by developing specific new annotation policies. 5.1 Alternate Translations The English translations of this data have 274 instances where both fluent and literal translations of some Chinese expressions are present (Bies et al., 2014). The inclusion of the alternates was intended to assist machine translation system development. Below, the Chinese phrase 老毛子 lǎo máozi is rendered with a fluent and literal translation: 为了目前我们既得利益，老 毛 子 的事可以先不 考虑。 For the sake of our current vested interests, we can disregard the matter of [Russians |Old Hairy] for the time being. In Light ERE, only the fluent translations are tagged. In Rich ERE, however, both translation alternates are annotated and coreferenced when appropriate. In some cases, the fluent translations do not match the literal translations exactly in terms the entities, rel"
L16-1589,R11-1017,0,0.0589143,"Missing"
L16-1589,W14-2904,1,0.867817,"Missing"
L16-1589,W15-0812,1,0.916562,"). Given the large number and variety of approaches to algorithm development within DEFT, we set out to define an annotation task that would be supportive of multiple research directions and technology evaluations, and that would provide a useful foundation for follow-on DEFT annotation tasks like entailment, inference and belief/sentiment. The resulting Entities, Relations and Events annotation task has evolved over the course of the DEFT program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program (Song et al. 2015). ERE corpora are used by DEFT performers as a general resource, and also serve as training data for several tracks within the Text Analysis Conference Knowledge Base Population (TAC KBP) evaluation series, which is open to non-DEFT participants conducted by the National Institute of Standards and Technology (NIST). TAC KBP aims to develop and evaluate technologies for building and populating knowledge bases from unstructured texts (NIST 2015). The ERE corpora provide a training resource for component evaluation tasks such as Entity Detection and Linking and Event Argument Linking. In keeping"
L16-1589,H01-1035,0,0.146667,"Missing"
L16-1589,P15-2064,0,0.0399635,"Missing"
L18-1265,P98-1013,0,0.752656,"ns, their participants, and their locations in text data. Given 1672 LORELEI’s low resource language setting combined with the need to simultaneously create resources for dozens of languages, the SSA task was designed with a naïve annotator in mind (i.e. without formal linguistic training or prior annotation experience). In this way it contrasts with more complex predicate-argument focused semantic representation schemes like Abstract Meaning Representation (AMR) (Banarescu, et al., 2013) Automatic Content Extraction (ACE) (Doddington, et al., 2004), PropBank (Palmer, et al., 2005), FrameNet (Baker, et al. 1998), Richer Event Description (RED) (Ikuta, et al., 2014), and Universal Decompositional Semantics (UDS) (White, et al., 2016), which require a background in linguistics and/or a long training period. In order to make SSA feasible for non-experts to master quickly, we annotate a small number of broad, underspecified predicate and argument categories that do not require fine-grained semantic distinctions. We generally select names, pronouns, or heads of nominal phrases as annotation extents, but annotators are allowed to select “intuitive extents” if needed (e.g. for multiword expressions), meanin"
L18-1265,W13-2322,1,0.919361,"Missing"
L18-1265,W14-2903,0,0.0418391,"Missing"
L18-1265,J05-1004,0,0.567902,"ts and disaster-relevant situations, their participants, and their locations in text data. Given 1672 LORELEI’s low resource language setting combined with the need to simultaneously create resources for dozens of languages, the SSA task was designed with a naïve annotator in mind (i.e. without formal linguistic training or prior annotation experience). In this way it contrasts with more complex predicate-argument focused semantic representation schemes like Abstract Meaning Representation (AMR) (Banarescu, et al., 2013) Automatic Content Extraction (ACE) (Doddington, et al., 2004), PropBank (Palmer, et al., 2005), FrameNet (Baker, et al. 1998), Richer Event Description (RED) (Ikuta, et al., 2014), and Universal Decompositional Semantics (UDS) (White, et al., 2016), which require a background in linguistics and/or a long training period. In order to make SSA feasible for non-experts to master quickly, we annotate a small number of broad, underspecified predicate and argument categories that do not require fine-grained semantic distinctions. We generally select names, pronouns, or heads of nominal phrases as annotation extents, but annotators are allowed to select “intuitive extents” if needed (e.g. for"
L18-1265,W15-0812,1,0.88758,"Missing"
L18-1265,L16-1521,1,0.774453,"Representative Language Packs – consisting of large volumes of formal and informal monolingual and parallel (with English) text with a variety of manual annotations to support situational awareness, plus a lexicon, grammatical sketch and basic processing tools – are designed to enable research into language universals and cross-language projection. Incident Language Packs contain manually labeled evaluation data designed to test system performance on tasks related to situational awareness for one or more surprise languages per year that remain unknown until the start of the annual evaluation (Strassel and Tracey, 2016). Akan (Twi) Amharic Arabic Bengali English Farsi Hausa Hindi Hungarian Indonesian Mandarin Oromo Russian Somali Spanish Swahili Tagalog Tamil Thai Tigrinya Turkish Ukrainian Uyghur Uzbek Vietnamese Wolof Yoruba Zulu Table 1: LORELEI Representative and Incident Languages This paper focuses on two semantic annotation tasks developed by LDC to support LORELEI research and evaluation: Simple Semantic Annotation (SSA) and Situation Frame (SF). Both SSA and SF label basic information relevant to humanitarian aid and disaster relief (HADR) scenarios. Situation Frame annotation directly corresponds t"
L18-1265,D16-1177,0,0.0476168,"Missing"
L18-1558,W14-2907,1,0.899127,"Missing"
L18-1558,araki-etal-2014-detecting,0,0.0892141,"to work that has been done for Entity Detection and Linking (EDL) (Ji et al., 2010), linking to such a KB of events would reduce the need to compare every relevant document-level event hopper to every potentially coreferent hopper, since many document-level hoppers could be linked directly to the event KB. The remaining document-level event hoppers that are not found in the KB would still need to be coreferenced via a pairwise comparison as in this paper (as NIL clusters are created for EDL entities). This direction, however, would require building such a KB before annotation. Giraldi et al. (2014) adopted this approach and demonstrated feasibility, but the event and coreference 7. Conclusion We created a small corpus annotated for cross-document and cross-lingual event coreference in 505 documents in three languages. Although we leveraged existing ERE annotation as input, this task required the development of new annotation guidelines, new data processes and user interfaces, and the creation of new cross-document and cross-lingual annotation. The more intuitive, coarser grained event hopper concept that was originally developed as part of within-document Rich ERE annotation (Song et al"
L18-1558,bejan-harabagiu-2008-linguistic,0,0.0363816,"ation extracted from multiple languages, sources and genres. In the sections that follow we present the results of our effort to define an approach to crossdocument, cross-lingual event coreference using event hoppers as part of the Rich ERE annotation task in DEFT. 2. Related Work There have been other efforts that have captured some variety of cross-document event-event coreference, which have informed our design of the cross-document event coreference annotation task. These include topic-clustering of documents and pair-wise comparison of event mentions. The EventCorefBank (ECB) (Bejan and Harabagiu, 2008) contains 482 documents clustered into 43 topics that were annotated for within-document and cross-document coreference according to the TimeML specification (Pustejovsky et al., 2013). Lee et al. (2012) extended ECB annotation, following the OntoNotes guidelines (Pradhan et al., 2007). These studies required both matching predicates and matching arguments for event coreference. The ECB+ corpus (Cybluska and Vossen, 2014a) is an extension of ECB with the addition of source documents as well as event components, using the CROMER (CROssdocument Main Events and entities Recognition) tool (Girardi"
L18-1558,W16-1004,1,0.885851,"Missing"
L18-1558,cybulska-vossen-2014-using,0,0.0798195,"Missing"
L18-1558,L18-1245,1,0.834948,"Consortium (LDC) to support multiple research directions and evaluations in DEFT. ERE builds on the approach to labeling entities, relations, events and their attributes under a pre-defined taxonomy, following the approach used in Automatic Content Extraction (ACE) (LDC, 2005; Walker et al., 2006; Song et al., 2015; Mott et al., 2016). One important DEFT use case is automatically building a structured Knowledge Base (KB) from scratch. This task, known as Cold Start, is one of several tasks relevant to DEFT that are evaluated in the NIST Knowledge Base Population evaluation series (NIST 2017; Getman et al, 2018). Given DEFT’s focus on whole-corpus understanding culminating in the Cold Start task, Rich ERE annotation has evolved over the course of the program to emphasize cross-document and cross-lingual approaches. Rich ERE event annotation includes 9 event types and 38 subtypes (e.g. Conflict.Attack, Contact.Meet, Movement.TransportPerson). For each event mention annotators label the most salient word evoking the event (the “event trigger”), the event type and subtype, the realis status (Actual, Other or Generic), all of the event’s arguments (e.g. agent, instrument) and several attributes like temp"
L18-1558,girardi-etal-2014-cromer,0,0.0176587,", 2008) contains 482 documents clustered into 43 topics that were annotated for within-document and cross-document coreference according to the TimeML specification (Pustejovsky et al., 2013). Lee et al. (2012) extended ECB annotation, following the OntoNotes guidelines (Pradhan et al., 2007). These studies required both matching predicates and matching arguments for event coreference. The ECB+ corpus (Cybluska and Vossen, 2014a) is an extension of ECB with the addition of source documents as well as event components, using the CROMER (CROssdocument Main Events and entities Recognition) tool (Girardi et al., 2014). Event coreference in ECB+ required matching time, place, and participants (Cybluska and Vossen, 2014b). A richer event-event relation annotation scheme (Hong et al., 2016) was developed to capture cross-document eventevent relations, including coreference. This data was constructed using ACE 2005 data and supplemented with data collected by researchers; event relations including coreference were annotated by pairwise comparison of events from documents within a given topic. Event coreference required event arguments in the pair to match. To support the cross-document component of Event Argum"
L18-1558,W16-1701,0,0.0905076,"Missing"
L18-1558,D12-1045,0,0.0197889,"g event hoppers as part of the Rich ERE annotation task in DEFT. 2. Related Work There have been other efforts that have captured some variety of cross-document event-event coreference, which have informed our design of the cross-document event coreference annotation task. These include topic-clustering of documents and pair-wise comparison of event mentions. The EventCorefBank (ECB) (Bejan and Harabagiu, 2008) contains 482 documents clustered into 43 topics that were annotated for within-document and cross-document coreference according to the TimeML specification (Pustejovsky et al., 2013). Lee et al. (2012) extended ECB annotation, following the OntoNotes guidelines (Pradhan et al., 2007). These studies required both matching predicates and matching arguments for event coreference. The ECB+ corpus (Cybluska and Vossen, 2014a) is an extension of ECB with the addition of source documents as well as event components, using the CROMER (CROssdocument Main Events and entities Recognition) tool (Girardi et al., 2014). Event coreference in ECB+ required matching time, place, and participants (Cybluska and Vossen, 2014b). A richer event-event relation annotation scheme (Hong et al., 2016) was developed t"
L18-1558,W15-0812,1,0.87345,"task first defined as part of DARPA’s Deep Exploration and Filtering of Text (DEFT) program (DARPA, 2012). The goal of the DEFT program is to develop technologies capable of extracting knowledge from unstructured text in multiple languages and genres. ERE annotation was developed at Linguistic Data Consortium (LDC) to support multiple research directions and evaluations in DEFT. ERE builds on the approach to labeling entities, relations, events and their attributes under a pre-defined taxonomy, following the approach used in Automatic Content Extraction (ACE) (LDC, 2005; Walker et al., 2006; Song et al., 2015; Mott et al., 2016). One important DEFT use case is automatically building a structured Knowledge Base (KB) from scratch. This task, known as Cold Start, is one of several tasks relevant to DEFT that are evaluated in the NIST Knowledge Base Population evaluation series (NIST 2017; Getman et al, 2018). Given DEFT’s focus on whole-corpus understanding culminating in the Cold Start task, Rich ERE annotation has evolved over the course of the program to emphasize cross-document and cross-lingual approaches. Rich ERE event annotation includes 9 event types and 38 subtypes (e.g. Conflict.Attack, Co"
li-etal-2012-parallel,maamouri-etal-2008-enhancing,1,\N,Missing
li-etal-2012-parallel,D11-1018,0,\N,Missing
li-etal-2012-parallel,W04-2208,0,\N,Missing
li-etal-2012-parallel,W06-2717,0,\N,Missing
li-etal-2012-parallel,W04-1602,1,\N,Missing
li-etal-2012-parallel,H05-1012,0,\N,Missing
li-etal-2012-parallel,J03-1002,0,\N,Missing
li-etal-2012-parallel,grimes-etal-2012-automatic,1,\N,Missing
li-etal-2012-parallel,li-etal-2010-enriching,1,\N,Missing
li-etal-2012-parallel,W11-4305,0,\N,Missing
li-etal-2012-parallel,N06-1014,0,\N,Missing
maamouri-etal-2006-developing,E06-1047,1,\N,Missing
maamouri-etal-2006-developing,W04-1602,1,\N,Missing
maamouri-etal-2006-developing,N04-4038,1,\N,Missing
maamouri-etal-2006-developing,P05-1071,1,\N,Missing
maamouri-etal-2008-diacritic,W04-1602,1,\N,Missing
maamouri-etal-2008-diacritic,P05-1071,0,\N,Missing
maamouri-etal-2010-speech,kulick-etal-2010-consistent,1,\N,Missing
maamouri-etal-2010-speech,maamouri-etal-2008-enhancing,1,\N,Missing
maamouri-etal-2010-speech,W04-1602,1,\N,Missing
maamouri-etal-2012-expanding,maamouri-etal-2010-speech,1,\N,Missing
maamouri-etal-2012-expanding,kulick-etal-2010-consistent,1,\N,Missing
maamouri-etal-2012-expanding,C10-1045,0,\N,Missing
maamouri-etal-2014-developing,C10-1045,0,\N,Missing
maamouri-etal-2014-developing,P06-1086,1,\N,Missing
maamouri-etal-2014-developing,habash-etal-2012-conventional,1,\N,Missing
maamouri-etal-2014-developing,N13-1044,1,\N,Missing
maamouri-etal-2014-developing,pasha-etal-2014-madamira,1,\N,Missing
maamouri-etal-2014-developing,W13-2301,1,\N,Missing
maamouri-etal-2014-developing,maamouri-etal-2006-developing,1,\N,Missing
maamouri-etal-2014-developing,W12-2301,1,\N,Missing
maamouri-etal-2014-developing,L06-1000,0,\N,Missing
N10-1094,J03-4003,0,\N,Missing
N10-1094,P00-1058,0,\N,Missing
N12-1031,P09-2056,0,0.0224385,"ructure. Following the motivation discussed in Section 1, the next step is straightforward - to adapt the algorithm to work on conversion from a dependency representation of the Arabic Treebank to the phrase structure representation necessary for the annotation pipeline. Following this, we will then experiment with parsing the Arabic dependency representation, converting to phrase structure, and evaluating the resulting phrase structure representation as usual for parsing evaluation. We will also experiment with dependency parsing for the PTB dependency representation discussed in this paper. Habash and Roth (2009) discuss an already-existing dependency representation of parts of the ATB and it will be interesting to compare the conversion accuracy using the different dependency representations, although we 313 expect that there will not be any major differences in the representations. One other aspect of future work is to implement the algorithm in Wang and Zong (2010), using our own dependency representation, since this would allow a precise investigation of what the phrase structure parser is contributing as compared to our automatic conversion. We note that this work also experimented with dependenc"
N12-1031,W08-1007,0,0.0225921,"correct some of the POS tag errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1 There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. 308 coding the phrase structure in the dependency tree via complex tags such as"
N12-1031,W07-2444,0,0.0311914,"errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1 There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. 308 coding the phrase structure in the dependency tree via complex tags such as SBARQ in Johansson and Nugues"
N12-1031,W07-2416,0,0.157017,"to the Arabic treebank as well. We expect this to be successful because the ATB has some fundamental similarities to the PTB in spite of the language difference (Maamouri and Bies, 2004). As mentioned above, one goal in our DS to PS conversion work is to base it on a minimal DS representation. By “minimal”, we mean that it does not include information that is redundant, together with our conversion code, with the implicit information in the dependency structure itself. As discussed more in Section 2.1, we aim to make our dependency representation simpler than “hybrid” representations such as Johansson and Nugues (2007). The reason for our interest in this minimal representation is parsing. We do not want to require the parser to recover such a complex dependency representations, when it is, in fact, unnecessary, as we believe our approach shows. The benefit of this approach can only be seen when this line of work is extended to experiments with parsing and Arabic conversion. The work described here is just the first step in this process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm d"
N12-1031,W04-1602,1,0.79084,"rabic, we are first focusing on 305 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305–314, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics a conversion routine for the English PTB because it is so well-established and the results are easier to interpret. The intent is then to transfer this conversion algorithm work to the Arabic treebank as well. We expect this to be successful because the ATB has some fundamental similarities to the PTB in spite of the language difference (Maamouri and Bies, 2004). As mentioned above, one goal in our DS to PS conversion work is to base it on a minimal DS representation. By “minimal”, we mean that it does not include information that is redundant, together with our conversion code, with the implicit information in the dependency structure itself. As discussed more in Section 2.1, we aim to make our dependency representation simpler than “hybrid” representations such as Johansson and Nugues (2007). The reason for our interest in this minimal representation is parsing. We do not want to require the parser to recover such a complex dependency representatio"
N12-1031,J93-2004,0,0.0422264,"lines. The resulting system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 1 Introduction and Motivation Recent years have seen a significant increase in interest in dependency treebanks and dependency parsing. Since the standard training and test set for English parsing is a phrase structure (PS) treebank, the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can However, our concern is somewhat different. We are specifically interested in experimenting with dependency parsing of Arabic as a step in the annotation of the A"
N12-1031,H94-1020,1,0.510763,"system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 1 Introduction and Motivation Recent years have seen a significant increase in interest in dependency treebanks and dependency parsing. Since the standard training and test set for English parsing is a phrase structure (PS) treebank, the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can However, our concern is somewhat different. We are specifically interested in experimenting with dependency parsing of Arabic as a step in the annotation of the Arabic Treebank, which"
N12-1031,D08-1052,0,0.431252,"Missing"
N12-1031,C10-2148,0,0.295097,"d the word y to its immediate right is its parent in the dependency tree and y has one of the verbal POS tags, then x receives the supertag P AUX, and otherwise P PP. Any word with the POS tag JJ, JJR, or JJS, receives the supertag P ADJP, and so on. The results for Xia and Palmer (2001) and Xia et al. (2009) were reported using an unlabeled version of evalb, so to compare properly we also report our results for Section 00 using an unlabeled evaluation of the run using the POS tags (USE-POS-UNLABEL), while all the other results use a labeled evaluation. We also compare our system with that of Wang and Zong (2010). Unlike the three other systems (including ours), this was not based on an automatic conversion from a gold dependency tree to phrase structure, but rather used the gold dependency tree as additional input for a phrase structure parser (the Berkeley parser). 4.1 Analysis While our system was developed using Section 24, the f-measure results for USE-SUPER are virtually identical across all four sections (96.4, 96.7, 96.7, 96.5). Interestingly, there is more variation in the Error type problem with PTB annotation ambiguous ADVP placement incorrect use of “single token rule” FRAG/X multiple leve"
N12-1031,H01-1014,0,0.280737,"process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm designed to take as input any arbitrary DS representation. However, the fundamentals of our dependency representation are not radically different than others - e.g. we make an auxiliary verb the child of the main verb, instead of the other way, but such choices can be adjusted for in the conversion. To evaluate the success of this conversion algorithm, we follow the same evaluation procedure as Xia et al. (2009) and Xia and Palmer (2001). We convert the PTB to a DS, and then use our algorithm to convert the DS back to a PS representation. The original PS and the converted-from-DS PS are then compared, in exactly the same way as parser output is compared with the original (gold) tree. We will show that our results in this area are a significant improvement above previous efforts. A key aspect of this work is that our DS-to-PS conversion encodes many of the properties of the PTB annotation guidelines (Bies et al., 1995), both 306 globally and for specific XP projections. The PTB guidelines are built upon broad decisions about P"
N12-1031,P00-1058,0,\N,Missing
N13-1061,P10-1075,0,0.014887,"As mentioned above, there is a certain class of inconsistencies which KBM will not pinpoint precisely, which requires adopting the “external check” from Kulick et al. (2012). The abstraction on inconsistency types described in Section 4 can also be taken further. For example, one might want to examine in particular inconsistency types that arise from PP attachment or that have to do with the PRN function tag. One main area for future work is the application of this work to parser evaluation as well as IAA. For this area, there is some connection to the work of Goldberg and Elhadad (2010) and Dickinson (2010), which are both concerned with examining dependency structures of more than one edge. The connection is that those works are focused on dependency representations, and ithe KBM system does phrase structure analysis using a TAG-like derivation tree, which strongly resembles a dependency tree (Rambow and Joshi, 1997). There is much in this area of common concern that is worth examining further. Acknowledgments actual The trees in (4) and (5) show two of the 88 nuclei grouped into the first inconsistency type. As with The word renaissance and The term renaissance in the English web corpus, nucle"
N13-1061,W10-2927,0,0.0210956,"to improve the current approach. As mentioned above, there is a certain class of inconsistencies which KBM will not pinpoint precisely, which requires adopting the “external check” from Kulick et al. (2012). The abstraction on inconsistency types described in Section 4 can also be taken further. For example, one might want to examine in particular inconsistency types that arise from PP attachment or that have to do with the PRN function tag. One main area for future work is the application of this work to parser evaluation as well as IAA. For this area, there is some connection to the work of Goldberg and Elhadad (2010) and Dickinson (2010), which are both concerned with examining dependency structures of more than one edge. The connection is that those works are focused on dependency representations, and ithe KBM system does phrase structure analysis using a TAG-like derivation tree, which strongly resembles a dependency tree (Rambow and Joshi, 1997). There is much in this area of common concern that is worth examining further. Acknowledgments actual The trees in (4) and (5) show two of the 88 nuclei grouped into the first inconsistency type. As with The word renaissance and The term renaissance in the Engl"
N13-1061,P11-2122,1,0.932454,"consisting of syntactic structure with words as the terminals, is by its nature more complex and therefore more prone to error than many other annotation tasks. However, high annotation consistency is crucial to providing reliable training and testing data for parsers and linguistic research. Error detection is therefore an important area of research, and the importance of work such as Dickinson and Meurers (2003) is that errors and annotation inconsistencies might be automatically discovered, and once discovered, be targeted for subsequent quality control. A recent approach to this problem (Kulick et al., 2011; Kulick et al., 2012) (which we will call the KBM system) improves upon Dickinson and Meurers (2003) by decomposing the full syntactic tree into smaller units, using ideas from Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997). This allows the comparison to be based on small syntactic units instead of string n-grams, improving the detection of inconsistent annotation. The KBM system, like that of Dickinson and Meurers (2003) before it, is based on the notion of comparing identical strings. In the general case, this is a problematic assumption, since annotation inconsistencies are missed"
N13-1061,kulick-etal-2012-developments,1,0.819494,"tic structure with words as the terminals, is by its nature more complex and therefore more prone to error than many other annotation tasks. However, high annotation consistency is crucial to providing reliable training and testing data for parsers and linguistic research. Error detection is therefore an important area of research, and the importance of work such as Dickinson and Meurers (2003) is that errors and annotation inconsistencies might be automatically discovered, and once discovered, be targeted for subsequent quality control. A recent approach to this problem (Kulick et al., 2011; Kulick et al., 2012) (which we will call the KBM system) improves upon Dickinson and Meurers (2003) by decomposing the full syntactic tree into smaller units, using ideas from Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997). This allows the comparison to be based on small syntactic units instead of string n-grams, improving the detection of inconsistent annotation. The KBM system, like that of Dickinson and Meurers (2003) before it, is based on the notion of comparing identical strings. In the general case, this is a problematic assumption, since annotation inconsistencies are missed because of superficial"
N13-1061,P00-1058,0,\N,Missing
P11-2122,E03-1068,0,0.189571,"ackground and Motivation 2.1 Previous Work - DECCA Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research. Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurring more than once in a corpus may occur with different “labels” (taken to be constituen"
P11-2122,P03-2041,0,0.026495,"to its having different etrees, differing in their node label for the substitution node. If the nucleus under comparison includes the verb but not any words from the complement, the inclusion of the different substitution nodes would cause irrelevant differences for that particular nucleus comparison. We solve these problems by mapping down the in the derivation tree. 5 A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). We suspect that there are some major differences between our approaches regarding such issues as the representation of adjuncts, but we leave such a comparison for future work. System DECCA Us nuclei 24,319 54,496 n-grams 1,158,342 not used instances 2,966,274 605,906 Table 1: Data examined by the two systems for the ATB System DECCA Us-internal nuclei found 4,140 9,984 non-duplicate nuclei found unknown 4,272 types of inconsistency unknown 1,911 Table 2: Annotation inconsistencies reported for the ATB representation of the etrees in a derivation tree fragment to form a “reduced” derivation"
P11-2122,C10-1045,0,0.34162,"ic Treebank and how this approach leads to high precision of error detection. 1 2 Background and Motivation 2.1 Previous Work - DECCA Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research. Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurrin"
P11-2122,P10-2014,0,0.232375,"A Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research. Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurring more than once in a corpus may occur with different “labels” (taken to be constituent node labels), and such differences in labels"
P11-2122,W10-4420,1,0.849308,"isting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurring more than once in a corpus may occur with different “labels” (taken to be constituent node labels), and such differences in labels might be the manifestation of an annotation error. Adopting their terminology, a “variation nucleus” is the string of words with a difference in the annotation (label), while a “variation n-gram” i"
P11-2122,P00-1058,0,\N,Missing
P14-2109,D08-1052,0,0.02555,"ow does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines two strands of research. First, inspired by the tradition of Tree Adjoining Grammar-based research (Joshi and Schabes, 1997; Bangalore and Joshi, 2010), we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in Kulick et al. (2011). In particular, we use the “spinal” structure approach of (Shen et al., 2008; Shen and Joshi, 2008), where etrees are constrained to be unary-branching. 2 Framework for analyzing parsing performance We first describe the use of the regexes in tree decomposition, and then give some examples of in1 We refer only to the WSJ treebank portion of OntoNotes, which is roughly a subset of the Penn Treebank (Marcus et al., 1999) with annotation revisions including the addition of NML nodes. 2 We parse (c) while training on (a) to follow the procedure in Petrov and McDonald (2012) 668 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668–673,"
P14-2109,N12-1031,1,0.896225,"Missing"
P14-2109,N13-1061,1,0.836283,"as well. For example, “make” is a match for VP-t in Figures 2 and 3, and is also a match for the span as well, since in both derivation trees it includes the words “make. . .Florida”. It is this matching for span as well as head that allows us to compare our results to evalb. We call the match just for the head the “Fh” score and the match that also includes the span information the “F-s” score. The F-s score roughly corresponds to the evalb score. However, the F2.4 Comparison with previous work This work is in the same basic line of research as the inter-annotator agreement analysis work in Kulick et al. (2013). However, that work did not utilize regexes, and focused on comparing sequences of identical strings. The current work scores on general categories of structures, without 6 A regex intermediate in a etree, such as VP-t above, is considered to have a default null attachment. Also, the attachment score is not relevant for regexes that already express a recursive structure, such as NP-modr. In Figure 2, NP-t in etree #a5 is considered as having the attachment to #a3. 670 regex NP-t VP-t PP-t S-vp NP-modr VP-aux SBAR-s ADVP-t NML-t ADJP-t QP-t NP-crd VP-crd S-crd SQ-v FRAG-nt Sections 2-21 (Onton"
P14-2109,D12-1096,0,0.0613251,"Missing"
P14-2109,J03-4003,0,\N,Missing
P14-2109,P11-2122,1,\N,Missing
palmer-etal-2008-pilot,W04-3212,1,\N,Missing
palmer-etal-2008-pilot,W05-0630,0,\N,Missing
palmer-etal-2008-pilot,W03-1006,0,\N,Missing
palmer-etal-2008-pilot,N07-2014,0,\N,Missing
palmer-etal-2008-pilot,P98-1013,0,\N,Missing
palmer-etal-2008-pilot,C98-1013,0,\N,Missing
palmer-etal-2008-pilot,P04-1043,0,\N,Missing
palmer-etal-2008-pilot,S07-1026,1,\N,Missing
palmer-etal-2008-pilot,W05-0620,0,\N,Missing
palmer-etal-2008-pilot,J02-3001,0,\N,Missing
palmer-etal-2008-pilot,P02-1031,1,\N,Missing
palmer-etal-2008-pilot,N04-1030,0,\N,Missing
palmer-etal-2008-pilot,N04-1032,0,\N,Missing
W04-1602,H94-1020,1,0.263796,"roughly 144K words from Al-Hayat distributed by Ummah Arabic News Text. New features of annotation in the UMAAH (UMmah Arabic Al-Hayat) corpus include complete vocalization (including case endings), lemma IDs, and more specific part-of-speech tags for verbs and particles. Arabic Treebank: Part 3 is currently underway, and consists of text from An-Nahar. (Maamouri and Cieri, 2002) The ATB corpora are annotated for morphological information, part-of-speech, English gloss (all in the “part-of-speech” phase of annotation), and for syntactic structure (Treebank II style). (Marcus, et al., 1993), (Marcus, et al., 1994) In addition to the usual issues involved with the complex annotation of data, we have come to terms with a number of issues that are specific to a highly inflected language with a rich history of traditional grammar. 2 2.1 Issues of methodology and training with Modern Standard Arabic Defining the specificities of ‘Modern Standard Arabic’ Modern Standard Arabic (MSA), the natural language under investigation, is not natively spoken by Arabs, who acquire it only through formal schooling. MSA is the only form of written communication in the whole of the Arab world. Thus, there exists a living w"
W04-1602,J93-2004,0,0.0238334,"Catalog No. LDC2004T02, roughly 144K words from Al-Hayat distributed by Ummah Arabic News Text. New features of annotation in the UMAAH (UMmah Arabic Al-Hayat) corpus include complete vocalization (including case endings), lemma IDs, and more specific part-of-speech tags for verbs and particles. Arabic Treebank: Part 3 is currently underway, and consists of text from An-Nahar. (Maamouri and Cieri, 2002) The ATB corpora are annotated for morphological information, part-of-speech, English gloss (all in the “part-of-speech” phase of annotation), and for syntactic structure (Treebank II style). (Marcus, et al., 1993), (Marcus, et al., 1994) In addition to the usual issues involved with the complex annotation of data, we have come to terms with a number of issues that are specific to a highly inflected language with a rich history of traditional grammar. 2 2.1 Issues of methodology and training with Modern Standard Arabic Defining the specificities of ‘Modern Standard Arabic’ Modern Standard Arabic (MSA), the natural language under investigation, is not natively spoken by Arabs, who acquire it only through formal schooling. MSA is the only form of written communication in the whole of the Arab world. Thus,"
W04-1602,E03-1014,0,0.0863735,"Missing"
W04-3111,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3111,P03-1002,0,\N,Missing
W04-3111,N03-1028,0,\N,Missing
W04-3111,P96-1008,0,\N,Missing
W04-3111,tateisi-tsujii-2004-part,0,\N,Missing
W05-0304,A00-2030,0,0.0185228,"uents, it makes such a mapping easier. The treebank annotation has been modified from the Penn Treebank guidelines in various ways, such as greater structure for prenominal modifiers. Again, while this would have been done regardless of the mapping of entities, it does make such a mapping more successful. Previous work on integrating syntactic structure with entity information, as well as relation infor21 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 21–28, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics mation, is described in (Miller et al., 2000). Our work is in much the same spirit, although we do not integrate relation annotation into the syntactic trees. PubMed abstracts are quite different from the newswire sources used in that earlier work, with several consequences discussed throughout, such as the use of discontinuous entities. Section 2 discusses some of the main issues around the development of the guidelines for entity annotation, and Section 3 discusses some of the changes that have been made for the treebank guidelines. Section 4 describes the annotation workflow and the resulting merged representation. Section 5 evaluates"
W06-0609,I05-1081,1,0.820744,"p (NP (NP musical acts) (PP for (NP a new record label))))) .) the original PropBank annotation, a further link is provided, which specifies the relative pronoun as being of “semantic type” answers. (9) Original PropBank annotation: Arg1: [NP *T*-6] -&gt; which -&gt; answers rel: have Arg0: [NP-SBJ *-3] -&gt; we In PropBank, the different pieces of the utterance, including the trace under the verb said, were concatenated This additional link between which and answers is important for many applications that make use of preferences for semantic types of verb arguments, such as Word Sense Disambiguation (Chen & Palmer 2005). In the new annotation scheme, annotators will first label traces as arguments: (12) Original PropBank annotation: ARG1: [ Among other things] [ Mr. Azoff] [ would develop musical acts for a new record label] [ [*T*-1]] ARG0: they rel: said (10) Revised PropBank annotation (stage 1): Rel: have Arg1: [*T*-6] Arg0: [NP-SBJ *-3] Under the new approach, in stage one, Treebank annotation will introduce not a trace of the S clause, but rather *?*, an empty category indicating ellipsis. In stage three, PropBank annotators will link this null element to the S node, but the resulting chain will not be"
W06-0609,H94-1020,1,0.457429,"o tasks: part-of-speech tagging and syntactic annotation. The first task is to provide a part-of-speech tag for every token. Particularly relevant for PropBank work, verbs in any form (active, passive, gerund, infinitive, etc.) are marked with a verbal part of speech (VBP, VBN, VBG, VB, etc.). (Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles. (Marcus, et al. 1994; Bies, et al. 1995) Constituent boundaries are shown through syntactic node labels in the trees. In the simplest case, a node will contain an entire constituent, complete with any associated arguments or modifiers. However, in structures involving syntactic movement, sub-constituents may be displaced. In these cases, Treebank annotation represents the original position with a trace and shows the relationship as co-indexing. In (1) below, for example, the direct object of entail is shown with the trace *T*, which is coindexed to the WHNP node of the question word what. Introduction The PropBan"
W06-0609,J93-2004,0,0.0290163,"ismatches between the syntactic bracketing and the semantic role labeling that can be found, and our plans for reconciling them. 1 1.1 Treebank The Penn Treebank annotates text for syntactic structure, including syntactic argument structure and rough semantic information. Treebank annotation involves two tasks: part-of-speech tagging and syntactic annotation. The first task is to provide a part-of-speech tag for every token. Particularly relevant for PropBank work, verbs in any form (active, passive, gerund, infinitive, etc.) are marked with a verbal part of speech (VBP, VBN, VBG, VB, etc.). (Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles. (Marcus, et al. 1994; Bies, et al. 1995) Constituent boundaries are shown through syntactic node labels in the trees. In the simplest case, a node will contain an entire constituent, complete with any associated arguments or modifiers. However, in structures involving syntactic movement, sub-constituents may be dis"
W06-0609,J05-1004,1,0.470057,"Missing"
W10-4420,J03-4003,0,0.0613377,"lem of comparing different annotations of the same data, such as determining where gold trees and parser output differ. Another such case is that of comparing inter-annotator agreement files during corpus construction. In both cases the typical need is to recognize which syntactic structures the two sets of trees are agreeing or disagreeing on. For this purpose it would be useful to be able to state queries in a way that relates to the decisions that annotators actually make, or that a parser mimics. We refer to this earlier work for arguments that (parent, head, sister) relations as in e.g. (Collins, 2003) are not sufficient, and that what is needed is the ability to state queries in terms of small chunks of syntactic structure. The solution we take is to use an extracted tree grammar, inspired by Tree Adjoining Grammar 2 Elementary Tree Extraction The work described and all our examples are taken from the Arabic Treebank, part 3, v3.2 (ATB3-v3.2) (Maamouri et al., 2010). As discussed above, we are aiming for an analysis of the trees that is directly expressed in terms of the core syntactic constructions. Towards this end we utilize ideas from the long line of TAG-based research that aims to id"
W10-4420,N06-1024,1,0.853985,"lication of structure in a typical treebank representation. From the perspective of database organization, the representation of the etree templates can be perhaps be viewed as a type of database “normalization”, in which duplicate tree structure information is placed in a separate table. A significant aspect of this decomposition of the parse output is that the tree decomposition relies upon the presence of function tags to help determine the argument status of nodes, and therefore what should be included in an elementary tree. We therefore use a modification of Bikel parser as described in (Gabbard et al., 2006), so that the output contains function tags. However, inaccuracy in the function tag recovery by the parser could certainly affect the formation of the elementary trees resulting from Runs 1 and 2. We do not include empty categories for the parser output, while they are present in the Gold trees.9 There are 929 etree templates in total, combining those for the three versions, with those for Run 1 and Run 2 overlapping almost entirely. The extracted tokens, etree templates, etree instances, and derivation trees are stored in a MySQL database for later search. The derivation tree is implemented"
W10-4420,N10-1094,1,0.728979,"e elementary trees, can be used as part of this analysis of parallel annotations over the same text. Recent work has proposed the use of an extracted tree grammar as the basis for treebank analysis, in which queries are stated over the elementary trees, which are small chunks of syntactic structure. In this work we integrate search over the derivation tree with this approach in order to analyze differences between two sets of annotation on the same text, an important problem for parser analysis and evaluation of inter-annotator agreement. 1 Introduction In earlier work (Kulick and Bies, 2009; Kulick and Bies, 2010; Kulick et al., 2010) we have described the need for a treebank search capability that compares two sets of trees over the same tokens. Our motivation is the problem of comparing different annotations of the same data, such as determining where gold trees and parser output differ. Another such case is that of comparing inter-annotator agreement files during corpus construction. In both cases the typical need is to recognize which syntactic structures the two sets of trees are agreeing or disagreeing on. For this purpose it would be useful to be able to state queries in a way that relates to t"
W10-4420,C88-2147,0,0.398666,"ull tree is shown in Figure 1, and the extracted elementary trees2 and derivation tree in Figure 2. (The ˆ symbol at the node NP[t]-SBJ in tree #1 indicates that it is a substitution node.) The extracted trees are the four trees numbered #1–#4. These trees are in effect the nodes in the derivation tree showing how the four elementary trees connect to each other. We briefly mention three unusual features of this extraction, and refer the reader to (Kulick and Bies, 2009) for detail and justification.3 Figure 2: Elementary Trees and Derivation Tree for the Tree Decomposition in Figure 1 system (Vijay-Shanker and Joshi, 1988) to handle function tags. 2. Etree #2 consists of two anchors, rather than splitting up the tree decomposition further. This is because this is an instance of the idafa (”construct state”) construction in Arabic, in which two or more words are grouped tightly together. 3. During the extraction process, additional information is added to the nodes in some cases, as further attributes for the “top” and “bottom” information, parallel to the function tag information. In this case, the root of etree #2 has the “bottom” attribute IDAFATOP, meaning that it is the top of an idafa structure, and the lo"
W10-4420,P00-1058,0,\N,Missing
W13-2301,P11-2062,1,0.825864,"s, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknown/malformed annotations with"
W13-2301,N13-1049,1,0.895093,"Missing"
W13-2301,N13-1066,1,0.784259,"currently considers words out of context, such a correction is not preferred because it requires more character edits (see Figure 2). We acknowledge this to be a limitation and plan to address it in the future. 11 The Levenshtein edit distance is defined as the minimum number of single-character edits (insertion, deletion and substitution) required to change one string into the other. For Arabic words and morphemes, we modify the cost of substitutions involving two phonologically or orthographically similar letters to count as half edits. We acquire the list of such letter substitutions from Eskander et al. (2013), who report them as the most frequent source of errors in Egyptian Arabic orthography. We map all diacritic-only morphemes to empty morphemes in both ways at a cost of half edit also. For POS tag edit distance, we use the standard definition of Levenshtein edit distance. Edit cost is an area where a lot of tuning could be done and we plan to explore it in the future. R AW Analysis D IAC M ORPH POS L EM Incorrect Annotation hayiÂaj∼iluh ha+yi+Âaj∼il+uh ñÊg. AJ ë hyAjlw Correct Annotation HayiÂaj∼iluwA Ha+yi+Âaj∼il+uwA FUT_PART+IV3MS+IV+IVSUFF_SUBJ:P FUT_PART+IV3P+IV+IVSUFF_SUBJ:P Âaj∼ill1 Âaj∼"
W13-2301,P05-1071,1,0.660451,"to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknown/malformed annotations with the morphological analyzer they use, thus forcing a reading on the word to make the unknown/malformed annotation usable. In our work, we address the cleaning issue directly. We intend to make these automatic corrections and extensions available in the future so that they can be used in future disambiguation tools. Maamouri et al. (2009) describ"
W13-2301,P06-1086,1,0.865193,"Missing"
W13-2301,habash-etal-2012-conventional,1,0.831293,"y and affixationally, and several classes of attachable clitics. For example, the Egyptian Arabic word AëñJ.JºJ ëð wi+ha+yi-ktib-uw+hA3 ‘and they will write it’ has two proclitics (+ ð wi+ ‘and’ and + è ha+ ‘will’), one prefix - ø yi- ‘3rd person’, one suffix ð- -uw ‘masculine plural’ and one pronominal enclitic Aë+ +hA ‘it/her’. The word is considered an inflected form of the lemma katab ‘write [lit. he wrote]’. An important challenge for NLP work on dialectal Arabic in general is the lack of an orthographic standard. Egyptian Arabic writers are often inconsistent even in their own writing (Habash et al., 2012a), e.g., the future particle h Ha appears as a separate word or as a proclitic + h/+ë Ha+/ha+, reflecting different pronunciations. Arabic orthography in general drops diacritical marks that mark short vowels and gemination. However in analyses, we want these diacritics to be indicated. Moreover, some letters in Arabic (in general) are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corresponding to multiple words), e.g., vari ˇ are often writants of Hamzated Alif, @ Â or @ A, ten without their Hamza ( Z ’): @"
W13-2301,W12-2301,1,0.867863,"y and affixationally, and several classes of attachable clitics. For example, the Egyptian Arabic word AëñJ.JºJ ëð wi+ha+yi-ktib-uw+hA3 ‘and they will write it’ has two proclitics (+ ð wi+ ‘and’ and + è ha+ ‘will’), one prefix - ø yi- ‘3rd person’, one suffix ð- -uw ‘masculine plural’ and one pronominal enclitic Aë+ +hA ‘it/her’. The word is considered an inflected form of the lemma katab ‘write [lit. he wrote]’. An important challenge for NLP work on dialectal Arabic in general is the lack of an orthographic standard. Egyptian Arabic writers are often inconsistent even in their own writing (Habash et al., 2012a), e.g., the future particle h Ha appears as a separate word or as a proclitic + h/+ë Ha+/ha+, reflecting different pronunciations. Arabic orthography in general drops diacritical marks that mark short vowels and gemination. However in analyses, we want these diacritics to be indicated. Moreover, some letters in Arabic (in general) are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corresponding to multiple words), e.g., vari ˇ are often writants of Hamzated Alif, @ Â or @ A, ten without their Hamza ( Z ’): @"
W13-2301,N13-1044,1,0.931926,"ust the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknown/malformed annotations with the morphological analyzer they use, thus forcing a reading on the word to make the unknown/malformed annotation usable. In our work, we address the cleaning issue directly. We intend to make these automatic corrections and extensions available in the future so that they can be used in future disambiguation tools. Maamouri et al. (2009) described a set of manual and automatic techniques used to improve on the quality of"
W13-2301,I08-2131,0,0.0212473,"on 6 presents some statistics on the Egyptian Arabic corpus annotated in one unified representation resulting from our correction and extension work. 1 ARZ is the language code for Egyptian Arabic, http://www-01.sil.org/iso639-3/ documentation.asp?id=arz 2 Related Work Much work has been done on automatic spelling correction. Both supervised and unsupervised approaches have been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependen"
W13-2301,N06-2015,0,0.0158196,"g a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknow"
W13-2301,W06-1648,0,0.0236005,"Arabic corpus annotated in one unified representation resulting from our correction and extension work. 1 ARZ is the language code for Egyptian Arabic, http://www-01.sil.org/iso639-3/ documentation.asp?id=arz 2 Related Work Much work has been done on automatic spelling correction. Both supervised and unsupervised approaches have been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological inf"
W13-2301,J93-2004,0,0.0472249,"urces developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. We present two methods for automatic correction and extension of morphological annotations, and demonstrate their success on three divergent Egyptian Arabic corpora. 1 Introduction Annotated corpora are essential for most research in natural language processing (NLP). For example, the development of treebanks, such as the Penn Treebank and the Penn Arabic Treebank, has been essential in pushing research on partof-speech (POS) tagging and parsing of English and Arabic (Marcus et al., 1993; Maamouri et al., 2004). The creation of such resources tends to be quite expensive and time consuming: guidelines need to be developed, annotators hired, trained, and regularly evaluated for quality control. For languages with complex morphologies, limited resources and tools, and/or lack of standard grammars, such as any of the Dialectal Arabic (DA) varieties, developing annotated resources can be a challenging task. As a result, annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. In the extremes, we fi"
W13-2301,mohamed-etal-2012-annotating,0,0.0579208,"tations: corrections of rich noisy annotations and extensions of clean but shallow ones. We present our work on Egyptian Arabic, an important Arabic dialect with limited resources, and rich and ambiguous morphology. Resulting from this effort is the largest Egyptian Arabic corpus annotated in one common representation by pooling resources from three very different sources: a non-final, pre-release version of the ARZ1 corpora from the Linguistic Data Consortium (LDC) (Maamouri et al., 2012g), the LDC’s CallHome Egypt transcripts (Gadalla et al., 1997) and CMU’s Egyptian Arabic corpus (CMUEAC) (Mohamed et al., 2012). Although the paper focuses on Arabic, the basic problem is relevant to other languages, especially spontaneously written colloquial language forms such as those used in social media. The general solutions we propose are language independent given availability of specific language resources. Next we discuss some related work and relevant linguistic facts (Sections 2 and 3, respectively). Section 4 presents our annotation correction technique; and Section 5 presents out annotation extension technique. Finally, Section 6 presents some statistics on the Egyptian Arabic corpus annotated in one un"
W13-2301,J96-1003,0,0.0812661,"n extension technique. Finally, Section 6 presents some statistics on the Egyptian Arabic corpus annotated in one unified representation resulting from our correction and extension work. 1 ARZ is the language code for Egyptian Arabic, http://www-01.sil.org/iso639-3/ documentation.asp?id=arz 2 Related Work Much work has been done on automatic spelling correction. Both supervised and unsupervised approaches have been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with t"
W13-2301,palmer-etal-2008-pilot,1,0.772222,"ve been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “"
W13-2301,H05-1060,0,0.0446191,"Missing"
W14-2904,N13-1061,1,0.891051,"Missing"
W14-3612,W14-1604,1,0.873348,"buṣṣ/ “look” is  أنظر/’unZur/ in MSA. 3 per as part of the automatic transliteration step because they target the same conventional orthography of dialectal Arabic (CODA) (Habash et al., 2012a, 2012b), which we also target. There are several commercial products that convert Arabizi to Arabic script, namely: Microsoft Maren, 2 Google Ta3reeb, 3 Basis Arabic chat translator4 and Yamli.5 Since these products are for commercial purposes, there is little information available about their approaches, and whatever resources they use are not publicly available for research purposes. Furthermore, as Al-Badrashiny et al. (2014) point out, Maren, Ta3reeb and Yamli are primarily intended as input method support, not full text transliteration. As a result, their users’ goal is to produce Arabic script text not Arabizi text, which affects the form of the romanization they utilize as an intermediate step. The differences between such “functional romanization” and real Arabizi include that the users of these systems will use less or no code switching to English, and may employ character sequences that help them arrive at the target Arabic script form faster, which otherwise they would not write if they were targeting Arab"
W14-3612,W12-4808,0,0.117255,"(MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al. (2014). All of these approaches rely on a model for character-to-character mapping that is used to generate a lattice of multiple alternative words which are then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here can help substantially improve the quality of such system. We use the system of Al-Badrashiny et al. (2014) in this pa2 http://www.getmaren.com http://www.google.com/ta3reeb 4 http://www.basistech.com/arabic-chat-trans"
W14-3612,R13-1026,0,0.0169932,"d topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communication. 4.1 In BOLT Phase 2"
W14-3612,W14-3901,1,0.757221,"Missing"
W14-3612,N13-1066,1,0.386912,"&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communication. 4.1 In BOLT Phase 2, LDC collected large volumes of naturally occurring informal text (SMS) and chat messages from individual users in English, Chinese and Egyptian Arabic (Song et al., 2014). Altogether we recruited 46 Egyptian Arabic participants, and of those 26 contributed data. To protect privacy, participation was"
W14-3612,P11-2008,0,0.0502639,"Missing"
W14-3612,W11-0704,0,0.0923825,"mixture model that is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level"
W14-3612,habash-etal-2012-conventional,1,0.702911,"cation in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary is informal with inte"
W14-3612,W12-2301,1,0.860943,"cation in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary is informal with inte"
W14-3612,P97-1017,0,0.0276316,"tion. As a result, their users’ goal is to produce Arabic script text not Arabizi text, which affects the form of the romanization they utilize as an intermediate step. The differences between such “functional romanization” and real Arabizi include that the users of these systems will use less or no code switching to English, and may employ character sequences that help them arrive at the target Arabic script form faster, which otherwise they would not write if they were targeting Arabizi (Al-Badrashiny et al., 2014). Name Transliteration There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic. Although the general goal of transliterating from one script to another is shared between these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter"
W14-3612,maamouri-etal-2014-developing,1,0.720718,"ect Arabizi is used to write in multiple dialects of Arabic, and differences between the dialects themselves have an effect on the spellings chosen by individual writers using Arabizi. Because Egyptian Arabic is the dialect of the corpus cre1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op erational_Language_Translation_%28BOLT%29.aspx 94 ated for this project, we will briefly discuss some of the most relevant features of Egyptian Arabic with respect to Arabizi transliteration. For a more extended discussion of the differences between MSA and Egyptian Arabic, see Habash et al. (2012a) and Maamouri et al. (2014). Phonologically, Egyptian Arabic is characterized by the following features, compared with MSA: (a) The loss of the interdentals /ð/ and /θ/ which are replaced by /d/ or /z/ and /t/ or /s/ respectively, thus giving those two original consonants a heavier load. Examples include  ذكر/zakar/ “to mention”,  ذبح/dabaħ/ “to slaughter”,  ثلج/talg/ “ice”,  ثمن/taman/ “price”, and  ثبت/sibit/ “to stay in place, become immobile”. (b) The exclusion of /q/ and /ǰ/ from the consonantal system, being replaced by the /ʔ/ and /g/, e.g.,  قطن/ʔuṭn/ “cotton”, and جمل /gamal/ “camel”. At the level"
W14-3612,pasha-etal-2014-madamira,1,0.89422,"Missing"
W14-3612,D11-1141,0,0.0398961,"is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communic"
W14-3612,W11-2602,1,0.567837,"013). Social media communication in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary i"
W14-3612,voss-etal-2014-finding,0,0.0375887,"then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here can help substantially improve the quality of such system. We use the system of Al-Badrashiny et al. (2014) in this pa2 http://www.getmaren.com http://www.google.com/ta3reeb 4 http://www.basistech.com/arabic-chat-translatortransforms-social-media-analysis/ 5 http://www.yamli.com/ 3 95 multilingual documents using a generative mixture model that is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic y"
W14-3612,P11-2007,0,0.0118384,"ere has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic. Although the general goal of transliterating from one script to another is shared between these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al."
W14-3612,Q14-1003,0,0.0204432,"een these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al. (2014). All of these approaches rely on a model for character-to-character mapping that is used to generate a lattice of multiple alternative words which are then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here"
W14-3612,W02-0505,0,\N,Missing
W14-3612,W14-3629,0,\N,Missing
W14-3612,song-etal-2014-collecting,1,\N,Missing
W14-3612,N06-1060,0,\N,Missing
W15-0809,doddington-etal-2004-automatic,1,0.296897,"nre NW DF NW DF Documents 77 74 101 99 Tokens 44,962 70,427 50,997 169,740 Table 1. Event Nugget Data Profile While the Light ERE and KBP Event Argument tasks rely on character offsets for annotation and scoring, the Event Nugget Tuple Scorer 2 (Liu, Mitamura & Hovy, 2015) requires tokenized data. Therefore, prior to annotation, all selected documents were automatically tokenized in the Penn English Treebank style. No manual correction was performed on the tokenization due to time constraints. 8 8.1 Corpus and Consistency Analysis Corpus Experience with event annotation for Light ERE and ACE (Doddington et al., 2004) and related tasks suggests that a major challenge for annotation consistency is poor recall – human annotators are not highly consistent in recognizing that a mention has occurred. To reduce the impact of this known issue for the Event Nugget task, two anno2 Event Nugget Tuple refers to the tuple made up of the nugget, event type/subtype, and realis. tators independently labeled each document (two first pass annotation passes, referred to as FP1 and FP2 below); a senior annotator then adjudicated discrepancies to create a gold standard. The team consisted of four first pass annotators, two of"
W15-0809,W15-0807,1,0.678904,"ed improvement in annotation quality in the workflow by comparing the adjudicated (ADJ) and first (FP1 and FP2) passes, shown in the columns ADJ vs. FP1 and FP2 in Table 3. The noticeable improvement in score shows the advantage of including adjudication as part of the annotation process. (For IAA purposes, there is obviously no gold or system, but in order to use the scorer we arbitrarily treated one file as the “gold”.) FP1 vs. FP2 Consistency Analysis We examined annotation consistency and quality by comparing different passes of the eval set annotation using the Event Nugget Tuple Scorer (Liu, Mitamura, & Hovy, 2015) developed for the event nugget evaluation task. This scorer treats one file as “gold” and the other as “system”, and matches each nugget in the gold file to one or more nuggets in the system file. This mapping is based on the overlap of the nugget spans. By nugget span, we 3 16 event nuggets in the training set did not receive a realis attribute, due to annotation error. 72 ADJ vs. FP1 78.2 71.7 63.2 ADJ vs. FP2 Span 69.0 89.3 Type 68.2 84.3 Realis 60.0 85.7 Table 3. Scores for Event Nugget Eval Set Annotation To gain some further insight into these numbers we expanded the analysis in two di"
W15-0809,E12-2021,0,\N,Missing
W15-0812,bies-etal-2014-incorporating,1,0.734052,"r Light ERE annotation effort also includes creating fully annotated resources in Chinese and Spanish in addition to English, with a portion of the annotation being cross-lingual. We developed a Chinese-English parallel Light ERE corpus which consists of approximately 100K words of Chinese data along with the corresponding English translation, both annotated in Light ERE. Portions of the parallel data have had other layers of annotation performed on it, particularly Chinese Treebank (CTB) on the Chinese side (Zhang and Xue, 2012) as well as English-Chinese Treebank (ECTB) on the English side (Bies et al., 2014). Light ERE annotation is in progress for Spanish on a dataset which is currently being annotated for Spanish Treebank as well. Multiple levels of annotation, such as ERE and treebank, that are keyed to the same dataset should together provide a resource that is expected to facilitate experimentation with machine learning methods that jointly manipulate the multiple levels. 2.2 TAC KBP Event Evaluations The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST) that was developed to encourage research in natural language p"
W15-0812,doddington-etal-2004-automatic,1,0.813222,"nships among them. Given the variety of approaches and evaluations within DEFT, we set out to define an annotation task that would be supportive of multiple research directions and evaluations, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased a"
W15-0812,W14-2904,1,0.767479,"to be met by the end of this year. 4.1 Smart Data Selection In an attempt to minimize annotator effort on documents with insufficient content, documents were fed into the annotation pipeline in descending order of event trigger density, defined as the number of event triggers per 1,000 tokens. Triggers were automatically tagged using a deep neural network based tagger trained on the ACE 2005 annotations (Walker et al., 2006) with orthographic and word 96 Rich ERE Challenges and Next Steps Inter-Annotator Agreement Work on inter-annotator agreement (IAA) will be based on the method outlined in Kulick et al. (2014), which described a matching algorithm used at each level of the annotation hierarchy, from entity mentions to events. This work focused on the evaluation for entity, relation, and event mentions, as well as for entities overall. The algorithm for entity mention mapping is based on the span for an entity mention, while the mapping for relation and event mentions is more complex, based on the mapping of the arguments, which in turn depends on the entity mention mapping. IAA work will be conducted on dual annotation for Rich ERE. Analysis will be reported in the future. 5 Conclusion Rich ERE ann"
W15-0812,W15-0809,1,0.3802,"collection and add it to a new or existing knowledge base. In 2014, TAC KBP moved into the events domain with the addition of the Event Argument Extraction (EAE) evaluation, in which systems were required to extract mentions of entities from unstructured text and indicate the roles they played in events as supported by text (Ellis et al., 2014). Additionally, TAC KBP 2014 also conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute (Mitamura et al., 2015). TAC KBP 2015 EAE and END evaluations both plan to expand the tasks such that event tuples would be grouped together or linked to one another to show event identity, either by linking event arguments that participate in the same event (EAE) or by grouping event nuggets that refer to the same event (END). Such expansion in both evaluations would require identification of event coreference, which is a challenging issue in both ACE and Light ERE. The transition from Light ERE to Rich ERE tackles this challenge with the addition of event hoppers. 3 Transition from Light ERE to Rich ERE The simpli"
W15-0812,J14-4007,0,0.017686,"ons, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased approach, starting with a fairly lightweight implementation and introducing additional complexity over time. In the first phase of the program, we defined Light ERE as a simplified form"
W15-0812,W11-0419,0,0.0236293,"election process have been very encouraging, with annotators reporting much richer documents on average, compared to the prior approach in which no ranking was imposed. 4.2 One of the challenges in event annotation is to determine the level of granularity that will be distinguished as sub-event vs. event hopper. We observed this issue in our pilot Rich ERE annotation, and the goal is to have sub-event annotation be a relationship between event hoppers in the future. In order to represent the relations between event hoppers, we are planning the addition of a notion such as Narrative Container (Pustejovsky and Stubbs, 2011) to capture non-identity eventevent relations such as causality, part-whole, precedence, enablement, etc. Event hoppers will serve as a level between individual event mentions and Narrative Containers. Event hoppers will be grouped into Narrative Containers, and so relations will be between event hoppers, instead of between individual event mentions. More specific relations between individual event mentions can then be derived from the event-event relations between the event hoppers within narrative containers or from relations between narrative containers. 4.3 The overall target for this phas"
W15-0812,strassel-etal-2010-darpa,1,0.908256,"notation task that would be supportive of multiple research directions and evaluations, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased approach, starting with a fairly lightweight implementation and introducing additional complexity over ti"
W15-0812,wright-etal-2012-annotation,1,0.855178,"ent hopper in Rich ERE, and all tagged event mentions that refer to the same event occurrence will be grouped into the same event hopper. Event hoppers will allow annotators to group together more event mentions and therefore also label more event arguments in Rich ERE. This richer annotation will lead to a more complete knowledge base and better support for the Event Argument Linking and END evaluations in 2015, when one of the goals is to evaluate event identity. 3.2 Development of an Annotation GUI for Rich ERE The Rich ERE annotation tool was developed following the framework described in Wright et al. (2012), allowing for rapid development of a new interface for Rich ERE. Numerous features were included “for free” in that they were developed for previous interfaces, and therefore required no additional development time. One important example of this is the representation of annotated text extents with underlines that can overlap arbitrarily, be color coded based on other annotations (e.g., entity type), and allow the user to click to navigate among the annotations. An important feature developed specifically for the Rich ERE tool is a “reference annotation”, which is essentially one widget pointi"
W15-0812,W12-6306,0,0.0230474,"ACE, only attested actual events are annotated (no irrealis events or arguments). Our Light ERE annotation effort also includes creating fully annotated resources in Chinese and Spanish in addition to English, with a portion of the annotation being cross-lingual. We developed a Chinese-English parallel Light ERE corpus which consists of approximately 100K words of Chinese data along with the corresponding English translation, both annotated in Light ERE. Portions of the parallel data have had other layers of annotation performed on it, particularly Chinese Treebank (CTB) on the Chinese side (Zhang and Xue, 2012) as well as English-Chinese Treebank (ECTB) on the English side (Bies et al., 2014). Light ERE annotation is in progress for Spanish on a dataset which is currently being annotated for Spanish Treebank as well. Multiple levels of annotation, such as ERE and treebank, that are keyed to the same dataset should together provide a resource that is expected to facilitate experimentation with machine learning methods that jointly manipulate the multiple levels. 2.2 TAC KBP Event Evaluations The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards a"
W15-0812,W14-2907,1,\N,Missing
W15-1615,doddington-etal-2004-automatic,0,0.0419984,"m as well. However, the novel constructions that were present in the data required new guidelines, and some new features also had to be added to the annotation tool. In this case, developing entirely new annotation guidelines and tools would have been prohibitively expensive in both time and effort, and the combination of existing and new worked well for the project (Bies et al., 2012). Similarly, LDC developed Entities, Relations, and Events (ERE) annotation to support requirements in the DEFT program, including informal genres, and based that development on adapting existing ACE guidelines (Doddington et al., 2004). LDC first defined Light ERE as a simplified form of ACE annotation, with the goal of being able to rapidly produce consistently labeled data in multiple languages (Aguilar et al., 2014), taking advantage of the taxonomy and distinctions developed for ACE. In a second phase of development, Rich ERE expanded entity, relation and event ontologies and also expanded the notion of what is taggable, to provide better support for evaluation tasks in the program. Rich ERE also introduced expanded event coreference with the notion of event hoppers, particularly with respect to event mention and event"
W15-1615,W13-2301,1,0.928953,"relation and event ontologies and also expanded the notion of what is taggable, to provide better support for evaluation tasks in the program. Rich ERE also introduced expanded event coreference with the notion of event hoppers, particularly with respect to event mention and event argument granularity variation (Song et al., 2015). Treebank and ERE guidelines that have been completed for English have been later adapted for 141 other languages as well – for example, Modern Standard Arabic and also dialectal Arabic treebanks (Maamouri and Bies, 2004; Maamouri et al, 2014; Maamouri et al., 2006; Eskander et al., 2013), as well as Chinese and Spanish ERE (Song et al., 2015). Clearly, new guidelines are necessary to account for language-specific constructions for each language and annotation task, but developing them based on existing guidelines for another language is a considerable head start. How do you decide on the granularity of the distinctions you choose to annotate? Give examples. We aim for a level of granularity in annotation distinctions that is  Consistent with goals of the annotation task and the guidelines  Useful for downstream users of the data or additional downstream annotation  Possibl"
W15-1615,W04-1602,1,0.685625,"for ACE. In a second phase of development, Rich ERE expanded entity, relation and event ontologies and also expanded the notion of what is taggable, to provide better support for evaluation tasks in the program. Rich ERE also introduced expanded event coreference with the notion of event hoppers, particularly with respect to event mention and event argument granularity variation (Song et al., 2015). Treebank and ERE guidelines that have been completed for English have been later adapted for 141 other languages as well – for example, Modern Standard Arabic and also dialectal Arabic treebanks (Maamouri and Bies, 2004; Maamouri et al, 2014; Maamouri et al., 2006; Eskander et al., 2013), as well as Chinese and Spanish ERE (Song et al., 2015). Clearly, new guidelines are necessary to account for language-specific constructions for each language and annotation task, but developing them based on existing guidelines for another language is a considerable head start. How do you decide on the granularity of the distinctions you choose to annotate? Give examples. We aim for a level of granularity in annotation distinctions that is  Consistent with goals of the annotation task and the guidelines  Useful for downs"
W15-1615,maamouri-etal-2006-developing,1,0.76901,"h ERE expanded entity, relation and event ontologies and also expanded the notion of what is taggable, to provide better support for evaluation tasks in the program. Rich ERE also introduced expanded event coreference with the notion of event hoppers, particularly with respect to event mention and event argument granularity variation (Song et al., 2015). Treebank and ERE guidelines that have been completed for English have been later adapted for 141 other languages as well – for example, Modern Standard Arabic and also dialectal Arabic treebanks (Maamouri and Bies, 2004; Maamouri et al, 2014; Maamouri et al., 2006; Eskander et al., 2013), as well as Chinese and Spanish ERE (Song et al., 2015). Clearly, new guidelines are necessary to account for language-specific constructions for each language and annotation task, but developing them based on existing guidelines for another language is a considerable head start. How do you decide on the granularity of the distinctions you choose to annotate? Give examples. We aim for a level of granularity in annotation distinctions that is  Consistent with goals of the annotation task and the guidelines  Useful for downstream users of the data or additional downstr"
W15-1615,maamouri-etal-2008-enhancing,1,0.852418,"Missing"
W15-1615,maamouri-etal-2014-developing,1,0.909597,"se of development, Rich ERE expanded entity, relation and event ontologies and also expanded the notion of what is taggable, to provide better support for evaluation tasks in the program. Rich ERE also introduced expanded event coreference with the notion of event hoppers, particularly with respect to event mention and event argument granularity variation (Song et al., 2015). Treebank and ERE guidelines that have been completed for English have been later adapted for 141 other languages as well – for example, Modern Standard Arabic and also dialectal Arabic treebanks (Maamouri and Bies, 2004; Maamouri et al, 2014; Maamouri et al., 2006; Eskander et al., 2013), as well as Chinese and Spanish ERE (Song et al., 2015). Clearly, new guidelines are necessary to account for language-specific constructions for each language and annotation task, but developing them based on existing guidelines for another language is a considerable head start. How do you decide on the granularity of the distinctions you choose to annotate? Give examples. We aim for a level of granularity in annotation distinctions that is  Consistent with goals of the annotation task and the guidelines  Useful for downstream users of the dat"
W15-1615,W15-0812,1,0.879008,"a simplified form of ACE annotation, with the goal of being able to rapidly produce consistently labeled data in multiple languages (Aguilar et al., 2014), taking advantage of the taxonomy and distinctions developed for ACE. In a second phase of development, Rich ERE expanded entity, relation and event ontologies and also expanded the notion of what is taggable, to provide better support for evaluation tasks in the program. Rich ERE also introduced expanded event coreference with the notion of event hoppers, particularly with respect to event mention and event argument granularity variation (Song et al., 2015). Treebank and ERE guidelines that have been completed for English have been later adapted for 141 other languages as well – for example, Modern Standard Arabic and also dialectal Arabic treebanks (Maamouri and Bies, 2004; Maamouri et al, 2014; Maamouri et al., 2006; Eskander et al., 2013), as well as Chinese and Spanish ERE (Song et al., 2015). Clearly, new guidelines are necessary to account for language-specific constructions for each language and annotation task, but developing them based on existing guidelines for another language is a considerable head start. How do you decide on the gra"
W15-1615,W14-2907,0,\N,Missing
W16-1004,W13-2322,1,0.735417,"me. RED also labels a causal and temporal relation between the two events, &quot;BEFORE/PRECONDITIONS&quot;, showing that the quitting event leads to, but does not directly cause, the replacement, and a temporal CONTAINS relation linking quit to Wednesday.     Event 1: quit - BEFORE DOCTIME, Actual Modality Event 2: replace - AFTER DOCTIME, Actual Modality Relation 1: quit BEFORE/ PRECONDITIONS replace Relation 2: Wednesday CONTAINS quit Although RED does not annotate the arguments of events, it is intended to be combined with semantic role annotations such as PropBank (Bonial et al., 2014) or AMR (Banarescu et al., 2013), which would provide the argument information. For this example, the quit and replace events would also be given the predicate argument structures below: quit.01 Arg0: Media Tycoon Barry Diller Arg1: as chief of Vivendi Universal Entertainment ArgM-TMP: on Wednesday replace.01 Arg2: Parent company chairman JeanRene Fourtou Arg1: Diller ArgM-MOD: will ArgM-PRD: as chief executive of US unit. EER: The following events are connected by Condition and Temporality relations:   Event 1 (Personnel.EndPosition): quit Event 2 (Personnel.StartPosition): replace 34 A preliminary analysis of the Rich ER"
W16-1004,W14-2903,1,0.927744,"Missing"
W16-1004,D12-1045,0,0.048737,"y – along with 21 sense-based subtypes (or relation senses), as shown in Table 1. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency_Causality will play Cause and Result roles respectively. Figure 1 shows more information about types and roles. Figure 1: Roles and examples specific to fine-grained event-event relation subtypes. 2.5 Richer Event Descriptions (RED) 3 RED annotation (Ikuta et al., 2014) marks all events in a document, as well as certain relations between those events. RED combines coreference (Pradhan et al., 2007; Lee et al., 2012) and THYME Temporal Relations annotation (Styler et al., 2014) to provide a thorough representation of entities, events and their relations. The RED schema also goes beyond prior annotations of coreference or temporal relations by also annotating subevent structure, cause-effect relations and reporting relations. Guidelines for RED annotation can be found at https://github.com/timjogorman/RicherEventDescr iption/blob/master/guidelines.md. 31 Annotation Data Annotated Features and The representation of events and the scope of annotation vary across the different annotation approaches. Table 2 c"
W16-1004,W15-0809,1,0.776814,"verlapping data set could be used to explore how the differences in annotation procedure lead to differences in decisions about event granularity. 2.3 Event Nugget (EN) An Event Nugget is a tuple of an event trigger, classification of event type and subtype, and realis attribute. It is similar to an event mention in ERE, but arguments are not labelled. EN annotation in 2014 focused on event nuggets (expanded triggers) only, and followed the same taxonomy of 33 event types and subtypes as Light ERE. However, instead of tagging minimal extent as the trigger, EN allowed multi-word event nuggets (Mitamura et al., 2015). Multi-word event nuggets can be either continuous or discontinuous, and are based on the goal of marking the maximal extent of a semantically meaningful unit to express the event in a sentence. EN also added a realis attribute for each event mention. The realis attribute labels each event as Actual, Generic, or Other. TAC KBP 2014 conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute. 30 Table 1: EER event relation types. In 2015,"
W16-1004,W15-0812,1,0.92996,"e taggable, and entity subtypes are not labeled). The event ontology of Light ERE is similar to ACE, with slight modification and reduction, and there is strict coreference of events within documents (Aguilar et al., 2014). As in ACE, the annotation of each event mention includes the identification of a trigger, the labeling of the event type, subtype, and participating event argument entities and time expressions. Simplifying from ACE, only attested actual events are annotated (no irrealis events or arguments). Rich ERE annotation expands on both the inventories and taggability of Light ERE (Song et al., 2015). Rich ERE Entity annotation adds nonspecific entities and nominal head marking, in addition to adding a distinction between Location and Facility entity types. Rich ERE Relation annotation doubles the Light ERE ontology to twenty relation subtypes, and also adds future, hypothetical, and 28 conditional relations. A new category of argument fillers was added for Rich ERE, to allow arguments that are not taggable as entities to be used as fillers for specific relation and event subtypes. For each event mention, Rich ERE labels the event type and subtype, its realis attribute, any of its argumen"
W16-1004,W16-1005,1,0.820435,"event relation types. In 2015, TAC KBP ran an open evaluation on EN that was expanded to three evaluation tasks: Event Nugget Detection, Event Nugget Detection and Coreference, and Event Nugget Coreference. Full Event Nugget Coreference is identified when two or more Event Nuggets refer to the same event. EN annotation in 2015 followed the Rich ERE event taxonomy, which added 5 event types and subtypes to make a total of 38 event types and subtypes, and also followed the Rich ERE guidelines on trigger extents, which adopted the minimal extent rule and disallowed discontinuous event triggers (Song et al., 2016). Annotation of Event Nugget Coreference adopted the concept of Event Hopper as in Rich ERE. 2.4 Event-Event Relations (EER) EER annotation focuses on relations between events in the ERE/ACE taxonomy, both within document and cross-document (Hong et al., 2016). Our general goal is to construct event-centric knowledge networks, where each node is an event and the edges effectively capture the relations between any two events. EER includes five main types of event relations – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 sense-based subtypes (or relation senses)"
W16-1005,babko-malaya-etal-2012-identifying,0,0.0482332,"Missing"
W16-1005,doddington-etal-2004-automatic,1,0.709108,"nault chief George Besse in 1986 and the head of government arms sales Rene Audran a year earlier. In 2015 EN annotation, the word “murder” would be the trigger for two Life.Die events, one with the victim “George Besse” and the other with “Rene Audran” as well as two Conflict.Attack events, one occurring in 1986 and the other in 1985. In 2014 EN annotation, the word “murder” would be the trigger for only one Conflict.Attack event. 3.2 Event Taxonomy EN annotation and evaluation focus on a limited inventory of event types and subtypes, as defined in ERE, based on Automatic Content Extraction (Doddington et al., 2004; Walker et al., 2006; Aguilar et 39 al., 2014; Song et al., 2015). The 2014 EN evaluation covered the inventory of event types and subtypes from Light ERE, including 8 event types and 33 subtypes. The 2015 evaluation added a new event type (Manufacture) and four new subtypes – Movement.TransportArtifact, Contact.Broadcast, Contact.Contact, Transaction.Transaction – which aligned the EN event ontology with that of Rich ERE in order to take advantage of the existing Rich ERE annotated data as training data. The EN annotation task also adopted a new approach for applying the Contact event subtyp"
W16-1005,W15-0807,1,0.796291,"Missing"
W16-1005,W15-0809,1,0.896858,"these annotations, except the annotation of event arguments. The EN task in 2014 adapted the event annotation guidelines from the Light ERE annotation task (Aguilar, et al., 2014) by incorporating modifications by the evaluation coordinators that focused on the text extents establishing valid references to events, clarifications on transaction event types, and the additional annotation of event realis attributes, which indicated whether each event mention was asserted (Actual), generic or habitual (Generic), or some other category, such as future, hypothetical, negated, or uncertain (Other) (Mitamura, et al., 2015) . In 2015, EN annotation followed the Rich ERE Event annotation guidelines (except for the annotation of event arguments). As compared to EN annotation in 2014, Rich ERE Event annotation and 2015 EN annotation include increased taggability in several areas: slightly expanded event ontology, additional attributes for contact and transaction events, and double tagging of event mentions for multiple types/subtypes and for certain types of coordination, in addition to event coreference.General Instructions 3 Event Nugget Annotation In this section, we describe the EN annotation as well as the maj"
W16-1005,N04-1019,0,0.189396,"Missing"
W16-1005,W15-0812,1,0.922361,"icipating systems must identify full event coreference links, given the annotated event nuggets in the text. ERE was developed as an annotation task that would be supportive of multiple research directions and evaluations in the DEFT program, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting ERE annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text (Light ERE), to a richer representation of phenomena of interest to the program (Rich ERE) (Song, et al., 2015). In ERE Event annotation, each event mention has annotation of event type and subtype, its realis attribute, any of its arguments or participants that are present, and a required “trigger” string in the text; furthermore, event mentions within the same document are coreferenced into event hoppers (Song, et al., 2015). EN annotation includes all of these annotations, except the annotation of event arguments. The EN task in 2014 adapted the event annotation guidelines from the Light ERE annotation task (Aguilar, et al., 2014) by incorporating modifications by the evaluation coordinators that fo"
W16-1005,W14-2907,1,\N,Missing
W19-6808,L18-1293,0,0.0456535,"Missing"
W19-6808,kulick-etal-2010-consistent,1,0.673071,"Missing"
