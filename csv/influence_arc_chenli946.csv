2020.aacl-main.61,P16-1046,0,0.0267521,"the rewriter. In addition, we observe that when training the model in a tem2tem way, we can get better NMS and EMS, which implies that training by tem2tem can improve the correctness of summaries. 5 Related Work Text summarization. Existing approaches can be grouped into two families: extractive models and abstractive models. Extractive models select a part of sentences from the source document as the summary. Traditional approaches (Carbonell and Goldstein, 1998; Erkan and Radev, 2004; McDonald, 2007) utilize graph or optimization techniques. Recently, neural models achieve good performance (Cheng and Lapata, 2016; Nallapati et al., 2017; Jadhav and Rajan, 2018). Abstractive summarization models aim to rephrase the source document. Most work applies neural models for this task. (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016; See et al., 2017; Gehrmann et al., 2018). Factual correctness of summaries. There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries (Falke et al., 2019; Kryscinski et al., 2019; Wang et al., 2020; Maynez et al., 2020; Zhu et al., 2020). Data-to-Text generation. Recently, generating news articles from"
2020.aacl-main.61,N16-1012,0,0.03086,"k Text summarization. Existing approaches can be grouped into two families: extractive models and abstractive models. Extractive models select a part of sentences from the source document as the summary. Traditional approaches (Carbonell and Goldstein, 1998; Erkan and Radev, 2004; McDonald, 2007) utilize graph or optimization techniques. Recently, neural models achieve good performance (Cheng and Lapata, 2016; Nallapati et al., 2017; Jadhav and Rajan, 2018). Abstractive summarization models aim to rephrase the source document. Most work applies neural models for this task. (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016; See et al., 2017; Gehrmann et al., 2018). Factual correctness of summaries. There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries (Falke et al., 2019; Kryscinski et al., 2019; Wang et al., 2020; Maynez et al., 2020; Zhu et al., 2020). Data-to-Text generation. Recently, generating news articles from different kinds of data-records becomes a popular research direction. Wiseman et al. (2017); Puduppully et al. (2019) focus on generating news from boxed-data. Zhang et al. (2016) and Yao et al. (2017) study"
2020.aacl-main.61,D15-1044,0,0.338259,", generating news from live commentaries has gradually attracted attention in the academic community (Zhang et al., 2016; Yao et al., 2017). At the same time, several trials have been done in the industry such as sports news from Toutiao’s Xiaoming Bot2 , Sohu Ruibao3 and AI football news4 . 1 The dataset is available at https://github.com/ ej0cl6/SportsSum 2 http://www.nbd.com.cn/columns/803 3 https://mp.sohu.com/profile?xpt= c29odW1wMzZpdDlzQHNvaHUuY29t 4 https://www.51zhanbao.com Table 1: An example of S PORTS S UM dataset. Unlike traditional text summarization tasks (Hermann et al., 2015; Rush et al., 2015), the source documents and the target summaries for sports game summarization tasks are written in quite different styles. Live commentaries are the real-time transcripts of the commentators. Accordingly, commentary sentences are more colloquial and informal. In contrast, news summaries are usually more narrative and well-organized since they are written after the games. In addition, commentaries contain a large number of player names. One player can be referred to multiple times in the whole game, and one commentary sentence may mention multiple player names simultaneously. Those properties 6"
2020.aacl-main.61,P17-1099,0,0.256143,"lent if and only if 1) subject1 is the same as subject2 and 2) verb1 and verb2 are synonym7 to each other. Let Eg and Ep represent ˜ respectively, the event the set of events in R and R, matching score is defined as ˜ = F-score(Eg , Ep ). EMS(R, R) Implementations and Models. We consider the convolutional neural network (Kim, 2014) as the selector. For the rewriter, we consider the following: (1) LSTM: a bidirectional LSTM with attention mechanism (Bahdanau et al., 2015). (2) Transformer. (Vaswani et al., 2017) (3) PGNet: pointergenerator network, an encoder-decoder model with copy mechanism (See et al., 2017). 612 7 Details to decide synonyms can be found in Appendix B. Method Model ROUGE-1 ROUGE-2 ROUGE-L NMS EMS Extractive Models RawSent LTR 26.52 24.44 7.64 6.39 25.42 23.19 57.33 51.63 36.17 29.03 Abstractive Models Abs-LSTM Abs-PGNet 30.54 34.02 10.16 11.09 29.78 33.13 10.87 17.87 14.03 19.76 Selector + Rewriter (Seq2seq) LSTM Transformer PGNet 41.39 41.71 43.17 16.99 18.10 18.66 40.53 40.96 42.27 28.48 35.63 48.18 25.19 30.94 36.94 Selector + Rewriter (Tem2tem) LSTM Transformer PGNet 41.71 41.47 41.95 17.08 17.18 17.09 40.82 40.54 41.01 59.54 58.26 59.35 40.34 39.33 40.46 Table 6: Evaluation"
2020.aacl-main.61,D18-1443,0,0.0286439,"Missing"
2020.aacl-main.61,P82-1020,0,0.787369,"Missing"
2020.aacl-main.61,P18-1014,0,0.0221523,"training the model in a tem2tem way, we can get better NMS and EMS, which implies that training by tem2tem can improve the correctness of summaries. 5 Related Work Text summarization. Existing approaches can be grouped into two families: extractive models and abstractive models. Extractive models select a part of sentences from the source document as the summary. Traditional approaches (Carbonell and Goldstein, 1998; Erkan and Radev, 2004; McDonald, 2007) utilize graph or optimization techniques. Recently, neural models achieve good performance (Cheng and Lapata, 2016; Nallapati et al., 2017; Jadhav and Rajan, 2018). Abstractive summarization models aim to rephrase the source document. Most work applies neural models for this task. (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016; See et al., 2017; Gehrmann et al., 2018). Factual correctness of summaries. There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries (Falke et al., 2019; Kryscinski et al., 2019; Wang et al., 2020; Maynez et al., 2020; Zhu et al., 2020). Data-to-Text generation. Recently, generating news articles from different kinds of data-records becomes a popular"
2020.aacl-main.61,D14-1181,0,0.00323275,"ing and R, score as ˜ = F-score(Ng , Np ). NMS(R, R) Similarly, the event matching score evaluates the ˜ We define closeness of the events in R and R. an event as a pair (subject, verb) in the sentence. Two pairs (subject1 , verb1 ) and (subject2 , verb2 ) are viewed as equivalent if and only if 1) subject1 is the same as subject2 and 2) verb1 and verb2 are synonym7 to each other. Let Eg and Ep represent ˜ respectively, the event the set of events in R and R, matching score is defined as ˜ = F-score(Eg , Ep ). EMS(R, R) Implementations and Models. We consider the convolutional neural network (Kim, 2014) as the selector. For the rewriter, we consider the following: (1) LSTM: a bidirectional LSTM with attention mechanism (Bahdanau et al., 2015). (2) Transformer. (Vaswani et al., 2017) (3) PGNet: pointergenerator network, an encoder-decoder model with copy mechanism (See et al., 2017). 612 7 Details to decide synonyms can be found in Appendix B. Method Model ROUGE-1 ROUGE-2 ROUGE-L NMS EMS Extractive Models RawSent LTR 26.52 24.44 7.64 6.39 25.42 23.19 57.33 51.63 36.17 29.03 Abstractive Models Abs-LSTM Abs-PGNet 30.54 34.02 10.16 11.09 29.78 33.13 10.87 17.87 14.03 19.76 Selector + Rewriter (S"
2020.aacl-main.61,2020.acl-main.173,0,0.0141788,"raph or optimization techniques. Recently, neural models achieve good performance (Cheng and Lapata, 2016; Nallapati et al., 2017; Jadhav and Rajan, 2018). Abstractive summarization models aim to rephrase the source document. Most work applies neural models for this task. (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016; See et al., 2017; Gehrmann et al., 2018). Factual correctness of summaries. There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries (Falke et al., 2019; Kryscinski et al., 2019; Wang et al., 2020; Maynez et al., 2020; Zhu et al., 2020). Data-to-Text generation. Recently, generating news articles from different kinds of data-records becomes a popular research direction. Wiseman et al. (2017); Puduppully et al. (2019) focus on generating news from boxed-data. Zhang et al. (2016) and Yao et al. (2017) study generating sports news from live commentaries, but their methods are based on hand-crafted features. 6 Conclusion We present S PORTS S UM, a Chinese dataset for sports game summarization, as well as a model that consists of a selector and a rewriter. To improve the quality of generated news, we train the"
2020.aacl-main.61,2020.acl-main.450,0,0.0246821,"ld, 2007) utilize graph or optimization techniques. Recently, neural models achieve good performance (Cheng and Lapata, 2016; Nallapati et al., 2017; Jadhav and Rajan, 2018). Abstractive summarization models aim to rephrase the source document. Most work applies neural models for this task. (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Zeng et al., 2016; See et al., 2017; Gehrmann et al., 2018). Factual correctness of summaries. There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries (Falke et al., 2019; Kryscinski et al., 2019; Wang et al., 2020; Maynez et al., 2020; Zhu et al., 2020). Data-to-Text generation. Recently, generating news articles from different kinds of data-records becomes a popular research direction. Wiseman et al. (2017); Puduppully et al. (2019) focus on generating news from boxed-data. Zhang et al. (2016) and Yao et al. (2017) study generating sports news from live commentaries, but their methods are based on hand-crafted features. 6 Conclusion We present S PORTS S UM, a Chinese dataset for sports game summarization, as well as a model that consists of a selector and a rewriter. To improve the quality of generate"
2020.aacl-main.61,D17-1239,0,0.0619326,"Missing"
2020.aacl-main.61,W17-3504,0,0.111937,"the 12 meters ahead the goal line was touched out by Suboti´c. In the 13th minute, Ribery passed the ball from the right, and Kroos’s shot near the 27 meters ahead the goal line missed. (...) Introduction There are a large number of sports games playing every day. Apparently, manually writing sports news articles to summarize every game is laborintensive and infeasible. How to automatically generate sports summaries, therefore, becomes a popular and demanding task. Recently, generating news from live commentaries has gradually attracted attention in the academic community (Zhang et al., 2016; Yao et al., 2017). At the same time, several trials have been done in the industry such as sports news from Toutiao’s Xiaoming Bot2 , Sohu Ruibao3 and AI football news4 . 1 The dataset is available at https://github.com/ ej0cl6/SportsSum 2 http://www.nbd.com.cn/columns/803 3 https://mp.sohu.com/profile?xpt= c29odW1wMzZpdDlzQHNvaHUuY29t 4 https://www.51zhanbao.com Table 1: An example of S PORTS S UM dataset. Unlike traditional text summarization tasks (Hermann et al., 2015; Rush et al., 2015), the source documents and the target summaries for sports game summarization tasks are written in quite different styles"
2020.aacl-main.61,P16-1129,0,0.239494,"Muller’s shot from the 12 meters ahead the goal line was touched out by Suboti´c. In the 13th minute, Ribery passed the ball from the right, and Kroos’s shot near the 27 meters ahead the goal line missed. (...) Introduction There are a large number of sports games playing every day. Apparently, manually writing sports news articles to summarize every game is laborintensive and infeasible. How to automatically generate sports summaries, therefore, becomes a popular and demanding task. Recently, generating news from live commentaries has gradually attracted attention in the academic community (Zhang et al., 2016; Yao et al., 2017). At the same time, several trials have been done in the industry such as sports news from Toutiao’s Xiaoming Bot2 , Sohu Ruibao3 and AI football news4 . 1 The dataset is available at https://github.com/ ej0cl6/SportsSum 2 http://www.nbd.com.cn/columns/803 3 https://mp.sohu.com/profile?xpt= c29odW1wMzZpdDlzQHNvaHUuY29t 4 https://www.51zhanbao.com Table 1: An example of S PORTS S UM dataset. Unlike traditional text summarization tasks (Hermann et al., 2015; Rush et al., 2015), the source documents and the target summaries for sports game summarization tasks are written in qui"
2020.acl-main.392,P19-1033,1,0.583974,"sponding author:Chuan Shi(shichuan@bupt.edu.cn) https://news.google.com/ A core problem in news recommendation is how to learn better representations of users and news. Recently, many deep learning based methods have been proposed to automatically learn informative user and news representations (Okura et al., 2017; Wang et al., 2018). For instance, DKN (Wang et al., 2018) learns knowledge-aware news representation via multi-channel CNN and gets a representation of a user by aggregating her clicked news history with different weights. However, these methods (Wu et al., 2019b; Zhu et al., 2019; An et al., 2019) usually focus on news contents, and seldom consider the collaborative signal in the form of high-order connectivity underlying the user-news interactions. Capturing high-order connectivity among users and news could deeply exploit structure characteristics and alleviate the sparsity, thus improving the rec4255 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4255–4264 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ommendation performance (Wang et al., 2019). For example, as shown in Figure 1, the high-order relationship u1 –d1"
2020.acl-main.392,D14-1181,0,0.00246914,"documents that have not previously existed in the user-news interaction graph G during training or testing. Our model takes these news documents as isolated nodes in the graph G. Their representations are based on only content feature hd without neighbor aggregation, and can also be disentangled via Eq. 3. 5.2 Performance Evaluation We evaluate the performance of our model GNUD by comparing it with the following state-of-the-art baseline methods: LibFM (Rendle, 2012), a feature-based matrix factorization method, with the concatenation of TF-IDF vectors of news title and profile as input. CNN (Kim, 2014), applying two parallel CNNs to word sequences in news titles and profiles respectively and concatenate them as news features. The user representation is learned from the user’s news history. DSSM (Huang et al., 2013), a deep structured semantic model. In our experiments, we model the user’s clicked news as the query and the candidate news as the documents. Wide & Deep (Cheng et al., 2016), a deep model for recommendation which combines a (Wide) linear model and (Deep) feed-forward neural network. We also use the concatenation of news title and profile embeddings as features. DeepFM (Guo et al"
2020.acl-main.392,D19-1671,1,0.648001,"Missing"
2020.acl-main.392,D18-1430,1,0.833329,"latent variable which can be inferred in an iterative process. The motivation of the iterative process is as follows. Given zu,k , the value of the latent variables {rd,k : 1 ≤ k ≤ K, (u, d) ∈ E} can be obtained by measuring the similarity between user u and her clicked news d under the k-th subspace, which is computed as Eq. 4. Initially, we set zu,k = su,k . On the other hand, after obtaining the latent variables {rd,k }, we can find an estimate of zu,k by aggregating information from the clicked news, which is computed as Eq. 5: 4258 Algorithm 1 Neighborhood Routing Algorithm According to (Yang et al., 2018), the mutual information maximization can be converted to the following form. Given the representation of a user u in k-th (1 ≤ k ≤ K) latent subspace, the preference regularizer P (k|zu,k ) estimates the probability of the k-th subspace (w.r.t. the k-th preference) that zu,k belongs to: Require: S si,k , i ∈ {u} {d : (u, d) ∈ E}, 1 ≤ k ≤ K; Ensure: zu,k , 1 ≤ k ≤ K; 1: ∀k = 1, ...K, zu,k ← su,k 2: for T iterations do 3: for d that satisfies (u, d) ∈ E do 4: ∀k = 1, · · · , K : rd,k ← z> u,k sd,k 5: ∀k = 1, · · · , K : rd,k ← softmax(rd,k ) 6: end for 7: for factor k = 1, P 2, ...K do 8: zu,k"
2020.emnlp-main.509,2020.acl-main.175,0,0.0198387,"he original meaning can hinder the deployment of summarization techniques in real-world scenarios, as inaccurate and untruthful summaries can lead the readers to false conclusions (Cao et al., 2018; Falke et al., 2019; Lebanoff et al., 2019). We aim to produce summary highlights in this paper, which will be overlaid on source documents to allow summaries to be interpreted in context. Generation of summary highlights is of crucial importance to tasks such as producing informative snippets from search outputs (Kaisser et al., 2008), summarizing viewpoints in opinionated text (Paul et al., 2010; Amplayo and Lapata, 2020), and annotating website privacy policies to assist users in answering important questions (Sadeh et al., 2013). Determining the most appropriate textual unit for highlighting, however, has been an understudied problem. Extractive summarization selects whole sentences from documents; a sentence can contain 20 to 30 words on average (Kamigaito et al., 2018). Keyphrases containing two to three words are much less informative (Hasan and Ng, 2014). Neither are ideal solutions. There is a rising need for other forms of highlighting, and we explore subsentence highlights that strike a balance betwee"
2020.emnlp-main.509,P18-1063,0,0.0319353,"d as winter storms hit during one of the year’s busiest travel weeks. Self-Contained Segments • Some interstates are closed • hundreds of flights have been canceled as winter storms hit • flights have been canceled as winter storms hit • winter storms hit during one of the year’s busiest travel weeks Non-Self-Contained Segments • Some interstates are • closed and hundreds of flights have been • been canceled as winter storms hit during one of • hit during one of the year’s Table 2: Examples of self-contained and non-self-contained segments extracted from a document sentence. Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Laban et al., 2020). With greater flexibility comes increased risk. Failing to accurately convey the original meaning can hinder the deployment of summarization techniques in real-world scenarios, as inaccurate and untruthful summaries can lead the readers to false conclusions (Cao et al., 2018; Falke et al., 2019; Lebanoff et al., 2019). We aim to produce summary highlights in this paper, which will be overlaid on source documents to allow summaries to be interpreted in context. Generation of summary highlights is of crucial"
2020.emnlp-main.509,P16-1046,0,0.0775281,"Missing"
2020.emnlp-main.509,P19-1098,1,0.890225,"s process produces a collection of self-contained and partially-overlapping segments from a set of documents. Next, we assess the informativeness of the segments and leverage DPP to identify a subset to form the summary highlights. 3.2 semi-definite matrix and Lij indicates the correlation between segments i and j; LY is a submatrix of L containing only entries indexed by elements in Y ; I is the identity matrix. This definition suggests that the probability of a summary P(Y ; L) is proportional to the determinant of LY . Segment Selection with DPP We employ the modeling framework proposed by Cho et al. (2019a) for modeling determinantal point processes. DPP (Kulesza and Taskar, 2012) defines a probability measure P over all subsets (2|Y |) of a ground set containing a collection of N segments Y = {1, 2, · · · , N}. The probability of an extractive summary, containing a subset of the segments Y ⊆ Y, is defined by Eq. (1), where det(·) is the determinant of a matrix; L ∈ RN×N is a positive A decomposition exists for the L-ensemble matrix: Lij = qi · Sij · qj where qi ∈ R+ is a quality score of the i-th segment and Sij is a pairwise similarity score between segments i and j. If q and S are available"
2020.emnlp-main.509,D19-5412,1,0.914146,"s process produces a collection of self-contained and partially-overlapping segments from a set of documents. Next, we assess the informativeness of the segments and leverage DPP to identify a subset to form the summary highlights. 3.2 semi-definite matrix and Lij indicates the correlation between segments i and j; LY is a submatrix of L containing only entries indexed by elements in Y ; I is the identity matrix. This definition suggests that the probability of a summary P(Y ; L) is proportional to the determinant of LY . Segment Selection with DPP We employ the modeling framework proposed by Cho et al. (2019a) for modeling determinantal point processes. DPP (Kulesza and Taskar, 2012) defines a probability measure P over all subsets (2|Y |) of a ground set containing a collection of N segments Y = {1, 2, · · · , N}. The probability of an extractive summary, containing a subset of the segments Y ⊆ Y, is defined by Eq. (1), where det(·) is the determinant of a matrix; L ∈ RN×N is a positive A decomposition exists for the L-ensemble matrix: Lij = qi · Sij · qj where qi ∈ R+ is a quality score of the i-th segment and Sij is a pairwise similarity score between segments i and j. If q and S are available"
2020.emnlp-main.509,P19-1102,0,0.197446,"onto the positive semi-definite (PSD) cone to ensure that it satisfies the PSD property (§3.2). This is accomplished in two steps, where L0 is the new L-ensemble. P L = ni=0 λi vi vi> (Eigenvalue decomposition) P L0 = ni=0 max{λi , 0}vi vi> (PSD projection) R-1 R-2 R-SU4 DPP-BERT (Cho et al., 2019b) DPP (Kulesza and Taskar, 2012) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) Centroid (Hong et al., 2014) ICSISumm (Gillick and Favre, 2009) Opinosis (Ganesan et al., 2010) Pointer-Gen (See et al., 2017) CopyTrans (Gehrmann et al., 2018) Hi-MAP (Fabbri et al., 2019) 39.05 38.10 29.48 31.04 34.44 35.49 37.31 27.07 31.43 28.54 35.78 10.23 9.14 4.25 6.03 7.11 7.80 9.36 5.03 6.03 6.38 8.90 14.35 13.40 8.64 10.23 11.19 12.02 13.12 8.63 10.01 7.22 11.43 HL-TreeSegs (Our work) HL-XLNetSegs (Our work) 39.18 39.26 10.30 10.70 14.37 14.47 Table 3: Results on DUC-04 dataset evaluated by ROUGE. 4 Experiments 4.1 Data Sets Our data comes from NIST. We use them to investigate the feasibility of the proposed multi-document summarization method. Particularly, we use DUC03/04 (Over and Yen, 2004) and TAC-08/09/10/11 datasets (Dang and Owczarzak, 2008), which contain 60/5"
2020.emnlp-main.509,P19-1213,0,0.0364517,"Missing"
2020.emnlp-main.509,C10-1039,0,0.213358,"t summarization data by maximizing log-likelihood. At each iteration, we project the L-ensemble onto the positive semi-definite (PSD) cone to ensure that it satisfies the PSD property (§3.2). This is accomplished in two steps, where L0 is the new L-ensemble. P L = ni=0 λi vi vi> (Eigenvalue decomposition) P L0 = ni=0 max{λi , 0}vi vi> (PSD projection) R-1 R-2 R-SU4 DPP-BERT (Cho et al., 2019b) DPP (Kulesza and Taskar, 2012) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) Centroid (Hong et al., 2014) ICSISumm (Gillick and Favre, 2009) Opinosis (Ganesan et al., 2010) Pointer-Gen (See et al., 2017) CopyTrans (Gehrmann et al., 2018) Hi-MAP (Fabbri et al., 2019) 39.05 38.10 29.48 31.04 34.44 35.49 37.31 27.07 31.43 28.54 35.78 10.23 9.14 4.25 6.03 7.11 7.80 9.36 5.03 6.03 6.38 8.90 14.35 13.40 8.64 10.23 11.19 12.02 13.12 8.63 10.01 7.22 11.43 HL-TreeSegs (Our work) HL-XLNetSegs (Our work) 39.18 39.26 10.30 10.70 14.37 14.47 Table 3: Results on DUC-04 dataset evaluated by ROUGE. 4 Experiments 4.1 Data Sets Our data comes from NIST. We use them to investigate the feasibility of the proposed multi-document summarization method. Particularly, we use DUC03/04 (O"
2020.emnlp-main.509,A83-1023,0,0.238369,"2019). We next discuss our method in greater detail. 3 Our Method We present a new method to identify self-contained segments, then select important and non-redundant segments to form a summary, as text fragments containing incomplete and disorganized information are hardly successful summary highlights. 3.1 Self-Contained Segments A self-contained segment is, in a sense, a miniature sentence. Any text segment containing incomplete or ungrammatical constructions is incomprehensible to humans. Table 2 presents examples of selfcontained and non-self-contained segments. Since its very inception (Vladutz, 1983), the concept of “semantically self-contained segment” has not been sufficiently examined in the literature and lacks an universal definition. We assume in this paper that a self-contained segment shall conform to certain syntactic validity constraints and there exists only weak dependencies between words that belong to the segment and those do not. The automatic identification of self-contained segments requires more than segmentation or parsing sentences into tree structures (Dozat and Manning, 2018). Self-contained segments do not necessarily correspond to constituents of the tree and furth"
2020.emnlp-main.509,P10-1058,0,0.0162456,"on average (Kamigaito et al., 2018). Keyphrases containing two to three words are much less informative (Hasan and Ng, 2014). Neither are ideal solutions. There is a rising need for other forms of highlighting, and we explore subsentence highlights that strike a balance between the amount and quality of emphasized content. It is best for highlighted segments to remain selfcontained. In fact, multiple partially-overlapping and self-contained segments can exist in a sentence, as illustrated in Table 2. Identifying self-contained segments has not been thoroughly investigated in previous studies. Woodsend and Lapata (2010) propose to generate story highlights by selecting and combining phrases. Li et al. (2016) explore elemen6283 tary discourse units generated using an RST parser as selection units. Spala et al. (2018) present a crowdsourcing method for workers to highlight sentences and compare systems. Arumae et al. (2019) propose to align human abstracts and source articles to create ground-truth highlight annotations. Importantly, and distinguishing our work from earlier literature, we make a first attempt to generate self-contained highlights, drawing on the successes of deep contextualized representations"
2020.findings-emnlp.184,P00-1037,0,0.336539,"ining data and use the top k prediction of each character as the semantic confusion set. For candidates generation, we substitute each character in the chunkij with its semantically similar characters and keep the candidates that can be found in the V . Similar to shape confusion set, in practice, we only consider candidates that have 1 edit distance (1 substitution) with the chunkij . 2.3 Correction Selection In this section, we introduce the training strategy for correction selection and the features we used for global optimization. Most of the previous work follows the noisy channel model (Brill and Moore, 2000), which formulates the error correction tasks as: sc = arg max p(ˆ s|s) (2) sˆ where the s is the input sentence, and sˆ refers to a possible correction. The formula can be further rewritten through the Bayes rule as: sc = arg max sˆ p(s|ˆ s) · p(ˆ s) p(s) Name ed pyed n-chunk wlm cem n-py n-shape n-lm Table 1: The features used for the correction selection. s and sˆ refer to the input sentence and a correction. correction and input sentence through characterlevel and pronunciation-level. A longer chunk is usually more unambiguous than a shorter one, thus a correction with less n-chunk is ofte"
2020.findings-emnlp.184,2020.acl-main.81,0,0.715775,"lts on the CSC Datasets We first report the performance of the proposed method on the csc13 , csc14 and csc15 dataset. As shown in Table 3, when comparing to previous strong CSC systems, our proposed chunk-based method achieves a significant improvement on the three datasets. Zhao et al. (2017) employ a graph-based model and integrate spelling checking with word segmentation. However, their proposed method only propython-pinyin 8 https://github.com/google-research/ bert 9 https://github.com/kpu/kenlm 2035 Dataset csc13 csc14 csc15 Model Yeh et al. (2015) Zhao et al. (2017) Hong et al. (2019)* Cheng et al. (2020)‡ our method Zhao et al. (2017) Hong et al. (2019) Cheng et al. (2020)‡ our method Zhang et al. (2015) Hong et al. (2019) Zhang et al. (2020) Cheng et al. (2020)‡ our method Acc 74.80 83.20 70.0 70.0 70.09 74.2 80.9 76.82 Detection Level P R 44.31 37.67 55.90 46.99 61.19 75.67 61.0 53.5 58.27 54.53 78.65 54.80 80.27 53.27 67.6 60.0 73.7 73.2 70.97 64.00 88.11 62.00 F1 40.72 51.06 67.66 57.0 56.28 64.59 64.04 63.5 73.5 67.30 72.79 Acc 66.30 37.00 60.5 67.20 69.3 68.08 69.18 73.7 77.4 74.64 Correction Level P R 70.30 62.50 70.50 35.60 73.1 60.5 44.58 37.47 74.34 67.20 55.50 39.14 59.4 52.0 51.01"
2020.findings-emnlp.184,I05-3017,0,0.110928,"propose a chunk-based framework to correct single-character and multi-character word errors uniformly; and 3) we adopt a global optimization strategy to enable a sentence-level correction selection. The experimental results show that the proposed approach achieves a new state-of-the-art performance on three benchmark datasets, as well as an optical character recognition dataset. 1 • For English spelling check, the basic unit is the word. However, Chinese characters are continuously written without word delimiter, and the word definition varies across different linguistic theories (Xue, 2003; Emerson, 2005). It makes the sentence with spelling errors more ambiguous, and more challenging for the spell checkers to detect and correct the errors. • Chinese words usually consist of one to four characters and are much shorter than the English word. Spelling errors can drastically change the meaning of the word. Thus, the CSC task relies on the contextual semantic information to find the best correction. Introduction Spelling check is a task to automatically detect and correct spelling errors in human writings. Spelling check is well-studied for languages such as English, and many resources and tools h"
2020.findings-emnlp.184,D19-5522,0,0.0985463,"s research demonstrates that most of the Chinese spelling errors come from similar pronunciations, shapes, or meanings (Liu et al., 2011; Chen et al., 2011). Previous CSC models usually employ the characters with similar pronunciation or shape as the confusion set to reduce the search space, but the visually and phonologically irrelevant typos cannot be handled. Recent work aims at replacing the pronunciation and shape confusion sets with a dynamically generated confusion set by masked language models, which retrieve the semantically related candidates according to the contextual information (Hong et al., 2019). However, due to the lack of knowledge about human errors, masked language models correct the spelling errors ignoring the pronunciation or shape similarity. Therefore, combining the two comes as a natural solution. For the second challenge, early works rely on the segmentation results from a Chinese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors"
2020.findings-emnlp.184,W13-4416,0,0.130413,"to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-based model cannot effectively utilize the word-level semantic information, and the correction is also more difficult to interpret. In order to explore and utilize the word-level information, the word-based methods are designed to do word segmentation and spelling error corrections jointly. Previous works show that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply a pipeline of submodules and handle special cases (e.g., single-character words) individually, the complex architecture makes it difficult to perform global optimization. For the third challenge, previous works mainly rely on the local context features such as point-wise mutual information (PMI), part-of-speech (POS) n-gram, and perplexity from an n-gram language model (Liu et al., 2013; Zhang et al., 2015; Yeh et al., 2015). As these statistical features are limited within a fixed-size"
2020.findings-emnlp.184,W14-6835,0,0.178993,"annot be handled. Recent work aims at replacing the pronunciation and shape confusion sets with a dynamically generated confusion set by masked language models, which retrieve the semantically related candidates according to the contextual information (Hong et al., 2019). However, due to the lack of knowledge about human errors, masked language models correct the spelling errors ignoring the pronunciation or shape similarity. Therefore, combining the two comes as a natural solution. For the second challenge, early works rely on the segmentation results from a Chinese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors often lead to incorrect segmentation results. The accumulated errors make the spell checking even more difficult. Thus, characterbased models are proposed to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-ba"
2020.findings-emnlp.184,W13-4409,0,0.271371,"ow that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply a pipeline of submodules and handle special cases (e.g., single-character words) individually, the complex architecture makes it difficult to perform global optimization. For the third challenge, previous works mainly rely on the local context features such as point-wise mutual information (PMI), part-of-speech (POS) n-gram, and perplexity from an n-gram language model (Liu et al., 2013; Zhang et al., 2015; Yeh et al., 2015). As these statistical features are limited within a fixed-size window, it is difficult to capture the deep contextual information. In the paper, we propose a unified framework combining features and benefits from previous works. We employ confusion sets from similar pronunciations, shapes, and semantics to deal with different types of spelling errors. A chunk-based decoding approach is proposed to model both singlecharacter and multi-character words in a uniform way. We also finetune an error model based on the large-scale pretrained language model to in"
2020.findings-emnlp.184,P03-1021,0,0.148574,"sˆ. the number of chunks in sˆ. the perplexity of sˆ measured by a word-level n-gram language model. the improvement of log probability from a character error model. the number of chunks that are from the pronunciation confusion set. the number of chunks that are from the shape confusion set. the number of chunks that are from the semantic confusion set. j X (log p(ˆ ck |ck , s) − log p(ck |ck , s)) (6) k=i where p(ˆ ck |ck , s) is the probability of replacing ck with cˆk given the input sentence s.4 For combining different features, we apply the Minimum Error Rate Training (MERT) algothrim (Och, 2003). Given the top n outputs, the MERT algorithm optimizes the scoring function by learning to rerank the decoded sentences according to their similarity to the gold sentence. Rather than a local ranking, the MERT algorithm measures the similarity directly by sentence-level metrics to achieve a global optimization. 3 Experiments In the following sections, we will introduce the datasets and the experimental settings first, and 4 Note that we use p(ˆ ck |ck ) to simulate the error model p(s|ˆ s), because our error model is contextualized and the calculation costs will be huge if we calculate p(s|ˆ"
2020.findings-emnlp.184,W15-3106,0,0.0217379,"lts demonstrate that CSC errors require more contextual information even for single-character errors. For sentences with more errors, the recall rate increases rapidly when the beam size is small (e.g., beam size from 1 to 2). However, the recall rate does not increase significantly after the beam grows to an appropriate size (e.g., a beam size of 4). This experiment result illustrates that, for sentences with multiple errors, the bottleneck comes from the candidate selection. 4 Related Work Previous work of CSC is closely related to a series of shared tasks (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015). The workflow of CSC systems can be roughly divided into two phases, candidate generation and candidate selection. For the candidate generation phase, most of the previous work retrieves the candidates according to pronunciation or shape (Liu et al., 2011; Chen et al., 2011; Yu and Li, 2014; Yeh et al., 2015). Recently, Hong et al. (2019) propose to replace the traditional confusion sets with a dynamically generated one. They treat the CSC as a sequence 2038 labeling problem and finetune a pretrained masked language model to generate candidates. For reducing the false alarm rate, they filter"
2020.findings-emnlp.184,D18-1273,0,0.47439,"vel P R 70.30 62.50 70.50 35.60 73.1 60.5 44.58 37.47 74.34 67.20 55.50 39.14 59.4 52.0 51.01 47.65 77.43 51.04 79.72 51.45 66.6 59.1 66.7 66.2 60.08 54.18 87.33 57.64 F1 66.17 47.31 66.2 40.72 70.59 45.90 55.4 49.27 61.52 62.54 62.6 66.4 56.98 69.44 Table 3: The main results on csc13 , csc14 and csc15 datasets. *The csc13 detection-level performance of Hong et al. (2019) is obtained on the test set of correction task and thus incomparable with the results from other work. The results with ‡ are reproduced by rerunning the released code and evaluation scripts on the standard CSC datasets. The Wang et al. (2018) and Wang et al. (2019) calculate the performance on the character-level, which makes their results incomparable with other works. cesses the multi-character words. Two types of single-character words are handled by rules and an individual module. The separated modules make their system difficult to fully explore the annotated data and obtain a global optimization. Zhang et al. (2015) combine the character-level candidate generation with a two-stage filter model. For the first stage, they use a logistic regression classifier to reduce the size of candidates. In the second stage, they utilize t"
2020.findings-emnlp.184,2020.acl-main.82,0,0.703171,"ese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors often lead to incorrect segmentation results. The accumulated errors make the spell checking even more difficult. Thus, characterbased models are proposed to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-based model cannot effectively utilize the word-level semantic information, and the correction is also more difficult to interpret. In order to explore and utilize the word-level information, the word-based methods are designed to do word segmentation and spelling error corrections jointly. Previous works show that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply a pipeline of submodules and handle spe"
2020.findings-emnlp.184,W15-3107,0,0.600575,"on the segmentation results from a Chinese word segmentation system (Yu and Li, 2014). However, as the 2031 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2031–2040 c November 16 - 20, 2020. 2020 Association for Computational Linguistics segmentation system is trained on the clean corpus, the spelling errors often lead to incorrect segmentation results. The accumulated errors make the spell checking even more difficult. Thus, characterbased models are proposed to perform the correction at the character-level directly, which are more robust to segmentation errors (Zhang et al., 2015; Hong et al., 2019; Zhang et al., 2020). However, the character-based model cannot effectively utilize the word-level semantic information, and the correction is also more difficult to interpret. In order to explore and utilize the word-level information, the word-based methods are designed to do word segmentation and spelling error corrections jointly. Previous works show that the wordbased correction models often perform better than their character-based counterparts (Jia et al., 2013; Hsieh et al., 2015; Yeh et al., 2015; Zhao et al., 2017). Since word-based correction models usually apply"
2020.findings-emnlp.184,P19-1578,0,0.426703,"50 35.60 73.1 60.5 44.58 37.47 74.34 67.20 55.50 39.14 59.4 52.0 51.01 47.65 77.43 51.04 79.72 51.45 66.6 59.1 66.7 66.2 60.08 54.18 87.33 57.64 F1 66.17 47.31 66.2 40.72 70.59 45.90 55.4 49.27 61.52 62.54 62.6 66.4 56.98 69.44 Table 3: The main results on csc13 , csc14 and csc15 datasets. *The csc13 detection-level performance of Hong et al. (2019) is obtained on the test set of correction task and thus incomparable with the results from other work. The results with ‡ are reproduced by rerunning the released code and evaluation scripts on the standard CSC datasets. The Wang et al. (2018) and Wang et al. (2019) calculate the performance on the character-level, which makes their results incomparable with other works. cesses the multi-character words. Two types of single-character words are handled by rules and an individual module. The separated modules make their system difficult to fully explore the annotated data and obtain a global optimization. Zhang et al. (2015) combine the character-level candidate generation with a two-stage filter model. For the first stage, they use a logistic regression classifier to reduce the size of candidates. In the second stage, they utilize the online translation s"
2020.findings-emnlp.184,W13-4406,0,0.379161,"Missing"
2020.findings-emnlp.184,O03-4002,0,0.15328,"rors; 2) we propose a chunk-based framework to correct single-character and multi-character word errors uniformly; and 3) we adopt a global optimization strategy to enable a sentence-level correction selection. The experimental results show that the proposed approach achieves a new state-of-the-art performance on three benchmark datasets, as well as an optical character recognition dataset. 1 • For English spelling check, the basic unit is the word. However, Chinese characters are continuously written without word delimiter, and the word definition varies across different linguistic theories (Xue, 2003; Emerson, 2005). It makes the sentence with spelling errors more ambiguous, and more challenging for the spell checkers to detect and correct the errors. • Chinese words usually consist of one to four characters and are much shorter than the English word. Spelling errors can drastically change the meaning of the word. Thus, the CSC task relies on the contextual semantic information to find the best correction. Introduction Spelling check is a task to automatically detect and correct spelling errors in human writings. Spelling check is well-studied for languages such as English, and many resou"
2020.findings-emnlp.431,W17-5102,0,0.0285895,"idation Self Assertions Figure 1: The preliminary resistance strategy framework Fransen et al. (2015) tics, these resistant utterances all collapse into one dialog-act “negative-reactions-towards-persuasion” in the provided annotation scheme, which makes it harder for the persuasive system to respond accordingly. Therefore, to achieve more efficient persuasion, we propose to study the resistance to persuasion in more details. Introduction and Related Work Persuasion plays a prominent role in human communication and has attracted more and more attentions in the NLP community (Tan et al., 2016; Hidey et al., 2017; Hidey and McKeown, 2018; Wang et al., 2019; Yang et al., 2019). During persuasion, the persuader attempts to convince the persuadee to change his/her attitude, opinion or behavior. Previous research on persuasive dialogs mainly study the persuader’s strategies (Wang et al., 2019; Shi et al., 2020; Li et al., 2019), which is helpful when the persuadee shows positive altitude towards the persuasion; but in other situations, the persuadees resists rather than embrace the persuasive attempt. For instance, in P ERSUASION F OR G OOD (Wang et al., 2019), 166 out of 1,017 persuasive dialogs contains"
2020.findings-emnlp.431,P19-1566,1,0.919835,"ing the content Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader’s strategies, and paid little attention to the persuadee (user). However, understanding and addressing users’ resistance strategies is an essential job of a persuasive dialog system. So, we adopt a preliminary framework on persuasion resistance in psychology, and design a fine-grained resistance strategy annotation scheme. We annotate the P ERSUASION F OR G OOD dataset with the scheme (Wang et al., 2019). With the enriched annotations, we build a classifier to predict the resistance strategies. Furthermore, we analyze the relationships between persuasion strategies and persuasion resistance strategies. Our work lays the ground for developing a persuasive dialogue system that can understand and address user resistance strategy appropriately. The code and data will be released. 1 Contesting Resistance Strategy Contesting the source Contesting the strategies used Weighting attributes Biased Processing Reducing impact Optimism Bias Attitude Bolstering Empowerment Social Validation Self Assertions"
2020.findings-emnlp.431,N19-1364,0,0.0209522,"ategy framework Fransen et al. (2015) tics, these resistant utterances all collapse into one dialog-act “negative-reactions-towards-persuasion” in the provided annotation scheme, which makes it harder for the persuasive system to respond accordingly. Therefore, to achieve more efficient persuasion, we propose to study the resistance to persuasion in more details. Introduction and Related Work Persuasion plays a prominent role in human communication and has attracted more and more attentions in the NLP community (Tan et al., 2016; Hidey et al., 2017; Hidey and McKeown, 2018; Wang et al., 2019; Yang et al., 2019). During persuasion, the persuader attempts to convince the persuadee to change his/her attitude, opinion or behavior. Previous research on persuasive dialogs mainly study the persuader’s strategies (Wang et al., 2019; Shi et al., 2020; Li et al., 2019), which is helpful when the persuadee shows positive altitude towards the persuasion; but in other situations, the persuadees resists rather than embrace the persuasive attempt. For instance, in P ERSUASION F OR G OOD (Wang et al., 2019), 166 out of 1,017 persuasive dialogs contains resistance strategy; although individuals resist persuasive att"
2020.findings-emnlp.431,D16-1076,0,0.0470442,"Missing"
2020.nlptea-1.6,W09-1119,0,0.0342198,"c F1 7.7 18.4 10.8 9.6 17.7 12.5 9.3 22.8 13.3 32.2 13.3 18.9 Table 2: Final results on the official evaluation testing data. “Run #1” represents the ensemble model with correction. “Run #2” represents the single best model with correction. “Run #3” represents the ensemble model with correction and CSC. ”Top 1” reports the highest F1 score with its precision and recall at different levels. Model BERT-CRF BERT-GCN-CRF BERT-CRF BERT-GCN-CRF and correct sentences for the seq2seq training without extra data. We used the CGED-2018 testing dataset as our validation dataset. We introduced the BIOES (Ratinov and Roth, 2009) scheme for tagging. Language Technology Plantform (LTP) (Che et al., 2010) was introduced to obtain the dependency tree. The hyper-parameters are selected according to the performance on the validation data through official metrics. For the GCN model, the hidden vector size was 256 with 2 layers. The batch size, learning rate, and GCN dropout were set to 32, 1e-5, 0.2. For the multi-task model, the batch size, learning rate and w are set to 32, 3e-5, 0.9. Transformer decoder parameters are initialized from the BERT parameters as much as possible. 4.2 Type R R M M Precision 42.6 36.2 36.3 32.8"
2020.nlptea-1.6,2020.findings-emnlp.184,1,0.785364,"Missing"
2020.nlptea-1.6,I17-4006,0,0.0600673,"this task. 3.2 h2 多 v1 h1 BERT 病 Graph Convolution Network The multi-layer GCN network accepts the high-level character information obtained by the BERT model and the adjacency matrix of the dependency tree. The convolution operation is adopted for each layer. f (A, H l ) = AHl Wlg (1) where Wlg ∈ RD×D is a trainable matrix for the l-th layer, A is the adjacency matrix of the dependency tree, Hl = (h1 , h2 , ..., hn ) is the hidden state of the characters. Words use the same input representation in the network to indicate the dependency relation of the characters. BERT-GCN-CRF Previous works (Yang et al., 2017; Fu et al., 2018) spent a lot of effort in feature engineering including pretrained features and parsing features. Part-ofspeech-tagging(POS), and dependency information are the most important parsing features, which indicates to us the task is closely associated with the structure of the sentence syntactic dependency. Specifically, the redundant error and the missing error sentences syntax tree are very different from the correct sentences as the Figure 2 shows. To understand the dependency structure of an input sentence better, we introduce the Graph Convolution Network (GCN) (Kipf and Well"
2020.nlptea-1.6,C10-3004,0,0.051755,"lts on the official evaluation testing data. “Run #1” represents the ensemble model with correction. “Run #2” represents the single best model with correction. “Run #3” represents the ensemble model with correction and CSC. ”Top 1” reports the highest F1 score with its precision and recall at different levels. Model BERT-CRF BERT-GCN-CRF BERT-CRF BERT-GCN-CRF and correct sentences for the seq2seq training without extra data. We used the CGED-2018 testing dataset as our validation dataset. We introduced the BIOES (Ratinov and Roth, 2009) scheme for tagging. Language Technology Plantform (LTP) (Che et al., 2010) was introduced to obtain the dependency tree. The hyper-parameters are selected according to the performance on the validation data through official metrics. For the GCN model, the hidden vector size was 256 with 2 layers. The batch size, learning rate, and GCN dropout were set to 32, 1e-5, 0.2. For the multi-task model, the batch size, learning rate and w are set to 32, 3e-5, 0.9. Transformer decoder parameters are initialized from the BERT parameters as much as possible. 4.2 Type R R M M Precision 42.6 36.2 36.3 32.8 Recall 28.3 34.8 26.6 30.0 F1 34.0 35.4 30.7 31.7 Table 3: The position le"
2020.nlptea-1.6,W18-3707,0,0.0871814,"v1 h1 BERT 病 Graph Convolution Network The multi-layer GCN network accepts the high-level character information obtained by the BERT model and the adjacency matrix of the dependency tree. The convolution operation is adopted for each layer. f (A, H l ) = AHl Wlg (1) where Wlg ∈ RD×D is a trainable matrix for the l-th layer, A is the adjacency matrix of the dependency tree, Hl = (h1 , h2 , ..., hn ) is the hidden state of the characters. Words use the same input representation in the network to indicate the dependency relation of the characters. BERT-GCN-CRF Previous works (Yang et al., 2017; Fu et al., 2018) spent a lot of effort in feature engineering including pretrained features and parsing features. Part-ofspeech-tagging(POS), and dependency information are the most important parsing features, which indicates to us the task is closely associated with the structure of the sentence syntactic dependency. Specifically, the redundant error and the missing error sentences syntax tree are very different from the correct sentences as the Figure 2 shows. To understand the dependency structure of an input sentence better, we introduce the Graph Convolution Network (GCN) (Kipf and Welling, 2016; Marcheg"
2020.nlptea-1.6,W18-3710,0,0.0273559,"insert 1 to 4 mask tokens to cover most of the cases and adopt the beam-search algorithm to reduce the search complexity. In the second method, we generate the candidates by a seq2seq model trained by mapping the wrong sentences to the correct sentences. According to the detection result, we keep generating next characters until the correct character appears within the beam-search algorithm, and then replace the incorrect span. (5) We use Viterbi Decoding (Huang et al., 2015) to inference answers. 3.3 Multi-task Most previous works trained their model by the sequence tags (Yang et al., 2017; Li and Qi, 2018; Fu et al., 2018). We utilize not only tags but also correct sentences during the training process. Correct sentences are important for providing better representation in the hidden state. Moreover, with the correct sentences, the model can have a better understanding of the original meaning of the input sentence. Therefore, we introduce the seq2seq task (Sutskever et al., 2014; Vaswani et al., 2017) treating the training process as multi-task learning. As shown in Figure 4, the sequence labeling model is the encoder in our structure combined with the transformer decoders to predict the truth"
2020.nlptea-1.6,D17-1159,0,0.0331148,", 2018) spent a lot of effort in feature engineering including pretrained features and parsing features. Part-ofspeech-tagging(POS), and dependency information are the most important parsing features, which indicates to us the task is closely associated with the structure of the sentence syntactic dependency. Specifically, the redundant error and the missing error sentences syntax tree are very different from the correct sentences as the Figure 2 shows. To understand the dependency structure of an input sentence better, we introduce the Graph Convolution Network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017). Figure 3 shows our BERT-GCN-CRF model architecture. We will explain each part in detail. Accumulated Output After the graph convolution network, we concatenate the representation Hl for the l-th layer and the BERT hidden state passing to a linear classifier as the input of the CRF layer. V = Linear(H0 ⊕ Hl ) (2) CRF Layer A CRF layer is introduced to predict the sequence tags for each token. Score(X, Y ) = n X Word Dependency We split the input sentences into words and obtain the dependency relation of 45 n X Vi,yi (3) exp(Score(X, Y )) P (Y |X) = P ˆ Yˆ exp(Score(X, Y )) (4) i=0 Ayi ,yi+1 +"
2021.emnlp-main.218,N16-1030,0,0.0592786,"chanism used by Kiperwasser and Goldberg (2016). As the biaffine attention is widely used in other tasks like NER (Yu et al., 2020a) and semantic role labeling (Li et al., 2019b), we propose to use it for the entity relation extraction task in this work. 3 Entity Relation Extraction as Dependency Parsing While encoding VRDs, previous works take Both semantic entity relation extraction and depenentity labeling task as sequence labeling and re- dency parsing tasks aim to decide whether there implement the named entity recognition (NER) exists relation between two entities/words and asframework (Lample et al., 2016) but ignore layout sume that links always point from key/head unit information. Then, many works introduce a GCN- to value/modifier unit shown in Figure 1. Therebased module to encode layout information and fore, we can draw lessons from the dependency combine textual and visual information together parsing exploration as it has been studied for sev(Liu et al., 2019a; Yu et al., 2020b; Wei et al., 2020; eral decades and achieved great progress. BiCarbonell et al., 2021). In the GCN module, Liu affine parser, a strong model in dependency parsing, et al. (2019a); Yu et al. (2020b) take layout fe"
2021.emnlp-main.218,2021.acl-long.201,0,0.0654113,"Missing"
2021.emnlp-main.218,2020.coling-main.82,0,0.0990775,"Missing"
2021.emnlp-main.218,2020.acl-main.577,0,0.304298,"to the in-house customs data, achieving reliable performance in the production setting. 1 Introduction In real-life scenarios, there are many types of visually rich documents (VRDs), such as invoices, questionnaire forms, declaration materials and so on. These documents contain abundant layout information which helps us to understand the content while texts alone are not enough. In recent years, many works focus on how to extract key information from VRDs based on the results of OCR (Optical Character Recognition), which recognizes bounding boxes and texts within the boxes (Liu et al., 2019a; Yu et al., 2020b). Each bounding box contains 1) a group of words that belong together ∗ Corresponding author. The author’s contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd. from a semantic and spatial standpoint and 2) visual features such as layout, tabular structure and font size of the boxes in the document. We call such bounding boxes and texts within the boxes semantic entities1 , and each entity contains the word group and layout coordinates2 . Key information extraction (KIE) is such a task to analyze visually rich documents, which usually con"
2021.emnlp-main.218,N19-2005,0,0.347814,"l has been applied to the in-house customs data, achieving reliable performance in the production setting. 1 Introduction In real-life scenarios, there are many types of visually rich documents (VRDs), such as invoices, questionnaire forms, declaration materials and so on. These documents contain abundant layout information which helps us to understand the content while texts alone are not enough. In recent years, many works focus on how to extract key information from VRDs based on the results of OCR (Optical Character Recognition), which recognizes bounding boxes and texts within the boxes (Liu et al., 2019a; Yu et al., 2020b). Each bounding box contains 1) a group of words that belong together ∗ Corresponding author. The author’s contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd. from a semantic and spatial standpoint and 2) visual features such as layout, tabular structure and font size of the boxes in the document. We call such bounding boxes and texts within the boxes semantic entities1 , and each entity contains the word group and layout coordinates2 . Key information extraction (KIE) is such a task to analyze visually rich documents,"
2021.emnlp-main.218,2021.ccl-1.108,0,0.0217989,"Missing"
2021.emnlp-main.218,2020.acl-main.580,0,0.0138516,"u et al., 2019a; Yu et al., 2020b). • We apply our model to the real-world customs data with different layouts and achieve high performance in the production setting. 2 Related Work Visually rich document understanding includes many tasks, such as layout recognization (Zhong et al., 2019b; Li et al., 2020), table detection and recognition (Li et al., 2019a; Zhong et al., • At the relation scorer layer, we extract rela- 2019a) and key information extraction (Grali´nski tive position features between entities accord- et al., 2020; Guo et al., 2019; Huang et al., 2019; G. Jaume and Thiran, 2019; Majumder et al., 2020). ing to their coordinates. Our paper focuses on the key information extracApart from the above, inspired by the joint POS tion task which contains two subtasks, entity latagging and dependency parsing model (Nguyen beling and relation extraction. The former subtask 2760 Figure 2: The architecture of our proposed entity relation extraction model (left) and the biaffine parser model (right). tags entities with predefined labels, such as Task 3 on the SROIE data released by Huang et al. (2019), while the latter discovers relations between entities, such as Subtask C(3) on the FUNSD data (G. Jaum"
2021.emnlp-main.218,K18-2008,0,0.0418932,"Missing"
C12-1097,P06-2005,0,0.627216,"Missing"
C12-1097,C10-2022,0,0.319386,"Missing"
C12-1097,W09-2010,0,0.781758,"Missing"
C12-1097,P11-1038,0,0.490235,"Missing"
C12-1097,C08-1056,0,0.033691,"Missing"
C12-1097,P12-1109,0,0.303833,"Missing"
C12-1097,P11-2013,1,0.899605,"Missing"
C12-1097,J03-1002,0,0.0083563,"Missing"
C12-1097,I11-1109,1,0.797792,"Missing"
C12-1097,D11-1141,0,0.0987585,"Missing"
C12-1097,P02-1019,0,0.11623,"Missing"
C12-1097,N09-2069,0,0.0689033,"Missing"
C12-1097,P07-2045,0,\N,Missing
C16-1054,C12-1029,0,0.0193289,"d rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and"
C16-1054,W08-1105,0,0.0909924,"Missing"
C16-1054,C12-1056,0,0.0179621,"t al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compress"
C16-1054,D14-1076,1,0.850198,"n summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user"
C16-1054,N15-1145,1,0.827258,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N15-1079,1,0.837783,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N10-1134,0,0.0299569,"s kind. 2 Related Work Our work is closely related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summa"
C16-1054,W04-1013,0,0.0084025,"Missing"
C16-1054,W09-1801,0,0.0273222,"mance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used"
C16-1054,D07-1047,0,0.024065,"Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articles and compress them to form a summary, and the other is that we directly use sentences from the posts as the summary. 3 Corpus Construction For our work, we manually collected popular"
C16-1054,P13-1136,0,0.0210716,"arization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for j"
C16-1054,C14-1083,1,0.847348,"mmarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articl"
C16-1054,C08-1124,0,0.0362832,"ttention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines"
C16-1054,D12-1022,0,0.0161237,"ently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of"
C16-1054,P11-1049,0,\N,Missing
D13-1047,D10-1047,0,0.0959622,"h has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A gr"
D13-1047,P13-1020,0,0.137564,"constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the"
D13-1047,P11-1049,0,0.810124,"ummaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extra"
D13-1047,C12-1029,0,0.553418,"insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to us"
D13-1047,N10-1131,0,0.0191521,"(2013) propose a graph-cut based method that improves the speed of joint compression and summarization. The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the doc"
D13-1047,W06-1643,0,0.0344792,"lts. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submo"
D13-1047,P13-1099,1,0.185695,"for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline a"
D13-1047,N10-1134,0,0.166154,"aximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selec"
D13-1047,W03-1101,0,0.0326955,"rest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of"
D13-1047,W04-1013,0,0.631382,"oint modeling studies. Using our created compression data, we train a supervised compression model using a variety of word-, sentence-, and document-level features. During summarization, we generate multiple compression candidates for each sentence, and use the ILP framework to select compressed summary sentences. In addition, we also propose to apply a preselection step to select some important sentences, which can both speed up the summarization system and improve performance. We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004). Our results show that by incorporating a guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been expl"
D13-1047,P09-2066,1,0.644677,"Missing"
D13-1047,W09-1801,0,0.668396,"ntence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before"
D13-1047,E06-1038,0,0.0976203,"ulti-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sente"
D13-1047,C12-1128,0,0.124461,"The above ILP method can offer an exact solution to the defined objective function. However, ILP is computationally expensive when the formulation involves large quantities of variables, i.e, when we have many sentences and a large number of candidate compressions for each sentence. We therefore propose to apply a sentence pre-selection step before the compression. This kind of selection step has been used in previous ILP-based summarization systems (Berg-Kirkpatrick et al., 2011; Gillick et al., 2009). In this work, we propose to use a simple supervised support vector regression (SVR) model (Ng et al., 2012) to predict a salience score for each sentence and select the top ranked sentences for further processing (compression and summarization). To train the SVR model, the target value for each sentence is the ROUGE-2 score between the sentence and the four human abstracts (this same value is used for sentence selection in corpus annotation (Section 3)). We employ three commonly used features: (1) sentence position in the document; (2) sentence length as indicated by a binary feature: it takes the value of 0 if the number of words in the sentence is greater than 50 or less than 10, otherwise the fe"
D13-1047,W02-0401,0,0.0970346,"rformance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the aut"
D13-1047,N07-1051,0,0.0111604,"rds and the current word. • POS n-grams: same as the word n-grams, but use the part-of-speech tags instead. • Named entity tags: binary features representing whether the current word is a person, location, or temporal expression. We use the Stanford CoreNLP tools3 for named entity tagging. • Stopwords: whether the current word is a stopword or not. • Conjunction features: (1) conjunction of the current word with its relative position in the sentence; (2) conjunction of the NER tag with its relative position. • Syntactic features: We obtain the syntactic parsing tree using the Berkeley Parser (Petrov and Klein, 2007), then obtain the following features: (1) the last sentence constituent tag in the path from the root to the word; (2) depth: length of the path starting from the root node to the word; (3) normalized depth: depth divided by the longest path in the parsing tree; (4) whether the word is under an SBAR node; (5) depth and normalized depth of the SBAR node if the word is under an SBAR node; • Dependency features: We employ the Penn2Malt toolkit 4 to convert the parse result from the Berkeley parser to the dependency parsing tree, and use these dependency 3 4 http://nlp.stanford.edu/software/corenl"
D13-1047,D13-1156,1,0.64139,"for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the 2013 Conference on E"
D13-1047,J02-4001,0,0.0756133,"Missing"
D13-1047,W13-3508,0,0.478243,"m’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sentence, and use an ILP formulation to select the best set of summary s"
D13-1047,P05-1036,0,0.0137398,"ssion model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteris"
D13-1047,P07-1070,0,0.0111622,"tators were explicitly informed about the important summary words during the compression annotation. We then train a supervised compression model to capture the guided compression process using a set of word-, sentence-, and document-level features. We conduct experiments on the TAC 2008 and 2011 summarization data sets and show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art. In future, we would like to further explore the reinforcement relationship between keywords and summaries (Wan et al., 2007), improve the readability of the sentences generated from the guided compression system, and report results using multiple evaluation metrics (Nenkova et al., 2007; Louis and Nenkova, 2012) as well as performing human evaluations. Acknowledgments Part of this work was done during the first author’s internship in Bosch Research and Technology Center. The work is also partially supported by NSF award IIS-0845484 and DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views of the fund"
D13-1047,P13-1136,0,0.610583,"ing compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work"
D13-1047,D12-1022,0,0.508507,"ses, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selection step. Many studies explore the joint sentence compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut base"
D13-1047,P12-2068,0,0.0263344,"e compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compres"
D13-1047,W01-0100,0,\N,Missing
D14-1076,D10-1047,0,0.018162,". 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that int"
D14-1076,P13-1020,0,0.0722131,"s since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constit"
D14-1076,P11-1049,0,0.380047,"and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick et al. (2011) proposed an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They trained the model using a margin-based objective whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly i"
D14-1076,briscoe-carroll-2002-robust,0,0.0757113,"preserve the grammar relationship; Wang et al. (2013) developed a novel beam search decoder using the treebased compression model on the constituent parse tree, which could find the most probable compression efficiently. Table 4 shows the compression results of various systems, along with the compression ratio (C Rate) of the system output. We adopt the compression metrics as used in (Martins and Smith, 2009) that measures the macro F-measure for the retained unigrams (Uni-F1), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by (Briscoe and Carroll, 2002) (Rel-F1). We can see that our proposed compression method performs well, similar to the state-of-the-art systems. To evaluate the power of using the expanded parse tree in our model, we conducted another experiment where we only consider the bottom level of the constituent parse tree. In some sense, this could be considered as the system in (Clarke and Lapata, 2008). Furthermore, we use two different setups: one uses the lexical features (about the words) and the other does not. Table 5 shows the results using the data in (Clarke and Lapata, 2008). For a comparison, we also include the result"
D14-1076,C12-1029,0,0.118385,"sed extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features fo"
D14-1076,D13-1047,1,0.165504,"constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality"
D14-1076,P13-1099,1,0.254022,"constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality"
D14-1076,N10-1134,0,0.234957,"ndant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick e"
D14-1076,W03-1101,0,0.0207147,"li,yangl@hlt.utdallas.edu} {feiliu@cs.cmu.edu} {lin.zhao,fuliang.weng@us.bosch.com} Abstract a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminati"
D14-1076,W04-1013,0,0.220689,"ered more difficult, involving sophisticated techniques for meaning representation, content planning, surface realization, etc. There has been a surge of interest in recent years on generating compressed document summaries as 691 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691–701, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics based summarization model is used to select the final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, w"
D14-1076,W02-1001,0,0.0347668,"icked during the spring are tasty. partmod(truffles,picked) Oil price futures. nn(futures,oil) She looks very beautiful. acomp(looks,beautiful) He felt sad after learning that tragedy. pcomp(after,learning) I am certain that he did it. ccomp(certain,did) Last night I swam in the pool. tmod(swam,night) Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and Manning, 2002) • For type III relations, if the parent node in these relations is retained, the child node should be kept as well. 3.4 weights using the structured perceptron learning strategy (Collins, 2002). The reference label for every node in the expanded constituent parse tree is obtained automatically from the bottom to the top of the tree. Since every leaf node (word) is human annotated (removed or retain), we annotate the internal nodes as removed if all of its children are removed. Otherwise, the node is annotated as retained. During perceptron training, a fixed learning rate is used and parameters are averaged to prevent overfitting. In our experiment, we observe stable convergence using the held-out development corpus, with best performance usually obtained around 10-20 epochs. Feature"
D14-1076,W03-0501,0,0.147922,"Missing"
D14-1076,D13-1155,0,0.0186923,"summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. Thadani and McKeown (2013) presented an approach for discriminative sentence compression that jointly produces sequential and syntactic representations for output text. Filippova and Altun (2013) presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence sele"
D14-1076,W09-1801,0,0.800543,"P) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick et al. (2011) proposed an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They trained the model using a margin-based objective whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compre"
D14-1076,E06-1038,0,0.489741,"al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression"
D14-1076,N10-1131,0,0.0361554,"e whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly improve the summarization performance. Zajic et al. (2007) compared two sentence compression approaches for multidocument summarization, including a ‘parse-andtrim’ and a noisy-channel approach. Galanis and Androutsopoulos (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddhartha"
D14-1076,N07-1023,0,0.0221158,"an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate vari"
D14-1076,W06-1643,0,0.0329105,"sed compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression proc"
D14-1076,C12-1128,0,0.243243,"ethod to select the best summary sentences from the multiple compressed sentences. The sentence pre-selection model is a simple supervised support vector regression (SVR) model that predicts a salience score for each sentence and selects the top ranked sentences for further processing (compression and summarization). The target value for each sentence during training is the ROUGE-2 score between the sentence and the human written abstracts. We use three common features: (1) sentence position in the document; (2) sentence length; and (3) interpolated n-gram document frequency as introduced in (Ng et al., 2012). The final sentence selection process follows the Table 2: Features used in our system besides those used in (Clarke and Lapata, 2008). 3.5 Learning To learn the feature weights during training, we perform ILP decoding on every sentence in the training set, to find the best hypothesis for each node in the expanded constituent parse tree. If the hypothesis is incorrect, we update the feature 696 CRF4 to implement the CRF sentence compression model. SVMlight5 is used for the summary sentence pre-selection model. Gurobi ILP solver6 does all ILP decoding. ILP method introduced in (Gillick et al.,"
D14-1076,W02-0401,0,0.0272814,"f instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence"
D14-1076,P06-1055,0,0.0140636,"nnotated by (Li et al., 2013a) using TAC2010 data. In the compression module, for each word we also used its document level feature.2 Compression Data We also evaluate our compression model using the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. Data Processing We use Stanford CoreNLP toolkit3 to tokenize the sentences, extract name entity tags, and generate the dependency parse tree. Berkeley Parser (Petrov et al., 2006) is adopted to obtain the constituent parse tree for every sentence and POS tag for every token. We use Pocket 4 http://sourceforge.net/projects/pocket-crf-1/ http://svmlight.joachims.org/ 6 http://www.gurobi.com 7 We chose to evaluate the linguistic quality for this system because of two reasons: one is that we have an implementation of that method; the other more important one is that it has the highest reported ROUGE results among the compared methods. 1 5 http://www.nist.gov/tac/data/index.html 2 Document level features for a word include information such as the word’s document frequency i"
D14-1076,D13-1156,1,0.838736,"ignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. Thi"
D14-1076,C04-1129,0,0.0108443,"los (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the parse trees and thus the compression decision can be made for a constituent, instead of a single word. ( 1 δi = 0 ∀i ∈ [1..n] ( 1 αi = 0 3 Sentence Compression Method if xi starts the compression otherwise ∀i ∈ [1..n] Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important content from the original sentence, and be grammatical. In some sense, sentence compression can be described as a"
D14-1076,W13-3508,0,0.0719819,"where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. Thadani and McKeown (2013) presented an approach for discriminative sentence compression that jointly produces sequential and syntactic representations for output text. Filippova and Altun (2013) presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulat"
D14-1076,P05-1036,0,0.0120166,"m entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the parse trees and thus the compression decision can be made for a constituent, instead of a single word. ( 1 δi = 0 ∀i ∈ [1..n] ( 1 αi = 0 3 Sentence Compression Method if xi starts the compression otherwise ∀i ∈ [1..n] Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important content from the original sentence, and be grammatical. In some sense, sentence compression can be described as a ‘scaled down version of the"
D14-1076,P13-1136,0,0.654749,"fuliang.weng@us.bosch.com} Abstract a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke an"
D14-1076,P10-1058,0,0.0165962,"ompression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly improve the summarization performance. Zajic et al. (2007) compared two sentence compression approaches for multidocument summarization, including a ‘parse-andtrim’ and a noisy-channel approach. Galanis and Androutsopoulos (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the p"
D14-1076,D12-1022,0,0.489972,"al Methods in Natural Language Processing (EMNLP), pages 691–701, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics based summarization model is used to select the final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the"
D14-1076,P12-2068,0,0.0436108,"final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve gram"
D14-1076,W01-0100,0,\N,Missing
D19-1095,P13-1023,0,0.022915,"on or even no bilingual resource (Ni et al., 2017; Fang and Cohn, 2017; Xie et al., 2018). However, word embedding spaces may not be completely isomorphic due to language-specific linguistic properties, and therefore cannot be perfectly aligned. For example, different from English, Chinese nouns do not distinguish singular and plural forms, while Spanish nouns distinguish masculine and feminine. On the other hand, NER tags such as person names, organizations, and locations are shared across different languages. Language-independent frameworks such as universal conceptual cognitive annotation (Abend and Rappoport, 2013), universal POS (Petrov et al., 2011a), and universal dependencies (Nivre et al., 2016) are defined to represent different languages in a unified formation. These work serves as our motivation to assume 1028 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1028–1039, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics that the semantic meanings of words from different languages can be roughly aligned at a conceptual level and it is more reaso"
D19-1095,Q16-1031,0,0.0181014,"rious topics and can thus be used to generate parallel/comparable corpora or even weakly annotated target language sentences (Kim et al., 2012). However, parallel corpora and Wikipedia can be rare for true low-resource languages. Mayhew et al. (2017) reduce the resource requirement by proposing a cheap translation method, which “translates” the training data from the source to the target language word by word through a bilingual lexicon. While Xie et al. (2018) reduce the requirement of bilingual lexicons by an unsupervised word-by-word translation through CLWEs. Duh, 2017; Yang et al., 2017; Ammar et al., 2016; Kim et al., 2017). However, these models are usually obtained through joint learning and require annotated data from the target language. 6 Conclusion In this paper, we focused on a low-resources cross-lingual setting and proposed transfer learning methods based on the alignment of deep semantic spaces between different languages. The proposed multilingual language model bridges different languages by automatically learning crosslingual disambiguated representations. Abundant NER and POS experiments are conducted on the benchmark datasets. Experimental results show that our approaches using"
D19-1095,D16-1153,0,0.0410584,"Missing"
D19-1095,W06-2920,0,0.0729701,", 2003), which contain four European languages, English (en), Spanish (es), Dutch (nl), German (de) and four entity types (person, location, organization, and MISC). We also evaluate a distant language pair, English-Chinese, on OntoNotes(v4.0) dataset (Hovy et al., 2006). We adopt the same dataset split and four valid entity types (person, location, organization, and GPE) as described in (Wang and Manning, 2014). For cross-lingual POS, we use the Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv) portion from CoNLL 2006/2007 dataset (Buchholz and Marsi, 2006; Nivre et al., 2007). Following previous work (Fang and Cohn, 2017), we train the sequence labeling model on Penn Treebank data and adopt the universal POS tagset (Petrov et al., 2011b). In all cases, the sequence labeling model is trained on the source language (English) training data and is tested on the target language test data. 3.2 Details of MLMA We adopt a 6-layer bi-directional Transformer decoder with 8 attention heads. The dimension size of hidden states and inner states are 512 and 2048, respectively. The dropout rates after attention and residual connection are both 0.1. We use th"
D19-1095,Q16-1026,0,0.0233195,"th no bilingual resources at all and take advantage of deep contextualized representations. Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages, and is also effective on distant language pairs such as English and Chinese.1 1 Introduction Sequence labeling tasks such as named entity recognition (NER) and part-of-speech (POS) tagging are fundamental problems in natural language processing (NLP). Recent sequence labeling models achieve state-of-the-art performance by combining both character-level and word-level information (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lample et al., 2016). However, these models heavily rely on large-scale annotated training data, which may not be available in most languages. Cross-lingual transfer learning is proposed to address the label scarcity problem by transferring annotations from high-resource languages (source languages) to low-resource languages (target languages). In this scenario, a ma∗ Corresponding Author. Kenny Q. Zhu was partially supported by NSFC grant 91646205 and Alibaba visiting scholar program. 1 The code is released at https://github.com/ baozuyi/MLMA. † jor challenge is how to br"
D19-1095,I17-2016,0,0.0370914,"Missing"
D19-1095,P11-1061,0,0.213661,"ves a significant improvement over the word-level methods. The initialization from CLWEs also proves its effectiveness for distant language pairs by gaining further improvement and reaching a comparable result with Wang and Manning (2014). This experiment suggests that cross-lingual transfer is still challenging between distant language pairs. 3.6 Results for POS We evaluate our methods on another sequence labeling task POS, and the results are shown in Table 3. We compare with previous studies using unsupervised cross-lingual clustering (Fang and Cohn, 2017) and large-scale parallel corpora (Das and Petrov, 2011). As shown in Table 3, our models with deep semantic alignment outperform previous lexicon-based cross-lingual clustering by a large margin. When comparing to the previous method with a small amount of training data, the MLMA-Avl method obtains an improved accuracy without training data in the target languages. For further comparison, We also list the performance of applying the method from Xie et al. (2018) and 1034 MUSE brown: oliv´aceo (olive), negruzcas (blackish), negruzco (blackish), marr´on (brown), ocr´aceo (ochraceous) chair: vicepresidenta (vice president), vicedecano (vice dean), c´"
D19-1095,R11-1017,0,0.200213,"Avl (init) shown in Table 1 indicate that the CLWEs lead to a better initialization and improved performance. Multi-source Transfer We conduct experiments of multi-source transfer based on method MLMAAvl and report the performance as MLMA-Avl (multi) in Table 1. The experiment settings largely follow the previous work (Mayhew et al., 2017). They employ two source languages for each target language and use syntactic features to choose the related source languages. For Spanish and German, we use English and Dutch as source languages. English and Spanish are adopted for 1033 Model Das and Petrov (2011) Fang and Cohn (2017)† Fang and Cohn (2017)†‡ Xie et al. (2018)* MUSE* Multilingual BERT* Our methods MLMA-Avl + s.w.s. MLMA-Avl + f.w.s. MLMA-Avl (init) + s.w.s. MLMA-Avl (init) + f.w.s. es 84.2 68.40 81.20 73.25 78.30 83.86 nl 79.5 64.50 82.30 75.46 80.84 84.79 de 82.8 65.90 78.90 80.72 81.10 87.16 da 83.2 73.5 81.9 29.75 73.99 83.77 el 82.5 65.5 80.1 71.65 63.16 82.27 it 86.8 64.8 81.9 71.19 80.63 88.39 pt 87.9 67.8 82.1 76.48 82.79 87.86 sv 80.5 66.0 78.1 64.36 66.38 81.07 Avg. 83.31 67.05 80.81 67.86 75.90 84.90 81.60 81.20 82.73 82.27 85.10 85.54 85.79 85.97 84.10 84.92 85.76 86.37 83.3"
D19-1095,P17-2093,0,0.188656,"l resources to transfer knowledge from source languages to target languages. Parallel corpora are widely used to project annotations from the source to the target side (Yarowsky et al., 2001; Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014). These methods could achieve strong performance with a large amount of bilingual data, which is scarce in low-resource settings. Recent research leverages cross-lingual word embeddings (CLWEs) to establish inter-lingual connections and reduce the requirements of parallel data to a small lexicon or even no bilingual resource (Ni et al., 2017; Fang and Cohn, 2017; Xie et al., 2018). However, word embedding spaces may not be completely isomorphic due to language-specific linguistic properties, and therefore cannot be perfectly aligned. For example, different from English, Chinese nouns do not distinguish singular and plural forms, while Spanish nouns distinguish masculine and feminine. On the other hand, NER tags such as person names, organizations, and locations are shared across different languages. Language-independent frameworks such as universal conceptual cognitive annotation (Abend and Rappoport, 2013), universal POS (Petrov et al., 2011a), and"
D19-1095,N06-2015,0,0.126615,"utput label sequences. 3 Experiments We first introduce the datasets used in the experiment and then the implementation details of our models, before presenting the results on NER and POS tasks. 3.1 Datasets For cross-lingual NER, we evaluate the proposed approaches on CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain four European languages, English (en), Spanish (es), Dutch (nl), German (de) and four entity types (person, location, organization, and MISC). We also evaluate a distant language pair, English-Chinese, on OntoNotes(v4.0) dataset (Hovy et al., 2006). We adopt the same dataset split and four valid entity types (person, location, organization, and GPE) as described in (Wang and Manning, 2014). For cross-lingual POS, we use the Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv) portion from CoNLL 2006/2007 dataset (Buchholz and Marsi, 2006; Nivre et al., 2007). Following previous work (Fang and Cohn, 2017), we train the sequence labeling model on Penn Treebank data and adopt the universal POS tagset (Petrov et al., 2011b). In all cases, the sequence labeling model is trained on the"
D19-1095,P15-1001,0,0.0272631,"uence labeling model is trained on the source language (English) training data and is tested on the target language test data. 3.2 Details of MLMA We adopt a 6-layer bi-directional Transformer decoder with 8 attention heads. The dimension size of hidden states and inner states are 512 and 2048, respectively. The dropout rates after attention and residual connection are both 0.1. We use the Adam optimization scheme (Kingma and Ba, 2014) with a learning rate of 0.0001 and a gradient clip norm of 5.0. The vocabulary size of each language is 200,000, and we train the model with a sampled softmax (Jean et al., 2015) of 8192 samples. We only keep the sentences containing less than 200 tokens for training and group them into batches by length. Each batch contains around 4096 tokens for each language. The language modeling weight λlm i is set to be 1.0 for each language. For alignv al ment, λm l , λl , λl are set to be 0.1, 0.01 and 1.0 for every layer l, and λiden is set to be 100. For languages except English, the latest dump of Wikipedia is used as monolingual corpora. For English, we use 1B Word Benchmark (Chelba et al., 2013) to reduce the effects of potential internal alignment in Wikipedia (Zirikly a"
D19-1095,D17-1302,0,0.02436,"thus be used to generate parallel/comparable corpora or even weakly annotated target language sentences (Kim et al., 2012). However, parallel corpora and Wikipedia can be rare for true low-resource languages. Mayhew et al. (2017) reduce the resource requirement by proposing a cheap translation method, which “translates” the training data from the source to the target language word by word through a bilingual lexicon. While Xie et al. (2018) reduce the requirement of bilingual lexicons by an unsupervised word-by-word translation through CLWEs. Duh, 2017; Yang et al., 2017; Ammar et al., 2016; Kim et al., 2017). However, these models are usually obtained through joint learning and require annotated data from the target language. 6 Conclusion In this paper, we focused on a low-resources cross-lingual setting and proposed transfer learning methods based on the alignment of deep semantic spaces between different languages. The proposed multilingual language model bridges different languages by automatically learning crosslingual disambiguated representations. Abundant NER and POS experiments are conducted on the benchmark datasets. Experimental results show that our approaches using only monolingual co"
D19-1095,P12-1073,0,0.178857,"oring crosslingual transfer through language-independent features, such as morphological features and universal POS tags for cross-lingual NER (Tsai et al., 2016) and dependency parsers (McDonald et al., 2011). However, these approaches require linguistic knowledge for language-independent feature engineering, which is expensive in low-resource settings. Other work relies on bilingual resources to transfer knowledge from source languages to target languages. Parallel corpora are widely used to project annotations from the source to the target side (Yarowsky et al., 2001; Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014). These methods could achieve strong performance with a large amount of bilingual data, which is scarce in low-resource settings. Recent research leverages cross-lingual word embeddings (CLWEs) to establish inter-lingual connections and reduce the requirements of parallel data to a small lexicon or even no bilingual resource (Ni et al., 2017; Fang and Cohn, 2017; Xie et al., 2018). However, word embedding spaces may not be completely isomorphic due to language-specific linguistic properties, and therefore cannot be perfectly aligned. For example, different from English"
D19-1095,N16-1030,0,0.227248,"dvantage of deep contextualized representations. Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages, and is also effective on distant language pairs such as English and Chinese.1 1 Introduction Sequence labeling tasks such as named entity recognition (NER) and part-of-speech (POS) tagging are fundamental problems in natural language processing (NLP). Recent sequence labeling models achieve state-of-the-art performance by combining both character-level and word-level information (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lample et al., 2016). However, these models heavily rely on large-scale annotated training data, which may not be available in most languages. Cross-lingual transfer learning is proposed to address the label scarcity problem by transferring annotations from high-resource languages (source languages) to low-resource languages (target languages). In this scenario, a ma∗ Corresponding Author. Kenny Q. Zhu was partially supported by NSFC grant 91646205 and Alibaba visiting scholar program. 1 The code is released at https://github.com/ baozuyi/MLMA. † jor challenge is how to bridge interlingual gaps with modest resour"
D19-1095,P18-1074,0,0.0374892,"Missing"
D19-1095,W02-0109,0,0.123498,"i is set to be 1.0 for each language. For alignv al ment, λm l , λl , λl are set to be 0.1, 0.01 and 1.0 for every layer l, and λiden is set to be 100. For languages except English, the latest dump of Wikipedia is used as monolingual corpora. For English, we use 1B Word Benchmark (Chelba et al., 2013) to reduce the effects of potential internal alignment in Wikipedia (Zirikly and Hagiwara, 2015; Tsai et al., 2016). All characters are preprocessed to lowercase, and Chinese text are converted into the simplified version through OpenCC2 . The corpora of European languages are tokenized by nltk (Loper and Bird, 2002) and Chinese text is segmented using Ltp3 . 3.3 Details of Sequence Labeling Model In our experiments, we set the hidden size of wordlevel LSTM and character-level LSTM to be 300 and 100, respectively. The character embedding size is set to be 100. We apply dropout at both the input and the output of word-level LSTM to prevent overfitting. The dropout rate is set to be 0.5. We train the sequence labeling model for 20 epochs using Adam optimizer with a batch size of 20 and perform an early stopping when there is no improvement for 3 epochs. We set the initial learning rate to be 0.001 and decay"
D19-1095,P16-1101,0,0.0394123,"s at all and take advantage of deep contextualized representations. Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages, and is also effective on distant language pairs such as English and Chinese.1 1 Introduction Sequence labeling tasks such as named entity recognition (NER) and part-of-speech (POS) tagging are fundamental problems in natural language processing (NLP). Recent sequence labeling models achieve state-of-the-art performance by combining both character-level and word-level information (Chiu and Nichols, 2016; Ma and Hovy, 2016; Lample et al., 2016). However, these models heavily rely on large-scale annotated training data, which may not be available in most languages. Cross-lingual transfer learning is proposed to address the label scarcity problem by transferring annotations from high-resource languages (source languages) to low-resource languages (target languages). In this scenario, a ma∗ Corresponding Author. Kenny Q. Zhu was partially supported by NSFC grant 91646205 and Alibaba visiting scholar program. 1 The code is released at https://github.com/ baozuyi/MLMA. † jor challenge is how to bridge interlingual g"
D19-1095,D17-1269,0,0.0324294,"Missing"
D19-1095,D11-1006,0,0.0930143,"Missing"
D19-1095,P17-1135,0,0.0242815,"Missing"
D19-1095,N18-1202,0,0.0322332,"ocessing and the 9th International Joint Conference on Natural Language Processing, pages 1028–1039, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics that the semantic meanings of words from different languages can be roughly aligned at a conceptual level and it is more reasonable to align deep semantic representations instead of shallow word embeddings. Meanwhile, monolingual contextualized embeddings derived from language models have shown to be effective for extracting semantic information and have achieved significant improvement on several NLP tasks (Peters et al., 2018). In this paper, we propose a Multilingual Language Model with deep semantic Alignment (MLMA). We train MLMA on monolingual corpora from each language and align its internal states across different languages. Then MLMA is utilized to generate language-independent representations and to bridge the gaps between highresource and low-resource languages. For evaluation, we conduct extensive experiments on the NER and POS benchmark datasets under crosslingual settings. The experiment results show that our methods achieve substantial improvements comparing to previous state-of-the-art methods in Euro"
D19-1095,Q13-1001,0,0.0690977,"Missing"
D19-1095,N12-1052,0,0.0722492,"Missing"
D19-1095,W02-2024,0,0.0634947,"Then the pre-trained CLCRk is concatenated with ek to form a word-level embedding xk . Finally, the sequence of word-level embeddings [x1 , x2 , . . . , xN ] are fed into the word-level LSTM, and the linear-chain CRF are employed to 1031 predict the probability distribution for all possible output label sequences. 3 Experiments We first introduce the datasets used in the experiment and then the implementation details of our models, before presenting the results on NER and POS tasks. 3.1 Datasets For cross-lingual NER, we evaluate the proposed approaches on CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain four European languages, English (en), Spanish (es), Dutch (nl), German (de) and four entity types (person, location, organization, and MISC). We also evaluate a distant language pair, English-Chinese, on OntoNotes(v4.0) dataset (Hovy et al., 2006). We adopt the same dataset split and four valid entity types (person, location, organization, and GPE) as described in (Wang and Manning, 2014). For cross-lingual POS, we use the Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv) portion"
D19-1095,K16-1022,0,0.0731161,"y problem by transferring annotations from high-resource languages (source languages) to low-resource languages (target languages). In this scenario, a ma∗ Corresponding Author. Kenny Q. Zhu was partially supported by NSFC grant 91646205 and Alibaba visiting scholar program. 1 The code is released at https://github.com/ baozuyi/MLMA. † jor challenge is how to bridge interlingual gaps with modest resource requirements. There is a large body of work exploring crosslingual transfer through language-independent features, such as morphological features and universal POS tags for cross-lingual NER (Tsai et al., 2016) and dependency parsers (McDonald et al., 2011). However, these approaches require linguistic knowledge for language-independent feature engineering, which is expensive in low-resource settings. Other work relies on bilingual resources to transfer knowledge from source languages to target languages. Parallel corpora are widely used to project annotations from the source to the target side (Yarowsky et al., 2001; Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014). These methods could achieve strong performance with a large amount of bilingual data, which is scarce in low-resource s"
D19-1095,Q14-1005,0,0.240746,"transfer through language-independent features, such as morphological features and universal POS tags for cross-lingual NER (Tsai et al., 2016) and dependency parsers (McDonald et al., 2011). However, these approaches require linguistic knowledge for language-independent feature engineering, which is expensive in low-resource settings. Other work relies on bilingual resources to transfer knowledge from source languages to target languages. Parallel corpora are widely used to project annotations from the source to the target side (Yarowsky et al., 2001; Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014). These methods could achieve strong performance with a large amount of bilingual data, which is scarce in low-resource settings. Recent research leverages cross-lingual word embeddings (CLWEs) to establish inter-lingual connections and reduce the requirements of parallel data to a small lexicon or even no bilingual resource (Ni et al., 2017; Fang and Cohn, 2017; Xie et al., 2018). However, word embedding spaces may not be completely isomorphic due to language-specific linguistic properties, and therefore cannot be perfectly aligned. For example, different from English, Chinese nouns do not di"
D19-1095,D18-1034,0,0.126039,"er knowledge from source languages to target languages. Parallel corpora are widely used to project annotations from the source to the target side (Yarowsky et al., 2001; Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014). These methods could achieve strong performance with a large amount of bilingual data, which is scarce in low-resource settings. Recent research leverages cross-lingual word embeddings (CLWEs) to establish inter-lingual connections and reduce the requirements of parallel data to a small lexicon or even no bilingual resource (Ni et al., 2017; Fang and Cohn, 2017; Xie et al., 2018). However, word embedding spaces may not be completely isomorphic due to language-specific linguistic properties, and therefore cannot be perfectly aligned. For example, different from English, Chinese nouns do not distinguish singular and plural forms, while Spanish nouns distinguish masculine and feminine. On the other hand, NER tags such as person names, organizations, and locations are shared across different languages. Language-independent frameworks such as universal conceptual cognitive annotation (Abend and Rappoport, 2013), universal POS (Petrov et al., 2011a), and universal dependenc"
D19-1095,H01-1035,0,0.584053,"uirements. There is a large body of work exploring crosslingual transfer through language-independent features, such as morphological features and universal POS tags for cross-lingual NER (Tsai et al., 2016) and dependency parsers (McDonald et al., 2011). However, these approaches require linguistic knowledge for language-independent feature engineering, which is expensive in low-resource settings. Other work relies on bilingual resources to transfer knowledge from source languages to target languages. Parallel corpora are widely used to project annotations from the source to the target side (Yarowsky et al., 2001; Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014). These methods could achieve strong performance with a large amount of bilingual data, which is scarce in low-resource settings. Recent research leverages cross-lingual word embeddings (CLWEs) to establish inter-lingual connections and reduce the requirements of parallel data to a small lexicon or even no bilingual resource (Ni et al., 2017; Fang and Cohn, 2017; Xie et al., 2018). However, word embedding spaces may not be completely isomorphic due to language-specific linguistic properties, and therefore cannot be perfectly alig"
D19-1095,P15-2064,0,0.091716,"l., 2015) of 8192 samples. We only keep the sentences containing less than 200 tokens for training and group them into batches by length. Each batch contains around 4096 tokens for each language. The language modeling weight λlm i is set to be 1.0 for each language. For alignv al ment, λm l , λl , λl are set to be 0.1, 0.01 and 1.0 for every layer l, and λiden is set to be 100. For languages except English, the latest dump of Wikipedia is used as monolingual corpora. For English, we use 1B Word Benchmark (Chelba et al., 2013) to reduce the effects of potential internal alignment in Wikipedia (Zirikly and Hagiwara, 2015; Tsai et al., 2016). All characters are preprocessed to lowercase, and Chinese text are converted into the simplified version through OpenCC2 . The corpora of European languages are tokenized by nltk (Loper and Bird, 2002) and Chinese text is segmented using Ltp3 . 3.3 Details of Sequence Labeling Model In our experiments, we set the hidden size of wordlevel LSTM and character-level LSTM to be 300 and 100, respectively. The character embedding size is set to be 100. We apply dropout at both the input and the output of word-level LSTM to prevent overfitting. The dropout rate is set to be 0.5."
D19-5412,C10-1039,0,0.699654,"are incorporated in this work to capture the position of a sentence in the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35"
D19-5412,W09-1802,0,0.288394,"resentations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the qualit"
D19-5412,hong-etal-2014-repository,0,0.313108,"e modelled using shallow and linguistically informed features, but the rise of deep contextualized representations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and select"
D19-5412,P19-1098,1,0.830669,"ters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019) by incorporating deep contextualized representations into DPP, with an emphasis on better sentence selection for extractive multi-document summarization. The major research contributions of this work include the following: (i) we make a first attempt to combine DPP with BERT representations to measure sentence quality and diversity and report encouraging results on benchmark summarization datasets; (ii) our findings suggest that it is best to model sentence quality, i.e., how important a sentence is to the summary, by combining semantic representations and surface indicators of the sentence,"
D19-5412,P19-1285,0,0.0200765,"high quality, any set containing it will have a high probability score. If two sentences contain redundant information, they cannot both be included in the summary, thus any set containing both of them will have a low probability. DPP focuses on selecting the most probable set of sentences to form a summary according to sentence quality and diversity measures. To better measure quality and diversity aspects, we draw on deep contextualized representations. A number of models have been proposed recently, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019) by incorporating deep contextualized representations into DPP, with an em"
D19-5412,P19-1209,1,0.829815,"s a feature vector for sentence i and θ are feature weights to be learned during DPP training. We optimize θ by maximizing log-likelihood with gradient descent, illustrated as follows: L(θ)= M X logP(Yˆ (m);L(m)(θ)), 2.1 We introduce two models that fine-tune the BERTbase architecture (Devlin et al., 2018) to calculate the similarity between a pair of sentences (BERTsim) and learn representations that characterize the importance of a single sentence (BERT-imp). Importantly, training instances for both BERT models are derived from single-document summarization dataset (Hermann et al., 2015) by Lebanoff et al. (2019), containing a collection of single sentences (or sentence pairs) and their associated labels. During testing, the trained BERT models are applied to single sentences and sentence pairs derived from multi-document input to obtain quality and similarity measures. BERT-sim takes as input a pair of sentences and transforms each token in the sentence into an embedding using an embedding layer. They are then passed through the BERT-base architecture to pro(3) m=1 ∇θ = M X X m=1 i∈Yˆ (m) f (i)− X (m) f (j)Kjj , BERT Architecture (4) j 99 duce a vector representing the input sentence pair. The vector"
D19-5412,D15-1228,1,0.848755,"dings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar, 2012). A quality measure is a positive number indicating how important 98 Proceedings of the 2nd Worksh"
D19-5412,W04-1013,0,0.028359,"ce pairs and the instances are balanced. 1 The sentence features include the length and position of a sentence, the cosine similarity between sentence and document TF-IDF vectors (Kulesza and Taskar, 2011). We abstain from using sophisticated features to avoid model overfitting. 2 100 The coefficient is set to be 0.9 for both datasets. DUC/TAC We evaluate our DPP approach (§2) on multi-document summarization datasets including DUC and TAC (Over and Yen, 2004; Dang and Owczarzak, 2008). The task is to generate a summary of 100 words from a collection of news articles. We report ROUGE F-scores (Lin, 2004)3 on DUC-04 (trained on DUC-03) and TAC-11 (trained on TAC-08/09/10) following standard settings (Hong et al., 2014). Ground-truth extractive summaries used in DPP training are obtained from Cho et al. (2019). 3.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm (Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) Experiment Settings We"
D19-5412,N10-1134,0,0.0438514,"DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar, 2012). A quality measure is a positive number indicating how important 98 Proce"
D19-5412,2021.ccl-1.108,0,0.129635,"Missing"
D19-5412,N18-1202,0,0.0532157,"ty measure compares a pair of sentences for redundancy. If a sentence is of high quality, any set containing it will have a high probability score. If two sentences contain redundant information, they cannot both be included in the summary, thus any set containing both of them will have a low probability. DPP focuses on selecting the most probable set of sentences to form a summary according to sentence quality and diversity measures. To better measure quality and diversity aspects, we draw on deep contextualized representations. A number of models have been proposed recently, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019)"
D19-5412,P17-1099,0,0.52447,"the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35 Table 2: Results on the DUC-04 dataset evaluated by ROUGE. † indica"
D19-5412,C10-1111,0,0.0374697,"ther, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar"
D19-5412,C18-1146,1,0.940578,"e the position of a sentence in the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35 Table 2: Results on the DUC-04 datas"
D19-5412,N09-1041,0,\N,Missing
N15-1079,baccianella-etal-2010-sentiwordnet,0,0.00305815,"entity from each sentence, and if the recognized entity is also identified as a named entity by Stanford CoreNLP8 , we use this entity’s DBpedia abstract content to extend the bigrams. For example, in the bigram ‘Kashmir area’, the word ‘Kashmir’ is recognized as an entity by both (Stanford CoreNLP and DBpedia Spotlight service), then we use the description for ‘Kashmir’ from DBpedia9 to extend this bigram, and calculate the cosine similarity between this description and the topic query and top-k most frequent unigrams in the documents. 4.7 Sentiment Feature from SentiWordNet10 SentiWordNet (Baccianella et al., 2010) is an extension on WordNet and it further assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity. The sentiment score of a bigram is the average score of the two words in the bigram. To sum up, the features we use include the internal features, and external ones derived from various resources: news article corpus with summaries, Wikipeida, DBpedia, WordNet and SentiWordNet. Some external features represent the inherent importance of bigrams. For example, features extracted from the news article corpus and wikipedia are used to represent how often bigrams"
N15-1079,W02-1001,0,0.020389,"ed to the bigram candidates. After the above filtering, we further drop bigrams if both words are stop words, as previous work in (Gillick et al., 2009). 3.2 Weight Training We propose to train the feature weights in a joint learning fashion. In the ILP summarization framework, we use the following new objective function: max P i (θ · f(bi ))ci (7) We replace the wi in Formula 1 with a vector inner product of bigram features and their corresponding weights. Constraints remain the same as those in Formula 2 to 6. To train the model (feature weights), we leverage structured perceptron strategy (Collins, 2002) to update the feature weights whenever the hypothesis offered by the ILP decoding process is incorrect. Binary class labels are used for bigrams in the learning process, that is, we only consider whether a bigram is in the system generated summary or human summaries, not their term or document frequency. During perceptron training, a fixed learning rate is used and parameters are averaged to prevent overfitting. 4 Features for Bigrams We use a rich set of features to represent each bigram candidate, including internal features based on the test documents, and features extracted from external"
N15-1079,P14-1119,0,0.0156288,"ation performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using external resources to extract and measure key phrases is very effective. In (Medelyan et al., 2009), Wikipedia-based key phrases are determined based on a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. Query logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the"
N15-1079,E14-1075,0,0.285923,"uery logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external information has also been used to measure word salience. Some TAC systems like (Kumar et al., 2010; Jia et al., 2010) used Wiki as an important external resource to measure the words’ importance, which helped improve the summarization results. Hong and Nenkova (2014) introduced a supervised model for predicting word importance that incorporated a rich set of features. Tweets information is leveraged by (Wei and Gao, 2014) to help generate news highlights. In this paper our focus is on choosing useful bigrams and estimating accurate weights to use in the concept-based ILP methods. We explore many external resources to extract features for bigram candidates, and more importantly, propose to estimate the feature weights in a joint process via structured perceptron learning that optimizes summary sentence selection. 3 Summarization System In this study we use"
N15-1079,O97-1002,0,0.045674,"lar to the above method, here we still focus on measuring the similarity between a bigram and the topic query, but based on WordNet. We use WordNet to identify the synonyms of nouns, verbs, 2 3 https://code.google.com/p/word2vec/ http://wordnet.princeton.edu/ and adjectives from each bigram and the query of the topic. Then every bigram and sentence can be represented as a bag of synonyms of the original words. Finally based on these synonyms we leverage the following four similarity measurements: Lin Similarity (Lin, 1998), Wu-Palmer Similarity (Wu and Palmer, 1994), Jiang-Conrath Similarity (Jiang and Conrath, 1997), and Resnik Similarity (Resnik, 1995). These four similarity measurements are all implemented in the NLTK toolkit4 . We expect that these features would improve the estimation accuracy because they can overcome the ambiguity and the diversity of the vocabulary. 4.5 Importance based on Wikipedia Wikipedia is a very popular resource used in many different tasks. In order to obtain more precise external information from Wikipedia for our task, we collect the articles from Wikipedia by two steps. If the query is already the title of a wiki page, we will not further gather other wiki pages for thi"
N15-1079,P98-1112,0,0.0433599,"ula 1) is simply set as their document frequency. Although this setting has been demonstrated to be quite effective, its gap with the oracle experiment (using bigrams that appear in the human summaries) is still very large, suggesting potential gains by using better 780 bigrams/concepts in the ILP optimization method. Details are described in (Gillick et al., 2009). In this paper, rather than considering all the bigrams, we propose to utilize syntactic information to help select important bigrams. Intuitively bigrams containing content words carry more topic related information. As proven in (Klavans and Kan, 1998), nouns, verbs, and adjectives were indeed beneficial in document analysis. Therefore we focus on choosing bigrams containing these words. First, we use a bottom-up strategy to go through the constituent parse tree and identify the ‘NP’ nodes in the lowest level of the tree. Then all the bigrams in these base NPs are kept as candidates. Second, we find the verbs and adjectives from the sentence based on the POS tags, and construct bigrams by concatenating the previous or the next word of that verb or adjective. If these bigrams are not included in those already found from the base NPs, they ar"
N15-1079,D13-1047,1,0.829429,"08 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sen"
N15-1079,P13-1099,1,0.723081,"08 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sen"
N15-1079,D14-1076,1,0.640471,"and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using external resources to extrac"
N15-1079,N10-1134,0,0.0647145,"mpetitiveness of our proposed methods. 2 Related Work Optimization methods have been widely used in extractive summarization lately. McDonald (2007) first introduced the sentence level ILP for summarization. Later Gillick et al. (2009) revised it to concept-based ILP, which is similar to the Bud779 geted Maximal Coverage problem in (Khuller et al., 1999). The concept-based ILP system performed very well in the TAC 2008 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also"
N15-1079,W04-1013,0,0.0553827,"Missing"
N15-1079,S10-1055,0,0.0277648,"ure key phrases is very effective. In (Medelyan et al., 2009), Wikipedia-based key phrases are determined based on a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. Query logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external information has also been used to measure word salience. Some TAC systems like (Kumar et al., 2010; Jia et al., 2010) used Wiki as an important external resource to measure the words’ importance, which helped improve the summarization results. Hong and Nenkova (2014) introduced a supervised model for predicting word importance that incorporated a rich set of features. Tweets information is leveraged by (Wei and Gao, 2014) to help generate news highlights. In this paper our focus is on choosing useful bigrams and estimating accurate weights to use in the concept-bas"
N15-1079,W09-1801,0,0.0746698,"oblem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the docu"
N15-1079,D09-1137,0,0.0146729,"sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using external resources to extract and measure key phrases is very effective. In (Medelyan et al., 2009), Wikipedia-based key phrases are determined based on a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. Query logs were also used as another external resource by (Yih et al., 2006) to exploit the observation that a candidate is potentially important if it was used as a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external informati"
N15-1079,C12-1128,0,0.0236835,".2.1 Summarization Results Table 1 shows the ROUGE-2 results of our proposed joint system, the ICSI system (which uses document frequency threshold to select bigram concepts and uses df as weights), the best performing system in the NIST TAC evaluation, and the state of the art performance we could find. The result of our proposed method is statistically significantly better than that of ICSI ILP (p &lt; 0.05 based on paired ttest). It is also statistically significantly (p &lt; 0.05) better than that of TAC Rank1 except 2011, and previous best in 2008 and 2010. The 2011 previous best results from (Ng et al., 2012) involve some rule-based sentence compression, which improves the ROUGE value. If we apply the same or similar rule-based sentence compression on our results, and the ROUGE-2 of our proposed method improves to 14.38. ICSI ILP TAC Rank1 Previous Best Proposed Method 2008 10.23 10.38 10.76† 11.84 2009 11.60 12.16 12.46† 12.77 2010 10.03 9.57 10.8‡ 11.78 2011 12.71 13.44 13.93∗ 13.97 34 30 28 26 24 22 20 2008 11 http://www.gurobi.com 783 2009 2010 2011 Figure 1: Percentage of correct bigrams in the selected bigrams from ICSI and our proposed system. Table 1: ROUGE-2 summarization results.† is fro"
N15-1079,W12-2601,0,0.0166704,"Missing"
N15-1079,P06-1055,0,0.0273014,"Missing"
N15-1079,E09-1089,0,0.0220486,"ncept-based ILP, which is similar to the Bud779 geted Maximal Coverage problem in (Khuller et al., 1999). The concept-based ILP system performed very well in the TAC 2008 and 2009 summarization task (Gillick et al., 2008; Gillick et al., 2009). After that, the global optimization strategy attracted increasing attention in the summarization task. Lin and Bilmes (2010) treated the summarization task as a maximization problem of submodular functions. Davis et al. (2012) proposed an optimal combinatorial covering algorithm combined with LSA to measure the term weight for extractive summarization. Takamura and Okumura (2009) also defined the summarization problem as a maximum coverage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and"
N15-1079,C14-1083,0,0.0184099,"a search query. Similarly terminological databases have been exploited to encode the salience of candidate key phrases in scientific papers (Lopez and Romary, 2010). In summarization, external information has also been used to measure word salience. Some TAC systems like (Kumar et al., 2010; Jia et al., 2010) used Wiki as an important external resource to measure the words’ importance, which helped improve the summarization results. Hong and Nenkova (2014) introduced a supervised model for predicting word importance that incorporated a rich set of features. Tweets information is leveraged by (Wei and Gao, 2014) to help generate news highlights. In this paper our focus is on choosing useful bigrams and estimating accurate weights to use in the concept-based ILP methods. We explore many external resources to extract features for bigram candidates, and more importantly, propose to estimate the feature weights in a joint process via structured perceptron learning that optimizes summary sentence selection. 3 Summarization System In this study we use the ILP-based summarization framework (Formulas 1-6) that tries to maximize the weights of the selected concepts (bigrams) under the summary length constrain"
N15-1079,D12-1022,0,0.195343,"rage problem and used a branch-and-bound method to search for the optimal solution. Li et al. (2013b) used the same ILP framework as (Gillick et al., 2009), but incorporated a supervised model to estimate the bigram frequency in the final summary. Similar optimization methods are also widely used in the abstractive summarization task. Martins and Smith (2009) leveraged ILP technique to jointly select and compress sentences for multi-document summarization. A novel summary guided sentence compression was proposed by (Li et al., 2013a) and it successfully improved the summarization performance. Woodsend and Lapata (2012) and Li et al. (2014) both leveraged constituent parser trees to help sentence compression, which is also modeled in the optimization framework. But these kinds of work involve using complex linguistic information, often based on syntactic analysis. Since the language concepts (or bigrams) can be considered as key phrases of the documents, the other line related to our work is how to extract and measure the importance of key phrases from documents. In particular, our work is related to key phrase extraction by using external resources. A survey by (Hasan and Ng, 2014) showed that using externa"
N15-1079,C98-1108,0,\N,Missing
N15-1079,P94-1019,0,\N,Missing
N15-1145,P11-1049,0,0.105003,"Missing"
N15-1145,N10-3003,0,0.132105,"on assumes that users already have some information about a given topic from an old data set, and thus for a new data set the system aims to generate a summary that contains as much novel information as possible. This task was first introduced at DUC 2007 and then continued until TAC 2011. It is very useful to chronological events in real applications. Most basic update summarization methods are variants of multi-document summarization methods, with some consideration of the difference between the earlier and later document sets (Boudin et al., 2008; Fisher and Roark, 2008; Long et al., 2010; Bysani, 2010). One important line is to use graphbased co-ranking. They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship. For example, Li et al. (2008) was inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processe"
N15-1145,W02-1001,0,0.113173,"by (Ng et al., 2012): α wu ∈S dfnew (wu )+(1−α) |S| wb ∈S dfnew (wb ) , where wu and wb are unigrams and bigrams respectively in sentence S. Feature 18 and 19 are variants of Features 11, where instead of document frequency (df in the formula above), bigram and unigram’s novelty and uniqueness values are used. Among these features, the feature values of feature 4, 5 and 6 are discrete. In this study, we discretized all the other continuous values into ten categories according to the value range in the training data. To train the model (feature weights), we use the average perceptron strategy (Collins, 2002) to update the feature weights whenever the hypothesis by the ILP decoding process is incorrect. Binary class labels are used for bigrams in the learning process, that is, we only consider whether a bigram is in the system generated summary or human summaries, not their term or document frequency. We use a fixed learning rate (0.1) in training. 2.3 Sentence Reranking on ILP Results In the ILP method, sentence selection is done by considering the concepts that a sentence contains. It is difficult to add indicative features in this framework to explicitly represent the sentence’s salience, and m"
N15-1145,E12-1022,0,0.0637541,"ame collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual news summarization. In addition, generative models, such as topic models, have also been adopted for this task. For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergK"
N15-1145,C08-1062,0,0.015948,"as possible. This task was first introduced at DUC 2007 and then continued until TAC 2011. It is very useful to chronological events in real applications. Most basic update summarization methods are variants of multi-document summarization methods, with some consideration of the difference between the earlier and later document sets (Boudin et al., 2008; Fisher and Roark, 2008; Long et al., 2010; Bysani, 2010). One important line is to use graphbased co-ranking. They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship. For example, Li et al. (2008) was inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual"
N15-1145,C12-1098,0,0.0634913,"m for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual news summarization. In addition, generative models, such as topic models, have also been adopted for this task. For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The"
N15-1145,D13-1047,1,0.859056,"variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP"
N15-1145,P13-1099,1,0.863919,"variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP"
N15-1145,D14-1076,1,0.872661,"ion (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts used in the ILP model. Second, we design a sentence r"
N15-1145,W04-1013,0,0.0277371,"utput from the first ILP step) is not too big, redundancy is not a big problem. 3 Experiments and Results 3.1 Data and Experiment Setup We evaluate our methods using several recent TAC data sets, from 2008 to 2011. Every topic has two sets of 10 documents (Set A and B). The update task aims to create a 100-word summary from Set B given a topic query and Set A. When evaluating on one year’s data, we use the data from the other three years as the training set. This applies to both the supervised ILP method and the sentence reranking regression model. All the summaries are evaluated using ROUGE (Lin, 2004). An academic free solver2 does all the ILP decoding and libsvm3 is used for SVR implementation. 3.2 Results Table 2 and Table 3 show the R2 and R-SU4 values on different TAC data sets for the following systems. • ILP baseline. This is the unsupervised ILPbased summarization system (Gillick et al., 2009), in which only bigrams with document frequency greater than 2 are used in the ILP summarization process, and weight wi is the document frequency of that bigram. • TAC best. This is the best result in the TAC update summarization evaluation.4 Note that 2 http://www.gurobi.com http://www.csie.nt"
N15-1145,W09-1801,0,0.0682842,"e, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set"
N15-1145,C12-1128,0,0.125028,"Missing"
N15-1145,C12-2126,0,0.163482,"Roark, 2008; Long et al., 2010; Bysani, 2010). One important line is to use graphbased co-ranking. They rank the sentences in the earlier and later document sets simultaneously by considering the sentence relationship. For example, Li et al. (2008) was inspired by the intuition that “a sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas receives a negative influence from the sentences that correlates to it in the different (or previously read) collection’, and proposed a graph based sentence ranking algorithm for update summarization. Wan (2012) integrated two co-ranking processes by adding some strict constraints, which led to more accurate computation of sentences’ scores for update summarization. A similar method was also applied earlier by (Wan et al., 2011) for multilingual news summarization. In addition, generative models, such as topic models, have also been adopted for this task. For example, Delort and Alfonseca (2012) proposed a novel nonparametric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of"
N15-1145,D12-1022,0,0.095154,"metric Bayesian approach, a variant of Latent Dirichlet Allocation (LDA), aiming to distinguish between common information and novel information. Li et al. (2012) borrowed the idea of evolutionary clustering and proposed a three-level HDP (Hierarchical Dirichlet Process) model to represent the diversity and commonality between aspects discovered from two different document data sets. One of the most competitive summarization methods is based on Integer Linear Programming (ILP). It has been widely adopted in the generic summarization task (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012; Li et al., 2013a; Li et al., 2013b; Li et al., 2014). In this paper, we use the ILP summarization 1317 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1317–1322, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics framework for the update summarization task, and make improvement from two aspects, with the goal to more discriminatively represent both the salience and novelty of words and sentences. First, we use supervised models and a rich set of features to learn the weights for the bigram concepts"
P13-1099,C12-1056,0,0.0466913,"ion task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Les"
P13-1099,W09-1802,0,0.829851,"gram-based ILP for Extractive Summarization Chen Li, Xian Qian, and Yang Liu The University of Texas at Dallas Computer Science Department chenli,qx,yangl@hlt.utdallas.edu Abstract tion. Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric. In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary. They used bigrams as such language concepts. The association between the language concepts and sentences serves as the constraints. This ILP method is formally represented as below (see (Gillick and Favre, 2009) for more details): In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our s"
P13-1099,R09-1002,0,0.0428398,"bigram b in reference summaries. Finally, we replace Nb,ref in Formula (15) with Eq (14) and get the objective function below: P ′ X i,b =b exp{w f (bi )} e (16) max Nb,ref log Pi ′ j exp{w f (bj )} b This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): – 10. Paragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph. 3 Experiments 3.1 Data We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query (with a title and more detailed description). For model training, we also included two years’ DUC data (2006 and 2007). When evaluating on one TAC data set, we use the other years of the TAC data plus the two DUC data sets as"
P13-1099,D10-1047,0,0.369937,"LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In addition, we revise the ILP to maximize the bigram gain (which is expected to be highly correlated with ROUGE-2"
P13-1099,P11-1049,0,0.132676,"nces. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for su"
P13-1099,P11-1050,0,0.0951625,"Missing"
P13-1099,C10-2060,0,0.0132945,"the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts a"
P13-1099,N10-1134,0,0.198027,"face realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. data imbalance problem (Xie and Liu, 2010)."
P13-1099,W04-1013,0,0.0839556,"hich are selected from a subset of the sentences, and their document frequency as the weight in the objective function. In this paper, we propose to find a candidate summary such that the language concepts (e.g., bigrams) in this candidate summary and the reference summary can have the same frequency. We expect this restriction is more consistent with the 1004 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004–1013, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ROUGE evaluation metric used for summarization (Lin, 2004). In addition, in the previous conceptbased ILP method, the constraints are with respect to the appearance of language concepts, hence it cannot distinguish the importance of different language concepts in the reference summary. Our method can decide not only which language concepts to use in ILP, but also the frequency of these language concepts in the candidate summary. To estimate the bigram frequency in the summary, we propose to use a supervised regression model that is discriminatively trained using a variety of features. Our experiments on several TAC summarization data sets demonstrate"
P13-1099,W04-3252,0,0.0126852,"maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework"
P13-1099,P11-2113,0,0.0162996,"etter than MMR and the diversity penalty strategy in sentence/concept selection. Other global optimization methods include submodularity (Lin and Bilmes, 2010) and graph-based approaches (Erkan and Radev, 2004; Leskovec et al., 2005; Mihalcea and Tarau, 2004). Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising. For example, (Celikyilmaz and Hakkani-T¨ur, 2011) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries. (Darling and Song, 2011) applied it to separate the semantically important words from the lowcontent function words. data imbalance problem (Xie and Liu, 2010). In this paper, we propose to incorporate the supervised method into the concept-based ILP framework. Unlike previous work using sentencebased supervised learning, we use a regression model to estimate the bigrams and their weights, and use these to guide sentence selection. Compared to the direct sentence-based classification or regression methods mentioned above, our method has an advantage. When abstractive summaries are given, one needs to use that informa"
P13-1099,W02-0401,0,0.20145,"er study closely related to ours is (Davis et al., 2012), which leveraged Latent Semantic Analysis (LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In additio"
P13-1099,D12-1024,0,0.0319853,"Latent Semantic Analysis (LSA) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem. 6 Conclusion and Future Work In contrast to these unsupervised approaches, there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not. Different features and classifiers have been explored for this task, such as Bayesian method (Kupiec et al., 1995), maximum entropy (Osborne, 2002), CRF (Galley, 2006), and recently reinforcement learning (Ryang and Abekawa, 2012). (Aker et al., 2010) used discriminative reranking on multiple candidates generated by A* search. Recently, research has also been performed to address some issues in the supervised setup, such as the class In this paper, we leverage the ILP method as a core component in our summarization system. Different from the previous ILP summarization approach, we propose a supervised learning method (a discriminatively trained regression model) to determine the importance of the bigrams fed to the ILP module. In addition, we revise the ILP to maximize the bigram gain (which is expected to be highly co"
P13-1099,P05-1044,0,0.0368922,"number of the selected sentences. – 7. Sentence similarity: Sentence similarity with topic’s query, which is the concatenation of topic title and description. – 8. Sentence position: Sentence position in the document. – 9. Sentence length: The number of words in the sentence. b eb,ref is the true normalized frequency of where N bigram b in reference summaries. Finally, we replace Nb,ref in Formula (15) with Eq (14) and get the objective function below: P ′ X i,b =b exp{w f (bi )} e (16) max Nb,ref log Pi ′ j exp{w f (bj )} b This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). We use gradient decent method for parameter estimation, initial w is set with zero. 2.3 Features Each bigram is represented using a set of features in the above regression model. We use two types of features: word level and sentence level features. Some of these features have been used in previous work (Aker and Gaizauskas, 2009; Brandow et al., 1995; Edmundson, 1969; Radev, 2001): – 10. Paragraph starter: Binary feature indicating whether this sentence is the beginning of a paragraph. 3 Experiments 3.1 Data We evaluate our method using several recent TAC data sets, from 2008 to 2011. The TA"
P13-1099,D12-1022,0,0.427964,"vised methods have been widely used. In particular, recently several optimization approaches have demonstrated 1010 competitive performance for extractive summarization task. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) uses a greedy algorithm to find summary sentences. (McDonald, 2007) improved the MMR algorithm to dynamic programming. They used a modified objective function in order to consider whether the selected sentence is globally optimal. Sentencelevel ILP was also first introduced in (McDonald, 2007), but (Gillick and Favre, 2009) revised it to concept-based ILP. (Woodsend and Lapata, 2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. (Galanis et al., 2012) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary. (Berg-Kirkpatrick et al., 2011) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization. (Jin et al., 2010) made a comparative study on sentence/concept selection and pairwise and list ranking algorithms, and concluded ILP performed better than MMR and the diversity penalty strategy in s"
P13-1099,W06-1643,0,\N,Missing
P14-3012,C12-1097,1,0.915643,"-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard words. A reverse look-up table is used to store the corresponding possible standard words for the non-standard words for reverse lookup during testing. Liu et al. (2011) extended the above model to handle other types of non-standard words. (Li and Liu, 2012a) used character-blocks (same ones as that in the character-block MT method above) as the units in this sequence labeling framework. There is one list of word candidates from this method. 2 Previous Normalization Methods Used in Reranking In this work we adopt several normalization methods developed in previous studies. The following briefly describes these previous approaches. Next section will introduce our proposed methods using unsupervised learning and discriminative reranking for system combination. 2.1 Character-block level MT Pennell and Liu (2011) proposed to use a character-level MT"
P14-3012,P06-2005,0,0.0548887,"addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible corVarious models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-st"
P14-3012,P05-1022,0,0.0101642,"quence labeling models have better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Section 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in Table 1. This maxent reranking method has shown success in many previous work such as (Charniak and Johnson, 2005; Ji et al., 2006). Features: 1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such features. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candidate. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard"
P14-3012,P11-2013,1,0.960083,"ional spell checking model, which is usually based on edit distance (Damerau, 1964; Levenshtein, 1966). However, this model can not well handle the nonstandard words in social media text due to the large variation in generating them. Another line of work in normalization adopts a noisy channel model. For a non-standard token A, this method finds the most possible standard word Sˆ based on the Bayes rule: Sˆ = argmaxP (S|A) = argmaxP (A|S) ∗ P (S). Different methods have been used to compute P (A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Ma"
P14-3012,W02-1001,0,0.00634159,"al features in the reranking model. We also tried some other lexical features such as the length difference of the non-standard word and the candidate, whether non-standard word contains numbers, etc. But they did not obtain performance gain. Another advantage of the reranker is that we can use information about multiple systems, such as the first three features. 3.2.2 weights), we perform sentence level Viterbi decoding on the training set to find the best hypothesis for each non-standard word. If the hypothesis is incorrect, we update the feature weight using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). Sentence Level Reranking and Decoding In the above reranking method, we only use information about the individual words. When contextual words are available (in sentences or"
P14-3012,P12-1109,0,0.656185,"ts from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done 86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, c Baltimore, Maryland"
P14-3012,C10-2022,0,0.0189352,"del to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible corVarious models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation,"
P14-3012,P12-1055,0,0.177774,"ts from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done 86 Proceedings of the ACL 2014 Student Research Workshop, pages 86–93, c Baltimore, Maryland"
P14-3012,W09-2010,0,0.0332492,"Different methods have been used to compute P (A|S). Pennell and Liu (2010) used a CRF sequence modeling approach for deletion-based abbreviations. Liu et al. (2011) further extended this work by considering more types of non-standard words without explicit pre-categorization for nonstandard tokens. In addition, the noisy channel model has also been utilized on the sentence level. Choudhury et al. (2007) used a hidden Markov model to simulate SMS message generation, considering the non-standard tokens in the input sentence as emission states in HMM and labeling results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at"
P14-3012,J03-1002,0,0.0163151,"Missing"
P14-3012,P11-1038,0,0.206393,"decoding on the training set to find the best hypothesis for each non-standard word. If the hypothesis is incorrect, we update the feature weight using structured perceptron strategy (Collins, 2002). We will explore these different feature and training configurations for reranking in the following experiments. 4 Experiments 4.1 Experimental Setup The following data sets are used in our experiments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised models. • Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by (Han and Baldwin, 2011). Sentence Level Reranking and Decoding In the above reranking method, we only use information about the individual words. When contextual words are available (in sentences or Tweets), we can use that information. If a sentence containing OOV words is given during testing, we can perform standard sentence level Viterbi decoding to combine information from the normalization candidates and language model scores. Furthermore, if sentences are available during training (not just isolated word pairs as used in all the previous supervised individual systems and the Maxent reranking above), we can al"
P14-3012,N13-1039,0,0.0125959,"Missing"
P14-3012,D12-1039,0,0.0303775,"translated to standard words in the second step. This method has been shown to yield high coverage (high accuracy in its n-best hypotheses). There are two candidate lists generated by this two-step MT method. The first one is based on the pronunciation list produced in the first step (some phonetic sequences directly correspond to standard words). The second list is generated from the second translation step. rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2.3 Character-Block level Sequence Labeling Pennell an"
P14-3012,P13-1155,0,0.142065,"in the first step (some phonetic sequences directly correspond to standard words). The second list is generated from the second translation step. rect word. Recently, a new line of work surges relying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2.3 Character-Block level Sequence Labeling Pennell and Liu (2010) used sequence labeling model (CRF) for normalizing deletion-based abbreviation at the character-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used fo"
P14-3012,I11-1109,1,0.92087,"results as possible candidates. Cook and Stevenson (2009) extended work by adding several more subsystems in this error model according to the most common non-standard token’s formation process. Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. Aw et al. (2006) treated SMS as another language, and used MT methods to translate this ‘foreign language’ to regular English. Contractor et al. (2010) used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly introduced an MT method at the token level which translates an unnormalized token to a possible corVarious models have been developed for normalizing informal text. In this paper, we propose two methods to improve normalization performance. First is an unsupervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation, together with other surface similarity measurement. Second we propose a reranking strategy to combine the results from different syste"
P14-3012,W06-3607,0,0.0218599,"better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Section 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in Table 1. This maxent reranking method has shown success in many previous work such as (Charniak and Johnson, 2005; Ji et al., 2006). Features: 1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such features. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candidate. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard word is the same."
P14-3012,W10-0513,0,0.0391831,"the difference is that this is an unsupervised method whereas the sequence labeling uses supervised learning to generate possible candidates. Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsupervised method based on the large corpus to induce dense realvalued low-dimension word embedding and then use the inner product as a measure of semantic similarity. We use the continuous bag-of-words model that is similar to the feedfo"
P14-3012,P07-2045,0,0.00256631,"Missing"
P14-3012,D11-1141,0,0.0159035,"e a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word- and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems. 1 Introduction There has been a lot of research efforts recently on analysis of social media text (e.g., from Twitter and Facebook) (Ritter et al., 2011; Owoputi et al., 2013; Liu et al., 2012b). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing modules. Text normalization has been an important topic for the text-to-speech field. See (Sproat et al., 2001) for a good report of this problem. Recently, much research on normalization has been done 86 Proceedings of the ACL 2014 Student Research Work"
P14-3012,P08-1068,0,0.0166074,"ible standard words for each non-standard word, which is used during testing. This framework is similar to the sequence labeling method described in Section 2.3 in the sense of creating the mapping table between the OOV and dictionary words. However, the difference is that this is an unsupervised method whereas the sequence labeling uses supervised learning to generate possible candidates. Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We p"
P14-3012,N12-1052,0,0.0205637,"Missing"
P14-3012,P10-1040,0,0.0105736,"s for each non-standard word, which is used during testing. This framework is similar to the sequence labeling method described in Section 2.3 in the sense of creating the mapping table between the OOV and dictionary words. However, the difference is that this is an unsupervised method whereas the sequence labeling uses supervised learning to generate possible candidates. Unsupervised Corpus-based Similarity for Normalization Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; T¨ackstr¨om et al., 2012). We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their corresponding standard words. We use the Edinburgh Twitter corpus (Petrovic et al., 2010), and a dictionary obtained from http://ciba.iciba.com/ to identify all the invocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair. The key question is how to compute this likelihood or similarity. We propose to use an unsu"
P14-3012,D13-1007,0,0.592257,"elying on the analysis of huge amount of twitter data, often in an unsupervised fashion. By using context information from a large corpus, Han et al. (2012) generated possible variant and normalization pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitution. Hassan and Menezes (2013) proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. Yang and Eisenstein (2013) presented a unified unsupervised statistical model for text normalization. 2.3 Character-Block level Sequence Labeling Pennell and Liu (2010) used sequence labeling model (CRF) for normalizing deletion-based abbreviation at the character-level. The model labels every character in a standard word as ‘Y’ or ‘N’ to represent whether it appears or not in a possible abbreviation token. The features used for the classification task represent the character’s position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard wor"
P15-1090,P14-3012,1,0.913154,"haracter level probability of this token based on a character level language model Normalization Features 11. whether each individual candidate list has any candidates for this token 12. how many candidates each individual candidate list has 13. whether each individual list’s top 10 candidates contain this token itself 14. the max number of lists that have the same top one candidate 15. the similarity value between each individual normalization system’s first candidate w and this token t, calculated by to combine and rerank these candidate lists, using a rich set of features. Please refer to (Li and Liu, 2014) for more details. By analyzing each individual system, we find that for ill-OOV words most normalization systems can generate many candidates, which may contain a correct candidate; for correct-OOV words, many normalization systems have few candidates or may not provide any candidates. For example, only two of the six lists have candidates for the token Newsfeed and Metropcs. Therefore, we believe the patterns of these normalization results contain useful information to classify OOVs. Note that this kind of feature is only applicable for those tokens that are judged as OOV by the given dictio"
P15-1090,P06-2005,0,0.0309784,"al. (2011) created a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same"
P15-1090,P12-1109,0,0.499758,"n NER compared with the baseline systems. Gimpel et al. (2011) created a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presen"
P15-1090,P14-2111,0,0.0217856,"common in short texts for various reasons (e.g., length limitation, need to convey much information, writing style). They post problems to many NLP techniques in this domain. 929 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 929–938, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics model, in which the relationship between the standard and non-standard words is characterized by a log-linear model, permitting the use of arbitrary features. Chrupała (2014) proposed a text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via recurrent network derived character-level neural text embeddings. sification process. Our experiment results demonstrate that our proposed system gives a significant performance improvement on NSW detection compared with the dictionary baseline system. On the other hand, the impact of normalization or NSW detection on NER has not been well studied in social media domain. In this paper, we propose two methods to incorporate the NSW detection infor"
P15-1090,P12-1055,0,0.135892,"n NER compared with the baseline systems. Gimpel et al. (2011) created a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presen"
P15-1090,W09-2010,0,0.0681296,"Missing"
P15-1090,P11-2093,0,0.015552,"n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art models of joint word segmentation and POS tagging in Japanese (Kudo et al., 2004; Neubig et al., 2011). Their model can also be trained on a partially annotated corpus. Li and Liu (2015) conducted a similar research on joint POS tagging and text normalization for English. Wang and Kan 930 words will be considered as correct-OOV. Therefore all the tokens will have these three labels: IV, ill-OOV, and correct-OOV. Throughout this paper, we use GNU spell dictionary (v0.60.6.1) to determine whether a token is OOV.1 Twitter mentions (e.g., @twitter), hashtags and urls are excluded from consideration for OOV. Dictionary lookup of Internet slang2 is performed to filter those ill-OOV words whose corre"
P15-1090,P11-2008,0,0.134859,"Missing"
P15-1090,N13-1039,0,0.0626686,"Missing"
P15-1090,P11-1038,0,0.659006,"We also create a new data set with newly added normalization annotation beyond the existing named entity labels. This is the first data set with such annotation and we release it for research purpose. Our experiment results demonstrate the effectiveness of our NSW detection method and the benefit of NSW detection for NER. Our proposed methods perform better than the state-of-the-art NER system. 1 However, most of previous work on normalization assumed that they already knew which tokens are NSW that need normalization. Then different methods are applied only to these tokens. To our knowledge, Han and Baldwin (2011) is the only previous work which made a pilot research on NSW detection. One straight forward method to do this is to use a dictionary to classify a token into in-vocabulary (IV) words and out-of-vocabulary (OOV) words, and just treat all the OOV words as NSW. The shortcoming of this method is obvious. For example, tokens like ‘iPhone’, ‘PES’(a game name) and ‘Xbox’ will be considered as NSW, however, these words do not need normalization. Han and Baldwin (2011) called these OOV words correct-OOV, and named those OOV words that do need normalization as ill-OOV. We will follow their naming conv"
P15-1090,I11-1109,1,0.852606,"ed a specific set of POS tags for twitter data. With this tag set and word cluster information extracted from a huge Twitter corpus, their proposed system obtained significant improvement on POS tagging accuracy in Twitter data. Related Work There has been a surge of interest in lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing resea"
P15-1090,P13-1155,0,0.0154267,"lexical normalization with the advent of social media data. Lots of approaches have been developed for this task, from using edit distance (Damerau, 1964; Levenshtein, 1966), to the noisy channel model (Cook and Stevenson, 2009; Pennell and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the sta"
P15-1090,W10-0513,0,0.0343477,"i+1 (i = −2, −1, 0, 1) 5. Token’s predicted NSW label: Unigram: Li (i = 0) Bigram: Li Li+1 (i = −2, −1, 0, 1) Trigram: Li−1 Li Li+1 (i = −2, −1, 0, 1) 6. Compound features using lexical and NSW labels: Wi Di , Wi Li , Wi Di Li (i = 0) 7. Compound features using POS and NSW labels: Pi Di , Pi Li , Pi Di Li (i = 0) 8. Compound features using word, POS, and NSW labels: Wi Pi Di Li (i = 0) Data and Experiment 4.1 Data Set and Experiment Setup The NSW detection model is trained using the data released by (Li and Liu, 2014). It has 2,577 Twitter messages (selected from the Edinburgh Twitter corpus (Petrovic et al., 2010)), in which there are 2,333 unique pairs of NSW and their standard words. This data is used for training the different normalization models. We labeled this data set using the given dictionary for NSW detection. 4,121 tokens are labeled as ill-OOV, 1,455 as correctOOV, and the rest 33,740 tokens are IV words. We have two test sets for evaluating the NSW detection system. One is from (Han and Baldwin, 2011), which includes 549 tweets. Each tweet contains at least one ill-OOV and the corresponding correct word. We call it Test set 1 in the following. The other is from (Li and Liu, 2015), who fur"
P15-1090,D14-1011,0,0.0112627,"improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art models of joint word segmentation and POS tagging in Japanese (Kudo et al., 2004; Neubig et al., 2011). Their model can also be trained on a partially annotated corpus. Li and Liu (2015) conducted a similar research on joint POS tagging and text normalization for English. Wang and Kan 930 words will be considered as correct-OOV. Therefore all the tokens will have these three labels: IV, ill-OOV, and correct-OOV. Throughout th"
P15-1090,C14-1092,0,0.0192864,"of named entity normalization (NEN) for tweets. They proposed a novel graphical model to simultaneously conduct NER and NEN on multiple tweets. Although this work involved text normalization, it only focused on the NER task, and there was no reported result for normalization. On Turkish tweets, Kucuk and Steinberger (2014) adapted NER rules and resources to better fit Twitter language by relaxing its capitalization constraint, expanding its lexical resources based on diacritics, and using a normalization scheme on tweets. These showed positive effect on the overall NER performance. Rangarajan Sridhar et al. (2014) decoupled the SMS translation task into normalization followed by translation. They exploited bi-text resources, and presented a normalization approach using distributed representation of words learned through neural networks. In this study, we propose new methods to effectively integrate information of OOV words and their normalization for the NER task. In particular, by adopting joint decoding for both NSW detection and NER, we are able to outperform stateof-the-art results for both tasks. This is the first study that systematically evaluates the effect of OOV words and normalization on NER"
P15-1090,W14-1309,0,0.0194336,"word segmentation in Chinese Microblog. But with their method, ill-OOV words are merely recognized and not normalized. Therefore, they did not investigate how to exploit the information that may be derived from normalization to increase word segmentation accuracy. Liu et al. (2012b) studied the problem of named entity normalization (NEN) for tweets. They proposed a novel graphical model to simultaneously conduct NER and NEN on multiple tweets. Although this work involved text normalization, it only focused on the NER task, and there was no reported result for normalization. On Turkish tweets, Kucuk and Steinberger (2014) adapted NER rules and resources to better fit Twitter language by relaxing its capitalization constraint, expanding its lexical resources based on diacritics, and using a normalization scheme on tweets. These showed positive effect on the overall NER performance. Rangarajan Sridhar et al. (2014) decoupled the SMS translation task into normalization followed by translation. They exploited bi-text resources, and presented a normalization approach using distributed representation of words learned through neural networks. In this study, we propose new methods to effectively integrate information"
P15-1090,D11-1141,0,0.780509,"|IV) Ill-OOV O IV p(correct-OOV|IV) correct-OOV I p(O|B) p(I|Messi) IV IV Ill-OOV Ill-OOV is well-known p(Ill-OOV|Messi) p(O|Messi) is Messi Messi well-known (A) B_IV (B) p(B|B)+ β * p(IVIIV) p(B|Messi)+ α* p(IV|Messi) B_correct-OOV p(B|B)+ β * p(ill-OOVIIV) B_IV B_correct-OOV B_IV B_correct-OOV p(B|is)+ α* p(correct-OOV|is) B_ill-OOV B_ill-OOV p(I|B)+ β * p(IVIIV) I_IV I_IV . . . . . . Messi is B_ill-OOV p(B|well-known)+ α* p(ill-OOV|well-known) I_IV p(I|B)+ β * p(IVIill-OOV) . . . well-known (C) Figure 1: Trellis Viterbi decoding for different systems. The data with the NER labels are from (Ritter et al., 2011) who annotated 2,396 tweets (34K tokens) with named entities, but there is no information on the tweets’ ill-OOV words. In order to evaluate the impact of ill-OOV on NER, we ask six annotators to annotate the ill-OOV words and the corresponding standard words in this data. There are only 1,012 sentences with ill-OOV words. We use all the sentences (2,396) for the NER experiments. This data set,6 to our knowledge, is the first one having both ill-OOV and NER annotation in social media domain. For joint decoding, the parameters α and β are empirically set as 0.95 and 0.5. 4.2 in Table 1. First n"
P15-1090,D14-1037,0,0.0278947,"Missing"
P15-1090,P13-1072,0,0.0609524,"Missing"
P15-1090,D13-1007,0,0.017368,"l and Liu, 2010; Liu et al., 2012a) and machine translation method (Aw et al., 2006; Pennell and Liu, 2011; Li and Liu, 2012b; Li and Liu, 2012a). Normalization performance on some benchmark data has been improved a lot. Currently, unsupervised models are widely used to extract latent relationship between non-standard words and correct words from a huge corpus. Hassan and Menezes (2013) applied the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus to build relation between non-standard tokens and correct words. Yang and Eisenstein (2013) presented a unified unsupervised statistical At the same time, increasing research work has been done to integrate lexical normalization into the NLP tasks in social media data. Kaji and Kitsuregawa (2014) combined lexical normalization, word segmentation and POS tagging on Japanese microblog. They used rich character-level and word-level features from the state-of-the-art models of joint word segmentation and POS tagging in Japanese (Kudo et al., 2004; Neubig et al., 2011). Their model can also be trained on a partially annotated corpus. Li and Liu (2015) conducted a similar research on join"
P15-1090,W04-3230,0,\N,Missing
P15-1090,C12-1097,1,\N,Missing
P15-2009,D14-1076,1,0.849007,"r than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive summary generation. We extend an unsupervised dependency"
P15-2009,C12-1029,0,0.0185575,"y reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compressio"
P15-2009,P06-1048,0,0.0250579,"subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The goal is to find a subtree with the highest score: X f (X) = xe × winf o"
P15-2009,W03-1101,0,0.0224033,"the gist of the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while"
P15-2009,W04-1013,0,0.00405067,"performs the corresponding original baseline, LexRank and HGRW. • The improvement obtained by LexRank+SC+both compared to LexRank is more promising than that obtained by HGRW+SC+both compared to HGRW. This may be because HGRW has used tweet information already, and leaves limited room for improvement for the sentence compression model when using the same source of information. Table 2: Overall Performance. Bold: the best value in each group in terms of different metrics. Following (Wei and Gao, 2014), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores (Lin, 2004) using human-generated highlights as the reference. The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, following (Filippova and Strube, 2008). We empirically use 0.8 for α, β and  such that tweets have more impact for both sentence selection and compression. We leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated summaries. The Stanford Parser4 is used to obtain dependency trees. The bac"
P15-2009,D13-1156,1,0.858019,"usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive summary generation. We extend an unsup"
P15-2009,D13-1155,0,0.114691,"elevant tweets. For a tweet winf o (e) = 51 Psummary (n) Particle (n) (4) wsyn (e) = P (l|h) PrelevantT (n) and PbackgroundT (n) are the unigram probabilities of word n in two language models trained on the relevant tweet dataset and a background tweet dataset respectively. The new syntactic importance score is: (5) where Psummary (n) and Particle (n) are the unigram probabilities of word n in the two language models trained on human generated summaries and the original articles respectively. P (l|h) is the conditional probability of label l given head h. Note that here we use the formula in (Filippova and Altun, 2013) for winf o (e), which was shown to be more effective for sentence compression than the original formula in (Filippova and Strube, 2008). The optimization problem can be solved under the tree structure and length constraints by integer linear programming1 . Given that L is the maximum number of words permitted for the compression, the length constraint is simply represented as: X xe ≤ L (6) T wsyn (e) = 3 Setup We evaluate our pipeline news highlights generation framework on a public corpus based on CNN/USAToday news (Wei and Gao, 2014). This corpus was constructed via an event-oriented strate"
P15-2009,N03-1026,0,0.0556107,"ut the top news sentences as the highlights, and the input to the subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The go"
P15-2009,W08-1105,0,0.386495,"DF value. Although both types of nodes can be ranked in this framework, we only output the top news sentences as the highlights, and the input to the subsequent compression component. Framework 2.2 We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 tfw,x tfw,y (idfw )2 qP 2 2 ∈x (tfwi ,x idfwi ) × w ∈y (tfwi ,y idfwi ) P sim(x, y) = qP Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1 , w2 , ..., wm , identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and"
P15-2009,N10-1131,0,0.0613254,"f the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to gu"
P15-2009,P13-1136,0,0.0247074,"mation load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we leverage tweet information to guide sentence compression for compressive s"
P15-2009,C14-1083,1,0.8058,"er together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇ Stajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the original sentences for extractive summarization in a fashion of supervised machine learning. Wei and Gao (2015) proposed a graph-based approach to simultaneously rank the We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating"
P15-2009,D13-1047,1,0.889893,"Missing"
P15-4016,H94-1020,0,0.545178,"DF serializations: JSON-LD, N-Triples and N-Quads, Notation3, RDF/XML, TriG, TriX, and Turtle. With the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of"
P15-4016,N06-4006,0,0.0351016,"ith the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of the available libraries that implement the full JSON-LD syntax and API specifications.6"
P15-4016,W12-3610,0,0.0188254,"format (Sporny et al., 2014) and is the recommended serialization of OA. Every JSON-LD document is both a JSON document and a representation of RDF data. Figure 2 shows an example of a simple annotation using the OA JSON-LD representation.2 ""@id"": ""@type"": ""target"": ""body"": Action Read annotation Read all annotations Update annotation Delete annotation Create annotation Table 1: HTTP verbs, resources, and actions. Read-only services support only the two GET requests. is an RDF-based graph representation compatible with linguistic annotation formalisms such as LAF/GrAF (Ide and Suderman, 2007; Verspoor and Livingston, 2012). At its most basic level, the OA model differentiates between three key components: annotation, body, and target, where the annotation expresses that the body is related to the target of the annotation (Figure 1). The body can carry arbitrarily complex embedded data. { Resource Annotation Collection Annotation Annotation Collection 3.1 OA Store The OA Store is a reference implementation of persistent, server-side annotation storage that allows clients to create, read, update and delete annotations using the API. The store uses MongoDB, which is well suited to the task as it is a documentorien"
P15-4016,wright-2014-restful,0,0.0306169,"o all four databases. They query a PostgreSQL back-end for text and annotations, which are formatted as OA JSON-LD using the standard Python json module. 4.4 5 Related work Our approach builds directly on the OA data model (Bradshaw et al., 2013), which harmonizes the earlier Open Annotation Collaboration (Haslhofer et al., 2011) and Annotation Ontology Initiative (Ciccarese et al., 2011) efforts and is currently developed further under the auspices of the W3C Web Annotation WG.8 Approaches building on RESTful architectures and JSON-LD are also being pursued by the Linguistic Data Consortium (Wright, 2014) and the Language Application Grid (Ide et al., 2014), among others. A number of annotation stores following similar protocols have also been released recently, including Lorestore (Hunter and Gerber, 2012), PubAnnotation (Kim and Wang, 2012), the Annotator.js store9 , and NYU annotations10 . 6 Conclusions and future work We have proposed to share annotations using a minimal RESTful interface for Open Annotation data in JSON-LD. We introduced reference implementations of a server, client, validation and conversion tools, and demonstrated the integration of several independently developed annot"
P15-4016,W07-1501,0,\N,Missing
P15-4016,W12-2425,0,\N,Missing
P15-4016,ide-etal-2014-language,0,\N,Missing
P15-4016,E12-2021,1,\N,Missing
P19-1212,K16-1028,0,0.0912215,"Missing"
P19-1212,P17-1099,0,0.401231,"Missing"
P19-1212,J99-3001,0,0.250824,"Missing"
P19-1212,W04-3252,0,\N,Missing
P19-1212,N09-1041,0,\N,Missing
P19-1212,N04-1015,0,\N,Missing
P19-1212,J95-2003,0,\N,Missing
P19-1212,J08-1001,0,\N,Missing
P19-1212,P06-4018,0,\N,Missing
P19-1212,W12-3018,0,\N,Missing
P19-1212,E17-2112,0,\N,Missing
P19-1212,P17-1108,0,\N,Missing
P19-1212,D18-1206,0,\N,Missing
P19-1212,D18-1443,0,\N,Missing
S17-2122,P14-1066,0,0.0330013,"Missing"
S17-2122,D14-1070,0,0.0320819,"2 We use rectified linear units, filter windows (h) P R F1 of 3, 4, 5 with 100 feature maps each, dropout rate Positive 0.5791 0.6748 0.5423 (p) of 0.5, l2 constraint (s) of 3, and mini-batch Negative 0.5655 0.5592 0.5065 size of 50. These values were chosen via a grid Neutral 0.5264 0.4340 0.4962 search on the dev sets. 265 226 4.2 Pre-trained Word Vectors 276 227 Initializing word vectors with those obtained from an unsupervised neural language model is a popular method to improve performance in the absence of a large supervised training set (Collobert C&W et al., 2011; Socher et al., 2011; Iyyer et al., 2014). First we use SSWE (Tang et al. 2014) which is a word vector contains sentiment information.We also use the publicly available word2vec vectors which were trained on 100 billion words from Google News. The vectors have dimensionality of 300 and were trained using the CBOW architecture.Because the dimensionality of vectors in SWEE is 50, so we extended it to 300 dimension by padding the 250 dimension randomly. 216 217 218 219 220 221 222 223 224 225 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 4.3 Environment of experiment The experiments were run on"
S17-2178,D14-1179,0,0.00765289,"Missing"
S17-2178,S15-2137,0,0.0362241,"al. Clinical TempEval is designed to ad-dress the challenge of understanding clinical timeline in medical narratives and it is based on the THYME corpus which includes temporal an-notations. Researchers have explored ways to extract temporal information from clinical text. Lee et al. (2016) developed an approach based on linear and structural (HMM) support vector machines using lexical, morphological, syntactic, discourse, and word representation features. P R, Sarath et al. (2016) used a hybrid approach(rule-based and machine learning) for temporal information extraction from clinical notes. Velupillai et al. (2015) developed a pipeline based on ClearTK and SVM with lexical features to extract TIMEX3 and EVENT mentions. Most of the participants of these challenges used CRF and SVM for event and time expression extraction with features including the information gathered from different resources like UMLS (Uniﬁed Medical Language System), output of TARSQI toolkit, Brown Clustering, Wikipedia and Metamap (Aronson and Lang, 2010). Those machine-learning methods are complex and they cost much time to run. However, they can be not only flexible but also convenient when compared to the handcrafting label. Other"
S17-2178,S15-2136,0,0.0435841,"ods may gain the better result. Since in I2b2 2012 temporal challenge, all top performing teams used a combination of supervised classiﬁcation and rule based methods for extracting temporal information and relations (Sun et al., 2013). Besides THYME corpus, there have been other efforts in clinical temporal annotation including works by Roberts et al. (2008), Savova et al. (2009), Galescu and Blaylock (2012) and so on. Recently, interest in temporal processing has moved forward in two directions: cross-document timeline extraction (Minard et al., 2015) and domain adaptation (Sun et al., 2013; Bethard et al., 2015). Based on the analysis above, our hybrid model utilize machine learning techniques and crafted rules which contains SVM (Support Vector Machine) classifier and RNN (Recurrent Neural Networks) classifier to extract Temporal Information from Clinical documents and make classifications. 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 99 199 100 200 101 201 102 202 103 203 104 204 105 205 106 206 107 207 108 208 109 209 110 210 111 211 112 212 113 213 114 214 115 215 116 216 117 217 2 2.1 a"
S18-1114,D14-1162,0,0.0810053,"Missing"
S18-1114,J92-4003,0,0.655373,"(Ma and Hovy, 2016), as shown in Figure 1, before feeding into the BiLSTM network, the model concatenates character-level representations obtained from CNN (LeCun et al., 1989), word-level representations and linguistic feature representations to acquire the final representation of the word. At the end, the model feeds the output POS Tags POS Tagging (part-of-speech Tagging), which attaches each word of a sentence a part of speech tag 1 Chunking Labels Similar words have similar distributions of words to their immediate left and right. Motivated by this intuition, Brown Clustering algorithm (Brown et al., 1992) gives an unsupervised class label to a word. Our system uses a C++ implementation3 of the Brown clustering algorithm (Liang, 2005) and sets cluster number as 50. The Brown clusters was trained on a large corpus of APT reports4 provided by the organizer. Feature Extraction 2.2.1 NER Labels Text chunking divides a text into phrases in such a way that syntactically related words become member of the same phrase. For instance, ”technology organizations” is a noun phrase, our system annotates ”technology” as ”B-NP” and ”organizations” as ”I-NP”. Data Preprocessing 2.2 Dependency Labels 3 https://s"
S18-1114,S18-1113,0,0.0334989,"Missing"
S18-1114,N16-1030,0,0.0942385,"d task. Our system is based on RNN network and ranked first in both token level and phrase level. Most existing high performance sequence labeling methods are linear statistical models, such as HMM (Hidden Markov Models) (Eddy, 1996) and CRF (Conditional Random Fields) (Lafferty et al., 2001). In the past few years, neural networks have been widely used to solve NLP problems. Specially, several RNN-based neural networks have been proposed to handle sequence labeling tasks including Chinese word segmentation (Yao and Huang, 2016), POS tagging (Huang et al., 2015), NER (Chiu and Nichols, 2015) (Lample et al., 2016), which achieved outstanding performance against traditional methods. In this paper, we simple derive the result of SubTask1 from SubTask2 and regard SubTask 2 as the preorder. Namely, our system firstly outputs sequence labels of a given sentence, and then checks whether some target labels turn out, such as Action, Entity, Modifier. Sentences which have Introduction As a growing number of mobile devices and facilities are getting connected and digitized, malware attacks become increasingly rampant and dangerous. CybersecUrity attracts more public attention but few NLP research and efforts. A"
S18-1114,P17-1143,0,0.0304662,"Missing"
S18-1114,P16-1101,0,0.0258725,"omly. We take four parts as training set and the rest as development set. 2.2.5 Based upon many previous work on sequence labeling, our system incorporates 5 types of features: POS tags, dependency parsing, NER labels, Chunking labels and Brown clustering. All features are generated automatically. In detail, we use Stanford CoreNLP (Manning et al., 2014) 1 to annotate POS tags, dependency parsing, NER labels, and use Apache OpenNLP 2 to annotate Chunking labels. Brown clustering labels are generated by an open source implementation. 2.3 2 Brown Clustering Labels Model Introduction Similar to (Ma and Hovy, 2016), as shown in Figure 1, before feeding into the BiLSTM network, the model concatenates character-level representations obtained from CNN (LeCun et al., 1989), word-level representations and linguistic feature representations to acquire the final representation of the word. At the end, the model feeds the output POS Tags POS Tagging (part-of-speech Tagging), which attaches each word of a sentence a part of speech tag 1 Chunking Labels Similar words have similar distributions of words to their immediate left and right. Motivated by this intuition, Brown Clustering algorithm (Brown et al., 1992)"
S18-1114,P14-5010,0,0.0047801,"m data provided by the organizers, we use a python program to correct spelling mistakes and unreadable characters. After that, in order to avoid data distribution problem, we mix the training set development set, and then shuffle and split them into five parts randomly. We take four parts as training set and the rest as development set. 2.2.5 Based upon many previous work on sequence labeling, our system incorporates 5 types of features: POS tags, dependency parsing, NER labels, Chunking labels and Brown clustering. All features are generated automatically. In detail, we use Stanford CoreNLP (Manning et al., 2014) 1 to annotate POS tags, dependency parsing, NER labels, and use Apache OpenNLP 2 to annotate Chunking labels. Brown clustering labels are generated by an open source implementation. 2.3 2 Brown Clustering Labels Model Introduction Similar to (Ma and Hovy, 2016), as shown in Figure 1, before feeding into the BiLSTM network, the model concatenates character-level representations obtained from CNN (LeCun et al., 1989), word-level representations and linguistic feature representations to acquire the final representation of the word. At the end, the model feeds the output POS Tags POS Tagging (par"
W09-1111,P98-1013,0,0.0186448,"Missing"
W09-1111,W04-3205,0,0.0297468,"tions, in particular on the quantifiers each other and one another (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We a"
W09-1111,P08-1079,0,0.122432,"er (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We also present some interesting observations on the data thus obtaine"
W09-1111,J06-3003,0,0.0412294,"and one another (Dalrymple et al., 1998; Heim, 1991; K¨onig, 2005). Most of this work has been done by language typologists (Maslova and Nedjalkov, 2005; Haspelmath, 2007) who are interested in how reciprocal constructions of these types vary from one language to another and they do this through comparative studies of large sets of world’s languages. In computational linguistics, our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs ((Hearst, 1998; Chklovski and Pantel, 2004; Etzioni et al., 2004; Turney, 2006; Davidov and Rappoport, 2008) inter alia). We extend over these approaches in two ways: (i) our patterns indicate a new type of relation between verbs, (ii) instead of seed or hook words we use a set of simple but effective pronoun templates which ensure the validity of the patterns extracted. To the best of our knowledge, the rest of our reciprocity model is novel. In particular, we use a novel procedure which extracts pairs of reciprocal instances and present two novel unsupervised clustering methods which group the instance pairs in meaningful ways. We also present some interesting observa"
W09-1111,H05-1044,0,0.00932439,"Missing"
W09-1111,C98-1013,0,\N,Missing
W15-0806,C08-1001,0,0.0775703,"Missing"
W15-3814,W13-2001,0,0.0348375,"embedding is a collective name for a set of language modelling and feature learning techniques, by which words in a vocabulary 2 2.1 Methods and results BioNLP GENIA task A series of efforts has been initiated to evaluate the available solutions and investigate potentials in event extraction technologies. Among them, the 121 Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015), pages 121–126, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics periments, we use LibSVM as the implementation of SVM. BioNLP Shared Tasks (BioNLP-ST) [7] have been consistently conducted since 2009 and attracted community-wide support. BioNLP-ST GENIA task is a core task and had the third edition in 2013. The task gradually increased its difficulties and complexities, for example, by upgrading from abstract-only text to full-text articles and subsuming co-reference tasks. In the latest GENIA 2013 task, EVEX achieves the best performance (F-score: 50.97; recall: 45.44; precision: 58.03) [8]. Our system achieves a comparable result with a higher precision (Fscore 47.33; recall: 37.14; precision: 65.21). 2.2 2.3 Word embedding for trigger and arg"
W15-3814,W11-1805,1,\N,Missing
W15-3814,W13-2002,0,\N,Missing
W16-2820,W13-4008,0,0.0520225,"Missing"
W16-2820,W10-0214,0,0.0388071,"nt features for the prediction of highly voted comments in terms of delta score and karma score respectively. Although they considered some sorts of argumentation related features, such features are merely based on lexical similarity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a disputation example from the online debating forum createdebate. It presents an origin"
W16-2820,W14-1305,0,0.0195494,"c is “Should the Gorilla have died?”) 1 Introduction the effectiveness of different features for the prediction of highly voted comments in terms of delta score and karma score respectively. Although they considered some sorts of argumentation related features, such features are merely based on lexical similarity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a dis"
W16-2820,P16-2032,1,0.678326,"ity, without modeling persuasion behaviors. With the popularity of the online debating forum such as idebate1 , convinceme2 and createdebate3 , researchers have been paying increasing attention to analyze debating content, including identification of arguing expressions in online debate (Trabelsi and Zaıane, 2014), recognition of stance in ideological online debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Ranade et al., 2013b), and debate summarization (Ranade et al., 2013a). However, there is still little research about quality evaluation of debating content. Tan et al. (2016) and Wei and Liu (2016) studied the persuasiveness of comments in sub-reddit change my view of Reddit.com. They evaluated In this paper, we focus on a particular action in the online debating forum, i.e., disputation. Within debate, disputation happens when a user disagrees with a specific comment. Figure 1 gives a disputation example from the online debating forum createdebate. It presents an original argument and an argument disputing it. Our study aims to evaluate the quality of a disputing comment given its original argument and the discussed topic. In order to have a deep understanding of disputation, we analyz"
W16-2820,D14-1083,0,\N,Missing
W16-3004,W16-3001,0,0.135668,"Missing"
W16-3004,W09-1407,0,0.0263285,"zhang @mail.xidian.edu.cn Introduction Bio-events are founding blocks of bio-networks depicting profound biological phenomena. Automatically extracting bio-events may assist researchers while facing the challenge of growing amount of biomedical information in textual form. A bio-event carries more semantic information biochemical reactions between entities, therefore, is more informative for studying associations between bio-concepts, e.g. gene and phenotype (Li et al., 2013). A number of methods have been proposed to process the automated extraction of biomedical events including rule-based (Cohen et al., 2009; Kilicoglu and Bergler, 2011; Bui and Sloot, 2011) 2 SeeDev Task As a popular task in unstructured data mining of biomedical interests, BioNLP has successfully 32 Proceedings of the 4th BioNLP Shared Task Workshop, pages 32–41, c Berlin, Germany, August 13, 2016. 2016 Association for Computational Linguistics Figure 1: Event relation examples. This sentence includes 4 events and 6 entities. For example, a Environmental Factor Yeast one-hybrid and a Protein ABI5 form a event Interacts With. An entity could participate in several events at the same time or none, such as AtEm6 promoter and lacZ."
W16-3004,W13-2003,0,0.0393296,"Missing"
W16-3004,W13-2004,0,0.0505886,"Missing"
W16-3004,W11-1820,0,0.0242234,"are founding blocks of bio-networks depicting profound biological phenomena. Automatically extracting bio-events may assist researchers while facing the challenge of growing amount of biomedical information in textual form. A bio-event carries more semantic information biochemical reactions between entities, therefore, is more informative for studying associations between bio-concepts, e.g. gene and phenotype (Li et al., 2013). A number of methods have been proposed to process the automated extraction of biomedical events including rule-based (Cohen et al., 2009; Kilicoglu and Bergler, 2011; Bui and Sloot, 2011) 2 SeeDev Task As a popular task in unstructured data mining of biomedical interests, BioNLP has successfully 32 Proceedings of the 4th BioNLP Shared Task Workshop, pages 32–41, c Berlin, Germany, August 13, 2016. 2016 Association for Computational Linguistics Figure 1: Event relation examples. This sentence includes 4 events and 6 entities. For example, a Environmental Factor Yeast one-hybrid and a Protein ABI5 form a event Interacts With. An entity could participate in several events at the same time or none, such as AtEm6 promoter and lacZ. Noticeably entity span overlap, like Gene AtEm6 an"
W16-3004,W11-1827,0,0.0174833,"du.cn Introduction Bio-events are founding blocks of bio-networks depicting profound biological phenomena. Automatically extracting bio-events may assist researchers while facing the challenge of growing amount of biomedical information in textual form. A bio-event carries more semantic information biochemical reactions between entities, therefore, is more informative for studying associations between bio-concepts, e.g. gene and phenotype (Li et al., 2013). A number of methods have been proposed to process the automated extraction of biomedical events including rule-based (Cohen et al., 2009; Kilicoglu and Bergler, 2011; Bui and Sloot, 2011) 2 SeeDev Task As a popular task in unstructured data mining of biomedical interests, BioNLP has successfully 32 Proceedings of the 4th BioNLP Shared Task Workshop, pages 32–41, c Berlin, Germany, August 13, 2016. 2016 Association for Computational Linguistics Figure 1: Event relation examples. This sentence includes 4 events and 6 entities. For example, a Environmental Factor Yeast one-hybrid and a Protein ABI5 form a event Interacts With. An entity could participate in several events at the same time or none, such as AtEm6 promoter and lacZ. Noticeably entity span overl"
W16-3004,W13-2014,0,0.0288567,"Missing"
W16-3004,W09-1401,0,0.115887,"ages 32–41, c Berlin, Germany, August 13, 2016. 2016 Association for Computational Linguistics Figure 1: Event relation examples. This sentence includes 4 events and 6 entities. For example, a Environmental Factor Yeast one-hybrid and a Protein ABI5 form a event Interacts With. An entity could participate in several events at the same time or none, such as AtEm6 promoter and lacZ. Noticeably entity span overlap, like Gene AtEm6 and Promoter AtEm6 promoter. held a series of biomedical event extraction tasks. GE (Genia Event Extraction) is a classic task initiated since the beginning of BioNLP (Kim et al., 2009), it attracts attention and leads to abundant works (Kim et al., 2011; Kim et al., 2013). Be similar to GE and others of BioNLP, SeeDev (Chaix et al., 2016) is a new task proposed in BioNLPST 2016, it dedicates to event extraction of genetic and molecular mechanisms involved in plant seed development. It is based on the knowledge model Gene Regulation Network for Arabidopsis (GRNA)1 . GRNA model defines 16 different types of entities, and 22 event types that may be combined in complex events. Table 1 shows these entities. Event types are presented in following. Figure 1 gives some examples of"
W16-3004,W13-2001,0,0.192543,"Missing"
W16-3004,W11-1802,0,0.0137618,"omputational Linguistics Figure 1: Event relation examples. This sentence includes 4 events and 6 entities. For example, a Environmental Factor Yeast one-hybrid and a Protein ABI5 form a event Interacts With. An entity could participate in several events at the same time or none, such as AtEm6 promoter and lacZ. Noticeably entity span overlap, like Gene AtEm6 and Promoter AtEm6 promoter. held a series of biomedical event extraction tasks. GE (Genia Event Extraction) is a classic task initiated since the beginning of BioNLP (Kim et al., 2009), it attracts attention and leads to abundant works (Kim et al., 2011; Kim et al., 2013). Be similar to GE and others of BioNLP, SeeDev (Chaix et al., 2016) is a new task proposed in BioNLPST 2016, it dedicates to event extraction of genetic and molecular mechanisms involved in plant seed development. It is based on the knowledge model Gene Regulation Network for Arabidopsis (GRNA)1 . GRNA model defines 16 different types of entities, and 22 event types that may be combined in complex events. Table 1 shows these entities. Event types are presented in following. Figure 1 gives some examples of event relations2 . Figure 2: Infrastructure of LitWay. in number (N´e"
W16-3004,W15-3803,0,0.0407001,"Missing"
W16-3004,J08-2005,0,0.0145216,"occur most times for each special event type. This could efficiently reduce false instances. (2) Entity structure rules: Many entities have complicated structure, an entity could span over another entity. This results in that some entity structures are less likely to be event arguments. Such as, an entity with smaller span is not an argument, as it is often the modifier of the larger one. Meanwhile, some event types have several fixed special entity argument structures. We summarize 3 particular rules from the training set: Syntactic parsing tree features are important for semantic relation (Punyakanok et al., 2008; D’Souza and Ng, 2012). Tree node depth, tree path, tree path length are used in our experiment. They are obtained from the syntactic parsing tree, generated during the pre-processing. Tree node depth is the distance between the corresponding tree node of an entity and the root node of the sentence. Tree path is the path between two entities. Tree path length is the number of middle nodes between two entities in their tree path. Word embedding has demonstrated the ability of well representing linguistic and semantic information of a text unit (Mikolov et al., 2013; Tang et al., 2014), e.g. PO"
W16-3004,W15-3814,1,0.896712,"Missing"
W16-3004,P14-1146,0,0.0294561,"on (Punyakanok et al., 2008; D’Souza and Ng, 2012). Tree node depth, tree path, tree path length are used in our experiment. They are obtained from the syntactic parsing tree, generated during the pre-processing. Tree node depth is the distance between the corresponding tree node of an entity and the root node of the sentence. Tree path is the path between two entities. Tree path length is the number of middle nodes between two entities in their tree path. Word embedding has demonstrated the ability of well representing linguistic and semantic information of a text unit (Mikolov et al., 2013; Tang et al., 2014), e.g. POS and N-gram. We continue using it as a feature in our system. Specifically, training, development and test datasets of SeeDev are used to obtain word embedding by using word2vec tool (Mikolov et al., 2013) after sentencization, tokenization and lemmatization on the original text. Since the word number of an entity is uncertain, we use the average value of all the word embeddings of an entity (Chen et al., 2015; Wang et al., 2015), i.e. average word embedding. Middle lemmas include all of the lemmas between two entities, they are treated as a bag-of-word (BOW) feature, some keyword in"
W16-3004,P14-5010,0,0.00406568,"If the classifier predicts that a candidate pair is a event belonging to Event-Set-A, the predication stays. Otherwise, a series of rules are used for deciding a type in Event-Set-B. Entity pair feature Tree path Tree path length Token distance Entity distance Middle lemmas Table 4: Features used in classifier. Entity features are extracted from two entities, separately. Entity pair features are extracted from a pair of entities. Pre-processing The pre-processing include tokenization, sentence splitter, part-of-speech (POS) tagging, lemmatization and syntactic parsing. Stanford CoreNLP tool (Manning et al., 2014) is adopted for the operations. 3.2 Feature Extraction The features are extracted and summarized in Table 4, which shows two types of features, entity features and entity pair features. Table 2: Event-Set-A and Event-Set-B. This partition was used during competition. 3.1 Number 1571 52 5 Word, lemma, Part-Of-Speech (POS) are features directly represent an entity’s lexical and grammatical characteristics. Adjacent words’ features are used to represent the entity’s contextual characteristics. Therefore, basic features include word, lemma, POS of entities, as well as the same information of the u"
W16-3004,W13-2002,0,\N,Missing
W18-3708,P06-1032,0,0.129847,"ed system should predict the ‘target edits’ in the format which is same as the training set, and for the error type of S and M, the system should predict the candidate corrections. We also used an external dataset Lang-81 to train our GEC models, which contains more than 700,000 items, and each item consists of an original sentence and corresponding corrected sentences. Each original sentence has k correction To address more complex errors, MT models are proposed and developed by many researchers. Statistical Machine Translation (SMT) has been dominant for the past two decades. In the work of Brockett et al. (2006), they propose an SMT model used for GEC, and later the round-trip translation is also used in GEC (Madnani et al., 2012). A POS-factored SMT system is proposed (Yuan and Felice, 2013) to correct five types of errors in the text. In the work of Felice et al. (2014), they propose a pipeline of the rule-based system and a phrase-based SMT system augmented by a sizeable web-based language model. The wordlevel Levenshtein distance between source and target can be used as a translation model feature (Junczys-Dowmunt and Grundkiewicz, 2014) to enhance the model. Rule-based method and ngram statistic"
W18-3708,J93-2003,0,0.0514692,"LSTM-CRF model to tag the possible error position at the detection stage. The three GEC models are Rule-based model, NMT model, and SMT model, which are able to cope with different types of grammatical errors. 1. If the segment consists of two or more characters, and turn out to be in the dictionary by permuting the characters, it will be added to the candidate list. 2. If the concatenation with a previous or next segment is in the dictionary, it will be added to the candidate list. 3. All possible keys in the dictionary with 62 lated from the parallel corpus. We used the noisy channel model (Brown et al., 1993) to combine the language model and the translation model, and incorporated beam search to decode the result. To explore the ability of SMT models with different configurations, we trained six SMT models with different data granularity and monolingual dataset as described in Section 5.1. Those six SMT models were denoted as Sj , where j ∈ {1, 2, 3, 4, 5, 6} was the model index. The correction result of sentence si generated by Sj was denoted as CiSj . the same or similar Pinyin (the Romanization system for Standard Chinese) or similar strokes to the segment are generated. The generated keys for"
W18-3708,C96-1031,0,0.567321,"Missing"
W18-3708,W14-1702,0,0.0182306,"than 700,000 items, and each item consists of an original sentence and corresponding corrected sentences. Each original sentence has k correction To address more complex errors, MT models are proposed and developed by many researchers. Statistical Machine Translation (SMT) has been dominant for the past two decades. In the work of Brockett et al. (2006), they propose an SMT model used for GEC, and later the round-trip translation is also used in GEC (Madnani et al., 2012). A POS-factored SMT system is proposed (Yuan and Felice, 2013) to correct five types of errors in the text. In the work of Felice et al. (2014), they propose a pipeline of the rule-based system and a phrase-based SMT system augmented by a sizeable web-based language model. The wordlevel Levenshtein distance between source and target can be used as a translation model feature (Junczys-Dowmunt and Grundkiewicz, 2014) to enhance the model. Rule-based method and ngram statistical method are combined (Wu et al., 2015) to get a hybrid system for CGED shared task. Recently Napoles and Callison-Bursh (2017) propose a lightweight approach to GEC called Specialized Machine translation for Error Correction. Nevertheless, Neural Machine Translat"
W18-3708,W16-4906,0,0.0374551,"Missing"
W18-3708,I17-4001,0,0.0816347,"ese has a lot of differences from other languages. For example, Chinese has neither the change of singular and plural, nor the tense change of the verb. It has quite flexible expressions and loose structural grammar. These traits bring a lot of trouble to CFL learners, so the demands for Chinese Grammatical Error Diagnosis (CGED) as well as Correction (CGEC) is growing rapidly. GEC for English has been studied for many years, with many shared tasks such as CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014), while those kinds of studies on Chinese is less yet. This CGED shared task (Gaoqi et al., 2017; Lee et al., 2016, 2015; Yu et al., 2014) gives researchers an opportunity to build the system and exchange opinions in this field. It could make the community more flourish which benefits all CFL learners. Compared with previous years, this year’s NLPTEA CGED shared task requests participants to generate candidate corrections for errors of M and S types. This correction subtask is more challenging and valuable, so we focused on this subtask and got the highest precision in this subtask. This paper is organized as follows: Section 2 describes some related works in English as well as Chinese."
W18-3708,W15-4401,0,0.251644,"Missing"
W18-3708,han-etal-2004-detecting,0,0.0737564,"nal corrections for M and S types. Our system reached the highest precision in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors. 1 Introduction More and more people are learning a second or third language as an interest, a career plus, or even a challenge to oneself. Chinese is one of the oldest and most versatile languages in the world. Many ∗ † Linlin Li 2 Related Work Earlier attempts to GEC involve rule-based models (Heidorn et al., 1982; Bustamante and Le´on, 1996) and classifier-based approaches (Han et al., 2004; Rozovskaya and Roth, 2011), which can cope with Equal Contribution This work was done while the author at Alibaba Group 60 Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 60–69 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Table 1: Typical examples for four types of errors Error Original Sentence Correct Sentence M R S W 中国已成了世界拥有最多“烟民”的国家。 孩子的教育不能只靠一个学 学校老师。 父母对孩子的爱 爱情 是最重要的。 生产率较低，那肯 肯定价格要上升。 中国已成了世界上 上拥有最多“烟民”的国家。 孩子的教育不能只靠一个老师。 父母对孩子的关 关爱 是最重要的。 生产率较低，那价 价格 肯定要上升。 map the sentence from l"
W18-3708,D15-1166,0,0.0411413,"nd CiM . With the edits eij of sentence si , which are generated by BiLSTM-CRF and GEC models, the next step of our system is to ensemble all those edits. When it comes to the ensemble, we tried two methods. One is merging, which combines all detections generated by BiLSTM-CRF model as well as those GEC models, and take the union of their editing sets. The other is voting, in which we The NMT model can capture complex relationships between the original sentence and the corrected sentence in GEC. We used the encoderdecoder structure (Bahdanau et al., 2014) with the general attention mechanism (Luong et al., 2015). We used two-layer LSTM model for both encoder and decoder. To enhance the ability of NMT models, we trained four NMT models with different parallel data pairs and configurations as described in Section 5.1. Those four NMT models were denoted as Nj , where j ∈ {1, 2, 3, 4} was the model index. The correction result of sentence si generated by Nj was denoted as CiNj . We used the character-based NMT because most characters in Chinese has its meaning, which is quite different from English characters, and the Chinese word’s meaning often depends on the meaning of its characters. For example, we"
W18-3708,W11-2123,0,0.0287044,"Missing"
W18-3708,W12-2005,0,0.0143218,"nd M, the system should predict the candidate corrections. We also used an external dataset Lang-81 to train our GEC models, which contains more than 700,000 items, and each item consists of an original sentence and corresponding corrected sentences. Each original sentence has k correction To address more complex errors, MT models are proposed and developed by many researchers. Statistical Machine Translation (SMT) has been dominant for the past two decades. In the work of Brockett et al. (2006), they propose an SMT model used for GEC, and later the round-trip translation is also used in GEC (Madnani et al., 2012). A POS-factored SMT system is proposed (Yuan and Felice, 2013) to correct five types of errors in the text. In the work of Felice et al. (2014), they propose a pipeline of the rule-based system and a phrase-based SMT system augmented by a sizeable web-based language model. The wordlevel Levenshtein distance between source and target can be used as a translation model feature (Junczys-Dowmunt and Grundkiewicz, 2014) to enhance the model. Rule-based method and ngram statistical method are combined (Wu et al., 2015) to get a hybrid system for CGED shared task. Recently Napoles and Callison-Bursh"
W18-3708,W17-5039,0,0.0224153,"Missing"
W18-3708,P17-1070,0,0.019948,"cope with Equal Contribution This work was done while the author at Alibaba Group 60 Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 60–69 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Table 1: Typical examples for four types of errors Error Original Sentence Correct Sentence M R S W 中国已成了世界拥有最多“烟民”的国家。 孩子的教育不能只靠一个学 学校老师。 父母对孩子的爱 爱情 是最重要的。 生产率较低，那肯 肯定价格要上升。 中国已成了世界上 上拥有最多“烟民”的国家。 孩子的教育不能只靠一个老师。 父母对孩子的关 关爱 是最重要的。 生产率较低，那价 价格 肯定要上升。 map the sentence from learner space to expert space. Recently Ji et al. (2017) propose a hybrid neural model with nested attention layers for GEC. only specific type of errors. As a sentence may contain multiple errors of different types, a practical GEC system should be able to cope with most of those errors, which is difficult to be achieved by rule-based or classifier models alone. The combination of rule-based and classifier models (Rozovskaya et al., 2013) can correct multiple errors, but it is useful only when the errors are independent of each other, which means that it is unable to solve the problem of dependent errors. 3 Dataset Description The dataset is provi"
W18-3708,W14-1701,0,0.0666187,"ber of CFL leaner grows rapidly. However, it would be difficult to learn Chinese, because Chinese has a lot of differences from other languages. For example, Chinese has neither the change of singular and plural, nor the tense change of the verb. It has quite flexible expressions and loose structural grammar. These traits bring a lot of trouble to CFL learners, so the demands for Chinese Grammatical Error Diagnosis (CGED) as well as Correction (CGEC) is growing rapidly. GEC for English has been studied for many years, with many shared tasks such as CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014), while those kinds of studies on Chinese is less yet. This CGED shared task (Gaoqi et al., 2017; Lee et al., 2016, 2015; Yu et al., 2014) gives researchers an opportunity to build the system and exchange opinions in this field. It could make the community more flourish which benefits all CFL learners. Compared with previous years, this year’s NLPTEA CGED shared task requests participants to generate candidate corrections for errors of M and S types. This correction subtask is more challenging and valuable, so we focused on this subtask and got the highest precision in this subtask. This paper"
W18-3708,W14-1703,0,0.018319,"Translation (SMT) has been dominant for the past two decades. In the work of Brockett et al. (2006), they propose an SMT model used for GEC, and later the round-trip translation is also used in GEC (Madnani et al., 2012). A POS-factored SMT system is proposed (Yuan and Felice, 2013) to correct five types of errors in the text. In the work of Felice et al. (2014), they propose a pipeline of the rule-based system and a phrase-based SMT system augmented by a sizeable web-based language model. The wordlevel Levenshtein distance between source and target can be used as a translation model feature (Junczys-Dowmunt and Grundkiewicz, 2014) to enhance the model. Rule-based method and ngram statistical method are combined (Wu et al., 2015) to get a hybrid system for CGED shared task. Recently Napoles and Callison-Bursh (2017) propose a lightweight approach to GEC called Specialized Machine translation for Error Correction. Nevertheless, Neural Machine Translation (NMT) systems have achieved substantial improvements in this field (Sutskever et al., 2014; Bahdanau et al., 2014). Inspired by this phenomenon, Sun et al. (2015) utilize the Convolutional Neural Network (CNN) for the article error correction. The Recurrent Neural Networ"
W18-3708,W13-3601,0,0.136412,"ose to learn Chinese, and the number of CFL leaner grows rapidly. However, it would be difficult to learn Chinese, because Chinese has a lot of differences from other languages. For example, Chinese has neither the change of singular and plural, nor the tense change of the verb. It has quite flexible expressions and loose structural grammar. These traits bring a lot of trouble to CFL learners, so the demands for Chinese Grammatical Error Diagnosis (CGED) as well as Correction (CGEC) is growing rapidly. GEC for English has been studied for many years, with many shared tasks such as CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014), while those kinds of studies on Chinese is less yet. This CGED shared task (Gaoqi et al., 2017; Lee et al., 2016, 2015; Yu et al., 2014) gives researchers an opportunity to build the system and exchange opinions in this field. It could make the community more flourish which benefits all CFL learners. Compared with previous years, this year’s NLPTEA CGED shared task requests participants to generate candidate corrections for errors of M and S types. This correction subtask is more challenging and valuable, so we focused on this subtask and got the highest prec"
W18-3708,W13-3602,0,0.0229658,"M R S W 中国已成了世界拥有最多“烟民”的国家。 孩子的教育不能只靠一个学 学校老师。 父母对孩子的爱 爱情 是最重要的。 生产率较低，那肯 肯定价格要上升。 中国已成了世界上 上拥有最多“烟民”的国家。 孩子的教育不能只靠一个老师。 父母对孩子的关 关爱 是最重要的。 生产率较低，那价 价格 肯定要上升。 map the sentence from learner space to expert space. Recently Ji et al. (2017) propose a hybrid neural model with nested attention layers for GEC. only specific type of errors. As a sentence may contain multiple errors of different types, a practical GEC system should be able to cope with most of those errors, which is difficult to be achieved by rule-based or classifier models alone. The combination of rule-based and classifier models (Rozovskaya et al., 2013) can correct multiple errors, but it is useful only when the errors are independent of each other, which means that it is unable to solve the problem of dependent errors. 3 Dataset Description The dataset is provided by the 5th Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA) 2018 with a Shared Task for CGED. The NLPTEA CGED has been held since 2014, and it provides several sets of training data for this field. Each instance in the CGED training dataset is composed of an original sentence with a unique sentence number ‘sid’, some ‘target edits’, and a co"
W18-3708,W04-1213,0,0.122401,"et Voting Detection Result Extract Original Sentence Error of M and S Match Three GEC Models Correction for Target Position Merge Predicted Corrections Rule-based Correction-rule Convert by NMT GEC Model Correction-NMT Min Edit Dist SMT GEC Model Correction-SMT Edit Set Three predicted correction sentences Detection Stage Correction Stage sentences, where k ≥ 0. 4 4.1 BiLSTM-CRF In the detection stage, we treated the error detection problem as a sequence labeling problem and utilized the BiLSTM-CRF model (Huang et al., 2015) to get the corresponding label sequence in the form of BIO encoding (Kim et al., 2004). More specifically, given an input sentence which is composed of characters as [c1 , c2 , ..., cn ], we utilized this model to predict the label Li of ci , for i ∈ 1, 2, ..., n. Since the prior knowledge can be used in this task, we incorporated many additional features for this sequence labeling problem, including Char Bigram, Part-of-speech (POS) tagging, POS score, Adjacent Word Collocation (AWC), Dependent Word Collocation (DWC), as used in (Xie et al., 2017). System Description We proposed a hybrid system for the CGED shared task this year, which contained two stages: the detection stage"
W18-3708,P11-1093,0,0.035703,"r M and S types. Our system reached the highest precision in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors. 1 Introduction More and more people are learning a second or third language as an interest, a career plus, or even a challenge to oneself. Chinese is one of the oldest and most versatile languages in the world. Many ∗ † Linlin Li 2 Related Work Earlier attempts to GEC involve rule-based models (Heidorn et al., 1982; Bustamante and Le´on, 1996) and classifier-based approaches (Han et al., 2004; Rozovskaya and Roth, 2011), which can cope with Equal Contribution This work was done while the author at Alibaba Group 60 Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 60–69 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics Table 1: Typical examples for four types of errors Error Original Sentence Correct Sentence M R S W 中国已成了世界拥有最多“烟民”的国家。 孩子的教育不能只靠一个学 学校老师。 父母对孩子的爱 爱情 是最重要的。 生产率较低，那肯 肯定价格要上升。 中国已成了世界上 上拥有最多“烟民”的国家。 孩子的教育不能只靠一个老师。 父母对孩子的关 关爱 是最重要的。 生产率较低，那价 价格 肯定要上升。 map the sentence from learner space to expert space"
W18-3708,P17-4012,0,0.0310237,"re constructed by ourselves. Besides, we also tested different granularities of the model, which means, used char-level or phrase-level translation model. It is worth to mention that we found that using dataall outperformed dataed significantly, so we only did detailed experiments on dataall because of the time limitation of the contest. The configurations of our six different SMT models Sj , j ∈ {1, 2, 3, 4, 5, 6} are shown in Table 3 Many excellent tools can emancipate us from the heavy burden of implementing models from scratch. For those NMT GEC models, we implemented it with the OpenNMT (Klein et al., 2017) toolkit, and for those SMT GEC models, we implemented the language model with KenLM (Heafield, 2011) toolkit and translation model with Moses (Koehn et al., 2007). For the Lang-8 dataset, we found that in those 717,241 lines data, 474,638 lines contained traditional Chinese. The traditional Chinese cannot convey more information than its corresponding simplified Chinese, but will make the size of vocabulary much larger. So, we used the opencc Evaluation and Discussion Data Split and Experiment Setting To train the BiLSTM-CRF model, we collected several datasets of CGED, which are 2015, 2016,"
W18-3708,P07-2045,0,0.00555906,"Missing"
W18-3708,W15-4418,0,0.0180066,"del used for GEC, and later the round-trip translation is also used in GEC (Madnani et al., 2012). A POS-factored SMT system is proposed (Yuan and Felice, 2013) to correct five types of errors in the text. In the work of Felice et al. (2014), they propose a pipeline of the rule-based system and a phrase-based SMT system augmented by a sizeable web-based language model. The wordlevel Levenshtein distance between source and target can be used as a translation model feature (Junczys-Dowmunt and Grundkiewicz, 2014) to enhance the model. Rule-based method and ngram statistical method are combined (Wu et al., 2015) to get a hybrid system for CGED shared task. Recently Napoles and Callison-Bursh (2017) propose a lightweight approach to GEC called Specialized Machine translation for Error Correction. Nevertheless, Neural Machine Translation (NMT) systems have achieved substantial improvements in this field (Sutskever et al., 2014; Bahdanau et al., 2014). Inspired by this phenomenon, Sun et al. (2015) utilize the Convolutional Neural Network (CNN) for the article error correction. The Recurrent Neural Network (RNN) is also used (Yuan and Briscoe, 2016) to 1 61 provided by NLPCC 2018 GEC shared task Figure"
W18-3708,I17-4006,1,0.860894,"Missing"
W18-3708,N16-1042,0,0.014705,"Rule-based method and ngram statistical method are combined (Wu et al., 2015) to get a hybrid system for CGED shared task. Recently Napoles and Callison-Bursh (2017) propose a lightweight approach to GEC called Specialized Machine translation for Error Correction. Nevertheless, Neural Machine Translation (NMT) systems have achieved substantial improvements in this field (Sutskever et al., 2014; Bahdanau et al., 2014). Inspired by this phenomenon, Sun et al. (2015) utilize the Convolutional Neural Network (CNN) for the article error correction. The Recurrent Neural Network (RNN) is also used (Yuan and Briscoe, 2016) to 1 61 provided by NLPCC 2018 GEC shared task Figure 1: The pipeline of our hybrid system Sequence Labeling LSTM-CRF Edit Set Voting Detection Result Extract Original Sentence Error of M and S Match Three GEC Models Correction for Target Position Merge Predicted Corrections Rule-based Correction-rule Convert by NMT GEC Model Correction-NMT Min Edit Dist SMT GEC Model Correction-SMT Edit Set Three predicted correction sentences Detection Stage Correction Stage sentences, where k ≥ 0. 4 4.1 BiLSTM-CRF In the detection stage, we treated the error detection problem as a sequence labeling problem"
W18-3708,W13-3607,0,0.0178845,"also used an external dataset Lang-81 to train our GEC models, which contains more than 700,000 items, and each item consists of an original sentence and corresponding corrected sentences. Each original sentence has k correction To address more complex errors, MT models are proposed and developed by many researchers. Statistical Machine Translation (SMT) has been dominant for the past two decades. In the work of Brockett et al. (2006), they propose an SMT model used for GEC, and later the round-trip translation is also used in GEC (Madnani et al., 2012). A POS-factored SMT system is proposed (Yuan and Felice, 2013) to correct five types of errors in the text. In the work of Felice et al. (2014), they propose a pipeline of the rule-based system and a phrase-based SMT system augmented by a sizeable web-based language model. The wordlevel Levenshtein distance between source and target can be used as a translation model feature (Junczys-Dowmunt and Grundkiewicz, 2014) to enhance the model. Rule-based method and ngram statistical method are combined (Wu et al., 2015) to get a hybrid system for CGED shared task. Recently Napoles and Callison-Bursh (2017) propose a lightweight approach to GEC called Specialize"
W19-3201,P16-1105,0,0.0687566,"Missing"
