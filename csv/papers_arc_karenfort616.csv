2021.eacl-tutorials.4,Reviewing Natural Language Processing Research,2021,-1,-1,2,0.949107,10471,kevin cohen,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"The reviewing procedure has been identified as one of the major issues in the current situation of the NLP field. While it is implicitly assumed that junior researcher learn reviewing during their PhD project, this might not always be the case. Additionally, with the growing NLP community and the efforts in the context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure."
2020.sltu-1.15,Text Corpora and the Challenge of Newly Written Languages,2020,-1,-1,2,1,5708,alice millour,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"Text corpora represent the foundation on which most natural language processing systems rely. However, for many languages, collecting or building a text corpus of a sufficient size still remains a complex issue, especially for corpora that are accessible and distributed under a clear license allowing modification (such as annotation) and further resharing. In this paper, we review the sources of text corpora usually called upon to fill the gap in low-resource contexts, and how crowdsourcing has been used to build linguistic resources. Then, we present our own experiments with crowdsourcing text corpora and an analysis of the obstacles we encountered. Although the results obtained in terms of participation are still unsatisfactory, we advocate that the effort towards a greater involvement of the speakers should be pursued, especially when the language of interest is newly written."
2020.lrec-1.34,Creating Expert Knowledge by Relying on Language Learners: a Generic Approach for Mass-Producing Language Resources by Combining Implicit Crowdsourcing and Language Learning,2020,0,0,5,0,3003,lionel nicolas,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce in this paper a generic approach to combine implicit crowdsourcing and language learning in order to mass-produce language resources (LRs) for any language for which a crowd of language learners can be involved. We present the approach by explaining its core paradigm that consists in pairing specific types of LRs with specific exercises, by detailing both its strengths and challenges, and by discussing how much these challenges have been addressed at present. Accordingly, we also report on on-going proof-of-concept efforts aiming at developing the first prototypical implementation of the approach in order to correct and extend an LR called ConceptNet based on the input crowdsourced from language learners. We then present an international network called the European Network for Combining Language Learning with Crowdsourcing Techniques (enetCollect) that provides the context to accelerate the implementation of this generic approach. Finally, we exemplify how it can be used in several language learning scenarios to produce a multitude of NLP resources and how it can therefore alleviate the long-standing NLP issue of the lack of LRs."
2020.lrec-1.541,Rigor Mortis: Annotating {MWE}s with a Gamified Platform,2020,0,0,1,1,10472,karen fort,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present here Rigor Mortis, a gamified crowdsourcing platform designed to evaluate the intuition of the speakers, then train them to annotate multi-word expressions (MWEs) in French corpora. We previously showed that the speakers{'} intuition is reasonably good (65{\%} in recall on non-fixed MWE). We detail here the annotation results, after a training phase using some of the tests developed in the PARSEME-FR project."
2020.jeptalnrecital-eternal.4,R{\\'e}pliquer et {\\'e}tendre pour l{'}alsacien {``}{\\'E}tiquetage en parties du discours de langues peu dot{\\'e}es par sp{\\'e}cialisation des plongements lexicaux{''} (Replicating and extending for {A}lsatian : {``}{POS} tagging for low-resource languages by adapting word embeddings{''}),2020,-1,-1,2,1,5708,alice millour,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). 2e atelier {\\'E}thique et TRaitemeNt Automatique des Langues (ETeRNAL)",0,"Nous pr{\'e}sentons ici les r{\'e}sultats d{'}un travail de r{\'e}plication et d{'}extension pour l{'}alsacien d{'}une exp{\'e}rience concernant l{'}{\'e}tiquetage en parties du discours de langues peu dot{\'e}es par sp{\'e}cialisation des plongements lexicaux (Magistry et al., 2018). Ce travail a {\'e}t{\'e} r{\'e}alis{\'e} en {\'e}troite collaboration avec les auteurs de l{'}article d{'}origine. Cette interaction riche nous a permis de mettre au jour les {\'e}l{\'e}ments manquants dans la pr{\'e}sentation de l{'}exp{\'e}rience, de les compl{\'e}ter, et d{'}{\'e}tendre la recherche {\`a} la robustesse {\`a} la variation."
2020.acl-tutorials.4,Reviewing Natural Language Processing Research,2020,-1,-1,2,0.949107,10471,kevin cohen,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"This tutorial will cover the theory and practice of reviewing research in natural language processing. Heavy reviewing burdens on natural language processing researchers have made it clear that our community needs to increase the size of our pool of potential reviewers. Simultaneously, notable {``}false negatives{''}---rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences{---}have raised awareness of the fact that our reviewing practices leave something to be desired. We do not often talk about {``}false positives{''} with respect to conference papers, but leaders in the field have noted that we seem to have a publication bias towards papers that report high performance, with perhaps not much else of interest in them. It need not be this way. Reviewing is a learnable skill, and you will learn it here via lectures and a considerable amount of hands-on practice."
R19-1089,Community Perspective on Replicability in Natural Language Processing,2019,0,0,2,0,3132,margot mieskes,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"With recent efforts in drawing attention to the task of replicating and/or reproducing results, for example in the context of COLING 2018 and various LREC workshops, the question arises how the NLP community views the topic of replicability in general. Using a survey, in which we involve members of the NLP community, we investigate how our community perceives this topic, its relevance and options for improvement. Based on over two hundred participants, the survey results confirm earlier observations, that successful reproducibility requires more than having access to code and data. Additionally, the results show that the topic has to be tackled from the authors{'}, reviewers{'} and community{'}s side."
R19-1090,Unsupervised Data Augmentation for Less-Resourced Languages with no Standardized Spelling,2019,0,0,2,1,5708,alice millour,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Building representative linguistic resources and NLP tools for non-standardized languages is challenging: when spelling is not determined by a norm, multiple written forms can be encountered for a given word, inducing a large proportion of out-of-vocabulary words. To embrace this diversity, we propose a methodology based on crowdsourced alternative spellings we use to extract rules applied to match OOV words with one of their spelling variants. This virtuous process enables the unsupervised augmentation of multi-variant lexicons without expert rule definition. We apply this multilingual methodology on Alsatian, a French regional language and provide an intrinsic evaluation of the correctness of the variants pairs, and an extrinsic evaluation on a downstream task. We show that in a low-resource scenario, 145 inital pairs can lead to the generation of 876 additional variant pairs, and a diminution of OOV words improving the part-of-speech tagging performance by 1 to 4{\%}."
W18-4923,{``}Fingers in the Nose{''}: Evaluating Speakers{'} Identification of Multi-Word Expressions Using a Slightly Gamified Crowdsourcing Platform,2018,0,0,1,1,10472,karen fort,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"This article presents the results we obtained in crowdsourcing French speakers{'} intuition concerning multi-work expressions (MWEs). We developed a slightly gamified crowdsourcing platform, part of which is designed to test users{'} ability to identify MWEs with no prior training. The participants perform relatively well at the task, with a recall reaching 65{\%} for MWEs that do not behave as function words."
L18-1071,Toward a Lightweight Solution for Less-resourced Languages: Creating a {POS} Tagger for {A}lsatian Using Voluntary Crowdsourcing,2018,0,0,2,1,5708,alice millour,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present here the results of an experiment aiming at crowdsourcing part-of-speech annotations for a less-resourced French regional language, Alsatian. We used for this purpose a specifically-developed slightly gamified platform, Bisame. It allowed us to gather annotations on a variety of corpora covering some of the language dialectal variations. The quality of the annotations, which reach an averaged F-measure of 93%, enabled us to train a first tagger for Alsatian that is nearly 84% accurate. The platform as well as the produced annotations and tagger are all freely available. The platform can easily be adapted to other languages, thus providing a solution to (some of) the less-resourced languages issue."
2017.jeptalnrecital-long.10,Vers une solution l{\\'e}g{\\`e}re de production de donn{\\'e}es pour le {TAL} : cr{\\'e}ation d{'}un tagger de l{'}alsacien par crowdsourcing b{\\'e}n{\\'e}vole (Toward a lightweight solution to the language resources bottleneck issue: creating a {POS} tagger for {A}lsatian using voluntary crowdsourcing),2017,-1,-1,2,1,5708,alice millour,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Nous pr{\'e}sentons ici les r{\'e}sultats d{'}une exp{\'e}rience men{\'e}e sur l{'}annotation en parties du discours d{'}un corpus d{'}une langue r{\'e}gionale encore peu dot{\'e}e, l{'}alsacien, via une plateforme de myriadisation (crowdsourcing) b{\'e}n{\'e}vole d{\'e}velopp{\'e}e sp{\'e}cifiquement {\`a} cette fin : Bisame1 . La plateforme, mise en ligne en mai 2016, nous a permis de recueillir 15 846 annotations gr{\^a}ce {\`a} 42 participants. L{'}{\'e}valuation des annotations, r{\'e}alis{\'e}e sur un corpus de r{\'e}f{\'e}rence, montre que la F-mesure des annotations volontaires est de 0, 93. Le tagger entra{\^\i}n{\'e} sur le corpus annot{\'e} atteint lui 82 {\%} d{'}exactitude. Il s{'}agit du premier tagger sp{\'e}cifique {\`a} l{'}alsacien. Cette m{\'e}thode de d{\'e}veloppement de ressources langagi{\`e}res est donc efficace et prometteuse pour certaines langues peu dot{\'e}es, dont un nombre suffisant de locuteurs est connect{\'e} et actif sur le Web. Le code de la plateforme, le corpus annot{\'e} et le tagger sont librement disponibles."
2017.jeptalnrecital-court.21,Vers l{'}annotation par le jeu de corpus (plus) complexes : le cas de la langue de sp{\\'e}cialit{\\'e} (Towards (more) complex corpora annotation using a game with a purpose : the case of scientific language),2017,-1,-1,1,1,10472,karen fort,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Nous avons pr{\'e}c{\'e}demment montr{\'e} qu{'}il est possible de faire produire des annotations syntaxiques de qualit{\'e} par des participants {\`a} un jeu ayant un but. Nous pr{\'e}sentons ici les r{\'e}sultats d{'}une exp{\'e}rience visant {\`a} {\'e}valuer leur production sur un corpus plus complexe, en langue de sp{\'e}cialit{\'e}, en l{'}occurrence un corpus de textes scientifiques sur l{'}ADN. Nous d{\'e}terminons pr{\'e}cis{\'e}ment la complexit{\'e} de ce corpus, puis nous {\'e}valuons les annotations en syntaxe de d{\'e}pendances produites par les joueurs par rapport {\`a} une r{\'e}f{\'e}rence mise au point par des experts du domaine."
L16-1252,"Yes, We Care! Results of the Ethics and Natural Language Processing Surveys",2016,0,3,1,1,10472,karen fort,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present here the context and results of two surveys (a French one and an international one) concerning Ethics and NLP, which we designed and conducted between June and September 2015. These surveys follow other actions related to raising concern for ethics in our community, including a Journ{\'e}e d{'}{\'e}tudes, a workshop and the Ethics and Big Data Charter. The concern for ethics shows to be quite similar in both surveys, despite a few differences which we present and discuss. The surveys also lead to think there is a growing awareness in the field concerning ethical issues, which translates into a willingness to get involved in ethics-related actions, to debate about the topic and to see ethics be included in major conferences themes. We finally discuss the limits of the surveys and the means of action we consider for the future. The raw data from the two surveys are freely available online."
C16-1286,Crowdsourcing Complex Language Resources: Playing to Annotate Dependency Syntax,2016,18,3,2,0,5831,bruno guillaume,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This article presents the results we obtained on a complex annotation task (that of dependency syntax) using a specifically designed Game with a Purpose, ZombiLingo. We show that with suitable mechanisms (decomposition of the task, training of the players and regular control of the annotation quality during the game), it is possible to obtain annotations whose quality is significantly higher than that obtainable with a parser, provided that enough players participate. The source code of the game and the resulting annotated corpora (for French) are freely available."
couillault-etal-2014-evaluating,Evaluating corpora documentation with regards to the Ethics and Big Data Charter,2014,10,3,2,0,34987,alain couillault,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The authors have written the Ethic and Big Data Charter in collaboration with various agencies, private bodies and associations. This Charter aims at describing any large or complex resources, and in particular language resources, from a legal and ethical viewpoint and ensuring the transparency of the process of creating and distributing such resources. We propose in this article an analysis of the documentation coverage of the most frequently mentioned language resources with regards to the Charter, in order to show the benefit it offers."
candito-etal-2014-deep,Deep Syntax Annotation of the Sequoia {F}rench Treebank,2014,27,7,5,0,16504,marie candito,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We define a deep syntactic representation scheme for French, which abstracts away from surface syntactic variation and diathesis alternations, and describe the annotation of deep syntactic representations on top of the surface dependency trees of the Sequoia corpus. The resulting deep-annotated corpus, named deep-sequoia, is freely available, and hopefully useful for corpus linguistics studies and for training deep analyzers to prepare semantic analysis."
guillaume-etal-2014-mapping,Mapping the Lexique des Verbes du Fran{\\c{c}}ais (Lexicon of {F}rench Verbs) to a {NLP} lexicon using examples,2014,10,0,2,0.611272,5831,bruno guillaume,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article presents experiments aiming at mapping the Lexique des Verbes du Fran{\c{c}}ais (Lexicon of French Verbs) to FRILEX, a Natural Language Processing (NLP) lexicon based on D ICOVALENCE. The two resources (Lexicon of French Verbs and D ICOVALENCE) were built by linguists, based on very different theories, which makes a direct mapping nearly impossible. We chose to use the examples provided in one of the resource to find implicit links between the two and make them explicit."
lafourcade-fort-2014-propa,Propa-{L}: a semantic filtering service from a lexical network created using Games With A Purpose,2014,14,2,2,0,5594,mathieu lafourcade,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article presents Propa-L, a freely accessible Web service that allows to semantically filter a lexical network. The language resources behind the service are dynamic and created through Games With A Purpose. We show an example of application of this service: the generation of a list of keywords for parental filtering on the Web, but many others can be envisaged. Moreover, the propagation algorithm we present here can be applied to any lexical network, in any language."
F14-3006,{ZOMBILINGO}: eating heads to perform dependency syntax annotation ({ZOMBILINGO} : manger des t{\\^e}tes pour annoter en syntaxe de d{\\'e}pendances) [in {F}rench],2014,0,0,1,1,10472,karen fort,Proceedings of TALN 2014 (Volume 3: System Demonstrations),0,None
F14-2031,Annotation scheme for deep dependency syntax of {F}rench (Un sch{\\'e}ma d{'}annotation en d{\\'e}pendances syntaxiques profondes pour le fran{\\c{c}}ais) [in {F}rench],2014,0,0,5,0,5832,guy perrier,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
F14-1026,Quantitative study of disfluencies in schizophrenics{'} speech: Automatize to limit biases ({\\'E}tude quantitative des disfluences dans le discours de schizophr{\\`e}nes : automatiser pour limiter les biais) [in {F}rench],2014,-1,-1,2,0,782,maxime amblard,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
F13-2016,Formalizing an annotation guide : some experiments towards assisted agile annotation (Exp{\\'e}riences de formalisation d{'}un guide d{'}annotation : vers l{'}annotation agile assist{\\'e}e) [in {F}rench],2013,-1,-1,2,0.611272,5831,bruno guillaume,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
W12-3606,Structured Named Entities in two distinct press corpora: Contemporary Broadcast News and Old Newspapers,2012,18,16,3,0.128351,5280,sophie rosset,Proceedings of the Sixth Linguistic Annotation Workshop,0,"This paper compares the reference annotation of structured named entities in two corpora with different origins and properties. It addresses two questions linked to such a comparison. On the one hand, what specific issues were raised by reusing the same annotation scheme on a corpus that differs from the first in terms of media and that predates it by more than a century? On the other hand, what contrasts were observed in the resulting annotations across the two corpora?"
fort-etal-2012-analyzing,Analyzing the Impact of Prevalence on the Evaluation of a Manual Annotation Campaign,2012,19,10,1,1,10472,karen fort,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This article details work aiming at evaluating the quality of the manual annotation of gene renaming couples in scientific abstracts, which generates sparse annotations. To evaluate these annotations, we compare the results obtained using the commonly advocated inter-annotator agreement coefficients such as S, Îº and {\""I}Â, the less known R, the weighted coefficients Îº{\""I}Â and {\^I}{\mbox{$\pm$}} as well as the F-measure and the SER. We analyze to which extent they are relevant for our data. We then study the bias introduced by prevalence by changing the way the contingency table is built. We finally propose an original way to synthesize the results by computing distances between categories, based on the produced annotations."
fort-claveau-2012-annotating,Annotating Football Matches: Influence of the Source Medium on Manual Annotation,2012,8,7,1,1,10472,karen fort,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present an annotation campaign of football (soccer) matches, from a heterogeneous text corpus of both match minutes and video commentary transcripts, in French. The data, annotations and evaluation process are detailed, and the quality of the annotated corpus is discussed. In particular, we propose a new technique to better estimate the annotator agreement when few elements of a text are to be annotated. Based on that, we show how the source medium influenced the process and the quality."
F12-2008,{TCOF}-{POS} : un corpus libre de fran{\\c{c}}ais parl{\\'e} annot{\\'e} en morphosyntaxe ({TCOF}-{POS} : A Freely Available {POS}-Tagged Corpus of Spoken {F}rench) [in {F}rench],2012,-1,-1,2,0,30098,christophe benzitoun,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
F12-2031,Annotation manuelle de matchs de foot : Oh la la la ! l{'}accord inter-annotateurs ! et c{'}est le but ! (Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal !) [in {F}rench],2012,0,0,1,1,10472,karen fort,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
C12-2079,Manual Corpus Annotation: Giving Meaning to the Evaluation Metrics,2012,15,13,3,0,32781,yann mathet,Proceedings of {COLING} 2012: Posters,0,"Computing inter-annotator agreement measures on a manually annotated corpus is necessary to evaluate the reliability of its annotation. However, the interpretation of the obtained results is recognized as highly arbitrary. We describe in this article a method and a tool that we developed which shuffles a reference annotation according to different error paradigms, thereby creating artificial annotations with controlled errors. Agreement measures are computed on these corpora, and the obtained results are used to model the behavior of these measures and understand their actual meaning."
C12-1055,Modeling the Complexity of Manual Annotation Tasks: a Grid of Analysis,2012,24,10,1,1,10472,karen fort,Proceedings of {COLING} 2012,0,"Manual corpus annotation is getting widely used in Natural Language Processing (NLP). While being recognized as a difficult task, no in-depth analysis of its complexity has been performed yet. We provide in this article a grid of analysis of the different complexity dimensions of an annotation task, which helps estimating beforehand the difficulties and cost of annotation campaigns. We observe the applicability of this grid on existing annotation campaigns and detail its application on a real-world example."
W11-1810,{B}io{NLP} Shared Task 2011 {--} Bacteria Gene Interactions and Renaming,2011,14,14,4,0,44281,julien jourde,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"We present two related tasks of the BioNLP Shared Tasks 2011: Bacteria Gene Renaming (Rename) and Bacteria Gene Interactions (GI). We detail the objectives, the corpus specification, the evaluation metrics, and we summarize the participants' results. Both issued from PubMed scientific literature abstracts, the Rename task aims at extracting gene name synonyms, and the GI task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria."
W11-0411,"Proposal for an Extension of Traditional Named Entities: From Guidelines to Evaluation, an Overview",2011,27,36,4,0.5,5675,cyril grouin,Proceedings of the 5th Linguistic Annotation Workshop,0,"Within the framework of the construction of a fact database, we defined guidelines to extract named entities, using a taxonomy based on an extension of the usual named entities definition. We thus defined new types of entities with broader coverage including substantive-based expressions. These extended named entities are hierarchical (with types and components) and compositional (with recursive type inclusion and metonymy annotation). Human annotators used these guidelines to annotate a 1.3M word broadcast news corpus in French. This article presents the definition and novelty of extended named entity annotation guidelines, the human annotation of a global corpus and of a mini reference corpus, and the evaluation of annotations through the computation of inter-annotator agreements. Finally, we discuss our approach and the computed results, and outline further work."
J11-2010,Last Words: {A}mazon {M}echanical {T}urk: Gold Mine or Coal Mine?,2011,16,157,1,1,10472,karen fort,Computational Linguistics,0,"Recently heard at a tutorial in our field: xe2x80x9cIt cost me less than one hundred bucks to annotate this using Amazon Mechanical Turk!xe2x80x9d Assertions like this are increasingly common, but we believe they should not be stated so proudly; they ignore the ethical consequences of using MTurk (Amazon Mechanical Turk) as a source of labor. Manually annotating corpora or manually developing any other linguistic resource, such as a set of judgments about system outputs, represents such a high cost that many researchers are looking for alternative solutions to the standard approach. MTurk is becoming a popular one. However, as in any scientific endeavor involving humans, there is an unspoken ethical dimension involved in resource construction and system evaluation, and this is especially true of MTurk. We would like here to raise some questions about the use of MTurk. To do so, we will define precisely what MTurk is and what it is not, highlighting the issues raised by the system. We hope that this will point out opportunities for our community to deliberately value ethics above cost savings."
2011.jeptalnrecital-long.12,Un turc m{\\'e}canique pour les ressources linguistiques : critique de la myriadisation du travail parcellis{\\'e} ({M}echanical {T}urk for linguistic resources: review of the crowdsourcing of parceled work),2011,-1,-1,2,0,250,benoit sagot,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l{'}utilisation est en plein essor depuis quelques ann{\'e}es dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui pr{\'e}vaut dans les articles du domaine, de faire d{\'e}velopper toutes sortes de ressources linguistiques de qualit{\'e}, pour un prix imbattable et en un temps tr{\`e}s r{\'e}duit, par des gens pour qui il s{'}agit d{'}un passe-temps. Nous allons ici d{\'e}montrer que la situation est loin d{'}{\^e}tre aussi id{\'e}ale, que ce soit sur le plan de la qualit{\'e}, du prix, du statut des travailleurs ou de l{'}{\'e}thique. Nous rappellerons ensuite les solutions alternatives d{\'e}j{\`a} existantes ou propos{\'e}es. Notre but est ici double : informer les chercheurs, afin qu{'}ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour am{\'e}liorer le d{\'e}veloppement de nouvelles ressources linguistiques en limitant les risques de d{\'e}rives {\'e}thiques et l{\'e}gales, sans que cela se fasse au prix de leur co{\^u}t ou de leur qualit{\'e}."
W10-1807,Influence of Pre-Annotation on {POS}-Tagged Corpus Development,2010,14,35,1,1,10472,karen fort,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed."
lux-pogodalla-etal-2010-fastkwic,"{F}ast{K}wic, an {``}Intelligent{``} Concordancer Using {FASTR}",2010,5,0,3,0,40018,veronika luxpogodalla,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we introduce the FastKwic (Key Word In Context using FASTR), a new concordancer for French and English that does not require users to learn any particular request language. Built on FASTR, it shows them not only occurrences of the searched term but also of several morphological, morpho-syntactic and syntactic variants (for example, image enhancement, enhancement of image, enhancement of fingerprint image, image texture enhancement). Fastkwic is freely available. It consists of two UTF-8 compliant Perl modules that depend on several external tools and resources : FASTR, TreeTagger, Flemm (for French). Licenses of theses tools and resources permitting, the FastKwic package is nevertheless self-sufficient. FastKwic first modules is for terminological resource compilation. Its input is a list of terms - as required by FASTR. FastKwic second module is for processing concordances. It relies on FASTR again for indexing the input corpus with terms and their variants. Its output is a concordancer: for each term and its variants, the context of occurrence is provided."
2010.jeptalnrecital-long.35,{\\'E}valuer des annotations manuelles dispers{\\'e}es : les coefficients sont-ils suffisants pour estimer l{'}accord inter-annotateurs ?,2010,0,3,1,1,10472,karen fort,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}objectif des travaux pr{\'e}sent{\'e}s dans cet article est l{'}{\'e}valuation de la qualit{\'e} d{'}annotations manuelles de relations de renommage de g{\`e}nes dans des r{\'e}sum{\'e}s scientifiques, annotations qui pr{\'e}sentent la caract{\'e}ristique d{'}{\^e}tre tr{\`e}s dispers{\'e}es. Pour cela, nous avons calcul{\'e} et compar{\'e} les coefficients les plus commun{\'e}ment utilis{\'e}s, entre autres kappa (Cohen, 1960) et pi (Scott, 1955), et avons analys{\'e} dans quelle mesure ils sont adapt{\'e}s {\`a} nos donn{\'e}es. Nous avons {\'e}galement {\'e}tudi{\'e} les diff{\'e}rentes pond{\'e}rations applicables {\`a} ces coefficients permettant de calculer le kappa pond{\'e}r{\'e} (Cohen, 1968) et l{'}alpha (Krippendorff, 1980, 2004). Nous avons ainsi {\'e}tudi{\'e} le biais induit par la grande pr{\'e}valence d{'}une cat{\'e}gorie et d{\'e}fini un mode de calcul des distances entre cat{\'e}gories reposant sur les annotations r{\'e}alis{\'e}es."
W09-3025,Towards a Methodology for Named Entities Annotation,2009,8,12,1,1,10472,karen fort,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. Those issues led us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, we put forward a number of methodological recommendations to ensure a coherent and reliable annotation scheme."
2009.jeptalnrecital-long.29,Vers une m{\\'e}thodologie d{'}annotation des entit{\\'e}s nomm{\\'e}es en corpus ?,2009,-1,-1,1,1,10472,karen fort,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La t{\^a}che, aujourd{'}hui consid{\'e}r{\'e}e comme fondamentale, de reconnaissance d{'}entit{\'e}s nomm{\'e}es, pr{\'e}sente des difficult{\'e}s sp{\'e}cifiques en mati{\`e}re d{'}annotation. Nous les pr{\'e}cisons ici, en les illustrant par des exp{\'e}riences d{'}annotation manuelle dans le domaine de la microbiologie. Ces probl{\`e}mes nous am{\`e}nent {\`a} reposer la question fondamentale de ce que les annotateurs doivent annoter et surtout, pour quoi faire. Nous identifions pour cela les applications n{\'e}cessitant l{'}extraction d{'}entit{\'e}s nomm{\'e}es et, en fonction des besoins de ces applications, nous proposons de d{\'e}finir s{\'e}mantiquement les {\'e}l{\'e}ments {\`a} annoter. Nous pr{\'e}sentons ensuite un certain nombre de recommandations m{\'e}thodologiques permettant d{'}assurer un cadre d{'}annotation coh{\'e}rent et {\'e}valuable."
C08-3003,A Toolchain for Grammarians,2008,12,3,5,0.3226,5831,bruno guillaume,Coling 2008: Companion volume: Demonstrations,0,"We present a chain of tools used by grammarians and computer scientists to develop grammatical and lexical resources from linguistic knowledge, for various natural languages. The developed resources are intended to be used in Natural Language Processing (NLP) systems."
2008.jeptalnrecital-court.6,Sylva : plate-forme de validation multi-niveaux de lexiques,2008,9,0,1,1,10472,karen fort,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La production de lexiques est une activit{\'e} indispensable mais complexe, qui n{\'e}cessite, quelle que soit la m{\'e}thode de cr{\'e}ation utilis{\'e}e (acquisition automatique ou manuelle), une validation humaine. Nous proposons dans ce but une plate-forme Web librement disponible, appel{\'e}e Sylva (Systematic lexicon validator). Cette plate-forme a pour caract{\'e}ristiques principales de permettre une validation multi-niveaux (par des validateurs, puis un expert) et une tra{\c{c}}abilit{\'e} de la ressource. La t{\^a}che de l{'}expert(e) linguiste en est all{\'e}g{\'e}e puisqu{'}il ne lui reste {\`a} consid{\'e}rer que les donn{\'e}es sur lesquelles il n{'}y a pas d{'}accord inter-validateurs."
W07-1603,{P}rep{L}ex: A Lexicon of {F}rench Prepositions for Parsing,2007,10,8,1,1,10472,karen fort,Proceedings of the Fourth {ACL}-{SIGSEM} Workshop on Prepositions,0,PrepLex is a lexicon of French prepositions which provides all the syntactic information needed for parsing. It was built by comparing and merging several authoritative lexical sources. This lexicon also includes information about the prepositions or classes of prepositions that appear in French verb subcategorization frames. This resource has been developed as a first step in making current French preposition lexicons available for effective natural language processing.
2007.jeptalnrecital-long.20,{P}rep{L}ex : un lexique des pr{\\'e}positions du fran{\\c{c}}ais pour l{'}analyse syntaxique,2007,-1,-1,1,1,10472,karen fort,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,PrepLex est un lexique des pr{\'e}positions du fran{\c{c}}ais. Il contient les informations utiles {\`a} des syst{\`e}mes d{'}analyse syntaxique. Il a {\'e}t{\'e} construit en comparant puis fusionnant diff{\'e}rentes sources d{'}informations lexicales disponibles. Ce lexique met {\'e}galement en {\'e}vidence les pr{\'e}positions ou classes de pr{\'e}positions qui apparaissent dans la d{\'e}finition des cadres de sous-cat{\'e}gorisation des ressources lexicales qui d{\'e}crivent la valence des verbes.
