2020.lrec-1.439,Q17-1010,0,0.0503315,"Missing"
2020.lrec-1.439,carreras-etal-2004-freeling,0,0.0709044,"Missing"
2020.lrec-1.439,K17-1042,0,0.0652146,"Missing"
2020.lrec-1.439,K18-2013,0,0.0632971,"Missing"
2020.lrec-1.439,K16-1006,0,0.0124046,"are composed entirely of numerical digits are assigned a special embedding for numbers, since we consider them to always have a more or less equivalent role in the context. Ambiguities between common nouns and proper nouns are ignored, as names are out of the scope of what we try to accomplish here and should be solved using Named Entity Recognition (NER) techniques instead. We keep punctuation in order to recover the sentence boundaries within the window, as these may still prove useful to generate a proper context. 5.2. Architecture & training Our model architecture is based on context2vec (Melamud et al., 2016), which itself is a modification of the original word2vec CBOW model (Mikolov et al., 2013). In context2vec, the context for each word is computed using a bidirectional LSTM, rather than as a vector average (as in word2vec), which enables the embeddings to capture sentence structure and word order, rather than only word cooccurrence. The training procedure is analogous to that of word2vec, since our objective is similar: given a context, compute the probability that the word belongs to that context, for each word in the vocabulary—in our case, instead of word Each training instance consists of"
2020.lrec-1.439,W13-5631,0,0.0411024,"Missing"
2020.lrec-1.48,abel-etal-2014-koko,0,0.0288683,"ors: answers that have the same lemma as the correct answer. “Non-word” is the subset with orthographic errors, which do not correspond to any real word form. “Diff. lemma” is the subset with answers whose lemma is different from the lemma of the correct answer. Additional exercise types, which will allow the learner to make certain types of errors—word order, omission/deletion/insertion—are planned for future releases of the system. Currently, the corpus mostly contains grammatical errors, which are difficult to annotate in traditional learner corpora. For example, the Koko corpus of German (Abel et al., 2014) mainly has non-grammatical errors annotated, with grammatical error annotation left for future work. 4.1. Automatic Annotation Texts are tokenized and analyzed using language-specific analyzers when they are uploaded to Revita.9 All orthographic errors are annotated automatically. This annotation is based on the output of a morphological analyzer: if the analyzer return no analysis for a word, it does not exist in the vocabulary, and we consider it as a word with orthographic errors (i.e., non-word errors). We are mostly interested in grammatical errors, which are also annotated automatically"
2020.lrec-1.48,W18-6111,0,0.102467,"arner corpora and collecting learner data are important for creating gold-standard data for NLP models for learner language. For example, predicting learner mistake patterns was at the focus of the Duolingo Shared Task, 2018.6 This is a crucial research problem for any language learning system, which attempts to make the process of L2 acquisition more effective. Considering the application of ReLCo for grammatical error detection/correction, we should stress that this problem is currently in the research focus primarily for English. Very few researchers focus on other languages, e.g., German (Boyd, 2018). The ReLCo approach to collecting learner data, which is available and growing, will help strengthen the link between learner corpora and intelligent tutoring systems— seen as a key future direction in learner corpora (Meurers, 2015). 4. Learner Corpora for Russian We next describe an initial, pilot version of ReLCo for Russian. This data includes answers to exercises which were practiced during 2017–2019 by 150 learners. The corpus is still rather small: around 1.35 million tokens, 8 422 sentences in total. Nevertheless, it is comparable in size with the released Write & Improve+LOCNESS corp"
2020.lrec-1.48,W19-4406,0,0.0318267,"e ReLCo approach to collecting learner data, which is available and growing, will help strengthen the link between learner corpora and intelligent tutoring systems— seen as a key future direction in learner corpora (Meurers, 2015). 4. Learner Corpora for Russian We next describe an initial, pilot version of ReLCo for Russian. This data includes answers to exercises which were practiced during 2017–2019 by 150 learners. The corpus is still rather small: around 1.35 million tokens, 8 422 sentences in total. Nevertheless, it is comparable in size with the released Write & Improve+LOCNESS corpus (Bryant et al., 2019; Granger, 1998) for the Low-resourced track at the BEA Shared Task on Grammatical Error Correction (Bryant et al., 2019).7 The W&I+LOCNESS corpus contains 801 361 tokens, including correct sentences. Every sentence in the Russian ReLCo includes answers given by students to the following types of exercises: • “cloze” exercises (fill-in-the-blank) with the lemma of the missing word given as a hint; • multiple-choice exercises—with distractors generated for many kinds of exercises; • listening exercises. The learner receives the text one “snippet” at a time (about 1 paragraph), with several word"
2020.lrec-1.48,W19-4451,1,0.32189,"vita language-learning platform. Revita is an online L2 learning system for learners beyond the beginner level. It covers several languages, most of which are highly inflectional, with rich morphology. Revita allows learners to practice with authentic texts, which can be chosen and uploaded to the platform by the learner herself, or by a teacher for a group of learners. The system creates a variety of exercises automatically trying to adapt the level of exercises to every user depending on her level of proficiency. A continuous assessment of the users’ answers is also performed automatically (Hou et al., 2019). At present, Revita provides no mode for submitting essays, so the data collected is based on pre-existing texts. Despite this limitation, ReLCo presents: • • • • authentic learners errors in context; the time when the errors were made; unique internal identifiers (IDs) of the learner; the types of exercises which were practiced. This makes ReLCo a valuable resource of learner data, which can be used for improving teaching and learning processes. Our main thesis in this paper is that although ReLCo is not a “conventional” learner corpus, it provides many of the same benefits as “proper” learn"
2020.lrec-1.48,W17-0304,1,0.535221,"ammar books (Granath, 2009). The prevalent tendency in studies of learner language is to collect corpora for specific experiments, and then either discard them or not make them fully available—from the same list of 174 corpora, only several can be freely downloaded. This makes the learner data not usable for research and creates obstacles to collaboration in developing better language resources (Nesselhauf, 2004). In this work we propose a new paradigm for creating learner corpora—building them automatically based on an existing language learning platform, in particular, the Revita platform, (Katinskaia et al., 2017; Katinskaia et al., 2018; Katinskaia and Yangarber, 2018).4 The learner corpus is collected continuously and annotated automatically while students practice the language by performing a variety of exercises. All collected learner data is used in Revita for generating new exercises—it creates the “learning feedback loop” helping students to improve their language skills more effectively. We make available the first version of the Russian ReLCo 5 and hope to encourage collaborators to join in improving the learner datasets and in performing research experiments based on collected data. The rema"
2020.lrec-1.48,L18-1644,1,0.728176,"9). The prevalent tendency in studies of learner language is to collect corpora for specific experiments, and then either discard them or not make them fully available—from the same list of 174 corpora, only several can be freely downloaded. This makes the learner data not usable for research and creates obstacles to collaboration in developing better language resources (Nesselhauf, 2004). In this work we propose a new paradigm for creating learner corpora—building them automatically based on an existing language learning platform, in particular, the Revita platform, (Katinskaia et al., 2017; Katinskaia et al., 2018; Katinskaia and Yangarber, 2018).4 The learner corpus is collected continuously and annotated automatically while students practice the language by performing a variety of exercises. All collected learner data is used in Revita for generating new exercises—it creates the “learning feedback loop” helping students to improve their language skills more effectively. We make available the first version of the Russian ReLCo 5 and hope to encourage collaborators to join in improving the learner datasets and in performing research experiments based on collected data. The remainder of this paper is st"
2020.lrec-1.48,W19-3702,1,0.522491,"Missing"
2020.lrec-1.48,W16-6509,0,0.0598923,"Missing"
2021.bea-1.15,C18-1139,0,0.0583672,"Missing"
2021.bea-1.15,W19-2305,0,0.0589239,"Missing"
2021.bea-1.15,W19-4410,0,0.0589337,"to the paucity of training data, we explore the ability of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data. In this work, we focus on errors in inflection. Our experiments A. show that pre-trained BERT performs worse at detecting grammatical irregularities for Russian than for English; B. show that fine-tuned BERT yields promising results on assessing correctness in grammatical exercises; and C. establish new GED benchmarks for Russian. To further investigate its performance, we compare fine-tuned BERT with one a state-of-theart model for GED (Bell et al., 2019) on our dataset, and on RULEC-GEC (Rozovskaya and Roth, 2019). We release our manually annotated learner dataset, used for testing, for general use. 1 Introduction Many intelligent tutoring systems (ITS) and computer-aided language learning systems (CALL) generate exercises and try to assess the learner’s answers automatically. Providing feedback to the learner is difficult, due to the critical requirement of very high precision—providing incorrect feedback is much more harmful than no feedback at 1 revita.cs.helsinki.fi all. For this reason, most existing systems have prefabricated sets of ex"
2021.bea-1.15,W18-6111,0,0.027845,"he experiments were extended by Wolf (2019) by evaluating the OpenAI Generative Pre-trained Transformer (GPT) of Radford et al. (2018). BERT outperformed the OpenAI GPT on the datasets from Linzen et al. (2016) and Goudalova et al. (2018), but not on the dataset from Marvin and Linzen (2018). The problem of data scarcity can be addressed by generating artificial training data. Among the existing approaches are oversampling a small learner corpus (Junczys-Dowmunt et al., 2018; Aprosio et al., 2019), utilizing additional resources, such as Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014; Boyd, 2018), or introducing natural and synthetic noise into error-free data (Belinkov and Bisk, 2017; Felice and Yuan, 2014). Natural noise means harvesting naturally occurring errors from the available corpora and creating a look-up table of possible replacements. Using natural noise also tries to imitate the distribution of errors in the available learner corpora. Synthetic noise can be generated by probabilistically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018)."
2021.bea-1.15,2020.emnlp-main.581,0,0.0965616,"Missing"
2021.bea-1.15,W07-1604,0,0.144247,"Missing"
2021.bea-1.15,W19-4423,0,0.0434826,"Missing"
2021.bea-1.15,E14-3013,0,0.0295293,"(GPT) of Radford et al. (2018). BERT outperformed the OpenAI GPT on the datasets from Linzen et al. (2016) and Goudalova et al. (2018), but not on the dataset from Marvin and Linzen (2018). The problem of data scarcity can be addressed by generating artificial training data. Among the existing approaches are oversampling a small learner corpus (Junczys-Dowmunt et al., 2018; Aprosio et al., 2019), utilizing additional resources, such as Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014; Boyd, 2018), or introducing natural and synthetic noise into error-free data (Belinkov and Bisk, 2017; Felice and Yuan, 2014). Natural noise means harvesting naturally occurring errors from the available corpora and creating a look-up table of possible replacements. Using natural noise also tries to imitate the distribution of errors in the available learner corpora. Synthetic noise can be generated by probabilistically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In genera"
2021.bea-1.15,W19-4427,0,0.0350771,"Missing"
2021.bea-1.15,N18-1108,0,0.0181361,"nce pairs: an error-free original and an erroneous one. The erroneous sentence can be built manually or automatically, and differs from the original by only one word—the target position. They feed complete sentences into the model, collect all predictions for the target position, and compare the scores assigned to the original correct word and the incorrect one, e.g., write vs. writes. Errors should have a lower probability than correct forms. The LM performs much worse than supervised models, especially in case of long syntactic dependencies (Jozefowicz et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018). This work was done on Italian, Hebrew, and Russian. Goldberg (2019) adapted the described evaluation methods and applied them to pre-trained BERT models by masking out the target words. BERT showed high scores on all test cases with subjectverb agreement and reflexive anaphora, except for sentences with relative clauses. The experiments were extended by Wolf (2019) by evaluating the OpenAI Generative Pre-trained Transformer (GPT) of Radford et al. (2018). BERT outperformed the OpenAI GPT on the datasets from Linzen et al. (2016) and Goudalova et al. (2018), but not on the dataset from Marvin"
2021.bea-1.15,W19-4451,1,0.834655,"flectional, with rich morphology. In contrast to the pre-fabricated approach, Revita allows the learner to upload arbitrary texts to be used as learning content, and automatically creates exercises based on the chosen content. At practice time, Revita presents the text one paragraph at a time with some words hidden and used as fill-inthe-blank (cloze) exercises. For each hidden word, Revita provides a hint—the base form (lemma) of the word. The learner should insert the inflected form of the lemma, given the context. Continuous assessment of the user’s answers is also performed automatically (Hou et al., 2019). Revita checks the learner’s answer by comparing it with the expected answer—the one found in the original text. The problem arises when, for some exercise, besides the expected answer, another answer is also valid in the context. As a result, Revita may provide undesirable feedback by flagging answers that are not expected, but nonetheless correct, as “errors”—this can strongly mislead, confuse and discourage the learner. For example, both highlighted answers in the example below can be considered correct, but Revita expects the learner to use only the past tense form “сдавал” (“took”): “Мне"
2021.bea-1.15,N18-1055,0,0.0286642,"get words. BERT showed high scores on all test cases with subjectverb agreement and reflexive anaphora, except for sentences with relative clauses. The experiments were extended by Wolf (2019) by evaluating the OpenAI Generative Pre-trained Transformer (GPT) of Radford et al. (2018). BERT outperformed the OpenAI GPT on the datasets from Linzen et al. (2016) and Goudalova et al. (2018), but not on the dataset from Marvin and Linzen (2018). The problem of data scarcity can be addressed by generating artificial training data. Among the existing approaches are oversampling a small learner corpus (Junczys-Dowmunt et al., 2018; Aprosio et al., 2019), utilizing additional resources, such as Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014; Boyd, 2018), or introducing natural and synthetic noise into error-free data (Belinkov and Bisk, 2017; Felice and Yuan, 2014). Natural noise means harvesting naturally occurring errors from the available corpora and creating a look-up table of possible replacements. Using natural noise also tries to imitate the distribution of errors in the available learner corpora. Synthetic noise can be generated by probabilistically injecting characterlevel or word-level noise into the s"
2021.bea-1.15,2020.acl-main.391,0,0.016376,"to determine whether the learner’s answer is also correct in the context. When comparing BERT’s predictions for the masked original word and the masked alternative-correct word, we conjecture that the model recognizes an alternative answer as grammatical if its predicted probability is at least as high as the probability of the expected answer. We also applied two masking strategies (one target vs. multiple targets), see accuracy Acccorr in Table 4. 4.2 Supervised Model Architecture Following prior experiments—which show that fine-tuning BERT for NER (Peters et al., 2019) and error detection (Kaneko et al., 2020) gives better performance than using the contextual representation of words from pre-trained BERT—we also fine-tune the pre-trained model. We modified the Huggingface Pytorch implementation of BERT for token classification and the code for the NER task8 (Debut et al., 2019). Hyper-parameters for fine-tuning BERT are the same as for the NER task: maximum number of epochs is 3, maximum input sequence length is 256, dropout rate is 0.1, batch size is 32, Adam optimizer, and the initial learning rate is set to 5E-5. We split the generated dataset into a training set, a development set, and a test"
2021.bea-1.15,I17-1005,0,0.0606421,"Missing"
2021.bea-1.15,D18-1541,0,0.017588,"stically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In general, the token- and POS-based pattern method demonstrated stronger results. If enough training data is available, errors can be generated by back-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This data was used as the test set for all experiments presented below. Then, we present the method for generating ungrammatical data for training. 3.1 Learner Data While students perform exercises using the Revita language-learning platform, it continuously collects3 and automatically annotates ReLCo—the longitudinal Revita Learner Corpus (Katinskaia et al., 2020), where each record includes: Synthetic error generation based on confusion sets extracted from a spellchecker was used by one of the top-scor"
2021.bea-1.15,2020.lrec-1.48,1,0.844987,"-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This data was used as the test set for all experiments presented below. Then, we present the method for generating ungrammatical data for training. 3.1 Learner Data While students perform exercises using the Revita language-learning platform, it continuously collects3 and automatically annotates ReLCo—the longitudinal Revita Learner Corpus (Katinskaia et al., 2020), where each record includes: Synthetic error generation based on confusion sets extracted from a spellchecker was used by one of the top-scoring systems at the Restricted and the Low Resource tracks at the BEA-2019 Shared task (Grundkiewicz et al., 2019). Both tracks suppose limited use of available learner corpora. This method was compared in (White and Rozovskaya, 2020) with another top scoring approach (Choe et al., 2019) which relies on tokenbased and POS-based confusion sets extracted from a small annotated sample of the W&I +LOCNESS 137 • an authentic learner error in the context; • uni"
2021.bea-1.15,W19-3702,1,0.875965,"Missing"
2021.bea-1.15,W17-0304,1,0.78285,"(ITS) and computer-aided language learning systems (CALL) generate exercises and try to assess the learner’s answers automatically. Providing feedback to the learner is difficult, due to the critical requirement of very high precision—providing incorrect feedback is much more harmful than no feedback at 1 revita.cs.helsinki.fi all. For this reason, most existing systems have prefabricated sets of exercises, with possible expected answers and prepared feedback. Revita is an online L2 learning system for learners beyond the beginner level, which can be used in the classroom and for self-study (Katinskaia et al., 2017; Katinskaia and Yangarber, 2018; Katinskaia et al., 2018). It covers several languages, most of which are highly inflectional, with rich morphology. In contrast to the pre-fabricated approach, Revita allows the learner to upload arbitrary texts to be used as learning content, and automatically creates exercises based on the chosen content. At practice time, Revita presents the text one paragraph at a time with some words hidden and used as fill-inthe-blank (cloze) exercises. For each hidden word, Revita provides a hint—the base form (lemma) of the word. The learner should insert the inflected"
2021.bea-1.15,N19-1333,0,0.0184582,"izing additional resources, such as Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014; Boyd, 2018), or introducing natural and synthetic noise into error-free data (Belinkov and Bisk, 2017; Felice and Yuan, 2014). Natural noise means harvesting naturally occurring errors from the available corpora and creating a look-up table of possible replacements. Using natural noise also tries to imitate the distribution of errors in the available learner corpora. Synthetic noise can be generated by probabilistically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In general, the token- and POS-based pattern method demonstrated stronger results. If enough training data is available, errors can be generated by back-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This"
2021.bea-1.15,Q16-1037,0,0.503864,"most challenging task, which is explained in part by the small size of RULEC-GEC. Related Work Early experiments with GED utilized rules (Foster and Vogel, 2004) and supervised learning from error-annotated corpora (Chodorow et al., 2007). Much work focused on detection of particular types of errors, e.g., verb forms (Lee and Seneff, 2008). 2 The annotated data is released with this paper. The dataset contains only replacement errors due to the current design of the practice mode in Revita. 136 The problem of scarce training data for GED can be approached by using pre-trained language models. Linzen et al. (2016) explored the ability of a LSTM model trained without grammatical supervision to detect grammatical errors by performing an unsupervised cloze test. The authors use a dataset of sentence pairs: an error-free original and an erroneous one. The erroneous sentence can be built manually or automatically, and differs from the original by only one word—the target position. They feed complete sentences into the model, collect all predictions for the target position, and compare the scores assigned to the original correct word and the incorrect one, e.g., write vs. writes. Errors should have a lower p"
2021.bea-1.15,D18-1151,0,0.0245225,"rs use a dataset of sentence pairs: an error-free original and an erroneous one. The erroneous sentence can be built manually or automatically, and differs from the original by only one word—the target position. They feed complete sentences into the model, collect all predictions for the target position, and compare the scores assigned to the original correct word and the incorrect one, e.g., write vs. writes. Errors should have a lower probability than correct forms. The LM performs much worse than supervised models, especially in case of long syntactic dependencies (Jozefowicz et al., 2016; Marvin and Linzen, 2018; Gulordava et al., 2018). This work was done on Italian, Hebrew, and Russian. Goldberg (2019) adapted the described evaluation methods and applied them to pre-trained BERT models by masking out the target words. BERT showed high scores on all test cases with subjectverb agreement and reflexive anaphora, except for sentences with relative clauses. The experiments were extended by Wolf (2019) by evaluating the OpenAI Generative Pre-trained Transformer (GPT) of Radford et al. (2018). BERT outperformed the OpenAI GPT on the datasets from Linzen et al. (2016) and Goudalova et al. (2018), but not o"
2021.bea-1.15,D19-5545,0,0.0353538,"Missing"
2021.bea-1.15,L18-1644,1,0.845823,") generate exercises and try to assess the learner’s answers automatically. Providing feedback to the learner is difficult, due to the critical requirement of very high precision—providing incorrect feedback is much more harmful than no feedback at 1 revita.cs.helsinki.fi all. For this reason, most existing systems have prefabricated sets of exercises, with possible expected answers and prepared feedback. Revita is an online L2 learning system for learners beyond the beginner level, which can be used in the classroom and for self-study (Katinskaia et al., 2017; Katinskaia and Yangarber, 2018; Katinskaia et al., 2018). It covers several languages, most of which are highly inflectional, with rich morphology. In contrast to the pre-fabricated approach, Revita allows the learner to upload arbitrary texts to be used as learning content, and automatically creates exercises based on the chosen content. At practice time, Revita presents the text one paragraph at a time with some words hidden and used as fill-inthe-blank (cloze) exercises. For each hidden word, Revita provides a hint—the base form (lemma) of the word. The learner should insert the inflected form of the lemma, given the context. Continuous assessme"
2021.bea-1.15,P17-1161,0,0.0539205,"Missing"
2021.bea-1.15,W19-4302,0,0.0188749,"returned by the LM, which cannot be used to determine whether the learner’s answer is also correct in the context. When comparing BERT’s predictions for the masked original word and the masked alternative-correct word, we conjecture that the model recognizes an alternative answer as grammatical if its predicted probability is at least as high as the probability of the expected answer. We also applied two masking strategies (one target vs. multiple targets), see accuracy Acccorr in Table 4. 4.2 Supervised Model Architecture Following prior experiments—which show that fine-tuning BERT for NER (Peters et al., 2019) and error detection (Kaneko et al., 2020) gives better performance than using the contextual representation of words from pre-trained BERT—we also fine-tune the pre-trained model. We modified the Huggingface Pytorch implementation of BERT for token classification and the code for the NER task8 (Debut et al., 2019). Hyper-parameters for fine-tuning BERT are the same as for the NER task: maximum number of epochs is 3, maximum input sequence length is 256, dropout rate is 0.1, batch size is 32, Adam optimizer, and the initial learning rate is set to 5E-5. We split the generated dataset into a tr"
2021.bea-1.15,2020.aacl-main.83,0,0.0370988,"Missing"
2021.bea-1.15,D19-1119,0,0.0122416,"es, such as Wikipedia edits (Grundkiewicz and JunczysDowmunt, 2014; Boyd, 2018), or introducing natural and synthetic noise into error-free data (Belinkov and Bisk, 2017; Felice and Yuan, 2014). Natural noise means harvesting naturally occurring errors from the available corpora and creating a look-up table of possible replacements. Using natural noise also tries to imitate the distribution of errors in the available learner corpora. Synthetic noise can be generated by probabilistically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In general, the token- and POS-based pattern method demonstrated stronger results. If enough training data is available, errors can be generated by back-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This data was used as the"
2021.bea-1.15,P08-1021,0,0.139783,"Missing"
2021.bea-1.15,P17-1194,0,0.062326,"Missing"
2021.bea-1.15,C16-1030,0,0.0554484,"Missing"
2021.bea-1.15,W17-5032,0,0.019515,"rated by probabilistically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In general, the token- and POS-based pattern method demonstrated stronger results. If enough training data is available, errors can be generated by back-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This data was used as the test set for all experiments presented below. Then, we present the method for generating ungrammatical data for training. 3.1 Learner Data While students perform exercises using the Revita language-learning platform, it continuously collects3 and automatically annotates ReLCo—the longitudinal Revita Learner Corpus (Katinskaia et al., 2020), where each record includes: Synthetic error generation based on confusion sets extracted from a spellchecker was used b"
2021.bea-1.15,P16-1112,0,0.0602483,"Missing"
2021.bea-1.15,W17-5004,0,0.036308,"Missing"
2021.bea-1.15,2020.tacl-1.54,0,0.0612579,"Missing"
2021.bea-1.15,Q19-1001,0,0.0221085,"ty of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data. In this work, we focus on errors in inflection. Our experiments A. show that pre-trained BERT performs worse at detecting grammatical irregularities for Russian than for English; B. show that fine-tuned BERT yields promising results on assessing correctness in grammatical exercises; and C. establish new GED benchmarks for Russian. To further investigate its performance, we compare fine-tuned BERT with one a state-of-theart model for GED (Bell et al., 2019) on our dataset, and on RULEC-GEC (Rozovskaya and Roth, 2019). We release our manually annotated learner dataset, used for testing, for general use. 1 Introduction Many intelligent tutoring systems (ITS) and computer-aided language learning systems (CALL) generate exercises and try to assess the learner’s answers automatically. Providing feedback to the learner is difficult, due to the critical requirement of very high precision—providing incorrect feedback is much more harmful than no feedback at 1 revita.cs.helsinki.fi all. For this reason, most existing systems have prefabricated sets of exercises, with possible expected answers and prepared feedback"
2021.bea-1.15,2020.bea-1.21,0,0.0707358,"Missing"
2021.bea-1.15,N18-1057,0,0.0230442,"aracterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In general, the token- and POS-based pattern method demonstrated stronger results. If enough training data is available, errors can be generated by back-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This data was used as the test set for all experiments presented below. Then, we present the method for generating ungrammatical data for training. 3.1 Learner Data While students perform exercises using the Revita language-learning platform, it continuously collects3 and automatically annotates ReLCo—the longitudinal Revita Learner Corpus (Katinskaia et al., 2020), where each record includes: Synthetic error generation based on confusion sets extracted from a spellchecker was used by one of the top-scoring systems at the"
2021.bea-1.15,2020.acl-main.310,0,0.270397,"ructions based on syntactic agreement and government. We use about 30 types of chunks, e.g., Prep+Adj+Noun or Noun+Conj+Noun.6 e is produced from a A synthetic sentence X source sentence X = (x1 , xi , ..., xn ) with n words by replacing the i-th word xi by a form from the paradigm of xi . The word is replaced, if: it has a valid morphological analysis; it is present in a frequency dictionary, which was computed from the entire “Taiga” corpus; and it has an inflected POS. Paradigms are generated by pymorphy2 (Korobov, 2015). Using the paradigm as a confusion set is similar to the approach in (Yin et al., 2020). For every xi , we pick a random sample from the uniform distribution. The word xi is replaced, if it does not belong to a chunk and the picked value is above the threshold θp = p(error) = 0.1. The word xi is also replaced, if it belongs to a chunk and the picked value is above the threshold θp,c = p(error, chunk) = 0.04. The thresholds denote a probability of inserting an error, and their values were chosen to reflect the distributions of errors in chunks and single tokens in the learner data. 6 For example, in Russian, as in many languages, prepositions govern nouns in a specific case; adje"
2021.bea-1.15,N19-1014,0,0.0249387,"edits (Grundkiewicz and JunczysDowmunt, 2014; Boyd, 2018), or introducing natural and synthetic noise into error-free data (Belinkov and Bisk, 2017; Felice and Yuan, 2014). Natural noise means harvesting naturally occurring errors from the available corpora and creating a look-up table of possible replacements. Using natural noise also tries to imitate the distribution of errors in the available learner corpora. Synthetic noise can be generated by probabilistically injecting characterlevel or word-level noise into the source sentence, as shown in (Lichtarge et al., 2019; Kiyono et al., 2019; Zhao et al., 2019). dataset (Yannakoudakis et al., 2018). Extensive evaluation showed that the methods are better suited for correcting different types of errors. In general, the token- and POS-based pattern method demonstrated stronger results. If enough training data is available, errors can be generated by back-translation from correct data to data with errors (reverse error correction), which can be modified by additional random noise (Rei et al., 2017; Kasewa et al., 2018; Xie et al., 2018; Kiyono et al., 2019). 3 Data First, we describe our real learner data. This data was used as the test set for all exp"
2021.bsnlp-1.15,doddington-etal-2004-automatic,0,0.292153,"y recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Worth mentioning in this context is Entity Discovery and Linking (EDL) (Ji et al., 2014, 2015), a track of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of documents in multiple languages (English, Chinese, and Spanis"
2021.bsnlp-1.15,huttunen-etal-2002-diversity,1,0.607213,"ukasz Kobyli´nski, 2018, 2020). and a recent shared task on NE Recognition in Russian (Starostin et al., 2016). ing for Slavic Languages, (Piskorski et al., 2017, 2019), which, to the best of our knowledge, are the first attempts at such shared tasks covering multiple Slavic languages. High-quality recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE"
2021.bsnlp-1.15,P16-1060,0,0.0711935,"Missing"
2021.bsnlp-1.15,W19-3709,1,0.734272,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.13,0,0.0284518,"lovenian. The system uses contemporary BERT and RoBERTa multilingual pre-trained models, which include Slovene among other languages. The system was further trained on the SlavNER dataset for the NER task and used the Dedupe method for the Entity Matching task. The best performing models were pre-trained on Slovene. The results also indicate that two-step prediction of NE could be beneficial. The team made their code publicly available. The Priberam Labs system, (Ferreira et al., 2021), focuses on the NER task. It uses three components: a multilingual contextual embedding The TraSpaS system, (Suppa and Jariabka, 2021), tests the assumption that the universal open-source NLP toolkits (such as SpaCy, Stanza or Trankit) could achieve competitive performance on the Multilingual NER task, using large pretrained Transformer-based language models available from HuggingfaceTransformers, which have not been available in previous editions of the Shared Task. The team tests the generalizability of the models to new low-resourced domains, and to languages such as Slovene and Ukrainian. The UWr-VL system, (Rychlikowski et al., 2021), utilizes large collections of unstructured and structured documents for unsupervised t"
2021.bsnlp-1.15,W17-1412,1,0.890203,"jubeši´c, 2014), tools for NE recognition in Slovene (Štajner et al., 2013; Ljubeši´c et al., 2013), a Czech corpus of 11K annotated NEs (Ševˇcíková et al., 2007), NER tools for Czech (Konkol and Konopík, 2013), tools and resources for fine-grained annotation of NEs in the National Corpus of Polish (Waszczuk et al., 2010; Savary and Piskorski, 2011), NER shared tasks for Polish organized under the umbrella of POLEVAL2 evaluation campaigns (Ogrodniczuk and Łukasz Kobyli´nski, 2018, 2020). and a recent shared task on NE Recognition in Russian (Starostin et al., 2016). ing for Slavic Languages, (Piskorski et al., 2017, 2019), which, to the best of our knowledge, are the first attempts at such shared tasks covering multiple Slavic languages. High-quality recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japane"
2021.bsnlp-1.15,W02-2024,0,0.125599,"elines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Worth mentioning in this context is Entity Discovery and Linking (EDL) (Ji et al., 2014, 2015), a track of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of documents in multiple languages (English, Chinese, and Spanish), and to partition the entities into cross-document equivalence classes, by either linking mentions to a knowledge base or directly clustering them. An important difference between EDL and our ta"
2021.bsnlp-1.15,W03-0419,0,0.550493,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.9,0,0.0409736,"aptation algorithm. It also uses other techniques to improve system’s NER performance, such as marking and enrichment of uppercase tokens, prediction of NE boundaries with a multitask approach, prediction of masked tokens, fine-tuning the language model to the domain of the document. Six teams submitted descriptions of their systems as BSNLP Workshop papers. We briefly review these systems here; for complete descriptions, please see the corresponding papers. Two additional teams submitted their results with short descriptions of their systems, which appear in this section. The UL FRI system, (Prelevikj and Zitnik, 2021), generated results for several settings, models and languages, although the team’s main motivation is to develop effective NER tools for Slovenian. The system uses contemporary BERT and RoBERTa multilingual pre-trained models, which include Slovene among other languages. The system was further trained on the SlavNER dataset for the NER task and used the Dedupe method for the Entity Matching task. The best performing models were pre-trained on Slovene. The results also indicate that two-step prediction of NE could be beneficial. The team made their code publicly available. The Priberam Labs sy"
2021.bsnlp-1.15,2021.bsnlp-1.11,0,0.0823823,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.14,0,0.0748319,"Missing"
A00-1039,P93-1022,0,0.00943648,"of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use t h e m to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &quot;company (hire/fire/expel...} person&quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in .our case the contexts are limited to selected patterns, relevant to the scenario. SE.g., &quot; J o h n sleeps&quot;, &quot;John is appointed by C o m p a n y &quot; , &quot;I saw a d o g which sleeps&quot;, &quot;She asked J o h n to buy a car&quot;. 6E.g., &quot; J o h n is appointed by Company&quot;, &quot;John is the p r e s i d e n t of Company&quot;, &quot;I saw a d o g which sleeps&quot;, The d o g which I saw sleeps. 7For example, &quot;She gave us our coffee b l a c k &quot; , &quot;Company appointed John Smith as p r e s i d e n t &quot; . 284 3.4 P a t t e r n Discovery Here we present the results from experiments we conducted on t"
A00-1039,M95-1011,0,0.0299138,"ed here are outside for clarity. of the central 1 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of MUC's highest-level"
A00-1039,J86-3002,1,0.637138,"ould s/he fail to provide an example of a particular class of syntactic/semantic construction, the system has no hope of recovering the corresponding events. Our experience has shown that (1) the process of discovering candidates is highly expensive, and (2) gaps in patterns directly translate into gaps in coverage. How can the system help automate the process of discovering new good candidates? The system should find examples of all common linguistic constructs relevant to a scenario. While there has been prior research on identifying the primary lexical patterns of a sub-language or corpus (Grishman et al., 1986; Riloff, 1996), the task here is more complex, since we are typically not provided in advance with a sub-corpus of relevant passages; these passages must themselves be found as part of the discovery process. The difficulty is that one of the best indications of the relevance of the passages is precisely the presence of these constructs. Because of this circularity, we propose to acquire the constructs and passages in tandem. 2 Solution We outline our procedure for automatic acquisition of patterns; details are elaborated in later sections. The procedure is unsupervised in that it does not req"
A00-1039,J93-1006,0,0.0233874,"acquire the constructs and passages in tandem. 2 Solution We outline our procedure for automatic acquisition of patterns; details are elaborated in later sections. The procedure is unsupervised in that it does not require the training corpus to be manually annotated with events of interest, nor a pro-classified corpus with relevance judgements, nor any feedback or intervention from the user 2. The idea is to combine IR-style document selection with an iterative relaxation process; this is similar to techniques used elsewhere in NLP, and is inspired in large part, if remotely, by the work of (Kay and RSscheisen, 1993) on automatic alignment of sentences and words in a bilingual corpus. There, the reasoning was: sentences that are translations of each 2however, it may be supervised after each iteration, where the user can answer yes/no questions to improve the quality of the results 283 other are good indicators that words they contain are translation pairs; conversely, words that are translation pairs indicate that the sentences which contain them correspond to one another. In our context, we observe that documents that are relevant to the scenario will necessarily contain good patterns; conversely, good p"
A00-1039,M92-1021,0,0.0805225,"t, constituents included here are outside for clarity. of the central 1 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of"
A00-1039,P93-1024,0,0.0163407,"e is reduced to a set of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use t h e m to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &quot;company (hire/fire/expel...} person&quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in .our case the contexts are limited to selected patterns, relevant to the scenario. SE.g., &quot; J o h n sleeps&quot;, &quot;John is appointed by C o m p a n y &quot; , &quot;I saw a d o g which sleeps&quot;, &quot;She asked J o h n to buy a car&quot;. 6E.g., &quot; J o h n is appointed by Company&quot;, &quot;John is the p r e s i d e n t of Company&quot;, &quot;I saw a d o g which sleeps&quot;, The d o g which I saw sleeps. 7For example, &quot;She gave us our coffee b l a c k &quot; , &quot;Company appointed John Smith as p r e s i d e n t &quot; . 284 3.4 P a t t e r n Discovery Here we present the results from experimen"
A00-1039,M95-1006,0,\N,Missing
A00-1039,A97-1011,1,\N,Missing
C00-2136,M95-1011,0,0.0047632,"portability and performance. Preparing good patterns for these systems requires considerable skill, and achieving good coverage requires the analysis of a large amount of text. These problems have been impediments to the wider use of extraction systems. These diÆculties have stimulated research on pattern acquisition. Some of this work has emphasized interactive tools to convert examples to extraction patterns (Yangarber and Grishman, 1997); much of the research has focused on methods for automatically converting a corpus annotated with extraction examples into patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These techniques may reduce the level of system expertise required to develop a new extraction application, but they do not lessen the burden of studying a large corpus in order to nd relevant candidates. The prior work most closely related to our own is that of (Rilo , 1996), who also seeks to build patterns automatically without the need to annotate a corpus with the information to be extracted. However, her work di ers from our own in several important respects. First, her patterns identify phrases that ll individual slots in the template, without specifying how thes"
C00-2136,M95-1014,1,0.40712,"rmally consists of an analysis of the text in terms of general linguistic structures and domain-speci c constructs, followed by a search for the scenariospeci c patterns. It is possible to build these constituent structures through a full syntactic analysis of the text, and the discovery procedure we describe below would be applicable to such an architecture. However, for reasons of speed, coverage, and system robustness, the more common approach at present is to perform a partial syntactic analysis using a cascade of nite-state transducers. This is the approach used by our extraction system (Grishman, 1995; Yangarber and Grishman, 1998). At the heart of our system is a regular expression pattern matcher which is capable of matching a set of regular expressions against a partially-analyzed text and producing additional annotations on the text. This core draws on a set of knowledge bases of varying degrees of domain- and task-speci city. The lexicon includes both a general English dictionary and de nitions of domain and scenario terms. The concept base arranges the domain terms into a semantic hierarchy. The predicate base describes the logical structure of the events to be extracted. The pattern"
C00-2136,M92-1021,0,0.0485974,"limitations on their portability and performance. Preparing good patterns for these systems requires considerable skill, and achieving good coverage requires the analysis of a large amount of text. These problems have been impediments to the wider use of extraction systems. These diÆculties have stimulated research on pattern acquisition. Some of this work has emphasized interactive tools to convert examples to extraction patterns (Yangarber and Grishman, 1997); much of the research has focused on methods for automatically converting a corpus annotated with extraction examples into patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These techniques may reduce the level of system expertise required to develop a new extraction application, but they do not lessen the burden of studying a large corpus in order to nd relevant candidates. The prior work most closely related to our own is that of (Rilo , 1996), who also seeks to build patterns automatically without the need to annotate a corpus with the information to be extracted. However, her work di ers from our own in several important respects. First, her patterns identify phrases that ll individual slots in the template, withou"
C00-2136,A97-1011,1,0.650996,"Missing"
C00-2136,A00-1039,1,0.782333,"e we are typically not provided in advance with a sub-corpus of relevant passages; these passages must themselves be found as part of the discovery procedure. The diÆculty is that one of the best indications of the relevance of the passages is precisely the presence of these constructs. Because of this circularity, we propose to acquire the constructs and passages in tandem. 2 ExDisco: the Discovery Procedure We rst outline ExDisco, our procedure for discovery of extraction patterns; details of some of the steps are presented in the section which follows, and an earlier paper on our approach (Yangarber et al., 2000). ExDisco is an unsupervised procedure: the training corpus does not need to be annotated with the speci c event information to be extracted, or even with information as to which documents in the corpus are relevant to the scenario. The only information the user must provide, as described below, is a small set of seed patterns regarding the scenario. Starting with this seed, the system automatically performs a repeated, automatic expansion of the pattern set. This is analogous to the process of automatic term expansion used in some information retrieval systems, where the terms from the most r"
C00-2136,M98-1011,1,\N,Missing
C00-2136,M95-1006,0,\N,Missing
C02-1154,A97-1029,0,0.0278225,"ies. This particular scenario requires a comprehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Director Dr. Gideon Br"
C02-1154,W98-1118,1,0.895488,"scenario requires a comprehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Director Dr. Gideon Bruckner said no cases of"
C02-1154,W99-0613,0,0.375558,"prehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Director Dr. Gideon Bruckner said no cases of mad cow disease have been f"
C02-1154,W99-0612,0,0.0484906,"Missing"
C02-1154,C96-2157,0,0.0991849,"the left and the right side of an instance in text. Separating the two sides allows the learner to accept weaker rules, and several correction phases compensate in cases of insuÆcient evidence by removing uncertain items, and preventing them from polluting the set of good seeds. Research in automatic terminology acquisition initially focused more on the problem of identi cation and statistical methods for this task, e.g., (Justeson and Katz, 1995), the CValue/NC-Value method, (Frantzi et al., 2000). Separately, the problem of classi cation or clustering is addressed in, e.g., (Ushioda, 1996) (Strzalkowski and Wang, 1996) presents an algorithm for learning universal concepts,&quot; which in principle includes both PNs and generic NPs|a step toward our notion of generalized names. The spotter&quot; proceeds iteratively from a handful of seeds and learns names in a single category. DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus. This allows the rules to use deeper, longer-range dependencies, which are diÆcult to express with surface-level information alone. However, a potential problem with using this approach for our task is that the Penn-Treebank-based p"
C02-1154,C96-2212,0,0.0182785,"separately for the left and the right side of an instance in text. Separating the two sides allows the learner to accept weaker rules, and several correction phases compensate in cases of insuÆcient evidence by removing uncertain items, and preventing them from polluting the set of good seeds. Research in automatic terminology acquisition initially focused more on the problem of identi cation and statistical methods for this task, e.g., (Justeson and Katz, 1995), the CValue/NC-Value method, (Frantzi et al., 2000). Separately, the problem of classi cation or clustering is addressed in, e.g., (Ushioda, 1996) (Strzalkowski and Wang, 1996) presents an algorithm for learning universal concepts,&quot; which in principle includes both PNs and generic NPs|a step toward our notion of generalized names. The spotter&quot; proceeds iteratively from a handful of seeds and learns names in a single category. DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus. This allows the rules to use deeper, longer-range dependencies, which are diÆcult to express with surface-level information alone. However, a potential problem with using this approach for our task is"
C02-1154,C96-1071,0,0.0321827,"al-purpose dictionaries. This particular scenario requires a comprehensive list of disease names. Other requisite classes of names include: biological agents causing disease, such as viruses and bacteria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment. 1.1 Generalized Names Names of these kinds, generalized names (GNs), di er from conventional proper names (PNs) that have been studied extensively in the literature, e.g., as part of the traditional Named Entity (NE) categorization task, which evolved out of the MUC NE evaluation, (Wakao et al., 1996; Bikel et al., 1997; Borthwick et al., 1998; Collins and Singer, 1999). The three mainstream NE kinds are location, person, and organization, and much research has centered on these classical&quot; kinds of proper names. On the other hand, the vast eld of terminology has traditionally dealt with identifying single- and multi-word domain-speci c expressions, for various NLP tasks, and recent years have seen a growing convergence between the two elds. In fact, good identi cation of names of both kinds is essential for IE in general. In IFE-BIO, for example, the text: National Veterinary Services Di"
C02-1154,C00-2136,1,0.441888,"ns, diseases) periodically enter into existence and literature. 3. A typical text contains all the information that is necessary for a human to infer the category. This makes discovering names in text an interesting research problem in its own right. The following section introduces the learning algorithm; Section 3 compares our approach to related prior work; Section 4 presents an evaluation of results; we conclude with a discussion of evaluation and current work, in Section 5. 2 Nomen: The Learning Algorithm Nomen is based on a bootstrapping approach, similar in essence to that employed in (Yangarber et al., 2000).1 The algorithm is trained on a large corpus of medical text, as described in Section 4. 2.1 Pre-processing 2.2 Unsupervised Learning A large text corpus is passed through a zoner, a tokenizer/lemmatizer, and a part-of-speech (POS) tagger. The zoner is a rule-based program to extract textual content from the mailing-list messages, i.e., stripping headers and footers. The tokenizer produces lemmas for the in ected surface forms. The statistical POS tagger is trained on the Wall Street Journal (possibly sub-optimal for texts about infectious disease). Unknown or foreign words are not lemmatized"
C02-1165,O97-1012,0,0.0620196,"ing a quanti ed NP, as in table 2 in paragraph (2). The trigger include (as a nite verb) functions similarly, but can also occur between sentences: [...] the Ugandan Ministry of Health has reported [...] 370 cases and 140 deaths. This gure includes 16 new con rmed cases in Gulu [...] In our training corpus, when these cue words occurred in this context, they consistently indicated an event inclusion relation. 5 Discussion Complexity of a scenario seems to depend of multiple factors. The notion of complexity, however, has not been investigated in great depth. Some research on this was done by (Bagga and Biermann, 1997; Bagga, 1997), classifying scenarios according to diÆculty by counting distances between components&quot; of an event in the text. In this way it attempts to account for variation in performance across the MUC scenarios. Our analysis suggests that the type and amount of inclusion relationships depend on the nature of the topic. In such scenarios as Management Succession and Corporate Acquisitions, an event usually occurs at one speci c point in time. By contrast, the Nature events typically take place across a span of time and space. As the event 	ravels&quot; and evolves, its manifestations are repo"
C02-1165,M95-1014,1,0.789406,"enting template structure. In section 4 we present examples of the linguistic cues to Disaster Date Location VictimDead Damage tornado Sunday night Georgia one person motel Disease Date Location VictimDead VictimSick Ebola since September Uganda 156 people - Table 1: Disaster Event and Disease Event recover the complex event structure, followed by discussion in section 5. 2 Background 2.1 Information Extraction Our IE system has been previously customized for several news topics, as part of the MUC program, such as Terrorist Attacks (MUC, 1991; MUC, 1992) and Management Succession (MUC, 1995; Grishman, 1995). Subsequently to the MUCs, we customized Proteus to extract, among other scenarios, Corporate Mergers and Acquisitions, Natural Disasters and Infectious Disease Outbreaks. We contrasted the Nature scenarios with the earlier MUC scenarios (Huttunen et al., 2002). The 	raditional&quot; template structure is such that all the information about the main event can be presented within a single template. The main events form separate instances, and there are no links between them. Management Succession scenario presents a slightly more complicated template structure, but it is still possible to present"
C02-1165,huttunen-etal-2002-diversity,1,0.641922,"e Nature scenarios, we encountered problems that did not arise in the traditional scenarios of the Message Understanding Conferences (MUCs). This included, in particular, delimiting the scope of a single event and organizing the events into templates. We identify two structural factors that contribute to the complexity of a scenario: rst, the scattering of events in text, and second, inclusion relationships between events. These factors cause diÆculty in representing the facts in an unambiguous way. We proposed that such event relationships can be described with a modular, hierarchical model (Huttunen et al., 2002). The phenomenon of inclusion is widespread in the Nature scenarios, and the types of inclusions are numerous. In this paper we present preliminary results obtained from our corpus analysis, with a classi cation and distribution of inclusion relationships. We discuss the potential for recovery of these inclusions from text with the help of the linguistic cues, of which we show some examples. This paper will argue that a thorough linguistic analysis of the corpus is needed to help recovery of the complex event structure in the text. In the next section we give a brief description of the scenari"
C02-1165,O98-3001,0,\N,Missing
C96-1078,C94-1015,0,\N,Missing
C96-1078,J93-2003,0,\N,Missing
C96-1078,C90-3044,0,\N,Missing
C96-1078,C90-3031,0,\N,Missing
C96-1078,C92-2101,0,\N,Missing
C96-1078,P85-1016,0,\N,Missing
C96-1078,P93-1004,0,\N,Missing
C96-1078,P85-1017,0,\N,Missing
C96-1078,P93-1002,0,\N,Missing
C96-1078,1992.tmi-1.23,1,\N,Missing
C98-2134,C90-3001,0,0.0674849,"Missing"
C98-2134,C94-1015,0,0.161477,"Missing"
C98-2134,C92-2101,0,0.764355,"Missing"
C98-2134,P93-1004,0,0.417268,"Missing"
C98-2134,C96-1078,1,0.927343,"Rules from Dominance-Preserving Alignments Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, Antonio Moreno-Sandoval* New York University 715 Broadway, 7th Floor, NY, NY 10003, USA tUniversidad Autdnoma de Madrid Cantoblanco, 28049-Madrid, SPAIN meyers/roman/grishman/macleod©cs, nyu. edu sandoval©lola.lllf.uam.es 1 Introduction Automatic acquisition of translation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et al., 1992), (Matsumoto et al., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (I{aji et al.. 1992), and (Furuse and Iida. 1994)). This paper I describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this a"
C98-2134,C90-3044,0,0.435501,"lation rules from parallel sentence-Migned text takes a variety of forms. Some machine translation (MT) svstems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et al., 1992), (Matsumoto et al., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structurM as well as lexical correspondences. A syntactically analyzed, aligned cor~ pus may serve ~s an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (I{aji et al.. 1992), and (Furuse and Iida. 1994)). This paper I describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; ~nd (2) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived bv ""cutting up"" the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a transla.tion of the source tree. This technique resembles work on MT using synchr"
E17-1103,P08-1030,0,0.0256019,"ystem. (Yangarber and Steinberger, 2009; Huttunen et al., 2013; Du et al., 2016) It uses patterns and rules to extract NEs; currently the system uses about a thousand patterns, some of which were learned (Yangarber, 2003) and some manually constructed. The system assigns a type to each NE—company, person, product, etc.—but the NE types are not used for grouping to reduce the effect of mistakes in analysis, e.g., when an entity is classified with different types across multiple documents. Rather, we consider clustering to be an earlier step in the overall processing pipeline (Yangarber, 2006). Ji and Grishman (2008) show that performance of an IE system can be improved by using clusters of topically-related documents. In PULS we use grouping to improve NE classification: we assign each entity a type based on the majority within the set of clustered documents. 2 http://www-2.cs.cmu.edu/ ˜TextLearning/datasets.html 3 http://about.reuters.com/ researchandstandards/corpus/ 4 https://catalog.ldc.upenn.edu/ LDC2001T57 Our definition of salience relies on the general nature of news articles. Authors typically mention the main event in the title, in condensed form; then, the main information is elaborated in the"
E17-1103,W13-1204,1,0.679951,"uch more on the keywords. We apply the same agglomerative clustering procedures as in other experiments to these juxtaposed vectors. We experiment with both vector representations, CBOW-st and CBOW-b. The second method, similar to that used in (Kumaran and Allan, 2004), requires that both word distance and NE distance should be sufficiently close—closer than corresponding thresholds. In this case, we cannot use the complete linkage metric since a maximum of distances is not defined if the distance is a pair of numbers. Thus, unlike all Data and annotation scheme From our business news corpus (Pivovarova et al., 2013) we selected one “typical” day for annotation, with a total of 3959 documents.7 We manually annotated all of these documents via a specialized interface, which displays documents pairwise and allows an annotator to make three main decisions: documents can be • Grouped: if their main stories are the same. • Not grouped: if their stories are not the same. • Partially grouped: if their main stories are not the same, but may partially overlap. For example, one article might mention the other’s main story toward the end. The interface provides other helpful options, for example, the annotator can u"
E17-1103,W00-0901,0,0.17251,"ctor representation, then use a metric to compute pairwise similarity between documents—often, cosine similarity. In this procedure, clustering quality crucially depends on document representation. Traditionally, a common way of representing documents for clustering is by a vector of TFIDF weights for each keyword, e.g., (Iglesias et al., 2016; Azzopardi and Staff, 2012; Vadrevu et al., 2011). Steinberger and Pouliquen (2008) use log-likelihood (LL) for weighting keywords rather than TF-IDF. LL statistics can be computed for each word in the corpus, relative to a separate, “reference” corpus (Rayson and Garside, 2000). Staff et al. (2015) claim that for search results, using raw term frequencies outperforms TF-IDF. Recent lines of research use word embeddings (Mikolov et al., 2013b) to represent documents. Sophisticated deep learning algorithms can also be applied to text clustering (Xu et al., 2015), but to date they require labeled training data, while the method proposed in this paper is unsupervised. In contrast to bag-of-words (BOW) schemes, named entities (NEs) can be used as features (Montalvo et al., 2012). In most cases, NEs are also weighted according to TF-IDF (Toda and Kataoka, 2005) or its var"
E17-1103,D07-1043,0,0.0331358,"rs—grouped or ungrouped—among all pairs (Rand, 1971). RI can be adjusted for chance, as described in (Hubert and Arabie, 1985): ARI(Srep ) = RI(Srep ) − E[RI(Schance )] 1 − E[RI(Schance )] (2) where Srep is the clustering strategy based on a document representation rep—rep is one of the representation strategies described in Section 4. The strategy Schance is a random clustering strategy; RI(S) is the RI of applying strategy S to the data; and E is expectation, which is estimated as described in (Hubert and Arabie, 1985). V-Measure is the harmonic mean of homogeneity (H) and completeness (C) (Rosenberg and Hirschberg, 2007). 9 We arrived at this annotation scheme through trial and error, since annotating thousands of documents is a complex and tedious task. This seems to be an effective approach. 10 Overall, the annotation process spanned across two calendar months. 11 Leftmost bar is cut off at 500 to improve readability. H = 1− H(C|K) H(C) C = 1− H(K|C) H(K) V = 2HC H +C where H denotes entropy, K are predicted labels, 1101 f (Srep ) − E[f (Sna¨ıve )] fˆna¨ıve (Srep ) = 1 − E[f (Sna¨ıve )] f (Srep ) − f (Sna¨ıve ) = 1 − f (Sna¨ıve ) (3) Equation 3 adjusts the score for the na¨ıve strategy—which shows how much"
E17-1103,van-erp-etal-2014-discovering,0,0.0577445,"Missing"
E17-1103,W15-1509,0,0.0322795,"s for each keyword, e.g., (Iglesias et al., 2016; Azzopardi and Staff, 2012; Vadrevu et al., 2011). Steinberger and Pouliquen (2008) use log-likelihood (LL) for weighting keywords rather than TF-IDF. LL statistics can be computed for each word in the corpus, relative to a separate, “reference” corpus (Rayson and Garside, 2000). Staff et al. (2015) claim that for search results, using raw term frequencies outperforms TF-IDF. Recent lines of research use word embeddings (Mikolov et al., 2013b) to represent documents. Sophisticated deep learning algorithms can also be applied to text clustering (Xu et al., 2015), but to date they require labeled training data, while the method proposed in this paper is unsupervised. In contrast to bag-of-words (BOW) schemes, named entities (NEs) can be used as features (Montalvo et al., 2012). In most cases, NEs are also weighted according to TF-IDF (Toda and Kataoka, 2005) or its variants (Cheng et al., 2012; 1097 Kiritoshi and Qiang, 2016). Kumaran and Allan (2004) combined three vector representations for a document, namely: all words, NEs, and all words except NEs. This is similar to the series of experiments in this paper; the difference is that Kumaran and Alla"
E17-1103,P03-1044,1,0.359265,"or that are related to a broader phenomenon are not considered the same story. If the same entities engage in two different activities, we consider that as two distinct stories. Therefore, we manually annotated a sample of our corpus, which is more suitable for evaluating our methods. 3 Named Entities and Salience We use a Named Entity Recognition module as part of the PULS news monitoring system. (Yangarber and Steinberger, 2009; Huttunen et al., 2013; Du et al., 2016) It uses patterns and rules to extract NEs; currently the system uses about a thousand patterns, some of which were learned (Yangarber, 2003) and some manually constructed. The system assigns a type to each NE—company, person, product, etc.—but the NE types are not used for grouping to reduce the effect of mistakes in analysis, e.g., when an entity is classified with different types across multiple documents. Rather, we consider clustering to be an earlier step in the overall processing pipeline (Yangarber, 2006). Ji and Grishman (2008) show that performance of an IE system can be improved by using clusters of topically-related documents. In PULS we use grouping to improve NE classification: we assign each entity a type based on th"
H05-1008,W04-0705,0,0.0268539,"-free. The idea is to mine the data base for association rules, and then to integrate these rules into the extraction process. The baseline system is obtained by supervised learning from a few hundred manually annotated examples. Then the IE system is applied to successively larger sets of unlabeled examples, and association rules are mined from the extracted facts. The resulting combined system (trained model plus association rules) showed an improvement in performance on a test set, which correlated with the size of the unlabeled corpus. In work on improving (Chinese) named entity tagging, (Ji and Grishman, 2004; Ji and Grishman, 2005), show benefits to this component from integrating decisions made in later stages, viz. coreference, and relation extraction. 1 Tighter coupling and integration between IE and KDD components for mutual benefit is advocated by (McCallum and Jensen, 2003), which present models based on CRFs and supervised training. This work is related in spirit to the work presented in this paper, in its focus on leveraging crossdocument information that information—though it is inherently noisy—to improve local decisions. We expect that the approach could be quite powerful when these id"
H05-1008,P05-1051,0,0.0155488,"ine the data base for association rules, and then to integrate these rules into the extraction process. The baseline system is obtained by supervised learning from a few hundred manually annotated examples. Then the IE system is applied to successively larger sets of unlabeled examples, and association rules are mined from the extracted facts. The resulting combined system (trained model plus association rules) showed an improvement in performance on a test set, which correlated with the size of the unlabeled corpus. In work on improving (Chinese) named entity tagging, (Ji and Grishman, 2004; Ji and Grishman, 2005), show benefits to this component from integrating decisions made in later stages, viz. coreference, and relation extraction. 1 Tighter coupling and integration between IE and KDD components for mutual benefit is advocated by (McCallum and Jensen, 2003), which present models based on CRFs and supervised training. This work is related in spirit to the work presented in this paper, in its focus on leveraging crossdocument information that information—though it is inherently noisy—to improve local decisions. We expect that the approach could be quite powerful when these ideas are used in combinat"
H05-1008,H05-2012,1,0.80816,"ed facts, and the specific problem under study—i.e., which aspects of these facts we first try to improve. 1 Performance on English named entity tasks reaches mid to high 90’s in many domains. Corpus 1000 We conducted experiments with redundancy-based auto-correction over a large database of facts extracted from the texts in ProMED-Mail, a mailing list which carries reports about outbreaks of infectious epidemics around the world and the efforts to contain them. This domain has been explored earlier; see, e.g., (Grishman et al., 2003) for an overview. Our underlying IE system is described in (Yangarber et al., 2005). The system is a hybrid automatically- and manually-built pattern base for finding facts, an HMM-based name tagger, automatically compiled and manually verified domainspecific ontology, based in part on MeSH, (MeS, 2004), and a rule-based co-reference module, that uses the ontology. The database is live on-line, and is continuously updated with new incoming reports; it can be accessed at doremi.cs.helsinki.fi/plus/. Text reports have been collected by ProMEDMail for over 10 years. The quality of reporting (and editing) has been rising over time, which is easy to observe in the text data. The"
H05-1008,C02-1165,1,\N,Missing
H05-2012,H05-1008,1,0.869073,"Missing"
H05-2012,C02-1154,1,0.821906,"Confidence for individual fields of extracted facts, and for entire facts, is based on document-local and global information. 4 22 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 22–23, Vancouver, October 2005. Knowledge bases: customizer Lexicon Ontology Patterns Inference rules Customization environment Candidate knowledge Noise reduction/ Data correction/ Cross-validation Extracted facts IE engine DB server Unsupervised learning Text documents Data collection Web server User query Response Other corpora publisher user Figure 1: System architecture of ProMED-PLUS acquisition, (Yangarber et al., 2002; Yangarber, 2003) requires a large corpus of domain-specific and general-topic texts. On the other hand, automatic error reduction requires a critical mass of extracted facts. Tighter integration between IE and KDD components, for mutual benefit, is advocated in recent related research, e.g., (Nahm and Mooney, 2000; McCallum and Jensen, 2003). In this system we have demonstrated that redundancy in the extracted data (despite the noise) can be leveraged to improve quality, by analyzing global trends and correcting erroneous fills which are due to local mis-analysis, (Yangarber and Jokipii, 200"
H05-2012,P03-1044,1,0.833827,"al fields of extracted facts, and for entire facts, is based on document-local and global information. 4 22 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 22–23, Vancouver, October 2005. Knowledge bases: customizer Lexicon Ontology Patterns Inference rules Customization environment Candidate knowledge Noise reduction/ Data correction/ Cross-validation Extracted facts IE engine DB server Unsupervised learning Text documents Data collection Web server User query Response Other corpora publisher user Figure 1: System architecture of ProMED-PLUS acquisition, (Yangarber et al., 2002; Yangarber, 2003) requires a large corpus of domain-specific and general-topic texts. On the other hand, automatic error reduction requires a critical mass of extracted facts. Tighter integration between IE and KDD components, for mutual benefit, is advocated in recent related research, e.g., (Nahm and Mooney, 2000; McCallum and Jensen, 2003). In this system we have demonstrated that redundancy in the extracted data (despite the noise) can be leveraged to improve quality, by analyzing global trends and correcting erroneous fills which are due to local mis-analysis, (Yangarber and Jokipii, 2005). For this kind"
huttunen-etal-2002-diversity,M95-1014,1,\N,Missing
huttunen-etal-2002-diversity,O98-3001,0,\N,Missing
huttunen-etal-2002-diversity,X98-1016,1,\N,Missing
huttunen-etal-2002-diversity,O97-1012,0,\N,Missing
K16-1014,W16-1905,1,0.822264,"Missing"
K16-1014,D07-1093,0,0.164083,"Missing"
K16-1014,N09-1008,0,0.366792,"Missing"
K16-1014,D11-1032,0,0.537494,"to build a complete alignment of a language family have not been reported previously, to the best of our knowledge. j k l m n o p r s t u v ä ö ü Finnish Figure 3: 1-1 alignment for Finnish and Hungarian put. We use the principle of recurrent sound correspondence, as in much of the literature. Alignment can be evaluated by measuring relationships among entire languages within the family. Construction of phylogenies is studied, e.g., in (Nakhleh et al., 2005; Ringe et al., 2002; Barbanc¸on et al., 2009). Our work is related to the generative “Berkeley” models, (Bouchard-Cˆot´e et al., 2007), (Hall and Klein, 2011), in the following respects. Context: in (Wettig et al., 2011) we capture some context by coding pairs of symbols, as in (Kondrak, 2003). Berkeley models handle context by conditioning the symbol being generated upon the immediately preceding and following symbols. Our method uses broader context by 3 Coding pairs of words We begin with baseline algorithms for pairwise coding: in (Wettig et al., 2011; Wettig et al., 2012) we code pairs of words, from two related languages in our corpus of cognates. For each word pair, the task of alignment is finding which sym138 Context model bols correspond"
K16-1014,R11-1016,1,0.112342,", the StarLing database, (Starostin, 2005), contains 2586 Uralic cognate sets, based on (R´edei, 1991). The etymological dictionary Suomen Sanojen Alkuper¨a (SSA), “The Origin of Finnish Words,” (Itkonen and Kulonen, 2000), has over 5000 cognate sets. One traditional arrangement of the Uralic languages is shown in Figure 1; several alternative arrangements appear in the literature. The last 15 years have seen a surge in computational modeling of language relationships, change and evolution. We provide a detailed discussion of related prior work in (Nouri et al., 2016). In earlier work, e.g., (Wettig et al., 2011), we presented two perspectives on the problem of finding regularity. It can be seen as a problem of aligning the data. From an information-theoretic perspective, finding regularity is a problem of compression: the more regularity we find in data, the more we can compress it. In (Wettig et al., 2011), we presented baseline models, which focus on alignment of symbols, in a 1-1 fashion. We showed that aligning more than one symbol at a time—e.g., 2-2—gives better performance. Alignment is a natural way to think of comparing languages. E.g., in Figure 2, obtained by the 11 model, we can observe4"
K16-1014,W12-0215,1,0.787802,"died, e.g., in (Nakhleh et al., 2005; Ringe et al., 2002; Barbanc¸on et al., 2009). Our work is related to the generative “Berkeley” models, (Bouchard-Cˆot´e et al., 2007), (Hall and Klein, 2011), in the following respects. Context: in (Wettig et al., 2011) we capture some context by coding pairs of symbols, as in (Kondrak, 2003). Berkeley models handle context by conditioning the symbol being generated upon the immediately preceding and following symbols. Our method uses broader context by 3 Coding pairs of words We begin with baseline algorithms for pairwise coding: in (Wettig et al., 2011; Wettig et al., 2012) we code pairs of words, from two related languages in our corpus of cognates. For each word pair, the task of alignment is finding which sym138 Context model bols correspond best; the task of coding is achieving more compression. The simplest form of symbol alignment is a pair (σ : τ ) ∈ Σ × T , a single symbol σ from the source alphabet Σ with a symbol τ from the target alphabet T . To model insertions and deletions, we augment both alphabets with a special “empty” symbol— denoted by a dot—and write the augmented alphabets as Σ. and T . . We can then align word pairs, such as hiiri—l¨oNk@r ("
K16-1014,C02-1016,0,0.358042,"ö ü Finnish Figure 2: 1-1 alignment for Finnish and Estonian many sounds correspond—they “align with themselves.” However, as languages diverge further, this correspondence becomes blurry; e.g., when we try to align Finnish and Hungarian, the probability distribution of aligned symbols has much higher entropy, Figure 3. The reason is that the regularity lies on a much deeper level: predicting which sound occurs in a given position in a word requires knowledge of a wider context, in both Finnish and Hungarian. Hence we will prefer to think in terms of coding, rather than alignment. Methods in (Kondrak, 2002), learn one-toone sound correspondences between words in pairs of languages. Kondrak (2003), Wettig et al. (2011) find more complex—many-to-many— correspondences. These methods focus on alignment, and model context of the sound changes in a limited way, while it is known that most evolutionary changes are conditioned on the context of the evolving sound. Bouchard-Cˆot´e et al. (2007) use MCMC-based methods to model context, and operate on more than a pair of languages.5 Our models, similarly to other work, operate at the phonetic level only, leaving semantic judgements to the creators of the d"
L16-1495,W10-2211,0,0.0243836,"ning the morphemes into words, e.g., (Koskenniemi, 1983). The morphological analyzer is then used to analyze any word in text, and is an essential low-level component in many tasks in natural language processing (NLP), such as parsing, machine translation, etc. Unsupervised learning of morphology is the inverse problem: to infer the morphology of all words in the language, given only a large number of words—given no dictionaries, rules, etc. A number of algorithms have been proposed and their performance has been compared, e.g., in a series of shared-task competitions, the Morpho-Challenges, (Kurimo et al., 2010). The competitions provided annotated data, and tools for evaluation of performance. The majority of algorithms for unsupervised learning of morphology (that we have reviewed), treat the segmentation of words into morphs either as the ultimate goal or as a first step on the way toward learning the “deeper” aspects of morphology, viz., allomorphy. Allomorphy (when it is treated) is seen by most as a second step, after a segmentation has been obtained. At the same time, some papers observe the difficulty of evaluating segmentations directly(Virpioja et al., 2011); we review some of these problem"
L16-1495,N09-1024,0,0.026626,"ering of word forms and full morphological analysis. Direct evaluation methods are, in general, believed to better reflect the characteristics of the algorithm, while indirect methods complicate the evaluation task, since one needs to minimize the effect of other factors in the larger task (Virpioja et al., 2011). Other evaluation methods, which evaluate more than segmentations, or require more output from the model than only morphological segmentations, are described in (Spiegler and Monson, 2010; Virpioja et al., 2011). A widely used approach (Kurimo et al., 2006; Snyder and Barzilay, 2008; Poon et al., 2009) for evaluating segmentation of words is the boundary precision, recall, and accuracy (BPRA2 ), based on the number of reference boundaries found and missed. If a set of correct segmentation points for the given words could be provided, the segmentation task could be viewed as an information retrieval (IR) task, where the items to be retrieved are the segmentation boundaries. Then the standard IR evaluation measures 2 Most earlier works do not include accuracy as one of the measures, however we will include it in this paper along with precision, recall, and F-score since it is a related and re"
L16-1495,P08-1084,0,0.0349688,", these tasks include clustering of word forms and full morphological analysis. Direct evaluation methods are, in general, believed to better reflect the characteristics of the algorithm, while indirect methods complicate the evaluation task, since one needs to minimize the effect of other factors in the larger task (Virpioja et al., 2011). Other evaluation methods, which evaluate more than segmentations, or require more output from the model than only morphological segmentations, are described in (Spiegler and Monson, 2010; Virpioja et al., 2011). A widely used approach (Kurimo et al., 2006; Snyder and Barzilay, 2008; Poon et al., 2009) for evaluating segmentation of words is the boundary precision, recall, and accuracy (BPRA2 ), based on the number of reference boundaries found and missed. If a set of correct segmentation points for the given words could be provided, the segmentation task could be viewed as an information retrieval (IR) task, where the items to be retrieved are the segmentation boundaries. Then the standard IR evaluation measures 2 Most earlier works do not include accuracy as one of the measures, however we will include it in this paper along with precision, recall, and F-score since it"
L16-1495,C10-1116,0,0.0202104,"introduces several new variations of the existing algorithms. Beyond morphological segmentation, these tasks include clustering of word forms and full morphological analysis. Direct evaluation methods are, in general, believed to better reflect the characteristics of the algorithm, while indirect methods complicate the evaluation task, since one needs to minimize the effect of other factors in the larger task (Virpioja et al., 2011). Other evaluation methods, which evaluate more than segmentations, or require more output from the model than only morphological segmentations, are described in (Spiegler and Monson, 2010; Virpioja et al., 2011). A widely used approach (Kurimo et al., 2006; Snyder and Barzilay, 2008; Poon et al., 2009) for evaluating segmentation of words is the boundary precision, recall, and accuracy (BPRA2 ), based on the number of reference boundaries found and missed. If a set of correct segmentation points for the given words could be provided, the segmentation task could be viewed as an information retrieval (IR) task, where the items to be retrieved are the segmentation boundaries. Then the standard IR evaluation measures 2 Most earlier works do not include accuracy as one of the measu"
L18-1644,W16-0503,0,0.0808767,"ch as: “... в тёмном лесу ...” in dark-Masc.Sg.Loc forest-Masc.Sg.Loc “... in a dark forest ...” where the case, number and gender categories agree. Then this sequence may be offered as an exercise, e.g., 11 By unification over the variables, which are in bold. with a multiple choice for the preposition (with the remaining options filled by distractor prepositions), and “cloze” boxes for the noun and adjective. Various approaches to the problem of generation of reliable distractors are described in (Lee and Luo, 2016; Correia et al., 2010; Rakangor and Ghodasara, 2014; Sakaguchi et al., 2013; Hill and Simha, 2016; Liang et al., 2017). The actual choice of candidates for exercises depends on the user model—the history of answers given previously by the user. The system computes weights (probabilities) for all potential candidates in the story snippet; words or constructions receive a lower weight if they had been answered mostly incorrectly (or mostly correctly) in previous sessions— since that implies that they were too difficult (or too easy) for the learner at present. Candidates with smaller weights appear with a lower probability in the next exercises. Weights are assigned not only to particular w"
L18-1644,W17-0304,1,0.61261,"age learning, a lack of assessment, and a lack of functioning systems. Golonka et al. (2014) published a review of computational methodologies in language learning; the review mentions three evaluations of ITS for language learning (Nagata, 1993; Nagata, 1997; Dodigovic, 2007). Although all three showed that ITS are more effective than traditional tutoring, the author was critical of the evaluation of the new methodologies. 3. The Revita system, situated at the intersection of ICALL and ITS—see Figure 1—attempts to address the requirements of both, and aims to move beyond existing solutions, (Katinskaia et al., 2017). 3.1. Addressing current problems in CALL and ITS We briefly review the desiderata of language-learning systems and the key problems in developing ITSs— the main observed pitfalls. Many of these have been brought to light in prior surveys, e.g., in (Slavuj et al., 2015). For each desideratum or problem we briefly mention how Revita satisfies the requirement, or how we intend to address it in the future.6 1. Over-restricting the learning domain horizontally: Horizontal restriction refers to the tendency to focus on a single linguistic skill. Revita offers a variety of practice modes for exerci"
L18-1644,P16-4020,0,0.0253778,"agreement between the adjective and noun. Using such rules, the system finds constructions, such as: “... в тёмном лесу ...” in dark-Masc.Sg.Loc forest-Masc.Sg.Loc “... in a dark forest ...” where the case, number and gender categories agree. Then this sequence may be offered as an exercise, e.g., 11 By unification over the variables, which are in bold. with a multiple choice for the preposition (with the remaining options filled by distractor prepositions), and “cloze” boxes for the noun and adjective. Various approaches to the problem of generation of reliable distractors are described in (Lee and Luo, 2016; Correia et al., 2010; Rakangor and Ghodasara, 2014; Sakaguchi et al., 2013; Hill and Simha, 2016; Liang et al., 2017). The actual choice of candidates for exercises depends on the user model—the history of answers given previously by the user. The system computes weights (probabilities) for all potential candidates in the story snippet; words or constructions receive a lower weight if they had been answered mostly incorrectly (or mostly correctly) in previous sessions— since that implies that they were too difficult (or too easy) for the learner at present. Candidates with smaller weights ap"
L18-1644,P13-2043,0,0.0290247,"finds constructions, such as: “... в тёмном лесу ...” in dark-Masc.Sg.Loc forest-Masc.Sg.Loc “... in a dark forest ...” where the case, number and gender categories agree. Then this sequence may be offered as an exercise, e.g., 11 By unification over the variables, which are in bold. with a multiple choice for the preposition (with the remaining options filled by distractor prepositions), and “cloze” boxes for the noun and adjective. Various approaches to the problem of generation of reliable distractors are described in (Lee and Luo, 2016; Correia et al., 2010; Rakangor and Ghodasara, 2014; Sakaguchi et al., 2013; Hill and Simha, 2016; Liang et al., 2017). The actual choice of candidates for exercises depends on the user model—the history of answers given previously by the user. The system computes weights (probabilities) for all potential candidates in the story snippet; words or constructions receive a lower weight if they had been answered mostly incorrectly (or mostly correctly) in previous sessions— since that implies that they were too difficult (or too easy) for the learner at present. Candidates with smaller weights appear with a lower probability in the next exercises. Weights are assigned no"
L18-1644,volodina-etal-2014-flexible,0,0.025446,"Missing"
M98-1011,M95-1014,1,0.603041,"Missing"
N18-3016,S17-2141,0,0.0159822,"eld recently as part of SemEval (Cortis et al., 2017), and provided a small dataset containing company names. This dataset contains only 1,000 news headlines, of which only 165 instances mention more than one company name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be"
N18-3016,D17-1070,0,0.021663,"oned in the same document. This corpus is suitable for experiments with entity-oriented polarity, and our experiments explicitly contrast models that take focus as an input against models that do not use the information about the target company’s position. Transfer learning: a.k.a. inductive transfer, is a technique for applying knowledge accumulated from solving one problem to improve the solution for a different problem. We use feature transfer, where the goal is to learn transferable representations for data, which are meaningful for multiple tasks (Pan and Yang, 2010; Bengio et al., 2013; Conneau et al., 2017), i.e., very general, low-level representations. On the other hand, one might consider two related tasks, and try to gain knowledge from one to help with the other. In such cases, one wishes to transfer representations at a much higher level (Glorot et al., 2011). An analysis of the trade-offs between generality and specificity of learned features can be found at (Yosinski et al., 2014). Deep learning with knowledge transfer has been previously applied to sentiment analysis in the context of domain adaptation (Glorot et al., 2011) and cross-lingual applications (Zhou et al., 2016). In our expe"
N18-3016,S17-2089,0,0.0559754,"s of 5,000 sentences published by Takala et al. (2014), most instances (sentences) contain no company name, and hence cannot be used for predicting polarity for specific entities. A dataset of 679 sentences in Dutch, annotated with entity-oriented business sentiment, was published by Van de Kauter et al. (2015). They demonstrate that a. in financial news, not all sentiment expressions within a sentence relate to the target company; b. sentiment is often expressed implicitly. A shared task on fine-grained sentiment analysis of financial microblogs and news was held recently as part of SemEval (Cortis et al., 2017), and provided a small dataset containing company names. This dataset contains only 1,000 news headlines, of which only 165 instances mention more than one company name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et"
N18-3016,C14-1008,0,0.0342106,"ferent task: each document has a set of event labels; some of these may be mapped to polarity labels. We explore two strategies for knowledge transfer: i) manually mapping from event labels to polarity labels, and ii) pre-training CNNs for the event classification task, followed by unsupervised transfer of high-level features from event classification to polarity. We demonstrate that unsupervised transfer improves performance. 2 Related work Sentiment analysis: Deep learning for sentiment analysis is an active area of research. Some methods learn vector representations for entire phrases (Dos Santos and Gatti, 2014; Socher et al., 2011); others learn syntactic tree structures (Tai et al., 2015; Socher et al., 2013). A simpler approach using CNNs (Kim, 2014) has demonstrated state-of-the-art performance (Tai et al., 2015). Interest in applying sentiment mining to the business domain is spurred by important industry applications, such as analyzing the impact of news on financial markets (Ahmad et al., 2016; Van de Kauter et al., 2015; Loughran and McDonald, 2011). If a company frequently appears in news in negative contexts it may affect its reputation, impact its stock price, etc., (Saggion and Funk, 200"
N18-3016,E17-1103,1,0.848765,"ploit this large data to improve polarity prediction. To this end, we attempt two approaches, with several variations: manual mapping and high-level feature transfer. For manual mapping, we manually selected those labels which we believe most clearly imply a polarity: e.g., Investment, Product launch and Sponsorship are considered positive, while Fraud, Layoff and Bankruptcy are negative; in all, we identified 26 “positive” and 12 “negative” labels. Using 131 2 The grouping algorithm takes into account the semantic similarity of the keywords, and the distributions of NEs within the documents (Escoter et al., 2017) A US appeals Court revived a civil suit accusing Apple of creating a monopoly text representation with word embeddings first convolutional layer with multiple filter widths feature maps second convolutional layer with multiple filter widths feature maps focus vector max-pooling fully-connected layer with dropout and softmax or sigmoid output Figure 1: A model architecture with focus vector and two convolution layers only these 38 event labels, we constructed a training set, removing documents with labels that result in no polarity, or conflicting polarities. Further, since it is impossible to"
N18-3016,S17-2154,0,0.0235511,"SemEval (Cortis et al., 2017), and provided a small dataset containing company names. This dataset contains only 1,000 news headlines, of which only 165 instances mention more than one company name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be mentioned multiple t"
N18-3016,S17-2152,0,0.0524057,"Missing"
N18-3016,S17-2150,0,0.0313078,"name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be mentioned multiple times, with many different names mentioned in the same document. This corpus is suitable for experiments with entity-oriented polarity, and our experiments explicitly contrast models that take"
N18-3016,D14-1181,0,0.0268766,"nually mapping from event labels to polarity labels, and ii) pre-training CNNs for the event classification task, followed by unsupervised transfer of high-level features from event classification to polarity. We demonstrate that unsupervised transfer improves performance. 2 Related work Sentiment analysis: Deep learning for sentiment analysis is an active area of research. Some methods learn vector representations for entire phrases (Dos Santos and Gatti, 2014; Socher et al., 2011); others learn syntactic tree structures (Tai et al., 2015; Socher et al., 2013). A simpler approach using CNNs (Kim, 2014) has demonstrated state-of-the-art performance (Tai et al., 2015). Interest in applying sentiment mining to the business domain is spurred by important industry applications, such as analyzing the impact of news on financial markets (Ahmad et al., 2016; Van de Kauter et al., 2015; Loughran and McDonald, 2011). If a company frequently appears in news in negative contexts it may affect its reputation, impact its stock price, etc., (Saggion and Funk, 2009). Although news reports usually have a time lag, events reported in news have longer-term impact on investor sentiment and attitudes toward a g"
N18-3016,S17-2153,0,0.0245285,"l., 2017), and provided a small dataset containing company names. This dataset contains only 1,000 news headlines, of which only 165 instances mention more than one company name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be mentioned multiple times, with many diffe"
N18-3016,S17-2138,0,0.0164094,"aset contains only 1,000 news headlines, of which only 165 instances mention more than one company name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be mentioned multiple times, with many different names mentioned in the same document. This corpus is suitable for exp"
N18-3016,S17-2095,0,0.0174397,"000 news headlines, of which only 165 instances mention more than one company name, of which only 20 instances contain names with different polarities (positive for one company but negative for another). Thus, using entity-oriented methods on this dataset may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be mentioned multiple times, with many different names mentioned in the same document. This corpus is suitable for experiments with entity-ori"
N18-3016,D14-1162,0,0.0827554,"Missing"
N18-3016,S17-2143,1,0.851125,"may not lead to an advantage in performance. Of the ten best-performing systems on the news sentiment task, many used sentence-level classification with no treatment of target company (Rotim et al., 2017; Cabanski et al., 2017; Ghosal et al., 2017; Kumar et al., 2017); others replace the target name with a special token (Mansar et al., 2017; Moore and Rayson, 2017; Jiang et al., 2017) or use company name as a feature (Kar et al., 2017), though none of the papers provide any evidence that special treatment of the target yields a gain in performance. In our experiments with the SemEval dataset (Pivovarova et al., 2017) a model with explicitly specified target worked slightly worse than a baseline. The dataset that we release with this paper is 20 times larger and contains entire documents, where a given entity may be mentioned multiple times, with many different names mentioned in the same document. This corpus is suitable for experiments with entity-oriented polarity, and our experiments explicitly contrast models that take focus as an input against models that do not use the information about the target company’s position. Transfer learning: a.k.a. inductive transfer, is a technique for applying knowledge"
N18-3016,W13-1204,1,0.839472,"ncipal event, whereas later text contains background information which may mention the company, but where the polarity may be different. In case this processing results in identical instances, we remove duplicates, and keep only one copy. The resulting dataset used in the experiments contains 14,172 distinct instances. The distribution of the data among the polarity classes is shown in Table 1. Instances labeled “contradictory” are not used for testing and training at present. The data were split into five folds for cross-validation. We also have a separate, large collection of news articles (Pivovarova et al., 2013), which is annotated for business events—for example, Merger, Contract, Investment, Product launch, Product recall, Fraud, Bankruptcy—291 labels in all. An article may have multiple event labels. Some of these labels may imply (or strongly correlate with) positive or negative polarity. We attempt to exploit this large data to improve polarity prediction. To this end, we attempt two approaches, with several variations: manual mapping and high-level feature transfer. For manual mapping, we manually selected those labels which we believe most clearly imply a polarity: e.g., Investment, Product la"
N18-3016,S17-2148,0,0.043562,"Missing"
N18-3016,D11-1014,0,0.183636,"nt has a set of event labels; some of these may be mapped to polarity labels. We explore two strategies for knowledge transfer: i) manually mapping from event labels to polarity labels, and ii) pre-training CNNs for the event classification task, followed by unsupervised transfer of high-level features from event classification to polarity. We demonstrate that unsupervised transfer improves performance. 2 Related work Sentiment analysis: Deep learning for sentiment analysis is an active area of research. Some methods learn vector representations for entire phrases (Dos Santos and Gatti, 2014; Socher et al., 2011); others learn syntactic tree structures (Tai et al., 2015; Socher et al., 2013). A simpler approach using CNNs (Kim, 2014) has demonstrated state-of-the-art performance (Tai et al., 2015). Interest in applying sentiment mining to the business domain is spurred by important industry applications, such as analyzing the impact of news on financial markets (Ahmad et al., 2016; Van de Kauter et al., 2015; Loughran and McDonald, 2011). If a company frequently appears in news in negative contexts it may affect its reputation, impact its stock price, etc., (Saggion and Funk, 2009). Although news repo"
N18-3016,D13-1170,0,0.00687954,"explore two strategies for knowledge transfer: i) manually mapping from event labels to polarity labels, and ii) pre-training CNNs for the event classification task, followed by unsupervised transfer of high-level features from event classification to polarity. We demonstrate that unsupervised transfer improves performance. 2 Related work Sentiment analysis: Deep learning for sentiment analysis is an active area of research. Some methods learn vector representations for entire phrases (Dos Santos and Gatti, 2014; Socher et al., 2011); others learn syntactic tree structures (Tai et al., 2015; Socher et al., 2013). A simpler approach using CNNs (Kim, 2014) has demonstrated state-of-the-art performance (Tai et al., 2015). Interest in applying sentiment mining to the business domain is spurred by important industry applications, such as analyzing the impact of news on financial markets (Ahmad et al., 2016; Van de Kauter et al., 2015; Loughran and McDonald, 2011). If a company frequently appears in news in negative contexts it may affect its reputation, impact its stock price, etc., (Saggion and Funk, 2009). Although news reports usually have a time lag, events reported in news have longer-term impact on"
N18-3016,P15-1150,0,0.0150342,"olarity labels. We explore two strategies for knowledge transfer: i) manually mapping from event labels to polarity labels, and ii) pre-training CNNs for the event classification task, followed by unsupervised transfer of high-level features from event classification to polarity. We demonstrate that unsupervised transfer improves performance. 2 Related work Sentiment analysis: Deep learning for sentiment analysis is an active area of research. Some methods learn vector representations for entire phrases (Dos Santos and Gatti, 2014; Socher et al., 2011); others learn syntactic tree structures (Tai et al., 2015; Socher et al., 2013). A simpler approach using CNNs (Kim, 2014) has demonstrated state-of-the-art performance (Tai et al., 2015). Interest in applying sentiment mining to the business domain is spurred by important industry applications, such as analyzing the impact of news on financial markets (Ahmad et al., 2016; Van de Kauter et al., 2015; Loughran and McDonald, 2011). If a company frequently appears in news in negative contexts it may affect its reputation, impact its stock price, etc., (Saggion and Funk, 2009). Although news reports usually have a time lag, events reported in news have"
N18-3016,takala-etal-2014-gold,0,0.0300198,"g the impact of news on financial markets (Ahmad et al., 2016; Van de Kauter et al., 2015; Loughran and McDonald, 2011). If a company frequently appears in news in negative contexts it may affect its reputation, impact its stock price, etc., (Saggion and Funk, 2009). Although news reports usually have a time lag, events reported in news have longer-term impact on investor sentiment and attitudes toward a given company (Boudoukh et al., 2013). A major difficulty in training entity-oriented polarity models is the lack of publicly available datasets. In the corpus of 5,000 sentences published by Takala et al. (2014), most instances (sentences) contain no company name, and hence cannot be used for predicting polarity for specific entities. A dataset of 679 sentences in Dutch, annotated with entity-oriented business sentiment, was published by Van de Kauter et al. (2015). They demonstrate that a. in financial news, not all sentiment expressions within a sentence relate to the target company; b. sentiment is often expressed implicitly. A shared task on fine-grained sentiment analysis of financial microblogs and news was held recently as part of SemEval (Cortis et al., 2017), and provided a small dataset con"
P03-1044,W99-0613,0,0.790814,"evant subsets to the scenario. (Yangarber et al., 2000) attempts to find extraction patterns, without a pre-classified corpus, starting from a set of seed patterns. This is the basic unsupervised learner on which our approach is founded; it is described in the next section. We first present the basic algorithm for pattern acquisition, similar to that presented in (Yangarber et al., 2000). Section 3.3 places the algorithm in the framework of counter-training. vocabulary (OOV) classes of items: dates, times, numeric and monetary expressions. Name classification is a well-studied subject, e.g., (Collins and Singer, 1999). The name recognizer we use is based on lists of common name markers—such as personal titles (Dr., Ms.) and corporate designators (Ltd., GmbH)—and hand-crafted rules. Parsing: After name classification, we apply a general English parser, from Conexor Oy, (Tapanainen and J¨arvinen, 1997). The parser recognizes the name tags generated in the preceding step, and treats them as atomic. The parser’s output is a set of syntactic dependency trees for each document. Syntactic Normalization: To reduce variation in the corpus further, we apply a tree-transforming program to the parse trees. For every ("
P03-1044,C96-2157,0,0.0178996,"knowledge for NL understanding, in particular in the context of IE. A typical architecture for an IE system includes knowledge bases (KBs), which must be customized when the system is ported to new domains. The KBs cover different levels, viz. a lexicon, a semantic conceptual hierarchy, a set of patterns, a set of inference rules, a set of logical representations for objects in the domain. Each KB can be expected to be domain-specific, to a greater or lesser degree. Among the research that deals with automatic acquisition of knowledge from text, the following are particularly relevant to us. (Strzalkowski and Wang, 1996) proposed a method for learning concepts belonging to a given semantic class. (Riloff and Jones, 1999; Riloff, 1996; Yangarber et al., 2000) present different combinations of learners of patterns and concept classes specifically for IE. In (Riloff, 1996) the system AutoSlog-TS learns patterns for filling an individual slot in an event template, while simultaneously acquiring a set of lexical elements/concepts eligible to fill the slot. AutoSlogTS, does not require a pre-annotated corpus, but does require one that has been split into subsets that are relevant vs. non-relevant subsets to the sce"
P03-1044,A97-1011,0,0.0259256,"Missing"
P03-1044,C00-2136,1,0.816666,"rpus, the result is a ranked list of patterns, and going down the list produces a curve which trades off precision for recall. Simply put, the unsupervised algorithm does not know when to stop learning. In the absence of a good stopping criterion, the resulting list of patterns must be manually reviewed by a human; otherwise one can set ad-hoc thresholds, e.g., on the number of allowed iterations, as in (Riloff and Jones, 1999), or else to resort to supervised training to determine such thresholds—which is unsatisfactory when our 1 As described in, e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000). goal from the outset is to try to limit supervision. Thus, the lack of natural stopping criteria renders these algorithms less unsupervised than one would hope. More importantly, this lack makes the algorithms difficult to use in settings where training must be completely automatic, such as in a generalpurpose information extraction system, where the topic may not be known in advance. At the same time, certain unsupervised learning algorithms in other domains exhibit inherently natural stopping criteria. One example is the algorithm for word sense disambiguation in (Yarowsky, 1995). Of parti"
P03-1044,C02-1154,1,0.272489,"upervised than one would hope. More importantly, this lack makes the algorithms difficult to use in settings where training must be completely automatic, such as in a generalpurpose information extraction system, where the topic may not be known in advance. At the same time, certain unsupervised learning algorithms in other domains exhibit inherently natural stopping criteria. One example is the algorithm for word sense disambiguation in (Yarowsky, 1995). Of particular relevance to our method are the algorithms for semantic classification of names or NPs described in (Thelen and Riloff, 2002; Yangarber et al., 2002). Inspired in part by these algorithms, we introduce the counter-training technique for unsupervised pattern acquisition. The main idea behind countertraining is that several identical simple learners run simultaneously to compete with one another in different domains. This yields an improvement in precision, and most crucially, it provides a natural indication to the learner when to stop learning—namely, once it attempts to wander into territory already claimed by other learners. We review the main features of the underlying unsupervised pattern learner and related work in Section 2. In Secti"
P03-1044,P95-1026,0,0.131255,"; Yangarber et al., 2000). goal from the outset is to try to limit supervision. Thus, the lack of natural stopping criteria renders these algorithms less unsupervised than one would hope. More importantly, this lack makes the algorithms difficult to use in settings where training must be completely automatic, such as in a generalpurpose information extraction system, where the topic may not be known in advance. At the same time, certain unsupervised learning algorithms in other domains exhibit inherently natural stopping criteria. One example is the algorithm for word sense disambiguation in (Yarowsky, 1995). Of particular relevance to our method are the algorithms for semantic classification of names or NPs described in (Thelen and Riloff, 2002; Yangarber et al., 2002). Inspired in part by these algorithms, we introduce the counter-training technique for unsupervised pattern acquisition. The main idea behind countertraining is that several identical simple learners run simultaneously to compete with one another in different domains. This yields an improvement in precision, and most crucially, it provides a natural indication to the learner when to stop learning—namely, once it attempts to wander"
P03-1044,W02-1028,0,\N,Missing
P98-2139,C90-3001,0,0.0745159,"Missing"
P98-2139,C94-1015,0,0.151793,"Missing"
P98-2139,C92-2101,0,0.758007,"Missing"
P98-2139,P93-1004,0,0.398705,"Missing"
P98-2139,C96-1078,1,0.924922,"erving Alignments Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, Antonio Moreno-Sandoval t New York U n i v e r s i t y 715 Broadway, 7th Floor, NY, NY 10003, USA tUniversidad A u t 6 n o m a de M a d r i d Cantoblanco, 28049-Madrid, SPAIN meyers/roman/grishman/macleod©cs, nyu. edu sandoval©lola, lllf. uam. es 1 Introduction Automatic acquisition of translation rules from parallel sentence-aligned text takes a variety of forms. Some machine translation (MT) systems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et hi., 1992), (Matsumoto et hi., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structural as well as lexical correspondences. A syntactically analyzed, aligned corpus may serve as an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (l(aji et al., 1992), and (Furuse and Iida. 1994)). This paper 1 describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; and (9) a procedure for deriving transfer rules from this al"
P98-2139,C90-3044,0,0.413629,"lation rules from parallel sentence-aligned text takes a variety of forms. Some machine translation (MT) systems treat aligned sentences as unstructured word sequences. Other systems, including our own ((Grishman, 1994) and (Meyers et al., 1996)), syntactically analyze sentences (parse) before acquiring transfer rules (cf. (Kaji et hi., 1992), (Matsumoto et hi., 1993), and (Kitamura and Matsumoto, 1995)). This has the advantage of acquiring structural as well as lexical correspondences. A syntactically analyzed, aligned corpus may serve as an example base for a form of example-based NIT (cf. (Sato and Nagao, 1990), (l(aji et al., 1992), and (Furuse and Iida. 1994)). This paper 1 describes: (1) an efficient algorithm for aligning a pair of source/target language parse trees; and (9) a procedure for deriving transfer rules from this alignment. Each transfer rule consists of a pair of tree fragments derived by &quot;cutting up&quot; the source and target trees. A set of transfer rules whose left-hand sides match a source language parse tree is used to generate a target language parse tree from their set of right-hand sides, which is a translation of the source tree. This technique resembles work on NIT using synchr"
R11-1016,C02-1016,0,0.44982,"nd in the daughter languages derive, or synchronic—i.e., of word forms that are missing from existing languages. Several approaches to etymological alignment have emerged over the last decade. The problem of discovering cognates is addressed, e.g., in, e.g., (BouchardCˆot´e et al., 2007; Kondrak, 2004; Kessler, 2001). In our work, we do not attempt to find cognate sets, but begin with given sets of etymological data for a language family, possibly different or even conflicting. We use the principle of recurrent sound correspondence, as in much of the literature, including the mentioned work, (Kondrak, 2002; Kondrak, 2003) and others. Modeling relationships within the language family arises in the process of evaluation of our alignment models. Phylogenetic reconstruction is studied extensively by, e.g.,(Nakhleh et al., 2005; Ringe et al., 2002; Barbancon et al., 2009); these work differ from ours in that they operate on pre-compiled sets of “characters”, capturing divergent features of entire languages within the family, whereas we operate at the level of words or cognate sets. Other related work is further mentioned in the body of the paper. We describe our datasets in the next section, present"
R11-1016,D07-1093,0,0.269029,"Missing"
R11-1016,W11-4634,1,0.177974,"a sets, in terms of their internal consistency. One of our main goals is to devise automatic methods for aligning the data that are as objective as possible, the models make no a priori assumptions—e.g., no preference for vowel-vowel or consonant-consonant alignments. We present a baseline model and several successive improvements, using data from the Uralic language family. 1 Introduction We present work on induction of alignment rules for etymological data, in a project that studies genetic relationships among the Uralic language family. This is a continuation of previous work, reported in (Wettig and Yangarber, 2011), where the methods were introduced. In this paper, we extend the models reported earlier and give a more comprehensive evaluation of results. In addition to the attempt to induce alignment rules, we aim to derive measures of quality of data sets in terms of their internal consistency. More consistent dataset should receive a higher score in the evaluations. Currently our goal is to analyze given, existing etymological datasets, rather than to construct cognate sets from raw linguistic data. The question to be answered is whether a complete description of the correspondence rules can be discov"
S17-2143,S16-1002,0,0.0678981,"Missing"
S17-2143,P15-1150,0,0.0341194,"Missing"
S17-2143,Q16-1019,0,0.0295876,"Missing"
S17-2143,D14-1181,0,0.00977442,"Missing"
S17-2143,D14-1162,0,0.0812249,", and a soft-max output layer. The output is a 2-dimensional vector that is interpreted as probability distributions over two possible outcomes: positive and negative. Thus, if an instance has a sentiment score -1 it is mapped into [1, 0], a score of 1 is mapped into [0,1]. A cross-entropy loss function is computed between the network’s output and the true value to update the network weights via back-propagation. Next, we briefly describe the details of the components of the model. Embeddings: Words are represented by 128dimensional embeddings. The initial embeddings were trained using GloVe (Pennington et al., 2014) on a corpus of 5 million business news articles. P rox(p) = 1 1 + |p − m| where p is the position of the current word and m is the position of the nearest mention of the target company. Thus, proximity is 1 for a company mention, 1/2 for its immediate neighbours, 1/3 for the next neighbours, etc. It is never 0, which allows a convolution filter to use information about focus points, even if it exceeds the filter length. 843 A US appeals Court revived a civil suit accusing Apple of creating a monopoly text representation with word embeddings first convolutional layer with multiple filter width"
W10-1105,N04-4028,0,0.1229,"Missing"
W10-1105,I08-2140,0,0.0168626,"sites may carry true information. This measure of quality is beyond the scope of this paper. 3 The System: Background PULS, the Pattern-based Understanding and Learning System, is developed at the University of Helsinki to extract factual information from plain text. PULS has been adapted to analyse texts for Epidemic Surveillance.1 The components of PULS have been described in detail previously, (Yangarber and Steinberger, 2009; Steinberger et al., 2008; Yangarber et al., 2007). In several respects, it is similar to other existing systems for automated epidemic surveillance, viz., BioCaster (Doan et al., 2008), MedISys and PULS (Yangarber and Steinberger, 2009), HealthMap (Freifeld et al., 2008), and others (Linge et al., 2009). PULS relies on EC-JRC’s MedISys for IR (information retrieval)—MedISys performs a broad Web search, using a set of boolean keyword-based queries, (Steinberger et al., 2008). The result is a continuous stream of potentially relevant documents, updated every few minutes. Second, an IE component, (Grishman et al., 2003; Yangarber and Steinberger, 2009), analyzes each retrieved document, to try to ﬁnd events of potential relevance to Public Health. The system stores the structu"
W10-1105,C02-1165,1,\N,Missing
W10-4003,2002.jeptalnrecital-long.4,0,0.0476365,"Missing"
W11-4616,O97-1012,0,0.0796755,"a news article. We distinguish discourse features and lexical features. Discourse features are based on properties of the article text and of the events extracted from it. Lexical features are simpler low-level features based on bags of words, discussed in section 3.2. In essence, lexical features capture local information, while discourse features capture longer-range relationships within the document. 3.1 Discourse Features Discourse features include information about the number of events, positioning of the event in the document, the compactness of the placement of the event’s attributes (Bagga and Biermann, 1997; Huttunen et al., 2002), and the recency of event occurrence. 3.1.1 Layout and positioning We introduce a set of features describing the position of the trigger sentence within the document. These help to quantify the assumption that important details of news topics are placed in the beginning of an article whereas less important details are stated later.4 Layout features include the length of the document and the position of the trigger sentence in the document. Figure 1 shows the distribution of the relative location of the event in the text, given that the event has a high relevance score"
W11-4616,N04-4028,0,0.0288839,"y of the fields in each record were correctly extracted by comparing the system’s answers to a set of answers pre-defined by human annotators. In the MUC and ACE initiatives, e.g., this was computed mainly in terms of recall and precision, and F-measure, (Hirschman, 1998; ACE, 2004). We would like to distinguish objective vs. subjective measures of quality. Objective measures take the perspective of the system in evaluating the obtained IE results in terms of correctness and confidence. Confidence has been studied to estimate the probability of the correctness of the system’s answer, in e.g. (Culotta and McCallum, 2004). Our IE system, PULS, computes confidence using discourse-level cues, (Steinberger et al., 2008), such as: confidence decreases as the distance between the sentence containing the event and event attributes increases; confidence increases if a document mentions only one country. Subjective measures reflect the end users’ perspective, that is the relevance (or utility) of the extracted information, and the reliability1 of the information found (von Etter et al., 2010) . Utility measures how useful the result is to the user irrespective of its correctness. An event may be correctly extracted, a"
W11-4616,W10-1105,1,0.88186,"Missing"
W11-4634,D07-1093,0,0.340371,"Missing"
W11-4634,C02-1016,0,0.469344,"respects from SSA. StarLing has under 2000 Uralic cognate sets, compared with over 5000 in SSA, and does not explicitly indicate dubious etymologies. However, Uralic data in StarLing is more evenly distributed, because it is not Finnish-centric like SSA is—cognate sets in StarLing are not required to contain a member from Finnish. StarLing also gives a reconstructed form for each cogset, which may be useful for testing algorithms that perform reconstruction. We are experimenting with the Uralic data by implementing algorithms modeling various etymological processes. A method due to Kondrak, (Kondrak, 2002) learns one-to-one regular sound correspondences between pairs of related languages in the data. The method in (Kondrak, 2003) finds attested complex (one-to-many) correspondences. These models are somewhat simplistic in that they operate only on one language pair at a time, and do not model the contexts of the sound changes, while we know that most etymological changes are conditioned on context. Our implementation of (Bouchard-Cˆot´e et al., 2007) found correspondence rules with correct contexts, using more than two languages. However, we found that this model’s running time did not scale if"
W12-0215,W09-1804,0,0.166188,"ed onto alignment in etymology. The intuition is that translation sentences in MT correspond to cognate words in etymology, while words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000); one might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Of the MT-related models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL (the Minimum Description Length Principle, introduced below). 2 Using this method, we found that the running time did not scale well for more than three languages. 109 3 Aligning Pairs of Words We begin with pairwise alignment: aligning pairs of words, from two related languages in our corpus of cognates. For each word pair, the task of alignment means finding exactly which symbols correspond. Some symbols may align with “themselves” (i.e., with similar or identical sounds), while others may have undergone changes during the time when the two rela"
W12-0215,D07-1093,0,0.405779,"Missing"
W12-0215,J93-2003,0,0.0434324,"Missing"
W12-0215,C02-1016,0,0.862135,"Missing"
W12-0215,W97-0311,0,0.242131,"raw, complete data. There is extensive work on alignment in the machine-translation (MT) community, and it has been observed that methods from MT alignment may be projected onto alignment in etymology. The intuition is that translation sentences in MT correspond to cognate words in etymology, while words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000); one might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Of the MT-related models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL (the Minimum Description Length Principle, introduced below). 2 Using this method, we found that the running time did not scale well for more than three languages. 109 3 Aligning Pairs of Words We begin with pairwise alignment: aligning pairs of words, from two related languages in our corpus of cognates. For each word pair, the task of alignment means finding exactly whi"
W12-0215,J00-2004,0,0.107358,"ata. There is extensive work on alignment in the machine-translation (MT) community, and it has been observed that methods from MT alignment may be projected onto alignment in etymology. The intuition is that translation sentences in MT correspond to cognate words in etymology, while words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000); one might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Of the MT-related models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL (the Minimum Description Length Principle, introduced below). 2 Using this method, we found that the running time did not scale well for more than three languages. 109 3 Aligning Pairs of Words We begin with pairwise alignment: aligning pairs of words, from two related languages in our corpus of cognates. For each word pair, the task of alignment means finding exactly which symbols corre"
W12-0215,C96-2141,0,0.516276,"ed that methods from MT alignment may be projected onto alignment in etymology. The intuition is that translation sentences in MT correspond to cognate words in etymology, while words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000); one might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Of the MT-related models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL (the Minimum Description Length Principle, introduced below). 2 Using this method, we found that the running time did not scale well for more than three languages. 109 3 Aligning Pairs of Words We begin with pairwise alignment: aligning pairs of words, from two related languages in our corpus of cognates. For each word pair, the task of alignment means finding exactly which symbols correspond. Some symbols may align with “themselves” (i.e., with similar or identical sounds), while others may have"
W12-0215,R11-1016,1,0.687417,"n and Kulonen, 2000), has over 5000 cognate sets, (about half of which are only in languages from the Balto-Finnic branch, closest to Finnish). Most importantly, for our models, SSA gives “dictionary” word-forms, which may contain extraneous morphological material, whereas StarLing data is mostly stemmed. One traditional arrangement of the Uralic languages1 is shown in Figure 1. We model etymological processes using these Uralic datasets. The methods in (Kondrak, 2002) learn regular one-to-one sound correspondences between pairs of related languages in the data. The methods in (Kondrak, 2003; Wettig et al., 2011) find more complex (one-to-many) correspondences. These models operate on one language pair at a time; also, they do not model the context of the sound changes, while most etymological changes are conditioned on context. The MCMC-based model proposed in (Bouchard-Cˆot´e et al., 2007) explicitly aims to model the context of changes, and op1 Adapted from Encyclopedia Britannica and (Anttila, 1989) We should note that our models at present operate at the phonetic level only, they leave semantic judgements of the database creators unquestioned. While other work, e.g. (Kondrak, 2004), has attempted"
W13-1011,J90-1003,0,0.216635,"s, extracted from a deeply annotated and carefully disambiguated (partly manually) sub-corpus of the Russian National Corpus (RNC). The size of disambiguated corpus used in this paper is 5 944 188 words of running text. 2 Related Work Much effort has been invested in automatic extraction of MWEs from text. A great variety of method are used, depending on the data, the particular tasks and the types of MWEs to be extracted. Pecina (2005) surveys 87 statistical measures and methods, and even that is not a complete list. The most frequently used metrics, inter alia, are Mutual Information (MI), (Church and Hanks, 1990), tscore (Church et al., 1991), and log-likelihood (Dunning, 1993). The common disadvantage of these is their dependency on the number of words included in the MWE. Although there is a large number of papers that use MI for bigram extraction, only a few use the MI measure for three or more collocates, ˇ e.g., (Tadi´c and Sojat, 2003; Wermter and Hahn, 2006; Kilgarriff et al., 2012), Frantzi et al. (2000) introduced the c-value and nc-value measures to extract terms of different lengths. Daudaravicius (2010) has developed a promising method that recognizes collocations in text. Rather than extr"
W13-1011,R11-1103,0,0.0257498,"son (2007) applied a set of statistical measures to classify verb+noun MWEs and used Kullback-Leibler divergence, among other methods, to measure the syntactic cohesion of a word combination. Van de Cruys and Moir´on (2007) used normalized Kullback-Leibler divergence to find idiomatic expression with verbs in Dutch. Russian MWE-studies have emerged over the last decade. Khokhlova and Zakharov (2009) applied MI, t-score and log-likelihood to extract verb collocations; Yagunova and Pivovarova (2010) studied the difference between Russian lemma/token collocations and also between various genres; Dobrov and Loukachevitch (2011) implemented term extraction algorithms. However, there is a lack of study of both colligations and collostructions in Russian. The only work known to us is by Sharoff (2004), who applied the MI-score to extract prepositional phrases; however, the only category he used was the POS. As far as we aware, the algorithm we present in this paper has not been applied to Russian or to other languages. 3 Method The input for our system is any n-gram of length 2– 4, where one position is a gap—the algorithm aims This is due to the fact that the preposition governs the case of the noun, but has no effect"
W13-1011,J93-1003,0,0.0520588,"nually) sub-corpus of the Russian National Corpus (RNC). The size of disambiguated corpus used in this paper is 5 944 188 words of running text. 2 Related Work Much effort has been invested in automatic extraction of MWEs from text. A great variety of method are used, depending on the data, the particular tasks and the types of MWEs to be extracted. Pecina (2005) surveys 87 statistical measures and methods, and even that is not a complete list. The most frequently used metrics, inter alia, are Mutual Information (MI), (Church and Hanks, 1990), tscore (Church et al., 1991), and log-likelihood (Dunning, 1993). The common disadvantage of these is their dependency on the number of words included in the MWE. Although there is a large number of papers that use MI for bigram extraction, only a few use the MI measure for three or more collocates, ˇ e.g., (Tadi´c and Sojat, 2003; Wermter and Hahn, 2006; Kilgarriff et al., 2012), Frantzi et al. (2000) introduced the c-value and nc-value measures to extract terms of different lengths. Daudaravicius (2010) has developed a promising method that recognizes collocations in text. Rather than extracting MWEs, this method cuts the text into a sequence of MWEs of"
W13-1011,W07-1102,0,0.0167033,"istic measures have been used for MWE extraction since the earliest work. For example, the main idea in (Shimohata et al., 1997; Resnik, 1997), is that the MWE’s idiosyncrasy, (Sag et al., 2002), is reflected in the distributions of the collocates. Ramisch et al. (2008) introduced the Entropy of Permutation and Insertion: EP I = − m X p(ngrama ) log[p(ngrama )] (1) a=0 where ngram0 is the original MWE, and ngrama are its syntactically acceptable permutations. Kullback-Leibler divergence was proposed by Resnik (1997) to measure selective preference for the word sense disambiguation (WSD) task. Fazly and Stevenson (2007) applied a set of statistical measures to classify verb+noun MWEs and used Kullback-Leibler divergence, among other methods, to measure the syntactic cohesion of a word combination. Van de Cruys and Moir´on (2007) used normalized Kullback-Leibler divergence to find idiomatic expression with verbs in Dutch. Russian MWE-studies have emerged over the last decade. Khokhlova and Zakharov (2009) applied MI, t-score and log-likelihood to extract verb collocations; Yagunova and Pivovarova (2010) studied the difference between Russian lemma/token collocations and also between various genres; Dobrov and"
W13-1011,huttunen-etal-2002-diversity,1,0.721162,"od that is able to determine patterns of frequently co-occurring lexical and grammatical features within a corpus can have farreaching practical implications. One particular application that we are exploring is the fine-tuning of semantic patterns that are commonly used in information extraction (IE), (Grishman, 2003). Our work on IE focuses on different domains and different languages, (Yangarber et al., 2007; Atkinson et al., 2011). Analysis of MWEs that occur in extraction patterns would provide valuable insights into how the patterns depend on the particular style or genre of the corpus, (Huttunen et al., 2002). Subtle, genre-specific differences in expression can indicate whether a given piece of text is signaling the presence an event of interest. 5.3 Creating Teaching-Support Tools Instructors teaching a foreign language are regularly asked how words co-occur: What cases and 80 word forms appear after a given preposition? Which ones should I learn by rote and which ones follow rules? The persistence of such questions indicates that this is an important challenge to be addressed— we should aim to build a system that can automatically generate an integrated answer. A tool that produces answers to t"
W13-1011,W10-3713,0,0.0648639,"rked word order that is admissible in this colloquial construction in Russian: “через часа два” (lit.: after hours two = idiom: after about two hours), where 78 Figure 4: Distributions of cases in the corpus and in a sample. (Arrows indicate syntactic dependency.) the preposition governs the Case of the numeral, and the numeral governs a noun that precedes it. Because our algorithm at the moment processes linear sequences, these kinds of syntactic inversion phenomena in Russian will pose a challenge. In general this problem can be solved by using tree-banks for MWE extraction, (Seretan, 2008; Martens and Vandeghinste, 2010). However, an appropriate treebank is not always available for a given language; in fact, we do not have access to any Russian tree-bank suitable for this task. 23: “с” (with) This is a genuine error. The algorithm misses two of four correct cases, Genitive and Accusative, because both are widely used across the corpus, which reduces their frequency ratio in the sub-sample. Our further work will focus on finding flexible frequency ratio thresholds, which is now set to one. Two of the correct cases (Instrumental and Gen2) are well over the threshold, while Genitive, with 0.6924, and Accusative,"
W13-1011,P05-2003,0,0.0199494,"language that is well-investigated. A good number of corpora and reference grammars are available to be used for evaluation. The data we use in this work is the n-gram corpus, extracted from a deeply annotated and carefully disambiguated (partly manually) sub-corpus of the Russian National Corpus (RNC). The size of disambiguated corpus used in this paper is 5 944 188 words of running text. 2 Related Work Much effort has been invested in automatic extraction of MWEs from text. A great variety of method are used, depending on the data, the particular tasks and the types of MWEs to be extracted. Pecina (2005) surveys 87 statistical measures and methods, and even that is not a complete list. The most frequently used metrics, inter alia, are Mutual Information (MI), (Church and Hanks, 1990), tscore (Church et al., 1991), and log-likelihood (Dunning, 1993). The common disadvantage of these is their dependency on the number of words included in the MWE. Although there is a large number of papers that use MI for bigram extraction, only a few use the MI measure for three or more collocates, ˇ e.g., (Tadi´c and Sojat, 2003; Wermter and Hahn, 2006; Kilgarriff et al., 2012), Frantzi et al. (2000) introduce"
W13-1011,W97-0209,0,0.104565,"specification, which aims to create an unified cross-language annotation scheme, defines 156 morphosyntactic tags for Russian as compared to 80 tags for English (http://nl.ijs.si/ME/V4/msd/html). 74 chunking for the same segment of text within different corpora. Nevertheless, extraction of variablelength MWE is a challenging task; the majority of papers in the field still use measures that take the number of collocates as a core parameter. Entropy and other probabilistic measures have been used for MWE extraction since the earliest work. For example, the main idea in (Shimohata et al., 1997; Resnik, 1997), is that the MWE’s idiosyncrasy, (Sag et al., 2002), is reflected in the distributions of the collocates. Ramisch et al. (2008) introduced the Entropy of Permutation and Insertion: EP I = − m X p(ngrama ) log[p(ngrama )] (1) a=0 where ngram0 is the original MWE, and ngrama are its syntactically acceptable permutations. Kullback-Leibler divergence was proposed by Resnik (1997) to measure selective preference for the word sense disambiguation (WSD) task. Fazly and Stevenson (2007) applied a set of statistical measures to classify verb+noun MWEs and used Kullback-Leibler divergence, among other"
W13-1011,W04-0403,0,0.0250144,"Van de Cruys and Moir´on (2007) used normalized Kullback-Leibler divergence to find idiomatic expression with verbs in Dutch. Russian MWE-studies have emerged over the last decade. Khokhlova and Zakharov (2009) applied MI, t-score and log-likelihood to extract verb collocations; Yagunova and Pivovarova (2010) studied the difference between Russian lemma/token collocations and also between various genres; Dobrov and Loukachevitch (2011) implemented term extraction algorithms. However, there is a lack of study of both colligations and collostructions in Russian. The only work known to us is by Sharoff (2004), who applied the MI-score to extract prepositional phrases; however, the only category he used was the POS. As far as we aware, the algorithm we present in this paper has not been applied to Russian or to other languages. 3 Method The input for our system is any n-gram of length 2– 4, where one position is a gap—the algorithm aims This is due to the fact that the preposition governs the case of the noun, but has no effect on gender. To measure this difference between the distributions we use the Kullback-Leibler divergence: Div(C) = N X Pipattern × log( i=1 Figure 1: Distributions of noun cas"
W13-1011,P97-1061,0,0.0495029,"ent 1 The Multitext-East specification, which aims to create an unified cross-language annotation scheme, defines 156 morphosyntactic tags for Russian as compared to 80 tags for English (http://nl.ijs.si/ME/V4/msd/html). 74 chunking for the same segment of text within different corpora. Nevertheless, extraction of variablelength MWE is a challenging task; the majority of papers in the field still use measures that take the number of collocates as a core parameter. Entropy and other probabilistic measures have been used for MWE extraction since the earliest work. For example, the main idea in (Shimohata et al., 1997; Resnik, 1997), is that the MWE’s idiosyncrasy, (Sag et al., 2002), is reflected in the distributions of the collocates. Ramisch et al. (2008) introduced the Entropy of Permutation and Insertion: EP I = − m X p(ngrama ) log[p(ngrama )] (1) a=0 where ngram0 is the original MWE, and ngrama are its syntactically acceptable permutations. Kullback-Leibler divergence was proposed by Resnik (1997) to measure selective preference for the word sense disambiguation (WSD) task. Fazly and Stevenson (2007) applied a set of statistical measures to classify verb+noun MWEs and used Kullback-Leibler divergenc"
W13-1011,P06-1099,0,0.102881,"e data, the particular tasks and the types of MWEs to be extracted. Pecina (2005) surveys 87 statistical measures and methods, and even that is not a complete list. The most frequently used metrics, inter alia, are Mutual Information (MI), (Church and Hanks, 1990), tscore (Church et al., 1991), and log-likelihood (Dunning, 1993). The common disadvantage of these is their dependency on the number of words included in the MWE. Although there is a large number of papers that use MI for bigram extraction, only a few use the MI measure for three or more collocates, ˇ e.g., (Tadi´c and Sojat, 2003; Wermter and Hahn, 2006; Kilgarriff et al., 2012), Frantzi et al. (2000) introduced the c-value and nc-value measures to extract terms of different lengths. Daudaravicius (2010) has developed a promising method that recognizes collocations in text. Rather than extracting MWEs, this method cuts the text into a sequence of MWEs of length from 1 to 7 words; the algorithm may produce different 1 The Multitext-East specification, which aims to create an unified cross-language annotation scheme, defines 156 morphosyntactic tags for Russian as compared to 80 tags for English (http://nl.ijs.si/ME/V4/msd/html). 74 chunking f"
W13-1204,C96-1079,0,0.079171,"eceives in-depth attention in current research. Events may have various relationships to realworld facts, and different sources may have contradictory views on the facts, (Saur´ı and Pustejovsky, 2012). Similarly to many other linguistic units, an event is a combination of meaning and form; the structure and content of an event is influenced by both the structure of the corresponding real-world fact and by the properties of the surrounding text. We use the notion of scenario to denote a set of structured events of interest in a real-world domain: e.g., the MUC Management Succession scenario, (Grishman and Sundheim, 1996), within the broader Business domain. The representation and the structure of events in text depends on the scenario. For example, Huttunen et al. (2002a; Huttunen et al. (2002b) points out that “classic” MUC scenarios, such as Management Succession or Terrorist Attacks, describe events that occur in a specific point in time, whereas other scenarios like Natural Disaster or Disease Outbreak describe processes that are spread out across time and space. As a consequence, events in the latter, “nature”-related scenarios are more complex, may have a hierarchical structure, and may overlap with eac"
W13-1204,E12-1029,0,0.0207926,"mantic class LAUNCH-PRODUCT. The subsequent analysis of the frequency lists can help improve the performance of the IE system by suggesting refinements to the ontology and the lexicon, as well as patterns and inference rules appropriate for the particular genre of the corpus. Our current work includes the adaptation of the IE system developed for the analyst reports to the general news corpus devoted to the same topics. We also plan to develop a hybrid methodology, to combine the presented corpus-driven analysis with opendomain techniques for pattern acquisition, (Chambers and Jurafsky, 2011; Huang and Riloff, 2012). The approach outlined here for analyzing the distributions of features in documents is useful for studying events within the context of a corpus. It demonstrates that event structure depends on the text genre, and that genre differences can be easily captured and measured. By analyzing document statistics and the output of the pattern-mining, we can demonstrate significant differences between the genres of analyst reports and general news, such as: sentence length, distribution of the domain vocabulary in the text, selectional preference in domain-specific verbs, word co-occurrences, usage o"
W13-1204,huttunen-etal-2002-diversity,1,0.594207,"the facts, (Saur´ı and Pustejovsky, 2012). Similarly to many other linguistic units, an event is a combination of meaning and form; the structure and content of an event is influenced by both the structure of the corresponding real-world fact and by the properties of the surrounding text. We use the notion of scenario to denote a set of structured events of interest in a real-world domain: e.g., the MUC Management Succession scenario, (Grishman and Sundheim, 1996), within the broader Business domain. The representation and the structure of events in text depends on the scenario. For example, Huttunen et al. (2002a; Huttunen et al. (2002b) points out that “classic” MUC scenarios, such as Management Succession or Terrorist Attacks, describe events that occur in a specific point in time, whereas other scenarios like Natural Disaster or Disease Outbreak describe processes that are spread out across time and space. As a consequence, events in the latter, “nature”-related scenarios are more complex, may have a hierarchical structure, and may overlap with each other in text. Linguistic cues that have been proposed in Huttunen et al. (2002a) to identify the overlapping or partial events include specific lexic"
W13-1204,P08-1030,0,0.125199,"Missing"
W13-1204,C94-2174,0,0.499076,"Missing"
W13-1204,P97-1005,0,0.316701,"Missing"
W13-1204,A97-1042,0,0.313882,"Missing"
W13-1204,J11-2004,0,0.0367705,"ions and verbs of belief and reporting.” It is clear that such sophisticated linguistic analysis increases the importance of text style and genre for Information Extraction. 29 Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 29–37, c Atlanta, Georgia, 14 June 2013. 2013 Association for Computational Linguistics The idea of statistical comparison between text types goes back at least as far as (Biber, 1991). It was subsequently used in a number of papers on automatic text categorization (Kessler et al., 1997; Stamatatos et al., 2000; Petrenz and Webber, 2011). Szarvas et al. (2012) studied the linguistic cues indicating uncertainty of events in three genres: news, scientific papers and Wikipedia articles. They demonstrate significant differences in lexical usage across the genres; for example, such words as fear or worry may appear relatively often in news and Wikipedia, but almost never in scientific text. They also investigate differences in syntactic cues: for example, the relation between a proposition and a real-word fact is more likely to be expressed in the passive voice in scientific papers (it is expected), whereas in news the same words"
W13-1204,W97-0313,0,0.147135,"he target events in text. 31 In the pattern-mining mode we use the general pattern SUBJECT–VERB–OBJECT, where the components may have any semantic type and are constrained only by their deep syntactic function— the system attempts to normalize many syntactic variants of the basic, active form: including passive clauses, relative clauses, etc.2 The idea of using very simple, local patterns for obtaining information from large corpora in the context of event extraction is similar to work reported previously, e.g., the bootstrapping approaches in (Thelen and Riloff, 2002; Yangarber et al., 2000; Riloff and Shepherd, 1997). Here, we do not use iterative learning, and focus instead on collecting and analyzing interesting statistics from a large number of S-V-O patterns. We collected all such “generalized” S-V-O triplets from the corpus and stored them in a database. In addition to the noun groups, we save the head nouns and their semantic classes. This makes it easy to use simple SQL queries to count instances of a particular pattern, e.g., all objects of a particular verb, or all actions that can be applied to an object of semantic class “PRODUCT.” For each triplet the database stores a pointer the original sen"
W13-1204,J12-2002,0,0.0655666,"Missing"
W13-1204,C00-2117,0,0.068607,"sequence and causal relations and verbs of belief and reporting.” It is clear that such sophisticated linguistic analysis increases the importance of text style and genre for Information Extraction. 29 Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 29–37, c Atlanta, Georgia, 14 June 2013. 2013 Association for Computational Linguistics The idea of statistical comparison between text types goes back at least as far as (Biber, 1991). It was subsequently used in a number of papers on automatic text categorization (Kessler et al., 1997; Stamatatos et al., 2000; Petrenz and Webber, 2011). Szarvas et al. (2012) studied the linguistic cues indicating uncertainty of events in three genres: news, scientific papers and Wikipedia articles. They demonstrate significant differences in lexical usage across the genres; for example, such words as fear or worry may appear relatively often in news and Wikipedia, but almost never in scientific text. They also investigate differences in syntactic cues: for example, the relation between a proposition and a real-word fact is more likely to be expressed in the passive voice in scientific papers (it is expected), wher"
W13-1204,J12-2004,0,0.0847279,"Missing"
W13-1204,W02-1028,0,0.0146387,"in all IE scenarios are responsible for finding the target events in text. 31 In the pattern-mining mode we use the general pattern SUBJECT–VERB–OBJECT, where the components may have any semantic type and are constrained only by their deep syntactic function— the system attempts to normalize many syntactic variants of the basic, active form: including passive clauses, relative clauses, etc.2 The idea of using very simple, local patterns for obtaining information from large corpora in the context of event extraction is similar to work reported previously, e.g., the bootstrapping approaches in (Thelen and Riloff, 2002; Yangarber et al., 2000; Riloff and Shepherd, 1997). Here, we do not use iterative learning, and focus instead on collecting and analyzing interesting statistics from a large number of S-V-O patterns. We collected all such “generalized” S-V-O triplets from the corpus and stored them in a database. In addition to the noun groups, we save the head nouns and their semantic classes. This makes it easy to use simple SQL queries to count instances of a particular pattern, e.g., all objects of a particular verb, or all actions that can be applied to an object of semantic class “PRODUCT.” For each tr"
W13-1204,C00-2136,1,0.708964,"esponsible for finding the target events in text. 31 In the pattern-mining mode we use the general pattern SUBJECT–VERB–OBJECT, where the components may have any semantic type and are constrained only by their deep syntactic function— the system attempts to normalize many syntactic variants of the basic, active form: including passive clauses, relative clauses, etc.2 The idea of using very simple, local patterns for obtaining information from large corpora in the context of event extraction is similar to work reported previously, e.g., the bootstrapping approaches in (Thelen and Riloff, 2002; Yangarber et al., 2000; Riloff and Shepherd, 1997). Here, we do not use iterative learning, and focus instead on collecting and analyzing interesting statistics from a large number of S-V-O patterns. We collected all such “generalized” S-V-O triplets from the corpus and stored them in a database. In addition to the noun groups, we save the head nouns and their semantic classes. This makes it easy to use simple SQL queries to count instances of a particular pattern, e.g., all objects of a particular verb, or all actions that can be applied to an object of semantic class “PRODUCT.” For each triplet the database store"
W13-1204,C02-1165,1,\N,Missing
W13-1204,W12-3011,0,\N,Missing
W13-2415,W09-2209,0,0.0630889,"Missing"
W13-2415,W06-0204,0,0.012566,"by one simple pattern: Patterns are language-dependent and domaindependent, which means that patterns must capture the lexical, syntactic and stylistic features of the analyzed text. It was not possible to directly translate or map the English pattern base into Russian for at least two reasons. The first reason is technical. PULS’s English pattern base has over 150 patterns for the epidemics domain, and over 300 patterns for security.9 These patterns were added to the system through an elaborate pattern-acquisition process, where semi-supervised pattern acquisition for English text was used, (Greenwood and Stevenson, 2006; Yangarber et al., 2000), to bootstrap many pattern candidates from raw text based on a small set of seed patterns; the candidates were subsequently checked manually and included in the system. Many of these patterns are typically in “base-form”, i.e., simple active clauses; the English system takes each active-clause, “subjectverb-object” pattern, and generalizes it to multiple syntactic variants, including passive clauses, relative clauses, etc. Thus we created the Russian domain-specific patterns directly in PULS’s pattern-specification language. A pattern consists of a regular expression"
W13-2415,huttunen-etal-2002-diversity,1,0.580048,"ssian Text Lidia Pivovarova,1,2 Mian Du,1 Roman Yangarber1 1 Department of Computer Science University of Helsinki, Finland 2 St. Petersburg State University, Russia Abstract security. The epidemics scenario is built to provide an early warning system for professionals and organizations responsible for tracking epidemic threats around the world. Because information related to outbreaks of infectious disease often appears in news earlier than it does in official sources, text mining from the Web for medical surveillance is a popular research topic, as discussed in, e.g., (Collier et al., 2008; Huttunen et al., 2002; Rortais et al., 2010; Zamite et al., 2010). Similarly, in the security scenario, the system tracks cross-border crime, including illegal migration, smuggling, human trafficking, as well as general criminal activity and crisis events; text mining for this scenario has been previously reported by (Ameyugo et al., 2012; Atkinson et al., 2011). The new component monitors open-source media in Russian, searching for incidents related to the given scenarios. It extracts information from plain, natural-language text into structured database records, which are used by domain specialists for daily eve"
W13-2415,C00-2136,1,0.660475,"are language-dependent and domaindependent, which means that patterns must capture the lexical, syntactic and stylistic features of the analyzed text. It was not possible to directly translate or map the English pattern base into Russian for at least two reasons. The first reason is technical. PULS’s English pattern base has over 150 patterns for the epidemics domain, and over 300 patterns for security.9 These patterns were added to the system through an elaborate pattern-acquisition process, where semi-supervised pattern acquisition for English text was used, (Greenwood and Stevenson, 2006; Yangarber et al., 2000), to bootstrap many pattern candidates from raw text based on a small set of seed patterns; the candidates were subsequently checked manually and included in the system. Many of these patterns are typically in “base-form”, i.e., simple active clauses; the English system takes each active-clause, “subjectverb-object” pattern, and generalizes it to multiple syntactic variants, including passive clauses, relative clauses, etc. Thus we created the Russian domain-specific patterns directly in PULS’s pattern-specification language. A pattern consists of a regular expression trigger and action code."
W13-2415,P06-1103,0,0.0236569,"point in time, whereas other scenarios, such as Natural Disasters or Disease Outbreaks describe a process that is spread out in time and space. Consequently, events in the latter (“nature”) scenarios are more complex, may have hierarchical structure, and may even overlap in text. From the theoretical point of view it would be interesting to compare how the events representation, (Pivovarova et al., 2013), differs in different languages. Moreover, such differences can be important in cross-language information summarization, (Ji et al., 2013). We use a freely-available comparable news corpus, (Klementiev and Roth, 2006), to investigate the difference of event representation in English and Russian. The corpus contains 2327 BBC messages from the time period from 1 January 2001 to 10 May 2005, and their approximate translations from the Lenta.ru website; the translations may be quite different from their English sources and are stylistically similar to standard Russian news. We processed the corpora with the security and epidemics IE systems, using the respective language; the results are presented in the Table 6. The table shows that for both scenarios the English system finds more events than the Russian, whi"
W13-2415,H05-1008,1,0.813655,"Missing"
W13-2415,R11-1029,0,0.0988693,"uage text into structured database records, which are used by domain specialists for daily event monitoring. The structure of the database records (called templates) depends on the scenario. For the epidemics scenario the system extracts the fields: disease name, location of the incident, date, number of victims, etc. In the security domain, the template contains the type of event, date and location, the perpetrator, number of victims (if any), goods smuggled, etc. Monolinguality is a serious limitation for IE, since end-users are under growing pressure to cover news from multiple languages, (Piskorski et al., 2011). The Russian-language component that we describe here is an experiment in extending PULS to multi-lingual coverage. Our aim is to explore whether a such an extension can be built with limited effort and resources. This paper describes a plug-in component to extend the PULS information extraction framework to analyze Russian-language text. PULS is a comprehensive framework for information extraction (IE) that is used for analysis of news in several scenarios from English-language text and is primarily monolingual. Although monolinguality is recognized as a serious limitation, building an IE sy"
W13-2415,P95-1026,0,0.244141,"several relations, among which the “is-a” relation is the most common. One key factor that enables the addition of new languages efficiently is that the ontology is language-independent. The system uses the lexicons to map words into concepts. A lexicon consists of word-forms and some common multi-word expressions (MWEs), which appear in text and represent some ontology concept. We assume that within a given domain each word or 2 http://news.yandex.ru/people http://dictum.ru/en/object-extraction/blog 4 http://www.rco.ru/eng/product.asp 3 101 MWE in the lexicon represents exactly one concept, (Yarowsky, 1995). A concept may be represented by more than one word or MWE.5 Each scenario has its own scenario-specific ontology and lexicons; the Epidemics ontology consists of more than 4000 concepts (which includes some disease names). Diseases are organized in a hierarchy, e.g., “hepatitis” is a parent term for “hepatitis A”. The Security ontology consists of 1190 concepts. The domain-specific lexicon is a collection of terms that are significant for a particular scenario, mapped to their semantic types/concepts. The Security and Epidemics scenarios use a common location lexicon, that contains approxima"
W13-2415,W13-1204,1,0.839635,"002), event representation in text may have different structure depending on the scenario: the “classic” IE scenarios, such as the MUC Management Succession or Terror Attacks, describe events that occur at a specific point in time, whereas other scenarios, such as Natural Disasters or Disease Outbreaks describe a process that is spread out in time and space. Consequently, events in the latter (“nature”) scenarios are more complex, may have hierarchical structure, and may even overlap in text. From the theoretical point of view it would be interesting to compare how the events representation, (Pivovarova et al., 2013), differs in different languages. Moreover, such differences can be important in cross-language information summarization, (Ji et al., 2013). We use a freely-available comparable news corpus, (Klementiev and Roth, 2006), to investigate the difference of event representation in English and Russian. The corpus contains 2327 BBC messages from the time period from 1 January 2001 to 10 May 2005, and their approximate translations from the Lenta.ru website; the translations may be quite different from their English sources and are stylistically similar to standard Russian news. We processed the corp"
W13-5209,W13-2413,0,0.034412,"Missing"
W13-5209,N13-1121,0,0.057951,"Missing"
W13-5209,D11-1141,0,0.0982391,"Missing"
W13-5209,P03-1044,1,0.538635,"them. 3 Figure 2: A news text and a “New Product” event, extracted from this document by IE system. nominations, etc. In this paper we focus on “New Product” events, i.e., when a company launches a new product or service on the market. Figure 2 presents an example of a piece of text from a news article and an event structure extracted from this text. A product event describes a company name, a product name, a location, a date, and the industry sector to which the event is related. These slots are filled by a combination of rule-based and supervised-learning approaches (Grishman et al., 2002; Yangarber, 2003; Huttunen et al., 2013). For identifying the industry sectors to which the events relate, we use a classification system, currently containing 40 broad sectors, e.g., “Electronics,” “Food,” or “Transport.” This classification system is similar to existing classification standards, such as the Global Industry Classification System (GICS),2 , or the Industry Classification Benchmark (ICB, http://www.icbenchmark.com/), with some simplifying modifications. The sector is assigned to the event using a Naive-Bayes classifier, which is trained on a manually-labeled set of news articles, approximately"
W13-5209,N13-1097,0,0.0693393,"Missing"
W14-4207,W12-0215,1,0.918949,"unded common language-independent phonetic space to be used for comparing word forms across languages. Instead, they focus on inferring the distances by comparing how meanings in language A are likely to be confused for each other, and comparing it to the confusion probabilities in language B. Given a lexicon containing mappings from a set of meanings M to a set of forms F , confusion 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our data set— the “corpus”—it builds a complete alignment for all symbols (Wettig et al., 2011). The basic model considers pairwise alignments only, i.e., two languages at a time; we call them the source and the target languages. Later models relax this restriction by using N-dimensional alignment, with N &gt; 2 languages aligned simultaneously. The basic model allows only 1-1 symbol alignments: one source symbol8 may correspond to one target symbol—or t"
W14-4207,W09-0304,0,0.0743867,"Missing"
W14-4207,W11-4634,1,0.847504,"nce measures Ellison and Kirby (2006) present a distance measure based on comparing intra-language lexica only, arguing that there is no well-founded common language-independent phonetic space to be used for comparing word forms across languages. Instead, they focus on inferring the distances by comparing how meanings in language A are likely to be confused for each other, and comparing it to the confusion probabilities in language B. Given a lexicon containing mappings from a set of meanings M to a set of forms F , confusion 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our data set— the “corpus”—it builds a complete alignment for all symbols (Wettig et al., 2011). The basic model considers pairwise alignments only, i.e., two languages at a time; we call them the source and the target languages. Later models relax this restriction by using N-dimensional alignment, with N &gt; 2 languag"
W14-4207,R11-1016,1,0.878292,"by (2006) present a distance measure based on comparing intra-language lexica only, arguing that there is no well-founded common language-independent phonetic space to be used for comparing word forms across languages. Instead, they focus on inferring the distances by comparing how meanings in language A are likely to be confused for each other, and comparing it to the confusion probabilities in language B. Given a lexicon containing mappings from a set of meanings M to a set of forms F , confusion 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our data set— the “corpus”—it builds a complete alignment for all symbols (Wettig et al., 2011). The basic model considers pairwise alignments only, i.e., two languages at a time; we call them the source and the target languages. Later models relax this restriction by using N-dimensional alignment, with N &gt; 2 languages aligned simultaneou"
W14-4207,P06-1035,0,\N,Missing
W14-4207,D07-1093,0,\N,Missing
W15-5307,W13-1011,1,0.926513,"cent constructional grammar approach, where the language is considered as a constructicon (Goldberg, 2006), i.e., an inventory of constructions or patterns that predefine both the grammatical and the lexical selectional preferences of words. Distinguishing collocations, i.e., “cooccurrences of words” from colligations, i.e., “cooccurrence of word forms with grammatical pheIntroduction We present a system that automatically extracts selectional preferences from a corpus. For a given word, the system finds its selectional preferences, both lexical and grammatical, using algorithms described in (Kopotev et al., 2013; Kormacheva et al., 2014). The system1 is developed as a part of CoCoCo Project: Collocations, Colligations, and Corpora. The system has two important features. First, it allows users to identify selectional preferences, based on a large underlying corpus on-line, in real time, rather than relying on pre-computed lists of multi-word expressions (MWEs). Second, it treats MWEs of various kinds—idioms, multiword lexemes, collocations and colligations—in a uniform fashion, returning MWEs of all these types in response to a given query. These features make the system useful for studying a wide var"
W16-1905,W09-1804,0,0.023064,"ymology. The intuition is that sentences that are translation of each other in MT correspond to cognate words in etymology, and words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000). One might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Among the MTrelated models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL, the Minimum Description Length principle. There are important differences between our alignment problem vs. alignment in MT. Evolutionary sound correspondence is conditioned by local context, whereas in MT correspondences may depend on much wider context. There is no analogue to the notion of phonetic features in MT. Phonetic correspondences in etymological data—which apply throughout the language—have no analogue in semantic shift processes in a such way as to be captured by MT alignment models. Neither are phonetic features used in the aforemen"
W16-1905,D07-1093,0,0.379888,"Missing"
W16-1905,W97-0311,0,0.13932,"Additional prior work related to the populationgenetics models is referenced throughout the paper and in Section 6. methods from MT alignment projected onto alignment in etymology. The intuition is that sentences that are translation of each other in MT correspond to cognate words in etymology, and words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000). One might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Among the MTrelated models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL, the Minimum Description Length principle. There are important differences between our alignment problem vs. alignment in MT. Evolutionary sound correspondence is conditioned by local context, whereas in MT correspondences may depend on much wider context. There is no analogue to the notion of phonetic features in MT. Phonetic correspondences in etymological data—which"
W16-1905,J00-2004,0,0.00895654,"r work related to the populationgenetics models is referenced throughout the paper and in Section 6. methods from MT alignment projected onto alignment in etymology. The intuition is that sentences that are translation of each other in MT correspond to cognate words in etymology, and words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000). One might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Among the MTrelated models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL, the Minimum Description Length principle. There are important differences between our alignment problem vs. alignment in MT. Evolutionary sound correspondence is conditioned by local context, whereas in MT correspondences may depend on much wider context. There is no analogue to the notion of phonetic features in MT. Phonetic correspondences in etymological data—which apply throughout"
W16-1905,N09-1008,0,0.170978,"Missing"
W16-1905,J93-2003,0,0.112355,"Missing"
W16-1905,K16-1014,1,0.770508,"In the context of population genetics, such an approach is introduced in (Sir´en et al., 2011; Sir´en et al., 1 The creators of the input dataset posit that the elements of a cognate set derive from a common origin—a word in the ancestral proto-language. 27 Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning, pages 27–37, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics ships, change and evolution. We have been developing a family of models for this task, called the Etymon models, (Wettig et al., 2011; Wettig et al., 2012; Nouri and Yangarber, 2016), etc.2 Methods introduced in (Kondrak, 2002), inspired by alignment in machine translation, learn one-to-one sound correspondences between words in pairs of languages. Kondrak (2003), and Wettig et al. (2011) find more complex—many-tomany—sound correspondences. These methods focus on alignment. They model the context of the sound changes in a limited way, while it is known that most evolutionary changes are conditioned on the context of the evolving sound. Bouchard-Cˆot´e et al. (2007) propose MCMC-based methods to model context, and operate on more than one pair of languages at a time.3 The"
W16-1905,D11-1032,0,0.524245,"cs model E. Distances F. Trees Figure 1: Outline of the components in the inference pipeline recognize “natural classes” in defining the context of a sound change, though not in generating the symbols themselves; (Bouchard-Cˆot´e et al., 2009) encode as a prior which sounds are “close” to each other. In (Wettig et al., 2012) and later Etymon models, we code each sound in terms of the individual phonetic features that make up the sound. Etymon models are based on the informationtheoretic MDL principle, e.g., (Gr¨unwald, 2007)—like (Wettig et al., 2011) and unlike (Bouchard-Cˆot´e et al., 2007; Hall and Klein, 2011). MDL brings important theoretical benefits, since models chosen in this way are guided by data with no free parameters or hand-picked “priors.” The data analyst chooses the model class and structure, and the coding scheme, i.e., a decodable way to encode both model and data. This determines the learning strategy—we optimize the cost function, which is the code length determined by these choices. Objective function: For the objective function to optimize during alignment, we use the prequential code-length (Dawid, 1984), as explained in (Wettig et al., 2011). Normalized Maximum Likelihood (NML"
W16-1905,C02-1016,0,0.903921,"is introduced in (Sir´en et al., 2011; Sir´en et al., 1 The creators of the input dataset posit that the elements of a cognate set derive from a common origin—a word in the ancestral proto-language. 27 Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning, pages 27–37, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics ships, change and evolution. We have been developing a family of models for this task, called the Etymon models, (Wettig et al., 2011; Wettig et al., 2012; Nouri and Yangarber, 2016), etc.2 Methods introduced in (Kondrak, 2002), inspired by alignment in machine translation, learn one-to-one sound correspondences between words in pairs of languages. Kondrak (2003), and Wettig et al. (2011) find more complex—many-tomany—sound correspondences. These methods focus on alignment. They model the context of the sound changes in a limited way, while it is known that most evolutionary changes are conditioned on the context of the evolving sound. Bouchard-Cˆot´e et al. (2007) propose MCMC-based methods to model context, and operate on more than one pair of languages at a time.3 The Etymon models, similarly to other work, opera"
W16-1905,C96-2141,0,0.590271,"s from MT alignment projected onto alignment in etymology. The intuition is that sentences that are translation of each other in MT correspond to cognate words in etymology, and words in MT correspond to sounds in etymology. The notion of regularity of sound change in etymology, which is what our models try to capture, is loosely similar to contextually conditioned correspondence of translation words across languages. For example, (Kondrak, 2002) employs MT alignment from (Melamed, 1997; Melamed, 2000). One might employ the IBM models for MT alignment, (Brown et al., 1993), or the HMM model, (Vogel et al., 1996). Among the MTrelated models, (Bodrumlu et al., 2009) is similar to ours in that it is based on MDL, the Minimum Description Length principle. There are important differences between our alignment problem vs. alignment in MT. Evolutionary sound correspondence is conditioned by local context, whereas in MT correspondences may depend on much wider context. There is no analogue to the notion of phonetic features in MT. Phonetic correspondences in etymological data—which apply throughout the language—have no analogue in semantic shift processes in a such way as to be captured by MT alignment model"
W16-1905,R11-1016,1,0.849676,"the most plausible genealogies from data. In the context of population genetics, such an approach is introduced in (Sir´en et al., 2011; Sir´en et al., 1 The creators of the input dataset posit that the elements of a cognate set derive from a common origin—a word in the ancestral proto-language. 27 Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning, pages 27–37, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics ships, change and evolution. We have been developing a family of models for this task, called the Etymon models, (Wettig et al., 2011; Wettig et al., 2012; Nouri and Yangarber, 2016), etc.2 Methods introduced in (Kondrak, 2002), inspired by alignment in machine translation, learn one-to-one sound correspondences between words in pairs of languages. Kondrak (2003), and Wettig et al. (2011) find more complex—many-tomany—sound correspondences. These methods focus on alignment. They model the context of the sound changes in a limited way, while it is known that most evolutionary changes are conditioned on the context of the evolving sound. Bouchard-Cˆot´e et al. (2007) propose MCMC-based methods to model context, and operate on"
W16-1905,W12-0215,1,0.498095,"enealogies from data. In the context of population genetics, such an approach is introduced in (Sir´en et al., 2011; Sir´en et al., 1 The creators of the input dataset posit that the elements of a cognate set derive from a common origin—a word in the ancestral proto-language. 27 Proceedings of the 7th Workshop on Cognitive Aspects of Computational Language Learning, pages 27–37, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics ships, change and evolution. We have been developing a family of models for this task, called the Etymon models, (Wettig et al., 2011; Wettig et al., 2012; Nouri and Yangarber, 2016), etc.2 Methods introduced in (Kondrak, 2002), inspired by alignment in machine translation, learn one-to-one sound correspondences between words in pairs of languages. Kondrak (2003), and Wettig et al. (2011) find more complex—many-tomany—sound correspondences. These methods focus on alignment. They model the context of the sound changes in a limited way, while it is known that most evolutionary changes are conditioned on the context of the evolving sound. Bouchard-Cˆot´e et al. (2007) propose MCMC-based methods to model context, and operate on more than one pair o"
W16-1905,W09-3526,0,0.0276163,"cription Length principle. There are important differences between our alignment problem vs. alignment in MT. Evolutionary sound correspondence is conditioned by local context, whereas in MT correspondences may depend on much wider context. There is no analogue to the notion of phonetic features in MT. Phonetic correspondences in etymological data—which apply throughout the language—have no analogue in semantic shift processes in a such way as to be captured by MT alignment models. Neither are phonetic features used in the aforementioned work from the area of automatic transliteration, e.g., (Zelenko, 2009). Our work on the Etymon models is closely related to a series of generative models in (Bouchard-Cˆot´e et al., 2007) through (Hall and Klein, 2011), in the following respects. In (Wettig et al., 2011) some context is modeled in the form of coding pairs of symbols, as in (Kondrak, 2003). Bouchard-Cˆot´e et al. (2007) and Hall and Klein (2011) handle context by conditioning the symbol being generated upon the symbols immediately preceding and following. Wettig et al. (2012) and Nouri and Yangarber (2016) use much broader context by building decision trees, so that non-relevant context informati"
W17-0304,H05-1103,0,0.0696998,"Missing"
W17-0304,P06-4001,0,0.0332386,"Missing"
W17-0304,W14-1817,0,0.0205834,"grams have been created. Some of the programs, such as Robo-Sensei (Nagata, 2002) and E-Tutor (Heift, 2001), use NLP (natural language processing) techniques, and may be called “intelligent” CALL systems. Revita’s main learning mode involves a type of exercise known as “cloze” in the literature, first described in (Taylor, 1953). In a cloze (deletion) test, a portion of text has some of the words removed, and the learner is asked to recover the missing words. Clozes require an understanding of the context, semantics and syntax in order to identify the missing words correctly. The approach in (Zesch and Melamud, 2014) involves generating distractors for vocabulary clozes—multiple-choice questions. The method for generating lists of distractors is as follows. First “context-insensitive inference rules” are used to generate a set of candidate distractors. This set includes the top-N matches for the target word w in the corpus—words which share some con1 Introduction Revita is an open online platform designed to help support endangered languages, by stimulating active language learning. Current focus is on several endangered languages inside the Russian Federation (RF), which have moderate to small numbers of"
W17-1412,agic-ljubesic-2014-setimes,0,0.0728339,"Missing"
W17-1412,M98-1001,0,0.552765,"revolving around a certain “focus” entity. The main rationale of such a setup is to foster development of “all-rounder” NER and cross-lingual entity matching solutions that are not tailored to specific, narrow domains. The shared task was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our task is the Entity Discovery and Linking (EDL) track (Ji et al., 2014; Ji et al., 2015) of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from"
W17-1412,doddington-etal-2004-automatic,0,0.348774,"ity. The main rationale of such a setup is to foster development of “all-rounder” NER and cross-lingual entity matching solutions that are not tailored to specific, narrow domains. The shared task was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our task is the Entity Discovery and Linking (EDL) track (Ji et al., 2014; Ji et al., 2015) of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of textual documents in multiple l"
W17-1412,W17-1413,0,0.0438103,"Missing"
W17-1412,W17-1414,0,0.0586994,"te a working solution, only two teams submitted results within the deadline. A total of two unique runs were submitted. JHU/APL team attempted the NER and Entity Matching sub-tasks. They employed a statistical 82 tagger called SVMLattice (Mayfield et al., 2003), with NER labels inferred by projecting English tags across bitext. The Illinois tagger (Ratinov and Roth, 2009) was used for English. A rule-based entity clusterer called “kripke” was used for Entity Matching (McNamee et al., 2013). The team (code “jhu”) attempted all languages available in the Challenge. More details can be found in (Mayfield et al., 2017). The G4.19 Research Group adapted Liner2 (Marci´nczuk et al., 2013)—a generic framework which can be used to solve various tasks based on sequence labeling, which is equipped with a set of modules (based on statistical models, dictionaries, rules and heuristics) which recognize and annotate certain types of phrases. The details of tuning Liner2 to tackle the shared task are described in (Marci´nczuk et al., 2017). The team (code “pw”) attempted only the Polish-language Challenge. The above systems met the deadline to participate in the first run of the Challenge—Phase I. Since the Challenge a"
W17-1412,P16-1060,0,0.06896,"Missing"
W17-1412,P13-1069,0,0.0148193,"tance, the inflected form of the Polish proper name Europejskiego Funduszu Rozwoju Regionalnego (EuropeanGEN FundGEN DevelopmentGEN RegionalGEN ) consists of two basic genitive noun phrases, of which only the first one (“European Fund”) needs to be normalized, whereas the second (“Regional Development”) should remain unchanged. The corresponding base form is “Europejski Fundusz Rozwoju Regionalnego”. Since in some Slavic languages adjectives may precede or follow a noun in a noun phrase (like in the example above), detection of inner syntactic structure of complex proper names is not trivial (Radziszewski, 2013), and thus complicates the process of automated lemmatization. Complex person name declension paradigms (Piskorski et al., 2009) add another level of complexity. It is worth mentioning that, for the sake of compliance with the NER guidelines in Section 2, documents that included hard-to-decide entity mention annotations were excluded from the test datasets for the present. A case in point is a document in Croatian that contained the phrase “Zagrebaˇcka, Sisaˇcko-Moslavaˇcka i Karlovaˇcka županija”—a contracted version of 80 three named entities (“Zagrebaˇcka županija”, Genitive Nominative (“ba"
W17-1412,W09-1119,0,0.0219001,"ems Eleven teams from seven countries—Czech Republic, Germany, India, Poland, Russia, Slovenia, and USA—registered for the evaluation task and received the trial datasets. Due to the complexity of the task and relatively short time available to create a working solution, only two teams submitted results within the deadline. A total of two unique runs were submitted. JHU/APL team attempted the NER and Entity Matching sub-tasks. They employed a statistical 82 tagger called SVMLattice (Mayfield et al., 2003), with NER labels inferred by projecting English tags across bitext. The Illinois tagger (Ratinov and Roth, 2009) was used for English. A rule-based entity clusterer called “kripke” was used for Entity Matching (McNamee et al., 2013). The team (code “jhu”) attempted all languages available in the Challenge. More details can be found in (Mayfield et al., 2017). The G4.19 Research Group adapted Liner2 (Marci´nczuk et al., 2013)—a generic framework which can be used to solve various tasks based on sequence labeling, which is equipped with a set of modules (based on statistical models, dictionaries, rules and heuristics) which recognize and annotate certain types of phrases. The details of tuning Liner2 to t"
W17-1412,W03-0419,0,0.841094,"Missing"
W17-1412,W02-2024,0,0.606186,"Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our task is the Entity Discovery and Linking (EDL) track (Ji et al., 2014; Ji et al., 2015) of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of textual documents in multiple languages (English, Chinese, and Spanish), and to partition the entities into cross-document equivalence classes, by either linking mentions to a knowledge base or directly Proceedings of the 6th Workshop on Balto-Slavic Nat"
W17-1412,W10-2403,0,\N,Missing
W17-1412,W07-1701,0,\N,Missing
W17-1412,W16-2709,0,\N,Missing
W18-3008,D14-1162,0,0.0823176,"her classifier alone. In that paper we applied a rulebased approach for NEs, and did not use NEs as features for machine learning. 3 Model The architecture of our CNN is shown in Figure 1. The inputs are fed into the network as zero-padded text fragments of fixed size, with each word represented by a fixed-dimensional embedding vector. The inputs are fed into a layer of convolutional filters with multiple widths, optionally followed by deeper convolutional layers. The results of the last convolutional layer are max-pooled, producing a 4 Data Representation We train the embeddings using GloVe (Pennington et al., 2014). As features we use lower-cased lemmas of all words. The rationale for this is that our 65 apple type glove-6B pear pear iphone unpasteurized unpasteurized microsoft juice juice intel apple computer fruit macintosh odwalla salmonella ipod strawberry peach ibm fruit taint ipad macintosh orange software meat crate google pear board strawberry itunes airline name type glove-6B carrier carrier airlines flight flight airways british airways passenger lufthansa american airlines aircraft carrier air france airport flights passenger air flight lufthansa pilot pilots air route qantas united airlines"
W18-3008,W13-1204,1,0.903406,"Missing"
W18-3008,C02-1165,1,0.431078,"ivovarova and Roman Yangarber University of Helsinki, Finland Department of Computer Science first.last@cs.helsinki.fi Abstract PULS news monitoring system (Pivovarova et al., 2013). While nominally RCV1 contains general news, it is skewed toward business; many of the topic labels are business-related (“Markets”, “Commodities”, “Share Capital,” etc.). Thus, we expect our business corpus to help in learning features for the Reuters classification tasks. We compare several NE representation to find the most suitable name features for each task. We use the PULS NER system (Grishman et al., 2003; Huttunen et al., 2002a,b) to find NEs and their types—company, location, person, etc. We compare various representations of NEs, by building embeddings, and training CNNs to find the best representation. We also compare building embeddings on the RCV1 corpus vs. using much larger external corpora. We explore representations for multitoken names in the context of the Reuters topic and sector classification tasks (RCV1). We find that: the best way to treat names is to split them into tokens and use each token as a separate feature; NEs have more impact on sector classification than on topic classification; replacing"
W18-3008,huttunen-etal-2002-diversity,1,0.305644,"ivovarova and Roman Yangarber University of Helsinki, Finland Department of Computer Science first.last@cs.helsinki.fi Abstract PULS news monitoring system (Pivovarova et al., 2013). While nominally RCV1 contains general news, it is skewed toward business; many of the topic labels are business-related (“Markets”, “Commodities”, “Share Capital,” etc.). Thus, we expect our business corpus to help in learning features for the Reuters classification tasks. We compare several NE representation to find the most suitable name features for each task. We use the PULS NER system (Grishman et al., 2003; Huttunen et al., 2002a,b) to find NEs and their types—company, location, person, etc. We compare various representations of NEs, by building embeddings, and training CNNs to find the best representation. We also compare building embeddings on the RCV1 corpus vs. using much larger external corpora. We explore representations for multitoken names in the context of the Reuters topic and sector classification tasks (RCV1). We find that: the best way to treat names is to split them into tokens and use each token as a separate feature; NEs have more impact on sector classification than on topic classification; replacing"
W18-3008,D14-1181,0,0.0159505,"Missing"
W19-3709,agic-ljubesic-2014-setimes,0,0.0441997,"Missing"
W19-3709,W19-3712,0,0.227817,"Jacquet et al., 2019a). The main focus of the approach is on generating the possible inflected variants for known names (Jacquet et al., 2019b). NLP Cube12 is an open-source NLP framework that handles sentence segmentation, POS Tagging and lemmatization. The low-level features obtained from the framework, such as part of speech tags, were used as input for an LSTM model. Each Figure 3: Average system performances on the test data language was trained individually, producing four models. The models were trained using DyNet13 . RIS is a modified BERT model, which uses CRF as the top-most layer (Arkhipov et al., 2019). The model was initialized with an existing BERT model trained on 100 languages. Sberiboba uses multilingual BERT embeddings, summed with learned weights and followed by BiLSTM, attention layers and NCRF++ on the top (Emelianov and Artemova, 2019). Multilin6 github.com/zalandoresearch/flair polyglot.readthedocs.io 8 github.com/huggingface/pytorch-pretrained-BERT, github.com/sberbank-ai/ner-bert 9 clarin-pl.eu/dspace/handle/11321/270 10 ufal.mff.cuni.cz/cnec/cnec2.0 11 poleval.pl/tasks/task2 12 github.com/adobe/NLP-Cube 7 13 69 dynet.io N ORD S TREAM System IIUWR.PL JRC-TMA RIS CogComp Sberibo"
W19-3709,W19-3714,1,0.83137,"ining corpora were used: KPWr9 for Polish, CNEC10 for Czech, and data extracted using heuristics from Wikipedia. Lemmatization is partially trained on Wikipedia and PolEval corpora,11 and partially rule-based. Entity linking is rule-based, and uses WikiData and FastText (Bojanowski et al., 2017). JRC-TMA-CC is a hybrid system combining a rule-based approach and machine learning techniques. It is a corpus-driven system, lightweight and highly multilingual, exploiting both automatically created lexical resources, such as JRCNames (Ehrmann et al., 2017), and external resources, such as BabelNet (Jacquet et al., 2019a). The main focus of the approach is on generating the possible inflected variants for known names (Jacquet et al., 2019b). NLP Cube12 is an open-source NLP framework that handles sentence segmentation, POS Tagging and lemmatization. The low-level features obtained from the framework, such as part of speech tags, were used as input for an LSTM model. Each Figure 3: Average system performances on the test data language was trained individually, producing four models. The models were trained using DyNet13 . RIS is a modified BERT model, which uses CRF as the top-most layer (Arkhipov et al., 201"
W19-3709,Q17-1010,0,0.0280073,"er, Russia. The system has a hybrid architecture, combining rule-based and ML techniques, where the ML-component is loosely related to (Antonova and Soloviev, 2013). As the system processes Russian, English and Ukrainian, the team submitted output only for Russian. IIUWR.PL combines Flair6 , Polyglot7 and BERT.8 Additional training corpora were used: KPWr9 for Polish, CNEC10 for Czech, and data extracted using heuristics from Wikipedia. Lemmatization is partially trained on Wikipedia and PolEval corpora,11 and partially rule-based. Entity linking is rule-based, and uses WikiData and FastText (Bojanowski et al., 2017). JRC-TMA-CC is a hybrid system combining a rule-based approach and machine learning techniques. It is a corpus-driven system, lightweight and highly multilingual, exploiting both automatically created lexical resources, such as JRCNames (Ehrmann et al., 2017), and external resources, such as BabelNet (Jacquet et al., 2019a). The main focus of the approach is on generating the possible inflected variants for known names (Jacquet et al., 2019b). NLP Cube12 is an open-source NLP framework that handles sentence segmentation, POS Tagging and lemmatization. The low-level features obtained from the"
W19-3709,M98-1001,0,0.502389,"and event. The input text collection consists of docPrior Work The work we describe here builds on the First Shared Task on Multilingual Named Entity Recognition, Normalization and cross-lingual Matching for Slavic Languages, (Piskorski et al., 2017), which, to the best of our knowledge, was the first attempt at such a shared task covering several Slavic languages. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang 1 bsnlp.cs.helsinki.fi/shared_task.html 63 Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 63–74, c Florence, Italy, 2 August 2019. 2019 Association for Computational Linguistics ize” all named-entity mentions in each of the documents, and to link across languages all named"
W19-3709,N19-1423,0,0.0156125,"LEA recall and precision are then defined as follows: P ki ∈K (imp(ki ) × res(ki )) P Recall LEA = kz ∈K imp(kz ) 6 Participant Systems Sixteen teams from eight countries registered for the shared task. Half of the registered teams submitted results by the deadline. Five teams submitted description of their systems in the form of a Workshop paper. The remaining teams submitted a short description of their systems. We briefly review the systems; complete descriptions appear in the corresponding papers. CogComp used multi-source BiLSTM-CRF models, using solely the BERT multilingual embeddings, (Devlin et al., 2019), which directly 68 allows the model to train on datasets in multiple languages. The team submitted several models trained on different combinations of input languages. They found that multi-source training with multilingual BERT outperforms singlesource. Cross-lingual (even cross-script) training worked remarkably well. Multilingual BERT can handle train/test sets with mismatching tagsets in certain situations. The best performing models were trained on a combination of data in four languages, while adding English into training data worsen the overall performance, (Tsygankova et al., 2019). C"
W19-3709,doddington-etal-2004-automatic,0,0.340979,"nsists of docPrior Work The work we describe here builds on the First Shared Task on Multilingual Named Entity Recognition, Normalization and cross-lingual Matching for Slavic Languages, (Piskorski et al., 2017), which, to the best of our knowledge, was the first attempt at such a shared task covering several Slavic languages. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang 1 bsnlp.cs.helsinki.fi/shared_task.html 63 Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 63–74, c Florence, Italy, 2 August 2019. 2019 Association for Computational Linguistics ize” all named-entity mentions in each of the documents, and to link across languages all named mentions referring to the same real-world entit"
W19-3709,P16-1101,0,0.038978,"most problematic categories (Piskorski et al., 2017). The P RO category also exhibits higher variation across languages and corpora than other categories, which might point to some annotation artefacts. The results for the E VT category are less informative, since there are few examples of this category in the dataset, as seen in Table 1. Figure 4: Evaluation results for closs-lingual entity linking. Averaged across two corpora. gual BERT is used only for the embeddings, with no fine-tuning for the tasks. TLR used a standard end-to-end architecture for sequence labeling, namely: LSTM-CNNCRF, (Ma and Hovy, 2016). It was combined with contextual embeddings using a weighted average (Reimers and Gurevych, 2019) of a BERT model pre-trained for multiple languages (including all of the languages of the Task). As seen from these descriptions, most of the teams use the BERT model, except NLP Cube, which uses another deep learning model (LSTM), and JRC, which uses rule-based processing of Slavic inflection. 7 Evaluation Results Figure 3 shows system performance averaged across all languages and two test corpora. We present results for seven teams, since CTC-NER submitted results only for Russian. For each tea"
W19-3709,marcinczuk-etal-2017-inforex,1,0.83713,"(Crawley and Wagner, 2010). In particular, some of the meta-data—i.e., creation date, title, URL—were automatically extracted using this tool. HTML parsing results may include not only the main text of a Web page, but also some additional text, e.g., labels from menus, user comments, etc., which may not constitute well-formed utterances in the target language.4 The resulting set of partially “cleaned” documents were used to manually select documents for each language and topic, for the final datasets. Documents were annotated using the Inforex5 web-based system for annotation of text corpora (Marcinczuk et al., 2017). Inforex allows parallel access and resource sharing by multiple annotators. It let us share a common list of entities, and perform entity-linking semi-automatically: for a given entity, an annotator sees a list of entities of the same type inserted by all annotators and can select an entity ID from the list. A snapshot of the Inforex interface is in Figure 1. In addition, Inforex keeps track of all lemmas and IDs inserted for each surface form, and inserts them automatically, so in many cases the annotator only confirms the proposed values, which speeds up the annotation process a great deal"
W19-3709,W19-3713,0,0.11752,"Missing"
W19-3709,P16-1060,0,0.103459,"Missing"
W19-3709,W17-1412,1,0.83185,"ultilingual NE recognition (NER), which aims at addressing these problems in a systematic way. The shared task was organized in the context of the 7th Balto-Slavic Natural Language Processing Workshop co-located with the ACL 2019 conference. The task covers four languages—Bulgarian, Czech, Polish and Russian—and five types of NE: person, location, organization, product, and event. The input text collection consists of docPrior Work The work we describe here builds on the First Shared Task on Multilingual Named Entity Recognition, Normalization and cross-lingual Matching for Slavic Languages, (Piskorski et al., 2017), which, to the best of our knowledge, was the first attempt at such a shared task covering several Slavic languages. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of"
W19-3709,W02-2024,0,0.219977,"nowledge, was the first attempt at such a shared task covering several Slavic languages. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang 1 bsnlp.cs.helsinki.fi/shared_task.html 63 Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 63–74, c Florence, Italy, 2 August 2019. 2019 Association for Computational Linguistics ize” all named-entity mentions in each of the documents, and to link across languages all named mentions referring to the same real-world entity. Formally, the Multilingual Named Entity Recognition task includes three sub-tasks: and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our ta"
W19-3709,W03-0419,0,0.87116,"Missing"
W19-3709,W19-3710,0,0.0903045,"ings, (Devlin et al., 2019), which directly 68 allows the model to train on datasets in multiple languages. The team submitted several models trained on different combinations of input languages. They found that multi-source training with multilingual BERT outperforms singlesource. Cross-lingual (even cross-script) training worked remarkably well. Multilingual BERT can handle train/test sets with mismatching tagsets in certain situations. The best performing models were trained on a combination of data in four languages, while adding English into training data worsen the overall performance, (Tsygankova et al., 2019). CTC-NER is a baseline prototype of a NER component of an entity recognition system currently under development at the Cognitive Technologies Center, Russia. The system has a hybrid architecture, combining rule-based and ML techniques, where the ML-component is loosely related to (Antonova and Soloviev, 2013). As the system processes Russian, English and Ukrainian, the team submitted output only for Russian. IIUWR.PL combines Flair6 , Polyglot7 and BERT.8 Additional training corpora were used: KPWr9 for Polish, CNEC10 for Czech, and data extracted using heuristics from Wikipedia. Lemmatizatio"
W19-6117,W19-3702,1,0.830289,"takes that the learners make over time, and insights into how the learning system can be improved on the basis of the collected data. 5.3 Multiple admissibility Multiple Admissibility (MA) in language learning occurs when more than one surface form of a given lemma fits syntactically and semantically within a given context. MA implies that multiple alternative answers are “correct” for the given context, not only the word that the author chose to use in the story. From the perspective of CALL and ITS (intelligent tutoring systems), MA forms a complex challenge, discussed in current research, (Katinskaia et al., 2019). The Sakha language presents a particularly rich source of scenarios for multiple-admissible answers. Due to the agglutinative morphology of Sakha, the learner can add affixes to a word, which carry additional information or connotations to slightly alter the meaning of the word. We briefly discuss several such scenarios. The category of possessiveness—possessive affixes on nominals—is one of the fundamental categories of Sakha grammar. Possessive forms are very common, and the scope of their usage is far wider than merely indicating possession in the strict sense; possessive affixes express"
W19-6117,L18-1644,1,0.749454,"form, and Russian has 50 hours. In summary, few linguistic resources exists for Sakha. 3.2 Revita is an e-learning platform, which uses methods from computer-assisted language learning (CALL) and intelligent tutoring systems (ITS).5 The platform provides a language-independent foundation for language learning, which can be adapted to support new languages, by adding language-specific resources, without modifying the core system. The platform is used for language teaching and learning at several universities in Europe and Asia. The goal of the system is to provide tools for language learning, (Katinskaia et al., 2018), and to support endangered languages, (Katinskaia and Yangarber, 2018; Yangarber, 2018). The system focuses on stimulating the student to actively produce language, rather than passively absorb exam1 3.1 Sakha language resources Despite the current advances in digitization, digital resources for the Sakha language are severely lacking. The creation of digital tools would strengthen Revita language learning platform www.sakhatyla.ru sah.wikipedia.org 3 www.memrise.com/course/153579/sakha-tylynleksikata-sakha-tyla-iakutskii/ 4 voice.mozilla.org/en/about 5 revita.cs.helsinki.fi 2 ples of languag"
W19-6124,Q16-1026,0,0.0462614,"results show that this method can produce annotated instances with high precision, and the resulting model achieves state-of-the-art performance. 1 Introduction The goal of Named Entity Recognition (NER) is to recognize names and classify them into pre-defined categories, based on their context. The quality of NER is crucial, since it is an important step in modern NLP, e.g., information retrieval (IR) or information extraction (IE) systems. Various approaches have been proposed to tackle the NER task, including (Finkel et al., 2005; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Reimers and Gurevych, 2017; Peters et al., 2018; Devlin et al., 2018). These approaches require large annotated datasets to train models, and have been shown to be effective for languages with abundant linguistic resources, such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora"
W19-6124,R11-1017,0,0.0241031,"et al., 2013). However, the amount of Wikipedia documents in Finnish is also relatively small. In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data. This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language. The core idea of our annotation approach is to utilize strong NER available for English and to match automatically annotated English data with Finnish data by resolving the base form of names. Ehrmann et al. (2011) proposed an idea of model projection similar to the one in the this work. However, rather than resolving the base form of named entities in target language internally as we do, they used machine translation as the basis for projection. This allows them to project models between different languages, including in languages with different writing systems, such as Russian and English. However, this assumes the existence of a high-quality machine translation system, and token binding between the languages, which determine the quality of the NER training dataset. Using the resulting annotated data,"
W19-6124,P05-1045,0,0.0786579,"s “artificially” annotated data to train a BiLSTM-CRF NER model for Finnish. Our results show that this method can produce annotated instances with high precision, and the resulting model achieves state-of-the-art performance. 1 Introduction The goal of Named Entity Recognition (NER) is to recognize names and classify them into pre-defined categories, based on their context. The quality of NER is crucial, since it is an important step in modern NLP, e.g., information retrieval (IR) or information extraction (IE) systems. Various approaches have been proposed to tackle the NER task, including (Finkel et al., 2005; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Reimers and Gurevych, 2017; Peters et al., 2018; Devlin et al., 2018). These approaches require large annotated datasets to train models, and have been shown to be effective for languages with abundant linguistic resources, such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few,"
W19-6124,I17-1042,0,0.0166427,"undant linguistic resources, such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora for training NER models is one solution to this problem. Several approaches have been proposed for building such corpora for NER. Most of these rely on the Wikipedia corpus, (Al-Rfou et al., 2015; Ghaddar and Langlais, 2017; Kim et al., 2012; Richman and Schone, 2008; Kazama and Torisawa, 2007; Toral and Munoz, 2006; Nothman et al., 2013). However, the amount of Wikipedia documents in Finnish is also relatively small. In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data. This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language. The core idea of our annotation approach is to utilize strong NER available for English and to match aut"
W19-6124,D07-1073,0,0.0688526,"are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora for training NER models is one solution to this problem. Several approaches have been proposed for building such corpora for NER. Most of these rely on the Wikipedia corpus, (Al-Rfou et al., 2015; Ghaddar and Langlais, 2017; Kim et al., 2012; Richman and Schone, 2008; Kazama and Torisawa, 2007; Toral and Munoz, 2006; Nothman et al., 2013). However, the amount of Wikipedia documents in Finnish is also relatively small. In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data. This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language. The core idea of our annotation approach is to utilize strong NER available for English and to match automatically annotated English data with Finnish data by resolving the ba"
W19-6124,P12-1073,0,0.0225939,"such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora for training NER models is one solution to this problem. Several approaches have been proposed for building such corpora for NER. Most of these rely on the Wikipedia corpus, (Al-Rfou et al., 2015; Ghaddar and Langlais, 2017; Kim et al., 2012; Richman and Schone, 2008; Kazama and Torisawa, 2007; Toral and Munoz, 2006; Nothman et al., 2013). However, the amount of Wikipedia documents in Finnish is also relatively small. In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data. This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language. The core idea of our annotation approach is to utilize strong NER available for English and to match automatically annotat"
W19-6124,N16-1030,0,0.0151652,"a BiLSTM-CRF NER model for Finnish. Our results show that this method can produce annotated instances with high precision, and the resulting model achieves state-of-the-art performance. 1 Introduction The goal of Named Entity Recognition (NER) is to recognize names and classify them into pre-defined categories, based on their context. The quality of NER is crucial, since it is an important step in modern NLP, e.g., information retrieval (IR) or information extraction (IE) systems. Various approaches have been proposed to tackle the NER task, including (Finkel et al., 2005; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Reimers and Gurevych, 2017; Peters et al., 2018; Devlin et al., 2018). These approaches require large annotated datasets to train models, and have been shown to be effective for languages with abundant linguistic resources, such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its per"
W19-6124,P16-1101,0,0.554825,"l for Finnish. Our results show that this method can produce annotated instances with high precision, and the resulting model achieves state-of-the-art performance. 1 Introduction The goal of Named Entity Recognition (NER) is to recognize names and classify them into pre-defined categories, based on their context. The quality of NER is crucial, since it is an important step in modern NLP, e.g., information retrieval (IR) or information extraction (IE) systems. Various approaches have been proposed to tackle the NER task, including (Finkel et al., 2005; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Reimers and Gurevych, 2017; Peters et al., 2018; Devlin et al., 2018). These approaches require large annotated datasets to train models, and have been shown to be effective for languages with abundant linguistic resources, such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatic"
W19-6124,W13-5631,0,0.0762888,"Missing"
W19-6124,N18-1202,0,0.0199989,"instances with high precision, and the resulting model achieves state-of-the-art performance. 1 Introduction The goal of Named Entity Recognition (NER) is to recognize names and classify them into pre-defined categories, based on their context. The quality of NER is crucial, since it is an important step in modern NLP, e.g., information retrieval (IR) or information extraction (IE) systems. Various approaches have been proposed to tackle the NER task, including (Finkel et al., 2005; Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Reimers and Gurevych, 2017; Peters et al., 2018; Devlin et al., 2018). These approaches require large annotated datasets to train models, and have been shown to be effective for languages with abundant linguistic resources, such as English. However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora for training NER models is one solution to this p"
W19-6124,P08-1001,0,0.0412536,"However, not all languages are as resource-rich as English. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora for training NER models is one solution to this problem. Several approaches have been proposed for building such corpora for NER. Most of these rely on the Wikipedia corpus, (Al-Rfou et al., 2015; Ghaddar and Langlais, 2017; Kim et al., 2012; Richman and Schone, 2008; Kazama and Torisawa, 2007; Toral and Munoz, 2006; Nothman et al., 2013). However, the amount of Wikipedia documents in Finnish is also relatively small. In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data. This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language. The core idea of our annotation approach is to utilize strong NER available for English and to match automatically annotated English data with Finni"
W19-6124,W06-2809,0,0.119674,"glish. There are significantly fewer resources for languages such as Finnish. Further, very few NER taggers or corpora are publicly available online. The FiNER tagger from the Language Bank of Finnish1 is one of the few, but we found no documentation of its performance. Automatically annotating corpora for training NER models is one solution to this problem. Several approaches have been proposed for building such corpora for NER. Most of these rely on the Wikipedia corpus, (Al-Rfou et al., 2015; Ghaddar and Langlais, 2017; Kim et al., 2012; Richman and Schone, 2008; Kazama and Torisawa, 2007; Toral and Munoz, 2006; Nothman et al., 2013). However, the amount of Wikipedia documents in Finnish is also relatively small. In this paper, we propose a novel approach for automatically marking Finnish text with NE annotations, for the purpose of training a statistical NER model from these annotated data. This can be viewed as a projection of a pre-existing NER model in one language to a NER model in another language. The core idea of our annotation approach is to utilize strong NER available for English and to match automatically annotated English data with Finnish data by resolving the base form of names. Ehrma"
W98-0604,M95-1014,1,0.820264,"position) Introduction Although, nominalizationQ are very common in written text, the computational linguistics literature provides few systematic accounts of how to deal with phrases containing these words. This paper focuses on this problem in the context of Information Extraction (IE). 2 Many extraction systems use either parsing combined with some form of syntactic regularization, or a meta-rule mechanism to automatically match variants of clausal syntactic structures (active main clause, passive, relative clause etc.), e.g., FASTUS (Appelt et al., 1995) and the Proteus Extraction System (Grishman, 1995). However, this mechanism does not extend to nominalization patterns, which must be coded separately from the clausal patterns. NOMLEX, a dictionary of nominalizations currently under development at NYU, (Macleod et al., 1997) provides a way to handle nominalizations more automatically, and with INominalizations are nouns which are related to words of another part of speech, most commonly verbs. In this paper, only verbal nominalizatious will be discussed. 2The Message Understanding Colfference Scenario Template Task (MUC, 1995), (MUC, 1998) is ore&quot; model for the kind of information that we ar"
W98-0604,P87-1019,0,\N,Missing
X98-1016,W98-1118,1,0.835143,"peration of the patterns. The user&apos;s input is reduced to • providing textual examples of events of interest, • describing the corresponding output structures (LFs) which the example text should induce. In the remaining sections we discuss how the system can use this information to * automatically build patterns to m a p the userspecified text into the user-specified LF, • generalize the newly created patterns to boost coverage. 6 T o a limited degree, the system is able to adapt to a new domain automatically: given training data in the domain, we can train a statistical proper name recognizer [3], in effect, obviating the need for building domain-specific name patterns. 99 ...Information Resources Inc.&apos;s Londonbased European Information Services operation has appointed George Garrick, .40 years old, president ... Field Position Company Location Person Status ;;; For &lt;company&gt; appoints &lt;person&gt; &lt;position&gt; (definePattern Appoint &quot;np(C-company)? vg(C-appoint) np(C-person) to-be? np(C-position): company=l.attributes, person=3.attributes, position=5.attributes I Value president European Information Services London George Garrick In (definehction Appoint (phrase-type) ( l e t ( ( p e r s o"
X98-1016,P93-1022,0,0.0138849,"co-descrip. Similarly, the classes C-city for city names and C-state for state names would be gathered under a concept C-location. The GUI tools then allow the user to perform semantic generalization on the individual constituents of the pattern&apos;s precondition; its final form becomes: In(C-company) &apos;s]? [n(C-location)-based]? n(C-company) n(C-co-descrip)? The semantic hierarchy is scenario-specific. It is built up dynamically through tools that draw on pre-existing domain-independent hierarchies, such as WordNet, as well as domain-specific word similarity measures and co-occurrence statistics [4]. By a similar process, we can now acquire a clausal pattern from the example in figure 3 at the beginning of this •section. The system proposes the precondition: np(C-company) vg(C-appoint) np(C-person) np(president) Applying semantic generalization to the last constituent yields: np(C-company) vg(C-appoint) np(C-person) np(C-title) where C-title is a semantic class that gathers all corporate titles. The user can now fill the slots in the LF for the event as in figure 7. 5 Meta-rules Consider the following variant of the original example: ... George Garrick, an avowed anticapitalist, was appo"
X98-1016,M95-1014,1,0.785154,"gn: Control is encapsulated in immutable core engines, which draw upon domain- or scenario-specific information stored in knowledge bases (KB) which are customized for each new domain and scenario. • Text analysis is based on pattern matching: regular expression pattern matching is a widely used strategy in the IE community. P a t t e r n matching is a form of deterministic b o t t o m - u p partial parsing. This approach has gained considerable popularity due to limitations on the accuracy of full syntactic parsers, and the adequacy of partial, semantically-constrained, parsing for this task [2, 1, 5]. Introduction The task of Information Extraction (IE) as understood in this paper is the selective extraction of meaning from free natural language text. 1 This kind of text analysis is distinguished from others in Natural Language Processing in t h a t &quot;meaning&quot; is understood in a narrow sense - in terms of a fixed set of semantic objects, namely, entities, relationships among these entities, and events in which these entities participate. These objects belong to a small number of types, all having fixed regular structure, within a fixed and closely circumscribed subject domain, which permit"
X98-1016,C96-1079,1,0.66057,"ns stored in a customizable knowledge base. Adapting an IE system to a new subject domain entails the construction of a new pattern base a time-consuming and expensive task. We describe a strategy for building patterns from examples. To a d a p t the IE system to a new domain quickly, the user chooses a set of examples in a training text, and for each example gives the logical form entries which the example induces. The system transforms these examples into patterns and then applies meta-rules to generalize these patterns. - ferences (MUCs), 2 conducted over the last decade - are described in [8, 6]. The MUCs have yielded some widely (if not universally) accepted wisdom regarding IE: • Customization and portability is an important problem: to be considered a useful tool, an IE system must be able to perform in a variety of domains. - 1 • Systems have modular design: Control is encapsulated in immutable core engines, which draw upon domain- or scenario-specific information stored in knowledge bases (KB) which are customized for each new domain and scenario. • Text analysis is based on pattern matching: regular expression pattern matching is a widely used strategy in the IE community. P a"
X98-1016,M95-1018,0,0.0697367,"Missing"
X98-1016,M98-1011,1,0.878511,"Missing"
