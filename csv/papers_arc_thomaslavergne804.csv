2021.eval4nlp-1.1,Differential Evaluation: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing,2021,-1,-1,4,0,8588,lucie gianola,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2020.lrec-1.241,Handling Entity Normalization with no Annotated Corpus: Weakly Supervised Methods Based on Distributional Representation and Ontological Information,2020,-1,-1,5,0,17095,arnaud ferre,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Entity normalization (or entity linking) is an important subtask of information extraction that links entity mentions in text to categories or concepts in a reference vocabulary. Machine learning based normalization methods have good adaptability as long as they have enough training data per reference with a sufficient quality. Distributional representations are commonly used because of their capacity to handle different expressions with similar meanings. However, in specific technical and scientific domains, the small amount of training data and the relatively small size of specialized corpora remain major challenges. Recently, the machine learning-based CONTES method has addressed these challenges for reference vocabularies that are ontologies, as is often the case in life sciences and biomedical domains. And yet, its performance is dependent on manually annotated corpus. Furthermore, like other machine learning based methods, parametrization remains tricky. We propose a new approach to address the scarcity of training data that extends the CONTES method by corpus selection, pre-processing and weak supervision strategies, which can yield high-performance results without any manually annotated examples. We also study which hyperparameters are most influential, with sometimes different patterns compared to previous work. The results show that our approach significantly improves accuracy and outperforms previous state-of-the-art algorithms."
2020.coling-main.609,{C}haracter{BERT}: Reconciling {ELM}o and {BERT} for Word-Level Open-Vocabulary Representations From Characters,2020,-1,-1,3,1,8589,hicham boukkouri,Proceedings of the 28th International Conference on Computational Linguistics,0,"Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of Transformers. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations."
P19-2041,Embedding Strategies for Specialized Domains: Application to Clinical Entity Recognition,2019,0,1,3,1,8589,hicham boukkouri,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Using pre-trained word embeddings in conjunction with Deep Learning models has become the {``}de facto{''} approach in Natural Language Processing (NLP). While this usually yields satisfactory results, off-the-shelf word embeddings tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain."
L18-1619,"Corpora with Part-of-Speech Annotations for Three Regional Languages of {F}rance: {A}lsatian, {O}ccitan and {P}icard",2018,0,1,15,0,27324,delphine bernhard,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This article describes the creation of corpora with part-of-speech annotations for three regional languages of France: Alsatian, Occitan and Picard. These manual annotations were performed in the context of the RESTAURE project, whose goal is to develop resources and tools for these under-resourced French regional languages. The article presents the tagsets used in the annotation process as well as the resulting annotated corpora."
2018.jeptalnrecital-court.22,Detecting context-dependent sentences in parallel corpora,2018,0,0,2,0,7687,rachel bawden,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"In this article, we provide several approaches to the automatic identification of parallel sentences that require sentence-external linguistic context to be correctly translated. Our long-term goal is to automatically construct a test set of context-dependent sentences in order to evaluate machine translation models designed to improve the translation of contextual, discursive phenomena. We provide a discussion and critique that show that current approaches do not allow us to achieve our goal, and suggest that for now evaluating individual phenomena is likely the best solution."
D17-1044,Learning the Structure of Variable-Order {CRF}s: a finite-state perspective,2017,18,1,1,1,8590,thomas lavergne,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"The computational complexity of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with morphologically rich languages or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies, we propose new ways to perform feature selection at large scale and report experimental results where we outperform strong baselines on a tagging task."
2017.jeptalnrecital-demo.11,Traitement automatique de la langue biom{\\'e}dicale au {LIMSI} (Biomedical language processing at {LIMSI}),2017,-1,-1,3,0,23937,christopher norman,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Nous proposons des d{\'e}monstrations de trois outils d{\'e}velopp{\'e}s par le LIMSI en traitement automatique des langues appliqu{\'e} au domaine biom{\'e}dical : la d{\'e}tection de concepts m{\'e}dicaux dans des textes courts, la cat{\'e}gorisation d{'}articles scientifiques pour l{'}assistance {\`a} l{'}{\'e}criture de revues syst{\'e}matiques, et l{'}anonymisation de textes cliniques."
2017.jeptalnrecital-court.28,D{\\'e}tection de concepts et granularit{\\'e} de l{'}annotation (Concept detection and annotation granularity ),2017,-1,-1,2,0,8591,pierre zweigenbaum,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Nous nous int{\'e}ressons ici {\`a} une t{\^a}che de d{\'e}tection de concepts dans des textes sans exigence particuli{\`e}re de passage par une phase de d{\'e}tection d{'}entit{\'e}s avec leurs fronti{\`e}res. Il s{'}agit donc d{'}une t{\^a}che de cat{\'e}gorisation de textes multi{\'e}tiquette, avec des jeux de donn{\'e}es annot{\'e}s au niveau des textes entiers. Nous faisons l{'}hypoth{\`e}se qu{'}une annotation {\`a} un niveau de granularit{\'e} plus fin, typiquement au niveau de l{'}{\'e}nonc{\'e}, devrait am{\'e}liorer la performance d{'}un d{\'e}tecteur automatique entra{\^\i}n{\'e} sur ces donn{\'e}es. Nous examinons cette hypoth{\`e}se dans le cas de textes courts particuliers : des certificats de d{\'e}c{\`e}s o{\`u} l{'}on cherche {\`a} reconna{\^\i}tre des diagnostics, avec des jeux de donn{\'e}es initialement annot{\'e}s au niveau du certificat entier. Nous constatons qu{'}une annotation au niveau de la Â« ligne Â» am{\'e}liore effectivement les r{\'e}sultats, mais aussi que le simple fait d{'}appliquer au niveau de la ligne un classifieur entra{\^\i}n{\'e} au niveau du texte est d{\'e}j{\`a} une source d{'}am{\'e}lioration."
W16-6113,Hybrid methods for {ICD}-10 coding of death certificates,2016,3,7,2,0,8591,pierre zweigenbaum,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
W16-5107,A Dataset for {ICD}-10 Coding of Death Certificates: Creation and Usage,2016,0,4,1,1,8590,thomas lavergne,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"Very few datasets have been released for the evaluation of diagnosis coding with the International Classification of Diseases, and only one so far in a language other than English. This paper describes a large-scale dataset prepared from French death certificates, and the problems which needed to be solved to turn it into a dataset suitable for the application of machine learning and natural language processing methods of ICD-10 coding. The dataset includes the free-text statements written by medical doctors, the associated meta-data, the human coder-assigned codes for each statement, as well as the statement segments which supported the coder{'}s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task."
W16-5109,Supervised classification of end-of-lines in clinical text with no manual annotation,2016,0,0,3,0,8591,pierre zweigenbaum,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"In some plain text documents, end-of-line marks may or may not mark the boundary of a text unit (e.g., of a paragraph). This vexing problem is likely to impact subsequent natural language processing components, but is seldom addressed in the literature. We propose a method which uses no manual annotation to classify whether end-of-lines must actually be seen as simple spaces (soft line breaks) or as true text unit boundaries. This method, which includes self-training and co-training steps based on token and line length features, achieves 0.943 F-measure on a corpus of short e-books with controlled format, F=0.904 on a random sample of 24 clinical texts with soft line breaks, and F=0.898 on a larger set of mixed clinical texts which may or may not contain soft line breaks, a fairly high value for a method with no manual annotation."
W16-2304,{LIMSI}@{WMT}{'}16: Machine Translation of News,2016,-1,-1,6,0.35874,5598,alexandre allauzen,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,19,0,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
2016.jeptalnrecital-poster.7,Une cat{\\'e}gorisation de fins de lignes non-supervis{\\'e}e (End-of-line classification with no supervision),2016,-1,-1,3,0,8591,pierre zweigenbaum,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Dans certains textes bruts, les marques de fin de ligne peuvent marquer ou pas la fronti{\`e}re d{'}une unit{\'e} textuelle (typiquement un paragraphe). Ce probl{\`e}me risque d{'}influencer les traitements subs{\'e}quents, mais est rarement trait{\'e} dans la litt{\'e}rature. Nous proposons une m{\'e}thode enti{\`e}rement non-supervis{\'e}e pour d{\'e}terminer si une fin de ligne doit {\^e}tre vue comme un simple espace ou comme une v{\'e}ritable fronti{\`e}re d{'}unit{\'e} textuelle, et la testons sur un corpus de comptes rendus m{\'e}dicaux. Cette m{\'e}thode obtient une F-mesure de 0,926 sur un {\'e}chantillon de 24 textes contenant des lignes repli{\'e}es. Appliqu{\'e}e sur un {\'e}chantillon plus grand de textes contenant ou pas des lignes repli{\'e}es, notre m{\'e}thode la plus prudente obtient une F-mesure de 0,898, valeur {\'e}lev{\'e}e pour une m{\'e}thode enti{\`e}rement non-supervis{\'e}e."
W15-3016,{LIMSI}@{WMT}{'}15 : Translation Task,2015,16,5,8,0,8610,benjamin marie,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes LIMSIxe2x80x99s submissions to the shared WMTxe2x80x9915 translation task. We report results for French-English, Russian-English in both directions, as well as for Finnish-into-English. Our submissions use NCODE and MOSES along with continuous space translation models in a post-processing step. The main novelties of this yearxe2x80x99s participation are the following: for Russian-English, we investigate a tailored normalization of Russian to translate into English, and a two-step process to translate first into simplified Russian, followed by a conversion into inflected Russian. For French-English, the challenge is domain adaptation, for which only monolingual corpora are available. Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side."
2015.jeptalnrecital-long.4,"Oublier ce qu{'}on sait, pour mieux apprendre ce qu{'}on ne sait pas : une {\\'e}tude sur les contraintes de type dans les mod{\\`e}les {CRF}",2015,-1,-1,3,0.757576,36855,nicolas pecheux,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Quand on dispose de connaissances a priori sur les sorties possibles d{'}un probl{\`e}me d{'}{\'e}tiquetage, il semble souhaitable d{'}inclure cette information lors de l{'}apprentissage pour simplifier la t{\^a}che de mod{\'e}lisation et acc{\'e}l{\'e}rer les traitements. Pourtant, m{\^e}me lorsque ces contraintes sont correctes et utiles au d{\'e}codage, leur utilisation lors de l{'}apprentissage peut d{\'e}grader s{\'e}v{\`e}rement les performances. Dans cet article, nous {\'e}tudions ce paradoxe et montrons que le manque de contraste induit par les connaissances entra{\^\i}ne une forme de sous-apprentissage qu{'}il est cependant possible de limiter."
2015.jeptalnrecital-court.29,Etiquetage morpho-syntaxique en domaine de sp{\\'e}cialit{\\'e}: le domaine m{\\'e}dical,2015,-1,-1,2,0,37988,christelle rabary,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"L{'}{\'e}tiquetage morpho-syntaxique est une t{\^a}che fondamentale du Traitement Automatique de la Langue, sur laquelle reposent souvent des traitements plus complexes tels que l{'}extraction d{'}information ou la traduction automatique. L{'}{\'e}tiquetage en domaine de sp{\'e}cialit{\'e} est limit{\'e} par la disponibilit{\'e} d{'}outils et de corpus annot{\'e}s sp{\'e}cifiques au domaine. Dans cet article, nous pr{\'e}sentons le d{\'e}veloppement d{'}un corpus clinique du fran{\c{c}}ais annot{\'e} morpho-syntaxiquement {\`a} l{'}aide d{'}un jeu d{'}{\'e}tiquettes issus des guides d{'}annotation French Treebank et Multitag. L{'}analyse de ce corpus nous permet de caract{\'e}riser le domaine clinique et de d{\'e}gager les points cl{\'e}s pour l{'}adaptation d{'}outils d{'}analyse morpho-syntaxique {\`a} ce domaine. Nous montrons {\'e}galement les limites d{'}un outil entra{\^\i}n{\'e} sur un corpus journalistique appliqu{\'e} au domaine clinique. En perspective de ce travail, nous envisageons une application du corpus clinique annot{\'e} pour am{\'e}liorer l{'}{\'e}tiquetage morpho-syntaxique des documents cliniques en fran{\c{c}}ais."
W14-4907,Optimizing annotation efforts to build reliable annotated corpora for training statistical models,2014,17,5,2,0.428571,5675,cyril grouin,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"Creating high-quality manual annotations on text corpus is time-consuming and often requires the work of experts. In order to explore methods for optimizing annotation efforts, we study three key time burdens of the annotation process: (i) multiple annotations, (ii) consensus annotations, and (iii) careful annotations. Through a series of experiments using a corpus of clinical documents annotated for personally identifiable information written in French, we address each of these aspects and draw conclusions on how to make the most of an annotation effort."
W14-3330,{LIMSI} @ {WMT}{'}14 Medical Translation Task,2014,-1,-1,7,0.757576,36855,nicolas pecheux,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
lavergne-etal-2014-automatic,Automatic language identity tagging on word and sentence-level in multilingual text sources: a case-study on {L}uxembourgish,2014,9,1,1,1,8590,thomas lavergne,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Luxembourgish, embedded in a multilingual context on the divide between Romance and Germanic cultures, remains one of Europe{'}s under-described languages. This is due to the fact that the written production remains relatively low, and linguistic knowledge and resources, such as lexica and pronunciation dictionaries, are sparse. The speakers or writers will frequently switch between Luxembourgish, German, and French, on a per-sentence basis, as well as on a sub-sentence level. In order to build resources like lexicons, and especially pronunciation lexicons, or language models needed for natural language processing tasks such as automatic speech recognition, language used in text corpora should be identified. In this paper, we present the design of a manually annotated corpus of mixed language sentences as well as the tools used to select these sentences. This corpus of difficult sentences was used to test a word-based language identification system. This language identification system was used to select textual data extracted from the web, in order to build a lexicon and language models. This lexicon and language model were used in an Automatic Speech Recognition system for the Luxembourgish language which obtain a 25{\textbackslash}{\%} WER on the Quaero development data."
W13-2321,Automatic Named Entity Pre-annotation for Out-of-domain Human Annotation,2013,23,3,3,0,5280,sophie rosset,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. Our study design includes two different corpora, three pre-annotation schemes linked to two annotation levels, both expert and novice annotators, a questionnaire-based subjective assessment and a corpus-based quantitative assessment. We observe that preannotation helps in all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome."
W13-2204,{LIMSI} @ {WMT}13,2013,27,0,5,0,38567,alexander allauzen,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes LIMSIxe2x80x99s submissions to the shared WMTxe2x80x9913 translation task. We report results for French-English, German-English and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this yearxe2x80x99s participation are the following: our first participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artificial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated."
W13-1733,{LIMSI}{'}s participation to the 2013 shared task on Native Language Identification,2013,9,1,1,1,8590,thomas lavergne,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper describes LIMSIxe2x80x99s participation to the first shared task on Native Language Identification. Our submission uses a Maximum Entropy classifier, using as features character and chunk n-grams, spelling and grammatical mistakes, and lexical preferences. Performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages."
F13-1033,A fully discriminative training framework for Statistical Machine Translation (Un cadre d{'}apprentissage int{\\'e}gralement discriminant pour la traduction statistique) [in {F}rench],2013,0,1,1,1,8590,thomas lavergne,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-3140,Joint {WMT} 2012 Submission of the {QUAERO} Project,2012,37,5,9,0,3519,markus freitag,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in Bleu and 3.4 points in Ter compared to the best single system."
W12-3141,{LIMSI} @ {WMT}12,2012,23,1,2,1,40944,haison le,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an on-the-fly translation model."
gahbiche-braham-etal-2012-joint,Joint Segmentation and {POS} Tagging for {A}rabic Using a {CRF}-based Classifier,2012,18,11,3,0,40027,souhir gahbichebraham,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Arabic is a morphologically rich language, and Arabic texts abound of complex word forms built by concatenation of multiple subparts, corresponding for instance to prepositions, articles, roots prefixes, or suffixes. The development of Arabic Natural Language Processing applications, such as Machine Translation (MT) tools, thus requires some kind of morphological analysis. In this paper, we compare various strategies for performing such preprocessing, using generic machine learning techniques. The resulting tool is compared with two open domain alternatives in the context of a statistical MT task and is shown to be faster than its competitors, with no significant difference in MT quality."
F12-2044,Rep{\\'e}rage des entit{\\'e}s nomm{\\'e}es pour l{'}arabe : adaptation non-supervis{\\'e}e et combinaison de syst{\\`e}mes (Named Entity Recognition for {A}rabic : Unsupervised adaptation and Systems combination) [in {F}rench],2012,0,0,3,0,40027,souhir gahbichebraham,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
W11-2135,{LIMSI} @ {WMT}11,2011,19,17,10,0.316142,5598,alexandre allauzen,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the French-English and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple filtering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10-gram SOUL model. We also briefly report experiments with several alternatives to the standard n-best MERT procedure, leading to a significant speed-up."
W11-2168,From n-gram-based to {CRF}-based Translation Models,2011,46,12,1,1,8590,thomas lavergne,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"A major weakness of extant statistical machine translation (SMT) systems is their lack of a proper training procedure. Phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many. In this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures. The tractability of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems."
2011.iwslt-evaluation.7,{LIMSI}{'}s experiments in domain adaptation for {IWSLT}11,2011,20,13,1,1,8590,thomas lavergne,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French using the in-house n-code system, which implements the n-gram based approach to Machine Translation. This framework not only allows to achieve state-of-the-art results for this language pair, but is also appealing due to its conceptual simplicity and its use of well understood statistical language models. Using this approach, we compare several ways to adapt our existing systems and resources to the TED task with mixture of language models and try to provide an analysis of the modest gains obtained by training a log linear combination of inand out-of-domain models."
2011.iwslt-evaluation.15,Advances on spoken language translation in the Quaero program,2011,25,2,6,0,43221,karim boudahmane,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality."
P10-1052,Practical Very Large Scale {CRF}s,2010,31,253,1,1,8590,thomas lavergne,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a l penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets."
P09-2063,Introduction of a new paraphrase generation tool based on {M}onte-{C}arlo sampling,2009,11,14,2,0,10791,jonathan chevelu,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"We propose a new specifically designed method for paraphrase generation based on Monte-Carlo sampling and show how this algorithm is suitable for its task. Moreover, the basic algorithm presented here leaves a lot of opportunities for future improvement. In particular, our algorithm does not constraint the scoring function in opposite to Viterbi based decoders. It is now possible to use some global features in paraphrase scoring functions. This algorithm opens new outlooks for paraphrase generation and other natural language processing applications like statistical machine translation."
