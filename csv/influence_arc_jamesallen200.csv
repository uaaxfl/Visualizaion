2020.lrec-1.396,S18-2028,1,0.906078,"ions. Most important, these constraints allow us to flag cases where the definition is likely unusable, either because of parsing errors or because it is a poor definition of the concept. 3.2 Identifying the Semantic Roles As discussed above, there is a relationship between the semantic roles of a target word sense and those used in its definition. One of the most common cases is that the elided roles in the definition correspond to the unfilled roles in the target. For instance, from the definition < > cause < > to die, we can lift the two elided roles to the target word, i.e., < > kill < >. Allen and Teng (2018) reported good accuracy in using gaps to identify semantic roles. We adopted and expanded on this approach. We identified a number of ways such elided roles (IMPROs) can be realized in a parse. For instance, for outweigh, the IMPROs occur in an embedded clause in the parse of its definition be heavier than. Some of these patterns and corresponding parse skeletons are shown in Table 4 and Figure 5. The rules for identifying the roles are ordered such that where multiple rules are applicable, only the one with the highest priority would be selected. We also identified other patterns commonly use"
2020.lrec-1.396,P98-1013,0,0.579897,"es no relation between similar verbs such as contradict, refute and disagree and no connections between kill and die. OntoNotes (Weischedel et al., 2011) provides a sense inventory by grouping WordNet senses with high interannotator agreement. PropBank frames are linked to OntoNotes senses. Some of the OntoNotes senses are further clustered into the Omega 5 ontology. However, many of the verb senses and most of the PropBank frames are not mapped to the ontology. For instance, Omega 5 contains entries for disagree, but not contradict or refute. Similarly it contains kill but not die. Framenet (Baker et al., 1998) organizes its lexicon around conceptual frames capturing everyday knowledge of events. While it provides informal definitions of these classes, there are no axioms. In addition, unlike the other resources, the semantic roles in Framenet are idiosyncratic to each class. TRIPS provides detailed lexical information integrated with an ontology. It uses a principled set of semantic roles (Allen & Teng, 2018) with selectional preferences expressed in the ontology, as well as linking templates. The main weaknesses are the coverage and the lack of axioms. A significant advantage of TRIPS is that it i"
2020.lrec-1.396,W13-2322,0,0.0127344,"ioms for broad classes of verbs. However, VerbNet organizes verbs into only 329 verb classes. Because each class covers a wide range of diverse verbs, VerbNet does not provide a finegrained semantics and the axioms defined for each class are necessarily quite abstract. For example, abate, gray, hybridize and reverse are all in the same class and share the same defining axiom. Still, VerbNet represents the state of the art in providing axioms related to verbs. Propbank (Palmer et al., 2005) is commonly used as a taxonomy of verb senses, and is used in the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). Each entry identifies a set of semantic roles related more to syntax than semantics. While Propbank identifies senses for individual verbs, it does not group similar verbs that would identify closely related concepts or categorize the senses into an ontology. For instance, PropBank indicates 3243 Lexical Resource Coverage Pros Cons (Verbs; Sense Types; Avg # Senses/Verb) WordNet 11531; 13782; 2.17 VerbNet 4577; 329; 1.48 Propbank/ 2681; 6267; 2.34 Ontonotes Framenet TRIPS COLLIE-V Extensive coverage, hypernym hierarchy, Hypernym hierarchy is only a example sentences and templates partial ont"
2020.lrec-1.396,W08-2205,0,0.10956,"Missing"
2020.lrec-1.396,D16-1235,0,0.0156962,"for being too fine-grained) and is fairly successful in organizing entries with similar definitions into a single ontology type. We further evaluated COLLIE along the following dimensions. 4.1 The Ontology As discussed above, our system makes use of WordNet and the TRIPS ontology in parallel, using the TRIPS ontology as an upper ontology and augmenting it with the WordNet hypernym hierarchy via mappings between the two. For evaluation we used two word-similarity datasets: (i) SimLex-999 (Hill et al., 2014) consists of 666 noun pairs, 222 verb pairs, and 111 adjective pairs; (ii) SimVerb-3500 (Gerz et al., 2016) consists of 3500 verb pairs. Both datasets focus specifically on similarity rather than relatedness. For example, while coffee and cup are highly related, the two words represent very different concepts. We evaluated the similarity between concept pairs using the WuP hierarchy similarity measure (Wu & Palmer, 1994). For each pair of words, we consider the highest similarity score over all possible pairs of senses. We chose WuP because it relies entirely on the structure of the hierarchy. We computed the WuP scores for verbs in WordNet and TRIPS respectively, using the SimVerb test set and the"
2020.lrec-1.396,J15-4004,0,0.0763325,"Missing"
2020.lrec-1.396,P14-2050,0,0.0133859,"th respect to several word embedding and ontology-based approaches over the SimLex-999 data set Spearman's correlation measured between each of these scores and the human annotated similarity scores. The similarity scores using the TRIPS ontology agree substantially more with human judgement than the scores computed using WordNet. In order to distinguish between lexical mappings and WordNet mappings in TRIPS, we added 1 to the depth of all WordNet-to-TRIPS mappings. Table 7 compares the ontology-based measures and several main word-embedding based approaches over the full SimLex-999 data set (Levy & Goldberg (2014), Glove (Pennington et al., 2014) and Swartz et al. (2015)). This is not a clean comparison though, for COLLIE-V (integrated with TRIPS) only concerns verbs, but the reported wordembedding results involve three parts-of-speech (nouns, verbs and adjectives). So we used WuP similarity scores computed using WordNet, TRIPS and a hybrid ontology in which noun pairs were evaluated using WordNet and verb and adjective pairs were evaluated using TRIPS. While Swartz et al. (2015) had the best correlation with human judgement, the ontology-based measures, using solely the structural information in the o"
2020.lrec-1.396,J05-1004,0,0.313238,"for semantic information. VerbNet provides extensive information on semantic roles, linking templates, and causal-temporal axioms for broad classes of verbs. However, VerbNet organizes verbs into only 329 verb classes. Because each class covers a wide range of diverse verbs, VerbNet does not provide a finegrained semantics and the axioms defined for each class are necessarily quite abstract. For example, abate, gray, hybridize and reverse are all in the same class and share the same defining axiom. Still, VerbNet represents the state of the art in providing axioms related to verbs. Propbank (Palmer et al., 2005) is commonly used as a taxonomy of verb senses, and is used in the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). Each entry identifies a set of semantic roles related more to syntax than semantics. While Propbank identifies senses for individual verbs, it does not group similar verbs that would identify closely related concepts or categorize the senses into an ontology. For instance, PropBank indicates 3243 Lexical Resource Coverage Pros Cons (Verbs; Sense Types; Avg # Senses/Verb) WordNet 11531; 13782; 2.17 VerbNet 4577; 329; 1.48 Propbank/ 2681; 6267; 2.34 Ontonotes Framene"
2020.lrec-1.396,D14-1162,0,0.0836773,"dding and ontology-based approaches over the SimLex-999 data set Spearman's correlation measured between each of these scores and the human annotated similarity scores. The similarity scores using the TRIPS ontology agree substantially more with human judgement than the scores computed using WordNet. In order to distinguish between lexical mappings and WordNet mappings in TRIPS, we added 1 to the depth of all WordNet-to-TRIPS mappings. Table 7 compares the ontology-based measures and several main word-embedding based approaches over the full SimLex-999 data set (Levy & Goldberg (2014), Glove (Pennington et al., 2014) and Swartz et al. (2015)). This is not a clean comparison though, for COLLIE-V (integrated with TRIPS) only concerns verbs, but the reported wordembedding results involve three parts-of-speech (nouns, verbs and adjectives). So we used WuP similarity scores computed using WordNet, TRIPS and a hybrid ontology in which noun pairs were evaluated using WordNet and verb and adjective pairs were evaluated using TRIPS. While Swartz et al. (2015) had the best correlation with human judgement, the ontology-based measures, using solely the structural information in the ontologies, were still fairly comp"
2020.lrec-1.396,K15-1026,0,0.0498585,"Missing"
C04-1055,swift-etal-2004-semi,1,\N,Missing
C04-1055,N04-1013,0,\N,Missing
C04-1055,J93-2004,0,\N,Missing
C04-1055,A00-2008,0,\N,Missing
C04-1055,W01-0521,1,\N,Missing
C04-1055,W04-0214,1,\N,Missing
C04-1055,J03-4003,0,\N,Missing
C04-1055,P03-1014,0,\N,Missing
C04-1055,P99-1010,0,\N,Missing
D15-1115,W08-2227,1,0.851693,"Missing"
D15-1115,W15-0103,1,0.80466,"JJ: Mary is 5 feet. • RB: Philip is driving 60mph. Extreme: Excessive Indicating having enough or too much of a quality or quantity. Polarity: Base form expression of +/- quality. Measurement: Indicating a measurement on a scale. Examples Table 1: The predicate types defined under our framework. Figure 1: A full annotation of a sample predicate-argument structure under the described semantic framework. tree, POS tag and lemma of two adjacent words, similarity features from WordNet (Miller, 1995), word polarity features, and most importantly ‘attribute concepts’ for words which are adjectives (Bakhshandeh and Allen, 2015). The ‘attribute concepts’ are the different properties that an adjective can describe, for instance ‘height’ and ‘thickness’ are the attributes of the adjective ‘gangling’. Last but not least, we include the conjunction of all these features. ing (Punyakanok et al., 2008): given a predicate, we collect all constituents in the sentence to build a set of plausible candidate arguments. As a result, each predicate has a set of candidate arguments which should be labeled with their argument types and be assigned with a semantic role edge. Here we jointly train two logistic regression classifiers f"
D15-1115,W13-2322,0,0.0708862,"comparatives, including all different types of comparison within comparaIntroduction Representing the meaning of text has long been a focus in linguistics and deriving computational models of meaning has been pursued by various semantic tasks such as semantic parsing. Deep semantic parsing (as opposed to shallow semantic parsing, such as semantic role labeling) aims to map a sentence in natural language into its corresponding formal meaning representation (Zelle and Mooney, 1996; Berant and Liang, 2014). There has been a renewed interest in deeper semantic representations of natural language (Banarescu et al., 2013) in NLP community. Opendomain semantic representations enable inference and reasoning, which is required for many language understanding tasks such as reading comprehension tests and open-domain question answering. Comparison is a common way for expressing differences in sentiment and other prop993 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 993–1002, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. tives, superlatives, equatives, excessives, and assetives, and the way they are related to their correspon"
D15-1115,P14-1133,0,0.063437,"s of the structure of comparison in natural language. This framework enables deeper representation of semantics of comparatives, including all different types of comparison within comparaIntroduction Representing the meaning of text has long been a focus in linguistics and deriving computational models of meaning has been pursued by various semantic tasks such as semantic parsing. Deep semantic parsing (as opposed to shallow semantic parsing, such as semantic role labeling) aims to map a sentence in natural language into its corresponding formal meaning representation (Zelle and Mooney, 1996; Berant and Liang, 2014). There has been a renewed interest in deeper semantic representations of natural language (Banarescu et al., 2013) in NLP community. Opendomain semantic representations enable inference and reasoning, which is required for many language understanding tasks such as reading comprehension tests and open-domain question answering. Comparison is a common way for expressing differences in sentiment and other prop993 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 993–1002, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Ling"
D15-1115,D14-1159,0,0.0184155,"es Given an input sentence, we want to predict the predicate operators, their semantic roles, and arguments. We decompose this problem into three sub-problems: • Labeling predicate candidates using a multiclass classifier • For each predicate, considering the set of all possible argument spans: – Use a classifier for predicting the role type label – Use a classifier for predicting the argument type label Our overall approach, to be described in this section, is similar to the works on joint inference with global constraints for learning event relations and process structures (Do et al., 2012; Berant et al., 2014). Predicting Predicates: The first step in comparison structure prediction is to identify and label the predicates. For this purpose we train a multi-class classifier that labels all one-word constituents in the sentence with any of predicate types in Table 1 or None (indicating that the constituent is not a predicate). The set of all possible predicate labels is named P . We used various features for training the predicate classifier: we extract the lemma and POS tag of the word, POS tag of children, siblings, parent and root of the sentence in the dependency Semantic Roles: Each predicate is"
D15-1115,W08-2222,0,0.177153,"entity. Comparison can happen in very simple structures such as ‘John is taller than Sam’, or more complicated constructions such as ‘The table is longer than the sofa is wide’. So far the computational semantics of comparatives and how they affect the meaning of text has not been studied effectively. That is, the difference between the existing semantic and syntactic representation of comparatives is not distinctive enough for enabling deeper understanding of a sentence. For instance, the general logical form representation of the sentence ‘John is taller than Susan’ using the Boxer system (Bos, 2008) is the following: Comparison is one of the most important phenomena in language for expressing objective and subjective facts about various entities. Systems that can understand and reason over comparative structure can play a major role in the applications which require deeper understanding of language. In this paper we present a novel semantic framework for representing the meaning of comparative structures in natural language, which models comparisons as predicate-argument pairs interconnected with semantic roles. Our framework supports not only adjectival, but also adverbial, nominal, and"
D15-1115,W02-1001,0,0.0958343,"e conjunction of all these features. ing (Punyakanok et al., 2008): given a predicate, we collect all constituents in the sentence to build a set of plausible candidate arguments. As a result, each predicate has a set of candidate arguments which should be labeled with their argument types and be assigned with a semantic role edge. Here we jointly train two logistic regression classifiers for predicting semantic role type and argument type of a predicate-argument pair, using argument identification features from (Punyakanok et al., 2008) and using the structured averaged Perceptron algorithm (Collins, 2002). The role types can be any of the roles from table 2 or None (set R), and the argument types can be any Predicting Roles and Arguments: Given the predicates, one should label the predicateargument role and predict the argument type. Here we take an approach used for semantic role label997 Relation Type Description Example Figure The main role being compared to something else. The main role against which the figure is compared. The ‘plus’ and ‘times’ roles, signifying an amount of difference on a degree. The explicit expression of the domain/population in which the comparison takes place The r"
D15-1115,D12-1062,0,0.0310794,"mparison Structures Given an input sentence, we want to predict the predicate operators, their semantic roles, and arguments. We decompose this problem into three sub-problems: • Labeling predicate candidates using a multiclass classifier • For each predicate, considering the set of all possible argument spans: – Use a classifier for predicting the role type label – Use a classifier for predicting the argument type label Our overall approach, to be described in this section, is similar to the works on joint inference with global constraints for learning event relations and process structures (Do et al., 2012; Berant et al., 2014). Predicting Predicates: The first step in comparison structure prediction is to identify and label the predicates. For this purpose we train a multi-class classifier that labels all one-word constituents in the sentence with any of predicate types in Table 1 or None (indicating that the constituent is not a predicate). The set of all possible predicate labels is named P . We used various features for training the predicate classifier: we extract the lemma and POS tag of the word, POS tag of children, siblings, parent and root of the sentence in the dependency Semantic Ro"
D15-1115,J08-2005,0,0.017772,"nder our framework. Figure 1: A full annotation of a sample predicate-argument structure under the described semantic framework. tree, POS tag and lemma of two adjacent words, similarity features from WordNet (Miller, 1995), word polarity features, and most importantly ‘attribute concepts’ for words which are adjectives (Bakhshandeh and Allen, 2015). The ‘attribute concepts’ are the different properties that an adjective can describe, for instance ‘height’ and ‘thickness’ are the attributes of the adjective ‘gangling’. Last but not least, we include the conjunction of all these features. ing (Punyakanok et al., 2008): given a predicate, we collect all constituents in the sentence to build a set of plausible candidate arguments. As a result, each predicate has a set of candidate arguments which should be labeled with their argument types and be assigned with a semantic role edge. Here we jointly train two logistic regression classifiers for predicting semantic role type and argument type of a predicate-argument pair, using argument identification features from (Punyakanok et al., 2008) and using the structured averaged Perceptron algorithm (Collins, 2002). The role types can be any of the roles from table"
gallo-etal-2008-production,W04-0304,1,\N,Missing
H89-2052,P89-1026,1,0.207004,"retation space that is superficially integrated into context. If a single specific interpretation is left, then that is taken as the interpretation. Otherwise, general plan recognition, using a system based on Kautz's algorithm, is used to further eliminate the remaining interpretations, or to suggest new interpretations.The prototype system currently operates on a limited knowledge base of speech acts and interpretation rules. All the examples given in this paper can be run, as well as a few others discussed in Hinkelman (1989). More details on the system and its capabilities can be found in Hinkelman & Allen (1989) as well. 7. DISCUSSION AND EXTENSIONS While the feasibility of this approach has been demonstrated by the prototype system, the work opens many possibilities for some interesting extensions that would make the system truly useful in general domains. A couple of these possibilities are discussed here. Perhaps the most interesting result of this work is that we now have a framework in which the intonational and prosodic cues to speech act interpretation can be explored and investigated. Although these cues can be very strong indicators of the intended act, no previous framework was able to inte"
H89-2052,J80-3003,0,0.0956282,"is derivable from the structural properties of the sentence. If this literal act is inappropriate in the context, the general reasoning processes are used to derive the indirect interpretation. Since speech acts are defined in terms of the recognized intentions of the speaker, if the hearer believes that the speaker intended he or she to perform this reasoning to the indirect act, then the utterance counts as an instance of the indirect act. This is a promising approach, and was the starting point for the computational model of indirect speech act recognition developed by Allen and Perrault (Perrault & Allen, 1980, Allen, 1983). This system was organized as shown in Figure 1. The parser produced a literal speech act interpretation based on the syntactic mood of the sentence. Imperative mood sentences (Close the door) became surface requests, declarative mood sentences (I want 387 you to close the door) became surface informs, interrogative mood sentences (Can you close the door?) became requests to inform. The surface act was the input to the plan inference component which used heuristic plan recognition techniques to derive the speakers plan, identifying the intended indirect act along the way. By car"
H89-2052,J80-3002,0,0.0768833,"Missing"
H94-1034,P92-1008,0,0.207237,"Missing"
H94-1034,A88-1019,0,0.0531913,"Missing"
H94-1034,P93-1008,0,0.188596,"Missing"
H94-1034,P94-1041,1,0.589442,"d Schubert, 1991), which is a long term research pmjeet to build a conversationally proficient planning assistant, we are collecting a corpus of problem solving dialogs. The dialogs involve two participants, one who is playing the role of a user and has a certain task to accomplish, and another, who is playing the role of the system by acting as a planning assistant (Gross, Allen and Traum, 1992). The entire corpus consists of 112 dialogs totaling almost eight hours in length and containing about 62,000 words and 6300 speaker turns. These dialogs have been segmented into utterance files (c.f. Heeman and Allen, 1994¢); words have been transcribed and the speech repairs have been annotated. For a training set, we use 40 of the dialogs, consisting of 24,000 words; and for testing, 7 of the dialogs, consisting of 5800 words. In order to provide a large training corpus for the statistical model, we use a tagged version of the Brown corpus, from the Penn Treebank (Marcus, Santorini and Marcinklewicz, 1993). We removed all p u n c h ~ o n in order to more closely approximate unsegmented spoken speech. This corpus provides us with category transition probabilities fur fluent speech. These probabilities have als"
H94-1034,P83-1019,0,0.638635,"Missing"
H94-1034,P93-1007,0,0.357392,"Missing"
H94-1034,J93-2006,0,0.0699002,"ns; (2) we separated uses of ""to"" as a preposition from in me as part of a to-infinilive; (3) rather than classify verbs by tense, we classified them into four groups, conjugations of ""be"", conjugations of ""have"", verbs that are followed by a to-infinitive, and verbs that are followed immediately by another verb. 2The l~noved text and editing terms might still contain pragmatical information, as the followingexample displays, ""Peter was.., well...he was fired/' are currently uying to tag and the previous categories. Good partof-speech results can be obtained using only the preceding category (Weischedel et al., 1993), which is what we will be using. In this case, the number of states of the Markov model will be N, where N is the number of tags. By making the Markov assumption, we can use the Viterbi Algorithrn to find a maximum probability path in linear time. Figure 1 gives a simplied view of a Markov model for part-of-speech tagging, where Ci is a possible category for the ith word, wi, and Ci+~ is a possible category for word wi÷t. The category transition probability is simply the probability of category Ci+, following category Ci, which is written as P(Ci+tICi), and the probability of word wi+, given"
H94-1034,J93-2004,0,\N,Missing
H94-1034,H93-1008,0,\N,Missing
J12-3002,C04-1181,0,0.034775,"ity of Rochester, Rochester, New York 14627. E-mail: swift@cs.rochester.edu. 420 Meliora, University of Rochester, Rochester, New York 14627. E-mail: mtan@bcs.rochester.edu. Submission received: 24 August 2009; revised submission received: 6 May 2010; accepted for publication: 20 September 2010. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 pragmatic aspects of the context in which they occurred. In recent years natural language processing (NLP) researchers have been working to incorporate visual and other context into their models and systems (DeVault and Stone 2004; Gabsdil and Lemon 2004; Schuler, Wu, and Schwartz 2009). This is consistent with the growing evidence in psycholinguistics that human language production crucially depends on such aspects of context. To take this NLP research further, there is a need for more corpora that include both variation in, and annotation of, visual and pragmatic context. There are still many open questions that span computational linguistics and psycholinguistics concerning how natural language and context are related. One core question at the intersection of these areas is how the inherent difﬁculty of describing a"
J12-3002,P04-1044,0,0.0217347,"ter, New York 14627. E-mail: swift@cs.rochester.edu. 420 Meliora, University of Rochester, Rochester, New York 14627. E-mail: mtan@bcs.rochester.edu. Submission received: 24 August 2009; revised submission received: 6 May 2010; accepted for publication: 20 September 2010. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 pragmatic aspects of the context in which they occurred. In recent years natural language processing (NLP) researchers have been working to incorporate visual and other context into their models and systems (DeVault and Stone 2004; Gabsdil and Lemon 2004; Schuler, Wu, and Schwartz 2009). This is consistent with the growing evidence in psycholinguistics that human language production crucially depends on such aspects of context. To take this NLP research further, there is a need for more corpora that include both variation in, and annotation of, visual and pragmatic context. There are still many open questions that span computational linguistics and psycholinguistics concerning how natural language and context are related. One core question at the intersection of these areas is how the inherent difﬁculty of describing an end-goal (i.e., its co"
J12-3002,gallo-etal-2008-production,1,0.927163,"porating pragmatic feedback about the visual world early in the parsing process was shown to substantially improve parsing efﬁciency as well as allowing parsing decisions to accurately reﬂect the visual world (Aist et al. 2006). Also using Fruit Carts, a dialogue system using continuous understanding was shown to be faster than, and preferred to, a counterpart that used a traditional pipeline architecture but was otherwise identical (Aist et al. 2007). For psycholinguistic research, Fruit Carts has also been used for studying the relationship between bi-clausal structure and theme complexity (Gallo et al. 2008) and testing hypotheses regarding the relationship of information in a message, resource limitations, and sentence production (Gallo, Jaeger, and Smyth 2008). 6. Discussion and Conclusions Fruit Carts also has a number of other advantages as well as some limitations. First, Fruit Carts provides ample temporary or local ambiguity in its utterances, a central challenge for continuous understanding systems and a classic target of research in psycholinguistics (for a review see Altmann [1998]). In a typical sequence such as okay take a ... small triangle with a dot on the corner (Appendix A, [D3])"
J12-3002,J09-3001,0,0.0241658,"Missing"
J12-3002,W06-1410,0,0.0144492,"a need for more corpora that include both variation in, and annotation of, visual and pragmatic context. There are still many open questions that span computational linguistics and psycholinguistics concerning how natural language and context are related. One core question at the intersection of these areas is how the inherent difﬁculty of describing an end-goal (i.e., its codability) will affect the structure and content of referring expressions and the referential strategy speakers adopt. Referential strategies are a topic of growing interest in natural language generation. In recent work, Viethen and Dale (2006) demonstrated that even when describing simple grid layouts, people adopt different referential strategies, due perhaps to proximity to landmarks (and hence codability): the orange drawer below the two yellow drawers, in contrast to the yellow drawer in the third column from the left second from the top. For systems to produce humanlike references in these situations, existing methods of reference generation will need to be modiﬁed or extended to include better models of the choice of referential strategies (Viethen and Dale 2006). Such models can also be expected to improve reference resoluti"
J18-1003,P92-1005,0,0.697689,"a sentence using an unscoped LF of the following general form: Every(x, Child(x, y), ), A(y, Politician(y), ), Run(x) (4) To scope this LF, at each step, a quantifier is picked and the main predication (i.e., Run(x)) or the partially scoped formula built so far is fused to its body hole: Step 1. Every(x, Child(x, y), Run(x)) Step 2. A(y, Politician(y), Every(x, Child(x, y), Run(x))) (5) By picking quantifiers in different orders, different scopings are generated. Next, the notion of constraints was introduced into the domain of scope underspecified semantics. For example, Quasi Logical Form (Alshawi and Crouch 1992) allows for constraints such as A > Every to be used to force one quantifier to rest within the scope of another. By inventing some machinery that allows for Discourse Representation Theory (Kamp 1981) to support scope underspecification, Underspecified Discourse Representation Theory (or UDRT) (Reyle 1993) takes the notion of constraint-based underspecification to a new level. UDRT introduces a complex system of constraints, that, among other things, can define a maximum and a minimum range for the scope of a quantifier relative to other scope bearing elements. It is fair to say that Reyle’s"
J18-1003,copestake-flickinger-2000-open,0,0.050146,"Missing"
J18-1003,P01-1019,0,0.265156,"Missing"
J18-1003,P04-1032,0,0.0628044,"ort towards defining a notion of well-formedness within the context of these frameworks. The goal has been to define a subset of URs, the so-called well-formed UR, for which the satisfiability problem becomes tractable. For example, Niehren and Thater (2003) defined the notion of (weak) net to characterize such a subset. Well-formedness was also intended to bridge the gap between these underspecification formalisms; the hope was that the differences between these formalisms disappear and they become equivalent once restricted to well-formed structures. As seen in Niehren and Thater (2003) and Fuchss et al. (2004), the problem with those efforts on defining a notion of well-formedness is that their satisfaction of both properties was only empirically supported, and hence the correctness of those statements has remained a conjecture. In better words, first, there was no mathematical proof to show that nets enforce the equivalence of qeq vs. dominance relations, the two different types of constraint used in MRS vs. Dominance Constraints/Hole Semantics, and second, although they were proved to be tractable, there was no convincing linguistic justification as to why nets cover all URs corresponding to cohe"
J18-1003,J03-1004,0,0.0812889,"Missing"
J18-1003,J87-1005,0,0.819072,"Missing"
J18-1003,P03-1047,0,0.0848282,"Missing"
J18-1003,S12-1022,1,0.733146,"Missing"
J18-1003,J82-1003,0,0.498732,"Missing"
J18-1003,D09-1152,0,0.0307979,"Missing"
J99-4003,P92-1008,0,0.0689893,"Missing"
J99-4003,P90-1003,0,0.251046,"Phrases As illustrated above, understanding a speaker&apos;s turn necessitates segmenting it into individual utterance units. However, there is no consensus as to how to define an utterance unit (Traum and Heeman 1997). The manner in which speakers break their speech into intonational phrases undoubtedly plays a major role in its definition. Intonational phrase endings are signaled through variations in the pitch contour, segmental lengthening, and pauses. Beach (1991) demonstrated that hearers can use intonational information early on in sentence processing to help resolve syntactic ambiguities. Bear and Price (1990) showed that a parser can use automatically extracted intonational phrasing to reduce ambiguity and improve efficiency. Ostendorf, Wightman, and Veilleux (1993) used hand-labeled intonational phrasing to do syntactic disambiguation and achieved performance comparable to that of human listeners. Due to their significance, we will focus on the task of detecting intonational phrase boundaries. 1.2 Speech Repairs The on-line nature of spoken dialogue forces conversants to sometimes start speaking before they are sure of what they want to say. Hence, the speaker might need to go back and repeat or"
J99-4003,H92-1026,0,0.0299252,"e n is a numeric constant. For a categorical variable C, it searches over questions of the form: &quot;is C E S&quot;, w h e r e S is a subset of the possible values of C. We also allow composite questions (Bahl et al. 1989), which are Boolean combinations of elementary questions. 3.3.2 Questions about POS Tags. The context that we use for estimating the probabilities includes both w o r d identities and POS tags. To make effective use of this information, we allow the decision tree algorithm to generalize b e t w e e n w o r d s and POS tags that behave similarly. To learn which ones behave similarly, Black et al. (1992) and M a g e r m a n (1994) used the clustering algorithm of Brown et al. (1992) to build a hierarchical classification tree. Figure 2 gives the tree that we built for the POS tags. The algorithm starts with each POS tag in a separate class and iteratively finds two classes to merge that results in the smallest loss of information about POS adjacency. This continues until only a single class remains. The order in which classes were merged, however, gives a binary tree with the root corresponding to the entire 540 Heeman and Allen ~ Modeling Speakers&apos; Utterances &lt;low&gt; 2 t h e m 157 me 85 us 176"
J99-4003,J92-4003,0,0.0259113,"ons of the form: &quot;is C E S&quot;, w h e r e S is a subset of the possible values of C. We also allow composite questions (Bahl et al. 1989), which are Boolean combinations of elementary questions. 3.3.2 Questions about POS Tags. The context that we use for estimating the probabilities includes both w o r d identities and POS tags. To make effective use of this information, we allow the decision tree algorithm to generalize b e t w e e n w o r d s and POS tags that behave similarly. To learn which ones behave similarly, Black et al. (1992) and M a g e r m a n (1994) used the clustering algorithm of Brown et al. (1992) to build a hierarchical classification tree. Figure 2 gives the tree that we built for the POS tags. The algorithm starts with each POS tag in a separate class and iteratively finds two classes to merge that results in the smallest loss of information about POS adjacency. This continues until only a single class remains. The order in which classes were merged, however, gives a binary tree with the root corresponding to the entire 540 Heeman and Allen ~ Modeling Speakers&apos; Utterances &lt;low&gt; 2 t h e m 157 me 85 us 176 they 89 we 766 648 Figure 3 Binary classification tree that encodes the persona"
J99-4003,H89-2027,0,0.0164345,"the special token &lt;turn&gt;. The end-of-turn marker is not included in the POS results, but is included in the perplexity results. We treat contractions, such as that&apos;ll and gonna, as separate words, treating them as that and &apos;ll for the first example, and going and ta for the second. We also changed all word fragments into a common token &lt;fragment&gt;. Since current speech recognition rates for spontaneous speech are quite low, we have run the experiments on the hand-collected transcripts. In searching for the best sequence of POS tags for the transcribed words, we follow the technique proposed by Chow and Schwartz (1989) and only keep a small number of alternative paths by pruning the low probability paths after processing each word. 3.4.2 Perplexity. A way to measure the effectiveness of the language model is to measure the perplexity that it assigns to a test corpus (Bahl et al. 1977). Perplexity is an estimate of how well the language model is able to predict the next word of a test corpus in terms of the number of alternatives that need to be considered at each point. For word-based language models, with estimated probability distribution of Pr(wilwl,i_l) , the perplexity of a test set Wl,N is calculated"
J99-4003,A88-1019,0,0.0152746,"ollowing: 61b = argmaxPr(WDIA ) = argmaxPr(AIWD ) Pr(WD) (6) The first term Pr(AIWD ) is the acoustic model, which can be approximated by Pr(AIW ). The second term Pr(WD) is the POS-based language model and accounts for both the sequence of words and their POS assignment. We rewrite this term as follows: N Pr(W1,ND1,N) = I-IPr(WiDilWl,i_lDl,i_l) i=1 N = l-I Pr(WilWl,i-lDl,i) Pr(DilWl,i-lDl,i-1) (7) i=1 Equation 7 involves two probability distributions that need to be estimated. These are the same distributions that are needed by previous POS-based language models (Equation 5) and POS taggers (Church 1988; Charniak et al. 1993). However, these approaches simplify the context so that the lexical probability is just conditioned on the POS category of the word, and the POS probability is conditioned on just the preceding POS tags, which leads to the following two approximations. Pr(WiIWl,i_lDl,i) ~ Pr(WilDi) (8) Pr(DiIWu_lDl,i_l) ~ Pr(DiIDu_l) (9) However, to successfully incorporate POS information, we need to account for the full richness of the probability distributions, as will be demonstrated in Section 3.4.4. 3.3 Estimating the Probabilities To estimate the probability distributions, we fol"
J99-4003,P93-1008,0,0.205241,"after the interruption point of a speech repair and in fluent speech, namely between utterances or after utterance-initial discourse markers. Word matchings can also be spurious, as evidenced by the 27 word matches with at most two intervening words across abridged repairs, as well as the matchings across intonational boundaries and fluent speech. Even syntactic ill-formedness at the interruption point is not always guaranteed, as the following example illustrates. Example 15 load two (d93-13.2 utt53) boxes of boxcars with oranges reparandum zp Hence using parser failures to find repairs (cf. Dowding et al. 1993) will not be robust. In this section, we augment our POS-based language model so that it also detects intonational boundaries and speech repairs, along with their editing terms. Although not all speech repairs have obvious syntactic anomalies, the probability distributions for words and POS tags are going to be different depending on whether they follow the interruption point of a speech repair, an intonational boundary, or fluent speech. So, it makes sense to take the speech repairs and intonational boundaries into account by directly modeling them when building the language model, which auto"
J99-4003,P97-1033,1,0.940156,"nto utterance units and resolving speech repairs are strongly intertwined with a third task: identifying whether words, such as so, well, and right, are part of the sentential content or are being used as discourse markers to relate the current speech to the preceding context. In the example above, the second and third utterances begin with discourse markers. 1.1 Utterance Units and Intonational Phrases As illustrated above, understanding a speaker&apos;s turn necessitates segmenting it into individual utterance units. However, there is no consensus as to how to define an utterance unit (Traum and Heeman 1997). The manner in which speakers break their speech into intonational phrases undoubtedly plays a major role in its definition. Intonational phrase endings are signaled through variations in the pitch contour, segmental lengthening, and pauses. Beach (1991) demonstrated that hearers can use intonational information early on in sentence processing to help resolve syntactic ambiguities. Bear and Price (1990) showed that a parser can use automatically extracted intonational phrasing to reduce ambiguity and improve efficiency. Ostendorf, Wightman, and Veilleux (1993) used hand-labeled intonational p"
J99-4003,W99-0617,1,0.813413,"Missing"
J99-4003,P94-1041,1,0.882873,"erwise 4.2 Editing Terms Editing terms are problematic for tagging speech repairs since they separate the end of the reparandum from the alteration onset, thus separating the discontinuity that gives evidence that a fresh start or modification repair occurred. For abridged repairs, they separate the word that follows the editing term from the context that is needed to determine the identity of the word and its POS tag. If editing terms could be identified without having to consider the context, we could skip over them, but still use them as part of the context for deciding the repair tag (cf. Heeman and Allen 1994). However, this assumption is not valid for words that are ambiguous as to whether they are an editing term, such as let me see. Even filled pauses are problematic since they are not necessarily part of the editing term of a repair. To model editing terms, we use the v a r i a b l e E i to indicate the type of editing term transition between word W/_ 1 and Wi. Push Ei : ET Pop null if if if if Wi-1 is not part of an editing term but Wi is Wi-1 and Wi are both part of an editing term Wi-1 is part of an editing term but Wi is not neither Wi-1 nor Wi are part of an editing term Below, we give an"
J99-4003,P83-1019,0,0.759876,"1991; Shriberg 1994) has come up with a number of classification systems. Categories are based on how the reparandum and alteration differ, for instance whether the alteration repeats the reparandum, makes it more appropriate, or fixes an error in the reparandum. Such an analysis can shed light on where in the production system the error and its repair originated. Our concern, however, is in computationally resolving repairs. The relevant features are those that the hearer has access to and can make use of in detecting and correcting a repair. Following loosely in the footsteps of the work of Hindle (1983), we divide them into the following categories: fresh starts, modification repairs, and abridged repairs. Fresh starts occur where the speaker abandons the current utterance and starts again, where the abandonment seems to be acoustically signaled either in the editing term or at the onset of the alteration. Example 3 illustrates a fresh start where the speaker abandons the partial utterance I need to send, and replaces it by the question how many boxcars can one engine take. 2 Our notation is adapted from Levelt (1983). We follow Shriberg (1994) and Nakatani and Hirschberg (1994) in using rep"
J99-4003,J93-3003,0,0.0753209,"tial interpretation. The hearer also has to determine that the reparandum is empty. As the example above illustrates, this is not necessarily a trivial task because of the spurious word correspondences between need to and manage to. 1.3 Discourse Markers Phrases such as so, now, firstly, moreover, and anyways can be used as discourse markers (Schiffrin 1987). Discourse markers are conjectured to give the hearer information about the discourse structure, and so aid the hearer in understanding how the new speech or text relates to what was previously said and for resolving anaphoric references (Hirschberg and Litman 1993). Although discourse markers, such as firstly, and moreover, are not commonly used in spoken dialogue (Brown and Yule 1983), a lot of other markers are employed. These markers are used to achieve a variety of effects: such as signal an acknowledgment or acceptance, hold a turn, stall for time, signal a speech repair, or signal an interruption in the discourse structure or the return from one. Although Schiffrin defines discourse markers as bracketing units of speech, she explicitly avoids defining what the unit is. We feel that utterance units are the building 530 Heeman and Allen Modeling Spe"
J99-4003,H92-1003,0,0.0158805,"t into an ideal human-computer interface that is able to engage in fluent conversation. Table 2 gives details about the Trains corpus. The corpus consists of six and a half hours of speech produced by 34 different speakers solving 20 different problems. The Trains corpus provides natural examples of dialogue usage that spoken dialogue systems need to handle in order to carry on a dialogue with a user. For instance, the corpus contains instances of overlapping speech, back-channel responses, and turn taking: phenomena that do not occur in collections of single speaker utterances, such as ATIS (MADCOW 1992). The Trains corpus also differs from the Switchboard corpus (Godfrey, Holliman, and McDaniel 1992) in that it is task oriented and has a limited domain, making it a more realistic domain for studying the types of conversations that people would want to have with a computer. 2.1 Word Transcription Table 3 gives a dialogue from the Trains corpus. Overlapping speech is indicated by the &quot; + &quot; markings. Each word was transcribed using its orthographic spelling, unless it was mispronounced and the speaker subsequently repairs the mispronunciation. Contractions, including words such as wanna, were t"
J99-4003,J93-2004,0,0.0607708,"Missing"
J99-4003,W96-0204,0,0.0387277,"Missing"
J99-4003,J95-2001,0,\N,Missing
J99-4003,1993.mtsummit-1.11,0,\N,Missing
J99-4003,J88-1003,0,\N,Missing
J99-4003,J93-2006,0,\N,Missing
J99-4003,C92-1029,0,\N,Missing
J99-4003,C94-1024,0,\N,Missing
J99-4003,C94-1098,0,\N,Missing
J99-4003,P96-1043,0,\N,Missing
J99-4003,J95-4004,0,\N,Missing
J99-4003,H93-1008,0,\N,Missing
J99-4003,P96-1009,1,\N,Missing
J99-4003,C88-2121,0,\N,Missing
J99-4003,P86-1021,0,\N,Missing
J99-4003,H94-1034,1,\N,Missing
J99-4003,P93-1005,0,\N,Missing
J99-4003,J86-3001,0,\N,Missing
J99-4003,P87-1023,0,\N,Missing
J99-4003,P84-1055,0,\N,Missing
K15-1023,W08-2227,1,0.677292,"Missing"
K15-1023,Q13-1016,0,0.0289007,"bjects called “red” share some similarity in color while objects called “ball” share some similarity in shape. Learning for them therefore requires both identification and establishing joint attention with the speaker before assigning a label to an object, while also applying other language learning strategies to narrow down the search space of possible referents, as illustrated by Quine’s “gavagai” problem (1964). 226 Proceedings of the 19th Conference on Computational Language Learning, pages 226–236, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics work by Krishnamurthy and Kollar (2013) and Yu and Siskind (2013), and our emphasis on attributes is related to work by Farhadi et al. (2009). However, our focus is on learning from situations that a child would be exposed to, without using annotated data, and to test implementations of child language learning strategies in a computational system. We use a tutor-directed approach to training our system where the speaker presents objects to the system and describes them, as in work by Skocaj et al. (2011). The focus of this work is in evaluating referring expressions as in work by Mohan et al. (2013), although without any dialogue f"
K15-1023,P13-1006,0,0.0335317,"arity in color while objects called “ball” share some similarity in shape. Learning for them therefore requires both identification and establishing joint attention with the speaker before assigning a label to an object, while also applying other language learning strategies to narrow down the search space of possible referents, as illustrated by Quine’s “gavagai” problem (1964). 226 Proceedings of the 19th Conference on Computational Language Learning, pages 226–236, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics work by Krishnamurthy and Kollar (2013) and Yu and Siskind (2013), and our emphasis on attributes is related to work by Farhadi et al. (2009). However, our focus is on learning from situations that a child would be exposed to, without using annotated data, and to test implementations of child language learning strategies in a computational system. We use a tutor-directed approach to training our system where the speaker presents objects to the system and describes them, as in work by Skocaj et al. (2011). The focus of this work is in evaluating referring expressions as in work by Mohan et al. (2013), although without any dialogue for disambiguation. Kollar"
K16-1007,W08-2222,0,0.159395,"i drove fast , which is in no way captured in Figure 12 . Having a comprehensive meaning representation of comparison strucRepresenting the underlying meaning of text has been a long-standing topic of interest in computational linguistics. Recently there has been a renewed interest in representation of meaning for various tasks such as semantic parsing, where the task is to map a natural language sentence into its corresponding formal meaning representation (Zelle and Mooney, 1996; Berant and Liang, 2014). Open-domain and broad-coverage semantic representation of text (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) is crucial for many language understanding tasks such as reading comprehension tests and question answering. 1 We used Semafor tool: http://demo.ark.cs.cmu.edu/parse The same shortcomings are shared among other generic meaning representations such as LinGO English Resource Grammar (ERG) (Flickinger, 2011), Boxer (Bos, 2008), or AMR (Banarescu et al., 2013), among others. 2 62 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 62–74, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics My Mazd"
K16-1007,W08-2227,1,0.802604,"Missing"
K16-1007,D15-1115,1,0.80383,"efforts on the computational modeling of comparatives have been in the context of sentiment analysis, ranging from works on identifying sentences containing comparisons (Jindal and Liu, 2006b) to identifying the components of the comparisons in the form of triplets or other templatic patterns (Jindal and Liu, 2006a; Xu et al., 2011; Kessler and Kuhn, 2014). These works provide a basis for computational analysis of comparatives, however, they lack depth and broader coverage as they are limited to only a few comparison patterns. The most recent work on the computational semantics of comparison (Bakhshandeh and Allen, 2015) sets the stage for a deeper semantic representation of comparisons. Bakshandeh and Allen introduce the first computational semantic frame3 Throughout this paper we refer to any statement comparing two or more entities as a comparison instance. 4 These morphemes are often referred to as the comparison operators. Figure 1: The frame-semantic parsing of the sentence My Mazda drove faster than his Hyundai. tures which can capture the mentioned phenomena can enable the development of computational semantic models which are suitable for various reasoning tasks. In this paper we introduce a joint th"
K16-1007,W13-2322,0,0.0758088,"e faster than his Hyundai drove fast , which is in no way captured in Figure 12 . Having a comprehensive meaning representation of comparison strucRepresenting the underlying meaning of text has been a long-standing topic of interest in computational linguistics. Recently there has been a renewed interest in representation of meaning for various tasks such as semantic parsing, where the task is to map a natural language sentence into its corresponding formal meaning representation (Zelle and Mooney, 1996; Berant and Liang, 2014). Open-domain and broad-coverage semantic representation of text (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) is crucial for many language understanding tasks such as reading comprehension tests and question answering. 1 We used Semafor tool: http://demo.ark.cs.cmu.edu/parse The same shortcomings are shared among other generic meaning representations such as LinGO English Resource Grammar (ERG) (Flickinger, 2011), Boxer (Bos, 2008), or AMR (Banarescu et al., 2013), among others. 2 62 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 62–74, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguist"
K16-1007,P14-1133,0,0.0232036,"sentence, the resolution of which results in complete understood reading of My Mazda drove faster than his Hyundai drove fast , which is in no way captured in Figure 12 . Having a comprehensive meaning representation of comparison strucRepresenting the underlying meaning of text has been a long-standing topic of interest in computational linguistics. Recently there has been a renewed interest in representation of meaning for various tasks such as semantic parsing, where the task is to map a natural language sentence into its corresponding formal meaning representation (Zelle and Mooney, 1996; Berant and Liang, 2014). Open-domain and broad-coverage semantic representation of text (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) is crucial for many language understanding tasks such as reading comprehension tests and question answering. 1 We used Semafor tool: http://demo.ark.cs.cmu.edu/parse The same shortcomings are shared among other generic meaning representations such as LinGO English Resource Grammar (ERG) (Flickinger, 2011), Boxer (Bos, 2008), or AMR (Banarescu et al., 2013), among others. 2 62 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages"
K16-1007,P14-5010,0,0.0036343,"pturing deeper generic semantics of text as opposed to surface word representations. One of the most common ways for expressing evaluative sentiment towards different entities is using comparison. A simple natural language example of comparison is Their pizza is the best. Capturing the underlying meaning of comparison structures, as opposed to their surface wording, is required for accurate evaluation of qualities and quantities. For instance, given a more complex comparison example, The pizza was great, but it was not as awesome as the sandwich, the state-ofthe-art sentiment analysis system (Manning et al., 2014) assigns an overall ‘neutral’ sentiment value, which clearly lacks deeper understanding of the comparison happening in the sentence. Consider the generic meaning representation depicted in in Figure 1 according to frame semantic parsing 1 (Das et al., 2014) for the following sentence: Domain-independent meaning representation of text has received a renewed interest in the NLP community. Comparison plays a crucial role in shaping objective and subjective opinion and measurement in natural language, and is often expressed in complex constructions including ellipsis. In this paper, we introduce a"
K16-1007,C04-1157,0,0.0495582,"ional Linguistics My Mazda Self_mover drove Self_motion faster than his Hyundai Manner Broadly, elliptical constructions involve the omission of one or more phrases from a clause (such as ‘drove fast’ phrase at the end of example (1)) whose content can still be fully recovered from the unelided words of the sentence (Kennedy, 2003; Merchant, 2013). Resolving ellipsis is crucial for deep language understanding. Although ellipsis has been studied in great depth in linguistics, there only have been a few computational studies of ellipsis, most of which have focused on Verb Phrase Ellipsis (VPE) (Nielsen, 2004; Schiehlen, 2002; Hardt, 1997) such as Larry is not telling the truth, neither is Jim ∆. where ∆ is a verb phrase ellipsis site, which can be resolved to ‘telling the truth’. In 2010, a SemEval task was organized with the goals of (1) automatically detecting VPE in text, and (2) resolving the antecedent of each VPE (Bos and Spenader, 2011). For this task, they manually annotated a portion of OntoNotes corpus, consisting of Wall Street Journal (WSJ) articles. Throughout all the 25 sections of WSJ, they found 487 instances of VPE (ranging from predicative ellipsis, deletion, and comparative con"
K16-1007,P05-1015,0,0.200396,"Missing"
K16-1007,P06-1055,0,0.0736102,"Missing"
K16-1007,P02-1010,0,0.0275384,"cs My Mazda Self_mover drove Self_motion faster than his Hyundai Manner Broadly, elliptical constructions involve the omission of one or more phrases from a clause (such as ‘drove fast’ phrase at the end of example (1)) whose content can still be fully recovered from the unelided words of the sentence (Kennedy, 2003; Merchant, 2013). Resolving ellipsis is crucial for deep language understanding. Although ellipsis has been studied in great depth in linguistics, there only have been a few computational studies of ellipsis, most of which have focused on Verb Phrase Ellipsis (VPE) (Nielsen, 2004; Schiehlen, 2002; Hardt, 1997) such as Larry is not telling the truth, neither is Jim ∆. where ∆ is a verb phrase ellipsis site, which can be resolved to ‘telling the truth’. In 2010, a SemEval task was organized with the goals of (1) automatically detecting VPE in text, and (2) resolving the antecedent of each VPE (Bos and Spenader, 2011). For this task, they manually annotated a portion of OntoNotes corpus, consisting of Wall Street Journal (WSJ) articles. Throughout all the 25 sections of WSJ, they found 487 instances of VPE (ranging from predicative ellipsis, deletion, and comparative constructions, to ps"
K16-1007,J14-1002,0,\N,Missing
manshadi-etal-2012-annotation,W08-2222,0,\N,Missing
manshadi-etal-2012-annotation,D09-1152,0,\N,Missing
manshadi-etal-2012-annotation,W07-1207,1,\N,Missing
manshadi-etal-2012-annotation,C02-2025,0,\N,Missing
manshadi-etal-2012-annotation,P11-2025,1,\N,Missing
manshadi-etal-2012-annotation,J03-1004,0,\N,Missing
manshadi-etal-2012-annotation,P11-1060,0,\N,Missing
N09-2012,mani-etal-2008-spatialml,0,0.0192143,"tities such as businesses, parks, bridges, etc.) An annotation example is shown in Figure 4, in which the utterance contains references to two Figure 4: Sample annotations of referring expressions to geospatial locations streets and an intersection. Here the intersection referring expression spans two referring expressions to streets, and each is annotated with a canonical name as well as lat/lon coordinates. Note also that our annotation schema allows us to annotate embedded references (here the streets within the intersection). 5 Related Work The SpatialML module for the Callisto annotator (Mani et al., 2008) was designed for human annotation of geospatial locations with ground truth by looking up targets in a gazetteer. It does not, however, have a geographic visualization components such as Google Earth and does not support GPS track playback. The TAME annotator (Leidner, 2004) is a similar tool, supporting hand annotation of toponym references by gazetteer lookup. It too does not, as far as we are aware, have a visualization component nor GPS track information, likely because the level of geospatial entities being looked at were at the city/state/country level. The PURSUIT Corpus mostly contain"
N16-1098,W08-2227,1,0.249101,"Missing"
N16-1098,D13-1178,0,0.046906,"ured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful info"
N16-1098,P13-1035,0,0.0363528,"esentations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques per"
N16-1098,D15-1075,0,0.0394456,"ressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a r"
N16-1098,P08-1090,1,0.58165,"used on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computation"
N16-1098,P09-1068,1,0.635097,"77). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008"
N16-1098,D13-1185,1,0.266302,"and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Picho"
N16-1098,N13-1104,1,0.900888,"ical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful information from noisy bl"
N16-1098,W07-1401,0,0.0338919,"Test’. 4.1 Story Cloze Test The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank. We introduce ‘Story Cloze Test’, in which a system is given a four-sentence ‘context’ and two alternative endings to the story, called ‘right ending’ and ‘wrong ending’. Hence, in this test the fifth sentence is blank. Then the system’s task is to choose the right ending. The ‘right ending’ can be viewed as ‘entailing’ hypothesis in a classic Recognizing Textual Entailment (RTE) framework (Giampiccolo et al., 2007), and ‘wrong’ ending can be seen as the ’contradicting’ hypothesis. Table 4 shows three example Story Cloze Test cases. Story Cloze Test will serve as a generic story understanding evaluation framework, also applicable to evaluation of story generation models (for instance by computing the log-likelihoods assigned to the two ending alternatives by the story generation model), which does not necessarily imply requirement for explicit narrative knowledge learning. However, it is safe to say that any model that performs well on Story Cloze Test is demonstrating some level of deeper story understa"
N16-1098,E12-1034,0,0.0756634,"Missing"
N16-1098,P04-1077,0,0.0329659,"orkers participated Average # cases written by one worker Max # cases written by one worker Average payment per test case (cents) Size of the final set (verified by human) 13,500 282 47.8 1461 10 3,744 Table 5: Statistics for crowd-sourcing Story Cloze Test instances. search engine8 hits of the main event (verb) together with its semantic roles (e.g., ‘I*poison*flowers’ vs ‘I*nourish*flowers’). We extract the main verb and its corresponding roles using TRIPS semantic parser. 2. N-gram Overlap: Simply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothe"
N16-1098,P14-5010,0,0.00272135,"imply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothesis that matches the sentiment of the last context sentence. 8 https://developers.google.com/ custom-search/ 846 6. Skip-thoughts Model: This model uses Skipthoughts’ Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels. This model is trained on the ‘BookCorpus’ (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books. We use the skip-thoughts embedding of the alternatives and contexts for making decision the same way as with GenSim model. 7. Narrati"
N16-1098,P09-1025,0,0.00927628,"ing the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M´endez et al., 2014; Riedl and Le´on, 2008). This paper is unique to these in its corpus of short, simple stories with a wide variety of commonsense events. We show these to be useful for learning, but also for enabling a rich evaluation framework for narrative understanding. 3 A Corpus of Short Commonsense Stories We aimed to build a corpus with two goals in mind: 1. The corpus contains a variety of commonsense causal and temporal relations between everyday events. This enables learning n"
N16-1098,W16-1007,1,0.12213,"Missing"
N16-1098,P15-1019,0,0.0707931,"Missing"
N16-1098,E14-1024,0,0.538376,"2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward child"
N16-1098,P10-1100,0,0.0173048,"g can help direct the field to a new direction of deeper language understanding. 2 Related Work Several lines of research have recently focused on learning narrative/event representations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a"
N16-1098,D13-1020,0,0.0780145,"vents, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, the"
N16-1098,D15-1195,0,0.170589,"Missing"
N16-1098,H89-1033,0,0.710176,"onsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding. 1 Introduction Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced"
P11-2025,P92-1005,0,0.558122,"iction on the type of quantification, are investigated for possible scope interactions. 1 Introduction Since the early days of natural language understanding (NLU), quantifier scope disambiguation has been an extremely hard task. Therefore, early NLU systems either devised some mechanism for leaving the semantic representation underspecified (Woods 1978, Hobbs and Shieber 1987), or tried to assign scoping to sentences based on heuristics (VanLehn 1978, Moran 1988, Alshawi 1992). There has been a lot of work since then on developing frameworks for scope-underspecified semantic representations (Alshawi and Crouch 1992, Bos 1996, Copestake et al., 2001, Egg et al., 2001). The motivation of most recent formalisms is to develop a constraint-based framework where you can incrementally add constraints to filter out unwanted scopings. However, almost all of these formalisms are based on hard constraints, which have to be satisfied in every reading of the sentence. It seems that the story is different in practice. Most of the constraints one can hope for (imposed by discourse, pragmatics, word knowledge, etc.) are soft constraints, that is they define a preference over the possible readings of a sentence. As a re"
P11-2025,P01-1019,0,0.018076,"n, are investigated for possible scope interactions. 1 Introduction Since the early days of natural language understanding (NLU), quantifier scope disambiguation has been an extremely hard task. Therefore, early NLU systems either devised some mechanism for leaving the semantic representation underspecified (Woods 1978, Hobbs and Shieber 1987), or tried to assign scoping to sentences based on heuristics (VanLehn 1978, Moran 1988, Alshawi 1992). There has been a lot of work since then on developing frameworks for scope-underspecified semantic representations (Alshawi and Crouch 1992, Bos 1996, Copestake et al., 2001, Egg et al., 2001). The motivation of most recent formalisms is to develop a constraint-based framework where you can incrementally add constraints to filter out unwanted scopings. However, almost all of these formalisms are based on hard constraints, which have to be satisfied in every reading of the sentence. It seems that the story is different in practice. Most of the constraints one can hope for (imposed by discourse, pragmatics, word knowledge, etc.) are soft constraints, that is they define a preference over the possible readings of a sentence. As a result, statistical methods seem to"
P11-2025,J03-1004,0,0.701723,"Missing"
P11-2025,J87-1005,0,0.511901,"he scope interaction between all scopal terms in the sentence from quantifiers to scopal adverbials, without putting any restriction on the number of scopal terms in a sentence. In addition, all NPs, explicitly quantified or not, with no restriction on the type of quantification, are investigated for possible scope interactions. 1 Introduction Since the early days of natural language understanding (NLU), quantifier scope disambiguation has been an extremely hard task. Therefore, early NLU systems either devised some mechanism for leaving the semantic representation underspecified (Woods 1978, Hobbs and Shieber 1987), or tried to assign scoping to sentences based on heuristics (VanLehn 1978, Moran 1988, Alshawi 1992). There has been a lot of work since then on developing frameworks for scope-underspecified semantic representations (Alshawi and Crouch 1992, Bos 1996, Copestake et al., 2001, Egg et al., 2001). The motivation of most recent formalisms is to develop a constraint-based framework where you can incrementally add constraints to filter out unwanted scopings. However, almost all of these formalisms are based on hard constraints, which have to be satisfied in every reading of the sentence. It seems"
P11-2025,P03-1054,0,0.00721369,"00 OR May 4000 AL June c josh 21 a adams 23 d sam 26 b john 25 OUTPUT 4000 AL June 3000 HU August 1000 NY April 4000 OR May a adams 23 b john 25 c josh 21 d sam 26 Figure 2. Two I/O pairs given for a single task pairs and hence expand the corpus in a bootstrapping fashion. The data acquired from Mechanical Turk is often quite noisy, therefore all sentences are reviewed manually and tagged with different categories (e.g. paraphrase of the original description, wrong but coherent description, etc.). 3.3 Pre-processing the corpus The corpus is tokenized and parsed using the Stanford PCFG parser (Klein and Manning 2003). We guide the parser by giving suggestions on part-ofspeech (POS) tags based on the gold standard POS tags provided for some classes of words such as verbs. Shallow NP chunks and negations are automatically extracted from the parse trees and indexed. The resulting NP-chunked sentences are then reviewed manually, first to fix the chunking errors, hence providing gold standard chunks, and second, to add chunks for other scopal operators such as sentential adverbials since the above automated approach will not extract those. Figure (3) shows the examples in figure (1) after chunking. As shown in"
P11-2025,P88-1005,0,0.773407,"without putting any restriction on the number of scopal terms in a sentence. In addition, all NPs, explicitly quantified or not, with no restriction on the type of quantification, are investigated for possible scope interactions. 1 Introduction Since the early days of natural language understanding (NLU), quantifier scope disambiguation has been an extremely hard task. Therefore, early NLU systems either devised some mechanism for leaving the semantic representation underspecified (Woods 1978, Hobbs and Shieber 1987), or tried to assign scoping to sentences based on heuristics (VanLehn 1978, Moran 1988, Alshawi 1992). There has been a lot of work since then on developing frameworks for scope-underspecified semantic representations (Alshawi and Crouch 1992, Bos 1996, Copestake et al., 2001, Egg et al., 2001). The motivation of most recent formalisms is to develop a constraint-based framework where you can incrementally add constraints to filter out unwanted scopings. However, almost all of these formalisms are based on hard constraints, which have to be satisfied in every reading of the sentence. It seems that the story is different in practice. Most of the constraints one can hope for (impo"
P11-2025,D09-1152,0,0.414511,"Missing"
P11-2061,W09-2418,0,0.0470907,"Missing"
P11-2061,tannier-muller-2008-evaluation,0,0.0595879,"e that measures the temporal awareness of each system. We use Timegraph (Miller and Schubert, 1990) for computing temporal closure, which makes our system scalable and computationally inexpensive. 2 Related Work To calculate the inter-annotator agreement between annotators in the temporal annotation task, some researchers have used semantic matching to reward distinct but equivalent temporal relations. Such techniques can equally well be applied to system evaluation. Setzer et al. (2003) use temporal closure to reward equivalent but distinct relations. Consider the example in Figure 1 (due to Tannier and Muller, 2008). Consider graph K as the reference annotation graph, and S1, S2 and S3 as outputs of different systems. The bold edges are the extracted relations and the dotted edges are derived. The traditional matching approach will fail to verify B<D is a correct relation in S2, since there is no explicit edge between B and D in reference annotation (K). But a metric using temporal closure would create all implicit edges and be able to reward B<D edge in S2. 351 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 351–356, c Portland, Oregon, June 19-"
P11-2061,S07-1014,0,0.333631,"th a single score, making comparison between different systems straightforward. Our approach is easy to implement, intuitive, accurate, scalable and computationally inexpensive. 1 Introduction The recent emergence of language processing applications like question answering, information extraction, and document summarization has motivated the need for temporally-aware systems. This, along with the availability of the temporal annotation scheme TimeML (Pustejovsky et al., 2003), a temporally annotated corpus, TimeBank (Pustejovsky et al., 2003) and the temporal evaluation challenges TempEval-1 (Verhagen et al., 2007) and TempEval-2 (Pustejovsky and Verhagen, 2010), has led to an explosion of research on temporal information processing (TIP). Prior evaluation methods (TempEval-1, 2) for different TIP subtasks have borrowed precision and recall measures from the information retrieval community. This has two problems: First, systems express temporal relations in different, yet equivalent, ways. Consider a scenario where the 1 Temporal closure is a reasoning mechanism that derives new implied temporal relations, i.e. makes implicit temporal relations explicit. For example, if we know A before B, B before C, t"
P11-2061,M95-1005,0,0.290128,"Missing"
P11-2061,S07-1000,0,\N,Missing
P13-1007,A88-1008,0,0.613809,"ork treats a conjunction of NPs as separate NPs. However, similar to plurals, NP conjunctions (disjunctions) introduce an extra scopal element: a universal (existential). We are working on an annotation scheme for NP conjunctions, so we have left this for after the annotations become available. 70 Implicit universals only (pairs with at least one id) P+ R+ der to evade scope disambiguation, yet be able to perform entailment, Koller and Thater (2010) propose an algorithm to calculate the weakest readings20 from a scope-underspecified representation. Early efforts on automatic QSD (Moran, 1988; Hurum, 1988) were based on heuristics, manually formed into rules with manually assigned weights for resolving conflicts. To the best of our knowledge, there have been four major efforts on statistical QSD for English: Higgins and Sadock (2003), Galen and MacCartney (2004), Srinivasan and Yates (2009), and Manshadi and Allen (2011a). The first three only scope two scopal terms in a sentence, where the scopal term is an NP with an explicit quantification. MA11 is the first to scope any number of NPs in a sentence with no restriction on the type of quantification. Besides ignoring negation and implicit univ"
P13-1007,P92-1005,0,0.405946,") optimizing a well justified criterion, by using automatically generated features instead of hand-annotated dependencies, and by boosting the performance by a large margin with the help of a rich feature vector. This work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision. Related work Since automatic QSD is in general challenging, traditionally quantifier scoping is left underspecified in deep linguistic processing systems (Alshawi and Crouch, 1992; Bos, 1996; Copestake et al., 2001). Some efforts have been made to move underspecification frameworks towards weighted constraint-based graphs in order to produce the most preferred reading (Koller et al., 2008), but the source of these types of constraint are often discourse, pragmatics, world knowledge, etc., and hence, they are hard to obtain automatically. In orAcknowledgement We need to thank William de Beaumont and Jonathan Gordon for their comments on the paper and Omid Bakhshandeh for his assistance. This work was supported in part by NSF grant 1012205, and ONR grant N000141110417. 1"
P13-1007,D08-1073,0,0.0342002,"universals and negations are easier to scope, even for the human annotators.17 There are several reasons for poor performance with negations as well. First, the number of negations in the corpus is small, therefore the data is very sparse. Second, the RPC model does not work well for negations. Scoping a negation relative to an NP chunk, with which it has a long distance dependency, often depends on the scope of the elements in between. Third, scoping negation usually requires a deep semantic analysis. In order to see how well our approximation algorithm is working, similar to the approach of Chambers and Jurafsky (2008), we tried an ILP solver18 for DAGs with at most 8 nodes to find the optimum solution, but we found the difference insignificant. In fact, the approximation algorithm finds the optimum solution in all but one case.19 5 6 Summary and future work We develop the first statistical QSD model addressing the interaction of quantifiers with negation and the implicit universal of plurals, defining a baseline for this task on QuanText data (Manshadi et al., 2012). In addition, our work improves upon Manshadi and Allen (2011a)’s work by (approximately) optimizing a well justified criterion, by using auto"
P13-1007,P03-1054,0,0.0134095,"nks only (no negation) Where ni is the number of scopal terms introduced by sentence i. Out of the 4500 samples, around 1800 involve at least one implicit universal (i.e., id), but only 120 samples contain a negation. We evaluate the performance of the system for implicit universals and negation both separately and in the context of full scope disambiguation. We split the corpus at random into three sets of 50, 100, and 350 sentences, as development, test, and train sets respectively.13 To extract part-of-speech tags, phrase structure trees, and typed dependencies, we use the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006) on both train and test sets. Since we are using SVM, we have passed the confidence levels through a softmax function to convert them λ before applying the algointo probabilities Pu,v rithm of Section 3. We take MA11’s system as the baseline. However, in order to have a fair comparison, we have used the output of the Stanford parser to automatically generate the same features that MA11 have hand-annotated.14 In order to run the baseline system on implicit universals, we take the feature vector of a plural NP and add a feature to indicate that this feature vector repr"
P13-1007,P10-1004,0,0.0149296,"to part-of-speech tags and untyped dependency relations. 15 SV M M ulticlass from SVM-light (Joachims, 1999). 16 In all experiments, we ignore NP conjunctions. Previous work treats a conjunction of NPs as separate NPs. However, similar to plurals, NP conjunctions (disjunctions) introduce an extra scopal element: a universal (existential). We are working on an annotation scheme for NP conjunctions, so we have left this for after the annotations become available. 70 Implicit universals only (pairs with at least one id) P+ R+ der to evade scope disambiguation, yet be able to perform entailment, Koller and Thater (2010) propose an algorithm to calculate the weakest readings20 from a scope-underspecified representation. Early efforts on automatic QSD (Moran, 1988; Hurum, 1988) were based on heuristics, manually formed into rules with manually assigned weights for resolving conflicts. To the best of our knowledge, there have been four major efforts on statistical QSD for English: Higgins and Sadock (2003), Galen and MacCartney (2004), Srinivasan and Yates (2009), and Manshadi and Allen (2011a). The first three only scope two scopal terms in a sentence, where the scopal term is an NP with an explicit quantifica"
P13-1007,P01-1019,0,0.0162985,"on, by using automatically generated features instead of hand-annotated dependencies, and by boosting the performance by a large margin with the help of a rich feature vector. This work can be improved in many directions, among which are scoping more elements such as other scopal operators and implicit entities, deploying more complex learning models, and developing models which require less supervision. Related work Since automatic QSD is in general challenging, traditionally quantifier scoping is left underspecified in deep linguistic processing systems (Alshawi and Crouch, 1992; Bos, 1996; Copestake et al., 2001). Some efforts have been made to move underspecification frameworks towards weighted constraint-based graphs in order to produce the most preferred reading (Koller et al., 2008), but the source of these types of constraint are often discourse, pragmatics, world knowledge, etc., and hence, they are hard to obtain automatically. In orAcknowledgement We need to thank William de Beaumont and Jonathan Gordon for their comments on the paper and Omid Bakhshandeh for his assistance. This work was supported in part by NSF grant 1012205, and ONR grant N000141110417. 17 Trivially, we have taken the relat"
P13-1007,P08-1026,0,0.0643141,"Missing"
P13-1007,W08-1301,0,0.0412108,"Missing"
P13-1007,P11-1060,0,0.0165486,"d corpora, resulting in the lack of work on extensive 1. Three words start with a capital letter. A deep understanding of this sentence, requires deciding whether each word in the set, referred to by Three words, starts with a potentially distinct capital letter (as in Apple, Orange, Banana) or there is a unique capital letter which each word starts with (as in Apple, Adam, Athens). By treating the NP Three words as a single atomic entity, earlier work on automatic QSD has overlooked this problem. In general, every plural NP potentially introduces an implicit universal, ranging 1 For example, Liang et al. (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model. 64 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 64–72, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics over the collection of entities introduced by the plural.2 Scoping this implicit universal is just as important. While explicit universals may not occur very often in natural language, the usage of plurals is very common. Plurals form 18"
P13-1007,de-marneffe-etal-2006-generating,0,0.029263,"Missing"
P13-1007,W11-1108,1,0.947536,"antly improve the performance of the previous model using a rich set of automatically generated features. 1 Introduction The sentence there is one faculty member in every graduate committee is ambiguous with respect to quantifier scoping, since there are at least two possible readings: If one has wide scope, there is a unique faculty member on every committee. If every has wide scope, there can be different faculty members on each committee. Over the past decade there has been some work on statistical quantifier scope disambiguation (QSD) (Higgins and Sadock, 2003; Galen and MacCartney, 2004; Manshadi and Allen, 2011a). However, the extent of the work has been quite limited for several reasons. First, in the past two decades, the main focus of the NLP community has been on shallow text processing. As a deep processing task, QSD is not essential for many NLP applications that do not require deep understanding. Second, there has been a lack of comprehensive scope-disambiguated corpora, resulting in the lack of work on extensive 1. Three words start with a capital letter. A deep understanding of this sentence, requires deciding whether each word in the set, referred to by Three words, starts with a potential"
P13-1007,P11-2025,1,0.911758,"Missing"
P13-1007,manshadi-etal-2012-annotation,1,0.910505,"se limitations and scope an arbitrary number of NPs in a sentence with no restriction on the type of quantification. However, although their corpus annotates the scope of negations and the implicit universal of plurals, their QSD system does not handle those. As a step towards comprehensive automatic QSD, in this paper we present our work on automatic scoping of the implicit universal of plurals and negations. For data, we use a new revision of MA11’s corpus, first introduced in Manshadi et al. (2011b). The new revision, called QuanText, carries a more detailed, fine-grained scope annotation (Manshadi et al., 2012). The performance of our model defines a baseline for future efforts on (comprehensive) QSD over QuanText. In addition to addressing plurality and negation, this work improves upon MA11’s in two directions. • We theoretically justify MA11’s ternaryclassification approach, formulating it as a general framework for learning to build partial orders. An n log n algorithm is then given to find a guaranteed approximation within a fixed ratio of the optimal solution from a set of pairwise preferences (Sect. 3.1). • We replace MA11’s hand-annotated features with a set of automatically generated lingui"
P13-1007,P88-1005,0,0.569371,"s. Previous work treats a conjunction of NPs as separate NPs. However, similar to plurals, NP conjunctions (disjunctions) introduce an extra scopal element: a universal (existential). We are working on an annotation scheme for NP conjunctions, so we have left this for after the annotations become available. 70 Implicit universals only (pairs with at least one id) P+ R+ der to evade scope disambiguation, yet be able to perform entailment, Koller and Thater (2010) propose an algorithm to calculate the weakest readings20 from a scope-underspecified representation. Early efforts on automatic QSD (Moran, 1988; Hurum, 1988) were based on heuristics, manually formed into rules with manually assigned weights for resolving conflicts. To the best of our knowledge, there have been four major efforts on statistical QSD for English: Higgins and Sadock (2003), Galen and MacCartney (2004), Srinivasan and Yates (2009), and Manshadi and Allen (2011a). The first three only scope two scopal terms in a sentence, where the scopal term is an NP with an explicit quantification. MA11 is the first to scope any number of NPs in a sentence with no restriction on the type of quantification. Besides ignoring negation and"
P13-1007,W10-1809,0,0.0294866,"scoping, i and j are incomparable. This happens if both orders are equivalent (as in two existentials) or when the two chunks have no scope interaction. Since a partial order can be represented by a Directed Acyclic Graph (DAG), we use DAGs to represent scopings. For example, G1 in Figure 1 represents the scoping in (4). 2.1 Evaluation metrics Given the gold standard DAG Gg = (V, Eg ) and the predicted DAG Gp = (V, Ep ), a similarity measure may be defined based on the ratio of the number of pairs (of nodes) labeled correctly to the 2 Although plurals carry different types of quantification (Herbelot and Copestake, 2010), almost always there exists an implicit universal. The importance of scoping this universal, however, may vary based on the type of quantification. 65 2 2 1 1 3 2 Our framework 3.1 3 3 4 1 4 2 1 Since we defined QSD as a partial ordering, automatic QSD would become the problem of learning to build partial orders. The machine learning community has studied the problem of learning total orders (ranking) in depth (Cohen et al., 1999; Furnkranz and Hullermeier, 2003; Hullermeier et al., 2008). Many ranking systems create partial orders as output when the confidence level for the relative order of"
P13-1007,W95-0107,0,0.133641,"set of pairwise preferences (Sect. 3.1). • We replace MA11’s hand-annotated features with a set of automatically generated linguistic features. Our rich set of features significantly improves the performance of the QSD model, even though we give up the goldstandard dependency features (Sect. 3.3). 2 Task definition In QuanText, scope-bearing elements (or, as we call them, scopal terms) of each sentence have been identified using labeled chunks, as in (3). 3. Replace [1/ every line] in [2/ the file] ending in [3/ punctuation] with [4/ a blank line] . NP chunks follow the definition of baseNP (Ramshaw and Marcus, 1995) and hence are flat. Outscoping relations are used to specify the relative scope of scopal terms. The relation i > j means that chunk i outscopes (or has wide scope over) chunk j. Equivalently, chunk j is said to have narrow scope with respect to i. Each sentence is annotated with its most preferred scoping (according to the annotators’ judgement), represented as a partial order: 4. SI : (2 > 1 > 4; 1 > 3) If neither i > j nor j > i is entailed from the scoping, i and j are incomparable. This happens if both orders are equivalent (as in two existentials) or when the two chunks have no scope in"
P13-1007,J03-1004,0,0.419339,"hich works very well in practice. Finally, we significantly improve the performance of the previous model using a rich set of automatically generated features. 1 Introduction The sentence there is one faculty member in every graduate committee is ambiguous with respect to quantifier scoping, since there are at least two possible readings: If one has wide scope, there is a unique faculty member on every committee. If every has wide scope, there can be different faculty members on each committee. Over the past decade there has been some work on statistical quantifier scope disambiguation (QSD) (Higgins and Sadock, 2003; Galen and MacCartney, 2004; Manshadi and Allen, 2011a). However, the extent of the work has been quite limited for several reasons. First, in the past two decades, the main focus of the NLP community has been on shallow text processing. As a deep processing task, QSD is not essential for many NLP applications that do not require deep understanding. Second, there has been a lack of comprehensive scope-disambiguated corpora, resulting in the lack of work on extensive 1. Three words start with a capital letter. A deep understanding of this sentence, requires deciding whether each word in the se"
P13-1007,D09-1152,0,0.0935619,"ection ic. The outscoping relation 1d > 2 in (6) states that every line in the collection, denoted by 1c, starts with its own punctuation character. Similarly, 1d > 3 indicates that every line has its own next non-blank line. Figure 4(a) shows a DAG for the scoping in (6). In (7) we have a sentence containing a negation. In QuanText, negation chunks are labeled with an uppercase “N” followed by a number. 3.3 Feature selection Previous work has shown that the lexical item of quantifiers and syntactic clues (often extracted from phrase structure trees) are good at predicting quantifier scoping. Srinivasan and Yates (2009) use the semantics of the head noun in a quantified NP to predict the scoping. MA11 also find the lexical item of the head noun to be a good predictor. In this paper, we introduce a new set of syntactic features which we found very informative: the “type” dependency features of de Marneffe et al. (2006). Adopting this new set of features, we outperform MA11’s system by a large margin. Another point to mention here is that the features that are predictive of the relative scope of quantifiers are not necessarily as helpful when determining the scope of negation and vice versa. Therefore we do no"
P17-1084,W08-2227,1,0.8411,"Missing"
P17-1084,K16-1007,1,0.85042,"such 7 http://trips.ihmc.us/parser/cgi/parse Refer to http://trips.ihmc.us/parser/ LFDocumentation.pdf for the full list of semantic roles in TRIPS parser. 9 A common construction which needs negation propagation is Neither X nor Y are ... . 8 6 https://code.google.com/archive/p/ word2vec/ 911 Comparative&gt; X is sweet -er Scale/+ than Y. Ground Figure Figure 8: The comparison construction predicted for the sentence X is sweeter than Y. ferent entities can be compared. In order to extract such scales and the relative standing of items on them we use the structured prediction model presented in Bakhshandeh et al. (2016), which given a sentence predicts its comparison structures. Figure 8 shows an example predicate-argument structure that is predicted by this model. We use pretrained model on the annotated corpus (Bakhshandeh et al., 2016) of comparison structures. Given a comparison structure such as the one presented in Figure 8, we can extract the information that on the scale of ‘sweetness’ X is higher than Y. It is clear that one can build a large knowledge base of such relations by reading large collections of comparison paragraphs. We populate our knowledge base of relative information about entities a"
P17-1084,W13-2322,0,0.0175983,"= n1 , y = n2 and x = n2 , y = n1 . Encoder-Decoder Recurrent Neural Net 5 Convolutional Neural Network (CNN) Encoder. As shown in the Figure 5b, this model first uses a Convolutional Neural Network (CNN) (LeCun and Bengio, 1998) for encoding the paragraph (Kim, 2014). We train a simple CNN with one layer of convolution on top of pre-trained word vectors. Here we use the word vectors trained by http://mattmahoney.net/dc/text8.zip 910 as X is sweeter which shares the same shortcomings. Our approach for better representation of comparison paragraphs starts with a broad-coverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008). A semantic parser maps an input sentence to its formal meaning representation, operating at the generic natural language level. Here we use the TRIPS7 (Allen et al., 2008) broad-coverage semantic parser. TRIPS provides a very rich semantic structure; mainly it provides sense disambiguated deep structures augmented with semantic ontology types. Figure 7 shows an example TRIPS semantic parse. In this graph representation, each node specifies a word in bold along with its corresponding ontology type on its left. The edges in the graph are semantic roles8 . As you"
P17-1084,W08-2222,0,0.019852,", y = n1 . Encoder-Decoder Recurrent Neural Net 5 Convolutional Neural Network (CNN) Encoder. As shown in the Figure 5b, this model first uses a Convolutional Neural Network (CNN) (LeCun and Bengio, 1998) for encoding the paragraph (Kim, 2014). We train a simple CNN with one layer of convolution on top of pre-trained word vectors. Here we use the word vectors trained by http://mattmahoney.net/dc/text8.zip 910 as X is sweeter which shares the same shortcomings. Our approach for better representation of comparison paragraphs starts with a broad-coverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008). A semantic parser maps an input sentence to its formal meaning representation, operating at the generic natural language level. Here we use the TRIPS7 (Allen et al., 2008) broad-coverage semantic parser. TRIPS provides a very rich semantic structure; mainly it provides sense disambiguated deep structures augmented with semantic ontology types. Figure 7 shows an example TRIPS semantic parse. In this graph representation, each node specifies a word in bold along with its corresponding ontology type on its left. The edges in the graph are semantic roles8 . As you can see, t"
P17-1084,P16-1223,0,0.0461886,"Missing"
P17-1084,W10-2903,0,0.0332183,"Missing"
P17-1084,D14-1181,0,0.00357922,"work forward, we take the probability of the decoder logits and choose the ordering which has the highest probability. (3) where C(wi )x,y indicates the context in which any occurrences of X have been replaced with x and Y’s have been replaced with y. For binary choice classification, we use the same modeling except that we only consider x = n1 , y = n2 and x = n2 , y = n1 . Encoder-Decoder Recurrent Neural Net 5 Convolutional Neural Network (CNN) Encoder. As shown in the Figure 5b, this model first uses a Convolutional Neural Network (CNN) (LeCun and Bengio, 1998) for encoding the paragraph (Kim, 2014). We train a simple CNN with one layer of convolution on top of pre-trained word vectors. Here we use the word vectors trained by http://mattmahoney.net/dc/text8.zip 910 as X is sweeter which shares the same shortcomings. Our approach for better representation of comparison paragraphs starts with a broad-coverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008). A semantic parser maps an input sentence to its formal meaning representation, operating at the generic natural language level. Here we use the TRIPS7 (Allen et al., 2008) broad-coverage semantic parser. TRIPS pr"
P17-1084,P15-2115,0,0.0624693,"Missing"
P17-1084,H92-1116,0,0.0846736,"Missing"
P17-1084,N16-1098,1,0.857984,"Missing"
P17-1084,D16-1264,0,0.0847624,"Missing"
P17-1084,speer-havasi-2012-representing,0,0.0457894,"Missing"
P17-1084,D11-1142,0,\N,Missing
P18-2119,P17-2097,0,0.228939,"Missing"
P18-2119,D17-1168,0,0.16196,"Missing"
P18-2119,W17-5401,0,0.0658452,"Missing"
P18-2119,K17-1004,0,0.161303,"Tv1.0 dataset. The feature distribution plots can be found in the supplementary material. As of 15th February 2018. http://mturk.com 753 Right ending Wrong ending p-value # of Tokens 8.705 8.466 6.63 × 10−5 Stanford Sentiment 2.04 2.02 0.526 VADER Sentiment 0.146 0.011 3.48 × 10−54 Frazier 1.09 1.08 0.135 Yngve 1.15 1.17 0.089 Table 2: The mean value for the ‘right endings’ and the ‘wrong endings’ for the two sample T-tests conducted for each feature. Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b)) as features. We see char-grams such as “sn’t” and “not” appear more commonly in the ‘wrong endings’, suggesting heavy negation. In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’. Artifacts such as ‘pizza’ are common in ’wrong endings,’ which could suggest that for a given topic, the authors may replace an object in a right ending with a wrong one and quickly think up a common item such as pizza to create a ‘wrong’ one. An extensive analysis of these features, including the n-gram analysis, can be found in the supplementary material. tained by"
P18-2119,W17-0907,0,0.0137958,"Tv1.0 dataset. The feature distribution plots can be found in the supplementary material. As of 15th February 2018. http://mturk.com 753 Right ending Wrong ending p-value # of Tokens 8.705 8.466 6.63 × 10−5 Stanford Sentiment 2.04 2.02 0.526 VADER Sentiment 0.146 0.011 3.48 × 10−54 Frazier 1.09 1.08 0.135 Yngve 1.15 1.17 0.089 Table 2: The mean value for the ‘right endings’ and the ‘wrong endings’ for the two sample T-tests conducted for each feature. Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b)) as features. We see char-grams such as “sn’t” and “not” appear more commonly in the ‘wrong endings’, suggesting heavy negation. In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’. Artifacts such as ‘pizza’ are common in ’wrong endings,’ which could suggest that for a given topic, the authors may replace an object in a right ending with a wrong one and quickly think up a common item such as pizza to create a ‘wrong’ one. An extensive analysis of these features, including the n-gram analysis, can be found in the supplementary material. tained by"
P18-2119,P14-5010,0,0.00452567,"Missing"
P18-2119,N16-1098,1,0.920871,"SCT dataset, which overcomes some of the biases. We benchmark a few models on the new dataset and show that the topperforming model on the original SCT dataset fails to keep up its performance. Our findings further signify the importance of benchmarking NLP systems on various evolving test sets. 1 Introduction Story comprehension has been one of the longestrunning ambitions in artificial intelligence (Dijk, 1980; Charniak, 1972). One of the challenges in expanding the field had been the lack of a solid evaluation framework and datasets on which comprehension models can be trained and tested. Mostafazadeh et al. (2016) introduced the Story Cloze Test (SCT) evaluation framework to address * This work was performed at University of Rochester. 752 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–757 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Context Right Ending Wrong Ending Ramona was very unhappy in her job. She asked for a raise, but was denied. The refusal prompted her to aggressively comb the want ads. She found an interesting new possibility and set up an interview. The teacher was walking wit"
P18-2119,S18-2023,0,0.0487891,"Missing"
P18-2119,W17-0911,0,0.0423556,"Missing"
P18-2119,W17-0910,0,0.0519401,"Missing"
P79-1021,J80-3003,0,\N,Missing
P81-1017,T75-2037,0,0.0726847,"Missing"
P81-1017,P79-1009,0,0.0153954,"ions that seems powerful enough to define a wide range of event and action verbs in English. This problem is interesting for two reasons• The first is that such a model is necessary to express the meaning of many sentences. The second is to analyze the language production and comprehension processes themselves as purposeful action. This was suggested some time ago by Bruce [1975] and Schmidt [1975]. Detailed proposals have been implemented recently for some aspects of language production [Cohen, 1978] and comprehension [Alien. 1979]. As interest in these methods grows (e.g., see [Grosz, 1979; Brachman, 1979]). the inadequacy of existing action models becomes increasingly obvious. The notions of prerequisite, result, and methods of performing actions will not arise in this study. While they are iraportant for reasoning about how to attain goals, they don&apos;t play an explicit role in defining when an action can be said to have occurred. To make this point clear, consider the simple action of turning on a light. There are few physical activities that are a necessary part of performing this action, Depending on the context, vastly different patterns or&quot; behavior can be classified as the same action, l"
P82-1004,P81-1017,1,0.84345,"e primary mode of interaction, others are also important. axioms that handle new features. In consistency mode, the query is checked to see if it is logically consistent with the facts in the knowledge base with respect to the limited inference mechanism. While consistency in general is undecidable, with respect to the limited inference mechanism it is computationally feasible. Note that, since the retriever is defined by a set of axioms rather than a program, consistency mode is easy to define. For example, we are currently incorporating a model of temporal knowledge based on time intervals [Allen, 1981a]. This is done by allowing any object, event, or relation to be qualified by a time interval as follows: for any untimed concept x, and any time interval t, there is a timed concept consisting of x viewed during t which is expressed by the term (t-concept x t). Another important mode is compatibility mode, which is very useful for determining the referents of description. A query in compatibility mode succeeds if there is a set of equality and inequality assertions that can be assumed so that the query would succeed in provability mode. For instance, suppose someone refers to an event in whi"
P84-1063,P84-1063,1,0.0511963,"Missing"
P89-1026,J80-3002,0,0.72166,"Missing"
P89-1026,J80-3003,0,0.72886,"Missing"
P94-1001,J86-3001,0,0.0742002,"Missing"
P94-1041,P92-1008,0,0.299476,"Missing"
P94-1041,A88-1019,0,0.435702,"nd its editing terms are deleted (this is done to prevent the algorithm from trying to annotate the fresh start as a repair). The speech is not marked with any intonational information, nor is any form of punctuation inserted. The results are given in Table 4. Detection Recall Detection Precision Correction Recall Correction Precision I think we I n e e d l to uh I I I n e e d l rI m I et I r] m I Algorithm Our algorithm for labeling potential repair patterns encodes the assumption that speech repairs can be processed one at a time. The algorithm runs in lockstep with a part-of-speech tagger (Church, 1988), which is used for deciding possible word replacements. Words are fed in one at a time. The detection clues are checked first. If one of them succeeds, and there is not a repair being processed, then a new repair pattern is started. Otherwise, if the clue is consistent with the current repair pattern, then the pattern is updated; otherwise, the current one is sent off to be judged, and a new repair pattern is started. When a new repair is started, a search is made to see if any of the text can contribute word correspondences to the repair. Likewise, if there is currently a repair being built,"
P94-1041,P93-1008,0,0.0664767,"Missing"
P94-1041,H94-1034,1,0.527523,"n and Schubert, 199 I), which is a long term research project to build a conversationally proficient planning assistant, we are collecting a corpus of problem solving dialogs. The dialogs involve two participants, one who is playing the role of a user and has a certain task to accomplish, and another, who is playing the role of the system by acting as a planning assistant. 4 The entire corpus consists of 112 dialogs totaling almost eight hours in length and containing about 62,000 words, 6300 speaker turns, and 40 different speakers. These dialogs have been segmented into utterance files (cf. Heeman and Allen, 1994b); words 3They referred to modification repairs as nontrivial repairs, and to abridged repairs as trivial repairs; however, these terms are misleading. Consider the utterance &quot;send it back to Elmira uh to make OJ&quot;. Determining that the corrected text should be &quot;send it back to Elmira to make OJ&quot; rather than &quot;send it back to make OJ&quot; is non trivial. 4Gross, Allen and Traum (1992) discuss the manner in which the first set of dialogues were collected, and provide transcriptions. 296 have been transcribed and the speech repairs have been annotated. For a training set, we use 40 of the dialogs, co"
P94-1041,P83-1019,0,0.801075,"Missing"
P94-1041,P93-1007,0,0.27945,"Missing"
P94-1041,J93-2006,0,0.039793,"diting terms when they have a sentential meanings, as in &quot;I guess we should load the oranges.&quot; 7About half of the difference between the detection recall rate and the correction recall rate is due to abridged repairs being misclassified as modification repairs. 299 Part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context (Church, 1988). The sentential context is typically approximated by only a set number of previous categories, usually one or two. Good part-of-speech results can be obtained using only the preceding category (Weischedel et al., 1993), which is what we will be using. In this case, the number of states of the Markov model will be N, where N is the number of tags. By using the Viterbi algorithm, the part-of-speech tags that lead to the maximum probability path can be found in linear time. Figure 2 gives a simplified view of a Markov model for part-of-speech tagging, where Ci is a possible category for the ith word, wi, and Gi+l is a possible category for word wi+l. The category transition probability is simply the probability of category Ci+l following category Gi, which is written as P(Ci+l ]Ci). The probability of word wi+"
P94-1041,H93-1008,0,\N,Missing
P96-1009,J90-2002,0,0.0453903,"s c o m p l e t e l y uninterpretable, the parser would still produce output - a TELL act with no content. For example, consider an utterance from the sample dialogue that was garbled: OKAY NOW ! TAKE THE LAST TRAIN IN GO FROM ALBANY TO IS. T h e best sequence of speech a c t s to cover this input consists of three acts: 1. a CONFIRM/ACKNOWLEDGE (OKAY) 2. a TELL, with content to take the last train (NOW I Since these experiments were performed, we have enhanced the channel model by relaxing the constraint that replacement errors be aligned on a word-by-word basis. We employ a fertility model (Brown et al, 1990) that indicates how likely each word is to map to multiple words or to a partial word in the SR output. This extension allows us to better handle the second example above, replacing TO TRY with DETROIT. For more details, see Ringger and Allen (1996). TAKE THE LAST TRAIN) 3. a REQUEST to go from Albany (Go FROM ALBANY) Note that the to is at the end of the utterance is simply ignored as it is uninterpretable. While not present in the output, the presence of unaccounted words will lower the parser's confidence score that it assigns to the interpretation. The actual utterance was Okay now let's t"
P96-1009,J86-3001,0,0.00462812,"st of which are domain-independent. For example, there is no general rule for PP attachment in the grammar. Rather there are rules for temporal adverbial modification (e.g., at eight o'clock), locational modification (e.g., in Chicago), and so on. 6. Robust Speech Act Processing The dialogue manager is responsible for interpreting the speech acts in context, formulating responses, and maintaining the system's idea of the state of the discourse. It maintains a discourse state that consists of a goal stack with similarities to the plan stack of Litman & Allen (1987) and the attentional state of Grosz & Sidner (1986). Each element of the stack captures 1. the domain or discourse goal motivating the segment 2. the object focus and history list for the segment 3. information on the status of problem solving activity (e.g., has the goal been achieved yet or not). The end result of parsing is a sequence of speech acts rather than a syntactic analysis. Viewing the output as a sequence of speech acts has significant impact on the form and style of the grammar. It forces an emphasis on encoding semantic and pragmatic features in the grammar. There are, for instance, numerous rules that encode specific convention"
P96-1009,H93-1074,0,0.0303486,"Missing"
P96-1009,P89-1026,1,\N,Missing
P97-1033,P92-1008,0,0.824352,"ce, the problem of identifying discourse markers also needs to be addressed with the segmentation and speech repair problems. These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often 254 signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994). However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983). Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation. Dialogs Speakers Words Turns Discourse Markers Boundary Tones Turn-Internal Boundary Tones Abridged Repairs Modification Repairs Fresh Starts Editing Terms 98 34 58298 6163 8278 10947 5535 423 1302 671 1128 Table 1: Frequency of Tones, Repairs and Editing Terms in the Trains Corpus Of course when dealing with spoken dialogue, one cannot forget the"
P97-1033,H92-1023,0,0.0820621,"bout the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as &apos;is the third bit of the POS tag encoding equal to one?&apos; Figure 1 shows a POS classification tree. The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag. Unlike other work (e.g. (Black et al., 1992; Materman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag. This has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information. As well, it constrains the task of building the word classification trees since the major distinctions are captured by the POS classification tree. IfVP = argmaxPr(WPIA) W,P :- arg max = argmaxPr(AIWP ) Pr(WP) WP 4 Pr(A[WP) P r ( W P ) Augmenting the Model Just as we redefined the speech recogniti"
P97-1033,P83-1019,0,0.306641,"Missing"
P97-1033,J93-3003,0,0.204047,"Missing"
P97-1033,J93-2004,0,0.0482919,"below illustrates how a repair is annotated in this scheme. E x a m p l e 6 (d93-15.2 u t t 4 2 ) engine two from Elmi(ra)- or engine three from Elmira m l r2 m3 m4 ml r2 m3 m4 ip:mod Tet 3 A POS-Based Language Model The goal of a speech recognizer is to find the sequence of words l~ that is maximal given the acoustic signal A. However, for detecting and correcting speech repairs, and identifying boundary tones and discourse markers, we need to augment the model so that it incorporates shallow statistical analysis, in the form of POS tagging. The POS tagset, based on the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz, 1993), includes special tags for denoting when a word is being used as a discourse marker. In this section, we give an overview of our basic language model that incorporates POS tagging. Full details can be found in (Heeman and Allen, 1997; Heeman~ 1997). To add in POS tagging, we change the goal of the speech recognition process to find the best word and POS tags given the acoustic signal. The derivation of the acoustic model and language model is now as follows. The first term Pr(AIWP ) is the factor due to the acoustic model, which we can approximate by Pr(A[W). The second term P r ( W P ) is t"
P97-1033,J92-4003,0,0.0137926,"ta. The decision tree algorithm starts with all of the training data in a single leaf node. For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where tile impurity measures how well each leaf predicts the events in the node. After the tree is grown, a heldout dataset is used to smooth the probabilities of each node with its parent (Bahl et al., 1989). To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as &apos;is the third bit of the POS tag encoding equal to one?&apos; Figure 1 shows a POS classification tree. The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag. Unlike other work (e.g. (Black et al., 1992; Materman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we buil"
P97-1033,P94-1041,1,0.95435,"ourse markers also needs to be addressed with the segmentation and speech repair problems. These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often 254 signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994). However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983). Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation. Dialogs Speakers Words Turns Discourse Markers Boundary Tones Turn-Internal Boundary Tones Abridged Repairs Modification Repairs Fresh Starts Editing Terms 98 34 58298 6163 8278 10947 5535 423 1302 671 1128 Table 1: Frequency of Tones, Repairs and Editing Terms in the Trains Corpus Of course when dealing with spoken dialogue, one cannot forget the initial problem of determ"
P97-1033,P97-1033,1,0.0511966,"Missing"
P97-1033,P95-1037,0,\N,Missing
P97-1033,J99-4003,1,\N,Missing
S10-1062,uzzaman-allen-2010-trios,1,0.791136,"type (onttype). TimeML tries to capture event information by very coarse-grained event features class or pos. The ontology type feature captures more fine-grained information about the event, but still coarse-grained than the words. The extraction rules also map our fine-grained types to the coarse-grained TimeML event class. We also extract relations between events (SLINK), whenever one event syntactically dominates the other, so it extracts more than TimeML’s SLINKs and another new relation, relation between event and its arguments (RLINK). Details about these new additions can be found in UzZaman and Allen (2010). This system extracts events from the TempEval-2 corpus with high recall. However, this high performance comes with the expense of precision. The reasons for lower precision include, (i) the fact that generic events are not coded as events in TempEval, (ii) errors in parsing and, (iii) legitimate events found by the parser but missed by TempEval annotators. To remedy this problem, we introduced a MLN based filtering classifier, using the event features extracted from the TRIPS parser. The formulas in MLN for filtering were derived by linguistic intuition and by checking the errors in our deve"
S10-1062,P09-1046,0,0.221808,"ormalized value of temporal expression, as suggested by TimeML scheme. We take the Document Creation Time (DCT) and then calculate the values for different dates in terms of document creation date, e.g. last month, Sunday, today. We will make our type and value extractor and temporal expression extractor modules available9 for public use. 4 Temporal Relation (Task C – F) Identification We identify temporal relations using a Markov Logic Network classifier, namely thebeast, by using linguistically motivated features that we extracted in previous steps. Our work matches closely with the work of Yoshikawa et al. (2009). We only consider the local classifiers, but we use more linguistically motivated features and features generated from text, whereas they used TempEval-1’s (Verhagen et al., 2007) annotations as input, along with their derived features. Other participants in TempEval 2 also used features from annotated corpus, making us the only group in TempEval-2 to use own-generated entities (events and temporal expression) and features. TempEval-2 has four subtasks for identifying temporal relations. The tasks are: (C) Determine the temporal relation between an event and temporal expression in the same se"
S10-1062,W08-2227,1,0.825067,"Missing"
S10-1062,A00-2008,0,\N,Missing
S10-1062,S07-1014,0,\N,Missing
S12-1022,P92-1005,0,0.784677,"is to use an Underspecified Representation (UR), that is to encode the ambiguity in the semantic representation and leave scoping underspecified. In an early effort, Woods (1986) developed an unscoped logical form where the above sentence is represented (roughly) as the formula: If we had pulled out A first, we would have had the other reading, with Every having wide scope. Hobbs and Shieber (1987) extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach. Since the introduction of Quasi Logical Form (Alshawi and Crouch, 1992), there has been a lot of work on designing constraint-based underspecification formalisms where the readings of a UR are not defined in a constructive fashion as shown above, but rather by a set of constraints. A fully-scoped structure is a reading iff it satisfies all the constraints. The advantage of these frameworks is that as the processing goes deeper, new (say pragmatically-driven) constraints can be added to the representation in order to filter out unwanted readings. Hole Semantics (Bos, 1996; Bos, 2002), Constraint Language for Lambda Structures (CLLS) (Egg et al., 2001), and Minimal"
S12-1022,P01-1019,0,0.327745,"rk on designing constraint-based underspecification formalisms where the readings of a UR are not defined in a constructive fashion as shown above, but rather by a set of constraints. A fully-scoped structure is a reading iff it satisfies all the constraints. The advantage of these frameworks is that as the processing goes deeper, new (say pragmatically-driven) constraints can be added to the representation in order to filter out unwanted readings. Hole Semantics (Bos, 1996; Bos, 2002), Constraint Language for Lambda Structures (CLLS) (Egg et al., 2001), and Minimal Recursion Semantics (MRS) (Copestake et al., 2001) are among these frameworks. In an effort to bridge the gap between the above formalisms, a graph theoretic model of scope underspecification was defined by Bodirsky et al. (2004), called Weakly Normal Dominance Graphs. This 142 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 142–150, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Figure 1: UG for Every child of a politician runs. Figure 2: Solutions of the UG in Figure 1. framework and its ancestor, Dominance Constraints (Althaus et al., 2003), are broad frameworks for solving c"
S12-1022,P04-1032,0,0.81241,"CLLS only generates underspecified representations that follow the definition of net and hence can be solved in polynomial time. They also prove that the same efficient algorithms can be used to solve the underspecification structures of Hole Semantics which satisfy the net condition. Unlike Hole Semantics and CLLS, MRS implicitly carries label-to-label constraints; hence the concept of net could not be applied to MRS. In order to address this, Niehren and Thater (2003) define the notion of weak net and conjecture that it covers all semantically complete MRS structures occurring in practice. Fuchss et al. (2004) supported the claim by investigating MRS structures in the Redwoods corpus (Oepen et al., 2002). Later coherent sentences were found in other corpora or suggested by other researchers (see Section 6.2.2 in Thater (2007)), whose UR violates the net condition, invalidating the conjecture. However, violating the net condition occurs in a similar way in those examples, suggesting a family of non-net structures, characterized in Section 4.2. Since then, it has been an open question whether there exists a tractable superset of weak nets, covering this family of non-net UGs. In the rest of this pape"
S12-1022,J87-1005,0,0.879527,"Every has wide scope) and one in which there is a unique website for all the politicians (i.e., Every has narrow scope). Since finding the most preferred reading automatically is very hard, the most widely adopted solution is to use an Underspecified Representation (UR), that is to encode the ambiguity in the semantic representation and leave scoping underspecified. In an early effort, Woods (1986) developed an unscoped logical form where the above sentence is represented (roughly) as the formula: If we had pulled out A first, we would have had the other reading, with Every having wide scope. Hobbs and Shieber (1987) extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach. Since the introduction of Quasi Logical Form (Alshawi and Crouch, 1992), there has been a lot of work on designing constraint-based underspecification formalisms where the readings of a UR are not defined in a constructive fashion as shown above, but rather by a set of constraints. A fully-scoped structure is a reading iff it satisfies all the constraints. The advantage of these frameworks is that as the processing goes deeper, new (say pragmat"
S12-1022,P10-1004,0,0.014156,"st using Regular Tree Grammars for scope underspecification, a probabilistic version of which could be used to find the best reading. The framework goes beyond the formalisms discussed in this paper and is expressively complete in Ebert (2005)’s sense of completeness, i.e. it is able to describe any subset of the readings of a UR. However, this power comes 149 at the cost of exponential complexity. In practice, RTG is built on top of weak nets, benefiting from the compactness of this framework to remain tractable. Being a super set of weak net, super net provides a more powerful core for RTG. Koller and Thater (2010) address the problem of finding the weakest readings of a UR, which are those entailed by some reading(s), but not entailing any other reading of the UR. By only considering the weakest readings, the space of solutions will be dramatically reduced. Note that entailment using the weakest readings is sound but not complete. 6 Summary and Future work Weakly normal dominance graph brings many current constraint-based formalisms under a uniform framework, but its configurability is intractable in its general form. In this paper, we present a tractable subset of this framework. We prove that this su"
S12-1022,P03-1047,0,0.854464,"r outside a 1 The main difference is in the concept of solution in the two frameworks. See Section 4.3 for details. 143 Figure 3: UG for the sentence in (5). polynomial-time algorithms for its satisfiability and enumeration problems in Section 3. In Section 4, we compare our framework with nets and weak nets. Section 5 discusses the related work, and Section 6 summarizes this work and discusses future work. 2 Figure 4: Two of the solutions to the UG in Figure 3. to-label constraints (e.g. the constraint between Every(x) and Run(x) in Figure 1) are not allowed. Using a sample grammar for CLLS, Koller et al. (2003) conjecture that the syntax/semantics interface of CLLS only generates underspecified representations that follow the definition of net and hence can be solved in polynomial time. They also prove that the same efficient algorithms can be used to solve the underspecification structures of Hole Semantics which satisfy the net condition. Unlike Hole Semantics and CLLS, MRS implicitly carries label-to-label constraints; hence the concept of net could not be applied to MRS. In order to address this, Niehren and Thater (2003) define the notion of weak net and conjecture that it covers all semantical"
S12-1022,P08-1026,0,0.018761,"characterize the set of all MRS structures generated by the MRS semantic composition process (Manshadi et al., 2008). CF-UR in its general form is not tractable. Therefore, we define a notion of coherence called heart-connectedness and show that all heart-connected CF-UR structures can be solved efficiently. We also show that heart-connected CF-UR covers the family of non-net structures, so CF-UR is in fact the first framework to address the non-net structures. In spite of that, CFUR is quite restricted and does not allow for adding new constraints after semantic composition. In recent work, Koller et al. (2008) suggest using Regular Tree Grammars for scope underspecification, a probabilistic version of which could be used to find the best reading. The framework goes beyond the formalisms discussed in this paper and is expressively complete in Ebert (2005)’s sense of completeness, i.e. it is able to describe any subset of the readings of a UR. However, this power comes 149 at the cost of exponential complexity. In practice, RTG is built on top of weak nets, benefiting from the compactness of this framework to remain tractable. Being a super set of weak net, super net provides a more powerful core for"
S12-1022,C02-2025,0,0.0401448,"an be solved in polynomial time. They also prove that the same efficient algorithms can be used to solve the underspecification structures of Hole Semantics which satisfy the net condition. Unlike Hole Semantics and CLLS, MRS implicitly carries label-to-label constraints; hence the concept of net could not be applied to MRS. In order to address this, Niehren and Thater (2003) define the notion of weak net and conjecture that it covers all semantically complete MRS structures occurring in practice. Fuchss et al. (2004) supported the claim by investigating MRS structures in the Redwoods corpus (Oepen et al., 2002). Later coherent sentences were found in other corpora or suggested by other researchers (see Section 6.2.2 in Thater (2007)), whose UR violates the net condition, invalidating the conjecture. However, violating the net condition occurs in a similar way in those examples, suggesting a family of non-net structures, characterized in Section 4.2. Since then, it has been an open question whether there exists a tractable superset of weak nets, covering this family of non-net UGs. In the rest of this paper, we answer this question. We modify the definition of weak net to define a superset of it, whi"
S13-2001,derczynski-gaizauskas-2010-analysing,1,0.706855,"ems (silver). The TempEval-3 platinum evaluation corpus was annotated/reviewed by the organizers, who are experts in the area. This process used the TimeML Annotation Guidelines v1.2.1 (Saur´ı et al., 2006). Every file was annotated independently by at least two expert annotators, and a third was dedicated to adjudicating between annotations and merging the final result. Some annotators based their work on TIPSem annotation suggestions (Llorens et al., 2012b). The GATE Annotation Diff tool was used for merging (Cunningham et al., 2013), a custom TimeML validator ensured integrity,3 and CAVaT (Derczynski and Gaizauskas, 2010) was used to determine various modes of TimeML mis-annotation and inconsistency that are inexpressable via XML schema. Post-exercise, that corpus (TempEval-3 Platinum with around 6K tokens, on completely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The Temp"
S13-2001,S10-1063,1,0.726158,"13 shows the results from event extraction. In this case, the previous state-of-the-art is not improved. Table 14 only shows the results obtained in temporal awareness by the state-of-the-art system since there were not participants on this task. We observe that TIPSemB-F approach offers competitive results, which is comparable to results obtained in TE3 English test set. 6.1 Comparison with TempEval-2 TempEval-2 Spanish test set is included as a subset of this TempEval-3 test set. We can therefore compare the performance across editions. Furthermore, we can include the full-featured TIPSem (Llorens et al., 2010), which unlike TIPSemB-F used the AnCora (Taul´e et al., 2008) corpus annotations as features including semantic roles. For timexes, as can be seen in Table 15, the original TIPSem obtains better results for timex extraction, which favours the hypothesis that machine learning systems are very well suited for this task (if the training data is sufficiently representative). However, for normalization (value F1), HeidelTime – a rule-engineered system – obtains better results. This indicates that rule-based approaches have the upper hand in this task. TIPSem uses FSS-TimEx TIPSemB-F TIPSem F1 59.0"
S13-2001,padro-stanilovsky-2012-freeling,0,0.0212522,"Missing"
S13-2001,S10-1062,1,0.680883,"ompletely new text) is released for the community to review 3 See and improve.4 Inter-annotator agreement (measured with F1, as per Hripcsak and Rothschild (2005)) and the number of annotation passes per document were higher than in existing TimeML corpora, hence the name. Details are given in Table 1. Attribute value scores are given based on the agreed entity set. These are for exact matches. The TempEval-3 silver evaluation corpus is a 600K word corpus collected from Gigaword (Parker et al., 2011). We automatically annotated this corpus by TIPSem, TIPSem-B (Llorens et al., 2013) and TRIOS (UzZaman and Allen, 2010). These systems were retrained on the corrected TimeBank and AQUAINT corpus to generate the original TimeML temporal relation set. We then merged these three state-of-the-art system outputs using our merging algorithm (Llorens et al., 2012a). In our selected merged configuration all entities and relations suggested by the best system (TIPSem) are added in the merged output. Suggestions from other systems (TRIOS and TIPSem-B) are added in the merged output, only if they are also supported by another system. The weights considered in our configuration are: TIPSem 0.36, TIPSemB 0.32, TRIOS 0.32."
S13-2001,P11-2061,1,0.562387,"the system identified both the entity and attribute (attr) together. Attribute Recall = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Refentity | Attribute Precision = |{∀x |x∈(Sysentity ∩Refentity )∧Sysattr (x)==Refattr (x)}| |Sysentity | Attribute F1-score = 2∗p∗r p+r Attribute (Attr) accuracy, precision and recall can be calculated as well from the above information. Attr Accuracy = Attr F1 / Entity Extraction F1 Attr R = Attr Accuracy * Entity R Attr P = Attr Accuracy * Entity P 4.2 Temporal Relation Processing To evaluate relations, we use the evaluation metric presented by UzZaman and Allen (2011).5 This metric captures the temporal awareness of an annotation in terms of precision, recall and F1 score. Temporal awareness is defined as the performance of an annotation as identifying and categorizing temporal relations, which implies the correct recognition and classification of the temporal entities involved in the relations. Unlike TempEval2 relation score, where only categorization is evaluated for relations, this metric evaluates how well pairs of entities are identified, how well the relations are categorized, and how well the events and temporal expressions are extracted. + |Sys− ∩"
S13-2001,S10-1010,1,\N,Missing
S13-2001,taule-etal-2008-ancora,0,\N,Missing
S15-2134,S13-2002,0,0.110539,"arated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-shelf system: trained and tested by organizers 5 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented wit"
S15-2134,Q14-1022,1,0.272596,"lar participants, optimized for task: • HITSZ-ICRC4 . rule-based timex module, SVM (liblinear) for event and relation detection and classification • hlt-fbk-ev1-trel1. SVM, separated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-s"
S15-2134,S10-1063,1,0.913075,"• hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-shelf system: trained and tested by organizers 5 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented with a postprocessing step. The goal is to analyze how a general time expression reasoner could improve results. The TREFL component is straightforward: resolve all ti"
S15-2134,S13-2001,1,0.797947,"Missing"
S15-2134,S07-1014,1,0.893706,"Missing"
S18-2028,J05-1004,0,0.440288,"and widely adopted in computational linguistics for semantic representations because of their compatibility with frame-based and graph-based (i.e., semantic networks) representations of meaning. Very roughly, computational approaches can be divided into two classes based on whether one believes there is a single universal set of roles (e.g., LiRICS (Bunt & Rosemary, 2002; Petukhova & Bunt, 2008), VerbNet (Kipper et al., 2008; Bonial et al., 2011)), or whether one believes each type may identify its own unique roles (e.g., FrameNet (Baker et al., 1998)). Straddling a middle ground is PropBank (Palmer et al., 2005), which uses a universal set of role names, but allows each type to define what their roles mean. Our interest is in defining a universal set of roles across all types. A key distinction that most frameworks make is between the inner (or core or argument) roles and the outer (or relational or adjunct) roles. The core roles identify objects that are typically required to fully specify the content of the type, while relational roles are typically optional but add additional information. For instance, in The snow melted into a puddle. The subject of this sentence is clearly a critical argument to"
S18-2028,petukhova-bunt-2008-lirics,0,0.0529159,"ion, namely, whether the roles of an unknown word sense can be derived from its definition. 2 Preliminaries Semantic roles have a long history, originating in linguistics as thematic roles (e.g., Fillmore, 1968; Dowty, 1991) and widely adopted in computational linguistics for semantic representations because of their compatibility with frame-based and graph-based (i.e., semantic networks) representations of meaning. Very roughly, computational approaches can be divided into two classes based on whether one believes there is a single universal set of roles (e.g., LiRICS (Bunt & Rosemary, 2002; Petukhova & Bunt, 2008), VerbNet (Kipper et al., 2008; Bonial et al., 2011)), or whether one believes each type may identify its own unique roles (e.g., FrameNet (Baker et al., 1998)). Straddling a middle ground is PropBank (Palmer et al., 2005), which uses a universal set of role names, but allows each type to define what their roles mean. Our interest is in defining a universal set of roles across all types. A key distinction that most frameworks make is between the inner (or core or argument) roles and the outer (or relational or adjunct) roles. The core roles identify objects that are typically required to fully"
S18-2028,P98-1013,0,0.759122,"uistics as thematic roles (e.g., Fillmore, 1968; Dowty, 1991) and widely adopted in computational linguistics for semantic representations because of their compatibility with frame-based and graph-based (i.e., semantic networks) representations of meaning. Very roughly, computational approaches can be divided into two classes based on whether one believes there is a single universal set of roles (e.g., LiRICS (Bunt & Rosemary, 2002; Petukhova & Bunt, 2008), VerbNet (Kipper et al., 2008; Bonial et al., 2011)), or whether one believes each type may identify its own unique roles (e.g., FrameNet (Baker et al., 1998)). Straddling a middle ground is PropBank (Palmer et al., 2005), which uses a universal set of role names, but allows each type to define what their roles mean. Our interest is in defining a universal set of roles across all types. A key distinction that most frameworks make is between the inner (or core or argument) roles and the outer (or relational or adjunct) roles. The core roles identify objects that are typically required to fully specify the content of the type, while relational roles are typically optional but add additional information. For instance, in The snow melted into a puddle."
swift-etal-2004-semi,brants-plaehn-2000-interactive,0,\N,Missing
swift-etal-2004-semi,J93-2004,0,\N,Missing
swift-etal-2004-semi,A00-2008,0,\N,Missing
swift-etal-2004-semi,H94-1020,0,\N,Missing
uzzaman-allen-2010-trios,S10-1062,1,\N,Missing
uzzaman-allen-2010-trios,S07-1014,0,\N,Missing
uzzaman-allen-2010-trios,W08-2227,1,\N,Missing
uzzaman-allen-2010-trios,P07-2044,0,\N,Missing
uzzaman-allen-2010-trios,P09-1046,0,\N,Missing
uzzaman-allen-2010-trios,P08-1090,0,\N,Missing
W02-0201,P99-1026,0,0.0606239,"Missing"
W02-0201,P99-1024,0,0.0208966,"r, if agent-based, are restricted to a pipelined flowof-information. The TRIPS dialogue architecture is agent-based and asynchronous, with several layers of information flow. We present this architecture and the synchronization issues we encountered in building a truly distributed, agentbased dialogue architecture. 1 Introduction More and more people are building dialogue systems. Architecturally, these systems tend to fall into two camps: those with pipelined architectures (e.g., (Lamel et al., 1998; Nakano et al., 1999)), and those with agent-based architectures (e.g., (Seneff et al., 1999; Stent et al., 1999; Rudnicky et al., 1999)). Agent-based architectures are advantageous because they free up system components to potentially act in a more asynchronous manner. However, in practice, most dialogue systems built on an agent-based architecture pass messages such that they are basically functioning in terms of a pipelined flow-of-information. Our original implementation of the TRIPS spoken dialogue system (Ferguson and Allen, 1998) was such an agent-based, pipelined flow-of-information system. Recently, however, we made changes to the system (Allen et al., 2001a) which allow it to take advantage of"
W02-0201,P94-1001,1,0.76676,"objectives, or exogenous events. If the system, say because of an exogenous event, decides to take initiative and communicate with the user, it sends an interaction act to the GM. The GM then, following the same path as above, outputs content to the user. 3 This is a selective broadcasts to the components which have registered for such messages. 2.2.2 Discourse Level The discourse level4 describes information which is not directly related to the task at hand, but rather is linguistic in nature. This information is represented as salience information (for Reference) and discourse obligations (Traum and Allen, 1994). When the user makes an utterance, the input passes (as detailed above) through the Speech Recognizer, to the Parser, and then to the IM, which calls Reference to do resolution. Based on this reference resolved form, the IM computes any discourse obligations which the utterance entails (e.g., if the utterance was a question, to address or answer it, also, to acknowledge that it heard the question). At this point, the IM broadcasts an “system heard” message, which includes incurred discourse obligations and changes in salience. Upon receipt of this message, Discourse Context updates its discou"
W04-0214,swift-etal-2004-semi,1,\N,Missing
W04-0214,J98-2001,0,\N,Missing
W04-0214,J96-3006,0,\N,Missing
W04-0214,brants-plaehn-2000-interactive,0,\N,Missing
W04-0214,J93-2004,0,\N,Missing
W04-0214,W98-1119,0,\N,Missing
W04-0214,J01-4003,1,\N,Missing
W04-0214,P02-1011,0,\N,Missing
W04-0214,J86-3001,0,\N,Missing
W04-0214,poesio-2000-annotating,0,\N,Missing
W04-0304,H92-1026,0,0.0243884,"phrases. Experiments with incremental feedback from a referAdvisor (reference) Inform Feedback Mediator Inform Client (parser) Modify Chart Figure 1: A General Architecture for Incremental Parsing ence resolution module and an NP suitability oracle are reported, and the ability of the implementation to incrementally instantiate semantically underspecified pronouns is outlined. We believe this research provides an important start towards developing endto-end continuous understanding models. 2 An Incremental Parsing Architecture Many current parsers fall into the class of historybased grammars (Black et al., 1992). The independence assumptions of these models make the parsing problem both stochastically and computationally tractable, but represent a simplification and may therefore be a source of error. In a continuous understanding framework, higher-level modules may have additional information that suggests loci for improvement, recognizing either invalid independence assumptions or errors in the underlying probability model. We have designed a general incremental parsing architecture (Figure 1) in which the Client, a dynamic programming parser, performs its calculations, the results of which are inc"
W04-0304,C98-1028,0,0.0112875,"s of natural human speech. Continuous understanding is necessary if the system is to respond before the entire utterance is analyzed, a prerequisite for incremental confirmation and clarification. The major computational advantage of continuous understanding models is that high-level expectations and feedback should be able to influence the search of lowerlevel processes, thus leading to a focused search through hypotheses that are plausible at all levels of processing. One of the major current applications of parsers that operate incrementally is for language modelling in speech recognition (Brill et al., 1998; Jelinek and Chelba, 1999). This work is important not only for its ability to improve performance on the speech recognition task; it also models the interactions between speech recognition and parsing in a continuous understanding system. Our research attempts to further the quest for continuous understanding by moving one step up the hierarchy, building an incremental parser which is the advisee rather than the advisor. We begin by presenting a general architecture for incremental interaction between the parser and higher-level modules, and then discuss a specific instantiation of this gene"
W04-0304,P93-1008,0,0.0222601,"n-path phenomenon and parsing preferences (Altmann and Steedman, 1988; Konieczny, 1996; Phillips, 1996). Moreover, a variety of eye-tracking experiments (Cooper, 1974; Tanenhaus and Spivey, 1996; Allopenna et al., 1998; Sedivy et al., 1999) suggest that complex semantic and referential constraints are incorporated on an incremental basis in human parsing decisions. Computational parsers, however, still tend to operate an entire sentence at a time, despite the advent of speech-to-intention dialogue systems such as Verbmobil (Kasper et al., 1996; Noth et al., 2000; Pinkal et al., 2000), Gemini (Dowding et al., 1993; Dowding et al., 1994; Moore et al., 1995) and TRIPS (Allen et al., 1996; Ferguson et al., 1996; Ferguson and Allen, 1998). Naturalness, robustness, and interactivity are goals of such systems, but control flow is typically the sequential execution of modules, each operating on the output of its predecessor; only after the entire sentence has been parsed do higher-level modules such as intention recognition and reference resolution get involved. In contrast to this sequential model is the continuous understanding approach, in which all levels of language analysis occur simultaneously, from sp"
W04-0304,P94-1016,0,0.0209813,"parsing preferences (Altmann and Steedman, 1988; Konieczny, 1996; Phillips, 1996). Moreover, a variety of eye-tracking experiments (Cooper, 1974; Tanenhaus and Spivey, 1996; Allopenna et al., 1998; Sedivy et al., 1999) suggest that complex semantic and referential constraints are incorporated on an incremental basis in human parsing decisions. Computational parsers, however, still tend to operate an entire sentence at a time, despite the advent of speech-to-intention dialogue systems such as Verbmobil (Kasper et al., 1996; Noth et al., 2000; Pinkal et al., 2000), Gemini (Dowding et al., 1993; Dowding et al., 1994; Moore et al., 1995) and TRIPS (Allen et al., 1996; Ferguson et al., 1996; Ferguson and Allen, 1998). Naturalness, robustness, and interactivity are goals of such systems, but control flow is typically the sequential execution of modules, each operating on the output of its predecessor; only after the entire sentence has been parsed do higher-level modules such as intention recognition and reference resolution get involved. In contrast to this sequential model is the continuous understanding approach, in which all levels of language analysis occur simultaneously, from speech recognition to in"
W04-0304,A00-2041,0,0.0542565,"ault, underspecified pronoun, and so cannot apply these restrictions to discriminate between referents. Our implementation performs the semantic vetting discussed above, but we have done no largescale experiments in this area. 6 Related Work There are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are designed as continuous understanding systems, where all levels of language analysis occur (virtually) simultaneously. For example, there are a number of robust semantic processing systems (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser w"
W04-0304,C02-1024,0,0.103995,"ms (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser which builds both a syntactic tree and a denotation-based semantic analysis as it parses. The denotations of constituents in the environment are used to inform parsing decisions, much as we use the static database of place names. However, the feedback in our system is richer, based on the context provided by the preceding discourse. Furthermore, as an instantiation of the general architecture presented in Section 2, our system is more easily extensible to other forms of feedback. 7 Future Work There is a catch-22 in that the accurate reference information necessary to impro"
W04-0304,W04-0214,1,0.799355,"hat a noun phrase constituent c would be in the final parse, conditioned on R and N , or Pr = p(c in final parse|R, N ). This probability was then linearly combined with the parser’s constituent probability, n Pp = p(c → wm ), according to the equation P (c) = (1 − λ) · Pp + λ · Pr for various values of λ. Evaluation using held-out data suggested that a value of λ = 0.2 would be optimal. This style of feedback is an example of chart subversion, as it is a direct modification of constituent probabilities by the Mediator, defining a new probability distribution. 4 Experiments The Monroe domain (Tetreault et al., 2004; Stent, 2001) is a series of task-oriented dialogues between human participants set in a simulated rescue operation domain, where participants collaboratively plan responses to emergency calls. Dialogues were recorded, broken up into utterances, and then transcribed by hand, removing speech repairs from the parser input. These transcriptions served as input for all experiments reported below. A probabilistic grammar was trained from supervised data, assigning PCFG probabilities for the rule expansions in the CFG backbone of the handcrafted, semantically constrained grammar. The parser was run"
W04-0304,P98-2229,0,0.0290917,"pecified pronoun, and so cannot apply these restrictions to discriminate between referents. Our implementation performs the semantic vetting discussed above, but we have done no largescale experiments in this area. 6 Related Work There are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are designed as continuous understanding systems, where all levels of language analysis occur (virtually) simultaneously. For example, there are a number of robust semantic processing systems (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser which builds"
W04-0304,P98-2236,0,0.199507,"noun, and so cannot apply these restrictions to discriminate between referents. Our implementation performs the semantic vetting discussed above, but we have done no largescale experiments in this area. 6 Related Work There are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are designed as continuous understanding systems, where all levels of language analysis occur (virtually) simultaneously. For example, there are a number of robust semantic processing systems (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser which builds both a syntactic"
W04-0304,H93-1008,0,\N,Missing
W04-0304,P98-1028,0,\N,Missing
W04-0304,C98-2231,0,\N,Missing
W04-0304,P93-1005,0,\N,Missing
W04-0304,C98-2224,0,\N,Missing
W04-2302,C00-1007,0,0.160911,"d surface generators for the air travel domain. Their input semantic form is a set of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter showed no significant difference. Most recently, Chen et al. utilized FERGUS (Bangalore and Rambow, 2000) and attempted to make it more domain independent in (Chen et al., 2002). There are two stochastic processes in FERGUS; a tree chooser that maps an input syntactic tree to a TAG tree, and a trigram language model that chooses the best sentence in the lattice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM. Work was done to try and use an independent LM, but (Rambow et al., 2001) found interrogatives to be unrepresented by a WSJ model and fell back on air travel models. This problem was not discussed in (Chen et al., 2002). Pe"
W04-2302,W00-1401,0,0.0453486,"eration is particularly difficult due to the diverse amount of correct output that can be generated. There are many ways to present a given semantic representation in English and what determines quality of content and form are often subjective measures. There are two general approaches to a surface generation evaluation. The first uses human evaluators to score the output with some pre-defined ranking measure. The second uses a quantitative automatic approach usually based on n-gram presence and word ordering. Bangalore et al. describe some of the quantitative measures that have been used in (Bangalore et al., 2000). Callaway recently used quantitative measures in an evaluation between symbolic and stochastic surface generators in (Callaway, 2003). The most common quantitative measure is Simple String Accuracy. This metric uses an ideal output string and compares it to a generated string using a metric that combines three word error counts; insertion, deletion, and substitution. One variation on this approach is tree-based metrics. These attempt to better represent how bad a bad result is. The tree-based accuracy metrics do not compare two strings directly, but instead build a dependency tree for the ide"
W04-2302,C02-1138,0,0.278187,"t of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter showed no significant difference. Most recently, Chen et al. utilized FERGUS (Bangalore and Rambow, 2000) and attempted to make it more domain independent in (Chen et al., 2002). There are two stochastic processes in FERGUS; a tree chooser that maps an input syntactic tree to a TAG tree, and a trigram language model that chooses the best sentence in the lattice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM. Work was done to try and use an independent LM, but (Rambow et al., 2001) found interrogatives to be unrepresented by a WSJ model and fell back on air travel models. This problem was not discussed in (Chen et al., 2002). Perhaps automatically extracted trees from the corpora are able to create"
W04-2302,P98-1116,0,0.144333,"within the air travel domain. The final generation system cannot be ported to a new domain without further effort. By creating grammar rules that convert a semantic form, some of these restrictions can be removed. The next section describes our stochastic approach and how it was modified from machine translation to spoken dialogue. 3 Stochastic Generation (HALogen) We used the HALogen framework (Langkilde-Geary, 2002) for our surface generation. HALogen was originally created for a domain within MT and is a sentence planner and a surface realizer. Analysis and MT applications can be found in (Langkilde and Knight, 1998; Knight and Langkilde, 2000). HALogen accepts a feature-value structure ranging from high-level semantics to shallow syntax. Figure 1 shows a mixture of both as an example. Given this input, generation is a two step process. First, the input form is converted into a word forest (a more efficient representation of a word lattice) as described in (LangkildeGeary, 2002). Second, the language model chooses the most probable path through the forest as the output sentence. (V68753 / move :TENSE past :AGENT (V68837 / person :QUANT three :NUMBER plural ) :THEME (V68846 / ambulance ) ) Figure 1: HALog"
W04-2302,W02-2103,0,0.729695,"re intentionally using two out-of-domain language models. Most of the work on FERGUS and the previous surface generation evaluations in dialogue systems are dependent on English syntax and word choice within the air travel domain. The final generation system cannot be ported to a new domain without further effort. By creating grammar rules that convert a semantic form, some of these restrictions can be removed. The next section describes our stochastic approach and how it was modified from machine translation to spoken dialogue. 3 Stochastic Generation (HALogen) We used the HALogen framework (Langkilde-Geary, 2002) for our surface generation. HALogen was originally created for a domain within MT and is a sentence planner and a surface realizer. Analysis and MT applications can be found in (Langkilde and Knight, 1998; Knight and Langkilde, 2000). HALogen accepts a feature-value structure ranging from high-level semantics to shallow syntax. Figure 1 shows a mixture of both as an example. Given this input, generation is a two step process. First, the input form is converted into a word forest (a more efficient representation of a word lattice) as described in (LangkildeGeary, 2002). Second, the language mo"
W04-2302,W00-0306,0,0.0627258,"based approach to surface generation does not use large linguistic databases but rather depends on language modeling of corpora to predict correct and natural utterances. The approach is attractive in comparison to templates and rule-based approaches because the language models implicitly encode the natural ordering of English. Recently, the results from corpus-based surface generation in dialogue systems have been within specific domains, the vast majority of which have used the Air Travel Domain with Air Travel corpora. Ratnaparkhi (Ratnaparkhi, 2000; Ratnaparkhi, 2002) and Oh and Rudnicky (Oh and Rudnicky, 2000) both studied surface generators for the air travel domain. Their input semantic form is a set of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter showed no significant difference. Most recently, Chen et al. utili"
W04-2302,2001.mtsummit-papers.68,0,0.0182222,"compares it to a generated string using a metric that combines three word error counts; insertion, deletion, and substitution. One variation on this approach is tree-based metrics. These attempt to better represent how bad a bad result is. The tree-based accuracy metrics do not compare two strings directly, but instead build a dependency tree for the ideal string and attempt to create the same dependency tree from the generated string. The score is dependent not only on word choice, but on positioning at the phrasal level. Finally, the most recent evaluation metric is the Bleu Metric from IBM(Papineni et al., 2001). Designed for Machine Translation, it scores generated sentences based on the n-gram appearance from multiple ideal sentences. This approach provides more than one possible realization of an LF and compares the generated sentence to all possibilities. Unfortunately, the above automatic metrics are very limited in mimicking human scores. The Bleu metric can give reasonable scores, but the results are not as good when only one human translation is available. These automatic metrics all compare the desired output with the actual output. We decided to ignore this evaluation because it is too depe"
W04-2302,H01-1055,0,0.21776,"Missing"
W04-2302,A00-2026,0,0.0523912,"d approach is difficult to port to new domains. The corpus-based approach to surface generation does not use large linguistic databases but rather depends on language modeling of corpora to predict correct and natural utterances. The approach is attractive in comparison to templates and rule-based approaches because the language models implicitly encode the natural ordering of English. Recently, the results from corpus-based surface generation in dialogue systems have been within specific domains, the vast majority of which have used the Air Travel Domain with Air Travel corpora. Ratnaparkhi (Ratnaparkhi, 2000; Ratnaparkhi, 2002) and Oh and Rudnicky (Oh and Rudnicky, 2000) both studied surface generators for the air travel domain. Their input semantic form is a set of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter sh"
W04-2302,P02-1040,0,\N,Missing
W04-2302,C98-1112,0,\N,Missing
W05-1525,A00-2008,0,0.0306644,"::Fruit Orange))) (THE v2 (:* LF::Vehicle Truck)) Figure 1: LF for Load the oranges into the truck. tures. An unscoped neo-Davidsonian semantic representation is built in parallel with the syntactic representation. A sample logical form (LF) representation for Load the oranges into the truck is shown above. The TRIPS LF provides the necessary information for reference resolution, surface speech act analysis, and interpretations for a wide variety of fragmentary utterances and conventional phrases typical in dialog. The LF content comes from a domain-independent ontology adapted from FrameNet (Johnson and Fillmore, 2000; Dzikovska et al., 2004) and linked to a domain-independent lexicon (Dzikovska, 2004). The parser uses a bottom-up chart algorithm with beam search. Alternative parses are scored with factors assigned to grammar rules and lexical entries by hand, because due to the limited amount of corpus data we have not yet been able to train a statistical model that outperforms our hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Wo"
W05-1525,N04-1013,0,0.0182154,"cal dialog systems such as plan-based assistants and tutorial systems. Development of such systems is time-consuming and costly as they are typically hand-crafted for each application, and dialog corpus data is more difficult to obtain than text. The TRIPS parser and grammar addresses these issues by providing broad coverage of common constructions in practical dialog and producing semantic representations suitable for dialog processing across domains. Our system bootstraps dialog system development in new domains and helps build parsed corpora.1 Evaluating deep parsers is a challenge (e.g., (Kaplan et al., 2004)). Although common bracketing accuracy metrics may provide a baseline, they are insufficient for applications such as ours that require complete and correct semantic representations produced by the parser. We evaluate our parser on bracketing accuracy against a statistical parser as a baseline, then on a word sense disambiguation task, and finally on full sentence syntactic and semantic accuracy in multiple domains as a realistic measure of system performance and portability. 2 The TRIPS Parser and Logical Form The TRIPS grammar is a linguistically motivated unification formalism using attribu"
W05-1525,swift-etal-2004-semi,1,0.838685,"ur hand-tuned factors. 3 Evaluation As a rough baseline, we compared the bracketing accuracy of our parser to that of a statistical parser (Bikel, 2002), Bikel-M, trained on 4294 TRIPS 196 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 196–197, c Vancouver, October 2005. 2005 Association for Computational Linguistics parse trees from the Monroe corpus (Stent, 2001), task-oriented human dialogs in an emergency rescue domain. 100 randomly selected utterances were held out for testing. The gold standard for evaluation is created with the help of the parser (Swift et al., 2004). Corpus utterances are parsed, and the parsed output is checked by trained annotators for full-sentence syntactic and semantic accuracy, reliable with a kappa score 0.79. For test utterances for which TRIPS failed to produce a correct parse, gold standard trees were manually constructed independently by two linguists and reconciled. Table 1 shows results for the 100 test utterances and for the subset for which TRIPS finds a spanning parse (74). Bikel-M performs somewhat better on the bracketing task for the entire test set, which includes utterances for which TRIPS failed to find a parse, but"
W05-1525,W04-0214,1,0.894526,"Missing"
W05-1526,C04-1055,1,0.772927,"alogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Experiments Introduction Unification parsers have problems with efficiency and selecting the best parse. Lexically-conditioned statistics as used by Collins (1999) may provide a solution. They have been used in three ways: as a postprocess for parse selection (Toutanova et al., 2005; Riezler et al., 2000; Riezler et al., 2002), a preprocess to find more probable bracketing structures (Swift et al., 2004), and online to rank each constituent produced, as in Tsuruoka et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same grammar and lexicon as TRIPS, but uses its statistical model to determine the order in which unifications are attempted. (trai"
W05-1526,W04-0214,1,0.849773,"Missing"
W05-1526,P00-1061,0,\N,Missing
W05-1526,W00-1320,0,\N,Missing
W05-1526,J03-4003,0,\N,Missing
W05-1526,P96-1009,1,\N,Missing
W05-1526,P02-1035,0,\N,Missing
W07-1207,J90-3001,0,0.130529,"ngrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as 51 a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addition, it has a relatively simple method of computing possible quantifier scoping, drawing from the approaches by (Hobbs & Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, the NP Every dog would be captured by the term (Every d1 DOG). The second type of term is the propositional term, which is represented in a neoDavidsonian representation (e.g., Parsons, 1990) using reified events and properties. It has the form (F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The propositional terms pro"
W07-1207,P02-1011,0,0.0400277,"fferent dialog systems use the same discourse processing, whether the task involves collaborative problem solving, learning from instruction or automated tutoring. 5.1 Reference Resolution Our domain-independent representation supports reference resolution in two ways. First, the quantifiers and dependency structure extracted from the sentence allow for implementing reference resolution algorithms based on extracted syntactic features. The system uses different strategies for reFigure 8: The LF-MRS conversion algorithm 53 solving each type of referring expression along the lines described in (Byron, 2002). Second, domain-independent semantic information helps greatly in resolving pronouns and definite descriptions. The general capability provided for resolving referring expressions is to search through the discourse history for the most recent entity that matches the semantic requirements, where recency within an utterance may be reordered to reflect focusing heuristics (Tetreault, 2001). For definite descriptions, the semantic information required is explicit in the lexicon. For pronouns, the parser can often compute semantic features from verb argument restrictions. For instance, the pronoun"
W07-1207,W05-1526,1,0.822144,"The grammar used in all our applications uses these hand-tuned rule weights, which have proven to work relatively well across domains. We do not use a statistical parser based on a trained corpus because in most dialogue-system projects, sufficient amounts of training data are not available and would be too time consuming to collect. In the one domain in which we have a reasonable amount of training data (about 9300 utterances), we experimented with a PCFG using trained probabilities with the Collins algorithm, but were not able to improve on the hand-tuned preferences in overall performance (Elsner et al., 2005). Figure 3 summarizes some of the most important preferences encoded in our rule weights. Because we are dealing with speech, which is often ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as 51 a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addi"
W07-1207,J87-1005,0,0.117043,"ith speech, which is often ungrammatical and fragmented, the grammar includes “robust” rules (e.g., allowing dropped determiners) that would not be found in a grammar of written English. 3 The Logical Form Language The logical form language captures a domainindependent semantic representation of the utterance. As shown later in this paper, it can be seen as 51 a variant of MRS (Copestake et al., 2006) but is expressed in a frame-like notation rather than predicate calculus. In addition, it has a relatively simple method of computing possible quantifier scoping, drawing from the approaches by (Hobbs & Shieber, 1987) and (Alshawi, 1990). A logical form is set of terms that can be viewed as a rooted graph with each term being a node identified by a unique ID (the variable). There are three types of terms. The first corresponds to generalized quantifiers, and is on the form (&lt;quant&gt; &lt;id&gt; &lt;type&gt; &lt;modifiers&gt;*). As a simple example, the NP Every dog would be captured by the term (Every d1 DOG). The second type of term is the propositional term, which is represented in a neoDavidsonian representation (e.g., Parsons, 1990) using reified events and properties. It has the form (F &lt;id&gt; &lt;type&gt; &lt;arguments&gt;*). The pro"
W07-1207,J01-4003,0,0.00831441,"ion algorithms based on extracted syntactic features. The system uses different strategies for reFigure 8: The LF-MRS conversion algorithm 53 solving each type of referring expression along the lines described in (Byron, 2002). Second, domain-independent semantic information helps greatly in resolving pronouns and definite descriptions. The general capability provided for resolving referring expressions is to search through the discourse history for the most recent entity that matches the semantic requirements, where recency within an utterance may be reordered to reflect focusing heuristics (Tetreault, 2001). For definite descriptions, the semantic information required is explicit in the lexicon. For pronouns, the parser can often compute semantic features from verb argument restrictions. For instance, the pronoun it carries little semantic information by itself, but in the utterance Eat it we know we are looking for an edible object. This simple technique performs well in practice. Because of the knowledge in the lexicon for role nouns such as author, we can also handle simple bridging reference. Consider the discourse fragment That book came from the library. The author …. The semantic represen"
W07-1207,W03-2106,0,\N,Missing
W07-1207,C94-1042,0,\N,Missing
W07-1207,P98-1013,0,\N,Missing
W07-1207,C98-1013,0,\N,Missing
W07-1207,P04-1032,0,\N,Missing
W08-2227,W07-1207,1,0.759675,"cal lookup. We also present an evaluation metric for the representation and use it to evaluate the performance of the TRIPS parser on the common task paragraphs. 343 344 Allen, Swift, and de Beaumont 1 Introduction As building rich semantic representations of text becomes more feasible, it is important to develop standard representations of logical form that can be used to share data and compare approaches. In this paper, we describe some general characteristics that such a logical form language should have, then present a graphical representation derived from the LF used in the TRIPS system (Allen et al., 2007). The Logical Form is a representation that serves as the interface between structural analysis of text (i.e., parsing) and the subsequent use of the information to produce knowledge, whether it be for learning by reading, question answering, or dialoguebased interactive systems. It’s important to distinguish two separable problems, namely the ontology used and the structure of the logical form language (LFL). The ontology determines the set of word senses and semantic relations that can be used. The LFL determines how these elements can be structured to capture the meaning of sentences. We ar"
W08-2227,C94-1042,0,0.0243195,"Missing"
W08-2227,A00-2008,0,0.0118824,"ring parsing, and reference resolution uses the semantic features to identify valid referents and discard invalid ones. The TRIPS LF ontology is designed to be linguistically motivated and domain independent. The semantic types and selectional restrictions are driven by linguistic considerations rather than requirements from reasoning components in the system (Dzikovska et al., 2003). Word senses are defined based on subcategorization patterns and domain independent selectional restrictions. As much as possible the semantic types in the LF ontology are compatible with types found in FrameNet (Johnson and Fillmore, 2000). FrameNet generally provides a good level of abstraction for applications since the frames are derived from corpus examples and can be reliably distinguished by human annotators. However we use a smaller, more general set of semantic roles for linking the syntactic and semantic arguments rather than FrameNet’s extensive set of specialized frame elements. The LF ontology defines approximately 650 semantic types and 30 semantic roles. See Dzikovska et al. (2004) for more discussion of the relationship between FrameNet and the LF ontology. We also expanded our verb coverage by integrating VerbNe"
W08-2227,W97-0213,0,0.0523892,"compute semantic precision and recall measures by comparing this to the G and T graphs aligned with themselves, which gives us the maximum possible gold and test scores. Precision(G,T) = Gscore(G,T)/Gscore(T,T) Recall(G,T) = Gscore(G,T)/Gscore(G,G) A more general function of node matching would be more informative. For instance, with words not in our core lexicon, we usually derive an abstract sense that is not the most specific sense in our ontology, however is an abstraction of the correct sense. A scoring function that would give such cases partial credit would have raised our scores (cf. Resnik and Yarowsky, 1997). Evaluation Procedure and Results To evaluate our system on the shared texts, we built gold representations for each. We did this by first generating an LF-graph by running the system, and then correcting this Allen, Swift, and de Beaumont 352 :OF (SPEECHACT SA_TELL) (F (:* CONJUNCT BUT)) :MOD :CONTENT (F (:* DISPERSE SPREAD)) :VAL W::PAST :TENSE W::PAST :OF :THEME :MOD :OF :TENSE :MOD (F (:* ADJUST PICK-UP)) (BARE (:* ACTION DISTRIBUTION)) :ASSOC-WITH (KIND (:* SUBSTANCE ELECTRICITY)) (F (:* EVENT-TIME-REL ONCE)) (F (:* TO-LOC TO)) :ASSOC-WITH :SIT-VAL W::PAST :MOD :THEME :OF (F (:* FREQUENC"
W08-2227,C04-1055,1,0.886347,"Missing"
W08-2227,C08-1116,0,0.0251718,"Missing"
W08-2227,W06-3504,1,\N,Missing
W11-0219,P05-1045,0,0.0029366,"rpose semantic lexicon and ontology which define a range of word senses and lexical semantic relations. The core semantic lexicon was constructed by hand and contains more than 7000 lemmas. It can be also dynamically augmented for unknown words by consulting WordNet (Miller, 1995). To support more robust processing as well as domain configurability, the core system is informed by a variety of statistical and symbolic preprocessors. These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al., 2005) and the Stanford Parser (Klein and Manning, 2003). The output of these and other specialized preprocessors (such as a street address recognizer) are sent to the parser as advice. The parser then can include or not include this advice (e.g., that a cer148 tain phrase is a named entity) as it searches for the optimal parse of the sentence. The result of parsing is a frame-like semantic representation that we call the Logical Form (LF). The LF representation includes semantic types, semantic roles for predicate arguments, and dependency relations. Figure 3 shows an LF example for the sentence “S"
W11-0219,W00-1308,0,0.118563,"Missing"
W11-0219,P05-3021,0,0.00519029,"number of serious efforts in medical NLP in general (Meystre et al., 2008) and in extracting temporal information from clinical text in particular. Some of this surge in interest has been spurred by dedicated competitions on extraction of concepts and events from clinical text (such as the i2b2 NLP challenges). At the same time, the evolution of temporal markup languages such as TimeML (Sauri et al., 2006), and temporal extraction/inference competitions (such as the two TempEval challenges, Verhagen et al., 2009) in the general area of NLP have led to the development of tools such as TARSQI (Verhagen et al., 2005) that could be adapted to the clinical domain. Although the prevailing paradigm in this area is to use superficial methods for extracting and classifying temporal expressions, it has long been recognized that higher level semantic processing, including discourse-level analysis, would have to be performed to get past the limits of the current approaches (cf. Zhou and Hripcsak, 2007). Recent attempts to use deeper linguistic features include the work of Bethard et al. (2007), who 153 used syntactic structure in addition to lexical and some minor semantic features to classify temporal relations o"
W11-1108,W07-1207,1,0.829846,"on There are at least two interpretations for the following sentence: (1) Every line ends with a digit. In one reading, there is a unique digit (say 2) at the end of all lines. This is the case where the quantifier A outscopes (aka having wide-scope over) the quantifier Every. The other case is the one in which &lt;Every x Line&gt; &lt;A y Digit&gt; Ends-with(x, y) in which, the relative scope of the quantifiers is underspecified. Since then scope underspecification has been the most popular way to deal with quantifier scope ambiguity in deep language understanding systems (e.g. Boxer (Bos 2004), TRAINS (Allen et al. 2007), BLUE (Clark and Harrison 2008), and DELPH-IN1). Scope underspecification works in practice, only because many NLP applications (e.g. machine translation) could be achieved without quantifier scope disambiguation. QSD on the other hand, is critical for many other NLP tasks such as question answering systems, dialogue systems and computing entailment. Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues (VanLehn 1 http://www.delph-in.net/ 51 Proceedings of t"
W11-1108,C04-1180,0,0.0295343,"Missing"
W11-1108,W08-2221,0,0.0151545,"interpretations for the following sentence: (1) Every line ends with a digit. In one reading, there is a unique digit (say 2) at the end of all lines. This is the case where the quantifier A outscopes (aka having wide-scope over) the quantifier Every. The other case is the one in which &lt;Every x Line&gt; &lt;A y Digit&gt; Ends-with(x, y) in which, the relative scope of the quantifiers is underspecified. Since then scope underspecification has been the most popular way to deal with quantifier scope ambiguity in deep language understanding systems (e.g. Boxer (Bos 2004), TRAINS (Allen et al. 2007), BLUE (Clark and Harrison 2008), and DELPH-IN1). Scope underspecification works in practice, only because many NLP applications (e.g. machine translation) could be achieved without quantifier scope disambiguation. QSD on the other hand, is critical for many other NLP tasks such as question answering systems, dialogue systems and computing entailment. Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues (VanLehn 1 http://www.delph-in.net/ 51 Proceedings of the TextGraphs-6 Workshop, pages"
W11-1108,P01-1019,0,0.211924,"Missing"
W11-1108,J03-1004,0,0.605268,"Missing"
W11-1108,A88-1008,0,0.869326,"of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues (VanLehn 1 http://www.delph-in.net/ 51 Proceedings of the TextGraphs-6 Workshop, pages 51–59, c Portland, Oregon, USA, 19-24 June 2011. 2011 Association for Computational Linguistics 1978, Moran 1988, Alshawi 1992). For example, it is widely known that in English, the quantifier each tends to have the widest scope. Also, the subject of a sentence often outscopes the direct object. 2 In cases where these heuristics conflict, (manually) weighted preference rules are adopted to resolve the conflict (Hurum 1988, , Pafel 1997). In the last decade there has been some effort to apply statistical and machine learning (ML) techniques to QSD. All the previous efforts, however, suffer from the following two limitations (see section 2 for details): • They only allow scoping two NPs per sentence. • The NPs must be explicitly quantified (e.g. they ignore definites or bare singulars/plurals), and the quantifiers are restricted to a predefined list. In this paper, we present the first work on applying statistical techniques to unrestricted QSD, where we put no restriction on the type or the number of NPs to be"
W11-1108,P08-1026,0,0.105988,"Missing"
W11-1108,P88-1005,0,0.823884,"machine translation) could be achieved without quantifier scope disambiguation. QSD on the other hand, is critical for many other NLP tasks such as question answering systems, dialogue systems and computing entailment. Almost all efforts in the 80s and 90s on QSD adopt heuristics based on the lexical properties of the quantifiers, syntactic/semantic properties of the sentences, and discourse/pragmatic cues (VanLehn 1 http://www.delph-in.net/ 51 Proceedings of the TextGraphs-6 Workshop, pages 51–59, c Portland, Oregon, USA, 19-24 June 2011. 2011 Association for Computational Linguistics 1978, Moran 1988, Alshawi 1992). For example, it is widely known that in English, the quantifier each tends to have the widest scope. Also, the subject of a sentence often outscopes the direct object. 2 In cases where these heuristics conflict, (manually) weighted preference rules are adopted to resolve the conflict (Hurum 1988, , Pafel 1997). In the last decade there has been some effort to apply statistical and machine learning (ML) techniques to QSD. All the previous efforts, however, suffer from the following two limitations (see section 2 for details): • They only allow scoping two NPs per sentence. • Th"
W11-1108,D09-1152,0,0.408532,"Missing"
W13-0103,W08-2227,1,0.893765,"Missing"
W13-0103,W08-2205,0,0.072958,"Missing"
W13-0103,C04-1185,0,0.0302426,"Missing"
W13-0103,P04-1036,0,0.081994,"Missing"
W13-0103,W99-0501,0,\N,Missing
W13-0905,W08-2227,1,0.746753,"Missing"
W13-0905,de-marneffe-etal-2006-generating,0,0.0037551,"Missing"
W13-0905,J83-3004,1,0.534234,"ct and so again trigger a metaphor. The preference notion was not initially intended to detect metaphor but to semantically disambiguate candidates at those sites by preferring those conceptual entities that did not violate such restrictions. In early work, preferences were largely derived by intuition and sometimes ordered by salience. Later (e.g. Resnik, 1997) there was a range of work on deriving such preferences from corpora; however, in VN the semantic preferences of verbs were again largely intuitive in origin. Early work linking preference violation to metaphor detection (summarised in Fass and Wilks, 1983, also Martin 1990) worked with handcrafted resources, but by 1995 Dolan had noted (Dolan, 1995) that large-scale lexical resources would have implications for metaphor detection, and WN was used in conjunction with corpora, by 37 (Peters and Wilks, 2003) using symbolic methods and by Mason (2004) and Krishnakumaran and Zhu (2007) using a combination of WN and statistical methods. Mason also acquires preferences automatically from corpora, and the latter two papers treat metaphor as a form of anomaly based on rare combinations of surface words and of WNderived hypernyms, a notion that appears"
W13-0905,P05-1045,0,0.00991608,". VN includes classes of verbs that map members to specific WN senses. VN also provides a hierarchy of verb object/subject inclusions, which we use for assessing whether one sentence object/subject type appears below another in this simple inclusion hierarchy, and so can be said to be semantically included in it. The selectional restrictions, however, are not linked to any lexicons so a mapping was constructed in order to allow for automated detection of preference violations. Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al., 2006) and Named Entity Recognizer (Finkel et al., 2005). The Stanford Parser identifies the verbs, as well as their corresponding subjects and direct objects. The Stanford Named Entity Recognizer was used to replace sequences of text representing names with WN senses whose hypernyms exist in the selectional restriction hierarchy. The first step in determining whether a sentence contains a metaphor is to extract all verbs along with the subject and direct object arguments for each verb. The Stanford Parser dependencies used to describe the relationships between verbs and 38 their arguments include agent, nsubj, and xsubj for subjects and dobj and n"
W13-0905,W07-0103,0,0.110031,"Missing"
W13-0905,C10-2078,0,0.0448363,"the original preference violation insight can be combined with large-scale investigations, using notions of machine learning and large-scale resources like WN. Our approach is smaller scale and does not involve machine learning: it simply seeks access to implicit metaphors built into the structure of WN by its creators, and which a preference-violation detection criterion cannot, by definition, access. Thus, we view our contribution as complementary to larger efforts on metaphor and interpretation detection, rather than a competing approach. We have not made comparisons here with the work of (Li and Sporleder, 2010), which is explicitly concerned with idioms, nor with (Markert and Nissim, 2009) which is focused on metonymy. 3 The Conventional Metaphor Detection Hypotheses Where WN codes conventionalized metaphors as senses, as in the initial cases described, then the senses expressing these will NOT violate preferences and so will not be detected by any metaphoras-violation hypothesis. For example, in “Jane married a brick” this will not be a preference violation against WN senses because WN explicitly codes brick as a reliable person, though we would almost certainly want to say this sentence contains a"
W13-0905,J04-1002,0,0.403161,"d sometimes ordered by salience. Later (e.g. Resnik, 1997) there was a range of work on deriving such preferences from corpora; however, in VN the semantic preferences of verbs were again largely intuitive in origin. Early work linking preference violation to metaphor detection (summarised in Fass and Wilks, 1983, also Martin 1990) worked with handcrafted resources, but by 1995 Dolan had noted (Dolan, 1995) that large-scale lexical resources would have implications for metaphor detection, and WN was used in conjunction with corpora, by 37 (Peters and Wilks, 2003) using symbolic methods and by Mason (2004) and Krishnakumaran and Zhu (2007) using a combination of WN and statistical methods. Mason also acquires preferences automatically from corpora, and the latter two papers treat metaphor as a form of anomaly based on rare combinations of surface words and of WNderived hypernyms, a notion that appears in (Guthrie et al., 2007) but based only on corpus sparsity and not WN codings. Other work on the automatic acquisition of preferences (McCarthy and Carrol, 2003) for WSD has also its considered extension to the detection of classes of metaphor. More recently, work by Shutova (Shutova et al., 2010"
W13-0905,J03-4004,0,0.0103056,"d have implications for metaphor detection, and WN was used in conjunction with corpora, by 37 (Peters and Wilks, 2003) using symbolic methods and by Mason (2004) and Krishnakumaran and Zhu (2007) using a combination of WN and statistical methods. Mason also acquires preferences automatically from corpora, and the latter two papers treat metaphor as a form of anomaly based on rare combinations of surface words and of WNderived hypernyms, a notion that appears in (Guthrie et al., 2007) but based only on corpus sparsity and not WN codings. Other work on the automatic acquisition of preferences (McCarthy and Carrol, 2003) for WSD has also its considered extension to the detection of classes of metaphor. More recently, work by Shutova (Shutova et al., 2010) has shown that the original preference violation insight can be combined with large-scale investigations, using notions of machine learning and large-scale resources like WN. Our approach is smaller scale and does not involve machine learning: it simply seeks access to implicit metaphors built into the structure of WN by its creators, and which a preference-violation detection criterion cannot, by definition, access. Thus, we view our contribution as complem"
W13-0905,P04-1036,0,0.0273827,"TRIPS lexicon and ontology to dynamically build lexical entries with approximate semantic and syntactic structures for words not in the core lexicon. This process may produce a range of different possibilities based on the different senses and possible subcategorization frames for the verbs that share the same TRIPS type. We feed all of these to the parser and let it determine the entries that best match the definition and examples. While WordNet may have multiple fine-grained senses for a given word, we set a parameter that has the system use only the most frequent sense(s) of the word (cf. McCarthy et al. 2004). We use TRIPS to parse the definitions and glosses into a logical form. Figure 3 shows the logical form produced for the definition cause to die. We then search the logical form for structures that signal a potential argument that would fill a role. Besides looking for gaps, we found some other devices that serve the same purpose and occur frequently in WordNet: • • • • elided arguments (an IMPRO in the logical form); indefinite pronouns (e.g., something, someone); prepositional/adverbial forms containing an IMPRO or an indefinite pronoun (e.g., give a benediction to); a noun phrase in parent"
W13-0905,J05-1004,0,0.00318773,"very broad WN sense sets those that are actually conventionalized metaphors: we determine that only the first sense, hopefully literal, should be able to satisfy any restriction. If a lower sense satisfies a verb, but the primary sense does not, we classify the satisfaction as being conventionalized, but a metaphor nonetheless. 6 Deriving Preferences and an Ontology from WordNet To date, VerbNet is the most extensive resource for verb roles and restrictions. It provides a rich semantic role taxonomy with some selectional restrictions. Still, VN has entries for less than 4000 verbs. PropBank (Palmer et al., 2005) has addi40 tional coverage, but uses a more surface oriented role set with no selectional restrictions. On the other hand, WordNet has many more verb entries but they lack semantic role information. However, we believe it is possible to extract automatically a comprehensive lexicon of verbs with semantic roles and selectional restrictions from WN by processing definitions in WN using deep understanding techniques. Specifically, each verb in WN comes with a gloss that defines the verb sense, and there we can find clues about the semantic roles and their selectional restrictions. Thus, we are t"
W13-0905,W97-0209,0,0.184674,"nomic value”, and muscle is not an abstract entity. There was discussion in those early days of syntactic-semantic interface cases like “John ran a mile” where a mile might be said to violate the preference of the (intransitive) verb for a zero object and so again trigger a metaphor. The preference notion was not initially intended to detect metaphor but to semantically disambiguate candidates at those sites by preferring those conceptual entities that did not violate such restrictions. In early work, preferences were largely derived by intuition and sometimes ordered by salience. Later (e.g. Resnik, 1997) there was a range of work on deriving such preferences from corpora; however, in VN the semantic preferences of verbs were again largely intuitive in origin. Early work linking preference violation to metaphor detection (summarised in Fass and Wilks, 1983, also Martin 1990) worked with handcrafted resources, but by 1995 Dolan had noted (Dolan, 1995) that large-scale lexical resources would have implications for metaphor detection, and WN was used in conjunction with corpora, by 37 (Peters and Wilks, 2003) using symbolic methods and by Mason (2004) and Krishnakumaran and Zhu (2007) using a com"
W13-0905,C10-1113,0,0.0788153,"and by Mason (2004) and Krishnakumaran and Zhu (2007) using a combination of WN and statistical methods. Mason also acquires preferences automatically from corpora, and the latter two papers treat metaphor as a form of anomaly based on rare combinations of surface words and of WNderived hypernyms, a notion that appears in (Guthrie et al., 2007) but based only on corpus sparsity and not WN codings. Other work on the automatic acquisition of preferences (McCarthy and Carrol, 2003) for WSD has also its considered extension to the detection of classes of metaphor. More recently, work by Shutova (Shutova et al., 2010) has shown that the original preference violation insight can be combined with large-scale investigations, using notions of machine learning and large-scale resources like WN. Our approach is smaller scale and does not involve machine learning: it simply seeks access to implicit metaphors built into the structure of WN by its creators, and which a preference-violation detection criterion cannot, by definition, access. Thus, we view our contribution as complementary to larger efforts on metaphor and interpretation detection, rather than a competing approach. We have not made comparisons here wi"
W14-2401,W08-2227,1,0.837592,"Missing"
W14-2401,W13-0103,1,0.891863,"Missing"
W14-2401,P10-1129,0,0.0319072,"in building a semantic lexicon on the scale of WordNet, with more coverage and detail that currently available in widely-used resources such as VerbNet. We view having such a lexicon as a necessary prerequisite for any attempt at attaining broad-coverage semantic parsing in any domain. The approach we described applies to all word classes, but in this paper we focus here on verbs, which are the most critical phenomena facing semantic parsing. 1. Introduction and Motivation Recently we have seen an explosion of work on learning semantic parsers (e.g., Matuszek, et al, 2012; Tellex et al, 2013; Branavan et al, 2010, Chen et al, 2011). While such work shows promise, the results are highly domain dependent and useful only for that domain. One cannot, for instance, reuse a lexical entry learned in one robotic domain in another robotic domain, let alone in a database query domain. Furthermore, the techniques being developed require domains that are simple enough so that the semantic models can be produced, either by hand or induced from the application. Language in general, however, involves much more complex concepts and connections, including discussion of involves abstract concepts, such as plans, theori"
W14-2401,W13-0905,1,\N,Missing
W15-0103,de-marneffe-etal-2006-generating,0,0.00939184,"Missing"
W15-0103,Q13-1023,0,0.0381,"Missing"
W15-0103,hartung-frank-2010-semi,0,0.0637396,"Missing"
W15-0103,C10-1049,0,0.257454,"Missing"
W15-0103,P93-1023,0,0.620473,"Missing"
W15-0103,P97-1023,0,0.721769,"Missing"
W15-0103,H05-1043,0,0.0526065,"ation of adjective scales. They presented an approach for clustering adjectives in the same scale based on their positive or negative orientation. Another work (Hatzivassiloglou and McKeown, 1997) proposes to classify the semantic polarity of adjectives based on their behavior in conjunction with other adjectives in a news corpus. They employ the existing clustering algorithms for this task.Turney and Littman (2003) decide on semantic orientation of a word (positive, negative combined with mild or strong) using statistical association with a set of positive and negative paradigm words. OPINE (Popescu and Etzioni, 2005), a system for product review mining, ranks opinion words by their strength. Our work differs fundamentally from these works in that it does not attempt to assign positive or negative polarities to adjectives. All such works focus on detection of semantic orientation of adjectives, and do not report on extracting attribute concepts or scales for adjectives. The information on orientation of adjectives is very helpful for understanding their semantics, but it is not sufficient for deep understanding which can enable further inference. Another ongoing research project is on adjective intensity o"
W15-0103,P95-1026,0,0.585392,"Missing"
W15-0103,H05-2017,0,\N,Missing
W15-3801,W08-2227,1,0.86986,"Missing"
W15-3801,W14-2401,1,0.836019,"information in WordNet (for all the WordNet senses) and the TRIPS ontology. For each identified TRIPS class it gathers all the possible constructions that words of this class in the TRIPS lexicon participate in. It then generates a set of lexical entries for the unknown word by combining each possible ontological class with each possible construction for that class. While this procedure may over-generate, the key is to include the correct constructions among the generated possibilities, since these correct constructions will be the ones realized in parsing sentences (for more information see Allen, 2014). The parser draws on a general purpose semantic lexicon and ontology which define a range of word senses and lexical semantic relations. The core semantic lexicon was constructed by hand and contains approximately 7,500 lemmas (generating approximately three times that many words) and 2,000 concepts in the ontology. The ontology is organized hierarchically and each ontology concept has associated with it possible semantic roles and selectional preferences that further refine the concept. For instance, it can be specified that the AFFECTED role of phosphorylate may only take a physical object"
W15-3801,W09-1401,0,0.0294811,"the capabilities to truly understand big and complex mechanisms. Related Work and Discussion With the advent of relatively successful text mining strategies (named entity recognition, information extraction and retrieval) for the recognition and normalization of biologically relevant entities, automatic extraction of more complex, relational information from the biomedical literature has become a very active area of research. Shared Tasks (STs) such as the Protein-Protein Interaction (PPI) Task introduced at BioCreative II (Krallinger et al., 2008) and the BioNLP GENIA Event Extraction Task (Kim et al., 2009; Kim et al., 2011; Kim et al., 2013) have spurred a lot of activity in this area, although examples of earlier work certainly exist. The goal in the PPI task is to extract binary protein-protein interaction pairs from full-text articles. More general biological events (e.g., regulatory events) beyond PPI involve much more varied relationships between entities and, indeed, between events themselves, leading to complex nested structures. The BioNLP STs have evolved to include more complex types of events and arguments. The GENIA ST (in particular 2013 which included coreference) and the Epigene"
W15-3801,W11-1802,0,0.0209702,"to truly understand big and complex mechanisms. Related Work and Discussion With the advent of relatively successful text mining strategies (named entity recognition, information extraction and retrieval) for the recognition and normalization of biologically relevant entities, automatic extraction of more complex, relational information from the biomedical literature has become a very active area of research. Shared Tasks (STs) such as the Protein-Protein Interaction (PPI) Task introduced at BioCreative II (Krallinger et al., 2008) and the BioNLP GENIA Event Extraction Task (Kim et al., 2009; Kim et al., 2011; Kim et al., 2013) have spurred a lot of activity in this area, although examples of earlier work certainly exist. The goal in the PPI task is to extract binary protein-protein interaction pairs from full-text articles. More general biological events (e.g., regulatory events) beyond PPI involve much more varied relationships between entities and, indeed, between events themselves, leading to complex nested structures. The BioNLP STs have evolved to include more complex types of events and arguments. The GENIA ST (in particular 2013 which included coreference) and the Epigenetics and Post-tran"
W15-3801,W11-1803,0,0.0167164,"his area, although examples of earlier work certainly exist. The goal in the PPI task is to extract binary protein-protein interaction pairs from full-text articles. More general biological events (e.g., regulatory events) beyond PPI involve much more varied relationships between entities and, indeed, between events themselves, leading to complex nested structures. The BioNLP STs have evolved to include more complex types of events and arguments. The GENIA ST (in particular 2013 which included coreference) and the Epigenetics and Post-translational Modifications task (EPI) introduced in 2011 (Ohta et al., 2011) are similar to our task. However, there are significant differences, too. We were not provided with gold annotations for entities; all relevant entities (including drugs, cell lines, cell components, sites) had to be extracted, and most of them had to be grounded in a reference database. Protein families were also important, as was the relation between families and the member proteins. Not only were coreferences supposed to be resolved, but, as indicated in Section 5, sometimes complex inferences were required to obtain a target event. In summary, our task was not designed to accommodate spec"
W15-3801,W00-1308,0,0.252629,"is the grammatical constructions that the word can participate in, in the form of rules that map syntactic patterns to instantiations of objects from the ontology. 2.3. 2.4. To support more robust processing and domain configurability, the core system has the capability to incorporate a variety of statistical and symbolic natural language processing components in the front end, as well as domain-specific components such as specialized named entity recognizers. These include several off-the-shelf natural language tools such as the Shlomo Yona sentencizer1 , the Stanford part-of-speech tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al., 2005) and the Stanford Parser (Klein and Manning, 2003). The output of these and other speExtending the Lexicon To attain broad lexical coverage beyond its handdefined core lexicon, TRIPS uses input from a variety of external resources, some of which will be described in the next sections. Using the builtin subsystem WordFinder, TRIPS can augment 1 Front End Components http://search.cpan.org/~kimryan/Lingua-EN-Sentence-0.29/ 3 Resource Entities References BRENDA Tissue Ontology tissues, cell types, cell lines Gremse et al., 2011 Cell"
W16-1007,W08-2227,1,0.373098,"Missing"
W16-1007,J08-4004,0,0.0549757,"ory for coders who have not captured this relation. The agreement according to Fleiss’s Kappa κ = 0.49 without applying basic closure and κ = 0.51 with closure10 , which shows moderate agreement. For reference, the agreement on semantic link annotation in the most recent clinical TempEval was 0.44 without closure and 0.47 with closure. 6 Related Work Given that there are no prefixed set of event entity spans for straight-forward computation of interannotator agreement, we do the following: among all the annotators, we aggregate the spans of the annotated event entity as the annotation object (Artstein and Poesio, 2008). Then, if there exists a span which is not annotated by one of the coders (annotators) it will be labeled as ‘NONE’ for its category. The agreement according to Fleiss’s Kappa κ = 0.91, which shows substantial agreement on event entity annotation. Although direct comparison of κ values is not possible, as a point of reference, the event One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) (Styler et al., 2014). This annotation guideline was devised for the purpose of establishing timelines in clinical narratives, i.e. the free text portions co"
W16-1007,D13-1178,0,0.0209354,"s. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality"
W16-1007,W13-2322,0,0.0370696,"and auxiliary verbs. Whenever the semantic contribution of the verb is minimal and the nonverb element of the construction is an event in the 4 These verbs are the same as aspectual events characterized by TimeML, which include ‘INITIATES’, ‘CULMINATES’, ‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’. TRIPS ontology, we annotate the non-verb element as the event. Thus, we annotate the noun predicate ’offer’ in the sentence ‘Yesterday, John made an offer to buy the house for 350,000’, similarly to the way Abstract Meaning Representation (AMR) drops the light verb and promotes the noun predicate (Banarescu et al., 2013). This annotation is also close to the PropBank annotation of copulas and light verbs (Bonial et al., 2014), where they annotate the noun predicate and predicate adjective as the event; however, PropBank includes an explicit marking of the verb as either a light verb or a copula verb. 3 The Semantic Relations Between Event Entities A more challenging problem than event entity detection is the identification of the semantic relation that holds between events. Events take place in time, hence temporal relations between events are crucial to study. Furthermore, causality plays a crucial role in e"
W16-1007,S15-2136,0,0.059425,"Missing"
W16-1007,S13-2002,0,0.0561543,"medical disorders and conditions, e.g., cancer, heart attack, stroke, etc. – Occurring: e.g., happen, occur. Our semantic framework captures the set of event entities and their pairwise semantic relations, which together form an inter-connected network of events. In this Section we define event entities and discuss their annotation process. 2.1 of interest in various NLP applications. However, there is still no consensus regarding the span of events and how they should be annotated. There has been some good progress in domain-specific annotation of events, e.g., recent Clinical TempEval task (Bethard, 2013) and THYME annotation scheme (Styler et al., 2014), however, the detection of events in broad-coverage natural language has been an ongoing endeavor in the field. One of the existing definitions for event is provided in the TimeML annotation schema (Pustejovsky et al., 2003): Definition Event is mainly used as a term referring to any situation that can happen, occur, or hold. The definition and detection of events has been a topic to the core story. 52 – Natural-phenomenon: e.g., earthquake, tsunami. This ontology has one of the richest event hierarchies, which perfectly serves our purpose of"
W16-1007,bittar-etal-2012-temporal,0,0.0283798,"t pair. Figure 2: Frequency of semantic links in our dataset. Figure 1: Semantic annotation of a sample story. – 9 causal relations: Including ‘cause (before/overlaps)’, ‘enable (before/overlaps)’, ‘prevent (before/overlaps)’, ‘cause-to-end (before/overlaps/during)’ – 4 temporal relations: Including ‘Before’, ‘Overlaps’, ‘Contains’, ‘Identity’. The semantic relation annotation between two events should start with deciding about any causal relations and then, if there was not any causal relation, proceed to choosing any existing temporal relation. 4 Annotating at Story level It has been shown (Bittar et al., 2012) that temporal annotation can be most properly carried out by taking into account the full context for sentences, as opposed to TimeML, which is a surface-based annotation. The scope and goal of this paper very well aligns with this observation. We carry out the annotation at the story level, meaning that we annotate inter-event relations across the five sentences of a story. It suffices to do the event-event relation specification minimally given the transitivity of temporal relations. For example for three consecutive events e1 e2 e3 one should only annotate the ‘before’ relation between e1"
W16-1007,bonial-etal-2014-propbank,0,0.0623713,"construction is an event in the 4 These verbs are the same as aspectual events characterized by TimeML, which include ‘INITIATES’, ‘CULMINATES’, ‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’. TRIPS ontology, we annotate the non-verb element as the event. Thus, we annotate the noun predicate ’offer’ in the sentence ‘Yesterday, John made an offer to buy the house for 350,000’, similarly to the way Abstract Meaning Representation (AMR) drops the light verb and promotes the noun predicate (Banarescu et al., 2013). This annotation is also close to the PropBank annotation of copulas and light verbs (Bonial et al., 2014), where they annotate the noun predicate and predicate adjective as the event; however, PropBank includes an explicit marking of the verb as either a light verb or a copula verb. 3 The Semantic Relations Between Event Entities A more challenging problem than event entity detection is the identification of the semantic relation that holds between events. Events take place in time, hence temporal relations between events are crucial to study. Furthermore, causality plays a crucial role in establishing semantic relation between events, specifically in stories. In this Section, we provide details"
W16-1007,P08-1090,1,0.737058,"is commonsense knowledge can be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commons"
W16-1007,P09-1068,1,0.706816,"be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . Thi"
W16-1007,N13-1104,1,0.852947,"knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality2 five-sentence stori"
W16-1007,W14-2903,0,0.0947084,"Missing"
W16-1007,C14-1198,0,0.162142,"annotates temporal and causal relations in parallel (Steven Bethard and Martin, 2008). Bethard et al. annotated a dataset of 1,000 conjoined-event temporal-causal relations, collected from Wall Street Journal corpus. Each event pair was annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE, NO-REL). For example, sentence 12 is an entry in their dataset. This dataset makes no distinction between various types of causal relation. (12) Fuel tanks had leaked and contaminated the soil. - (leaked BEFORE contaminated) - (leaked CAUSED contaminated). A recent work (Mirza and Tonelli, 2014) has proposed a TimeML-style annotation standard for capturing causal relations between events. They mainly introduce ‘CLINK’, analogous to ‘TLINK’ in TimeML, to be added to the existing TimeML link tags. Under this framework, Mirza et al (Mirza and Tonelli, 2014) annotates 318 CLINKs in TempEval3 TimeBank. They only annotate explicit causal relations signaled by linguistic markers, such as {because of, as a result of, due to, so, therefore, 59 thus}. Another relevant work is Richer Event Descriptions (RED) (Ikuta et al., 2014), which combines event coreference and THYME annotations, and also"
W16-1007,N16-1098,1,0.0733092,"s need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality2 five-sentence stories 1 These stories can be found here: http://cs. rochester.edu/nlp/rocstories 2 Each of these stories have the following major characteristics: is realistic, has a clear beginning and ending where something happens in between, does not include anything irrelevant 51 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics that are fu"
W16-1007,P15-1019,0,0.0541783,"Missing"
W16-1007,E14-1024,0,0.0242643,"g happens in between, does not include anything irrelevant 51 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics that are full of stereotypical causal and temporal relations between events, making them a perfect resource for learning narrative schemas. One of the prerequisites for learning scripts from these stories is to extract events and find inter-event semantic relations. Earlier work (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015) defines verbs as events and uses TimeML-based (Pustejovsky et al., 2003) learning for temporal ordering of events. This clearly has many shortcomings, including, but not limited to (1) not capturing a wide range of non-verbal events such as ‘earthquake’, (2) not capturing a more comprehensive set of semantic relations between events such as causality, which is a core relation in stories. In this paper we formally define a new comprehensive semantic framework for capturing stereotypical event-event temporal and causal relations in commonsense stories, the details of whi"
W16-1007,W11-0419,0,0.0270517,"κ values is not possible, as a point of reference, the event One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) (Styler et al., 2014). This annotation guideline was devised for the purpose of establishing timelines in clinical narratives, i.e. the free text portions contained in electronic health records. In their work, they combine the TimeML annotation schema with Allen Interval Algebra, identifying the five temporal relations BEFORE, OVERLAP, BEGINS-ON, ENDS-ON, and CONTAINS. Of note is that they adopt the notion of narrative containers (Pustejovsky and Stubbs, 2011), which are time slices in which events can take place, such as DOCTIME (time of the report) and before DOCTIME. As such, the THYME guideline focuses on ordering events with respect to specific time intervals, while in our work, we are only focused on the relation between two events, without concern for ordering. Their simplification of temporal links is similar to ours, however, our reasoning for simplification takes 9 This is based on the fact that any relation such as ‘A enablebefore B’ or ‘A overlaps B’ can be naively approximated to ‘A before B’. 10 Temporal closure (Gerevini et al., 1995"
W16-1007,D15-1195,0,0.0555407,"Missing"
W16-1007,bethard-etal-2008-building,0,0.134989,"Missing"
W16-1007,Q14-1012,0,0.0456985,"Missing"
W16-1007,H89-1033,0,0.739052,"ons between events. By annotating a total of 1,600 sentences in the context of 320 five-sentence short stories sampled from ROCStories corpus, we demonstrate that these stories are indeed full of causal and temporal relations. Furthermore, we show that the CaTeRS annotation scheme enables high inter-annotator agreement for broad-coverage event entity annotation and moderate agreement on semantic link annotation. 1 Introduction Understanding events and their relations in natural language has become increasingly important for various NLP tasks. Most notably, story understanding (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000) which is an extremely challenging task in natural language understanding, is highly dependent on understanding events and their relations. Recently, we have witnessed a renewed interest in story and narrative understanding based on the progress made in core NLP tasks. Perhaps the biggest challenge of story understanding (and story generation) is having commonsense knowledge for the interpretation of narrative events. This commonsense knowledge can be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences t"
W16-1007,miltsakaki-etal-2004-penn,0,\N,Missing
W16-1007,prasad-etal-2008-penn,0,\N,Missing
W16-2505,marelli-etal-2014-sick,0,0.0191622,"Missing"
W16-2505,N16-1098,1,0.795855,"Missing"
W16-2505,D15-1036,0,0.0222761,"search in the past few years. While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation. The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores. It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks (Schnabel et al., 2015; Tsvetkov et al., 2015). Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method. 1 Examples of this include the semantic relatedness (SICK) dataset (Marelli et al., 2014), where given two sentences, the task is to produce a score of how semantically related these sentences are 24 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 24–29, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Lingu"
W16-2505,D15-1243,0,0.0454412,"years. While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation. The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores. It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks (Schnabel et al., 2015; Tsvetkov et al., 2015). Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method. 1 Examples of this include the semantic relatedness (SICK) dataset (Marelli et al., 2014), where given two sentences, the task is to produce a score of how semantically related these sentences are 24 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 24–29, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics cs.rochester.edu/"
W16-2505,D13-1020,0,\N,Missing
W16-6006,W08-2227,1,0.860443,"Missing"
W16-6006,D15-1115,1,0.885117,"sis We introduce a novel framework for modeling the semantics of comparison and ellipsis as interconnected predicate-argument structures. According to this framework, comparison and ellipsis operators are the predicates, where each predicate has a set of arguments called its semantic frame. For example, in the sentence ‘[Sam] is the tallest [student] [in the gym]’, the morpheme -est is the comparison operator (hence, the comparison predicate) and the entities in the brackets are the arguments. 2.1 Comparison Structures 2.1.1 Predicates We consider two main categories of comparison predicates (Bakhshandeh and Allen, 2015; Bakhshandeh et al., 2016), Ordering and Extreme, each of which can grade any of the four parts of speech including adjectives, adverbs, nouns, and verbs. • Ordering: Shows the ordering of two or more entities on a scale, with the following subtypes: – Comparatives expressed by the morphemes more/-er and less, with ‘&gt;’, ‘&lt;’ indicating that one degree is greater or lesser than the other. (1) The steak is tastier than the potatoes. Grammar (ERG) (Flickinger, 2011), Boxer (Bos, 2008), or AMR (Banarescu et al., 2013), among others. 28 Superlative+ Joe is the most Figure eager boy ever. Scale/+ Do"
W16-6006,K16-1007,1,0.325661,"ework for modeling the semantics of comparison and ellipsis as interconnected predicate-argument structures. According to this framework, comparison and ellipsis operators are the predicates, where each predicate has a set of arguments called its semantic frame. For example, in the sentence ‘[Sam] is the tallest [student] [in the gym]’, the morpheme -est is the comparison operator (hence, the comparison predicate) and the entities in the brackets are the arguments. 2.1 Comparison Structures 2.1.1 Predicates We consider two main categories of comparison predicates (Bakhshandeh and Allen, 2015; Bakhshandeh et al., 2016), Ordering and Extreme, each of which can grade any of the four parts of speech including adjectives, adverbs, nouns, and verbs. • Ordering: Shows the ordering of two or more entities on a scale, with the following subtypes: – Comparatives expressed by the morphemes more/-er and less, with ‘&gt;’, ‘&lt;’ indicating that one degree is greater or lesser than the other. (1) The steak is tastier than the potatoes. Grammar (ERG) (Flickinger, 2011), Boxer (Bos, 2008), or AMR (Banarescu et al., 2013), among others. 28 Superlative+ Joe is the most Figure eager boy ever. Scale/+ Domain Domain-specifier Figur"
W16-6006,W13-2322,0,0.0324405,"s 2.1.1 Predicates We consider two main categories of comparison predicates (Bakhshandeh and Allen, 2015; Bakhshandeh et al., 2016), Ordering and Extreme, each of which can grade any of the four parts of speech including adjectives, adverbs, nouns, and verbs. • Ordering: Shows the ordering of two or more entities on a scale, with the following subtypes: – Comparatives expressed by the morphemes more/-er and less, with ‘&gt;’, ‘&lt;’ indicating that one degree is greater or lesser than the other. (1) The steak is tastier than the potatoes. Grammar (ERG) (Flickinger, 2011), Boxer (Bos, 2008), or AMR (Banarescu et al., 2013), among others. 28 Superlative+ Joe is the most Figure eager boy ever. Scale/+ Domain Domain-specifier Figure 2: An example predicate-argument structure consisting of superlative predicate type and its corresponding semantic frame of arguments. – Equatives expressed by as in constructions such as as tall or as much, with ‘≥’ indicating that one degree equals or is greater than another. (2) The Mazda drives as fast as the Nissan. – Superlatives expressed by most/-est and least, indicates that an entity or event has the ‘highest’ or ‘lowest’ degree on a scale. (3) That chef made the best soup. T"
W16-6006,P14-1133,0,0.0754885,"Missing"
W16-6006,W08-2222,0,0.280168,"rage Meaning Representation: The Case of Comparison Structures Omid Bakhshandeh University of Rochester omidb@cs.rochester.edu 1 James F. Allen University of Rochester / IHMC james@cs.rochester.edu My Mazda Self_mover Introduction Representing the underlying meaning of text has been a long-standing topic of interest in computational linguistics. Recently there has been a renewed interest in computational modeling of meaning for various tasks such as semantic parsing (Zelle and Mooney, 1996; Berant and Liang, 2014). Opendomain and broad-coverage semantic representation (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) is essential for many language understanding tasks such as reading comprehension tests and question answering. One of the most common way for expressing evaluative sentiment towards different entities is to use comparison. Comparison can happen in very simple structures such as ‘John is taller than Susan’, or more complicated constructions such as ‘The table is longer than the sofa is wide’. So far the computational semantics of comparatives and how they affect the meaning of the surrounding text has not been studied effectively. That is, the difference between the existi"
W16-6006,P14-5010,0,0.0030453,"ure 1: The frame-semantic parsing of the sentence My Mazda drove faster than his Hyundai. not fully capture the underlying semantics of the adjective ‘tall’ and what it means to be ‘taller’. A human reader can easily infer that the ‘height’ attribute of John is greater than Susan’s. Capturing the underlying meaning of comparison structures, as opposed to their surface wording, is crucial for accurate evaluation of qualities and quantities. Consider a more complex comparison example, ‘The pizza was great, but it was still worse than the sandwich’. The stateof-the-art sentiment analysis system (Manning et al., 2014) assigns an overall ‘negative’ sentiment value to this sentence, which clearly lacks the understanding of the comparison happening in the sentence. As another example, consider the generic meaning representation of the sentence ‘My Mazda drove faster than his Hyundai’, according to frame semantic parsing using Semafor1 tool (Das et al., 2014) as depicted in Figure 1. It is evident that this meaning representation does not fully capture how the semantics of the adjective fast relates to the driving event, and what it actually means for a car to drive faster than another car. More importantly, t"
W16-6006,P05-1015,0,0.120334,"nd testing models. The diversity and comprehensiveness of the comparison structures represented in our dataset is dependent on the genre of sentences comprising it. Earlier, we had experimented with annotating semantic structures on OntoNotes dataset (Bakhshandeh and Allen, 2015). Recently (Bakhshandeh et al., 2016), We have shifted our focus to actual product and restaurant reviews, which include many natural comparison instances. For this purpose we mainly use Google English Web Treebank4 which comes with gold constituency parse trees. We augment this dataset with the Movie Reviews dataset (Pang and Lee, 2005), where we use Berkeley parser (Petrov et al., 2006) to obtain parse trees. We trained linguists by asking them to read the semantic framework annotation manual as summa3 VP-deletion and stripping are the more frequent types. https://catalog.ldc.upenn.edu/ LDC2012T13 4 29 rized in Section 2. The annotations were done via our interactive two-stage tree-based annotation tool. For this task, the annotations were done on top of constituency parse trees. This process yielded a total of 2,800 annotated sentences. Figure 3 visualizes the distribution of predicate types from the various resources. As"
W16-6006,P06-1055,0,0.0616914,"ess of the comparison structures represented in our dataset is dependent on the genre of sentences comprising it. Earlier, we had experimented with annotating semantic structures on OntoNotes dataset (Bakhshandeh and Allen, 2015). Recently (Bakhshandeh et al., 2016), We have shifted our focus to actual product and restaurant reviews, which include many natural comparison instances. For this purpose we mainly use Google English Web Treebank4 which comes with gold constituency parse trees. We augment this dataset with the Movie Reviews dataset (Pang and Lee, 2005), where we use Berkeley parser (Petrov et al., 2006) to obtain parse trees. We trained linguists by asking them to read the semantic framework annotation manual as summa3 VP-deletion and stripping are the more frequent types. https://catalog.ldc.upenn.edu/ LDC2012T13 4 29 rized in Section 2. The annotations were done via our interactive two-stage tree-based annotation tool. For this task, the annotations were done on top of constituency parse trees. This process yielded a total of 2,800 annotated sentences. Figure 3 visualizes the distribution of predicate types from the various resources. As this Figure shows, reviews are indeed a very rich re"
W17-0906,W17-0908,0,0.124042,"Missing"
W17-0906,W17-0909,0,0.387095,"d. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as wo"
W17-0906,W17-0912,0,0.164607,"ls can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the"
W17-0906,H05-1044,0,0.0509919,"Missing"
W17-0906,H89-1033,0,0.679051,"Missing"
W17-0906,N16-1098,1,0.310191,"esident Marco was excited to be a registered voter. He thought long and hard about who to vote for. Finally he had decided on his favorite candidate. He placed his vote for that candidate. Marco was proud that he had finally voted. Spaghetti Sauce Tina made spaghetti for her boyfriend. It took a lot of work, but she was very proud. Her boyfriend ate the whole plate and said it was good. Tina tried it herself, and realized it was disgusting. She was touched that he pretended it was good to spare her feelings. Table 2: Example ROCStories instances from the Winter 2017 release. 3 As described in Mostafazadeh et al. (2016), the SCT cases are collected through Amazon Mechanical Turk (Mturk) on the basis of the ROCStories corpus, a collection of five-sentence everyday life stories which are full of stereotypical sequence of events. To construct SCT cases, they randomly sampled complete five-sentence stories from the ROCStories corpus and presented only the first four sentences of each story to the Mturk workers. Then, for each story, a worker was asked to write a ‘right ending’ and a ‘wrong ending’. This resulting set was further filtered by human verification: they compile each SCT case into two independent five"
W17-0906,W17-0911,0,0.0613484,"2017). Linguistic features include aspects of sentiment, negation, pronominalization and n-gram overlap between the story and possible endings. 98,159 1,871 1,871 Table 3: The size of the provided shared task datasets. uate the systems in terms of accuracy, which we #correct measure as #test Any other details recases . garding our shared task can be accessed via our shared task page http://cs.rochester. edu/nlp/rocstories/LSDSem17/. 4 roemmele (University of Southern California). Binary classifier based on a recurrent neural network that operates over (sentence-level) Skipthought embeddings (Roemmele et al., 2017). For training, different data augmentation methods are explored. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they l"
W17-0906,W17-0910,0,0.073945,"Missing"
W17-0906,W17-0907,0,0.273047,"exically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the same vector space. This model is called Deep Structured Semantic Model (DSSM) (Huang et al., 2013) and had outperformed all the other baselines reported in Mostafazadeh et al. (2016). cogcomp"
W17-1719,W07-1207,1,0.875655,"to Section 3 for our take on such cases. 139 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 139–148, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics 2 in specific verb classes. For example, particle up has a Direction sense when it appears in resultative VPCs with verbs of motion, such as wander/stroll/go/run up (Villavicencio, 2006). Hence, in this paper, we provide a set of senses that particles in VPCs display across many verbs in a verb class. To make use of these senses, we encode semantics of particles in an ontology, namely TRIPS (Allen et al., 2007) LF ontology which is designed to be linguistically informed.3 The ontology encodes semantic types, the set of word senses and semantic relations that can be used in logical form (LF) graphs. Word senses are defined based on subcategorization patterns and selectional restrictions driven by linguistic considerations. The semantic types in the ontology are, to a large extent, compatible with FrameNet (Johnson and Fillmore, 2000). The ontology uses a rich semantic feature set, the features used are an extended version of EuroWordNet (Vossen, 1997). Unlike WordNet, the TRIPS ontology does not atte"
W17-1719,W08-2227,1,0.84555,"Missing"
W17-1719,W03-1812,0,0.0342379,"ocus 3 The TRIPS ontology can be accessed at:http://www. cs.rochester.edu/research/cisd/projects/ trips/lexicon/browse-ont-lex-ajax.html 4 The TRIPS parser can be accessed at: http:// trips.ihmc.us/parser/cgi/parse 140 is on obtaining productive patterns in VPCs rather than on their interpretation. Our work also differs from the previous works mentioned above in the following respect: we emphasize on building complete semantic representations of the sentences, not just on particles’ semantics or just classification of VPCs. Similar to our criteria for compositionality, McCarthy et al. (2003), Baldwin et al. (2003), Bannard et al. (2003) have looked at distributional similarity as a measure of compositionality of VPCs. In contrast to the approaches focusing on statistical classification based on word/syntax features, we present our heuristics for classification of VPCs based on WordNet and discuss how we compute the semantics of the compositional classes. 3 provide the meaning of the VPC fly up. We distinguish the other two compositional VPC types from the symmetrically compositional VPCs only in the aspect that in the other two types, the particle or the verb have a relatively lighter contribution6 tha"
W17-1719,W03-1809,0,0.428326,"Y 14627 {abhatia,cmteng,jallen}@ihmc.us Abstract (VPCs). These consist of a verb and an adverbial or prepositional particle, e.g., eat up, fade out, go on, show off and walk down.1 Adding every single occurrence of such verb particle combinations in a lexicon is possible but not ideal as, for example, some VPCs may be interpretable compositionally, i.e., the verb and the particle contribute their simplex meanings, e.g. fly up. Other compositional VPCs include cases such as finish up and made away for which either the verb or the particle, respectively, seems to contribute its simplex meaning (Bannard et al., 2003).2 However, other VPCs indeed are noncompositional and require special interpretation, and hence need to be added into the lexicon, e.g., bake off ’contest’ and egg on ’urge someone for an action that might not be a good idea’. For an interpretation of the compositional types above, we need to determine the best senses for the verb and the particle in the VPCs. There are many lexical resources for an inventory of senses for verbs, such as WordNet (Miller, 1995), (Fellbaum, 1998) and VerbNet (Kipper-Schuler, 2005). But there is not much for the particles except for a few attempts at the semanti"
W17-1719,W13-1001,0,0.0348795,"Missing"
W17-1719,W06-1207,0,0.152562,"ncompositional and require special interpretation, and hence need to be added into the lexicon, e.g., bake off ’contest’ and egg on ’urge someone for an action that might not be a good idea’. For an interpretation of the compositional types above, we need to determine the best senses for the verb and the particle in the VPCs. There are many lexical resources for an inventory of senses for verbs, such as WordNet (Miller, 1995), (Fellbaum, 1998) and VerbNet (Kipper-Schuler, 2005). But there is not much for the particles except for a few attempts at the semantics for a few particles, such as up (Cook and Stevenson, 2006) and out (Tyler and Evans, 2003). Our investigation of hundreds of VPCs has shown that the semantics of particles is also important, as can also be gathered from others’ proposals for similar classifications of VPCs as mentioned above involving VPC types where particles contribute to the meaning, see Section 3 for details. Particles are not just the vacuous entities structurally required by the verbs in VPCs, they also have their own semantics which is found to be general across verbs We are developing a broad-coverage deep semantic lexicon for a system that parses sentences into a logical for"
W17-1719,U04-1022,0,0.0348904,"two specific particles rather than on a broader coverage of particles. Vincze (2011) presents the Wiki50 corpus that has 446 VPCs (342 unique types) annotated. Bannard (2002) makes an attempt to identify different types of VPCs in terms of compositionality and builds a (decision tree) classifier to identify the four types. Bannard et al. (2003) also adopt a similar approach for compositionality. As an annotation experiment, they investigate various VPCs to see whether the sense is contributed by the verb and/or the particle. They build four classifiers for automatic semantic analysis of VPCs. Patrick and Fletcher (2004) also have a similar approach but focus on automatic classification of different types of compositionality. Unlike our work, in all these works, the focus is on compositionality only, not on actual senses of the particles. Cook and Stevenson (2006) discuss various senses for the particle up in cognitive grammar framework and annotate a dataset and perform some classification experiments to identify the senses of up in unseen data. As a linguistic study, Jackendoff (2002) provides a very nice discussion of various types of VPCs involving particles such as directional particles, aspectual partic"
W17-1719,W03-1808,0,0.0582902,"the sense classes of the particles and how different particle sense classes relate to verbal ontological classes. Fraser (1976) mentions semantic properties of verbs affecting patterns of verb particle combinations, e.g. semantically similar verbs bolt/cement/clam/glue/paste/nail all can combine with the particle down and specify the objects that can be used to join material. Our approach is also based on the similar assumption that there are generalizations, such as combinations of particles with specific verb classes or ontological classes result in specific sense classes for the particles. Villavicencio (2003) also adopts the same approach where she tries to encode the information in terms of lexical rules and restrictions etc, however her focus 3 The TRIPS ontology can be accessed at:http://www. cs.rochester.edu/research/cisd/projects/ trips/lexicon/browse-ont-lex-ajax.html 4 The TRIPS parser can be accessed at: http:// trips.ihmc.us/parser/cgi/parse 140 is on obtaining productive patterns in VPCs rather than on their interpretation. Our work also differs from the previous works mentioned above in the following respect: we emphasize on building complete semantic representations of the sentences, n"
W17-1719,A00-2008,0,0.0660968,"ide a set of senses that particles in VPCs display across many verbs in a verb class. To make use of these senses, we encode semantics of particles in an ontology, namely TRIPS (Allen et al., 2007) LF ontology which is designed to be linguistically informed.3 The ontology encodes semantic types, the set of word senses and semantic relations that can be used in logical form (LF) graphs. Word senses are defined based on subcategorization patterns and selectional restrictions driven by linguistic considerations. The semantic types in the ontology are, to a large extent, compatible with FrameNet (Johnson and Fillmore, 2000). The ontology uses a rich semantic feature set, the features used are an extended version of EuroWordNet (Vossen, 1997). Unlike WordNet, the TRIPS ontology does not attempt to capture all possible word senses but rather focuses on the level of abstraction that affects linguistic processing. We use TRIPS, a broad coverage deep semantic parser (driven by the ontology) to combine semantic, ontological and grammatical information to produce semantic representation. For a more detailed overview of the TRIPS system, refer Allen & Teng (2017) and Allen et al. (2008).4 The paper is organized as follo"
W17-1719,R11-1040,0,0.0617028,"Missing"
W17-1719,W03-1810,0,0.0665547,"tions etc, however her focus 3 The TRIPS ontology can be accessed at:http://www. cs.rochester.edu/research/cisd/projects/ trips/lexicon/browse-ont-lex-ajax.html 4 The TRIPS parser can be accessed at: http:// trips.ihmc.us/parser/cgi/parse 140 is on obtaining productive patterns in VPCs rather than on their interpretation. Our work also differs from the previous works mentioned above in the following respect: we emphasize on building complete semantic representations of the sentences, not just on particles’ semantics or just classification of VPCs. Similar to our criteria for compositionality, McCarthy et al. (2003), Baldwin et al. (2003), Bannard et al. (2003) have looked at distributional similarity as a measure of compositionality of VPCs. In contrast to the approaches focusing on statistical classification based on word/syntax features, we present our heuristics for classification of VPCs based on WordNet and discuss how we compute the semantics of the compositional classes. 3 provide the meaning of the VPC fly up. We distinguish the other two compositional VPC types from the symmetrically compositional VPCs only in the aspect that in the other two types, the particle or the verb have a relatively li"
W18-1402,W08-2227,1,0.888774,"Missing"
W18-1402,N15-1086,0,0.0323511,"mappings (Fauconnier, 1997), we focus on projection mappings between structure and set features and demonstrate instances of common situated language that makes use of such mappings. With these concepts grounded in a situated space, we believe we will be poised to extend the concepts in Blocks World into more abstract reasoning and language through grounded metaphor. We also demonstrate the ability of our system to build up a model of a class of structures through natural language dialogue. Rather than constructing a new domain-specific representation for storing such knowledge, as in work by Hixon et al. (2015), we retain the semantic logical form structure as our base representation, using ontological concepts of comparison and semantic argument structures to ground concepts and predicates in the situated environment. We therefore aim to show that a linguistic structures from a semantic parser can serve as a strong base for reasoning and model-building in a situated context. We demonstrate a system for understanding natural language utterances for structure description and placement in a situated blocks world context. By relying on a rich, domainspecific adaptation of a generic ontology and a logic"
W18-1402,D12-1040,0,0.0260451,"as early as one of the first Blocks World natural language interaction systems, SHRDLU (Winograd, 1971), discussions about structures and space have been viewed as the foundation for future language understanding systems dealing with more abstract and higher-level concepts. Since then, the field has advanced in the task of learning how to understand utterances in Blocks World and other situated environments by using statistical methods grounding syntactic trees to entities and actions in the world to learn placement descriptions (Bisk et al., 2016), predicates (Kollar et al., 2013), actions (Kim and Mooney, 2012; She et al., 2014) or a combination of paths and actions (Tellex et al., 2011). However, rather than considering grounding solely as a mapping to actions and objects in the world, we use the deep language understanding capabilities of the TRIPS parser (Allen et al., 2008) to find deeper conceptual connections to primitive, composable, and often recursive aspects of structures, and use this 2 Capabilities and Tasks We evaluate our system in a situated blocks world environment with 6-inch cubes placed on a table. Aside from unique identifiers for tracking, each cube is considered identical. Our"
W18-1402,D17-1106,0,0.0510797,"Missing"
W18-1402,W14-4313,0,0.0695726,"e first Blocks World natural language interaction systems, SHRDLU (Winograd, 1971), discussions about structures and space have been viewed as the foundation for future language understanding systems dealing with more abstract and higher-level concepts. Since then, the field has advanced in the task of learning how to understand utterances in Blocks World and other situated environments by using statistical methods grounding syntactic trees to entities and actions in the world to learn placement descriptions (Bisk et al., 2016), predicates (Kollar et al., 2013), actions (Kim and Mooney, 2012; She et al., 2014) or a combination of paths and actions (Tellex et al., 2011). However, rather than considering grounding solely as a mapping to actions and objects in the world, we use the deep language understanding capabilities of the TRIPS parser (Allen et al., 2008) to find deeper conceptual connections to primitive, composable, and often recursive aspects of structures, and use this 2 Capabilities and Tasks We evaluate our system in a situated blocks world environment with 6-inch cubes placed on a table. Aside from unique identifiers for tracking, each cube is considered identical. Our physical apparatus"
W18-1402,P16-1224,0,0.0282078,"speech act given dialogue context and further refines roles and interpretations according to domain-specific rules and ontological constraints. Figure 1: The apparatus used to interact with the system. We focus on two tasks for evaluating our system within the context of a natural language dialogue system. The first is correctly understanding a variety of natural placement utterances and generating the expected placement of blocks that satisfies the command. There has been significant previous work on learning to interpret typical placement instructions (Bisk et al., 2016; Misra et al., 2017; Wang et al., 2016) or descriptions of block scenes (Bisk et al., 2018). While we have limited capabilities for understanding such instructions, this prior work is better suited for more robust and precise placement interaction that does not utilize conceptual composition. Therefore, rather than solely understanding simple phrases such as “Place a block on top of the leftmost block”, we focus our efforts towards understanding more complex phrases that utilize context, such as “Add another one,” and linguistic/semantic composition, as in, “Place three towers in a row with increasing height”. The second task is te"
W18-5010,W08-2227,1,0.764316,"Missing"
W18-5010,K15-1023,1,0.854188,"estions about demonstrated examples and simulates new examples to check its knowledge and verify the user’s description is complete. We find that this task is non-trivial for users and generates natural language that is varied yet understandable by our deep language understanding architecture. 1 2 The Structure Learning Task Many natural language dialogue tasks in a Blocks World environment focus on querying the environment (Winograd, 1971), block placement (Bisk et al., 2016; She et al., 2014), or training visual classifiers and grounding perception (Matuszek et al., 2012; Mast et al., 2016; Perera and Allen, 2015). Reference resolution has also been extensively studied in this environment and statistical methods show strong performance in quickly learning referring expressions (Kennington et al., 2015). However, our focus is exploring collaborative concept transfer with the goal of having situated agents learn from natural language dialogue and physical interaction to become better collaborators. With the goal of the system as a collaborator, we find it is important that the task carried out be non-trivial for the user. However, more difficult tasks can have drawbacks – they involve larger amounts of b"
W18-5010,W18-1402,1,0.806846,"aints. Currently we encourage users to describe the structure in a general way – however, this type of utterance demonstrates an example where a specific error or instance applies generally enough to serve as a constraint. Procedural/Construction Modality – “No, you’re missing a row. Subtract by one every time you go up.” – Here the user is describing the structure by providing the process for creating it. While our current system does not understand these types of utterances, we are working on a similar task in parallel with this environment that can be leveraged to understand this modality (Perera et al., 2018). The key aspect of understanding this type of utterance is the compositionality of primitive concepts, namely a row, its length, and the sequence of rows that will presumably be placed by the agent building the structure. Description Modalities We recognized several different description modalities participants used when describing the structure without responding to a particular feature query. When the system asked questions, typically the user responded directly to the question, reducing the utterance complexity. However, these variations on the expected descriptions reveal interesting insi"
W18-5010,W14-4313,0,0.0318182,"ser determines the underlying constraints to be provided to the system in natural language. The system in turn asks questions about demonstrated examples and simulates new examples to check its knowledge and verify the user’s description is complete. We find that this task is non-trivial for users and generates natural language that is varied yet understandable by our deep language understanding architecture. 1 2 The Structure Learning Task Many natural language dialogue tasks in a Blocks World environment focus on querying the environment (Winograd, 1971), block placement (Bisk et al., 2016; She et al., 2014), or training visual classifiers and grounding perception (Matuszek et al., 2012; Mast et al., 2016; Perera and Allen, 2015). Reference resolution has also been extensively studied in this environment and statistical methods show strong performance in quickly learning referring expressions (Kennington et al., 2015). However, our focus is exploring collaborative concept transfer with the goal of having situated agents learn from natural language dialogue and physical interaction to become better collaborators. With the goal of the system as a collaborator, we find it is important that the task"
W18-5010,W15-0124,0,0.0291171,"tes natural language that is varied yet understandable by our deep language understanding architecture. 1 2 The Structure Learning Task Many natural language dialogue tasks in a Blocks World environment focus on querying the environment (Winograd, 1971), block placement (Bisk et al., 2016; She et al., 2014), or training visual classifiers and grounding perception (Matuszek et al., 2012; Mast et al., 2016; Perera and Allen, 2015). Reference resolution has also been extensively studied in this environment and statistical methods show strong performance in quickly learning referring expressions (Kennington et al., 2015). However, our focus is exploring collaborative concept transfer with the goal of having situated agents learn from natural language dialogue and physical interaction to become better collaborators. With the goal of the system as a collaborator, we find it is important that the task carried out be non-trivial for the user. However, more difficult tasks can have drawbacks – they involve larger amounts of background knowledge and reasoning, progress can be difficult to evaluate, and often the language and concepts learned do not extend easily to other real world applications. With these constrai"
W18-5010,P16-4012,0,0.0837508,"exible corpus-based models in the future, aided by transcripts of previous trials. 4 System Architecture The heart of the dialogue management is the TRIPS architecture (Allen et al., 2001), which connects a number of components through KQML message passing (Finin et al., 1994), with each component augmented with domain-specific knowledge to varying extents. This dialogue management component, including parser, a generic ontology, and an API for interacting with a domain-specific module is open-source and available for download 1 . As opposed to other dialogue management systems like OpenDial (Lison and Kennington, 2016) or POMDP dialogue systems (Williams and Young, 2007), this dialogue management system is primarily suited for collaborative tasks where there is little to no knowledge of what dialogue state typically follows from the previous one – the user can move from statements about goals to assertions to questions in any order, determined primarily by the speech act detected in their utterance. For semantic language understanding and speech act interpretation of the user’s utterances, the domain-generic 1 4.1 Collaborative State Manager The Collaborative State Manager stores and responds to queries reg"
W18-5048,2005.sigdial-1.20,1,0.523694,"-solving level, where each agent manages its own problemsolving state, plans and executes individual actions, etc.;  A collaborative problem-solving level, which models and manages the joint or collaborative problem-solving state (shared goals, resources, situations);  An interaction level, where individual agents negotiate changes in the joint problem-solving state; and, finally,  A communication level, where speech acts realize the interaction level acts. This model was refined in a series of publications, and several prototype systems were developed for illustration (Allen et al., 2002; Blaylock and Allen, 2005; Allen et al., 2007; Ferguson and Allen, 2007), all based on the TRIPS system (Allen et al., 2000). One of the main benefits of this model is that linguistic interpretation and high-level intention recognition could be performed independently of the individual problem-solving level, whose contribution to interpretation would be to specialize the higher-level intentions into concrete problemsolving actions and verify that such actions make sense. The corollary is that in this model the backend problem solvers would be insulated from the need to worry about linguistic issues. On this basis, it"
W18-5048,N18-5018,0,0.0302804,"cesses and disease, building and visualizing complex causal models, running simulations on these models to detect their dynamic properties, etc. To manage this wide range of problemsolving behaviors, BoB’s BA integrates a variety of agents with specific expertise. 3 https://www.darpa.mil/program/communicating-withcomputers Musica: This system uses a computational model of music cognition, as well as knowledge about existing pieces of music, to help a human composer create and edit a musical score (Quick and Morrison, 2017). SMILEE: This system acts as a partner for playing a cooperative game (Kim et al., 2018). The game involves placing pieces (blocks) on a board to create complex symmetrical configurations. Players alternate, but each player can hold their turn for multiple rounds. Each player has some freedom to be creative with respect to the configuration being pursued (it is not set in advance). Thus, they have to negotiate turn taking, and they can ask for explanations to achieve a shared understanding about the properties of the configuration being created. Aesop: A system for building animated stories. The user acts as a director, and can choose scenes, props, characters, direct them what t"
W18-5048,W18-5010,1,0.731742,"Missing"
W18-5048,P16-4012,0,0.0313083,"nce of tools and frameworks that can be customized to develop specific applications. In the area of dialogue systems, there are few such tools and frameworks and they mostly remain focused on simple tasks that can be encoded in a state-based dialogue model (see, e.g., Williams et al., 2016 and the Dialogue State Tracking Challenge1). In this category some of the more expressive approaches to dialogue modeling are based on the information state (Cooper, 1997); notable toolkits include TrindiKit (Larsson and Traum, 2000) and its open-source successor trindikit.py (Ljunglöf, 2009), and OpenDial (Lison and Kennington, 2016). Unfortunately, there is a dearth of tools for developing mixed-initiative dialogue systems that involve complex back-end reasoning systems. Early theoretical work of SharedPlans (Grosz and Kraus, 1996; Lochbaum et al., 1990) and planbased dialogue systems (e.g., Allen and Perrault, 1980; Litman and Allen, 1987) laid good foundations. The Collaborative Problem Solving (CPS) model (Allen et al., 2002) seemed to promise a solution but that model has never been implemented in a truly domain-independent way. Ravenclaw (Bohus and Rudnicky, 2009) is a plan-based dialog management framework that has"
