C10-2085,P09-2007,1,0.530226,"their Hanyu pinyin. The digit that follows the symbols for the sound is the tone for the character. the incorrect characters. For instance, one might use “ӊ” (hwa2) in the place of “㣣”(hwa4) in “څ㣣ຝ” (ke1 hwa4 xing2 xiang4) partially because of phonological similarity; one might replace “( ”ܟzhuo2) in “Ј㞇Κ( ”ܟxin1 lao2 li4 zhuo2) with “䶅” (chu4) partially due to visual similarity. (We do not claim that the visual or phonological similarity alone can explain the observed errors.) Similar characters are important for understanding the errors in both traditional and simplified Chinese. Liu et al. (2009a-c) applied techniques for manipulating correctness of Chinese words to computer assisted test-item generation. Research in psycholinguistics has shown that the number of neighbor characters influences the timing of activating the mental lexicon during the process of understanding Chinese text (Kuo et al. 2004; Lee et al. 2006). Having a way to compute and find similar characters will facilitate the process of finding neighbor words, so can be instrumental for related studies in psycholinguistics. Algorithms for optical character recognition for Chinese and for recognizing written Chinese try"
C10-2085,W09-3412,1,0.904204,"their Hanyu pinyin. The digit that follows the symbols for the sound is the tone for the character. the incorrect characters. For instance, one might use “ӊ” (hwa2) in the place of “㣣”(hwa4) in “څ㣣ຝ” (ke1 hwa4 xing2 xiang4) partially because of phonological similarity; one might replace “( ”ܟzhuo2) in “Ј㞇Κ( ”ܟxin1 lao2 li4 zhuo2) with “䶅” (chu4) partially due to visual similarity. (We do not claim that the visual or phonological similarity alone can explain the observed errors.) Similar characters are important for understanding the errors in both traditional and simplified Chinese. Liu et al. (2009a-c) applied techniques for manipulating correctness of Chinese words to computer assisted test-item generation. Research in psycholinguistics has shown that the number of neighbor characters influences the timing of activating the mental lexicon during the process of understanding Chinese text (Kuo et al. 2004; Lee et al. 2006). Having a way to compute and find similar characters will facilitate the process of finding neighbor words, so can be instrumental for related studies in psycholinguistics. Algorithms for optical character recognition for Chinese and for recognizing written Chinese try"
C10-2085,P08-2024,1,\N,Missing
O01-1012,O98-3002,1,0.895819,"Missing"
O01-1012,W00-1203,1,0.863178,"Missing"
O04-1012,W97-0209,0,0.141996,"Missing"
O04-1012,A97-1004,0,0.021467,"Missing"
O05-1021,P99-1009,0,0.0493036,"Missing"
O05-1021,P98-1034,0,0.032955,"Missing"
O05-1021,A88-1019,0,0.102673,"Missing"
O05-1021,W00-0730,0,0.504047,"照明 劉昭麟 台大資工系 台大外文系 政大資科系 b91083@csie.ntu.edu.tw zmgao@ntu.edu.tw chaolin@nccu.edu.tw 摘要 本文實做 Kudo and Matsumoto (2000, 2001)以向量支撐機（SVM）辨識基底名詞組（base NP）演算法。我們以 中央研究院中文句結構樹資料庫 Sinica Treebank 3.0 的 80％作為訓練語料,20％作為測試語料,並比較以 Sinica Treebank 三種不同的詞性標記集訓練出來的 SVM 的辨識率（簡化標記，精簡標記，及簡化標記的大類） 。實驗 的結果顯示具備詳細次分類的簡化標記的辨識率最高，在封閉測試的 F-measure 為 87.43％，初步小規模開放測 試的 F-measure 為 78.79％。詳細次分類的標記集的名詞組辨識率較高的原因是中文某些類別的動詞能夠修飾名 詞，因此沒有詳細次分類的詞類標記集無法區別那些類別的動詞可以修飾名詞。與英文日文高達 94％以上的辨 識率相比較，SVM 在中文基底名詞組辨識的效果並不理想，我們認為中研院句法樹的表示法與中文本身的特性 是造成辨識率不夠高的主要原因。 1. 前言 名詞組的辨識與標示（NP Chunking）是自然語言處理（NLP）的一個重要研究議題 (Ramshaw and Marcus (1995), Kudo and Matsumoto (2000, 2001))，無論是句法處理中的剖析(parsing)語意處理中的語意角色的標示（semantic role labeling）及篇章處理中的回指(co-reference)與連貫性（coherence） ，其它領域如資訊檢索(information retrieval) 資訊擷取（information extraction）文件探勘（text mining）文件分類，與文件自動摘要都需要名詞組的辨識， 例如在資訊檢索中最常被檢索的大都是名詞組（特別是人名，地名，組織名等所謂的 name entity） ，因此在文件 或網頁中自動辨識名詞組並建立索引以方便檢索分類及自動摘要是智慧型資訊處理極為重要的一環。 一般名詞組的辨識指的是基底名詞組（base NP） ，也就是將名詞組下面又包含名詞組的複雜名詞組（如關係子 句及名詞組並列結構(NP conjunction)）排除在外。目前英文名詞組的辨識正確率可以達到 94％以上(Kudo and Matsumoto (2000, 2001))，但中文名詞組的辨識至今只有少數零星的研究。本文採用監督式機器學習（supervised learning）嘗試以向量支撐機(SVM, support vector machine)透過中研院句法樹庫實做 Kudo and Matsumoto (2000, 2001)所提出的演算法。本研究的主要目"
O05-1021,W00-1303,0,0.451893,"照明 劉昭麟 台大資工系 台大外文系 政大資科系 b91083@csie.ntu.edu.tw zmgao@ntu.edu.tw chaolin@nccu.edu.tw 摘要 本文實做 Kudo and Matsumoto (2000, 2001)以向量支撐機（SVM）辨識基底名詞組（base NP）演算法。我們以 中央研究院中文句結構樹資料庫 Sinica Treebank 3.0 的 80％作為訓練語料,20％作為測試語料,並比較以 Sinica Treebank 三種不同的詞性標記集訓練出來的 SVM 的辨識率（簡化標記，精簡標記，及簡化標記的大類） 。實驗 的結果顯示具備詳細次分類的簡化標記的辨識率最高，在封閉測試的 F-measure 為 87.43％，初步小規模開放測 試的 F-measure 為 78.79％。詳細次分類的標記集的名詞組辨識率較高的原因是中文某些類別的動詞能夠修飾名 詞，因此沒有詳細次分類的詞類標記集無法區別那些類別的動詞可以修飾名詞。與英文日文高達 94％以上的辨 識率相比較，SVM 在中文基底名詞組辨識的效果並不理想，我們認為中研院句法樹的表示法與中文本身的特性 是造成辨識率不夠高的主要原因。 1. 前言 名詞組的辨識與標示（NP Chunking）是自然語言處理（NLP）的一個重要研究議題 (Ramshaw and Marcus (1995), Kudo and Matsumoto (2000, 2001))，無論是句法處理中的剖析(parsing)語意處理中的語意角色的標示（semantic role labeling）及篇章處理中的回指(co-reference)與連貫性（coherence） ，其它領域如資訊檢索(information retrieval) 資訊擷取（information extraction）文件探勘（text mining）文件分類，與文件自動摘要都需要名詞組的辨識， 例如在資訊檢索中最常被檢索的大都是名詞組（特別是人名，地名，組織名等所謂的 name entity） ，因此在文件 或網頁中自動辨識名詞組並建立索引以方便檢索分類及自動摘要是智慧型資訊處理極為重要的一環。 一般名詞組的辨識指的是基底名詞組（base NP） ，也就是將名詞組下面又包含名詞組的複雜名詞組（如關係子 句及名詞組並列結構(NP conjunction)）排除在外。目前英文名詞組的辨識正確率可以達到 94％以上(Kudo and Matsumoto (2000, 2001))，但中文名詞組的辨識至今只有少數零星的研究。本文採用監督式機器學習（supervised learning）嘗試以向量支撐機(SVM, support vector machine)透過中研院句法樹庫實做 Kudo and Matsumoto (2000, 2001)所提出的演算法。本研究的主要目"
O05-1021,J93-2004,0,0.0268618,"Missing"
O05-1021,N04-1030,0,0.0328025,"arning 技術 (Boser, Guyon, and Vapnik (1992), Corts and Vapnik (1995)) 它使用一些 策略來最大化具有不同特徵的資料中間的界限, 並針對未知資料的特徵來判斷它屬於哪個類別。SVM 已在文件 分類（Joachims （1998） Taira and Haruno （1999） ）以及名詞組標示 (Kudo and Matsumoto （2000, 20001）) 取得超越其它作法的準確性, 而近幾年應用在自然語言處理的各個議題的研究更是方興未艾，如未知詞辨識 （unknown word guessing）(Nakagawa, Kudo, and Matsumoto (2001)）詞性標注(part of speech tagging)Nakagawa, Kudo, and Matsumoto (2002)， Giménez J esús and Márquez Lluís (2004))句法依存關係辨識(dependency analysis)(Kudo and Matsumoto (2000))詞義辨別與標注(word sense disambiguation and sense tagging) (Cabezas, Resnik, and Stevens (2001))語意剖析(semantic parsing) (Pradhan et al. (2004) Sun and Jurafsky (2004))等都取得不 錯的成果。 SVM 是一個分類用的 machine。請參照圖（一，二）, 圖一 圖二 SVM 找出兩種資料 (黑色方形與白色圓形) 中間的界限, 圖一，圖二顯示出可能的兩種分割方式, 顯然的, 後 者的切割方式是較佳的 (兩種資料的界線為兩平行線之中線), 而 SVM 以滿足下面條件 2 mi n Φ( ω) =( 1/ 2) | ω| 找出最佳平面 (即在線性可分的情況下, 可視為解二次規畫的問題), 而此可由拉格朗日乘子法（Lagrange multiplier）求解. 由於很多的問題常常並不是線性可分的 (如我們的詞組切割), 這個時候 SVM 在比現有資料更高的向量空間 H 使用線性分類函數 Φ:Rd → H 將 x 對應到高維空間, 便可在此以不破壞資料特徵亦不增加複雜度的方式對 其進行分類. 在轉換的過程中, 我們會使用一 kernel function: K(xi,xj) = Φ(xi)TΦ(xj) 來實現非線性變換後的線性分類, 而使用 不同的 kernel function 對不同的資料會有不同的效果. 以下為一個簡單的 SVM 運作方式 給定一個訓練的資料集合: (xi, yi) { i= 1, 2, ..., l; xi 屬於 Rn; yi 屬於 { 1, -1 }} 其中 l 為訓練之資料數, x"
O05-1021,W95-0107,0,0.0784395,"Missing"
O05-1021,W98-1117,0,0.0522401,"Missing"
O05-1021,N04-1032,0,0.0235636,"n, and Vapnik (1992), Corts and Vapnik (1995)) 它使用一些 策略來最大化具有不同特徵的資料中間的界限, 並針對未知資料的特徵來判斷它屬於哪個類別。SVM 已在文件 分類（Joachims （1998） Taira and Haruno （1999） ）以及名詞組標示 (Kudo and Matsumoto （2000, 20001）) 取得超越其它作法的準確性, 而近幾年應用在自然語言處理的各個議題的研究更是方興未艾，如未知詞辨識 （unknown word guessing）(Nakagawa, Kudo, and Matsumoto (2001)）詞性標注(part of speech tagging)Nakagawa, Kudo, and Matsumoto (2002)， Giménez J esús and Márquez Lluís (2004))句法依存關係辨識(dependency analysis)(Kudo and Matsumoto (2000))詞義辨別與標注(word sense disambiguation and sense tagging) (Cabezas, Resnik, and Stevens (2001))語意剖析(semantic parsing) (Pradhan et al. (2004) Sun and Jurafsky (2004))等都取得不 錯的成果。 SVM 是一個分類用的 machine。請參照圖（一，二）, 圖一 圖二 SVM 找出兩種資料 (黑色方形與白色圓形) 中間的界限, 圖一，圖二顯示出可能的兩種分割方式, 顯然的, 後 者的切割方式是較佳的 (兩種資料的界線為兩平行線之中線), 而 SVM 以滿足下面條件 2 mi n Φ( ω) =( 1/ 2) | ω| 找出最佳平面 (即在線性可分的情況下, 可視為解二次規畫的問題), 而此可由拉格朗日乘子法（Lagrange multiplier）求解. 由於很多的問題常常並不是線性可分的 (如我們的詞組切割), 這個時候 SVM 在比現有資料更高的向量空間 H 使用線性分類函數 Φ:Rd → H 將 x 對應到高維空間, 便可在此以不破壞資料特徵亦不增加複雜度的方式對 其進行分類. 在轉換的過程中, 我們會使用一 kernel function: K(xi,xj) = Φ(xi)TΦ(xj) 來實現非線性變換後的線性分類, 而使用 不同的 kernel function 對不同的資料會有不同的效果. 以下為一個簡單的 SVM 運作方式 給定一個訓練的資料集合: (xi, yi) { i= 1, 2, ..., l; xi 屬於 Rn; yi 屬於 { 1, -1 }} 其中 l 為訓練之資料數, xi 為一個 n 維向量, yi 則是其類別 (分"
O05-1021,E99-1023,0,0.0564355,"Missing"
O05-1021,P00-1042,0,0.040122,"Missing"
O05-4001,W97-0209,0,0.290543,"Missing"
O05-4001,A97-1004,0,0.0413742,"Missing"
O05-4001,Y03-1009,0,\N,Missing
O05-4001,O98-3002,0,\N,Missing
O05-4001,W99-0707,0,\N,Missing
O05-4001,J90-1003,0,\N,Missing
O05-4001,J93-2004,0,\N,Missing
O05-4001,W96-0213,0,\N,Missing
O05-4001,W03-1728,0,\N,Missing
O05-4001,C02-1055,0,\N,Missing
O05-4001,W02-1817,0,\N,Missing
O05-4001,E99-1023,0,\N,Missing
O05-4001,W03-1719,0,\N,Missing
O05-4001,W03-1720,0,\N,Missing
O05-4001,W02-1815,0,\N,Missing
O05-4001,C02-1049,0,\N,Missing
O05-4001,O04-3004,0,\N,Missing
O05-4001,C02-1145,0,\N,Missing
O05-4001,W03-1701,0,\N,Missing
O05-4001,W03-1730,0,\N,Missing
O05-4001,N01-1025,0,\N,Missing
O05-4001,W03-1705,0,\N,Missing
O05-4001,W03-1726,0,\N,Missing
O07-2011,J90-2002,0,0.525601,"Missing"
O07-2011,E99-1010,0,0.0731497,"Missing"
O07-2011,P00-1056,0,0.016806,"Missing"
O07-2011,P02-1040,0,0.075263,"Missing"
O07-2011,O02-1003,0,0.0558717,"Missing"
O08-1008,O05-4001,1,0.886507,"Missing"
O08-2005,W03-0206,0,0.0669373,"Missing"
O08-2005,O05-4001,1,0.894876,"Missing"
O08-2005,W03-0203,0,0.0317189,"Missing"
O08-2006,H92-1086,0,0.01509,"Missing"
O08-2006,N03-1017,0,0.00446258,"Missing"
O08-2006,E99-1010,0,0.101172,"Missing"
O08-2007,P98-1012,0,0.0242074,"Missing"
O09-1007,ma-2006-champollion,0,0.24097,"Missing"
O09-1007,2007.mtsummit-papers.63,0,0.111376,"Missing"
O09-1007,C96-1035,0,0.143587,"Missing"
O09-1013,E91-1005,0,0.178489,"Missing"
O09-1013,J00-4006,0,0.0125854,"Missing"
O09-1013,P93-1016,0,0.0652275,"Missing"
O10-1007,O09-3003,0,0.108707,"Missing"
O10-1007,A92-1018,0,0.440932,"Missing"
O10-1009,D09-1050,0,0.0610885,"Missing"
O10-1009,J97-2004,0,0.102597,"Missing"
O10-1009,2007.mtsummit-papers.63,0,0.0617377,"Missing"
O10-1009,W03-0301,0,0.0990029,"Missing"
O10-5003,Y06-1004,0,0.0665362,"Missing"
O10-5003,O10-1007,1,0.881477,"Missing"
O10-5003,O00-1009,0,0.0662325,"Missing"
O10-5003,O08-4004,0,0.0613091,"Missing"
O10-5003,O09-3003,0,\N,Missing
O11-1006,C10-3012,0,0.0285502,"Missing"
O11-1006,H05-1113,0,\N,Missing
O11-1006,W10-4110,0,\N,Missing
O11-3003,O03-4004,0,0.115302,"Missing"
O11-3003,W06-0122,0,0.0691491,"Missing"
O11-3003,O07-1015,0,0.0622639,"Missing"
O12-1031,C02-1049,0,0.021005,"Missing"
O12-1031,P08-1102,0,0.0607095,"Missing"
O12-1031,W03-1701,0,0.0350922,"Missing"
O12-1031,C10-1132,0,0.027032,"Missing"
O12-4001,W08-0336,0,0.053015,"Missing"
O12-4001,O05-4002,0,0.0251591,"Missing"
O12-4001,Y11-1052,1,0.19687,"Missing"
O12-4001,I05-7001,0,0.0183747,"Missing"
O12-4001,P03-1054,0,0.00685974,"Missing"
O12-4001,N03-1017,0,0.0155359,"Missing"
O12-4001,J04-1003,0,0.0799208,"Missing"
O12-4001,W10-4110,0,0.0595846,"Missing"
O12-4001,P04-1022,0,0.061195,"Missing"
O12-4001,ma-2006-champollion,0,0.0193097,"Missing"
O12-4001,J96-1001,0,0.22678,"Missing"
O12-4001,2006.amta-papers.24,0,0.0329601,"Missing"
O12-4001,O09-1007,1,0.881308,"Missing"
O12-4001,2009.mtsummit-wpt.5,0,0.0466233,"Missing"
O12-4001,O11-1006,1,\N,Missing
O12-4001,J06-1003,0,\N,Missing
O12-4001,O04-1027,0,\N,Missing
O12-4001,I05-3027,0,\N,Missing
O12-4001,Y09-2022,1,\N,Missing
O90-1004,J83-3004,0,\N,Missing
O90-1004,J82-3004,0,\N,Missing
O90-1004,C88-2133,1,\N,Missing
O90-1004,P85-1037,0,\N,Missing
O90-1004,W89-0210,1,\N,Missing
P09-2007,W09-3412,1,0.349789,"Missing"
P09-2007,P08-2024,1,0.394469,"submitted queries is an important factor, and, in other cases, incorrect words were more commonly used. 5 Facilitating Test Item Authoring Incorrect character correction is a very popular type of test in Taiwan. There are simple test items for young children, and there are very challenging test items for the competitions among adults. Finding an attractive incorrect character to replace a correct character to form a test item is a key step in authoring test items. We have been trying to build a software environment for assisting the authoring of test items for incorrect character correction (Liu and Lin, 2008, Liu et al., 2009). It should be easy to find a lexicon that contains pronunciation information about Chinese characters. In contrast, it might not be easy to find visually similar Chinese characters with computational methods. We expanded the original Cangjie codes (OCC), and employed the expanded Cangjie codes (ECC) to find visually similar characters (Liu and Lin, 2008). With a lexicon, we can find characters that can be pronounced in a particular way. However, this is not enough for our goal. We observed that there were different symptoms when people used incorrect characters that are rel"
W05-0201,O05-4001,1,0.896332,"arge-scale item banks, and has attracted active study in the past decade (Deane and Sheehan, 2003; Irvine and Kyllonen, 2002). Applying techniques for natural language processing (NLP), CAIG offers the possibility of creating a large number of items of different challenging levels, thereby paving a way to make computers more adaptive to students of different competence. Moreover, with the proliferation of Web contents, one may search and sift online text files for candidate sentences, and come up with a list of candidate cloze 1 A portion of results reported in this paper will be expanded in (Liu et al., 2005; Huang et al., 2005). items economically. This unleashes the topics of the test items from being confined by item creators’ personal interests. NLP techniques serve to generate multiple-choice cloze items in different ways. (For brevity, we use cloze items or items for multiple-choice cloze items henceforth.) One may create sentences from scratch by applying template-based methods (Dennis et al., 2002) or more complex methods based on some predetermined principles (Deane and Sheehan, 2003). Others may take existing sentences from a corpus, and select those that meet the criteria for becoming"
W05-0201,W97-0209,0,0.0389715,"2 extracts qualified sentences from the corpus. A sentence must contain the desired key of the requested POS to be considered as a candidate target sentence. Having identified such a candidate sentence, the item generator needs to determine whether the sense of the key also meets the requirement. We conduct this WSD task based on an extended notion of selectional preferences. 4.1 Extended Selectional Preferences Selectional preferences generally refer to the phenomenon that, under normal circumstances, some verbs constrain the meanings of other words in a sentence (Manning and Sch¨utze, 1999; Resnik, 1997). We can extend this notion to the relationships between a word of interest and its signals, with the help of HowNet. Let w be the word of interest, and π be the first listed class, in HowNet, of a signal word that has the syntactic relationship µ with w. We define the strength of the association of w and π as follows: Prµ (w, π) , (1) Aµ (w, π) = Prµ (w) where Prµ (w) is the probability of w participating in the µ relationship, and Prµ (w, π) is the probability that both w and π participate in the µ relationship. 4.2 Word Sense Disambiguation We employ the generalized selectional preferences"
W05-0201,A97-1004,0,0.0527533,"web crawler, we retrieve the contents of Taiwan Review <publish.gio.gov.tw>, Taiwan Journal <taiwanjournal.nat.gov.tw>, and China Post <www.chinapost.com.tw>. Currently, we have 127,471 sentences that consist of 2,771,503 words in 36,005 types in the corpus. We look for useful sentences from web pages that are encoded in the HTML format. We need to extract texts from Figure 4: An output after Figure 3 the mixture of titles, main body of the reports, and multimedia contents, and then segment the extracted paragraphs into individual sentences. We segment sentences with the help of MXTERMINATOR (Reynar and Ratnaparkhi, 1997). We then tokenize words in the sentences before assigning useful tags to the tokens. We augment the text with an array of tags that facilitate cloze item generation. We assign tags of part-of-speech (POS) to the words with MXPOST that adopts the Penn Treebank tag set (Ratnaparkhi, 1996). Based on the assigned POS tags, we annotate words with their lemmas. For instance, we annotate classified with classify and classified, respectively, when the original word has VBN and JJ as its POS tag. We also employ MINIPAR (Lin, 1998) to obtain the partial parses of sentences that we use extensively in ou"
W09-3412,P08-2024,1,0.368853,"lly examined all of the n-grams in the initial list, and removed such n-grams from the list. In addition to considering the frequencies of ngrams formed by the basic Cangjie codes to determine the list of components, we also took advantage of radicals that are used to categorize Chinese characters in typical printed dictionaries. Radicals that are standalone Chinese words were included in the list of components. After selecting the list of basic components with the above procedure, we encoded the words in Elist with these basic components. We inherited the 12 ways reported in a previous work (Liu and Lin, 2008) to decompose Chinese characters. There are other methods for decomposing Chinese characters into components. Juang et al. (2005) and their team at the Sinica Academia propose 13 different ways for decomposing characters. At the same time when we annotated individual characters with their ECCs, we may revise the list of basic components. If a character that actually contained an intuitively “common” part and that part had not been included in the list of basic component, we would add this part into the list to make it a basic component and revised the ECC for all characters accordingly. The ju"
W10-4107,J93-2003,0,0.0390042,"Missing"
W10-4107,P96-1041,0,0.0211056,"Language model Since there is no large corpus of student essays, we used a news corpus to train the language model. The size of the news corpus is 1.5 GB, which consists of 1,278,787 news articles published between 1998 and 2001. The n-gram language model was adopted to calculate the probability of a sentence p(S). The general n-gram formula is: p ( S ) = p ( wn |wnn−−1N +1 ) (1) Where N was set to two for bigram and N was set to one for unigram. The Maximum Likelihood Estimation (MLE) was used to train the n-gram model. We adopted the interpolated Kneser-Ney smoothing method as suggested by Chen & Goodman (1996). As following: pint erpolate ( w |wi −1 ) = λpbigram ( w |wi −1 ) + (1 − λ ) punigram ( w) (2) To determine whether a replacement is good or not, our system use the modified perplexity: Perplexity = 2 − log( p ( S )) / N (3) Where N is the length of a sentence and p(S) is the bigram probability of a sentence after smoothing. 3.3 Dictionary and test set We used a free online dictionary provided by Taiwan’s Ministry of Education, MOE (2007). We filtered out one character words and used the remaining 139,976 words which were more than one character as our lexicon in the following experiments. Th"
W10-4107,O09-2007,1,0.838226,"pendently constructed confusion sets, and a fixed language model to reconstruct the systems. We performed tests on the same test set. 3 3.1 Data in Experiments Confusion sets Confusion sets are a collection of sets for each individual Chinese character. A confusion set of a certain character consists of phonologically or logographically similar characters. For example, the confusion set of “辦” might consist of the following characters with the same pronunciation“半伴扮姅拌絆瓣＂ or with similar forms“辨瓣辮辯避僻辣梓辭鋅辟滓辛宰癖 莘辜薜薛闢”. In this study, we use the confusion sets used by Liu, Tien, Lai, Chuang, & Wu (2009). The similar Cangjie (SC1 and SC2) sets of similar forms, and both the same-sound-same-tone (SSST) and same-sound-different-tone (SSDT) sets for similar pronunciation were used in the experiments. There were 5401 confusion sets for each of the 5401 high frequency characters. The size of each confusion set was one to twenty characters. The characters in each confusion set were ranked according to Google search results. 3.2 Language model Since there is no large corpus of student essays, we used a news corpus to train the language model. The size of the news corpus is 1.5 GB, which consists of"
W10-4107,W09-3412,1,\N,Missing
W10-4107,P09-2007,1,\N,Missing
W13-4406,P00-1032,0,0.81353,"the correct characters of detected errors. Spelling check must be done within a context, say a sentence or a long phrase with a certain meaning, and cannot be done within one word (Mays et al., 1991). However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. There are several previous studies addressing the Chinese spelling check problem. Chang (1995) has proposed a bi-gram language model to substitute the confusing character for error detection and correction. Zhang et al. (2000) have presented an approximate word-matching algorithm to detect and correct Chinese spelling errors us35 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 35–42, Nagoya, Japan, 14 October 2013. uation. Section 4 proposes the evaluation metrics for both subtasks. Section 5 presents the results of participants’ approaches for performance comparison. Section 6 elaborates on the semantic and pragmatic aspects of automatic correction of Chinese text. Finally, we conclude this paper with the findings and future research direction in the Section 7. 2 <DOC Ni"
W16-4023,C12-2061,0,0.0338842,"tically hard for human experts to achieve before. Studying Chinese poetry with computing technologies started at least two decades ago, so we do not mean to provide a comprehensive review of the literature. Lo and her colleagues implemented a computer assisted environment (Lo et al. 1997). Hu and Yu (2001) reported some analyses of unigrams and bigrams in Tang poems, and looked for Chinese synonyms in Tang and Song poems (Hu & Yu 2002). Lee attempted to do dependency parsing of Tang poems (Lee & Kong 2012), and explored the roles of named entities, e.g., seasons and directions, in Tang poems (Lee & Wong 2012). We present some experiences in analyzing and comparing the contents of the Complete Tang Poem (全唐詩 /quan2 tang2 shi1/, CTP henceforth) and the Complete Song Lyrics (全宋詞 /quan2 song4 ci2/, CSL henceforth) with software tools. We choose CTP and CSL because Tang (618-907AD) and Song (960-1279AD) are arguably the most influential stages in the history of Chinese literature and because poem (詩, /shi1/) and lyrics (詞, /ci2/) are, respectively, the most representative forms of poetry in these dynasties. The influences of the poetry in these dynasties last until today. In addition, we access the Chi"
W16-4023,Y15-2016,1,0.84281,"ntents of poems in Section 4, and, in Section 5, we discuss some interesting findings that we noticed with the help of our tools. We briefly review some challenging issues and make concluding remarks in Section 6. 2 More Background Information Software tools for textual analysis provide ample opportunities for us to study Chinese poetry from a variety of new positions. On comparing the poems of Li Bai (李白) 2 and Du Fu (杜甫), two very famous Tang poets, Jiang (2003) presented his observations from a close-reading viewpoint, and we showed the poets’ differences from a distant-reading standpoint (Liu et al. 2015). Researchers may focus their investigation on a special aspect of CTP, e.g., Pan (2015) introduced his observations about words about plants and flowers in Chinese poetry. We consider that colors portray the scenery that could be delivered by a poem; just like that audio effects drive the atmosphere in a movie. The most frequent color in CTP is white (白 /bai2/). Following this direction, we have reported some findings about poets’ styles and cultural implications that are related to colors (Liu et al. 2015, Cheng et al. 2015). In addition, we found that red (紅 /hong2/) is the most frequent co"
Y11-1052,J06-1003,0,0.0354124,"ynonyms if the cosine value of any of their sense vectors exceeded 0.7. Given an entry in our bilingual dictionary, we compute the near synonyms of the Chinese translations of each English word. This was carried out by comparing the sense vectors of a Chinese translation with 88074 Chinese words in E-HowNet. The qualified words were added to the Chinese translations of the English word in our dictionary. The purpose of adding information about near synonyms into our bilingual dictionary was to increase the recall rates of VN-pair alignment. More complex methods for identifying synonyms, e.g. (Bundanitsky and Hirst, 2006; Chang and Chiou, 2010) will be instrumental for the study. 5 Design of the Experiments We have conducted experiments to translate from English to Chinese and from Chinese to English. In addition, we have tried to find the best translations of verbs, and tried to find the best translations of objects of the verbs given their contexts. Nevertheless, we present only the experiments of translating English verbs to Chinese verbs in this paper. 5.1 Statistics about the Aligned VN pairs We calculated the frequencies of the verbs in the 35811 aligned VN pairs, and ranked the verbs based on the obser"
Y11-1052,I05-7001,0,0.191939,"Missing"
Y11-1052,N03-1017,0,0.00832293,"ese translations of the English objects. We explored the issue with thousands of samples that we extracted from 2011 NTCIR PatentMT workshop. The results indicated that, when the English verbs and objects were known, the information about the object’s Chinese translation could still improve the quality of the verb’s translations but not quite significantly. Keywords: Machine Translation, Feature Comparison 1 Introduction Researchers have studied extensively the problems related to verbs (e.g., Dorr et al., 2002; Lapata and Brew, 2004) and phrases-based translations (e.g., Chuang et al., 2005; Koehn et al., 2003). Some techniques were developed for text of special domains (Seneff et al., 2006). The techniques are applicable in many real-world problems, including computer-assisted language learning (Chang et al., 2008) and cross-language information retrieval (Chen et al., 2000). We work on the processing of patent documents (Lu et al., 2010; Yokoama and Okuyama, 2009), and present an experience in translating common verbs and their direct object based on bilingual contextual information. In this study, we took an extreme assumption of the availability of the Chinese translations of the English objects"
Y11-1052,J04-1003,0,0.165671,"s. In this study, we took an extreme venue – assuming the availability of the Chinese translations of the English objects. We explored the issue with thousands of samples that we extracted from 2011 NTCIR PatentMT workshop. The results indicated that, when the English verbs and objects were known, the information about the object’s Chinese translation could still improve the quality of the verb’s translations but not quite significantly. Keywords: Machine Translation, Feature Comparison 1 Introduction Researchers have studied extensively the problems related to verbs (e.g., Dorr et al., 2002; Lapata and Brew, 2004) and phrases-based translations (e.g., Chuang et al., 2005; Koehn et al., 2003). Some techniques were developed for text of special domains (Seneff et al., 2006). The techniques are applicable in many real-world problems, including computer-assisted language learning (Chang et al., 2008) and cross-language information retrieval (Chen et al., 2000). We work on the processing of patent documents (Lu et al., 2010; Yokoama and Okuyama, 2009), and present an experience in translating common verbs and their direct object based on bilingual contextual information. In this study, we took an extreme as"
Y11-1052,W10-4110,0,0.138097,"quite significantly. Keywords: Machine Translation, Feature Comparison 1 Introduction Researchers have studied extensively the problems related to verbs (e.g., Dorr et al., 2002; Lapata and Brew, 2004) and phrases-based translations (e.g., Chuang et al., 2005; Koehn et al., 2003). Some techniques were developed for text of special domains (Seneff et al., 2006). The techniques are applicable in many real-world problems, including computer-assisted language learning (Chang et al., 2008) and cross-language information retrieval (Chen et al., 2000). We work on the processing of patent documents (Lu et al., 2010; Yokoama and Okuyama, 2009), and present an experience in translating common verbs and their direct object based on bilingual contextual information. In this study, we took an extreme assumption of the availability of the Chinese translations of the English objects to examine whether the extra information will improve the quality of verbs’ translation. The proposed methods are special in that we are crossing the boundary between translation models and language models, by considering information of the target language in the translation task. The purpose of conducting such experiments was to i"
Y11-1052,ma-2006-champollion,0,0.112129,"Missing"
Y11-1052,2006.amta-papers.24,0,0.363166,"samples that we extracted from 2011 NTCIR PatentMT workshop. The results indicated that, when the English verbs and objects were known, the information about the object’s Chinese translation could still improve the quality of the verb’s translations but not quite significantly. Keywords: Machine Translation, Feature Comparison 1 Introduction Researchers have studied extensively the problems related to verbs (e.g., Dorr et al., 2002; Lapata and Brew, 2004) and phrases-based translations (e.g., Chuang et al., 2005; Koehn et al., 2003). Some techniques were developed for text of special domains (Seneff et al., 2006). The techniques are applicable in many real-world problems, including computer-assisted language learning (Chang et al., 2008) and cross-language information retrieval (Chen et al., 2000). We work on the processing of patent documents (Lu et al., 2010; Yokoama and Okuyama, 2009), and present an experience in translating common verbs and their direct object based on bilingual contextual information. In this study, we took an extreme assumption of the availability of the Chinese translations of the English objects to examine whether the extra information will improve the quality of verbs’ trans"
Y11-1052,O09-1007,1,0.413267,"Missing"
Y11-1052,2009.mtsummit-wpt.5,0,0.472662,"tly. Keywords: Machine Translation, Feature Comparison 1 Introduction Researchers have studied extensively the problems related to verbs (e.g., Dorr et al., 2002; Lapata and Brew, 2004) and phrases-based translations (e.g., Chuang et al., 2005; Koehn et al., 2003). Some techniques were developed for text of special domains (Seneff et al., 2006). The techniques are applicable in many real-world problems, including computer-assisted language learning (Chang et al., 2008) and cross-language information retrieval (Chen et al., 2000). We work on the processing of patent documents (Lu et al., 2010; Yokoama and Okuyama, 2009), and present an experience in translating common verbs and their direct object based on bilingual contextual information. In this study, we took an extreme assumption of the availability of the Chinese translations of the English objects to examine whether the extra information will improve the quality of verbs’ translation. The proposed methods are special in that we are crossing the boundary between translation models and language models, by considering information of the target language in the translation task. The purpose of conducting such experiments was to investigate how the availabil"
Y11-1052,O04-1027,0,\N,Missing
Y11-1052,O05-4002,0,\N,Missing
Y11-1052,Y09-2022,1,\N,Missing
Y11-1065,J93-1005,0,0.0595604,"25th Pacific Asia Conference on Language, Information and Computation, pages 607–614 607 (2) And he soon became aware that the government was able to show a flow of millions of dollars in illicit funds into his account. V NP1 Prep NP2 Here the head of NP1 (a flow) is to be interpreted along with into and NP2 (his account), rather than with the preceding verb (to show). Computational linguists have found these two structures causing parsing problems in natural language processing (NLP) and referred to this problem of determining the site of PP to be attached as the PP attachment problem (e.g., Hindle and Rooth, 1993; Volk, 2006). As illustrated in (1) and (2), this problem is conventionally formalized as a binary choice (Merlo and Ferrer, 2005), either verb-attached for (1) or noun-attached for (2). In the minimalist syntax, ternary structures like (3a) are to be transformed by deriving an explicit causative construction (3b) (Radford, 2004). The operation involves raising of the verb roll to join the causative verb made to adhere to a binary operation. (3a) He rolled the ball down the hill. V NP1 Prep NP2 (3b) He made + roll the ball (roll) down the hill. V-causative + V NP1 trace Prep NP2 (Radford, 200"
Y11-1065,S07-1005,0,0.0257056,"Missing"
Y11-1065,J06-3002,0,0.0661102,"Missing"
Y11-1065,W06-2112,0,0.0234833,"Missing"
Y15-1011,I11-1049,0,0.0135347,"ountered Chinese patterns that are quite different from the ones that we need to handle in DFZ. Previous works for inducing grammars of literary Chinese employed some forms of pre-existing information to begin the induction procedures. Given that literary Chinese texts consisted of just long sequences of characters, the needs for external information for grammar induction should be expected. Hwa (1999) assumed that the training corpus was partially annotated with high-level syntactic labels. Lü et al. (2002) started with bilingual corpora. Yu et al. (2010) embarked with a sample treebank, and Boonkwan and Steedman (2011) began with some syntactic prototypes. We tackle the NER tasks in literary Chinese from two unexplored perspectives. First, we employ the biographical information in the China Biographical Database (CBDB, henceforth) to annotate the DFZ texts, learn language models (LMs, henceforth) from the annotated texts, and extract biographical information based on the learned models. Alternatively, we train conditional-random-field (Sutton and McCallum, 2011) models with a set of labeled DFZ data that were achieved in (Bol et al., 2012; 88 Data Sources, Problem Definitions, and Motivation We provide info"
Y15-1011,J05-4005,0,0.0252238,"a general name for a large number of local gazetteers that were compiled by local governments of different levels in China since as early as the 6th century AD (cf. Hargett, 1996). DFZ contain a wide range of information about their host locations, and the biographical information about the government officers is our current focus. The main barrier for achieving our goals is that there is little completed work in the literature about the grammars for literary Chinese, while grammars are central for extracting named entities like person and location names from texts with computational methods (Gao et al., 2005; Nadeau and Sekine, 2007). Figure 1 shows the image of a sample DFZ page. In the old days, Chinese texts were written from top to bottom and from right to left on a page. Most linguists know that there are no word boundaries in modern Chinese. It might be quite surprising for researchers outside of the Chinese community that there were even no punctuations in literary Chinese. Without clear delimiters between words and sentences, it is very challenging even for people to read literary Chinese, so it takes a serious research to find ways not just for segmenting words but also for splitting sen"
Y15-1011,W10-4103,0,0.021631,"Figure 1 shows the image of a sample DFZ page. In the old days, Chinese texts were written from top to bottom and from right to left on a page. Most linguists know that there are no word boundaries in modern Chinese. It might be quite surprising for researchers outside of the Chinese community that there were even no punctuations in literary Chinese. Without clear delimiters between words and sentences, it is very challenging even for people to read literary Chinese, so it takes a serious research to find ways not just for segmenting words but also for splitting sentences in literary Chinese (Huang et al., 2010). Grammar induction (de la Higuera, 2005) is a general name for enabling computers to learn the grammars of natural languages. Some researchers worked on the grammars for selected sources of Chinese. Huang et al. (2001) ex87 29th Pacific Asia Conference on Language, Information and Computation pages 87 - 95 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Chao-Lin Liu, Chih-Kai Huang, Hongsu Wang and Peter K. Bol PACLIC 29 Pang et al., 2014), and use the conditional-random-field (CRF, henceforth) models to extract candidate names from the test data, which is another set of DFZ"
Y15-1011,P99-1010,0,0.0234312,". Kuo (2009) tried to find phrase-structure rules for modern Chinese texts, and Lee and Kong (2012) built treebanks for Tang poems. Although these researchers worked on grammars for Chinese texts, they encountered Chinese patterns that are quite different from the ones that we need to handle in DFZ. Previous works for inducing grammars of literary Chinese employed some forms of pre-existing information to begin the induction procedures. Given that literary Chinese texts consisted of just long sequences of characters, the needs for external information for grammar induction should be expected. Hwa (1999) assumed that the training corpus was partially annotated with high-level syntactic labels. Lü et al. (2002) started with bilingual corpora. Yu et al. (2010) embarked with a sample treebank, and Boonkwan and Steedman (2011) began with some syntactic prototypes. We tackle the NER tasks in literary Chinese from two unexplored perspectives. First, we employ the biographical information in the China Biographical Database (CBDB, henceforth) to annotate the DFZ texts, learn language models (LMs, henceforth) from the annotated texts, and extract biographical information based on the learned models. A"
Y15-1011,C02-1003,0,0.0561933,"ilt treebanks for Tang poems. Although these researchers worked on grammars for Chinese texts, they encountered Chinese patterns that are quite different from the ones that we need to handle in DFZ. Previous works for inducing grammars of literary Chinese employed some forms of pre-existing information to begin the induction procedures. Given that literary Chinese texts consisted of just long sequences of characters, the needs for external information for grammar induction should be expected. Hwa (1999) assumed that the training corpus was partially annotated with high-level syntactic labels. Lü et al. (2002) started with bilingual corpora. Yu et al. (2010) embarked with a sample treebank, and Boonkwan and Steedman (2011) began with some syntactic prototypes. We tackle the NER tasks in literary Chinese from two unexplored perspectives. First, we employ the biographical information in the China Biographical Database (CBDB, henceforth) to annotate the DFZ texts, learn language models (LMs, henceforth) from the annotated texts, and extract biographical information based on the learned models. Alternatively, we train conditional-random-field (Sutton and McCallum, 2011) models with a set of labeled DFZ"
Y15-1011,P95-1026,0,0.265078,"ferring longer words is very common for Chinese word segmentation. In T1, both “中書省都事” (zhong1 shu1 sheng3 dou1 shi4) and “中書省” can be labeled as office names in the Yuan dynasty, but we would choose the former because “中書省都事” is a longer string. In contrast, we do not have “中書 省都事” for the Ming dynasty, so will use “中書” and “都事” for Ming. We also assume that named entities in a passage should be consistent in some senses, as another heuristic principle for disambiguation. This consistent principle should be reminiscent of the “one sense per discourse” principle for word sense disambiguation (Yarowsky, 1995). 90 Currently, we presume that named entities in a context of six labels should be referring to something of the same dynasty, where six is an arbitrary choice and can be varied. We have not used addresses to check consistency because we are still expanding our list of addresses. Therefore, we do not accept a “陳瑜” of the Qing dynasty because neither “中書省都事” nor “中 書” is an entity in Qing. Using the consistent principle, we will keep labels only for the Song and Ming dynasties for the sample passage, thereby achieving some disambiguation effects. Hence, we have two consistent sequences: [name("
Y15-1011,C10-2162,0,0.0371377,"Missing"
Y15-1011,W02-1806,0,\N,Missing
Y15-2016,I05-7003,0,0.0408809,"the contents of CTP for a wide variety of explorations. Lo and her colleagues pioneered to handle texts of Chinese classical poetry with computer software (Lo et al., 1997). Hu and Yu (2001) achieved a better environment and demonstrated its functions with a temporal analysis of selected Chinese unigrams, i.e., 愁(chou2), 苦(ku3), 恨 (hen4), 悲(bei1), 哀(ai1), and 憂(you1). Jiang (2003) employed tools for information retrieval to find and study selected poems of Li Bai and Du Fu that mentioned “wind” and “moon”. Huang (2004) analyzed the ontology in Su Shi’s (蘇軾) poems based on 300 Tang Poems, and Chang et al. (2005) continued this line of work. Lo then built a more complete taxonomy for Tang poems (Lo, 2008; Fang et al., 2009). Lee conduct part-of-speech analysis of CTP (Lee, 2012) and dependency trees (Lee and Kong, 2012). They also explored the roles of a variety of named entities, e.g., seasons, directions, and colors, in CTP (Lee and Wong, 2012), and used their analysis of CTP for teaching computational linguistics (Lee et al., 2013). CTP can serve as the bases of other innovative applications. Zhao and his colleagues have created a website 4 for suggesting couplets, which was accomplished partially"
Y15-2016,N12-1020,0,0.0145704,"nvironment and demonstrated its functions with a temporal analysis of selected Chinese unigrams, i.e., 愁(chou2), 苦(ku3), 恨 (hen4), 悲(bei1), 哀(ai1), and 憂(you1). Jiang (2003) employed tools for information retrieval to find and study selected poems of Li Bai and Du Fu that mentioned “wind” and “moon”. Huang (2004) analyzed the ontology in Su Shi’s (蘇軾) poems based on 300 Tang Poems, and Chang et al. (2005) continued this line of work. Lo then built a more complete taxonomy for Tang poems (Lo, 2008; Fang et al., 2009). Lee conduct part-of-speech analysis of CTP (Lee, 2012) and dependency trees (Lee and Kong, 2012). They also explored the roles of a variety of named entities, e.g., seasons, directions, and colors, in CTP (Lee and Wong, 2012), and used their analysis of CTP for teaching computational linguistics (Lee et al., 2013). CTP can serve as the bases of other innovative applications. Zhao and his colleagues have created a website 4 for suggesting couplets, which was accomplished partially based on their analysis of the CTP (Jiang and Zhou 2008; Zhou et al., 2009). Voigt and Jarafsky (2013) considered CTP when they compared ancient and modern verses of China and Taiwan. Our work is special in that"
Y15-2016,C12-2061,0,0.348168,"the main focus because it is the most frequent color in CTP. We also explore some cases of using colored words in antithesis(對仗) 3 pairs that were central for fostering the imageries of the poems. CTP also contains useful historical information, and we extract person names in CTP to study the social networks of the Tang poets. Such information can then be integrated with the China Biographical Database of Harvard University. 1 Introduction Complete Tang Poems (CTP) is the single most important collection for studying Tang poems from the literary and linguistic perspectives (Fang et al., 2009; Lee and Wong, 2012). CTP 1 A majority of the contents of this paper was also published in Chinese in (Liu et al., 2015). 2 Romanized Chinese names are in the order of surname and first name, following the request of a reviewer. 3 “Antithesis” is not a perfect translation of “對仗” (dui4 zhang4). Roughly speaking, “對仗” refers to constrained collocations, and requires two terms to have opposite relationships in pronunciations, but does not demand the terms to be opposite in meanings. In English, “antithesis” carries a rather obvious demand for two referred terms to be opposite in meanings. was officially compiled du"
Y15-2016,W09-4104,0,0.0719605,"” (bai2, white) is the main focus because it is the most frequent color in CTP. We also explore some cases of using colored words in antithesis(對仗) 3 pairs that were central for fostering the imageries of the poems. CTP also contains useful historical information, and we extract person names in CTP to study the social networks of the Tang poets. Such information can then be integrated with the China Biographical Database of Harvard University. 1 Introduction Complete Tang Poems (CTP) is the single most important collection for studying Tang poems from the literary and linguistic perspectives (Fang et al., 2009; Lee and Wong, 2012). CTP 1 A majority of the contents of this paper was also published in Chinese in (Liu et al., 2015). 2 Romanized Chinese names are in the order of surname and first name, following the request of a reviewer. 3 “Antithesis” is not a perfect translation of “對仗” (dui4 zhang4). Roughly speaking, “對仗” refers to constrained collocations, and requires two terms to have opposite relationships in pronunciations, but does not demand the terms to be opposite in meanings. In English, “antithesis” carries a rather obvious demand for two referred terms to be opposite in meanings. was o"
Y15-2016,C08-1048,0,0.0441881,"Missing"
Y15-2016,W12-1011,0,0.0216063,"and Yu (2001) achieved a better environment and demonstrated its functions with a temporal analysis of selected Chinese unigrams, i.e., 愁(chou2), 苦(ku3), 恨 (hen4), 悲(bei1), 哀(ai1), and 憂(you1). Jiang (2003) employed tools for information retrieval to find and study selected poems of Li Bai and Du Fu that mentioned “wind” and “moon”. Huang (2004) analyzed the ontology in Su Shi’s (蘇軾) poems based on 300 Tang Poems, and Chang et al. (2005) continued this line of work. Lo then built a more complete taxonomy for Tang poems (Lo, 2008; Fang et al., 2009). Lee conduct part-of-speech analysis of CTP (Lee, 2012) and dependency trees (Lee and Kong, 2012). They also explored the roles of a variety of named entities, e.g., seasons, directions, and colors, in CTP (Lee and Wong, 2012), and used their analysis of CTP for teaching computational linguistics (Lee et al., 2013). CTP can serve as the bases of other innovative applications. Zhao and his colleagues have created a website 4 for suggesting couplets, which was accomplished partially based on their analysis of the CTP (Jiang and Zhou 2008; Zhou et al., 2009). Voigt and Jarafsky (2013) considered CTP when they compared ancient and modern verses of Chi"
Y15-2016,P98-2127,0,0.552065,"Missing"
Y15-2016,Y04-1003,0,\N,Missing
Y15-2016,C98-2122,0,\N,Missing
Y15-2016,W13-1403,0,\N,Missing
