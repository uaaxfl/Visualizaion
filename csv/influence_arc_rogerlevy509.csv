2020.acl-demos.10,D19-1592,0,0.132669,"Missing"
2020.acl-demos.10,P19-1285,0,0.068298,"Missing"
2020.acl-demos.10,N16-1024,0,0.0768916,"Missing"
2020.acl-demos.10,N19-1004,1,0.895935,"Missing"
2020.acl-demos.10,2020.tacl-1.25,0,0.301762,"aying results through interactive visualizations. Language scientists can use the site to design and submit targeted syntactic evaluations, testing whether language models have derived human-like syntactic knowledge. IndepenIntroduction Recent work in evaluating neural network language models focuses on investigating models’ fine-grained prediction behavior on carefully designed examples. Unlike broad-coverage language modeling metrics such as perplexity, these evaluations are targeted to reveal whether models have learned specific knowledge about the syntactic structure of language (see e.g. Warstadt et al., 2020; Futrell et al., 2019; Marvin and Linzen, 2018). Research in this line of work requires an uncommon intersection of skills: a) the engineering strength of NLP researchers necessary to train and 70 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 70–76 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 2.2 dently, NLP experts can submit their own language models for evaluation on these assays. By separating the tasks performed by these two user groups, the SyntaxGym site lowers the barrier to entry for the broader community of"
2020.acl-demos.10,N18-1108,0,0.0251441,"aken together, these tools create a platform that makes the process of targeted syntactic evaluation more standardized, reproducible, and accessible to the broader communities of NLP experts and language scientists. Our goal is for SyntaxGym to facilitate the advancement of language model evaluation, leading to the development of models with more human-like linguistic knowledge. 2 Alternatively, a language model can be evaluated on its ability to make human-like generalizations for specific syntactic phenomena. The targeted syntactic evaluation paradigm (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019; Warstadt et al., 2020) incorporates methods from psycholinguistic experiments, designing sentences which hold most lexical and syntactic features of each sentence constant while minimally varying features that determine grammaticality or surprise characteristics of the sentence. For example, the following minimalpair sentences differ in subject–verb agreement: (1) (2) 3 Perplexity Standard left-to-right language models are trained to predict the next token given a context of previous tokens. Language models are typically assessed by their perple"
2020.acl-demos.10,P82-1020,0,0.801638,"Missing"
2020.acl-demos.10,2020.acl-main.158,1,0.880764,"e first sentence than to the ungrammatical singular verb in the second (Linzen et al., 2016). Before presenting the SyntaxGym framework, we briefly introduce the targeted syntactic evaluation paradigm as a way to assess the quality of neural language models. 2.1 Targeted tests for syntactic generalization 3.1 (1) However, a broad-coverage metric such as perplexity may not be ideal for assessing whether a language model has human-like syntactic knowledge. Recent empirical results suggest that models with similar perplexity measures can still exhibit substantial variance in syntactic knowledge (Hu et al., 2020; van Schijndel et al., 2019), according to evaluation paradigms described in the next section. Standardizing targeted syntactic evaluation We represent targeted syntactic evaluations as test suites, visualized in Figure 2. These test suites are the core component of psycholinguistic assessment, and should be familiar to those experienced in psycholinguistic experimental design. We will present the structure of a test suite using the running example of subject–verb agreement, introduced in the previous section. We describe the components of a test suite from bottom-up: 71 Regions Condition int"
2020.acl-demos.10,Q16-1037,0,0.323394,"rationalized in a second tool lm-zoo. Taken together, these tools create a platform that makes the process of targeted syntactic evaluation more standardized, reproducible, and accessible to the broader communities of NLP experts and language scientists. Our goal is for SyntaxGym to facilitate the advancement of language model evaluation, leading to the development of models with more human-like linguistic knowledge. 2 Alternatively, a language model can be evaluated on its ability to make human-like generalizations for specific syntactic phenomena. The targeted syntactic evaluation paradigm (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019; Warstadt et al., 2020) incorporates methods from psycholinguistic experiments, designing sentences which hold most lexical and syntactic features of each sentence constant while minimally varying features that determine grammaticality or surprise characteristics of the sentence. For example, the following minimalpair sentences differ in subject–verb agreement: (1) (2) 3 Perplexity Standard left-to-right language models are trained to predict the next token given a context of previous tokens. Language mod"
2020.acl-demos.10,D18-1151,0,0.572796,"ons. Language scientists can use the site to design and submit targeted syntactic evaluations, testing whether language models have derived human-like syntactic knowledge. IndepenIntroduction Recent work in evaluating neural network language models focuses on investigating models’ fine-grained prediction behavior on carefully designed examples. Unlike broad-coverage language modeling metrics such as perplexity, these evaluations are targeted to reveal whether models have learned specific knowledge about the syntactic structure of language (see e.g. Warstadt et al., 2020; Futrell et al., 2019; Marvin and Linzen, 2018). Research in this line of work requires an uncommon intersection of skills: a) the engineering strength of NLP researchers necessary to train and 70 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 70–76 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 2.2 dently, NLP experts can submit their own language models for evaluation on these assays. By separating the tasks performed by these two user groups, the SyntaxGym site lowers the barrier to entry for the broader community of language researchers. While SyntaxGym will serv"
2020.acl-main.158,P04-3031,0,0.208884,"roper subset of each larger corpus. We call these four corpora BLLIP- XS (40K sentences, 1M tokens); BLLIP- SM (200K sentences, 5M tokens); BLLIPMD (600K sentences, 14M tokens); and BLLIP- LG (2M sentences, 42M tokens). Table 1 summarizes statistics of the training set for each corpus. To ensure consistency in perplexity evaluation across datasets, we report perplexity scores achieved by the models on a shared held-out test set. We additionally use a shared held-out validation for tuning and early stopping. We use the NLTK implementation of the Penn Treebank tokenizer to process all datasets (Bird and Loper, 2004; Marcus et al., 1993). 1728 LSTM ON-LSTM RNNG GPT-2 # layers # hidden units Embedding size 2 3 2 12 256 1150 256 768 256 400 256 768 BLLIP sizes: Table 2: Size of neural models in our controlled experiments. BLLIP sizes: LSTM ON-LSTM RNNG GPT-2 XS SM MD LG 13.4M 30.8M 22.8M 124.4M 30.5M 44.2M 48.4M 124.4M 52.2M 61.2M 81.1M 124.4M 88.1M 89.2M 134.9M 124.4M Table 3: Parameter counts for neural models in our controlled experiments. Out-of-vocabulary tokens For each corpus, we designate a token as OOV if the token appears fewer than two times in the training set. Our larger training datasets thus"
2020.acl-main.158,2020.scil-1.1,0,0.0290966,"Missing"
2020.acl-main.158,C18-1012,0,0.0879253,"Missing"
2020.acl-main.158,P19-1285,0,0.110458,"Missing"
2020.acl-main.158,N16-1024,0,0.0581949,"yperparameters reported in the papers introducing each model (Table 2).4 The full parameter counts and perplexity scores for each model × corpus combination are given in Tables 3 and 4, respectively. LSTM Our baseline neural model is a vanilla long short-term memory network (LSTM; Hochreiter and Schmidhuber, 1997) based on the boilerplate PyTorch implementation (Paszke et al., 2017). Ordered-Neurons We consider the OrderedNeurons LSTM architecture (ON-LSTM; Shen et al., 2019), which encodes an explicit bias towards modeling hierarchical structure. RNNG Recurrent neural network grammars (RNNG; Dyer et al., 2016) model the joint probability of a sequence of words and its syntactic structure. RNNG requires labeled trees that contain complete constituency parses, which we produce for BLLIP sentences with an off-the-shelf constituency parser (Kitaev and Klein, 2018).5 To compute surprisals from RNNG, we use wordsynchronous beam search (Stern et al., 2017) to approximate the conditional probability of the current word given the context. 4 Due to computational constraints, we performed only minimal tuning past these recommended hyperparameters. 5 While the BLLIP corpus already contains Treebank-style parse"
2020.acl-main.158,W18-5426,0,0.0605239,"capacities. In addition to the insight these results provide about neural NLP systems, they also bear on questions central to cognitive science and linguistics, putting lower bounds on what syntactic knowledge can be acquired from string input alone. Targeted syntactic evaluation is just one in a series of complementary methods being developed to assess the learning outcomes of neural language processing models. Other methods include classifying sentences as grammatical or ungrammatical (Warstadt et al., 2019b), decoding syntactic features from a model’s internal state (Belinkov et al., 2017; Giulianelli et al., 2018), or transfer learning to a strictly syntactic task such as parsing or POS tagging (Hewitt and Manning, 2019). As each task brings an explicit set of assumptions, complementary assessment methods can collectively provide greater insight into models’ learning outcomes. Although this paper, together with Warstadt et al. (2020), report what is to our knowledge the largestscale targeted syntactic evaluations to date, we emphasize that they are only first steps toward a comprehensive understanding of the syntactic capabilities of contemporary language models. This understanding will be further adva"
2020.acl-main.158,P18-1249,0,0.0282262,"hort-term memory network (LSTM; Hochreiter and Schmidhuber, 1997) based on the boilerplate PyTorch implementation (Paszke et al., 2017). Ordered-Neurons We consider the OrderedNeurons LSTM architecture (ON-LSTM; Shen et al., 2019), which encodes an explicit bias towards modeling hierarchical structure. RNNG Recurrent neural network grammars (RNNG; Dyer et al., 2016) model the joint probability of a sequence of words and its syntactic structure. RNNG requires labeled trees that contain complete constituency parses, which we produce for BLLIP sentences with an off-the-shelf constituency parser (Kitaev and Klein, 2018).5 To compute surprisals from RNNG, we use wordsynchronous beam search (Stern et al., 2017) to approximate the conditional probability of the current word given the context. 4 Due to computational constraints, we performed only minimal tuning past these recommended hyperparameters. 5 While the BLLIP corpus already contains Treebank-style parses, we strip the terminals and re-parse in order to obtain more accurate, up-to-date syntactic parses. 1729 GPT2XL * 1.0 SG score 0.8 0.6 0.7 0.4 0.2 GPT GPT Tran JRNN *GPT GRN RNN ONL LSTM ngra 2XL * 2 * sform 2 N* G m STM erXL * Model Figure 1:"
2020.acl-main.158,Q16-1037,0,0.320989,"n-theoretic metrics, such as perplexity, as well as targeted linguistic evaluation. Benchmarks such as GLUE (Wang et al., 2019a,b) have demonstrated that neural language models trained on naturalistic corpora for next-word prediction learn representations that can yield remarkable performance on many semantic tasks. Targeted syntactic evaluations have shown that these models also implicitly capture many syntactic generalizations, ranging from subject–verb agreement Materials and code can be found at https://github. com/cpllab/syntactic-generalization. to long-distance filler–gap dependencies (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2019b). This paper aims to bring targeted evaluations of syntactic performance to scale, complementing similar developments in semantic evaluation (McCoy et al., 2019). Because the most widespread currency of evaluation for language models is perplexity—how well, on average, a model predicts a word in its context— a primary focus of this paper is the relationship between a model’s perplexity and its performance on targeted syntactic evaluations. As perplexity improves, can we expect more human-like syntactic generalization? How do"
2020.acl-main.158,J93-2004,0,0.0696311,"arger corpus. We call these four corpora BLLIP- XS (40K sentences, 1M tokens); BLLIP- SM (200K sentences, 5M tokens); BLLIPMD (600K sentences, 14M tokens); and BLLIP- LG (2M sentences, 42M tokens). Table 1 summarizes statistics of the training set for each corpus. To ensure consistency in perplexity evaluation across datasets, we report perplexity scores achieved by the models on a shared held-out test set. We additionally use a shared held-out validation for tuning and early stopping. We use the NLTK implementation of the Penn Treebank tokenizer to process all datasets (Bird and Loper, 2004; Marcus et al., 1993). 1728 LSTM ON-LSTM RNNG GPT-2 # layers # hidden units Embedding size 2 3 2 12 256 1150 256 768 256 400 256 768 BLLIP sizes: Table 2: Size of neural models in our controlled experiments. BLLIP sizes: LSTM ON-LSTM RNNG GPT-2 XS SM MD LG 13.4M 30.8M 22.8M 124.4M 30.5M 44.2M 48.4M 124.4M 52.2M 61.2M 81.1M 124.4M 88.1M 89.2M 134.9M 124.4M Table 3: Parameter counts for neural models in our controlled experiments. Out-of-vocabulary tokens For each corpus, we designate a token as OOV if the token appears fewer than two times in the training set. Our larger training datasets thus contain larger vocabu"
2020.acl-main.158,D18-1151,0,0.300501,"such as perplexity, as well as targeted linguistic evaluation. Benchmarks such as GLUE (Wang et al., 2019a,b) have demonstrated that neural language models trained on naturalistic corpora for next-word prediction learn representations that can yield remarkable performance on many semantic tasks. Targeted syntactic evaluations have shown that these models also implicitly capture many syntactic generalizations, ranging from subject–verb agreement Materials and code can be found at https://github. com/cpllab/syntactic-generalization. to long-distance filler–gap dependencies (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2019b). This paper aims to bring targeted evaluations of syntactic performance to scale, complementing similar developments in semantic evaluation (McCoy et al., 2019). Because the most widespread currency of evaluation for language models is perplexity—how well, on average, a model predicts a word in its context— a primary focus of this paper is the relationship between a model’s perplexity and its performance on targeted syntactic evaluations. As perplexity improves, can we expect more human-like syntactic generalization? How do training dataset size an"
2020.acl-main.158,D19-1592,0,0.123129,"Missing"
2020.acl-main.158,P16-1162,0,0.0270087,"lity of RNNG to that of ON-LSTM). Some off-the-shelf models, such as GPT-2-XL, perform near ceiling on the original five test suites and are not affected at all by intervening content. 0.6 0.4 0.2 0.0 GPT GPT Tran JRNN *GPT GRN RNN ONL LSTM ngra 2XL * 2 * sform 2 N* G m STM erXL * Model Figure 6: SG score on the pairs of test suites with and without intervening modifiers: Center Embedding, Cleft, MVRR, NPZ-Ambiguous, and NPZ-Object. 1732 4.5 Effects of model pre-processing The GPT-2 models trained and evaluated in this paper use a sub-word vocabulary learned by byte-pair encoding (BPE; Sennrich et al., 2016) to represent their inputs, while all other models represent and compute over word-level inputs. This byte-pair encoding was taken from the pre-trained GPT-2 model trained on a much larger corpus. The results reported for these models thus conflate a choice of model class (a deep Transformer architecture) and preprocessing standard (sub-word tokenization computed on a larger corpus). Some preliminary work suggests that sub-word tokenization is indeed responsible for much of the larger GPT-2 models’ success: we find that GPT-2 models trained on word-level representations of BLLIP- LG and BLLIP-"
2020.acl-main.158,2020.tacl-1.25,0,0.360508,"Missing"
2020.acl-main.158,D17-1178,0,0.0367013,"rch implementation (Paszke et al., 2017). Ordered-Neurons We consider the OrderedNeurons LSTM architecture (ON-LSTM; Shen et al., 2019), which encodes an explicit bias towards modeling hierarchical structure. RNNG Recurrent neural network grammars (RNNG; Dyer et al., 2016) model the joint probability of a sequence of words and its syntactic structure. RNNG requires labeled trees that contain complete constituency parses, which we produce for BLLIP sentences with an off-the-shelf constituency parser (Kitaev and Klein, 2018).5 To compute surprisals from RNNG, we use wordsynchronous beam search (Stern et al., 2017) to approximate the conditional probability of the current word given the context. 4 Due to computational constraints, we performed only minimal tuning past these recommended hyperparameters. 5 While the BLLIP corpus already contains Treebank-style parses, we strip the terminals and re-parse in order to obtain more accurate, up-to-date syntactic parses. 1729 GPT2XL * 1.0 SG score 0.8 0.6 0.7 0.4 0.2 GPT GPT Tran JRNN *GPT GRN RNN ONL LSTM ngra 2XL * 2 * sform 2 N* G m STM erXL * Model Figure 1: Average SG score by model class. Asterisks denote off-the-shelf models. Error bars denote b"
2020.acl-main.158,Q19-1040,0,0.122126,") found should be more probable in C than A; (iii) the C–D surprisal difference should be smaller than the A–B surprisal difference—a 2 × 2 interaction effect on surprisal—because the syntactic disambiguation effect of not reducing the relative clause was achieved by using a part-of-speech unambiguous verb. We will use these controlled tests to help us describe and test for human-like syntactic knowledge in language models. 2.3 Related work The testing paradigm presented here differs in several crucial ways from recent, related syntactic assessments and provides complementary insights. Unlike Warstadt et al. (2019a), our approach does not involve fine-tuning, but rather assesses what syntactic knowledge is induced from the language modeling objective alone. The most closely related work is the Benchmark of Linguistic Minimal Pairs (Warstadt et al., 2020), which is a challenge set of automatically-generated sentence pairs also designed to test language models on a large set of syntactic phenomena. Our approach differs in important ways: we compare critical sentence regions instead of full-sentence probabilities, and employ a 2 × 2 paradigm with a strict, multi-fold success criterion inspired by psycholi"
2020.acl-main.158,W19-4819,1,0.942412,"tic evaluation. Benchmarks such as GLUE (Wang et al., 2019a,b) have demonstrated that neural language models trained on naturalistic corpora for next-word prediction learn representations that can yield remarkable performance on many semantic tasks. Targeted syntactic evaluations have shown that these models also implicitly capture many syntactic generalizations, ranging from subject–verb agreement Materials and code can be found at https://github. com/cpllab/syntactic-generalization. to long-distance filler–gap dependencies (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2019b). This paper aims to bring targeted evaluations of syntactic performance to scale, complementing similar developments in semantic evaluation (McCoy et al., 2019). Because the most widespread currency of evaluation for language models is perplexity—how well, on average, a model predicts a word in its context— a primary focus of this paper is the relationship between a model’s perplexity and its performance on targeted syntactic evaluations. As perplexity improves, can we expect more human-like syntactic generalization? How do training dataset size and model architecture jointly affect syntact"
2020.acl-main.158,W18-5423,1,0.856492,"r gross syntactic expectation include Subordination (4 suites) from Futrell et al. (2018). • Center Embedding sentences are sentences recursively nested within each other. Subject and verbs must match in a first-in-last-out order, meaning models must approximate a stack-like data-structure in order to successfully process them. Our 2 suites of Center Embedding sentences come from the items presented in Wilcox et al. (2019a). • Long-Distance Dependencies are covariations between two tokens that span long distances in tree depth. Test suites include Filler-Gap Dependencies (FGD) (6 suites) from Wilcox et al. (2018) and Wilcox et al. (2019b), and 2 novel Cleft suites, described in detail below. Novel test suite: Cleft We introduce one novel test suite that assesses models’ ability to process pseudo-cleft constructions, which are used to put a particular syntactic constituent into focus via passive transformation. Consider Example (1): BLLIP sizes: # sentences # tokens # non-UNK types # UNK types XS SM MD LG 40K 1M 24K 68 200K 4.8M 57K 70 600K 14M 100K 71 1.8M 42M 170K 74 Table 1: Statistics of training set for each corpus size. (1) a. What he did after coming in from the rain was eat a hot meal. [DO/VP]"
2020.acl-main.158,N19-1334,1,0.947251,"tic evaluation. Benchmarks such as GLUE (Wang et al., 2019a,b) have demonstrated that neural language models trained on naturalistic corpora for next-word prediction learn representations that can yield remarkable performance on many semantic tasks. Targeted syntactic evaluations have shown that these models also implicitly capture many syntactic generalizations, ranging from subject–verb agreement Materials and code can be found at https://github. com/cpllab/syntactic-generalization. to long-distance filler–gap dependencies (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2019b). This paper aims to bring targeted evaluations of syntactic performance to scale, complementing similar developments in semantic evaluation (McCoy et al., 2019). Because the most widespread currency of evaluation for language models is perplexity—how well, on average, a model predicts a word in its context— a primary focus of this paper is the relationship between a model’s perplexity and its performance on targeted syntactic evaluations. As perplexity improves, can we expect more human-like syntactic generalization? How do training dataset size and model architecture jointly affect syntact"
2020.acl-main.158,N07-1051,0,\N,Missing
2020.acl-main.158,N01-1021,0,\N,Missing
2020.acl-main.158,P06-4018,0,\N,Missing
2020.acl-main.158,W12-1706,1,\N,Missing
2020.acl-main.158,P17-1080,0,\N,Missing
2020.acl-main.158,P19-1334,0,\N,Missing
2020.acl-main.158,N19-1004,1,\N,Missing
2020.acl-main.158,N19-1419,0,\N,Missing
2020.acl-main.158,N19-1423,0,\N,Missing
2020.acl-main.158,W18-0102,0,\N,Missing
2020.acl-main.158,2020.scil-1.39,1,\N,Missing
2020.acl-main.507,D16-1203,0,0.0724024,"Missing"
2020.acl-main.507,N18-1040,0,0.0493981,"Missing"
2020.acl-main.507,P16-1223,0,0.115961,"Missing"
2020.acl-main.507,P17-1147,0,0.0206855,"ar’s test). This outcome makes sense in the absence of the paragraph, as while the other answers are constrained by the specifics of the paragraph, D distractors may appeal to general world knowledge and reasoning which can be beyond the capacities of RoBERTa. 5 Related Work A considerable number of reading comprehension datasets have been introduced in NLP. A large fraction of these datasets can be broadly divided into three tasks: Cloze (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016), span identification QA (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) and multiple choice QA (Richardson et al., 2013; Lai et al., 2017). Our approach primarily falls into the third category. The basic 4-answer format we use is identical to RACE (Lai et al., 2017), which enables training models on RACE and evaluating them on OneStopQA. Our dataset is considerably smaller than RACE, but is of appropriate size for robust evaluations and error analyses. As demonstrated in this work, OneStopQA annotations are of substantially higher quality than RACE, and enable analyses which are not possible with RACE. MCTest (Richardson et al., 2013) w"
2020.acl-main.507,N18-1023,0,0.0127002,": spans are not 5733 considered as answers but rather as text regions that contain the critical information for the respective answer. This difference enables a higher difficulty degree and a wider scope of question types. The combination of this approach with a multiple choice answer structure which always has a span misinterpretation distractor facilitates deeper probing of text understanding and is designed to allow for more robustness to simple pattern matching. Prior work has explored both manual and automatic auxiliary span annotations for correct answers in multiple choice QA datasets (Khashabi et al., 2018; Wang et al., 2019). Our framework extends such annotations to include multiple distractor types, with B distractors providing an additional guarantee that simply identifying the critical span is not sufficient for answering the question correctly. We further demonstrate the utility of our distractor structure for automatic verification of annotation quality through ablation experiments, as well as detailed error comparisons between human and machine readers. 6 Discussion We introduce a new annotation framework for reading comprehension and an accompanying highquality dataset. We leverage the"
2020.acl-main.507,Q19-1026,0,0.0872347,"e annotation scheme, the framework provides span annotations for the different answer choices. This approach creates a systematic correspondence between answers and their textual support. Specifically, the correct answer relies on a critical span which contains the 5726 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5726–5735 c July 5 - 10, 2020. 2020 Association for Computational Linguistics essential information for answering the question. In contrast to span identification datasets such as SQUAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019), we do not consider the span as the correct answer, but rather as a text region that contains the critical information required for answering the question correctly. The second answer represents a misunderstanding of that same span. Finally, the information referred to in the third answer is marked in a distractor span. In this paper we demonstrate that the combination of a consistent answer structure with span annotations opens the door for new approaches to automatic verification of annotations and enables new types of analyses for reading comprehension. We further introduce OneStopQA, a ne"
2020.acl-main.507,2021.ccl-1.108,0,0.138255,"Missing"
2020.acl-main.507,D16-1264,0,0.225396,"ations To further enhance the versatility of the annotation scheme, the framework provides span annotations for the different answer choices. This approach creates a systematic correspondence between answers and their textual support. Specifically, the correct answer relies on a critical span which contains the 5726 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5726–5735 c July 5 - 10, 2020. 2020 Association for Computational Linguistics essential information for answering the question. In contrast to span identification datasets such as SQUAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019), we do not consider the span as the correct answer, but rather as a text region that contains the critical information required for answering the question correctly. The second answer represents a misunderstanding of that same span. Finally, the information referred to in the third answer is marked in a distractor span. In this paper we demonstrate that the combination of a consistent answer structure with span annotations opens the door for new approaches to automatic verification of annotations and enables new types of analyses for reading co"
2020.acl-main.507,D13-1020,0,0.298648,"we benchmark two neural reading comprehension models, the Stanford Attentive Reader (AR) (Chen et al., 2016), and RoBERTA (Liu et al., 2019) a state-of-the-art model on RACE. We train the models on RACE, and evaluate their accuracy on RACE and OneStopQA. To reduce the impact of potential domain differences, we also provide an evaluation in which we further finetune the models on OneStopQA with 5-fold cross validation, where in each fold 18 articles are used for training, 6 for development and 6 for testing. Additionally, we report the performance of the commonly used sliding window baseline (Richardson et al., 2013). In parallel with the two neural model evaluation regimes for OneStopQA, we perform two evaluations for this baseline, one in which the window size is optimized on the RACE development set, and one in which it is optimized on OneStopQA using 5-fold cross validation. Table 4 presents the results of this experiment. We observe that the two weaker models, Sliding Window and Stanford AR, perform better on RACE than on OneStopQA. Particularly notable is the large drop in the performance of Stanford AR from 42.8 on RACE to 34.3 on OneStopQA (p  .001, ttest). This suggests that OneStopQA is more ro"
2020.acl-main.507,W17-2623,0,0.02042,"ce for D (p < .05, McNemar’s test). This outcome makes sense in the absence of the paragraph, as while the other answers are constrained by the specifics of the paragraph, D distractors may appeal to general world knowledge and reasoning which can be beyond the capacities of RoBERTa. 5 Related Work A considerable number of reading comprehension datasets have been introduced in NLP. A large fraction of these datasets can be broadly divided into three tasks: Cloze (Hermann et al., 2015; Hill et al., 2015; Bajgar et al., 2016), span identification QA (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Kwiatkowski et al., 2019) and multiple choice QA (Richardson et al., 2013; Lai et al., 2017). Our approach primarily falls into the third category. The basic 4-answer format we use is identical to RACE (Lai et al., 2017), which enables training models on RACE and evaluating them on OneStopQA. Our dataset is considerably smaller than RACE, but is of appropriate size for robust evaluations and error analyses. As demonstrated in this work, OneStopQA annotations are of substantially higher quality than RACE, and enable analyses which are not possible with RACE. MCTest (Richar"
2020.acl-main.507,W18-0535,0,0.233567,"Missing"
2020.acl-main.507,K19-1065,0,0.015359,"nsidered as answers but rather as text regions that contain the critical information for the respective answer. This difference enables a higher difficulty degree and a wider scope of question types. The combination of this approach with a multiple choice answer structure which always has a span misinterpretation distractor facilitates deeper probing of text understanding and is designed to allow for more robustness to simple pattern matching. Prior work has explored both manual and automatic auxiliary span annotations for correct answers in multiple choice QA datasets (Khashabi et al., 2018; Wang et al., 2019). Our framework extends such annotations to include multiple distractor types, with B distractors providing an additional guarantee that simply identifying the critical span is not sufficient for answering the question correctly. We further demonstrate the utility of our distractor structure for automatic verification of annotation quality through ablation experiments, as well as detailed error comparisons between human and machine readers. 6 Discussion We introduce a new annotation framework for reading comprehension and an accompanying highquality dataset. We leverage the novel structure of"
2020.acl-main.507,D17-1082,0,\N,Missing
2020.conll-1.11,K18-1030,0,0.385535,"sk specific reading condition (Hunting) and a task-independent condition (Gathering), as well as both external (Dundee) and corpus specific (OneStopQA) eye-tracking data. Our QA task can be viewed as a generalization of the work in Mirsha et al. (2016; 2017; 2018), where instead of being asked about the sentiment of a paragraph, subjects are presented with arbitrary questions. Our multitask approach for jointly performing the QA task and predicting gaze is similar to Klerke et al. (2016), Berrett et al. (2018) and Mishra et al. (2018). In particular, in Equation 4 we use the same loss term as Barrett et al. (2018) which consists of a linear combination of an NLP task loss and gaze prediction loss. Our approach differs from Barrett et al. (2018) in that their model uses the gaze predictions as input attention weights for the NLP task, while our model treats gaze only as an output. Our approach provides a parallel to human reading, in which eye movements are an external behavior rather than an input to language processing tasks. Our work differs from Mishra et al. (2018) in the model and the use of a single auxiliary objective based on gaze. Finally, we note that in Vajjala et al. (2016) eye-tracking dat"
2020.conll-1.11,P16-2094,0,0.0728897,"ntity which is the correct answer to the question are longer if participants are shown the question before reading the passage as compared to ordinary reading. Our work builds on this result, introducing a more general QA setup which is not restricted to questions whose answer is a named entity. Crucially, we further leverage this information for improving machine question answering. The second research area to which or work contributes is augmenting NLP models with gaze data. In this area, gaze during reading has been used for tasks such as syntactic annotation (Barrett and Søgaard, 2015a,b; Barrett et al., 2016; Strzyz et al., 2019), text compression (Klerke et al., 2016), text readability (Gonz´alez-Gardu˜no and Søgaard, 2017), Named Entity Recognition (Hollenstein and Zhang, 2019), and sentiment classification (Mishra et al., 2016, 2017, 2018). Work on the first four tasks used task-independent eye-tracking corpora, primarily the Dundee corpus (Kennedy et al., 2003) and GECO (Cop et al., 2017). For the task of sentiment classification, the authors used task specific eye-tracking corpora in which the participants were asked to perform sentiment classification. Our study differs from this literature"
2020.conll-1.11,K15-1038,0,0.0289177,"t reading times on the named entity which is the correct answer to the question are longer if participants are shown the question before reading the passage as compared to ordinary reading. Our work builds on this result, introducing a more general QA setup which is not restricted to questions whose answer is a named entity. Crucially, we further leverage this information for improving machine question answering. The second research area to which or work contributes is augmenting NLP models with gaze data. In this area, gaze during reading has been used for tasks such as syntactic annotation (Barrett and Søgaard, 2015a,b; Barrett et al., 2016; Strzyz et al., 2019), text compression (Klerke et al., 2016), text readability (Gonz´alez-Gardu˜no and Søgaard, 2017), Named Entity Recognition (Hollenstein and Zhang, 2019), and sentiment classification (Mishra et al., 2016, 2017, 2018). Work on the first four tasks used task-independent eye-tracking corpora, primarily the Dundee corpus (Kennedy et al., 2003) and GECO (Cop et al., 2017). For the task of sentiment classification, the authors used task specific eye-tracking corpora in which the participants were asked to perform sentiment classification. Our study dif"
2020.conll-1.11,W15-2401,0,0.024356,"t reading times on the named entity which is the correct answer to the question are longer if participants are shown the question before reading the passage as compared to ordinary reading. Our work builds on this result, introducing a more general QA setup which is not restricted to questions whose answer is a named entity. Crucially, we further leverage this information for improving machine question answering. The second research area to which or work contributes is augmenting NLP models with gaze data. In this area, gaze during reading has been used for tasks such as syntactic annotation (Barrett and Søgaard, 2015a,b; Barrett et al., 2016; Strzyz et al., 2019), text compression (Klerke et al., 2016), text readability (Gonz´alez-Gardu˜no and Søgaard, 2017), Named Entity Recognition (Hollenstein and Zhang, 2019), and sentiment classification (Mishra et al., 2016, 2017, 2018). Work on the first four tasks used task-independent eye-tracking corpora, primarily the Dundee corpus (Kennedy et al., 2003) and GECO (Cop et al., 2017). For the task of sentiment classification, the authors used task specific eye-tracking corpora in which the participants were asked to perform sentiment classification. Our study dif"
2020.conll-1.11,2020.acl-main.507,1,0.831433,", we study the possibility of providing the model with an inductive bias by using human behavioral signals based on eye movements in reading as an additional source of information which can guide NLP models to adequately process linguistic input and solve linguistic tasks. As a case study, we examine reading comprehension, a task of central importance for probing both human and machine understanding of text. To enable this study, we collect eye movement data from 269 participants who engage in a reading comprehension Yevgeni Berzak MIT BCS berzak@mit.edu task using the materials of OneStopQA (Berzak et al., 2020). We argue that reading comprehension is a particularly well-suited task for linking human eye movement information to NLP modelling due to the substantial correspondence between reading times and the relevance of the text segment for answering the question. Hahn and Keller (2018) have shown this correspondence by establishing increased reading times on the correct answer in a question answering task where answers are named entities. Our study generalizes this result to an arbitrary QA setting, and demonstrates longer reading times for portions of the text which are most pertinent for answerin"
2020.conll-1.11,P16-1223,0,0.0271095,"objective based on gaze. Finally, we note that in Vajjala et al. (2016) eye-tracking data from ESL learners was collected for 4 articles from the same source of OneStopEnglish articles (Vajjala and Luˇci´c, 2018) used here, and utilized to study the influence of text difficulty level on fixation measures and reading comprehension. Our work focuses on a different task and a different population of readers. 143 A large body of work exists on QA, including span prediction (e.g. BiDAF (Seo et al., 2017)), cloze (e.g. (Hermann et al., 2015)), and multiple choice QA (e.g. Stanford Attentive Reader (Chen et al., 2016)). Here, we focus on multiple choice QA due to its prevalence in human evaluations of reading comprehension, and use RoBERTa due to its state-of-the-art performance on this task. Further, neural models for QA deploy various notions of internal attention. The study of NLP model internal attention has drawn much interest in recent years (Adi et al., 2017; Clark et al., 2019; Serrano and Smith, 2019; Kovaleva et al., 2019; Hoover et al., 2019, among others). In this work we abstract away from model internal dynamics due to their complexity, and the theoretical justification for treating gaze as a"
2020.conll-1.11,W19-4828,0,0.0370215,"a different population of readers. 143 A large body of work exists on QA, including span prediction (e.g. BiDAF (Seo et al., 2017)), cloze (e.g. (Hermann et al., 2015)), and multiple choice QA (e.g. Stanford Attentive Reader (Chen et al., 2016)). Here, we focus on multiple choice QA due to its prevalence in human evaluations of reading comprehension, and use RoBERTa due to its state-of-the-art performance on this task. Further, neural models for QA deploy various notions of internal attention. The study of NLP model internal attention has drawn much interest in recent years (Adi et al., 2017; Clark et al., 2019; Serrano and Smith, 2019; Kovaleva et al., 2019; Hoover et al., 2019, among others). In this work we abstract away from model internal dynamics due to their complexity, and the theoretical justification for treating gaze as an external behavior rather than an internal model property. Examination of internal model attention and its relation to human gaze is however an intriguing research direction that we intend to pursue in future work. 3 3.1 prehension. It comprises reading comprehension examination materials for middle school and high school students in China. Similarly to OneStopQA, RACE q"
2020.conll-1.11,W17-5050,0,0.053678,"Missing"
2020.conll-1.11,N19-1001,0,0.177907,"k builds on this result, introducing a more general QA setup which is not restricted to questions whose answer is a named entity. Crucially, we further leverage this information for improving machine question answering. The second research area to which or work contributes is augmenting NLP models with gaze data. In this area, gaze during reading has been used for tasks such as syntactic annotation (Barrett and Søgaard, 2015a,b; Barrett et al., 2016; Strzyz et al., 2019), text compression (Klerke et al., 2016), text readability (Gonz´alez-Gardu˜no and Søgaard, 2017), Named Entity Recognition (Hollenstein and Zhang, 2019), and sentiment classification (Mishra et al., 2016, 2017, 2018). Work on the first four tasks used task-independent eye-tracking corpora, primarily the Dundee corpus (Kennedy et al., 2003) and GECO (Cop et al., 2017). For the task of sentiment classification, the authors used task specific eye-tracking corpora in which the participants were asked to perform sentiment classification. Our study differs from this literature in several aspects. First, we address the previously unexplored task of reading comprehension, which has established theoretical and empirical connections to eye movements in"
2020.conll-1.11,W16-4105,0,0.0231658,"ame loss term as Barrett et al. (2018) which consists of a linear combination of an NLP task loss and gaze prediction loss. Our approach differs from Barrett et al. (2018) in that their model uses the gaze predictions as input attention weights for the NLP task, while our model treats gaze only as an output. Our approach provides a parallel to human reading, in which eye movements are an external behavior rather than an input to language processing tasks. Our work differs from Mishra et al. (2018) in the model and the use of a single auxiliary objective based on gaze. Finally, we note that in Vajjala et al. (2016) eye-tracking data from ESL learners was collected for 4 articles from the same source of OneStopEnglish articles (Vajjala and Luˇci´c, 2018) used here, and utilized to study the influence of text difficulty level on fixation measures and reading comprehension. Our work focuses on a different task and a different population of readers. 143 A large body of work exists on QA, including span prediction (e.g. BiDAF (Seo et al., 2017)), cloze (e.g. (Hermann et al., 2015)), and multiple choice QA (e.g. Stanford Attentive Reader (Chen et al., 2016)). Here, we focus on multiple choice QA due to its pr"
2020.conll-1.11,N16-1179,0,0.174434,"f participants are shown the question before reading the passage as compared to ordinary reading. Our work builds on this result, introducing a more general QA setup which is not restricted to questions whose answer is a named entity. Crucially, we further leverage this information for improving machine question answering. The second research area to which or work contributes is augmenting NLP models with gaze data. In this area, gaze during reading has been used for tasks such as syntactic annotation (Barrett and Søgaard, 2015a,b; Barrett et al., 2016; Strzyz et al., 2019), text compression (Klerke et al., 2016), text readability (Gonz´alez-Gardu˜no and Søgaard, 2017), Named Entity Recognition (Hollenstein and Zhang, 2019), and sentiment classification (Mishra et al., 2016, 2017, 2018). Work on the first four tasks used task-independent eye-tracking corpora, primarily the Dundee corpus (Kennedy et al., 2003) and GECO (Cop et al., 2017). For the task of sentiment classification, the authors used task specific eye-tracking corpora in which the participants were asked to perform sentiment classification. Our study differs from this literature in several aspects. First, we address the previously unexplor"
2020.conll-1.11,D19-1445,0,0.0291904,"e body of work exists on QA, including span prediction (e.g. BiDAF (Seo et al., 2017)), cloze (e.g. (Hermann et al., 2015)), and multiple choice QA (e.g. Stanford Attentive Reader (Chen et al., 2016)). Here, we focus on multiple choice QA due to its prevalence in human evaluations of reading comprehension, and use RoBERTa due to its state-of-the-art performance on this task. Further, neural models for QA deploy various notions of internal attention. The study of NLP model internal attention has drawn much interest in recent years (Adi et al., 2017; Clark et al., 2019; Serrano and Smith, 2019; Kovaleva et al., 2019; Hoover et al., 2019, among others). In this work we abstract away from model internal dynamics due to their complexity, and the theoretical justification for treating gaze as an external behavior rather than an internal model property. Examination of internal model attention and its relation to human gaze is however an intriguing research direction that we intend to pursue in future work. 3 3.1 prehension. It comprises reading comprehension examination materials for middle school and high school students in China. Similarly to OneStopQA, RACE questions are multiple choice, with four possible"
2020.conll-1.11,2021.ccl-1.108,0,0.120777,"Missing"
2020.conll-1.11,P17-1035,0,0.0408251,"Missing"
2020.conll-1.11,K16-1016,0,0.316064,"which is not restricted to questions whose answer is a named entity. Crucially, we further leverage this information for improving machine question answering. The second research area to which or work contributes is augmenting NLP models with gaze data. In this area, gaze during reading has been used for tasks such as syntactic annotation (Barrett and Søgaard, 2015a,b; Barrett et al., 2016; Strzyz et al., 2019), text compression (Klerke et al., 2016), text readability (Gonz´alez-Gardu˜no and Søgaard, 2017), Named Entity Recognition (Hollenstein and Zhang, 2019), and sentiment classification (Mishra et al., 2016, 2017, 2018). Work on the first four tasks used task-independent eye-tracking corpora, primarily the Dundee corpus (Kennedy et al., 2003) and GECO (Cop et al., 2017). For the task of sentiment classification, the authors used task specific eye-tracking corpora in which the participants were asked to perform sentiment classification. Our study differs from this literature in several aspects. First, we address the previously unexplored task of reading comprehension, which has established theoretical and empirical connections to eye movements in reading (Just and Carpenter, 1980; Reichle et al.,"
2020.conll-1.11,P16-1162,0,0.00675073,"on Duration (T F ). Specifically, for each passage word w and subject s, we consider the subject’s Total Fixation Duration on the word T Fs (w) normalized by the sum of all their fixation durations over the passage, and then average this quantity across all subjects who read the passage. RT (w) = 1 X T Fs (w) P 0 S s w0 T Fs (w ) Lgaze = − X RT (w) log RTpred (w) (3) w The final loss term is a convex combination of the gaze loss term and the answer prediction loss, where a hyperparameter α is the relative weight assigned to the gaze loss term: (2) In cases where RoBERTa’s byte pair tokenizer (Sennrich et al., 2016) splits a single word into multiple tokens, we evenly split the gaze time associated with the word among the resulting tokens. We take the encoding of each passage word at the last layer of RoBERTa for each candidate answer y and add a linear layer parameterized by a weight vector v ∈ Rd shared across all passage word positions, where d is the RoBERTa embedding dimension. For each passage word w, this layer maps from the d-dimensional word embedding to a scalar gaze value. These values are put through a softmax layer, obtaining predictions RTpredy (w) which are guaranteed to be between 0 and 1"
2020.conll-1.11,P19-1282,0,0.0381467,"on of readers. 143 A large body of work exists on QA, including span prediction (e.g. BiDAF (Seo et al., 2017)), cloze (e.g. (Hermann et al., 2015)), and multiple choice QA (e.g. Stanford Attentive Reader (Chen et al., 2016)). Here, we focus on multiple choice QA due to its prevalence in human evaluations of reading comprehension, and use RoBERTa due to its state-of-the-art performance on this task. Further, neural models for QA deploy various notions of internal attention. The study of NLP model internal attention has drawn much interest in recent years (Adi et al., 2017; Clark et al., 2019; Serrano and Smith, 2019; Kovaleva et al., 2019; Hoover et al., 2019, among others). In this work we abstract away from model internal dynamics due to their complexity, and the theoretical justification for treating gaze as an external behavior rather than an internal model property. Examination of internal model attention and its relation to human gaze is however an intriguing research direction that we intend to pursue in future work. 3 3.1 prehension. It comprises reading comprehension examination materials for middle school and high school students in China. Similarly to OneStopQA, RACE questions are multiple cho"
2020.conll-1.11,D19-1160,0,0.02626,"Missing"
2020.conll-1.11,W18-0535,0,0.0272523,"Missing"
2020.conll-1.49,2020.tacl-1.3,0,0.0207937,"diverge from corpus probabilities (Smith and Levy, 2011). Our analysis extends these findings by testing current state-of-the-art LMs trained on much larger datasets, and showing that, while better estimates of corpus probabilities may yield better models of human next-word predictions, there does not seem to be a strict positive correlation between the ability to approximate corpus probabilities and the ability to predict human reading times, as evidenced by models with higher hSi i being on-par and even better at predicting reading times compared to models with lower hSi i. Recent studies (Ettinger, 2020; Hao et al., 2020; Jacobs and McCarthy, 2020) have found similar trends when comparing LMs to cloze data. Hu et al. (2020) also found only a loose relationship between perplexity (a monotonic function of hSi i) and syntactic generalization, adding to a growing body of evidence suggesting that while optimizing for corpus probabilities can create somewhat psycholinguistically-enabled language models (Linzen et al., 2016; Futrell et al., 2019; Hu et al., 2020), there may be a dissociation between corpus probabilities and human expectations. 4 Cloze Distillation Here, we show how to leverage thes"
2020.conll-1.49,P19-1285,0,0.0647461,"Missing"
2020.conll-1.49,N19-1423,0,0.148017,"els (LMs) demonstrate outstanding general-purpose command over language. The majority of these models acquire language by maximizing the in-context probability of each word in their training corpus (Figure 1), typically with a self-supervised objective. This simple corpus probability matching has resulted in models that learn impressive powers of both psychometric prediction (Frank and Bod, 2011; Fossum and Levy, 2012; Frank et al., 2015; Goodkind and Bicknell, 2018; Hale et al., 2018; van Schijndel and Linzen, 2018; Warstadt and Bowman, 2020; Wilcox et al., 2020) and language more generally (Devlin et al., 2019; Radford et al., 2019). In humans, prediction may underlie both learning (Kuhl, 2004; Huang and Snedeker, 2013) and processing (Ryskin et al., 2020; Levy, 2008; Clark, 2013). Human linguistic prediction can be understood as not only lexical but also as taking place both above and below the word level (Federmeier and Kutas, 1999; Federmeier et al., 2002); parallel, i.e., predictive commitments are maintained over several linguistic units at once (Levy, 2008); and graded, i.e., commitment is licensed to varying degrees based on features of the linguistic unit being predicted. Rather than placin"
2020.conll-1.49,W12-1706,1,0.777994,"Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data. 1 Introduction Modern language models (LMs) demonstrate outstanding general-purpose command over language. The majority of these models acquire language by maximizing the in-context probability of each word in their training corpus (Figure 1), typically with a self-supervised objective. This simple corpus probability matching has resulted in models that learn impressive powers of both psychometric prediction (Frank and Bod, 2011; Fossum and Levy, 2012; Frank et al., 2015; Goodkind and Bicknell, 2018; Hale et al., 2018; van Schijndel and Linzen, 2018; Warstadt and Bowman, 2020; Wilcox et al., 2020) and language more generally (Devlin et al., 2019; Radford et al., 2019). In humans, prediction may underlie both learning (Kuhl, 2004; Huang and Snedeker, 2013) and processing (Ryskin et al., 2020; Levy, 2008; Clark, 2013). Human linguistic prediction can be understood as not only lexical but also as taking place both above and below the word level (Federmeier and Kutas, 1999; Federmeier et al., 2002); parallel, i.e., predictive commitments are m"
2020.conll-1.49,N19-1004,1,0.889496,"Missing"
2020.conll-1.49,2020.acl-demos.10,1,0.711737,"ve as well as a segment level recurrence mechanism and relative positional embeddings. Training data consists of ∼30 billion tokens across 6 different copora. Models We consider in our analysis the following LMs: 1. 5-gram: N-gram model using a window size of 5 with Kneser-Ney smoothing, obtained via the SRILM language modeling toolkit (Stolcke, 2002). 2. LSTM: A standard 2-layer LSTM RNN implemented in PyTorch (Paszke et al., 2017), used here with 256 hidden units and word embedding size of 256, and trained on the wikitext-103 corpus (Merity et al., 2016) via We use the LMzoo python package (Gauthier et al., 2020) to access the 5-gram model, and the HuggingFace transformers python package (Wolf et al., 2019) for accessing Transformer models (gpt2-large, transfo-xl-wt103, and xlnet-large-cased respectively). These Transformer models use subword tokens (Sennrich et al., 2016); we defined word probabilities for these models as the joint probability of the subword tokens comprising the word given the context. 611 Model Cloze GPT-2 XLNet TXL LSTM 5-gram hDi i NA 2.30 ± 1.57 2.39 ± 1.68 3.27 ± 1.92 3.74 ± 1.86 3.89 ± 1.84 hτi i NA −0.57 ± 0.004 −0.58 ± 0.005 −0.47 ± 0.005 −0.39 ± 0.006 −0.20 ± 0.007 hSi i 3."
2020.conll-1.49,W18-0102,0,0.011813,"eline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data. 1 Introduction Modern language models (LMs) demonstrate outstanding general-purpose command over language. The majority of these models acquire language by maximizing the in-context probability of each word in their training corpus (Figure 1), typically with a self-supervised objective. This simple corpus probability matching has resulted in models that learn impressive powers of both psychometric prediction (Frank and Bod, 2011; Fossum and Levy, 2012; Frank et al., 2015; Goodkind and Bicknell, 2018; Hale et al., 2018; van Schijndel and Linzen, 2018; Warstadt and Bowman, 2020; Wilcox et al., 2020) and language more generally (Devlin et al., 2019; Radford et al., 2019). In humans, prediction may underlie both learning (Kuhl, 2004; Huang and Snedeker, 2013) and processing (Ryskin et al., 2020; Levy, 2008; Clark, 2013). Human linguistic prediction can be understood as not only lexical but also as taking place both above and below the word level (Federmeier and Kutas, 1999; Federmeier et al., 2002); parallel, i.e., predictive commitments are maintained over several linguistic units at once ("
2020.conll-1.49,N01-1021,0,0.451647,"tic prediction can be understood as not only lexical but also as taking place both above and below the word level (Federmeier and Kutas, 1999; Federmeier et al., 2002); parallel, i.e., predictive commitments are maintained over several linguistic units at once (Levy, 2008); and graded, i.e., commitment is licensed to varying degrees based on features of the linguistic unit being predicted. Rather than placing bets (Jackendoff, 1987) on which single word will come next, humans make many diffuse bets at multiple linguistic levels (e.g., syntactic, orthographic, lexical, etc.). Surprisal theory (Hale, 2001; Levy, 2008) describes the utility of the approach taken by the human language processor, as lexical prediction is often an ill-constrained classification problem — for agents with very large vocabularies (LMs, humans), context is often not sufficiently constraining for high accuracy multiple, thousand-way classification decisions, but is typically constraining enough to accurately infer next-word features (such as part of speech, and semantic category). A large body of evidence demonstrates that these graded next-word predictions are reflected in human processing times (Ehrlich and Rayner, 1"
2020.conll-1.49,P18-1254,0,0.021507,"ntial improvement in reading time prediction and generalization to held-out human cloze data. 1 Introduction Modern language models (LMs) demonstrate outstanding general-purpose command over language. The majority of these models acquire language by maximizing the in-context probability of each word in their training corpus (Figure 1), typically with a self-supervised objective. This simple corpus probability matching has resulted in models that learn impressive powers of both psychometric prediction (Frank and Bod, 2011; Fossum and Levy, 2012; Frank et al., 2015; Goodkind and Bicknell, 2018; Hale et al., 2018; van Schijndel and Linzen, 2018; Warstadt and Bowman, 2020; Wilcox et al., 2020) and language more generally (Devlin et al., 2019; Radford et al., 2019). In humans, prediction may underlie both learning (Kuhl, 2004; Huang and Snedeker, 2013) and processing (Ryskin et al., 2020; Levy, 2008; Clark, 2013). Human linguistic prediction can be understood as not only lexical but also as taking place both above and below the word level (Federmeier and Kutas, 1999; Federmeier et al., 2002); parallel, i.e., predictive commitments are maintained over several linguistic units at once (Levy, 2008); and gr"
2020.conll-1.49,P19-1337,0,0.0182768,"also proven useful to cognitive scientists in creating low-dimensional neural network cognitive models (Schaeffer et al., 2020). When humans are used as the ‘teacher’ this can be seen as a specific case of a more general cognitive modeling strategy, task-based modeling. 4.2 The Cloze Distillation Objective Knowledge distillation has proven its usefulness in NLP where researchers have distilled knowledge from very large and/or syntactically aware language models into naive models showing it is possible to transfer even subtle linguistic preferences from teacher to student (Kim and Rush, 2016; Kuncoro et al., 2019; Sanh et al., 2020; Kuncoro et al., 2020). We take inspiration from this work and leverage the general framework both as a method for distilling knowledge from a ‘teacher’ with desirable linguistic biases (humans in our case) and as a tool for cognitive modeling by using empirical cloze distributions Pcloze as target distributions in a knowledge distillation framework. We follow this approach to arrive at the following loss function for Cloze Distillation (CD): Li = αDi − (1 − α)Si . (3) That is, for each context x<i we compute the CD loss by linearly interpolating Di , the KL divergence betw"
2020.conll-1.49,2020.cmcl-1.10,0,0.0188,"rpus probabilities (Smith and Levy, 2011). Our analysis extends these findings by testing current state-of-the-art LMs trained on much larger datasets, and showing that, while better estimates of corpus probabilities may yield better models of human next-word predictions, there does not seem to be a strict positive correlation between the ability to approximate corpus probabilities and the ability to predict human reading times, as evidenced by models with higher hSi i being on-par and even better at predicting reading times compared to models with lower hSi i. Recent studies (Ettinger, 2020; Hao et al., 2020; Jacobs and McCarthy, 2020) have found similar trends when comparing LMs to cloze data. Hu et al. (2020) also found only a loose relationship between perplexity (a monotonic function of hSi i) and syntactic generalization, adding to a growing body of evidence suggesting that while optimizing for corpus probabilities can create somewhat psycholinguistically-enabled language models (Linzen et al., 2016; Futrell et al., 2019; Hu et al., 2020), there may be a dissociation between corpus probabilities and human expectations. 4 Cloze Distillation Here, we show how to leverage these findings to impr"
2020.conll-1.49,2020.tacl-1.50,0,0.0306493,"ts in creating low-dimensional neural network cognitive models (Schaeffer et al., 2020). When humans are used as the ‘teacher’ this can be seen as a specific case of a more general cognitive modeling strategy, task-based modeling. 4.2 The Cloze Distillation Objective Knowledge distillation has proven its usefulness in NLP where researchers have distilled knowledge from very large and/or syntactically aware language models into naive models showing it is possible to transfer even subtle linguistic preferences from teacher to student (Kim and Rush, 2016; Kuncoro et al., 2019; Sanh et al., 2020; Kuncoro et al., 2020). We take inspiration from this work and leverage the general framework both as a method for distilling knowledge from a ‘teacher’ with desirable linguistic biases (humans in our case) and as a tool for cognitive modeling by using empirical cloze distributions Pcloze as target distributions in a knowledge distillation framework. We follow this approach to arrive at the following loss function for Cloze Distillation (CD): Li = αDi − (1 − α)Si . (3) That is, for each context x<i we compute the CD loss by linearly interpolating Di , the KL divergence between the distributions of the human teacher"
2020.conll-1.49,2020.acl-main.158,1,0.826057,"f-the-art LMs trained on much larger datasets, and showing that, while better estimates of corpus probabilities may yield better models of human next-word predictions, there does not seem to be a strict positive correlation between the ability to approximate corpus probabilities and the ability to predict human reading times, as evidenced by models with higher hSi i being on-par and even better at predicting reading times compared to models with lower hSi i. Recent studies (Ettinger, 2020; Hao et al., 2020; Jacobs and McCarthy, 2020) have found similar trends when comparing LMs to cloze data. Hu et al. (2020) also found only a loose relationship between perplexity (a monotonic function of hSi i) and syntactic generalization, adding to a growing body of evidence suggesting that while optimizing for corpus probabilities can create somewhat psycholinguistically-enabled language models (Linzen et al., 2016; Futrell et al., 2019; Hu et al., 2020), there may be a dissociation between corpus probabilities and human expectations. 4 Cloze Distillation Here, we show how to leverage these findings to improve the ability of LMs to match human expectations, providing more appealing neural language models for h"
2020.conll-1.49,2020.winlp-1.29,0,0.0300196,"(Smith and Levy, 2011). Our analysis extends these findings by testing current state-of-the-art LMs trained on much larger datasets, and showing that, while better estimates of corpus probabilities may yield better models of human next-word predictions, there does not seem to be a strict positive correlation between the ability to approximate corpus probabilities and the ability to predict human reading times, as evidenced by models with higher hSi i being on-par and even better at predicting reading times compared to models with lower hSi i. Recent studies (Ettinger, 2020; Hao et al., 2020; Jacobs and McCarthy, 2020) have found similar trends when comparing LMs to cloze data. Hu et al. (2020) also found only a loose relationship between perplexity (a monotonic function of hSi i) and syntactic generalization, adding to a growing body of evidence suggesting that while optimizing for corpus probabilities can create somewhat psycholinguistically-enabled language models (Linzen et al., 2016; Futrell et al., 2019; Hu et al., 2020), there may be a dissociation between corpus probabilities and human expectations. 4 Cloze Distillation Here, we show how to leverage these findings to improve the ability of LMs to ma"
2020.conll-1.49,D16-1139,0,0.0294542,"dge distillation has also proven useful to cognitive scientists in creating low-dimensional neural network cognitive models (Schaeffer et al., 2020). When humans are used as the ‘teacher’ this can be seen as a specific case of a more general cognitive modeling strategy, task-based modeling. 4.2 The Cloze Distillation Objective Knowledge distillation has proven its usefulness in NLP where researchers have distilled knowledge from very large and/or syntactically aware language models into naive models showing it is possible to transfer even subtle linguistic preferences from teacher to student (Kim and Rush, 2016; Kuncoro et al., 2019; Sanh et al., 2020; Kuncoro et al., 2020). We take inspiration from this work and leverage the general framework both as a method for distilling knowledge from a ‘teacher’ with desirable linguistic biases (humans in our case) and as a tool for cognitive modeling by using empirical cloze distributions Pcloze as target distributions in a knowledge distillation framework. We follow this approach to arrive at the following loss function for Cloze Distillation (CD): Li = αDi − (1 − α)Si . (3) That is, for each context x<i we compute the CD loss by linearly interpolating Di ,"
2020.conll-1.49,Q16-1037,0,0.0338234,"to predict human reading times, as evidenced by models with higher hSi i being on-par and even better at predicting reading times compared to models with lower hSi i. Recent studies (Ettinger, 2020; Hao et al., 2020; Jacobs and McCarthy, 2020) have found similar trends when comparing LMs to cloze data. Hu et al. (2020) also found only a loose relationship between perplexity (a monotonic function of hSi i) and syntactic generalization, adding to a growing body of evidence suggesting that while optimizing for corpus probabilities can create somewhat psycholinguistically-enabled language models (Linzen et al., 2016; Futrell et al., 2019; Hu et al., 2020), there may be a dissociation between corpus probabilities and human expectations. 4 Cloze Distillation Here, we show how to leverage these findings to improve the ability of LMs to match human expectations, providing more appealing neural language models for human language processing. To this end, we propose Cloze Distillation: a method for using human next-word predictions as learning targets together with corpus statistics within a knowledge distillation framework. do not include cloze probabilities of zero (which would yield infinite surprisal). 613"
2020.conll-1.49,P16-1009,0,0.0404281,"h Kneser-Ney smoothing, obtained via the SRILM language modeling toolkit (Stolcke, 2002). 2. LSTM: A standard 2-layer LSTM RNN implemented in PyTorch (Paszke et al., 2017), used here with 256 hidden units and word embedding size of 256, and trained on the wikitext-103 corpus (Merity et al., 2016) via We use the LMzoo python package (Gauthier et al., 2020) to access the 5-gram model, and the HuggingFace transformers python package (Wolf et al., 2019) for accessing Transformer models (gpt2-large, transfo-xl-wt103, and xlnet-large-cased respectively). These Transformer models use subword tokens (Sennrich et al., 2016); we defined word probabilities for these models as the joint probability of the subword tokens comprising the word given the context. 611 Model Cloze GPT-2 XLNet TXL LSTM 5-gram hDi i NA 2.30 ± 1.57 2.39 ± 1.68 3.27 ± 1.92 3.74 ± 1.86 3.89 ± 1.84 hτi i NA −0.57 ± 0.004 −0.58 ± 0.005 −0.47 ± 0.005 −0.39 ± 0.006 −0.20 ± 0.007 hSi i 3.99 ± 2.60 6.11 ± 5.00 6.39 ± 5.70 8.09 ± 5.50 8.58 ± 4.90 12.48 ± 7.00 Fintr 198.10 252.70 260.50 238.30 361.20 161.00 Fbase 30.90 46.11 46.08 30.54 41.47 16.72 ρgaze 0.36 0.40 0.41 0.39 0.47 0.31 ρfreq −0.43 −0.46 −0.48 −0.50 −0.63 −0.41 Table 1: Evaluation of LMs"
2020.emnlp-main.375,N19-1334,1,0.531877,"component of human linguistic capabilities that has been so far untested in the neural setting. 1.1 Related Work Bayesian models of word learning have shown successes in acquiring proper syntactic generalizations from minimal exposure (Tenenbaum and Xu, 2000; Wang et al., 2017), however it is not clear how well neural network models would exhibit these rapid generalizations. Comparing between neural network architectures, recent work has shown that models enhanced with explicit structural supervision during training produce more humanlike syntactic generalizations (Kuncoro et al., 2017, 2018; Wilcox et al., 2019), but it remains untested whether such supervision helps learn properties of tokens that occur rarely during training. Previous studies have found that Artificial Neural Networks (ANNs) are capable of learning some argument structure paradigms and make correct predictions across multiple frames (Kann et al., 2018), however these capabilities remain untested for incremental language models. Much has been written about the ability of ANNs to learn number agreement (Linzen et al., 2016; Gulordava et al., 2018; Giulianelli et al., 2018), including their ability to maintain the dependency across di"
2020.scil-1.39,P19-1285,0,0.0391498,"ar variant. We test two LSTMs that differ significantly in vocabulary size and have been shown to learn syntactic dependencies to varying degrees of success. The Gulordava et al. (2018) LSTM (“GRNN”) was trained on a subset of English Wikipedia with 90M training tokens. The Jozefowicz et al. (2016) LSTM (“JRNN”) was trained on the One Billion Word Benchmark (Chelba et al., 2013). JRNN additionally has convolutional neural network character input embeddings. Transformer-XL and BERT Next, we test two models based on the Transformer architecture (Vaswani et al., 2017). Transformer-XL (“TransXL”; Dai et al., 2019) reuses the hidden states obtained in previous segments, which facilitates modeling of long-term dependencies. BERT (Devlin et al., 2018) is bi-directional, in that it is trained to predict the identity of masked words based on the preceding and following context.5 Both models were trained on document-level corpora instead of shuffled sentences: WikiText-103 5 We use the small, uncased version of BERT (BERTBASE ) with no fine-tuning after the initial pre-training tasks. Model Architecture Training data BERT Transformer BooksCorpus, Wikipedia TransXL Transformer WikiText-103 JRNN LSTM 1B Word B"
2020.scil-1.39,N16-1024,0,0.060047,"(trees) Wikipedia 950K 23K 114 95 12 90M 50K 10K 17K 4K Table 2: Language models evaluated in our experiments, along with raw frequency counts of reflexives in the training data. Pre-training data was not publicly released for BERT. (Merity et al., 2017) for TransXL, and a combination of BooksCorpus (Zhu et al., 2015) and Wikipedia for BERT. Recent work has shown BERT to perform well on reflexive constructions (Goldberg, 2019). RNNG and TinyLSTM The last two neural models in our test suite have identical vocabularies but differing inductive biases: a recurrent neural network grammar (“RNNG”; Dyer et al., 2016) and a vanilla LSTM (“TinyLSTM”). Both models were trained on the 1-million-word English Penn Treebank §2-21 (Marcus et al., 1993), but TinyLSTM is only trained on the terminal word sequences, while RNNG is trained on the full annotations, which contain complete constituency parses. This minimal difference allows us to observe the effect of structural supervision, which has been shown to be beneficial in acquiring certain grammatical dependencies (Kuncoro et al., 2017; Wilcox et al., 2019). Crucially, the vocabulary of these models is too small to acommodate the lexical items used in previous"
2020.scil-1.39,N18-1108,0,0.0907345,"he sentence. If an NLM has learned the linguistic phenomenon in question, then it 1 Code and data are available at https://github. com/jennhu/reflexive-anaphor-licensing. Roger Levy Dept. of Brain and Cognitive Sciences Massachusetts Institute of Technology Cambridge, MA rplevy@mit.edu should assign higher probability to sentences that humans judge to be more acceptable. This approach has shown NLMs to be capable of learning some grammatical phenomena (e.g. subject-verb agreement and filler-gap dependencies) while failing on others (Linzen et al., 2016; Lau et al., 2017; Futrell et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018; Tran et al., 2018; Wilcox et al., 2018). In evaluating these mixed learning outcomes, we raise a broader question that remains largely unaddressed in the field: What is the standard to which we should be holding artificial language models? An engineering goal within the machine learning community is to build NLMs that approximate human behavior. In this case, an ideal NLM should achieve high performance even on low-frequency constructions, and the learning signal should be detectable even with coarse experimental paradigms. However, if a scientific goal is to highlig"
2020.scil-1.39,P18-1254,0,0.0151863,"est a 5-gram model trained on the same Wikipedia data as GRNN. We use Kneser-Ney smoothing to perform backoff. 4.1 Computing word probabilities In practice, we calculate accuracy (see Section 3.2) by comparing differentials in log probability space at the target pronoun. To obtain the log probability of word wi assigned by the LSTMs and Transformer models, we compute log2 p(wi |hi 1 ), (1) where hi 1 is the model’s hidden state before observing wi . This probability is calculated from the model’s softmax activation. To obtain the log probability of wi in the RNNG, we follow the method used in Hale et al. (2018). We use word-synchronous beam search (Stern et al., 2017) to find the most likely incremental parses, and sum their forward probabilities to approximate P (w1 , . . . , wi+ 1 ) and P (w1 , . . . , wi 1 ). We use 100 for the action beam size and 10 for the word beam size. In contrast to the other models in our test suite, BERT is bi-directional. To obtain the log probability of wi , we first feed BERT a sentence with wi masked out and obtain the word predictions for the masked position. This gives us a probability distribution over words. In practice, since the target reflexive in our items al"
2020.scil-1.39,P82-1020,0,0.799525,"Missing"
2020.scil-1.39,E17-1117,0,0.0517699,"Missing"
2020.scil-1.39,Q16-1037,0,0.0506791,"target word or phrase that determines the acceptability of the sentence. If an NLM has learned the linguistic phenomenon in question, then it 1 Code and data are available at https://github. com/jennhu/reflexive-anaphor-licensing. Roger Levy Dept. of Brain and Cognitive Sciences Massachusetts Institute of Technology Cambridge, MA rplevy@mit.edu should assign higher probability to sentences that humans judge to be more acceptable. This approach has shown NLMs to be capable of learning some grammatical phenomena (e.g. subject-verb agreement and filler-gap dependencies) while failing on others (Linzen et al., 2016; Lau et al., 2017; Futrell et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018; Tran et al., 2018; Wilcox et al., 2018). In evaluating these mixed learning outcomes, we raise a broader question that remains largely unaddressed in the field: What is the standard to which we should be holding artificial language models? An engineering goal within the machine learning community is to build NLMs that approximate human behavior. In this case, an ideal NLM should achieve high performance even on low-frequency constructions, and the learning signal should be detectable even with coarse exp"
2020.scil-1.39,J93-2004,0,0.0693247,"requency counts of reflexives in the training data. Pre-training data was not publicly released for BERT. (Merity et al., 2017) for TransXL, and a combination of BooksCorpus (Zhu et al., 2015) and Wikipedia for BERT. Recent work has shown BERT to perform well on reflexive constructions (Goldberg, 2019). RNNG and TinyLSTM The last two neural models in our test suite have identical vocabularies but differing inductive biases: a recurrent neural network grammar (“RNNG”; Dyer et al., 2016) and a vanilla LSTM (“TinyLSTM”). Both models were trained on the 1-million-word English Penn Treebank §2-21 (Marcus et al., 1993), but TinyLSTM is only trained on the terminal word sequences, while RNNG is trained on the full annotations, which contain complete constituency parses. This minimal difference allows us to observe the effect of structural supervision, which has been shown to be beneficial in acquiring certain grammatical dependencies (Kuncoro et al., 2017; Wilcox et al., 2019). Crucially, the vocabulary of these models is too small to acommodate the lexical items used in previous RAL studies. n-gram As a baseline, we test a 5-gram model trained on the same Wikipedia data as GRNN. We use Kneser-Ney smoothing"
2020.scil-1.39,D18-1151,0,0.0829075,"as learned the linguistic phenomenon in question, then it 1 Code and data are available at https://github. com/jennhu/reflexive-anaphor-licensing. Roger Levy Dept. of Brain and Cognitive Sciences Massachusetts Institute of Technology Cambridge, MA rplevy@mit.edu should assign higher probability to sentences that humans judge to be more acceptable. This approach has shown NLMs to be capable of learning some grammatical phenomena (e.g. subject-verb agreement and filler-gap dependencies) while failing on others (Linzen et al., 2016; Lau et al., 2017; Futrell et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018; Tran et al., 2018; Wilcox et al., 2018). In evaluating these mixed learning outcomes, we raise a broader question that remains largely unaddressed in the field: What is the standard to which we should be holding artificial language models? An engineering goal within the machine learning community is to build NLMs that approximate human behavior. In this case, an ideal NLM should achieve high performance even on low-frequency constructions, and the learning signal should be detectable even with coarse experimental paradigms. However, if a scientific goal is to highlight the grammatical phenom"
2020.scil-1.39,N18-2002,0,0.0691818,"Missing"
2020.scil-1.39,D17-1178,0,0.0709785,"GRNN. We use Kneser-Ney smoothing to perform backoff. 4.1 Computing word probabilities In practice, we calculate accuracy (see Section 3.2) by comparing differentials in log probability space at the target pronoun. To obtain the log probability of word wi assigned by the LSTMs and Transformer models, we compute log2 p(wi |hi 1 ), (1) where hi 1 is the model’s hidden state before observing wi . This probability is calculated from the model’s softmax activation. To obtain the log probability of wi in the RNNG, we follow the method used in Hale et al. (2018). We use word-synchronous beam search (Stern et al., 2017) to find the most likely incremental parses, and sum their forward probabilities to approximate P (w1 , . . . , wi+ 1 ) and P (w1 , . . . , wi 1 ). We use 100 for the action beam size and 10 for the word beam size. In contrast to the other models in our test suite, BERT is bi-directional. To obtain the log probability of wi , we first feed BERT a sentence with wi masked out and obtain the word predictions for the masked position. This gives us a probability distribution over words. In practice, since the target reflexive in our items always occurs directly before the final token ‘.’, we do not"
2020.scil-1.39,D18-1503,0,0.0424988,"Missing"
2020.scil-1.39,W18-5423,1,0.885537,"tion, then it 1 Code and data are available at https://github. com/jennhu/reflexive-anaphor-licensing. Roger Levy Dept. of Brain and Cognitive Sciences Massachusetts Institute of Technology Cambridge, MA rplevy@mit.edu should assign higher probability to sentences that humans judge to be more acceptable. This approach has shown NLMs to be capable of learning some grammatical phenomena (e.g. subject-verb agreement and filler-gap dependencies) while failing on others (Linzen et al., 2016; Lau et al., 2017; Futrell et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018; Tran et al., 2018; Wilcox et al., 2018). In evaluating these mixed learning outcomes, we raise a broader question that remains largely unaddressed in the field: What is the standard to which we should be holding artificial language models? An engineering goal within the machine learning community is to build NLMs that approximate human behavior. In this case, an ideal NLM should achieve high performance even on low-frequency constructions, and the learning signal should be detectable even with coarse experimental paradigms. However, if a scientific goal is to highlight the grammatical phenomena that can be learned from sequential d"
2020.scil-1.39,N19-1334,1,0.768495,"ite have identical vocabularies but differing inductive biases: a recurrent neural network grammar (“RNNG”; Dyer et al., 2016) and a vanilla LSTM (“TinyLSTM”). Both models were trained on the 1-million-word English Penn Treebank §2-21 (Marcus et al., 1993), but TinyLSTM is only trained on the terminal word sequences, while RNNG is trained on the full annotations, which contain complete constituency parses. This minimal difference allows us to observe the effect of structural supervision, which has been shown to be beneficial in acquiring certain grammatical dependencies (Kuncoro et al., 2017; Wilcox et al., 2019). Crucially, the vocabulary of these models is too small to acommodate the lexical items used in previous RAL studies. n-gram As a baseline, we test a 5-gram model trained on the same Wikipedia data as GRNN. We use Kneser-Ney smoothing to perform backoff. 4.1 Computing word probabilities In practice, we calculate accuracy (see Section 3.2) by comparing differentials in log probability space at the target pronoun. To obtain the log probability of word wi assigned by the LSTMs and Transformer models, we compute log2 p(wi |hi 1 ), (1) where hi 1 is the model’s hidden state before observing wi . T"
2021.acl-long.289,2020.findings-emnlp.89,1,0.929937,"steps on two hidden vectors, creates a bottleneck that forces the model to focus on the local state. This is a situation where a syntax-aware representation can provide additional value by enabling the local state to better encompass past structures. We conjecture that a similarly constrained local state might benefit Transformer models in learning linguistic regularities, especially in a limited training data scenario. In an attempt to capture a similar effect in the Transformer, we explore here the idea of masking some attention heads to reflect the parser state as in the stack-Transformer (Astudillo et al., 2020). In the stack-Transformer, two attention heads are specialized to attend only to the contents of buffer and stack respectively for dependency and semantic parsing tasks. Here we choose to specialize two heads as well for each layer in Equation 4, as depicted in Fig. 2. One attention head attends to the contents of the last open constituent whereas another head attends all other past decisions not involving that constituent. The rest of the heads are left free as in the original Transformer architecture. To constrain the attention heads, we only need to alter the mask M in Equation 5 to depend"
2021.acl-long.289,D16-1257,0,0.158209,"human-like syntactic generalization. This work hypothesizes that the Transformer language model may benefit from explicit generative structural supervision to systematically generalize syntactic knowledge. Here we explore two major classes of structural guidance for Transformer language models based on joint modeling of language and constituency parses. The “generative parsing as language modeling” approach builds a Transformer-parameterized model to learn to predict actions that incrementally build constituency trees along with terminal words, following prior work on RNNs (Dyer et al., 2016; Choe and Charniak, 2016). The “structural scaffolding” approach follows the general idea of regularizing hidden representation through multi-task learning objective, with prior success in various NLP tasks (Zhang S y0:1 NP hBOSi VP NT ( S ) y1:2 NT ( NP ) w0 The birds sang y2:3 The birds w1 REDUCE NT ( VP ) w2 sang NT ( ADVP ) ··· w3 ADVP y0:1 w1 w2 w3 NT ( S ) NT ( NP ) w0 w1 w2 hBOSi (a) Vanilla language model y3:4 The NT ( S ) NT ( NP ) y1:2 y2:3 y0:1 hPADi y1:2 birds REDUCE w1 w2 w3 w1 w2 w3 The birds w0 w1 w2 w0 w1 w2 (b) Parsing as Language Modelling (c) Language models with Structural Scaffold Figure 1: Top: I"
2021.acl-long.289,W19-4828,0,0.0213124,"data intensive pre-training. 1 Introduction Pre-trained Transformer architectures have led to huge progress in building more human-like language processing systems (Radford et al.; Devlin et al., 2019; Brown et al., 2020, among others). These models achieve impressive perplexity results on language modelling datasets, perform well on grammatical judgments (Warstadt et al., 2020), and provide useful linguistic representations that benefit a wide range of downstream tasks. Probing analyses also suggest that these models learn to implicitly encode syntactic information (Hewitt and Manning, 2019; Clark et al., 2019) that may support better linguistic generalization than recurrent neural network architectures (RNNs). However, the Transformer architecture (Vaswani et al., 2017) is an interesting subject of study beyond its success in transfer-learning settings. Transformer models lack the inductive biases of RNNs. Rather than maintaining vector-valued state and updating it in a recurrent manner, auto-regressive Transformer models encode all past decisions simultaneously at each inference step, thanks to a self-attention mechanism. The only notion of sequence order is also given by position embeddings summe"
2021.acl-long.289,N19-1423,0,0.0391228,"er language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP. Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training. 1 Introduction Pre-trained Transformer architectures have led to huge progress in building more human-like language processing systems (Radford et al.; Devlin et al., 2019; Brown et al., 2020, among others). These models achieve impressive perplexity results on language modelling datasets, perform well on grammatical judgments (Warstadt et al., 2020), and provide useful linguistic representations that benefit a wide range of downstream tasks. Probing analyses also suggest that these models learn to implicitly encode syntactic information (Hewitt and Manning, 2019; Clark et al., 2019) that may support better linguistic generalization than recurrent neural network architectures (RNNs). However, the Transformer architecture (Vaswani et al., 2017) is an interesting"
2021.acl-long.289,2020.acl-main.591,0,0.0350639,"Missing"
2021.acl-long.289,N16-1024,0,0.48788,"effectively induce human-like syntactic generalization. This work hypothesizes that the Transformer language model may benefit from explicit generative structural supervision to systematically generalize syntactic knowledge. Here we explore two major classes of structural guidance for Transformer language models based on joint modeling of language and constituency parses. The “generative parsing as language modeling” approach builds a Transformer-parameterized model to learn to predict actions that incrementally build constituency trees along with terminal words, following prior work on RNNs (Dyer et al., 2016; Choe and Charniak, 2016). The “structural scaffolding” approach follows the general idea of regularizing hidden representation through multi-task learning objective, with prior success in various NLP tasks (Zhang S y0:1 NP hBOSi VP NT ( S ) y1:2 NT ( NP ) w0 The birds sang y2:3 The birds w1 REDUCE NT ( VP ) w2 sang NT ( ADVP ) ··· w3 ADVP y0:1 w1 w2 w3 NT ( S ) NT ( NP ) w0 w1 w2 hBOSi (a) Vanilla language model y3:4 The NT ( S ) NT ( NP ) y1:2 y2:3 y0:1 hPADi y1:2 birds REDUCE w1 w2 w3 w1 w2 w3 The birds w0 w1 w2 w0 w1 w2 (b) Parsing as Language Modelling (c) Language models with Structural"
2021.acl-long.289,P81-1022,0,0.640942,"he parameters they can be considered a case of multi-task learning (Caruana, 1997). Of interest here is the model proposed in Recurrent Neural Network Grammars (RNNGs; Dyer et al., 2016) and parsing as language model (LSTM-LM; Choe and Charniak, 2016). Both approaches model the joint distribution of words W and constituency tree components Y as p(Y, W ) = p(a1 , · · · , aR ) = R Y p(at |a<t ) (2) t=1 where at are transitions of a state machine that generates both the sentence and the tree. These transitions are similar to the well-established transition sets used for transition-based parsing (Earley, 1970) but adapted to generate both text and parse simultaneously. For the reminder of this work, we will consider each at to be integer valued and indexing a dictionary of transitions. A transition a can be a word w or a transition action that generates a component of the constituency tree y. The actions include non-terminal symbols that open and label a new constituent with the label x, indicated as NT(x), or a REDUCE action closing the closest open constituent. An example of a partial parse tree and transitions can be found at the top of Figure 1. RNNG and LSTM-LM parametrize the same factorizati"
2021.acl-long.289,N19-1004,1,0.883267,"Missing"
2021.acl-long.289,D19-1538,0,0.0225821,"nduce first-order logic representation in a unsupervised fashion, by clustering the dependency structures. In both cases syntax forms part of a pipeline and is not strictly supervision for the end task. This trend continued with the rise of neural models. Collobert et al. (2011) improved deep convolution neural network for syntactic chunking models with additional POS supervision. Zhang and Weiss (2016); Søgaard and Goldberg (2016) observe the benefits of POS supervision at different depths of a neural network model with impact on dependency parsing, tagging and CCG super tagging performance. He et al. (2019) perform a syntax-based pruning of semantic roles, showing benefits in a multilingual setting. More recently, Sachan et al. (2020) incorporate a syntactic graph recurrent neural network into BERT models for better semantic role labeling. However, their method shows little or no benefit of syntax modeling for Named Entity Recognition and relation linking task. Neural machine translation (Chen et al., 2018) and text generation (Li et al., 2020a) have also been shown to benefit from syntactic modeling. In a recent work, Li et al. (2020b) use syntactic modeling in BERT based transformers to achiev"
2021.acl-long.289,N19-1419,0,0.0302156,"dels without the need for data intensive pre-training. 1 Introduction Pre-trained Transformer architectures have led to huge progress in building more human-like language processing systems (Radford et al.; Devlin et al., 2019; Brown et al., 2020, among others). These models achieve impressive perplexity results on language modelling datasets, perform well on grammatical judgments (Warstadt et al., 2020), and provide useful linguistic representations that benefit a wide range of downstream tasks. Probing analyses also suggest that these models learn to implicitly encode syntactic information (Hewitt and Manning, 2019; Clark et al., 2019) that may support better linguistic generalization than recurrent neural network architectures (RNNs). However, the Transformer architecture (Vaswani et al., 2017) is an interesting subject of study beyond its success in transfer-learning settings. Transformer models lack the inductive biases of RNNs. Rather than maintaining vector-valued state and updating it in a recurrent manner, auto-regressive Transformer models encode all past decisions simultaneously at each inference step, thanks to a self-attention mechanism. The only notion of sequence order is also given by posi"
2021.acl-long.289,D19-1376,0,0.0479667,"Missing"
2021.acl-long.289,D09-1001,0,0.0329735,"ategory and binding PLM variants seem inferior to all other models. 4 Related Work Multitask learning (Caruana, 1997) has been applied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a). A recurring theme has been the use of structure in the form of syntactic trees to benefit other NLP tasks. Among the early works exploring this direction, Punyakanok et al. (2008) showed that syntactic parses can benefit Semantic Role Labeling (SRL). Poon and Domingos (2009) extended this idea to induce first-order logic representation in a unsupervised fashion, by clustering the dependency structures. In both cases syntax forms part of a pipeline and is not strictly supervision for the end task. This trend continued with the rise of neural models. Collobert et al. (2011) improved deep convolution neural network for syntactic chunking models with additional POS supervision. Zhang and Weiss (2016); Søgaard and Goldberg (2016) observe the benefits of POS supervision at different depths of a neural network model with impact on dependency parsing, tagging and CCG sup"
2021.acl-long.289,2020.acl-main.158,1,0.918599,"uage models with Structural Scaffold Figure 1: Top: Illustration of a partial constituency tree and corresponding transitions. Bottom: unidirectional transformer language model (a) without explicit structural supervision, (b) for modelling generative action parsing sequence, and (c) with structural scaffold for predicting the local incremental parsing state. and Weiss, 2016; Søgaard and Goldberg, 2016; Swayamdipta et al., 2018). We test these two approaches on two subsets of the BLLIP dataset (Charniak et al., 2000) and evaluate models’ syntactic generalization performances on SG Test Suites (Hu et al., 2020) and a sampled subset of the BLiMP Benchmark (Warstadt et al., 2020). We show evidence that generative structural supervision indeed induces more robust and human-like linguistic generalization in Transformer language models and explore the different trade-offs involved in the presented methods. 2 Models Here we explore joint modelling of structures and words parametrized with Transformers by considering both a sentence W and its constituency parse Y and modeling the joint distribution P (W, Y ). 2.1 Generative Parsing as Language Modeling A language model can be described formally as a probab"
2021.acl-long.289,2020.acl-main.467,0,0.0377753,"Missing"
2021.acl-long.289,P18-1249,0,0.0184951,"r goal here is to study whether structural guidance helps models learn robust humanlike generalization of syntactic knowledge, we train our model on the BLLIP dataset (Charniak et al., 2000), an English newswire style corpus used in Hu et al. (2020). This makes the results here more comparable to the results reported in previous work, especially with RNNGs. We train the proposed models and the baseline vanilla Transformer language models on BLLIP- MD, a 14 million-token corpus, and BLLIP- LG, a 46 million-token corpus, both of which are auto-parsed using a state-of-theart constituency parser (Kitaev and Klein, 2018). We used the parsed sentences to generate oracle parsing action sequence for PLM and PLM-mask. We collected a list of word-synchronous parsing 1 Code available at https://github.com/IBM/ transformers-struct-guidance action sequences from the train and development oracle of BLLIP- LG and use it to parametrize the action n-gram vocabulary of ScLMs trained on both BLLIP- MD and BLLIP- LG. There are 3756 action n-gram types from the corpora, including one padding token and one blank token. All models were trained with learning rate 10−5 , AdamW optimizer, and minibatch of size 5. We trained the m"
2021.acl-long.289,J08-2005,0,0.0403732,"to the PLM model, even outperforming it clearly for the gross syntactic state suites. In this category and binding PLM variants seem inferior to all other models. 4 Related Work Multitask learning (Caruana, 1997) has been applied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a). A recurring theme has been the use of structure in the form of syntactic trees to benefit other NLP tasks. Among the early works exploring this direction, Punyakanok et al. (2008) showed that syntactic parses can benefit Semantic Role Labeling (SRL). Poon and Domingos (2009) extended this idea to induce first-order logic representation in a unsupervised fashion, by clustering the dependency structures. In both cases syntax forms part of a pipeline and is not strictly supervision for the end task. This trend continued with the rise of neural models. Collobert et al. (2011) improved deep convolution neural network for syntactic chunking models with additional POS supervision. Zhang and Weiss (2016); Søgaard and Goldberg (2016) observe the benefits of POS supervision at d"
2021.acl-long.289,P18-1132,0,0.0490279,"Missing"
2021.acl-long.289,2020.tacl-1.50,0,0.0736996,"Missing"
2021.acl-long.289,D18-1151,0,0.0419679,"of size 5. We trained the models with multiple seeds within the capacity of our resources, in order to accommodate potential variance. In total, there are three seeds of LM, four of ScLM-past, four of ScLM-next, three of PLM, and three of PLM-mask for BLLIP- MD, and the same number of seeds of each model type for BLLIP- LG. Models were trained until convergence, as suggested by the loss of the development set during training. 3.2 Targeted Syntactic Evaluation To assess whether a trained model systematically generalizes its syntactic knowledge, we employ targeted syntactic evaluation paradigm (Marvin and Linzen, 2018). Specifically, we measure models’ performance on two held-out test datasets, a collection of syntactic generalization test suites from Hu et al. (2020) and BLiMP Benchmark from Warstadt et al. (2020). These two datasets cover a wide range of English syntactic phenomena. Tests from Hu et al. (2020), which we refer as SG Test Suites, consist of hand-designed test suites for evaluating fine-grained syntactic generalization in incremental processing of a linguistic input. The general method is to compare models’ surprisals p(continuation|prefix) of grammatical and ungrammatical continuations give"
2021.acl-long.289,A00-2030,0,0.126104,"ncremental processing phenomenon such as garden-path effects, but seems to slightly hurt the performance on gross syntactic state. While overall the RNNG shows a poor performance this is mostly due to its very low scores for licensing suites. Excluding these suites only the RNNG shows a performance close to the PLM model, even outperforming it clearly for the gross syntactic state suites. In this category and binding PLM variants seem inferior to all other models. 4 Related Work Multitask learning (Caruana, 1997) has been applied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a). A recurring theme has been the use of structure in the form of syntactic trees to benefit other NLP tasks. Among the early works exploring this direction, Punyakanok et al. (2008) showed that syntactic parses can benefit Semantic Role Labeling (SRL). Poon and Domingos (2009) extended this idea to induce first-order logic representation in a unsupervised fashion, by clustering the dependency structures. In both cases syntax forms part of a pipeline and is not strict"
2021.acl-long.289,2021.naacl-main.132,0,0.0571289,"Missing"
2021.acl-long.289,P16-2038,0,0.497271,"w2 w3 NT ( S ) NT ( NP ) w0 w1 w2 hBOSi (a) Vanilla language model y3:4 The NT ( S ) NT ( NP ) y1:2 y2:3 y0:1 hPADi y1:2 birds REDUCE w1 w2 w3 w1 w2 w3 The birds w0 w1 w2 w0 w1 w2 (b) Parsing as Language Modelling (c) Language models with Structural Scaffold Figure 1: Top: Illustration of a partial constituency tree and corresponding transitions. Bottom: unidirectional transformer language model (a) without explicit structural supervision, (b) for modelling generative action parsing sequence, and (c) with structural scaffold for predicting the local incremental parsing state. and Weiss, 2016; Søgaard and Goldberg, 2016; Swayamdipta et al., 2018). We test these two approaches on two subsets of the BLLIP dataset (Charniak et al., 2000) and evaluate models’ syntactic generalization performances on SG Test Suites (Hu et al., 2020) and a sampled subset of the BLiMP Benchmark (Warstadt et al., 2020). We show evidence that generative structural supervision indeed induces more robust and human-like linguistic generalization in Transformer language models and explore the different trade-offs involved in the presented methods. 2 Models Here we explore joint modelling of structures and words parametrized with Transfor"
2021.acl-long.289,D17-1178,0,0.0988413,"earnt with an LSTM (Hochreiter and Schmidhuber, 1997). It should also be noted that the LSTMLM is designed as a parser, while RNNG is also used as a language model. In order to derive a language model from a joint model, it is is necessary to marginalize over all possible parse trees X p(W ) = p(Y, W ) (3) Y ∈Y(W ) which is an intractable problem since there is an exponentially large number of possible trees. The original RNNG work (Dyer et al., 2016) proposes an approximate solution based on importance sampling. In this work we use the word-synchronous beam search approximation introduced in Stern et al. (2017). The marginalized likelihood language model in Equation 3 is desirable because it makes no statistical independence assumption between language and syntax and shares all parameters across both tasks, with the exception of action specific embeddings. Particularly relevant for this work is the fact that both word and non-word transitions are predicted as language model output indiscriminately and are available at each prediction step through its history a<t . In this work we propose to parametrize Eq 2 with a Transformer language model (Vaswani et al., 2017). This is equivalent to the flat para"
2021.acl-long.289,D18-1548,0,0.0984704,"Missing"
2021.acl-long.289,W05-0636,0,0.12263,"phenomenon such as garden-path effects, but seems to slightly hurt the performance on gross syntactic state. While overall the RNNG shows a poor performance this is mostly due to its very low scores for licensing suites. Excluding these suites only the RNNG shows a performance close to the PLM model, even outperforming it clearly for the gross syntactic state suites. In this category and binding PLM variants seem inferior to all other models. 4 Related Work Multitask learning (Caruana, 1997) has been applied to a variety of NLP tasks with traditional modeling approaches (Miller et al., 2000; Sutton and McCallum, 2005; Sutton et al., 2007) as well as more recent neural models (Collobert et al., 2011; Li et al., 2020a). A recurring theme has been the use of structure in the form of syntactic trees to benefit other NLP tasks. Among the early works exploring this direction, Punyakanok et al. (2008) showed that syntactic parses can benefit Semantic Role Labeling (SRL). Poon and Domingos (2009) extended this idea to induce first-order logic representation in a unsupervised fashion, by clustering the dependency structures. In both cases syntax forms part of a pipeline and is not strictly supervision for the end"
2021.acl-long.289,D18-1412,0,0.110089,"w1 w2 hBOSi (a) Vanilla language model y3:4 The NT ( S ) NT ( NP ) y1:2 y2:3 y0:1 hPADi y1:2 birds REDUCE w1 w2 w3 w1 w2 w3 The birds w0 w1 w2 w0 w1 w2 (b) Parsing as Language Modelling (c) Language models with Structural Scaffold Figure 1: Top: Illustration of a partial constituency tree and corresponding transitions. Bottom: unidirectional transformer language model (a) without explicit structural supervision, (b) for modelling generative action parsing sequence, and (c) with structural scaffold for predicting the local incremental parsing state. and Weiss, 2016; Søgaard and Goldberg, 2016; Swayamdipta et al., 2018). We test these two approaches on two subsets of the BLLIP dataset (Charniak et al., 2000) and evaluate models’ syntactic generalization performances on SG Test Suites (Hu et al., 2020) and a sampled subset of the BLiMP Benchmark (Warstadt et al., 2020). We show evidence that generative structural supervision indeed induces more robust and human-like linguistic generalization in Transformer language models and explore the different trade-offs involved in the presented methods. 2 Models Here we explore joint modelling of structures and words parametrized with Transformers by considering both a"
2021.acl-long.289,D19-1098,0,0.0496292,"Missing"
2021.acl-long.289,D19-1286,0,0.0301139,"Missing"
2021.acl-long.289,N19-1334,1,0.887511,"arning settings. Transformer models lack the inductive biases of RNNs. Rather than maintaining vector-valued state and updating it in a recurrent manner, auto-regressive Transformer models encode all past decisions simultaneously at each inference step, thanks to a self-attention mechanism. The only notion of sequence order is also given by position embeddings summed to content embeddings in both input and auto-regressive signals. Previous works have shown the advantage of structural supervision in RNNs in learning to maintain syntactic states and non-local dependencies (Kuncoro et al., 2018; Wilcox et al., 2019; Futrell et al., 2019). It remains an open question whether Transformer language models can similarly benefit from generative structural supervision, and what form of structural supervision would more effectively induce human-like syntactic generalization. This work hypothesizes that the Transformer language model may benefit from explicit generative structural supervision to systematically generalize syntactic knowledge. Here we explore two major classes of structural guidance for Transformer language models based on joint modeling of language and constituency parses. The “generative parsing"
2021.acl-long.289,P16-1147,0,0.0454269,"Missing"
2021.acl-long.76,N16-1024,0,0.0318413,"e ‘BIG LSTM+CNN Inputs’ from Jozefowicz et al. (2016). It was trained on the One Billion Word Benchmark (Chelba et al., 2013) with two hidden layers of 8196 units each and CNN character embeddings as input. GRNN is the best-performing model described in the supplementary materials of Gulordava et al. (2018). It was trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units. GPT-2 is the model presented in Radford et al. (2019), and was trained on 40GB of internet text. We use the version of GPT-2 available through the Language Modeling Zoo distribution3 RNNG (Dyer et al., 2016) jointly models a sentence as well as its syntactic parse. The model explicitly represents parse trees and composes partially built phrase structures. Models are supervised with Penn-Treebank style parses during training. We use the average of the three RNNG-BLLIP-LG models from Hu et al. (2020). 942 3 https://cpllab.github.io/lm-zoo/index. html#welcome-to-lm-zoo Accuracy/Consistency Scores Human RTs vs. Model Surprisals Cleft FGD-obj FGD-pp FGD-sbj MVRR NPL-any-orc NPL-any-src NPL-ever-orc Score 1.00 0.75 0.50 0.25 0.00 1.00 0.75 0.50 0.25 0.00 model human gpt2 NPL-ever-src RNA-f-orc RNA-f-sr"
2021.emnlp-main.454,2020.acl-main.179,0,0.0517235,"Missing"
2021.emnlp-main.454,N16-1024,0,0.18315,"rence on Empirical Methods in Natural Language Processing, pages 5604–5620 c November 7–11, 2021. 2021 Association for Computational Linguistics We design six classes of Mandarin test suites covering a range of syntactic and semantic relationships, some specific to Mandarin and some comparable to English. We train neural language models with differing inductive biases on two datasets of different sizes, and compare models’ performance on our targeted evaluation materials. While most prior work investigating syntactically guided language models has used Recurrent Neural Network Grammar models (Dyer et al., 2016) – potentially conflating structural supervision with a particular parameterization – this work further explores structured Transformer language models (Qian et al., 2021). Our results are summarized as follows. We find that structural supervision yields greatest performance advantages in low-data settings, in line with prior work on English language models. Our results also suggest a potential benefit of structural supervision in deriving garden-path effects induced by local classifier–noun mismatch, and in maintaining syntactic expectations across intervening content within a dependency rela"
2021.emnlp-main.454,N19-1004,1,0.888287,"Missing"
2021.emnlp-main.454,Q16-1037,0,0.16431,"corpus studies suggest that the average dependency length of Mandarin Chinese sentences is larger than that of English sentences (Jiang and Liu, 2015), with potential implications for language modeling. On 1 Introduction the one hand, the need to track input across long deA rich collection of targeted linguistic evaluations pendencies may make structural supervision more has shown that neural language models can surpris- beneficial for Mandarin Chinese language models; ingly learn many aspects of grammar from unla- on the other hand, the prevalence of these depenbeled linguistic input (e.g., Linzen et al., 2016; Gu- dencies may make it easier for them to learn to lordava et al., 2018; Warstadt et al., 2020; Hu et al., maintain non-local information without explicitly 2020; Xiang et al., 2021). There is also growing ev- modeling syntax. Other fine-grained differences idence that explicit modeling of syntax helps neural in typology also affect the types of syntactic tests network-based language models represent syntactic that can be conducted. For example, since relative state and exhibit human-like processing behaviors clauses precede the head noun in Chinese (unlike in of non-local grammatical depen"
2021.emnlp-main.454,J93-2004,0,0.0747493,"ross different conditions in a test item, we do tie-breaking by randomly flipping a fair coin to determine the outcome for that particular item. 3.4 Corpus Data We consider two datasets to explore how training data size affects models ability to acquire grammatical knowledge (similar to Hu et al., 2020). The LSTM and Transformer models are trained on the raw text only, whereas the RNNG and PLM models are tained with additional syntactic annotations. Chinese Treebank (CTB) The Chinese Treebank (CTB 9.0; Xue, Nianwen et al., 2016) is a Chinese language corpus annotated with Penn Treebank-style (Marcus et al., 1993) constituency parses. We use the Newswire, Magazine articles, Broadcast news, Broadcast conversations, and Weblogs sections, as we expect these sources to contain well-formed sentences with a variety of syntactic constructions. We follow the split defined by Shao et al. (2017) to construct training, development, and test sets. Xinhua News Data To investigate the effects of increased training data size on models’ syntactic generalization, we create a larger corpus combining CTB with a subset from the Xinhua News corpus (Wu, Zhibiao, 1995).4 The Xinhua corpus contains metadata and content for 40"
2021.emnlp-main.454,D18-1151,0,0.0199398,"r form ‘drinks’ agrees with the subject, (A) a positive surprisal difference (ungrammatical − is grammatical, whereas (B) is not. grammatical). If this criterion is satisfied, then the The closest work to ours is the corpus of Chi- model achieves a success score of 1, and 0 othernese linguistic minimal pairs (CLiMP; Xiang et al., wise. These binary scores are averaged over test 2021), which provides a benchmark for testing suite items and/or classes to obtain accuracy scores. 5605 Linguistic minimal pairs have been used to construct syntactic test suites in English (e.g., Linzen et al., 2016; Marvin and Linzen, 2018; Mueller et al., 2020; Davis and van Schijndel, 2020; Warstadt et al., 2020) and other languages such as Italian, Spanish, French, and Russian (Ravfogel et al., 2018; Gulordava et al., 2018; An et al., 2019; Mueller et al., 2020; Davis and van Schijndel, 2020). A minimal pair is formed by two sentences that differ in grammaticality or acceptability but are otherwise matched in structure and lexical content. (A) and (B) form an example English minimal pair differing in subject-verb agreement: 3.2 Test Suites We organize our materials into six classes of test suites, each of which assesses mode"
2021.emnlp-main.454,2020.acl-main.490,0,0.0169385,"th the subject, (A) a positive surprisal difference (ungrammatical − is grammatical, whereas (B) is not. grammatical). If this criterion is satisfied, then the The closest work to ours is the corpus of Chi- model achieves a success score of 1, and 0 othernese linguistic minimal pairs (CLiMP; Xiang et al., wise. These binary scores are averaged over test 2021), which provides a benchmark for testing suite items and/or classes to obtain accuracy scores. 5605 Linguistic minimal pairs have been used to construct syntactic test suites in English (e.g., Linzen et al., 2016; Marvin and Linzen, 2018; Mueller et al., 2020; Davis and van Schijndel, 2020; Warstadt et al., 2020) and other languages such as Italian, Spanish, French, and Russian (Ravfogel et al., 2018; Gulordava et al., 2018; An et al., 2019; Mueller et al., 2020; Davis and van Schijndel, 2020). A minimal pair is formed by two sentences that differ in grammaticality or acceptability but are otherwise matched in structure and lexical content. (A) and (B) form an example English minimal pair differing in subject-verb agreement: 3.2 Test Suites We organize our materials into six classes of test suites, each of which assesses models’ knowledge of a par"
2021.emnlp-main.454,2021.acl-long.289,1,0.926066,"andarin test suites covering a range of syntactic and semantic relationships, some specific to Mandarin and some comparable to English. We train neural language models with differing inductive biases on two datasets of different sizes, and compare models’ performance on our targeted evaluation materials. While most prior work investigating syntactically guided language models has used Recurrent Neural Network Grammar models (Dyer et al., 2016) – potentially conflating structural supervision with a particular parameterization – this work further explores structured Transformer language models (Qian et al., 2021). Our results are summarized as follows. We find that structural supervision yields greatest performance advantages in low-data settings, in line with prior work on English language models. Our results also suggest a potential benefit of structural supervision in deriving garden-path effects induced by local classifier–noun mismatch, and in maintaining syntactic expectations across intervening content within a dependency relation. These findings suggest that the benefits of hierarchical inductive biases in acquiring dependency relationships may not be specific to English. 2 Targeted Linguistic"
2021.emnlp-main.454,W18-5412,0,0.0211369,"isfied, then the The closest work to ours is the corpus of Chi- model achieves a success score of 1, and 0 othernese linguistic minimal pairs (CLiMP; Xiang et al., wise. These binary scores are averaged over test 2021), which provides a benchmark for testing suite items and/or classes to obtain accuracy scores. 5605 Linguistic minimal pairs have been used to construct syntactic test suites in English (e.g., Linzen et al., 2016; Marvin and Linzen, 2018; Mueller et al., 2020; Davis and van Schijndel, 2020; Warstadt et al., 2020) and other languages such as Italian, Spanish, French, and Russian (Ravfogel et al., 2018; Gulordava et al., 2018; An et al., 2019; Mueller et al., 2020; Davis and van Schijndel, 2020). A minimal pair is formed by two sentences that differ in grammaticality or acceptability but are otherwise matched in structure and lexical content. (A) and (B) form an example English minimal pair differing in subject-verb agreement: 3.2 Test Suites We organize our materials into six classes of test suites, each of which assesses models’ knowledge of a particular linguistic phenomenon.1 Within each class, we develop individual test suites with different types of modifiers that intervene between th"
2021.emnlp-main.454,I17-1018,0,0.0187248,"milar to Hu et al., 2020). The LSTM and Transformer models are trained on the raw text only, whereas the RNNG and PLM models are tained with additional syntactic annotations. Chinese Treebank (CTB) The Chinese Treebank (CTB 9.0; Xue, Nianwen et al., 2016) is a Chinese language corpus annotated with Penn Treebank-style (Marcus et al., 1993) constituency parses. We use the Newswire, Magazine articles, Broadcast news, Broadcast conversations, and Weblogs sections, as we expect these sources to contain well-formed sentences with a variety of syntactic constructions. We follow the split defined by Shao et al. (2017) to construct training, development, and test sets. Xinhua News Data To investigate the effects of increased training data size on models’ syntactic generalization, we create a larger corpus combining CTB with a subset from the Xinhua News corpus (Wu, Zhibiao, 1995).4 The Xinhua corpus contains metadata and content for 406K Mandarin news articles, collected from three mainstream media sources. Only the article contents are used for our training purposes. The texts from Xinhua corpus are first split into sentences and then tokenized into words with SpaCy (Honnibal et al., 2020). We then obtain"
2021.emnlp-main.454,D17-1178,0,0.0161581,"vocabulary for each training dataset. Model “If he doesn’t try, he will lose the opportunity.” sizes are reported in Table 3a in Appendix B. ∗(6.b) 如果 他 不 尝试 。 For the LSTM and Tranformer models, we calif he NEG try . culate the surprisal at the target region by taking “If he doesn’t try.” the negative log of the model’s predicted condiIn this case, we test the surprisal at the sentence- tional probability. We estimate the RNNGs and final period. If the model correctly represents the PLMs’ word surprisals with word-synchronous gross syntactic state within the subordinate clause, beam search (Stern et al., 2017), following Hale et al. (2018) and Wilcox et al. (2019). The action 3 We do not consider the conventional modifier types used beam size is 100 and the word beam size is 10. For in the other test suite classes because adding an ORC can be confounding — the language model might be surprised at the regions with multi-token content, we sum over the ungrammatical target region due to the RC verb instead of the probabilities of each token. main transitive verb. Therefore, we instead focus on SRCs for this particular class. As a baseline, we additionally implement an n5608 gram model with Kneser-Ney"
2021.emnlp-main.454,2020.tacl-1.25,0,0.167563,"ger than that of English sentences (Jiang and Liu, 2015), with potential implications for language modeling. On 1 Introduction the one hand, the need to track input across long deA rich collection of targeted linguistic evaluations pendencies may make structural supervision more has shown that neural language models can surpris- beneficial for Mandarin Chinese language models; ingly learn many aspects of grammar from unla- on the other hand, the prevalence of these depenbeled linguistic input (e.g., Linzen et al., 2016; Gu- dencies may make it easier for them to learn to lordava et al., 2018; Warstadt et al., 2020; Hu et al., maintain non-local information without explicitly 2020; Xiang et al., 2021). There is also growing ev- modeling syntax. Other fine-grained differences idence that explicit modeling of syntax helps neural in typology also affect the types of syntactic tests network-based language models represent syntactic that can be conducted. For example, since relative state and exhibit human-like processing behaviors clauses precede the head noun in Chinese (unlike in of non-local grammatical dependencies, including English), we can manipulate the distance of a verb– number agreement (Kuncoro"
2021.emnlp-main.454,N19-1334,1,0.911482,"CLiMP focuses on building a comprehensive challenge set, the current work performs controlled experiments to investigate the effect of structural supervision on language models’ ability to learn syntactic and semantic relationships. Moreover, the test items in CLiMP are (semi)-automatically generated, which may result in semantically anomalous sentences and introduce noise into the evaluation phase. In contrast, we manually construct the items in our test suites to sound as natural as possible. 3 3.1 Methods Evaluation Paradigm The general structure of our evaluation paradigm follows that of Wilcox et al. (2019) and Hu et al. (2020). We use surprisal as a linking function between the language model output and human expectations (Hale, 2001). Surprisal is defined as the inverse log probability of a word (wi ) conditioned on the preceding words in the same context (w1 . . . wi−1 ): surprisal(wi ) = log 1 p(wi |w1 , . . . wi−1 ) Our test suites take the form of a group of handwritten, controlled sentence sets. Each sentence set, or test item, contains at least two minimally differing sentences, and each sentence contains a stimulus prefix and a downstream target region. The content of the target region"
2021.emnlp-main.454,2021.eacl-main.242,0,0.0247719,"language modeling. On 1 Introduction the one hand, the need to track input across long deA rich collection of targeted linguistic evaluations pendencies may make structural supervision more has shown that neural language models can surpris- beneficial for Mandarin Chinese language models; ingly learn many aspects of grammar from unla- on the other hand, the prevalence of these depenbeled linguistic input (e.g., Linzen et al., 2016; Gu- dencies may make it easier for them to learn to lordava et al., 2018; Warstadt et al., 2020; Hu et al., maintain non-local information without explicitly 2020; Xiang et al., 2021). There is also growing ev- modeling syntax. Other fine-grained differences idence that explicit modeling of syntax helps neural in typology also affect the types of syntactic tests network-based language models represent syntactic that can be conducted. For example, since relative state and exhibit human-like processing behaviors clauses precede the head noun in Chinese (unlike in of non-local grammatical dependencies, including English), we can manipulate the distance of a verb– number agreement (Kuncoro et al., 2018), negative object dependency by inserting relative clauses in polarity lice"
2021.emnlp-main.454,N18-1181,1,0.765032,"Missing"
2021.emnlp-main.74,W18-4605,0,0.0218524,"hesis in language production span levels of linguistic structure: from phonetics (Aylett and Turk, 2004) to lexical choice (Mahowald et al., 2013), to syntax (Jaeger, 2010), and to discourse (Torabi Asr and Demberg 2015) (though see Zhan and Levy 2018, 2019). Despite this evidence, there are several aspects of the UID hypothesis that lack clarity or unity. For example, there is a dearth of converging evidence from studies in language comprehension. Furthermore, multiple candidate operationalizations of UID have been proposed, each without formal justification for their choices (Collins, 2014; Jain et al., 2018; Meister et al., 2020; Wei et al., 2021). In this work, we attempt to shed light on these issues: we first study the relationship between the dis1 Introduction tribution of information content throughout a senThe uniform information density (UID) hypothesis tence and native speakers’ (i) sentence-level read(Fenk and Fenk, 1980; Levy and Jaeger, 2007) ing times and (ii) sentence acceptability judgments. states that language users prefer when information While our results for sentence-level reading times content (measured information-theoretically as do not contradict previous word-level readin"
2021.emnlp-main.74,2021.acl-long.405,0,0.0226906,"quadratic test might be too restrictive. Our approach, which explores a more fine-grained range of k, is potentially more comprehensive, and indeed we find that values of k slightly greater than 1 often fit the data at least as well as k = 1, and can certainly not be ruled out. Other potential virtues of our analysis are (1) Our analysis is performed at the sentence- (rather than word-) level. This is arguably a better method for analyzing a sequence-level phenomenon, i.e., UID, and (2) specifically for eye movement data, we include re-reading times after the first pass. called into question (Kuribayashi et al., 2021). As such, while we find convincing preliminary evidence in our analyzed languages, we are not able to fully test the hypothesis that the pressure for UID is at the language-level. Further, we have no evidence as to whether there may be pressure towards a cross-linguistic µc , which would be relevant to cross-linguistic interpretations of UID (Pimentel et al., 2021). Another important limitation of this work is the restriction to psychometric data from the written domain. To fully grasp the effects of the distribution of information in linguistic signals on language comprehension, spoken langu"
2021.emnlp-main.74,P16-1162,0,0.013764,"Missing"
2021.emnlp-main.74,N18-1181,1,0.785228,"oefficient between (negative) sum of surprisals raised to the k th power and linguistic acceptability judgments of a sentence. The higher correlation when k > 1 implies sentences with a more uniform distribution of information are more acceptable. surprisal) is distributed as smoothly as possible throughout an utterance. The studies adduced in support of this hypothesis in language production span levels of linguistic structure: from phonetics (Aylett and Turk, 2004) to lexical choice (Mahowald et al., 2013), to syntax (Jaeger, 2010), and to discourse (Torabi Asr and Demberg 2015) (though see Zhan and Levy 2018, 2019). Despite this evidence, there are several aspects of the UID hypothesis that lack clarity or unity. For example, there is a dearth of converging evidence from studies in language comprehension. Furthermore, multiple candidate operationalizations of UID have been proposed, each without formal justification for their choices (Collins, 2014; Jain et al., 2018; Meister et al., 2020; Wei et al., 2021). In this work, we attempt to shed light on these issues: we first study the relationship between the dis1 Introduction tribution of information content throughout a senThe uniform information"
2021.findings-acl.76,P18-1198,0,0.179211,"al., 2020). These connectionist models, trained on large corpora in a largely unsupervised manner, learn to map words into numerical representations, or embeddings, that support languagereasoning tasks. Fine-tuning these models on tasks like extractive question answering specializes these generic models into performant, task-specific models (Wolf et al., 2019). In conjunction with the rise of these powerful neural models, researchers have investigated what the models have learned. Probes, tools built to reveal properties of a trained model, are a favored approach (Hall Maudslay et al., 2020; Conneau et al., 2018). For example, Hewitt and Manning (2019) have uncovered compelling evidence that several models encode syntactic information in their embeddings. That is, by passing embeddings through a trained probe, one may recover information about a sentence’s syntax. Although these results are impressive, they fall short of clearly demonstrating what linguistic information the language models actually use. Syntactic information is present in sentences; that embeddings also encode syntax does not imply that a model uses syntactic knowledge. In order to truly query a model’s understanding, one must use cau"
2021.findings-acl.76,N19-1423,0,0.180208,"technique, we produce evidence that suggests some BERT-based models use a tree-distancelike representation of syntax in downstream prediction tasks. 1 Roger P. Levy MIT rplevy@mit.edu Probe M “was” z0 “were” Figure 1: A language model, M , outputs predictions and a probe estimates properties from the model representation. We use probes to generate counterfactual representations, z 0 , based on syntactic manipulations, revealing reasoning within the model. Introduction Large neural models like BERT and GPT-3 have established a new state of the art in a variety of challenging linguistic tasks (Devlin et al., 2019; Brown et al., 2020). These connectionist models, trained on large corpora in a largely unsupervised manner, learn to map words into numerical representations, or embeddings, that support languagereasoning tasks. Fine-tuning these models on tasks like extractive question answering specializes these generic models into performant, task-specific models (Wolf et al., 2019). In conjunction with the rise of these powerful neural models, researchers have investigated what the models have learned. Probes, tools built to reveal properties of a trained model, are a favored approach (Hall Maudslay et a"
2021.findings-acl.76,J93-2004,0,0.0787571,"ded similar results. On a Linux desktop with an Nvidia GEForce RTX 2080 graphics card, generating a single counterfactual took less than 1 minute, and the process was easily parallelized to batches of 80 embeddings, reducing the mean computation time to under one second. For both the QA and Mask models, we trained all probe types (depth, distance, 2-layer dist, and 3-layer dist) on each of the model’s 25 layers. We used 5000 entries from the Penn Treebank (PTB) for training, with the standard validation and test sets of nearly 4000 entries used for early stopping and evaluation, respectively (Marcus et al., 1993). 4.5 Metrics We used two sets of metrics in our experiments. First, we measured probe performance using the Root Accuracy, UUAS, and Spearman Coefficient metrics used by Hewitt and Manning (2019) and refer to their work for details. Intuitively, these metrics captured how accurately the probes predicted aspects of syntactic structure from embeddings. Second, we measured changes in model outputs when using counterfactual embeddings. The Mask model produced a probability distribution over more than 30,000 possible words for the masked location, but we restricted our attention to only a subset o"
2021.findings-acl.76,D19-1050,1,0.811197,"that the counterfactual embeddings induced the correct structure, indicating that the QA model simply did not use such structure in downstream predictions. Furthermore, given the success of our probes and technique with the Mask model, these poor results for the QA model suggest (but admittedly cannot definitely prove) that it may not have learned to use the syntactic information detected by the probes. This theory is consistent with prior art that finds that fine-tuning on specific tasks, as was done for the QA model, worsens the alignment between model and human representations of language (Gauthier and Levy, 2019). 869 QA Model Likelihood of NP1 Start by Layer Figure 7: Mean effects of using counterfactual updates from the 3-layer dist probe on the QA model for the RC (top) and NP/VP (bottom) corpora. 6 Conclusion In this work, we proposed and evaluated a new technique for producing counterfactual embeddings that tested syntactic understanding of models and probes. On the one hand, we uncovered clear evidence supporting a causal link between a distancebased representation of syntax and the outputs of a masked-word model. On the other hand, depthbased manipulations of embeddings had little effect, and w"
2021.findings-acl.76,2020.acl-main.659,0,0.0263911,"et al., 2019; Brown et al., 2020). These connectionist models, trained on large corpora in a largely unsupervised manner, learn to map words into numerical representations, or embeddings, that support languagereasoning tasks. Fine-tuning these models on tasks like extractive question answering specializes these generic models into performant, task-specific models (Wolf et al., 2019). In conjunction with the rise of these powerful neural models, researchers have investigated what the models have learned. Probes, tools built to reveal properties of a trained model, are a favored approach (Hall Maudslay et al., 2020; Conneau et al., 2018). For example, Hewitt and Manning (2019) have uncovered compelling evidence that several models encode syntactic information in their embeddings. That is, by passing embeddings through a trained probe, one may recover information about a sentence’s syntax. Although these results are impressive, they fall short of clearly demonstrating what linguistic information the language models actually use. Syntactic information is present in sentences; that embeddings also encode syntax does not imply that a model uses syntactic knowledge. In order to truly query a model’s understa"
2021.findings-acl.76,D19-1275,0,0.0112257,"in language models (Alain and Bengio, 2018; Conneau et al., 2018; Hewitt and Manning, 2019; Coenen et al., 2019, among others). Our work uses two syntactic probes developed by Hewitt and Manning (2019) that map from model embeddings to predictions about word locations in a parse tree. These probes are simple by design – merely linear transformations – in order to prevent the probes themselves from doing parsing. Recent work directly addresses the topic of probe simplicity. On the one hand, if probes are too expressive, they may reveal their own learning instead of a model’s (Liu et al., 2019; Hewitt and Liang, 2019). On the other hand, Pimentel et al. (2020) argue from an information-theoretical perspective that more expressive probes are always preferable. Our work differs from much prior art in probe design by leveraging causal analysis, which uses counterfactual data to test probes and models. This provides direct evidence of whether a model uses the same features as a probe, allowing us to experiment beyond linear probes (and indeed, we found that more complex probes offered an advantage in some cases). 2.2 Causal Analysis of Language Models Motivated by the limitations of traditional, correlative pr"
2021.findings-acl.76,N19-1419,0,0.109806,"els, trained on large corpora in a largely unsupervised manner, learn to map words into numerical representations, or embeddings, that support languagereasoning tasks. Fine-tuning these models on tasks like extractive question answering specializes these generic models into performant, task-specific models (Wolf et al., 2019). In conjunction with the rise of these powerful neural models, researchers have investigated what the models have learned. Probes, tools built to reveal properties of a trained model, are a favored approach (Hall Maudslay et al., 2020; Conneau et al., 2018). For example, Hewitt and Manning (2019) have uncovered compelling evidence that several models encode syntactic information in their embeddings. That is, by passing embeddings through a trained probe, one may recover information about a sentence’s syntax. Although these results are impressive, they fall short of clearly demonstrating what linguistic information the language models actually use. Syntactic information is present in sentences; that embeddings also encode syntax does not imply that a model uses syntactic knowledge. In order to truly query a model’s understanding, one must use causal analysis. Recently, several authors"
2021.findings-acl.76,N19-1112,0,0.0147499,"tax and semantics in language models (Alain and Bengio, 2018; Conneau et al., 2018; Hewitt and Manning, 2019; Coenen et al., 2019, among others). Our work uses two syntactic probes developed by Hewitt and Manning (2019) that map from model embeddings to predictions about word locations in a parse tree. These probes are simple by design – merely linear transformations – in order to prevent the probes themselves from doing parsing. Recent work directly addresses the topic of probe simplicity. On the one hand, if probes are too expressive, they may reveal their own learning instead of a model’s (Liu et al., 2019; Hewitt and Liang, 2019). On the other hand, Pimentel et al. (2020) argue from an information-theoretical perspective that more expressive probes are always preferable. Our work differs from much prior art in probe design by leveraging causal analysis, which uses counterfactual data to test probes and models. This provides direct evidence of whether a model uses the same features as a probe, allowing us to experiment beyond linear probes (and indeed, we found that more complex probes offered an advantage in some cases). 2.2 Causal Analysis of Language Models Motivated by the limitations of tr"
2021.findings-acl.76,2020.acl-main.420,0,0.0123304,"Conneau et al., 2018; Hewitt and Manning, 2019; Coenen et al., 2019, among others). Our work uses two syntactic probes developed by Hewitt and Manning (2019) that map from model embeddings to predictions about word locations in a parse tree. These probes are simple by design – merely linear transformations – in order to prevent the probes themselves from doing parsing. Recent work directly addresses the topic of probe simplicity. On the one hand, if probes are too expressive, they may reveal their own learning instead of a model’s (Liu et al., 2019; Hewitt and Liang, 2019). On the other hand, Pimentel et al. (2020) argue from an information-theoretical perspective that more expressive probes are always preferable. Our work differs from much prior art in probe design by leveraging causal analysis, which uses counterfactual data to test probes and models. This provides direct evidence of whether a model uses the same features as a probe, allowing us to experiment beyond linear probes (and indeed, we found that more complex probes offered an advantage in some cases). 2.2 Causal Analysis of Language Models Motivated by the limitations of traditional, correlative probes, researchers have recently turned to c"
2021.findings-acl.76,D16-1264,0,0.0113767,"question answering is framed by Wolf et al. (2019) as follows: given a sentence, S, comprising word tokens (s0 , s1 , ...sn ) and a question, identify the start and end tokens (si , sj ; 0 ≤ i ≤ j ≤ n) denoting a contiguous stretch of the sentence that answers the question. For example, given the sentence [‘I’, ‘ate’, ‘two’, ‘apples’, ‘.’] and the question “How many apples did I eat?,” a correct answer could be [2, 2] (“two”) or [2, 3] (“two apples”). We used huggingface’s “BertForQuestionAnswering,” already fine-tuned on the SQuAD dataset, and referred to the model as QA (Wolf et al., 2019; Rajpurkar et al., 2016). 4.2 Probes Our technique for generating counterfactual embeddings depended on probes, so we used four different syntactic probes drawn from prior art and our own design. The depth probe from Hewitt and Manning (2019) maps from embeddings to predictions over words’ depths in a sentence’s parse tree. The distance probe, given a pair of words, predicts the distance between the words in the parse tree (i.e., how many edges must be traversed). Both probes consist of a linear transformation from embedding to prediction. We further implemented “deep” versions of the distance probe by creating two-"
C16-1209,P14-1103,1,0.919949,"e probabilistic context-free grammar of constraint structure to jointly infer (binary) feature values for multiple constraints and likely symbolic definitions for those constraints. We ground the model in a morphophonological setting, using the model to infer what phonological constraints affect the output form of regular English plurals, although it can be applied to other problems for which a constraint grammar can be defined. The inference procedure moves beyond existing methods for learning extensional definitions of constraint values (Griffiths and Ghahramani, 2005; G¨or¨ur et al., 2006; Doyle et al., 2014) from observational data to incorporate top-down information about likely intensional constraint definitions, improving both the applicability of the constraints and the theoretical basis for their values. We show that learning the constraints through this model performs as well as using pre-specified phonologically-standard constraints in explaining both observed and novel regular plural morphophonology. In addition, the structure of the learned constraints is similar to standard phonological constraints, showing that the model can be useful in both applications and theory-building. 2 Constra"
C16-1209,J98-2006,0,0.232724,"Missing"
C16-1209,P02-1038,0,0.175669,"a grammar of articulatory features, perform as well as theoretically-defined constraints on both observed and novel forms of English regular plurals. The learned constraint values and definitions also closely resemble standard constraints defined within phonological theory. 1 Introduction Constraint-based models of language, often in the form of “maximum entropy” or “log-linear” models, are prominent in many applications and theoretical analyses in computational linguistics and psycholinguistics, including in text segmentation (Beeferman et al., 1999; Poon et al., 2009), machine translation (Och and Ney, 2002), syntactic alternation choice (Bresnan et al., 2007), and phonology (Goldwater and Johnson, 2003). Building successful models – and learning about human behavior from them – relies on the ability to identify relevant constraints, and this can be a difficult problem. In this paper, we propose a system for learning both the values of and symbolic definitions for such constraints. We present a framework that combines observed data about linguistic outcomes with a flexible probabilistic context-free grammar of constraint structure to jointly infer (binary) feature values for multiple constraints"
C16-1209,N09-1024,0,0.0275472,"case. The inferred constraints, based on a grammar of articulatory features, perform as well as theoretically-defined constraints on both observed and novel forms of English regular plurals. The learned constraint values and definitions also closely resemble standard constraints defined within phonological theory. 1 Introduction Constraint-based models of language, often in the form of “maximum entropy” or “log-linear” models, are prominent in many applications and theoretical analyses in computational linguistics and psycholinguistics, including in text segmentation (Beeferman et al., 1999; Poon et al., 2009), machine translation (Och and Ney, 2002), syntactic alternation choice (Bresnan et al., 2007), and phonology (Goldwater and Johnson, 2003). Building successful models – and learning about human behavior from them – relies on the ability to identify relevant constraints, and this can be a difficult problem. In this paper, we propose a system for learning both the values of and symbolic definitions for such constraints. We present a framework that combines observed data about linguistic outcomes with a flexible probabilistic context-free grammar of constraint structure to jointly infer (binary)"
D08-1025,J99-1004,0,0.0465065,"rence study of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC (T, w|w∗ ). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed 238 Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation"
D08-1025,P04-1030,0,0.0128826,"substitutions (w′ → w)S . Due to the symmetry of the Levenshtein 1...k distance, the paths P and P ′ will have identical costs. Therefore the kernel is indeed symmetric. 2.3 Efficient computation of posterior beliefs The problem of finding structures or strings with high posterior probability given a particular input string w∗ is quite similar to the problem faced in the parsing of speech, where the acoustic input I to a parser can be represented as a lattice of possible word sequences, and the edges of the lattice have weights determined by a model of acoustic realization of words, P (I|w) (Collins et al., 2004; Hall and Johnson, 2003, 2004). The two major differences between lattice parsing and our problem are (a) we have integrated out the expected effect of noise, which is thus implicit in our choice of kernel; and (b) the loops in the Levenshtein-distance kernel mean that the input to parsing is no longer a lattice. This latter difference means that some of the techniques applicable to string parsing and lattice parsing – notably the computation of inside probabilities – are no longer possible using exact methods. We return to this difference in Sections 3 and 4. 3 Global inference One clear pre"
D08-1025,P02-1026,0,0.130453,"CFGs. Considering the adversity of the conditions under which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide, 1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental lev"
D08-1025,W03-1009,0,0.292275,"Missing"
D08-1025,N01-1021,0,0.311883,"can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide, 1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: word recognition is an evidential process whose output is nonetheless a specific “winner-takes-all” sequence of words, which is in turn the input to an evidential sentencecomprehension process. It is theoretically possible that this partition is real and is an optimal solution to the problem of language comprehension under gross architectural constraints that favor modularity 234 Pro"
D08-1025,P04-1006,0,0.0373131,"Missing"
D08-1025,W05-1506,0,0.0202128,"ng the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC (T, w|w∗ ). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed 238 Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation over the entire parsed Brown corpus. For each of the sentence sets like (1) used in Experiments 1a, 1b, and 2 of Christianson et al. (2001) that have complete lexical"
D08-1025,P04-1005,0,0.0794515,"put uncertainty can be modeled by a kernel function on input string pairs. (Similar conclusions result when the posterior distribution of interest is over structures T .) It is an open question which kernel functions might best model the inferences made in human sentence comprehension. Most obviously the kernel function should account for noise (environmental, perceptual, and attentional) introduced into the signal en route to the neural stage of abstract sentence processing. In addition, this kernel function might also be a natural means of accounting for modeling error such as disfluencies (Johnson and Charniak, 2004), word/phrase swaps, and even well-formed utterances that the speaker did not intend. For purposes of this paper, we limit ourselves to a simple kernel based on the Levenshtein distance LD(w, w′ ) between words and constructed in the form of a weighted finite-state automaton (Mohri, 1997). 2.2 sat/3 sat/3 cat/3 The Levenshtein-distance kernel Suppose that the input word string w∗ consists of words w1...n . We define the Levenshtein-distance kernel as follows. Start with a weighted finite-state automaton in the log semiring over the vocabulary Σ with states 0 . . . n, state 0 being the start st"
D08-1025,W04-3241,0,0.384616,"which linguistic communication takes place in everyday life—ambiguity of the signal, environmental competition for our attention, speaker error, and so forth—it is perhaps remarkable that we are as successful at it as we are. Perhaps the leading explanation of this success is that (a) the linguistic signal is redundant, and (b) diverse information sources are generally available that can help us obtain infer the intended message (or something close enough) when comprehending an utterance (Tanenhaus et al., 1995; Altmann and Kamide, 1999; Genzel and Charniak, 2002, 2003; Aylett and Turk, 2004; Keller, 2004; Levy and Jaeger, 2007). Given the difficulty of this task coupled with the availability of redundancy and useful information sources, it would seem rational for all available information to be used to its fullest in sentence comprehension. This idea is either implicit or explicit in several interactivist theories of probabilistic language comprehension (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2002; Levy, 2008). However, these theories have implicitly assumed a partitioning of interactivity that distinguishes the word as a fundamental level of linguistic information processing: wo"
D08-1025,levy-andrew-2006-tregex,1,0.840222,"Missing"
D08-1025,J97-2003,0,0.0680937,"e kernel function should account for noise (environmental, perceptual, and attentional) introduced into the signal en route to the neural stage of abstract sentence processing. In addition, this kernel function might also be a natural means of accounting for modeling error such as disfluencies (Johnson and Charniak, 2004), word/phrase swaps, and even well-formed utterances that the speaker did not intend. For purposes of this paper, we limit ourselves to a simple kernel based on the Levenshtein distance LD(w, w′ ) between words and constructed in the form of a weighted finite-state automaton (Mohri, 1997). 2.2 sat/3 sat/3 cat/3 The Levenshtein-distance kernel Suppose that the input word string w∗ consists of words w1...n . We define the Levenshtein-distance kernel as follows. Start with a weighted finite-state automaton in the log semiring over the vocabulary Σ with states 0 . . . n, state 0 being the start state 236 and n the (zero-cost) final state. We add two types of arcs to this automaton: (a) substitution/deletion arcs (i − 1, w′ ) → i, i ∈ 1, . . . , n, each with cost λ LD(wi , w′ ), for all w′ ∈ Σ ∪ {ǫ}; and (b) insertion loop arcs (j, w′ ) → j, j ∈ 0, . . . , n, each with cost λ LD(ǫ,"
D08-1025,W03-3016,0,0.123945,"→ NP VP SBAR → IN S NP → DT NN NP → NNS NP → NNP NP → DT NNS NP → PRP NP → NN VP → VBD RB VP → VBD PP VP → VBD NP VP → VBD RP VP → VBD VP → VBD JJ PP → IN NP 0.0 6.3 4.6 0.0 0.1 0.0 1.9 4.4 3.3 4.5 1.3 3.1 9.7 2.2 1.2 8.3 2.0 3.4 0.0 Figure 2: The PCFG used in the global-inference study of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC (T, w|w∗ ). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channe"
D08-1025,J07-4003,0,0.113935,"of Section 3. Rule weights given as negative log-probabilities in bits. distance kernel. A probabilistic context-free grammar (PCFG) representing the comprehender’s grammatical knowledge can be intersected with that wFSA using well-understood techniques, generating a new weighted CFG (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). This intersection thus represents the unnormalized posterior PC (T, w|w∗ ). Because there are loops in the wFSA generated by the Levenshtein-distance kernel, exact normalization of the posterior is not tractable (though see Nederhof and Satta, 2003; Chi, 1999; Smith and Johnson, 2007 for possible approaches to approximating the normalization constant). We can, however, use the lazy k-best algorithm of Huang and Chiang (2005; Algorithm 3) to obtain the word-string/parsetree pairs with highest posterior probability. 3.1 Experimental Verification To test our account of the rational noisy-channel interpretation of sentences such as (1), we defined a small PCFG using the phrasal rules listed in Figure 2, with rule probabilities estimated from the parsed 238 Brown corpus.4 Lexical rewrite probabilities were determined using relative-frequency estimation over the entire parsed B"
D08-1025,J03-4003,0,\N,Missing
D19-1050,W19-4820,0,0.0669731,"explore the brain– machine link in language understanding, asking whether human brain activations can be matched with the activations of computational language models. Mitchell et al. (2008) first demonstrated that distributional word representations could be used to predict human brain activations, when subjects were presented with individual words in isolation. Huth et al. (2016) replicated and extended these results using distributed word representations, and Pereira et al. (2018) extended these results to sentence stimuli. Wehbe et al. (2014), Qian et al. (2016), Jain and Huth (2018), and Abnar et al. (2019) next introduced more complex word and sentence meaning representations, demonstrating that neural network language models could better account for brain activation by incorporating repre530 Figure 1: Brain decoding methodology. We use human brain activations in response to sentences to predict how neural networks represent those same sentences. sentations of longer-term linguistic context. Gauthier and Ivanova (2018) and Sun et al. (2019) further demonstrated that optimizing model representations for different objectives yielded substantial differences in brain decoding performance. This pape"
D19-1050,N19-1419,0,0.0811424,"Missing"
D19-1050,D14-1162,0,0.0832959,"of this task, yielding representations CLM,` . 2.2.2 Word vector baseline As a baseline comparison, we also include sentence representations computed from GloVe word 8 This shuffling method removes first-order cues to constituency: for example, the fact that the table appears directly to the right of on suggests that they are members of a single constituent. It is still possible that the model can exploit second-order cues to structure. For example, if two nouns ranch and rancher appear in the same sentence as dressing, we can still guess that ranch is more likely to modify dressing. vectors (Pennington et al., 2014). Unlike BERT’s word representations, these word vectors are insensitive to their surrounding sentential context. These word vectors have nevertheless successfully served as sentence meaning representations in prior studies (Pereira et al., 2018; Gauthier and Ivanova, 2018). We let CGloVe (w1 , . . . , wT ) = T1 e(wt ), where e(wi ) retrieves the GloVe embedding for word wi .9 2.3 Brain decoding We next learn a suite of decoders: regression models mapping from descriptions of human brain activation to model activations in response to sentences. Let Bi ∈ R384×dB represent the brain activations"
D19-1050,T75-2034,0,0.569886,"Missing"
D19-1050,P18-2124,0,0.0828572,"Missing"
D19-1050,silveira-etal-2014-gold,0,0.17148,"Missing"
D19-1050,D13-1170,0,0.00702066,"Missing"
D19-1050,D14-1030,0,0.445854,"ue monkey’s visual cortex in response to the same images. This result and others have led to an increasingly detailed understanding of the contents of brain representations (Schrimpf et al., 2018) and novel artificial neural network architectures (Kubilius et al., 2018) in the domain of vision. In language understanding, several authors have exploited neural network representations as proxies for sentence meaning, and demonstrated that human brain activations in response to sentences can match with these meaning representations at well above chance performance (see e.g. Mitchell et al., 2008; Wehbe et al., 2014; Huth et al., 2016; Pereira et al., 2018). Our aim in this paper is to further understand why these mappings are successful, uncovering the parallel representational contents shared between human brains and neural networks. We evaluate the link between human brain activity and neural network models as the models are optimized for different tasks. We find that neural network models quickly diverge in their capacity to match human brain activations as they are optimized for different NLU objectives. We further locate correlates of these changes in representational content, finding that the gran"
D19-1287,N16-1024,0,0.0314449,"ture of a sentence by learning to predict the next action required to construct a phrasestructure parse (Choe and Charniak, 2016). The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate P (wi |w1···i−1) by marginalizing over the most-likely partial parses found by wordsynchronous beam search (Stern et al., 2017). Generative Recurrent Neural Network Grammars (RNNG) jointly model the word sequence as well as the underlying syntactic structure (Dyer et al., 2016). Following Hale et al. (2018), we estimate surprisal using word-synchronous beam search (Stern et al., 2017). We use the same hyperparameter settings as Dyer et al. (2016). The annotation schemes used to train the syntactically-supervised models differ slightly between French and English. In the PTB (English) CoordNPs are flat structures bearing an ‘NP’ label. In FTB (French), CoordNPs are binary-branching, labeled as NPs, except for the phrasal node dominating the coordinating conjunction, which is labeled ‘COORD’. We examine the effects of annotation schemes on model performance in Appendix"
D19-1287,P16-1079,0,0.060932,"Missing"
D19-1287,N18-1108,0,0.0701839,"moon is ··· are ··· (a) The star and the moon (b) Figure 1: Subject-verb agreement with (a) the head of a noun phrase structure, and (b) the coordination structure. Introduction Humans deploy structure-sensitive expectations to guide processing during natural language comprehension (Levy, 2008). While it has been shown that neural language models show similar structure-sensitivity in their predictions about upcoming material (Linzen et al., 2016; Futrell et al., 2018), previous work has focused on dependencies that are conditioned by features attached to a single word, such as subject number (Gulordava et al., 2018; Marvin and Linzen, 2018) or whquestion words (Wilcox et al., 2018). There has been no systematic investigation into models’ ability to compute phrase-level features—features that are attached to a set of words—and whether models can deploy these more abstract properties to drive downstream expectations. In this work, we assess whether state-of-the-art neural models can compute and employ phraselevel gender and number features of coordinated subject Noun Phrases (CoordNPs) with two nouns. Typical syntactic phrases are ENDOCENTRIC: they are HEADED by a single child, whose features determine th"
D19-1287,N01-1021,0,0.136491,"cabinet, drives expectations about the number of the matrix verb. If models are able to robustly modulate their expectations based on the internal components of the CoordNP, this will provide evidence that the networks are building up a context-sensitive phrase-level representation. We quantify model expectations as SURPRISAL VALUES. Surprisal is the negative log-conditional probability S(xi ) = − log2 p(xi |x1 . . . xi−1 ) of a sentence’s ith word xi given the previous words. Surprisal tells us how strongly xi is expected in context and is known to correlate with human processing difficulty (Hale, 2001; Levy, 2008; Smith and Levy, 2013). In the CoordNP/Verb agreement studies presented here, cases where the proceeding context sets high expectation for a number-inflected verb form wi , (e.g. singular ‘is’) we would expect S(wi ) to be lower than its number-mismatched counterpart (e.g. plural ‘are’). 2.2 Models Tested 2.1 Psycholinguistics Paradigm To determine whether state-of-the-art neural architectures are capable of learning humanlike CoordNP/verb agreement properties, we adopt the psycholinguistics paradigm for model assessment. In this paradigm the models are tested using handcrafted se"
D19-1287,P18-1254,0,0.0189345,"to predict the next action required to construct a phrasestructure parse (Choe and Charniak, 2016). The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate P (wi |w1···i−1) by marginalizing over the most-likely partial parses found by wordsynchronous beam search (Stern et al., 2017). Generative Recurrent Neural Network Grammars (RNNG) jointly model the word sequence as well as the underlying syntactic structure (Dyer et al., 2016). Following Hale et al. (2018), we estimate surprisal using word-synchronous beam search (Stern et al., 2017). We use the same hyperparameter settings as Dyer et al. (2016). The annotation schemes used to train the syntactically-supervised models differ slightly between French and English. In the PTB (English) CoordNPs are flat structures bearing an ‘NP’ label. In FTB (French), CoordNPs are binary-branching, labeled as NPs, except for the phrasal node dominating the coordinating conjunction, which is labeled ‘COORD’. We examine the effects of annotation schemes on model performance in Appendix A. 1 3 Experiment 1: Non-coor"
D19-1287,D18-1151,0,0.0616511,"The star and the moon (b) Figure 1: Subject-verb agreement with (a) the head of a noun phrase structure, and (b) the coordination structure. Introduction Humans deploy structure-sensitive expectations to guide processing during natural language comprehension (Levy, 2008). While it has been shown that neural language models show similar structure-sensitivity in their predictions about upcoming material (Linzen et al., 2016; Futrell et al., 2018), previous work has focused on dependencies that are conditioned by features attached to a single word, such as subject number (Gulordava et al., 2018; Marvin and Linzen, 2018) or whquestion words (Wilcox et al., 2018). There has been no systematic investigation into models’ ability to compute phrase-level features—features that are attached to a set of words—and whether models can deploy these more abstract properties to drive downstream expectations. In this work, we assess whether state-of-the-art neural models can compute and employ phraselevel gender and number features of coordinated subject Noun Phrases (CoordNPs) with two nouns. Typical syntactic phrases are ENDOCENTRIC: they are HEADED by a single child, whose features determine the agreement requirements f"
D19-1287,D17-1178,0,0.0214102,"ddings and hidden layers to 400 for the LSTM (frWaC) model since it is trained on a large dataset. ActionLSTM models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrasestructure parse (Choe and Charniak, 2016). The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate P (wi |w1···i−1) by marginalizing over the most-likely partial parses found by wordsynchronous beam search (Stern et al., 2017). Generative Recurrent Neural Network Grammars (RNNG) jointly model the word sequence as well as the underlying syntactic structure (Dyer et al., 2016). Following Hale et al. (2018), we estimate surprisal using word-synchronous beam search (Stern et al., 2017). We use the same hyperparameter settings as Dyer et al. (2016). The annotation schemes used to train the syntactically-supervised models differ slightly between French and English. In the PTB (English) CoordNPs are flat structures bearing an ‘NP’ label. In FTB (French), CoordNPs are binary-branching, labeled as NPs, except for the phrasa"
D19-1287,W18-5423,1,0.852016,"rb agreement with (a) the head of a noun phrase structure, and (b) the coordination structure. Introduction Humans deploy structure-sensitive expectations to guide processing during natural language comprehension (Levy, 2008). While it has been shown that neural language models show similar structure-sensitivity in their predictions about upcoming material (Linzen et al., 2016; Futrell et al., 2018), previous work has focused on dependencies that are conditioned by features attached to a single word, such as subject number (Gulordava et al., 2018; Marvin and Linzen, 2018) or whquestion words (Wilcox et al., 2018). There has been no systematic investigation into models’ ability to compute phrase-level features—features that are attached to a set of words—and whether models can deploy these more abstract properties to drive downstream expectations. In this work, we assess whether state-of-the-art neural models can compute and employ phraselevel gender and number features of coordinated subject Noun Phrases (CoordNPs) with two nouns. Typical syntactic phrases are ENDOCENTRIC: they are HEADED by a single child, whose features determine the agreement requirements for the entire phrase. In Figure 1a, for ex"
E17-1065,S13-1035,0,0.0175684,"we find that words at close distances tend to have higher pmi, regardless of whether they are in a dependency relationship. We have shown that noisy-context surprisal derives information locality, and argued that dependency locality can be seen as a special case of information locality. However, deriving dependency locality requires a crucial assumption that words linked in a dependency have higher mutual information than those words that are not. To test this assumption, we calculated mutual information between wordforms in various dependency relations in the Google Syntactic n-gram corpus (Goldberg and Orwant, 2013). We compared the mutual information of content words in a direct dependency relationship to content words in grandparent–grandchild and sister–sister dependency relationships. Mutual information was estimated using maximum likelihood estimation from frequencies, treating the corpus as samples from a distribution over (head, dependent) pairs. In order to exclude nonlinguistic forms, we only included wordforms if they were among the top 10000 most frequent wordforms in the corpus. The direct head–dependent frequencies were calculated from the same corpus as the grandparent-grandchild frequencie"
E17-1065,J13-4008,0,0.0222546,"ng what computational problem the human sentence processing system is solving—the problem of how update one’s belief about a sentence given a new word—without specifying implementation details. Memory-based theories such as Lewis and Vasishth (2005) are for the most part based in mechanistic algorithmic-level theories describing the actions of a specific incremental parser. Previous theories that capture both surprisal and locality effects have typically done so by augmenting parsing models with a special predictionverification operation to capture surprisal effects (Demberg and Keller, 2009; Demberg et al., 2013), or by combining surprisal and memorybased cost derived from a parsing model as separate factors in a linear model (Shain et al., 2016). These models capture surprisal and locality effects at the same time, but they do not clearly capture phenomena involving the interaction of memory and probabilistic expectations such as languagedependent structural forgetting (see Section 3). Here we develop a computational-level model capturing both memory and expectation effects from a single set of principles, without reference to a specific parsing algorithm. In our model, the processing cost of a word"
E17-1065,N01-1021,0,0.880626,"), which are previously not well modeled by either expectation-based or memory-based approaches. Additionally, we show that this model derives and generalizes locality effects (Gibson, 1998; Demberg and Keller, 2008), a signature prediction of memory-based models. We give corpusbased evidence for a key assumption in this derivation. 1 Introduction Models of human sentence processing difficulty can be divided into two kinds, expectation-based and memory-based. Expectation-based models predict the processing difficulty of a word from the word’s surprisal given previous material in the sentence (Hale, 2001; Levy, 2008a). These models have good coverage: they can account for effects of syntactic construction frequency and resolution of ambiguity on incremental processing difficulty. Memory-based models, on the other hand, explain difficulty resulting from working memory limitations during incremental parsing (Gibson, 1998; 688 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 688–698, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics the difference between English and German ("
E17-1065,C96-1058,0,0.13859,"formation of linguistic elements has been found to decrease with distance (Li, 1989; Lin and Tegmark, 2016), although this claim has only been tested for letters, not for larger linguistic units such as morphemes. The fact that linguistic units that are close typically have high mutual information could result from optimization of word order for information locality. The idea that syntactically dependent words have high mutual information is also ubiquitously implicit in probabilistic models of language and in practical NLP models. For example, it is implied by head-outward generative models (Eisner, 1996; Eisner, 1997; Klein and Manning, 2004), the first successful models for grammar induction. Mutual information has been used directly for unsupervised discovery of syntactic dependencies (Yuret, 1998) and evaluation of dependency parses (de Paiva Alves, 1996), as well as commonly for collocation detection (Church and Hanks, 1990). In addition to providing evidence for a crucial assumption in the derivation of information locality, our results also give evidence backing up the theoretical validity of such models and methods. The derivation of information locality given here assumed progressive"
E17-1065,P04-1061,0,0.10755,"ents has been found to decrease with distance (Li, 1989; Lin and Tegmark, 2016), although this claim has only been tested for letters, not for larger linguistic units such as morphemes. The fact that linguistic units that are close typically have high mutual information could result from optimization of word order for information locality. The idea that syntactically dependent words have high mutual information is also ubiquitously implicit in probabilistic models of language and in practical NLP models. For example, it is implied by head-outward generative models (Eisner, 1996; Eisner, 1997; Klein and Manning, 2004), the first successful models for grammar induction. Mutual information has been used directly for unsupervised discovery of syntactic dependencies (Yuret, 1998) and evaluation of dependency parses (de Paiva Alves, 1996), as well as commonly for collocation detection (Church and Hanks, 1990). In addition to providing evidence for a crucial assumption in the derivation of information locality, our results also give evidence backing up the theoretical validity of such models and methods. The derivation of information locality given here assumed progressive erasure noise for concreteness, but we"
E17-1065,D08-1025,1,0.782305,"previously not well modeled by either expectation-based or memory-based approaches. Additionally, we show that this model derives and generalizes locality effects (Gibson, 1998; Demberg and Keller, 2008), a signature prediction of memory-based models. We give corpusbased evidence for a key assumption in this derivation. 1 Introduction Models of human sentence processing difficulty can be divided into two kinds, expectation-based and memory-based. Expectation-based models predict the processing difficulty of a word from the word’s surprisal given previous material in the sentence (Hale, 2001; Levy, 2008a). These models have good coverage: they can account for effects of syntactic construction frequency and resolution of ambiguity on incremental processing difficulty. Memory-based models, on the other hand, explain difficulty resulting from working memory limitations during incremental parsing (Gibson, 1998; 688 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 688–698, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics the difference between English and German (Section 3),"
E17-1065,P11-1106,1,0.840905,"the noisy-channel probability of a word given a noisy context, computed via marginalization: X pNC pL (wi |w1:i−1 )pNC (w1:i−1 |V ) L (wi |V ) = w1:i−1 with pNC (w1:i−1 |V ) computed via Bayes Rule: pNC (w1:i−1 |V ) ∝ pN (V |w1:i−1 )pL (w1:i−1 ). Note here that wi ’s cost is computed using its true identity but a noisy representation of the context: from the incremental perspective, wi is observed now, but context is stored and retrieved in a potentially noisy storage medium. This asymmetry between noise levels for proximal versus distal input differs from the noisy-channel surprisal model of Levy (2011), and is crucial to the derivation of information locality we present in Section 4. Here we use two types of noise distributions for pN : erasure noise and deletion noise. In erasure noise, a symbol in the context is probabilistically erased and replaced with a special symbol E with probability e. In deletion noise, a symbol is erased from the sequence completely, leaving no trace. Given deletion noise, a comprehender does not know how many symbols were in the original context; with erasure noise, the comprehender knows exactly which symbols were affected by noise. In both cases, we assume tha"
E17-1065,P10-1021,0,0.0257595,"ce, and thus E has no interaction information with any subset of those elements. That is, i(wi ; vI1 ; ...; vIn ) = 0 unless vIj = wIj for all j. This allows us to write: Memory-based models of sentence processing account for apparent dependency locality effects, which is processing cost apparently arising from two words linked in a syntactic dependency appearing far from one another (Gibson, 1998). Dependency length has been proposed as a rough measure of comprehension and production difficulty, and studied as a predictor of reaction times (Grodner and Gibson, 2005; Demberg and Keller, 2008; Mitchell et al., 2010; Shain et al., 2016), and also as a theory of production preferences and linguistic typology, under the assumption that people prefer to produce sentences with short dependencies (dependency length minimization) (Hawkins, 1994; Gildea and Temperley, 2010; Futrell et al., 2015; Rajkumar et al., 2016). Dependency locality follows from information locality if words linked in a syntactic dependency have particularly high mutual information. To see this, consider only the lowest-order interaction information terms in Equation 7, truncating the summation over n at 1. We can write C(wi |w1:i−1 ) = h"
E17-1065,W16-4106,1,0.735479,"Missing"
E17-1065,J90-1003,0,\N,Missing
E17-1065,P96-1055,0,\N,Missing
K18-1029,D14-1162,0,0.0843534,"nouns as a noun pair. Each scenario also contains a set of clues drawn from 100 descriptive adjectives. Throughout the paper, we will interchangeably refer to codenames as nouns and to clues as adjectives. A configuration is a scenario that additionally includes an index, either indicating the target noun 1 https://github.com/heyyjudes/ codenames-language-game/ 293 Listener Speaker PL0 (p|a) ∝ sp,a PS1 (a|p) ∝ [PL0 (p|a)]α PL1 (p|a) ∝ PS1 (a|p) PS0 (a|p) ∝ sp,a PL1 (p|a) ∝ [PS0 (a|p)]α PS1 (a|p) ∝ PL1 (p|a) 2.2.2 Vector embedding cosine distance Global Vectors for Word Representation (GloVe) (Pennington et al., 2014) and skip-gram model trained vectors (Word2Vec) provide vector representations for words that encompass semantic and linguistic similarity. We examine the Twitter GloVe set (d = 200), the Wiki-GigaWord GloVe set of (d = 200) (Pennington et al., 2014), and Google News Word2Vec vectors (d = 300) (Mikolov et al., 2013). To calculate noun– adjective similarities, we compute cosine distance between each noun–adjective pair’s vector embeddings. Table 1: RSA equations for constructing literal and pragmatic models from semantic metrics. with the pragmatic versions using PL1 (p|a) and PS1 (p|a). 2.2 Mo"
K18-1029,rose-etal-2002-reuters,0,0.00782517,"WordNet (Miller, 1995), to create a comprehensive network of common-sense relationships with crowd-sourced human ratings (Speer and Havasi, 2013). Knowledge about words is represented as a semantic graph and relatedness of concepts are edges in this graph. We use these relatedness scores to construct noun–adjective associations. 2.2.4 Topic Modeling (LDA) Topic models assume that words in a document are generated from a mixture of topics, defined as probability distributions over the lexicon. We train a Latent Dirichlet Allocation (LDA) model (Blei, Ng & Jordan, 2002) on the RCV1 news corpus (Rose et al., 2002, 804k documents). A noun–adjective similarity metric was obtained by computing the Euclidean distance between each word’s respective distribution over topics z. Bigram semantic association The first metric we consider is derived from the bigram co-occurrence counts of noun–adjective pairs zn,a , describing how relevant an adjective a ∈ A is for a noun n ∈ N . We create one set of these relationships using Google Ngram probabilities averaged across the years 1990 to 2000 (Michel et al., 2011). A comparison set is obtained from a real-world corpus containing 30B messages from Twitter. The seman"
levy-andrew-2006-tregex,C02-1126,0,\N,Missing
levy-andrew-2006-tregex,P05-1073,0,\N,Missing
levy-andrew-2006-tregex,U04-1019,0,\N,Missing
N09-1038,P07-1021,0,0.0241175,"Missing"
N09-1038,P06-2066,0,0.0241698,"ree, and non-projective linearizations that involve at least one crossing dependency pair. Example (1), for example, is projective, whereas Example (2) is non-projective due to the crossing between the Yesterday→arrived and woman←who dependencies. Beyond this dichotomy, however, the homomorphism from headed tree structures to dependency structures (Miller, 2000) can be used together with work on the mildly context-sensitive formalism linear context-free rewrite systems (LCFRSs) (VijayShanker et al., 1987) to characterize various classes of mildly non-projective dependency-tree linearizations (Kuhlmann and Nivre, 2006). The LCFRSs are an infinite sequence of classes of formalism for generating surface strings through derivation trees in a rule-based context-free rewriting system. The i-th LCFRS class (for i = 0, 1, 2, . . . ) imposes the con336 Figure 1: Sample dependency subtree for Figure 2 straint that every node in the derivation tree maps to to a collection of at most i+1 contiguous substrings. The 0-th class of LCFRS, for example, corresponds to the context-free grammars, since each node in the derivation tree must map to a single contiguous substring; the 1st class of LCFRS corresponds to TreeAdjoini"
N09-1038,P04-1042,1,0.710894,"ty low enough to make the algorithm tractable and even very efficient, as we show in the following section. 5 Empirical results Using the above algorithm, we calculated minimal dependency lengths for English sentences from the WSJ portion of the Penn Treebank, and for German sentences from the NEGRA corpus. The EnglishGerman comparison is of interest because word order is freer, and crossing dependencies more common, in German than in English (Kruijff and Vasishth, 2003). We extracted dependency trees from these corpora using the head rules of Collins (1999) for English, and the head rules of Levy and Manning (2004) for German. Two dependency trees were extracted from each sentence, the surface tree extracted by using the head rules on the context341 free tree representation (i.e. no crossing dependencies), and the deep tree extracted by first returning discontinuous dependents (marked by *T* and *ICH* in WSJ, and by *T* in the Penn-format version of NEGRA) before applying head rules. Figure 7 shows the average time it takes to calculate the minimal dependency length with crossing dependencies for WSJ sentences using the unoptimized algorithm of Section 4.1 and the fully optimized algorithm of Section 4."
N09-1038,P07-1024,0,0.378241,"dependency length of the sentence can be reduced by extraposing the relative clause who was wearing a hat, resulting in (2), in which the dependency Yesterday→arrived crosses the dependency woman←who. (1) Yesterday a woman who was wearing a hat arrived. (2) Yesterday a woman arrived who was wearing a hat. 335 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 335–343, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics There has been some recent work on dependency length minimization in natural language sentences (Gildea and Temperley, 2007), but the relationship between the precise constraints on available linearizations and dependency length minimization remains little explored. In this paper, we introduce the first efficient algorithm for obtaining linearizations of dependency trees that minimize overall dependency lengths subject to the constraint of mild context-sensitivity, and use it to investigate the relationship between this constraint and the distribution of dependency length actually observed in natural languages. 2 Projective and mildly non-projective dependency-tree linearizations In the last few years there has bee"
N09-1038,P05-1012,0,0.0208537,"ttle explored. In this paper, we introduce the first efficient algorithm for obtaining linearizations of dependency trees that minimize overall dependency lengths subject to the constraint of mild context-sensitivity, and use it to investigate the relationship between this constraint and the distribution of dependency length actually observed in natural languages. 2 Projective and mildly non-projective dependency-tree linearizations In the last few years there has been a resurgence of interest in computation on dependency-tree structures for natural language sentences, spurred by work such as McDonald et al. (2005a,b) showing that working with dependency-tree syntactic representations in which each word in the sentence corresponds to a node in the dependency tree (and vice versa) can lead to algorithmic benefits over constituency-structure representations. The linearization of a dependency tree is simply the linear order in which the nodes of the tree occur in a surface string. There is a broad division between two classes of linearizations: projective linearizations that do not lead to any crossing dependencies in the tree, and non-projective linearizations that involve at least one crossing dependenc"
N09-1038,H05-1066,0,0.126226,"Missing"
N09-1038,P87-1015,0,0.676987,"Missing"
N09-1038,J03-4003,0,\N,Missing
N09-1075,P81-1022,0,0.24535,"c category X ∈ N , the probability distribution P (Xik≥j |I) for some information I is over a binary random variable indicating the presence of X. The different syntactic categories X that could span from i to any k are not mutually exclusive, hence we cannot define size of belief update as a single K-L divergence defined over multinomial distributions. 668 grammar (SCFG), and show that our model makes the correct predictions using an SCFG for English on the original local-coherences experiment of Tabor et al. (2004). 5 Computing priors and posteriors For SCFGs, a probabilistic Earley parser (Earley, 1970; Stolcke, 1995) provides the basic quantities we need to compute the prior (2) and posterior (3) for each category X. Following Stolcke, we use capital Latin characters to denote non-terminal categories and use lowercase Greek characters to denote (possibly null) sequences of terminals and non-terminals. We write the probability that a nonterminal X can be recursively rewritten by SCFG rules as a certain series of symbols µ by ∗ P (X ⇒ µ) An edge built from the rule X → λµ where λ has been recognized as beginning at position i and ending at position j is denoted j : Xi → λ.µ The forward proba"
N09-1075,P02-1026,0,0.0331679,"man sentence processing as consequences of updates from bottom-up prior to posterior beliefs Klinton Bicknell and Roger Levy Department of Linguistics University of California, San Diego 9500 Gilman Dr, La Jolla, CA 92093-0108 {kbicknell,rlevy}@ling.ucsd.edu Abstract (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). Human sentence processing involves integrating probabilistic knowledge from a variety of sources in order to incrementally determine the hierarchical structure for the serial input stream. While a large number of sentence processing effects have been explained in terms of comprehenders’ rational use of probabilistic information, effects of local coherences have not. We present here a new model of local coherences, viewing them as resulting from a belief-update process, and show that the relevant probabilities in our model are c"
N09-1075,W03-1009,0,0.0148857,"consequences of updates from bottom-up prior to posterior beliefs Klinton Bicknell and Roger Levy Department of Linguistics University of California, San Diego 9500 Gilman Dr, La Jolla, CA 92093-0108 {kbicknell,rlevy}@ling.ucsd.edu Abstract (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). Human sentence processing involves integrating probabilistic knowledge from a variety of sources in order to incrementally determine the hierarchical structure for the serial input stream. While a large number of sentence processing effects have been explained in terms of comprehenders’ rational use of probabilistic information, effects of local coherences have not. We present here a new model of local coherences, viewing them as resulting from a belief-update process, and show that the relevant probabilities in our model are calculable from a probabilis"
N09-1075,N01-1021,0,0.227115,"ied: he does not discuss how the top-down probabilities are calculated, nor what the precise linking hypothesis is between the final P˜ and reading times. Finally, it is not at all clear why the top-down expectations should be smoothed, since the smoothing actually has negative consequences on the processor’s performance. 4 Parsing as belief update The basic intuition behind the model presented here is that incrementally processing a sentence can be conceptualized as a process of updating one’s beliefs. Such an analogy has been used to motivate surprisal-based theories of sentence processing (Hale, 2001; Levy, 2008a), where beliefs about the structure of a sentence after seeing the first i − 1 words in the sentence, which we denote as w0i−1 , are updated upon encountering wi . In this case, the surprisal of a word (− log P (wi |w0i−1 )) is equivalent to the Kullback-Leibler divergence of the beliefs after w0i from the beliefs after w0i−1 (Levy, 2008a). Our model focuses on another belief-update process in sentence processing: updating beliefs about the structures that a string of words is likely to have independent of context to beliefs about what structures it is likely to have in context."
N09-1075,W04-3241,0,0.0186646,"m bottom-up prior to posterior beliefs Klinton Bicknell and Roger Levy Department of Linguistics University of California, San Diego 9500 Gilman Dr, La Jolla, CA 92093-0108 {kbicknell,rlevy}@ling.ucsd.edu Abstract (Jurafsky, 1996; Hale, 2001; Narayanan and Jurafsky, 2001; Levy, 2008b; Levy et al., 2009). Similarly, a number of other effects in both comprehension and production have been modeled as resulting from rational strategies of languages users that take into account all the probabilistic information present in the linguistic signal (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Levy and Jaeger, 2007). Human sentence processing involves integrating probabilistic knowledge from a variety of sources in order to incrementally determine the hierarchical structure for the serial input stream. While a large number of sentence processing effects have been explained in terms of comprehenders’ rational use of probabilistic information, effects of local coherences have not. We present here a new model of local coherences, viewing them as resulting from a belief-update process, and show that the relevant probabilities in our model are calculable from a probabilistic Earley par"
N09-1075,D08-1025,1,0.902858,"not discuss how the top-down probabilities are calculated, nor what the precise linking hypothesis is between the final P˜ and reading times. Finally, it is not at all clear why the top-down expectations should be smoothed, since the smoothing actually has negative consequences on the processor’s performance. 4 Parsing as belief update The basic intuition behind the model presented here is that incrementally processing a sentence can be conceptualized as a process of updating one’s beliefs. Such an analogy has been used to motivate surprisal-based theories of sentence processing (Hale, 2001; Levy, 2008a), where beliefs about the structure of a sentence after seeing the first i − 1 words in the sentence, which we denote as w0i−1 , are updated upon encountering wi . In this case, the surprisal of a word (− log P (wi |w0i−1 )) is equivalent to the Kullback-Leibler divergence of the beliefs after w0i from the beliefs after w0i−1 (Levy, 2008a). Our model focuses on another belief-update process in sentence processing: updating beliefs about the structures that a string of words is likely to have independent of context to beliefs about what structures it is likely to have in context. A bit more f"
N09-1075,J95-2002,0,0.1012,"N , the probability distribution P (Xik≥j |I) for some information I is over a binary random variable indicating the presence of X. The different syntactic categories X that could span from i to any k are not mutually exclusive, hence we cannot define size of belief update as a single K-L divergence defined over multinomial distributions. 668 grammar (SCFG), and show that our model makes the correct predictions using an SCFG for English on the original local-coherences experiment of Tabor et al. (2004). 5 Computing priors and posteriors For SCFGs, a probabilistic Earley parser (Earley, 1970; Stolcke, 1995) provides the basic quantities we need to compute the prior (2) and posterior (3) for each category X. Following Stolcke, we use capital Latin characters to denote non-terminal categories and use lowercase Greek characters to denote (possibly null) sequences of terminals and non-terminals. We write the probability that a nonterminal X can be recursively rewritten by SCFG rules as a certain series of symbols µ by ∗ P (X ⇒ µ) An edge built from the rule X → λµ where λ has been recognized as beginning at position i and ending at position j is denoted j : Xi → λ.µ The forward probability of that e"
N10-4007,E99-1016,0,\N,Missing
N10-4007,C00-1017,0,\N,Missing
N10-4007,C92-1032,0,\N,Missing
N10-4007,W02-1503,0,\N,Missing
N10-4007,N01-1021,0,\N,Missing
N10-4007,C92-1025,0,\N,Missing
N10-4007,P84-1038,0,\N,Missing
N10-4007,W83-0114,0,\N,Missing
N10-4007,C67-1009,0,\N,Missing
N13-1012,W08-2109,0,0.185006,"speech may seem so easy that it barely needed to be learned. However, pauses in speech and word boundaries are not well correlated (Cole & Jakimik, 1980), word boundaries are marked by a conspiracy of partially-informative cues (Johnson & Jusczyk, 2001), and different languages mark their boundaries differently (Cutler & Carter, 1987). This makes the problem of unsupervised word segmentation acquisition, whether by a computational model or an infant, a daunting task. Previous work The prevailing unsupervised word segmentation systems (e.g., Brent, 1999; Goldwater, Griffiths, & Johnson, 2006; Blanchard & Heinz, 2008) use only phonemic information to segment speech. However, human segmenters use additional information types, notably stress information, in their segmentation. We present an overview of these phonemic models here before discussing the prosodic model expansion. A more complete review is available in Goldwater (2007). 2.1 Goldwater et al (2006) The Goldwater et al model is related to Brent (1999)’s model, both of which use strictly phonemic information to segment. The model assumes that the corpus is generated by a Dirichlet process over 117 Proceedings of NAACL-HLT 2013, pages 117–126, c Atlan"
N13-1012,P12-1020,0,0.101011,"Missing"
N13-1012,P06-1085,0,0.584449,"language, word segmentation from fluid speech may seem so easy that it barely needed to be learned. However, pauses in speech and word boundaries are not well correlated (Cole & Jakimik, 1980), word boundaries are marked by a conspiracy of partially-informative cues (Johnson & Jusczyk, 2001), and different languages mark their boundaries differently (Cutler & Carter, 1987). This makes the problem of unsupervised word segmentation acquisition, whether by a computational model or an infant, a daunting task. Previous work The prevailing unsupervised word segmentation systems (e.g., Brent, 1999; Goldwater, Griffiths, & Johnson, 2006; Blanchard & Heinz, 2008) use only phonemic information to segment speech. However, human segmenters use additional information types, notably stress information, in their segmentation. We present an overview of these phonemic models here before discussing the prosodic model expansion. A more complete review is available in Goldwater (2007). 2.1 Goldwater et al (2006) The Goldwater et al model is related to Brent (1999)’s model, both of which use strictly phonemic information to segment. The model assumes that the corpus is generated by a Dirichlet process over 117 Proceedings of NAACL-HLT 20"
N18-1180,P16-2094,0,0.0218929,"et al. (2017) collected the dataset used in our work and used it to predict the first language of non-native English readers from gaze. We build on these studies to motivate our task and design feature representations which encode linguistic factors known to affect the human reading process. Related work in NLP developed predictive models of reading times in reading of free-form text (e.g. Nilsson and Nivre, 2009; Hara et al., 2012; Hahn and Keller, 2016). In a complementary vein, eyetracking signal has been used for linguistic annotation tasks such as POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016) and prediction of syntactic functions (Barrett and Søgaard, 2015b). Both lines of investigation provide further evidence for the tight interaction between eye movements and linguistic properties of the text, which we leverage in our work for inference about the linguistic knowledge of the reader. 7 Conclusion and Discussion We present a novel approach for automated assessment of language proficiency which relies on eye movements during reading of free-form text. Our EyeScore test captures the similarity of language learners’ gaze patterns to those of native speakers, and correlates well with"
N18-1180,K15-1038,0,0.0185946,"nd without dyslexia. Berzak et al. (2017) collected the dataset used in our work and used it to predict the first language of non-native English readers from gaze. We build on these studies to motivate our task and design feature representations which encode linguistic factors known to affect the human reading process. Related work in NLP developed predictive models of reading times in reading of free-form text (e.g. Nilsson and Nivre, 2009; Hara et al., 2012; Hahn and Keller, 2016). In a complementary vein, eyetracking signal has been used for linguistic annotation tasks such as POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016) and prediction of syntactic functions (Barrett and Søgaard, 2015b). Both lines of investigation provide further evidence for the tight interaction between eye movements and linguistic properties of the text, which we leverage in our work for inference about the linguistic knowledge of the reader. 7 Conclusion and Discussion We present a novel approach for automated assessment of language proficiency which relies on eye movements during reading of free-form text. Our EyeScore test captures the similarity of language learners’ gaze patterns to those of native speakers, a"
N18-1180,W15-2401,0,0.0195882,"nd without dyslexia. Berzak et al. (2017) collected the dataset used in our work and used it to predict the first language of non-native English readers from gaze. We build on these studies to motivate our task and design feature representations which encode linguistic factors known to affect the human reading process. Related work in NLP developed predictive models of reading times in reading of free-form text (e.g. Nilsson and Nivre, 2009; Hara et al., 2012; Hahn and Keller, 2016). In a complementary vein, eyetracking signal has been used for linguistic annotation tasks such as POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016) and prediction of syntactic functions (Barrett and Søgaard, 2015b). Both lines of investigation provide further evidence for the tight interaction between eye movements and linguistic properties of the text, which we leverage in our work for inference about the linguistic knowledge of the reader. 7 Conclusion and Discussion We present a novel approach for automated assessment of language proficiency which relies on eye movements during reading of free-form text. Our EyeScore test captures the similarity of language learners’ gaze patterns to those of native speakers, a"
N18-1180,P17-1050,1,0.32029,"1996 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics on ordinary reading. Our framework builds on previous research in psycholinguistics demonstrating that the eyetracking record reflects how readers interact with the text and how language processing unfolds over time (Frazier and Rayner, 1982; Rayner, 1998; Rayner et al., 2012). In particular, it has been shown that key aspects of the reader’s characteristics and cognitive state, such as mind wandering during reading (Reichle et al., 2010), dyslexia (Rello and Ballesteros, 2015) and native language (Berzak et al., 2017) can be inferred from their gaze record. Despite these advances, the potential of the rich and highly informative behavioral signal obtainable from human reading for automated inference about readers, and specifically about their linguistic proficiency has thus far been largely unutilized. Here, we first introduce EyeScore, an independent measure of ESL proficiency which reflects the extent to which a learner’s English reading patterns resemble those of native speakers. Second, we present a regression model which uses gaze features to predict the learner’s scores on specific external proficien"
N18-1180,D16-1009,0,0.012064,"ent in attentive versus mindless reading. In Rello and Ballesteros (2015) eye movements were used to distinguish between readers with and without dyslexia. Berzak et al. (2017) collected the dataset used in our work and used it to predict the first language of non-native English readers from gaze. We build on these studies to motivate our task and design feature representations which encode linguistic factors known to affect the human reading process. Related work in NLP developed predictive models of reading times in reading of free-form text (e.g. Nilsson and Nivre, 2009; Hara et al., 2012; Hahn and Keller, 2016). In a complementary vein, eyetracking signal has been used for linguistic annotation tasks such as POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016) and prediction of syntactic functions (Barrett and Søgaard, 2015b). Both lines of investigation provide further evidence for the tight interaction between eye movements and linguistic properties of the text, which we leverage in our work for inference about the linguistic knowledge of the reader. 7 Conclusion and Discussion We present a novel approach for automated assessment of language proficiency which relies on eye movements duri"
N18-1180,N01-1021,0,0.149898,"e not part of the training set. Figure 1 presents a schematic overview of our MET split. For TOEFL, due to the limited available data, in Section 4 we report EyeScore correlations for all the 53 test takers, and in Section 5 we perform regression experiments using leave-one-out cross validation. 3 3.1 Type-Level Features Word Property Coefficients (WP-Coefficients) This new feature set quantifies the influence of three key word characteristics on reading times of individual readers: word length, word frequency and surprisal. The last measures the difficulty of processing a word in a sentence (Hale, 2001; Levy, 2008), and is defined as its negative log probability given a sentential context: • Total Fixation duration (TF) The sum of all fixation times on a word. surprisal(wi |w1...i−1 ) = − log(wi |w1...i−1 ) (1) In the reading literature, these three characteristics were suggested as the most prominent linguistic factors influencing word reading times (e.g. Inhoff and Rayner, 1986; Rayner and Well, 1996; Pollatsek et al., 2008; Kliegl et al., 2004; Rayner et al., 2004, 2011; Smith and Levy, 2013; Luke and Christianson, 2016); whereby longer, less frequent and contextually less predictable wo"
N18-1180,W12-4905,0,0.0183217,"ategorically different in attentive versus mindless reading. In Rello and Ballesteros (2015) eye movements were used to distinguish between readers with and without dyslexia. Berzak et al. (2017) collected the dataset used in our work and used it to predict the first language of non-native English readers from gaze. We build on these studies to motivate our task and design feature representations which encode linguistic factors known to affect the human reading process. Related work in NLP developed predictive models of reading times in reading of free-form text (e.g. Nilsson and Nivre, 2009; Hara et al., 2012; Hahn and Keller, 2016). In a complementary vein, eyetracking signal has been used for linguistic annotation tasks such as POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016) and prediction of syntactic functions (Barrett and Søgaard, 2015b). Both lines of investigation provide further evidence for the tight interaction between eye movements and linguistic properties of the text, which we leverage in our work for inference about the linguistic knowledge of the reader. 7 Conclusion and Discussion We present a novel approach for automated assessment of language proficiency which reli"
N18-1180,W03-0209,0,0.0800498,"neralize the task of language proficiency prediction to arbitrary sentences. 6 Related Work Our work lies on the intersection of language proficiency assessment, second language acquisition (SLA), the psychology of reading and NLP. Automated language proficiency assessment from free-form linguistic performance has been studied mainly in language production (Dikli, 2006; Williamson, 2009; Shermis and Burstein, 2013). Over the past several decades, multiple essay and speech scoring systems have been developed for learner language using a wide range of linguistically motivated feature sets (e.g. Lonsdale and Strong-Krause, 2003; Landauer, 2003; Xi et al., 2008; Yannakoudakis et al., 2011). Some of these systems have been deployed in official language proficiency tests, for example the e-rater essay scoring system (Attali and Burstein, 2004) used in TOEFL (Ramineni et al., 2012). While this line of work focuses on assessment of language production, here we introduce and address for the first time automated language assessment during online language comprehension. In SLA, there has been considerable interest in eyetracking, where studies have mostly focused on controlled experiments examining processing of specific li"
N18-1180,J93-2004,0,0.0654046,"ter Q on a blank screen. A 300ms fixation on this letter triggers a question about the sentence on a new screen. Participants provide a yes/no answer to the question and are subsequently informed if 1 The data was collected under IRB approval, and all the participants provided written informed consent. 1987 they answered correctly. The first trial of the experiment was presented to familiarize participants with the experimental setup, and is discarded from the analysis. Each participant read a total of 156 English sentences, randomly drawn from the Wall Street Journal Penn Treebank (WSJ-PTB) (Marcus et al., 1993). The maximal sentence length was set to 100 characters, yielding an average sentence length of 11.4 words. All the sentences include the manual PTB annotations of POS tags (Santorini, 1990) and phrase structure trees, as well as Google universal POS tags (Petrov et al., 2012) and dependency trees obtained from the Universal Dependency Treebank (UDT) (McDonald et al., 2013). 2.2 Experimental Regimes Half of the 156 sentences presented to each participant belong to the Fixed Text regime, and the other half belong to the Any Text regime. Sentences from the two regimes were interleaved randomly a"
N18-1180,W09-1113,0,0.0378236,"e movement patterns are categorically different in attentive versus mindless reading. In Rello and Ballesteros (2015) eye movements were used to distinguish between readers with and without dyslexia. Berzak et al. (2017) collected the dataset used in our work and used it to predict the first language of non-native English readers from gaze. We build on these studies to motivate our task and design feature representations which encode linguistic factors known to affect the human reading process. Related work in NLP developed predictive models of reading times in reading of free-form text (e.g. Nilsson and Nivre, 2009; Hara et al., 2012; Hahn and Keller, 2016). In a complementary vein, eyetracking signal has been used for linguistic annotation tasks such as POS tagging (Barrett and Søgaard, 2015a; Barrett et al., 2016) and prediction of syntactic functions (Barrett and Søgaard, 2015b). Both lines of investigation provide further evidence for the tight interaction between eye movements and linguistic properties of the text, which we leverage in our work for inference about the linguistic knowledge of the reader. 7 Conclusion and Discussion We present a novel approach for automated assessment of language pro"
N18-1180,petrov-etal-2012-universal,0,0.0137728,"written informed consent. 1987 they answered correctly. The first trial of the experiment was presented to familiarize participants with the experimental setup, and is discarded from the analysis. Each participant read a total of 156 English sentences, randomly drawn from the Wall Street Journal Penn Treebank (WSJ-PTB) (Marcus et al., 1993). The maximal sentence length was set to 100 characters, yielding an average sentence length of 11.4 words. All the sentences include the manual PTB annotations of POS tags (Santorini, 1990) and phrase structure trees, as well as Google universal POS tags (Petrov et al., 2012) and dependency trees obtained from the Universal Dependency Treebank (UDT) (McDonald et al., 2013). 2.2 Experimental Regimes Half of the 156 sentences presented to each participant belong to the Fixed Text regime, and the other half belong to the Any Text regime. Sentences from the two regimes were interleaved randomly and presented to all participants in the same order. Fixed Text In this regime, all the participants read the same suite of 78 pre-selected sentences (900 words). The Fixed Text regime supports token-level comparisons of reading patterns for specific words in the same contexts"
N18-1180,P11-1019,0,0.025161,"entences. 6 Related Work Our work lies on the intersection of language proficiency assessment, second language acquisition (SLA), the psychology of reading and NLP. Automated language proficiency assessment from free-form linguistic performance has been studied mainly in language production (Dikli, 2006; Williamson, 2009; Shermis and Burstein, 2013). Over the past several decades, multiple essay and speech scoring systems have been developed for learner language using a wide range of linguistically motivated feature sets (e.g. Lonsdale and Strong-Krause, 2003; Landauer, 2003; Xi et al., 2008; Yannakoudakis et al., 2011). Some of these systems have been deployed in official language proficiency tests, for example the e-rater essay scoring system (Attali and Burstein, 2004) used in TOEFL (Ramineni et al., 2012). While this line of work focuses on assessment of language production, here we introduce and address for the first time automated language assessment during online language comprehension. In SLA, there has been considerable interest in eyetracking, where studies have mostly focused on controlled experiments examining processing of specific linguistic phenomena such as syntactic ambiguities, cognates and"
N18-1181,buck-etal-2014-n,0,0.0680386,"Missing"
N18-1181,N01-1021,0,0.30828,"Missing"
N18-1181,P13-1043,0,0.0214001,"ingful content but automatically generated text such as legal notices. To use this corpus as a reasonable approximation of language experience of speakers, we performed deduplication on the data, following similar practice adopted by other work dealing with web-based corpora (Buck et al., 2014). After cleaning the text, we removed repeated lines in the corpus. 4.2 Word segmentation, POS-tagging and syntactic parsing We used the Stanford CoreNLP toolkit for word segmentation, part-of-speech tagging, and syntactic parsing (Manning et al., 2014). We used CoreNLP’s Shift-Reduce model for parsing (Zhu et al., 2013). We also got dependency parsing results as part of the Stanford CoreNLP output. 2000 Noun 个 (ge, CL.general) 项 (xiang, CL.item) 张 (zhang, CL.flat) 公告 一 口 气 发布 11 个 公告 连续 发布 三 项 公告 门口 贴了 一 张 公告 announ- a CL breath release 11 CL consecutively release three CL door paste a CL announcement cement announcement announcement “release 11 announcements at one “release three announcements in a” go” row” 账单 女儿 拿着 一 个 账单 就 过来了 bill daughter carry a CL bill at once “there is an announcement on the door” 在 一 张 账单 上 解决 所有 收费 问题 not co-occurring on a CL bill solve all charge problem come “daughter came with"
N18-1181,P14-5010,0,0.0025412,"eduplication Since the data contain web pages, many snippets are not meaningful content but automatically generated text such as legal notices. To use this corpus as a reasonable approximation of language experience of speakers, we performed deduplication on the data, following similar practice adopted by other work dealing with web-based corpora (Buck et al., 2014). After cleaning the text, we removed repeated lines in the corpus. 4.2 Word segmentation, POS-tagging and syntactic parsing We used the Stanford CoreNLP toolkit for word segmentation, part-of-speech tagging, and syntactic parsing (Manning et al., 2014). We used CoreNLP’s Shift-Reduce model for parsing (Zhu et al., 2013). We also got dependency parsing results as part of the Stanford CoreNLP output. 2000 Noun 个 (ge, CL.general) 项 (xiang, CL.item) 张 (zhang, CL.flat) 公告 一 口 气 发布 11 个 公告 连续 发布 三 项 公告 门口 贴了 一 张 公告 announ- a CL breath release 11 CL consecutively release three CL door paste a CL announcement cement announcement announcement “release 11 announcements at one “release three announcements in a” go” row” 账单 女儿 拿着 一 个 账单 就 过来了 bill daughter carry a CL bill at once “there is an announcement on the door” 在 一 张 账单 上 解决 所有 收费 问题 not co-occu"
N19-1004,2017.lilt-15.3,0,0.0190167,"Department of Linguistics and Philosophy, MIT 5 Department of Brain and Cognitive Sciences, MIT, {pqian,rplevy}@mit.edu 6 IBM Research, MIT-IBM Watson AI Lab, miguel.ballesteros@ibm.com 2 Department Abstract language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind. The basic idea is to examine language models’ behavior on targeted sentences chosen to probe particular aspects of the learned representations. This approach was introduced by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphor"
N19-1004,J92-4003,0,0.532968,"mbeddings as input. The second large LSTM is the model described in the supplementary materials of Gulordava et al. (2018), which we call “GRNN”, trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), and the surprisal of the i-th word in the sentence min (w1...i ) is estimated as log PPmin (w1...i−1 ) . Our action beam is size 100, and our word beam is size 10. Finally, to disentangle effects of training set from model architecture, we use an LSTM trained on string data from the Penn T"
N19-1004,C18-1012,0,0.10732,"by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a represen"
N19-1004,P18-1132,0,0.0960351,"Missing"
N19-1004,N16-1024,1,0.930831,"es which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signalling changes in syntactic state. 1 In this work, we consider syntactic representations of a different kind. Previous studies have focused on relationships of dependency: one word licenses another word, which is tested by asking whether a language model favors one (grammatically licensed) form over another in a particular context. Here we fo"
N19-1004,K17-1003,0,0.0839398,"Missing"
N19-1004,E17-1065,1,0.849761,"gs of NAACL-HLT 2019, pages 32–42 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (1) In psycholinguistics, the common practice is to study reaction times per word (for example, reading time as measured by an eyetracker), as a measure of the word-by-word difficulty of online language processing. These reading times are often taken to reflect the extent to which humans expect certain words in context, and may be generally proportional to surprisal given the comprehender’s probabilistic language model (Hale, 2001; Levy, 2008; Smith and Levy, 2013; Futrell and Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used as models of human online language processing (Hale et al.,"
N19-1004,Q16-1037,0,0.203867,"arch Institute, Kyoto University, tmorita@alum.mit.edu 4 Department of Linguistics and Philosophy, MIT 5 Department of Brain and Cognitive Sciences, MIT, {pqian,rplevy}@mit.edu 6 IBM Research, MIT-IBM Watson AI Lab, miguel.ballesteros@ibm.com 2 Department Abstract language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind. The basic idea is to examine language models’ behavior on targeted sentences chosen to probe particular aspects of the learned representations. This approach was introduced by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli"
N19-1004,W18-0102,0,0.157489,"comprehender’s probabilistic language model (Hale, 2001; Levy, 2008; Smith and Levy, 2013; Futrell and Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used as models of human online language processing (Hale et al., 2018). As the doctor studied the textbook, the nurse walked into the office. In this work, we use a targeted evaluation approach (Marvin and Linzen, 2018) to elicit evidence for syntactic state representations from language models. That is, we examine language model behavior on artificially constructed sentences designed to expose behavior that is crucially dependent on syntactic state representations. In particular, we study complex subordinate clauses and garden path effects (based on mainverb/reduced-rel"
N19-1004,N18-1108,0,0.360138,"partment of Brain and Cognitive Sciences, MIT, {pqian,rplevy}@mit.edu 6 IBM Research, MIT-IBM Watson AI Lab, miguel.ballesteros@ibm.com 2 Department Abstract language models using experimental techniques that were originally developed in the field of psycholinguistics to study language processing in the human mind. The basic idea is to examine language models’ behavior on targeted sentences chosen to probe particular aspects of the learned representations. This approach was introduced by Linzen et al. (2016), followed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We inv"
N19-1004,J93-2004,0,0.0644458,"ated that humans maintain representations of this kind in syntactic processing (Staub and Clifton, 2006; Lau et al., 2006; Levy et al., 2012). Here we ask whether the string completion probabilities produced by neural language models show evidence of the same knowledge. and CNN character embeddings as input. The second large LSTM is the model described in the supplementary materials of Gulordava et al. (2018), which we call “GRNN”, trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), a"
N19-1004,P18-1254,0,0.26142,"Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used as models of human online language processing (Hale et al., 2018). As the doctor studied the textbook, the nurse walked into the office. In this work, we use a targeted evaluation approach (Marvin and Linzen, 2018) to elicit evidence for syntactic state representations from language models. That is, we examine language model behavior on artificially constructed sentences designed to expose behavior that is crucially dependent on syntactic state representations. In particular, we study complex subordinate clauses and garden path effects (based on mainverb/reduced-relative ambiguities and NP/Z ambiguities). We ask three general questions: (1) Is there basic e"
N19-1004,D18-1151,0,0.404611,"., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to updat"
N19-1004,N01-1021,0,0.780657,"upon recent work studying neural 32 Proceedings of NAACL-HLT 2019, pages 32–42 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (1) In psycholinguistics, the common practice is to study reaction times per word (for example, reading time as measured by an eyetracker), as a measure of the word-by-word difficulty of online language processing. These reading times are often taken to reflect the extent to which humans expect certain words in context, and may be generally proportional to surprisal given the comprehender’s probabilistic language model (Hale, 2001; Levy, 2008; Smith and Levy, 2013; Futrell and Levy, 2017). In this study, we take language model surprisal as the analogue of human reading time, using it to probe the neural networks’ expectations about what words will follow in certain contexts. There is a long tradition linking RNN performance to human language processing (Elman, 1990; Christiansen and Chater, 1999; MacDonald and Christiansen, 2002) and grammaticality judgments (Lau et al., 2017), and RNN surprisals are a strong predictor of human reading times (Frank and Bod, 2011; Goodkind and Bicknell, 2018). RNNGs have also been used"
N19-1004,N18-1202,0,0.0711524,"r of neural language models reflects the kind of generalizations that would be captured using a stack-based incremental parse state in a symbolic grammar-based model. For example, during the underlined portion of Example (1), an incremental language model should represent and maintain the knowledge that it is currently inside a subordinate clause, implying (among other things) that a full main clause must follow. Introduction It is now standard practice in NLP to derive sentence representations using neural sequence models of various kinds (Elman, 1990; Sutskever et al., 2014; Goldberg, 2017; Peters et al., 2018; Devlin et al., 2018). However, we do not yet have a firm understanding of the precise content of these representations, which poses problems for interpretability, accountability, and controllability of NLP systems. More specifically, the success of neural sequence models has raised the question of whether and how these networks learn robust syntactic generalizations about natural language, which would enable robust performance even on data that differs from the peculiarities of the training set. Here we build upon recent work studying neural 32 Proceedings of NAACL-HLT 2019, pages 32–42 c Mi"
N19-1004,D18-1499,0,0.0904612,"Missing"
N19-1004,D17-1178,0,0.0835211,"materials of Gulordava et al. (2018), which we call “GRNN”, trained on 90 million tokens of English Wikipedia with two hidden layers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), and the surprisal of the i-th word in the sentence min (w1...i ) is estimated as log PPmin (w1...i−1 ) . Our action beam is size 100, and our word beam is size 10. Finally, to disentangle effects of training set from model architecture, we use an LSTM trained on string data from the Penn Treebank training set, which we call TinyLSTM. For TinyLSTM we use 256dimensional wor"
N19-1004,J95-2002,0,0.114217,"ers of 650 hidden units each. Our RNNG is trained on syntactically labeled Penn Treebank data (Marcus et al., 1993), using 256-dimensional word embeddings for the input layer and 256-dimensional hidden layers, and dropout probability 0.3. Next-word predictions are obtained through hierarchical softmax with 140 clusters, obtained with the greedy agglomerative clustering algorithm of Brown et al. (1992). We estimate word surprisals using word-synchronous beam search (Stern et al., 2017; Hale et al., 2018): at each word wi a beam of incremental parses is filled, the summed forward probabilities (Stolcke, 1995) of all candidates on the beam is taken as a lower bound on the prefix probability: Pmin (w1...i ), and the surprisal of the i-th word in the sentence min (w1...i ) is estimated as log PPmin (w1...i−1 ) . Our action beam is size 100, and our word beam is size 10. Finally, to disentangle effects of training set from model architecture, we use an LSTM trained on string data from the Penn Treebank training set, which we call TinyLSTM. For TinyLSTM we use 256dimensional word-embedding inputs and hidden layers and dropout probability 0.3, just as with the RNNG. 3 We can detect the knowledge of synt"
N19-1004,W18-5423,1,0.824536,"ed more recently by others (Bernardy and Lappin, 2017; Enguehard et al., 2017; Gulordava et al., 2018), who used an agreement prediction task (Bock and Miller, 1991) to study whether RNNs learn a hierarchical morphosyntactic dependency: for example, that The key to the cabinets. . . can grammatically continue with was but not with were. This dependency turns out to be learnable from a language modeling objective (Gulordava et al., 2018). Subsequent work has extended this approach to other grammatical phenomena, with positive results for filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018) and negative results for anaphoric dependencies (Marvin and Linzen, 2018). We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic st"
N19-1334,D16-1257,0,0.0739912,"Missing"
N19-1334,C18-1012,0,0.0860248,"gap through certain types of syntactic nodes, illustrated in Figure 4 (Ross, 1967). Contemporary theories variously attribute island effects to grammatical rules, incremental processing considerations, or discourse-structural factors (Ambridge and Goldberg, 2008; Hofmeister and Sag, 2010; Sprouse and Hornstein, 2013). In our setting, a language model is sensitive to an island constraint if it fails to show a wh-licensing interaction between a filler and a gap that cross an island. Wilcox et al. (2018) found evidence that large-data LSTMs are sensitive to some island constraints (although see Chowdhury and Zamparelli (2018) for a contrasting view), but not to others. Here we investigate whether LSTMs would learn these from smaller training datasets, and if an RNNG’s syntactic supervision provides a learning advantage for island constraints. In this section we measure the wh-licensing interaction in the material immediately following the potential gap site, which is guaranteed to implicate the model’s (lack of) expectation for a gap inside the island, rather than throughout the entire embedded clause, which also implicates filler-driven expectations after the end of the island. 5.1 Adjunct Islands Adjunct clauses"
N19-1334,N16-1024,1,0.948293,"ntial contingencies in impressive detail and have been shown to acquire a number of non-local grammatical dependencies with some success. Here we investigate whether supervision with hierarchical structure enhances learning of a range of grammatical dependencies, a question that has previously been addressed only for subject-verb agreement. Using controlled experimental methods from psycholinguistics, we compare the performance of word-based LSTM models versus two models that represent hierarchical structure and deploy it in left-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et al., 2016) and a incrementalized version of the Parsing-as-Language-Modeling configuration from Charniak et al. (2016). Models are tested on a diverse range of configurations for two classes of non-local grammatical dependencies in English—Negative Polarity licensing and Filler–Gap Dependencies. Using the same training data across models, we find that structurally-supervised models outperform the LSTM, with the RNNG demonstrating best results on both types of grammatical dependencies and even learning many of the Island Constraints on the filler–gap dependency. Structural supervision thus provides data"
N19-1334,N18-1108,0,0.058097,". (3) a. *The senator that supported the measure has ever found any support from her constituents. b. No senator that supported the measure has ever found any support from her constituents. c. *The senator that supported no measure has ever found any support from her constituents. d. No senator that supported no measure has ever found any support from her constituents. Learning of NPI licensing conditions by LSTM language models trained on large corpora has previously been investigated by Marvin and Linzen (2018) and Futrell et al. (2018). Futrell et al. found that the language models of both Gulordava et al. (2018) and Jozefowicz et al. (2016) (hereafter called ‘Large Data LSTMs’) learned a contingency between licensors and NPIs: the NPIs in examples like (3) were lower-surprisal when linearly preceded by negative licensors. However, both papers reported that these models failed to constrain the contingency along the correct structural lines: negative NPI surprisal was decreased at least as much by a preceding negative distractor as by a negative licensor. Syntactic supervision might plausibly facilitate learning of NPI licensing conditions. We tested this following the method of Futrell et al. (2018),"
N19-1334,N01-1021,0,0.377516,"m syntactic annotation–crucially, only constituent boundaries and major syntactic categories, with functional tags and empty categories stripped away—whereas the LSTM language model only uses the sequences of terminal words. We train the models until performance converges on the held-out PTB development-set data. 2.2 Psycholinguistic Assessment Paradigm 2.2.1 Surprisal The surprisal, or negative log-conditional probability, S(xi ) of a sentence’s ith word xi , tells us how strongly xi is expected in context and is also known to correlate with human processing difficulty (Smith and Levy, 2013; Hale, 2001; Levy, 2008). For sentences out of context, surprisal is: S(xi ) = − log p(xi |x1 . . . xi−1 ) We investigate a model’s knowledge of a grammatical dependency, which is the co-variance between an upstream licensor and a downstream licensee, by measuring the effect that an upstream licensor has on the surprisal of a downstream licensee. The idea is that grammatical licensors 3303 should set up an expectation for the licensee thus reducing its surprisal compared to minimal pairs in which the licensor is absent. We derive the word surprisal from the LSTM language model by directly computing the n"
N19-1334,P18-1254,0,0.0358147,"e of a grammatical dependency, which is the co-variance between an upstream licensor and a downstream licensee, by measuring the effect that an upstream licensor has on the surprisal of a downstream licensee. The idea is that grammatical licensors 3303 should set up an expectation for the licensee thus reducing its surprisal compared to minimal pairs in which the licensor is absent. We derive the word surprisal from the LSTM language model by directly computing the negative log value of the predicted conditional probability p(xi |x1 . . . xi−1 ) from the softmax layer. Following the method in Hale et al. (2018) for estimating word surprisals from RNNG, we use word-synchronous beam search (Stern et al., 2017) to find a set of most likely incremental parses and sum their forward probabilities to approximate P(x1 , . . . xi ) and P(x1 , . . . xi−1 ) for computing the surprisal. We set the action beam size to 100 and word beam size to 10. We ensured that the correct incremental RNNG parses were present on the beam immediately before and throughout the material over which surprisal was calculated through manual spot inspection; the correct parse was almost always at the top of the beam. 2.2.2 Wh-Licensin"
N19-1334,P18-1132,0,0.153611,"Missing"
N19-1334,Q16-1037,0,0.151024,"Island Constraints on the filler–gap dependency. Structural supervision thus provides data efficiency advantages over purely stringbased training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies. 1 Introduction Long Short-Term Memory Recurrent Neural Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) have achieved state of the art language modeling performance (Jozefowicz et al., 2016) and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (Linzen et al., 2016; Wilcox et al., 2018), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing (Marvin and Linzen, 2018; Futrell et al., 2018). LSTMs, however, require large amounts of training data and remain relatively uninterpretable. One model that attempts to address both these issues is the Recurrent Neural Network Grammar (Dyer et al., 2016). RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing. They can achieve state-of-the-art broad-coverage scores on language modeling a"
N19-1334,J93-2004,0,0.0674976,"ic phrasal boundary marker. The model was trained using embedding size 256, dropout 0.3, and was able to achieve a parsing F1 score of 92.81 on the PTB, which is only marginally better than the performance of the original architecture on the same test set, as reported in Kuncoro et al. (2016). We will refer to this model as the “ActionLSTM” model in the following sections. All three models are trained on the trainingset portion of the English Penn Treebank standardly used in the parsing literature (PTB; sections 2-21), which consists of about 950,000 tokens of English language news-wire text (Marcus et al., 1993). The RNNG and Action models get supervision from syntactic annotation–crucially, only constituent boundaries and major syntactic categories, with functional tags and empty categories stripped away—whereas the LSTM language model only uses the sequences of terminal words. We train the models until performance converges on the held-out PTB development-set data. 2.2 Psycholinguistic Assessment Paradigm 2.2.1 Surprisal The surprisal, or negative log-conditional probability, S(xi ) of a sentence’s ith word xi , tells us how strongly xi is expected in context and is also known to correlate with hum"
N19-1334,D18-1151,0,0.566222,"ng of neural language models in acquiring human-like generalizations about non-local grammatical dependencies. 1 Introduction Long Short-Term Memory Recurrent Neural Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) have achieved state of the art language modeling performance (Jozefowicz et al., 2016) and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (Linzen et al., 2016; Wilcox et al., 2018), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing (Marvin and Linzen, 2018; Futrell et al., 2018). LSTMs, however, require large amounts of training data and remain relatively uninterpretable. One model that attempts to address both these issues is the Recurrent Neural Network Grammar (Dyer et al., 2016). RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing. They can achieve state-of-the-art broad-coverage scores on language modeling and phrase structure parsing tasks, learn Noun Phrase headedness (Kuncoro et al., 2016), and outperform linear models at learning subject-verb numbe"
N19-1334,D17-1178,0,0.298463,"eam licensee, by measuring the effect that an upstream licensor has on the surprisal of a downstream licensee. The idea is that grammatical licensors 3303 should set up an expectation for the licensee thus reducing its surprisal compared to minimal pairs in which the licensor is absent. We derive the word surprisal from the LSTM language model by directly computing the negative log value of the predicted conditional probability p(xi |x1 . . . xi−1 ) from the softmax layer. Following the method in Hale et al. (2018) for estimating word surprisals from RNNG, we use word-synchronous beam search (Stern et al., 2017) to find a set of most likely incremental parses and sum their forward probabilities to approximate P(x1 , . . . xi ) and P(x1 , . . . xi−1 ) for computing the surprisal. We set the action beam size to 100 and word beam size to 10. We ensured that the correct incremental RNNG parses were present on the beam immediately before and throughout the material over which surprisal was calculated through manual spot inspection; the correct parse was almost always at the top of the beam. 2.2.2 Wh-Licensing Interaction Unlike NPI, licensing, the filler—gap dependency is the covariance between a piece of"
N19-1334,W18-5423,1,0.195244,"the filler–gap dependency. Structural supervision thus provides data efficiency advantages over purely stringbased training of neural language models in acquiring human-like generalizations about non-local grammatical dependencies. 1 Introduction Long Short-Term Memory Recurrent Neural Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) have achieved state of the art language modeling performance (Jozefowicz et al., 2016) and have been shown to indirectly learn a number of non-local grammatical dependencies, such as subject-verb number agreement and filler-gap licensing (Linzen et al., 2016; Wilcox et al., 2018), although they fail to learn others, such as Negative Polarity Item and anaphoric pronoun licensing (Marvin and Linzen, 2018; Futrell et al., 2018). LSTMs, however, require large amounts of training data and remain relatively uninterpretable. One model that attempts to address both these issues is the Recurrent Neural Network Grammar (Dyer et al., 2016). RNNGs are generative models, which represent hierarchical syntactic structure and use neural control to deploy it in left-toright processing. They can achieve state-of-the-art broad-coverage scores on language modeling and phrase structure pa"
N19-5005,P18-1043,0,0.0294535,"s. To achieve this goal, most work in CL has focused on developing models for different linguistic tasks such as semantic role labeling and natural language inference. However, recent research in CL has started investigating the missing ingredients required to move towards building systems with general linguistic intelligence. For example, one area of focus is multitask learning – building models that perform well on a number of linguistic tasks (e.g., Devlin et al., 2018). Other research has investigated the importance of introducing commonsense into natural language processing models (e.g., Rashkin et al., 2018). Despite the recent advances in the field, we are still far from systems that exhibit human-level linguistic intelligence: great performance on a set of predefined linguistics tasks does not result in systems that can understand and produce natural language in general settings. An alternative research direction is to build systems that mimic language acquisition and processing as it is performed by humans. Such a system might achieve the linguistic efficacy required for understanding and producing human languages. But we first need to understand how children so effortlessly learn their langua"
P03-1056,A00-2018,0,\N,Missing
P03-1056,J98-4004,0,\N,Missing
P03-1056,J82-3004,0,\N,Missing
P03-1056,W99-0623,0,\N,Missing
P03-1056,W00-1201,0,\N,Missing
P03-1056,C02-1126,0,\N,Missing
P03-1056,J03-4003,0,\N,Missing
P03-1056,P98-1115,0,\N,Missing
P03-1056,C98-1111,0,\N,Missing
P03-1056,C02-1145,0,\N,Missing
P04-1042,A00-2018,0,0.0258632,"Missing"
P04-1042,W03-1005,0,0.352649,"g null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional i"
P04-1042,P03-1055,0,0.702112,"g null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional i"
P04-1042,J02-3001,0,0.0282752,"ctic parsing is as an aid to semantic interpretation, in pursuit of broader goals of natural language understanding. Proponents of traditional ‘deep’ or ‘precise’ approaches to syntax, such as GB, CCG, HPSG, LFG, or TAG, have argued that sophisticated grammatical formalisms are essential to resolving various hidden relationships such as the source phrase of moved whphrases in questions and relativizations, or the controller of clauses without an overt subject. Knowledge of these hidden relationships is in turn essential to semantic interpretation of the kind practiced in the semantic parsing (Gildea and Jurafsky, 2002) and QA (Pasca and Harabagiu, 2001) literatures. However, work in statistical parsing has for the most part put these needs aside, being content to recover surface context-free (CF) phrase structure trees. This perhaps reflects the fact that context-free phrase structure grammar (CFG) is in some sense at the the heart of the majority of both formal and computational syntactic research. Although, upon introducing it, Chomsky (1956) rejected CFG as an adequate framework for natural language description, the majority of work in the last half century has used context-free structural descriptions a"
P04-1042,P02-1018,0,0.103897,"e problems SBAR WHNP-1 0 S NP VP PRP VBZ NP it sees *T*-1 Figure 1: Example of empty and nonlocal annotations from the Penn Treebank of English, including null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divi"
P04-1042,N04-1013,0,0.0243171,"elative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testin"
P04-1042,J93-4001,0,0.0114419,"investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and tree"
P04-1042,P03-1054,1,0.155811,"Missing"
P04-1042,2000.iwpt-1.20,0,0.121449,"he first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three anonymous reviewers, and from presentation at the Institute of Cognitive Science, University of Colorado at Boulder. The authors are also grateful to Dan Klein and Jenny Finkel for use of maximum-entropy software they wrote. This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligenc"
P04-1042,P02-1035,0,0.0422572,"r resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from"
P04-1042,A97-1014,0,0.476477,"ng that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testing. Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. Consistent with prior work and with common practice in statistical parsing, we stripped categories of all functiona"
P04-1042,J03-4003,0,\N,Missing
P04-1042,P03-1013,0,\N,Missing
P10-1119,P02-1026,0,0.0207092,"s of reading, listening, and even speaking are remarkably difficult. Good performance at each one requires integrating a range of types of probabilistic information and making incremental predictions on the basis of noisy, incomplete input. Despite these requirements, empirical work has shown that humans perform very well (e.g., Tanenhaus, SpiveyKnowlton, Eberhard, & Sedivy, 1995). Sophisticated models have been developed that explain many of these effects using the tools of computational linguistics and large-scale corpora to make normative predictions for optimal performance in these tasks (Genzel & Charniak, 2002, It is important to note that this notion of word difficulty is an abstraction over the actual task of reading, which is made up of more fine-grained decisions about how long to leave the eyes in their current position, and where to move them next, producing the series of relatively stable periods (fixations) and movements (saccades) that characterize the eye tracking record. While there has been much empirical work on reading at this fine-grained scale (see Rayner, 1998 for an overview), and there are a number of successful models (Reichle, Pollatsek, & Rayner, 2006; Engbert, Nuthmann, Richt"
P10-1119,W03-1009,0,0.0458823,"Missing"
P10-1119,N01-1021,0,0.0665547,"behavior, it suggests that humans are making rational use of all the information available to them in language processing. In the domain of incremental language comprehension, especially, there is a substantial amount of computational work suggesting that humans behave rationally (e.g., Jurafsky, 1996; Narayanan & Jurafsky, 2001; Levy, 2008; Levy, Reali, & Griffiths, 2009). Most of this work has taken as its task predicting the difficulty of each word in a sentence, a major result being that a large component of the difficulty of a word appears to be a function of its probability in context (Hale, 2001; Smith & Levy, 2008). Much of the empirical basis for this work comes from studying reading, where word difficulty can be related to the amount of time that a reader spends on a particular word. To relate these predictions about word difficulty to the data obtained in eye tracking experiments, the eye movement record has been summarized through word aggregate measures, such as the average duration of the first fixation on a word, or the amount of time between when a word is first fixated and when the eyes move to its right (‘go-past time’). A number of results in the study of realtime sentenc"
P10-1119,W04-3241,0,0.0473716,"Missing"
P10-1119,D08-1025,1,0.842709,"evy Department of Linguistics University of California, San Diego 9500 Gilman Dr, La Jolla, CA 92093-0108 {kbicknell,rlevy}@ling.ucsd.edu Abstract 2003; Keller, 2004; Levy & Jaeger, 2007; Jaeger, 2010). To the extent that the behavior of these models looks like human behavior, it suggests that humans are making rational use of all the information available to them in language processing. In the domain of incremental language comprehension, especially, there is a substantial amount of computational work suggesting that humans behave rationally (e.g., Jurafsky, 1996; Narayanan & Jurafsky, 2001; Levy, 2008; Levy, Reali, & Griffiths, 2009). Most of this work has taken as its task predicting the difficulty of each word in a sentence, a major result being that a large component of the difficulty of a word appears to be a function of its probability in context (Hale, 2001; Smith & Levy, 2008). Much of the empirical basis for this work comes from studying reading, where word difficulty can be related to the amount of time that a reader spends on a particular word. To relate these predictions about word difficulty to the data obtained in eye tracking experiments, the eye movement record has been summ"
P10-1119,J97-2003,0,0.101318,"the closest such position (2b). If m( j) ≥ β for all j &lt; `i , then the model initiates a saccade to n characters past the closest position to the right j &gt; `i for which m( j) &lt; α (2c).3 Finally, if no such positions exist to the right, the model stops reading the sentence (2d). Intuitively, then, the model reads by making a rightward sweep to bring its confidence in each character up to α, but pauses to move left if confidence in a previous character falls below β . 4.5 Implementation with wFSAs This model can be efficiently and simply implemented using weighted finite-state automata (wFSAs; Mohri, 1997) as follows: First, we begin with a wFSA representation of the language model, where each arc emits a single character (or is an epsilon-transition emitting nothing). To perform belief update given a new visual input, we create a new wFSA to represent the likelihood of each character from the sample. Specifically, this wFSA has only a single chain of states, where, e.g., the first and second state in the chain are connected by 27 (or fewer) arcs, which emit each of 3 The role of n is to ensure that the model does not center its visual field on the first uncertain character. We did not attempt"
P11-1094,P06-1032,0,0.629044,"tself can also be thought of as a machine ing grammatical errors by looking for contextual translation task, where we are trying to ‘translate’ a cues in a ±2 word window around a target word. sentence from an ‘incorrect grammar’ language to To identify errors, they searched for cues which did a ‘correct grammar’ language. Under this idea, the not appear in the correct usage of words. Eeg- use of statistical machine translation techniques to olofsson and Knutsson (2003) used rule-based meth- correct grammatical errors has also been explored. ods to approach the problem of discovering preposi- Brockett et al. (2006) uses phrasal SMT techniques tion and determiner errors of L2 writers, and var- to identify and correct mass noun errors of ESL stuious classifier-based methods using Maximum En- dents. D´esilets and Hermet (2009) use a round-trip tropy models have also been proposed (Izumi et al., translation from L2 to L1 and back to L2 to cor2003; Tetreault and Chodorow, 2008; De Felice and rect errors using an SMT system, focusing on errors Pulman, 2008). Some classifier-based methods can which link back to the writer’s native language. be used not only to identify errors, but also to deterDespite the unde"
P11-1094,A00-2019,0,0.379911,"Missing"
P11-1094,C08-1022,0,0.055501,"Missing"
P11-1094,2009.mtsummit-posters.3,0,0.468076,"Missing"
P11-1094,D08-1113,0,0.0205827,"model, and various noise models which introduce spelling errors, article choice errors, preposition choice errors, etc. 1 In reality, the language model will most likely produce sentences with errors as seen by humans, but from the modeling perspective, we assume that the language model is a perfect representation of the language for our task. 936 Figure 1: Example of noisy channel model All models are implemented using weighted finitestate tranducers (wFST). For operations on the wFSTs, we use OpenFST (Allauzen et al., 2007), along with expectation semiring code supplied by Markus Dryer for Dreyer et al. (2008). 4.1 Base language model The base language model is a bigram model implemented by using a weighted finite-state transducer (wFST). The model parameters are learned from the British National Corpus modified to use American English spellings with Kneser-Ney smoothing. To lower our memory usage, only bigrams whose words are found in the observed sentences, or are determined to be possible candidates for the correct words of the original sentence (due to the noise models) are used. While we use a bigram model here for simplicity, any probabilistic language model having a tractable intersection wi"
P11-1094,P02-1001,0,0.0532483,"e the probuse an unsupervised learning method to learn our pa- ability of the path given the output sentence, and get rameters. the expected counts of going over each erroneous To learn the parameters of the noise models, we and error-free arc to learn the parameters of the noise assume that the collected sentences are random out- model. To find a wFST with just the possible paths put of our model, and train our model using the for each observed sentence, we can compose the EM algorithm. This was done by making use of language-noise wFST with the observed sentence the V -expectation semiring (Eisner, 2002). The wFST. The observed sentence wFST is created in V -expectation semiring is a semiring in which the the following manner. Given an observed sentence, weight is defined as R≥0 × V , where R can be used an initial state is created. For each word in the sento keep track of the probability, and V is a vector tence, in the order appearing in the sentence, a new which can be used to denote arc traversal counts or state is added, and an arc is created going from the feature counts. The weight for each of the arcs in the previously added state to the newly added state. The noise models was set so"
P11-1094,P03-2026,0,0.357647,"Missing"
P11-1094,W07-0734,0,0.0148384,"oning is introduced, the lesions may not always reflect the actual errors found in human data, and it is difficult to replicate the actual tendency of humans to make a variety of different mistakes in a single sentence. Thus, this method of evaluation, which may be suitable for evaluating the correction performance of specific grammatical errors, would not be fit for evaluating our model’s overall performance. For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). Machine translation addresses the problem of changing a sentence in one language to a sentence of another. The task of correcting erroneous sentences can also be thought of as translating a sentence from a given language A, to another language B, where A is a broken language, and B is the correct language. Under this context, we can apply machine translation evaluation techniques to evaluate the performance of our system. Our model’s sentence correcMETEOR BLEU tions can be thought of as the output translation to be Original ESL sentences 0.8327 0.7540 evaluated. In order to use BLEU and METE"
P11-1094,P02-1040,0,0.112026,". In the case where artificial lesioning is introduced, the lesions may not always reflect the actual errors found in human data, and it is difficult to replicate the actual tendency of humans to make a variety of different mistakes in a single sentence. Thus, this method of evaluation, which may be suitable for evaluating the correction performance of specific grammatical errors, would not be fit for evaluating our model’s overall performance. For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). Machine translation addresses the problem of changing a sentence in one language to a sentence of another. The task of correcting erroneous sentences can also be thought of as translating a sentence from a given language A, to another language B, where A is a broken language, and B is the correct language. Under this context, we can apply machine translation evaluation techniques to evaluate the performance of our system. Our model’s sentence correcMETEOR BLEU tions can be thought of as the output translation to be Original ESL sentences 0.8327 0.7540 eva"
P11-1094,C08-1109,0,0.106554,"n the correct usage of words. Eeg- use of statistical machine translation techniques to olofsson and Knutsson (2003) used rule-based meth- correct grammatical errors has also been explored. ods to approach the problem of discovering preposi- Brockett et al. (2006) uses phrasal SMT techniques tion and determiner errors of L2 writers, and var- to identify and correct mass noun errors of ESL stuious classifier-based methods using Maximum En- dents. D´esilets and Hermet (2009) use a round-trip tropy models have also been proposed (Izumi et al., translation from L2 to L1 and back to L2 to cor2003; Tetreault and Chodorow, 2008; De Felice and rect errors using an SMT system, focusing on errors Pulman, 2008). Some classifier-based methods can which link back to the writer’s native language. be used not only to identify errors, but also to deterDespite the underlying commonality between the mine suggestions for corrections by using the scores tasks of machine translation and grammar correcor probabilities from the classifiers for other possi- tion, there is a practical difference in that the field ble words. While this is a plausible approach for of grammar correction suffers from a lack of good grammar correction, th"
P11-1106,J99-4004,0,0.0157015,"a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2 . To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial results in a chart. After completion of this bottom-up parse, every rule that will have non-zero probability in the intersection PCFG will be identifiable with a set of entries in the chart, but not all entries in this chart will have non-zero probability, since some are not connected to the root. Hence we perform a second, topdown Boolean-semiring parsing pass on the bottomup chart, throwing out entries that cannot be derived from the root. We can then include in the intersection grammar only those rules from the classic construction that can be ident"
P11-1106,N01-1021,0,0.76759,"curred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input In this paper we explore a more dramatic prediction of such an uncertain-input theory: that, when faced with sufficiently biasing input, comprehenders might under some circumstances adopt"
P11-1106,C88-1075,0,0.204526,"beliefs in grammatical analyses that require modification of the surface input itself. Our results also bring a new degree of nuance to surprisal theory, demonstrating that perceptual neighbors of true preceding input may need to be taken into account in order to estimate how surprising a comprehender will find subsequent input to be. Beyond the domain of psycholinguistics, the methods employed here might also be usefully applied to practical problems such as parsing of degraded or fragmentary sentence input, allowing joint constraint derived from grammar and available input to fill in gaps (Lang, 1988). Of course, practical applications of this sort would raise challenges of their own, such as extending the grammar to broader coverage, which is delicate here since the surface input places a weaker check on overgeneration from the grammar than in traditional probabilistic parsing. Larger grammars also impose a technical burden since parsing uncertain input is in practice more computationally intensive than parsing clean input, raising the question of what approximate-inference algorithms might be well-suited to processing uncertain input with grammatical knowledge. Answers to this question m"
P11-1106,D08-1025,1,0.935248,"bsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory. 1 Introduction In most formal theories of human sentence comprehension, input recognition and syntactic analysis are taken to be distinct processes, with the only feedback from syntax to recognition being prospective prediction of likely upcoming input (Jurafsky, 1996; Narayanan and Jurafsky, 1998, 2002; Hale, 2001, 2006; Levy, 2008a). Yet a system making optimal use of all available information might be expected to perform fully joint inference on sentence identity and structure given perceptual input, using linguistic knowledge both prospectively and retrospectively in drawing inferences as to how raw input should be segmented and recognized as a sequence of linguistic tokens, and about the degree to which each input In this paper we explore a more dramatic prediction of such an uncertain-input theory: that, when faced with sufficiently biasing input, comprehenders might under some circumstances adopt a grammatical ana"
P11-1106,levy-andrew-2006-tregex,1,0.534085,"ith probabilities estimated from the parsed Brown corpus. Brown corpus (Kuˇcera and Francis, 1967; Marcus et al., 1994), covering sentence-initial subordinate clause and locative-inversion constructions.4,5 The non-terminal rewrite rules are shown in Table 1, along with their probabilities; of terminal rewrite rules for all words which either appear in the sentences to be parsed or appeared at least five times in the corpus, with probabilities estimated by relative frequency. As we describe in the following two sections, un4 Rule counts were obtained using tgrep2/Tregex patterns (Rohde, 2005; Levy and Andrew, 2006); the probabilities given are relative frequency estimates. The patterns used can be found at http://idiom.ucsd.edu/˜rlevy/papers/ acl2011/tregex_patterns.txt. 5 Similar to the case noted in Footnote 2, a small number of VP -> V , PP ... rules can be found in the parsed Brown corpus. However, the PPs involved are overwhelmingly (i) set expressions, such as for example, in essence, and of course, or (ii) manner or temporal adjuncts. The handful of true locative PPs (5 in total) are all parentheticals intervening between the verb and a complement strongly selected by the verb (e.g., [VP means, i"
P11-1106,W03-3016,0,0.152881,"D → PP VBD SBAR → INSBAR S VP → VBD RB VP → VBD PP VP → VBD NP VP → VBD PP PP VP → VBD RP VP → VBD VP → VBD JJ PP → IN NP NP → DT NN NP → NNS NP → NNP NP → DT NNS NP → PRP NP → NN 1.000000 0.003257 0.012289 0.041753 0.942701 1.000000 1.000000 0.002149 0.202024 0.393660 0.028029 0.005731 0.222441 0.145966 1.000000 0.274566 0.047505 0.101198 0.045082 0.412192 0.119456 certain input is represented as a weighted finite-state automaton (WFSA), allowing us to represent the incremental inferences of the comprehender through intersection of the input WFSA with the PCFG above (Bar-Hillel et al., 1964; Nederhof and Satta, 2003, 2008). 4.1 Uncertain-input representations Table 1: A small PCFG (lexical rewrite rules omitted) covering the constructions used in (4)–(6), with probabilities estimated from the parsed Brown corpus. Brown corpus (Kuˇcera and Francis, 1967; Marcus et al., 1994), covering sentence-initial subordinate clause and locative-inversion constructions.4,5 The non-terminal rewrite rules are shown in Table 1, along with their probabilities; of terminal rewrite rules for all words which either appear in the sentences to be parsed or appeared at least five times in the corpus, with probabilities estimate"
P11-1106,D09-1034,0,0.013548,"theoretic measure of the log of the inverse of the word’s conditional probability (also called its “surprisal” or “Shannon information content”) in its intra-sentential context w1,...,i−1 and extra-sentential context Ctxt: Effort(wi ) ∝ log 1 P (wi |w1...i−1 , Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt.) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). Surprisal has had considerable success in accounting for one of the best-known phenomena in psycholinguistics, the GARDEN - PATH SENTENCE (Frazier, 1979), in which a local ambiguity biases the comprehender’s incremental syntactic interpretation so strongly that upon encountering disambiguating input the correct interpretation can only be recovered with great effort, if at all. The most famous example is (1) below (Bever, 1970): (1) The horse raced"
P11-1106,J07-4003,0,0.0134236,"g constant β . Note that we must have α &lt; 1, placing a constraint on is 1−α the value that γ can take (above which the normalizing constant diverges). 7 Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the av1060 which are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Hence we are left with finding the partition function of a WCFG, which cannot be computed exactly, but a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2 . To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial results in a chart. After c"
P11-1106,J95-2002,0,0.028517,"its normalizing constant β . Note that we must have α &lt; 1, placing a constraint on is 1−α the value that γ can take (above which the normalizing constant diverges). 7 Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the av1060 which are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Hence we are left with finding the partition function of a WCFG, which cannot be computed exactly, but a number of approximation methods are known (Stolcke, 1995; Smith and Johnson, 2007; Nederhof and Satta, 2008). In practice, the computation required to compute the partition function under any of these methods increases with the size of the WCFG resulting from the intersection, which for a binarized PCFG with R rules and an n-state WFSA is Rn2 . To increase efficiency we implemented what is to our knowledge a novel method for finding the minimal grammar including all rules that will have non-zero probability in the intersection. We first parse the WFSA bottom-up with the item-based method of Goodman (1999) in the Boolean semiring, storing partial re"
P11-1106,J93-2004,0,\N,Missing
P14-1103,J98-2006,0,0.353403,"could be integrated into a joint constraint learning process in the form of a naturalness bias on the constraint weights or a phonologically-motivated replacement for the IBP prior. The results presented here use binary constraints, where each candidate violates each constraint only once, a result of the IBP’s restriction to binary matrices. Non-binarity can be handled by using the binary matrix M to indicate whether a candidate violates a constraint, with a second 1101 distribution determining the number of violations. Alternately, a binary matrix can directly capture non-binary constraints; Frank and Satta (1998) converted existing non-binary constraints into a binary OT system by representing non-binary constraints as a set of equally-weighted overlapping constraints, each accounting for one violation. The non-binary harmony constraint, for instance, becomes a set {*(at least one disharmony), *(at least two disharmonies), etc.}. Lastly, the Wolof vowel harmony problem provides a test case with overlaps in the candidate sets for different inputs. This candidate overlap helps the model find appropriate constraint structures. Analyzing other phenomena may require the identification of appropriate abstra"
P16-1225,H92-1086,0,0.522577,"5.2 Behavioral Evaluation of Systematicity Measure We empirically tested whether the systematicity measure based on SMLKR regression error accords with na¨ıve human judgments about how well-suited a word’s form is to its meaning (its “phonosemantic feeling”) (Stefanowitsch, 2002). We recruited 60 native English-speaking participants through Mechanical Turk, and asked them to judge the phonosemantic feeling of the 60 words in Table 1 on a sliding scale from 1 to 5.2 We used Cronbach’s α to measure inter-annotator reliability at α = 0.96, indicating a high degree of interannotator reliability (Cronbach, 1951; George, 2000). The results showed that the words in the SMLKR list were rated higher for phonosemantic feeling than the words in the Correlation and Random lists. We fit a parametric linear mixedeffects model to the phonosemantic feeling judgments (Baayen et al., 2008), as implemented in the lme4 library for R. As fixed effects, we entered the list identity (SMLKR, Correlation, Random), the word length, and the log frequency of the word in our corpus. Our random effects structure included a random intercept for word, and random subject slopes for all fixed effects, with all correlations allo"
W04-0902,copestake-flickinger-2000-open,0,0.0772614,"ies the implicature that in some possible solution, three sculptures are indeed exhibited in the same room. Systematic calculation of presuppositions and implicatures has been given less attention in NLP and is less understood than the calculation of meaning. Yet computing and verifying them can provide valuable hints to the system whether it understood the meaning of the text correctly. 4 Morpho-Syntactic Analysis While traditional hand-built grammars often include a rich semantics, we have found their coverage inadequate for the logic puzzles task. For example, the English Resource Grammar (Copestake and Flickinger, 2000) fails to parse any of the sentences in Figure 1 for lack of coverage of some words and of several different syntactic structures; and parsable simplified versions of the text produce dozens of unranked parse trees. For this reason, we use a broadcoverage statistical parser (Klein and Manning, 2003) trained on the Penn Treebank. In addition to robustness, treebank-trained statistical parsers have the benefit of extensive research on accurate ambiguity resolution. Qualitatively, we have found that the output of the parser on logic puzzles is quite good (see §10). After parsing, each word in the"
W04-0902,J02-3001,0,0.0173597,"ambiguities, and incomplete domain knowledge. Recent work in NLP has consequently focused on more robust, broadcoverage techniques, but with the effect of overall shallower levels of processing. Thus, state-of-the-art work on probabilistic parsing (e.g., (Collins, 1999)) provides a good solution to robust, broad coverage parsing with automatic and frequently successful ambiguity resolution, but has largely ignored issues of semantic interpretation. The field of Question Answering (Pasca and Harabagiu, 2001; Moldovan et al., 2003) focuses on simple-fact queries. And socalled semantic parsing (Gildea and Jurafsky, 2002) provides as end output only a flat classification of semantic arguments of predicates, ignoring much of the semantic content, such as quantifiers. A major research question that remains unanswered is whether there are methods for get† Department of Linguistics Stanford University Stanford, CA 94305-2150, USA rog@stanford.edu ting from a robust “parse-anything” statistical parser to a semantic representation precise enough for knowledge representation and automated reasoning, without falling afoul of the same problems that stymied the broad application of traditional approaches. This paper pre"
W04-0902,W01-0521,0,0.0288028,"emmer. A few tree-transformation rules are applied on the parse trees to make them more convenient for combinatorial semantics. Most of them are general, e.g. imposing a binary branching structure on verb phrases, and grouping expressions like “more than”. A few of them correct some parsing errors, such as nouns marked as names and vice-versa. There is growing awareness in the probabilistic parsing literature that mismatches between training and test set genre can degrade parse accuracy, and that small amounts of correct-genre data can be more important than large amounts of wrong-genre data (Gildea, 2001); we have found corroborating evidence in misparsings of noun phrases common in puzzle texts, such as “Sculptures C and E”, which do not appear in the Wall Street Journal corpus. Depending on the severity of this problem, we may hand-annotate a small amount of puzzle texts to include in parser training data. 5 Combinatorial Semantics Work in NLP has shifted from hand-built grammars that need to cover explicitly every sentence structure and that break down on unexpected inputs to more robust statistical parsing. However, grammars that involve precise semantics are still largely hand-built (e.g."
W04-0902,P99-1042,0,0.0587378,"information-extraction and question-answering as a substitute for deep understanding. A prerequisite for successful inference is precise understanding of semantic phenomena like modals and quantifiers, in contrast with much current NLP work that just ignores such items. We believe that representations with a well-defined model-theoretic semantics are required. Finally, the task has a clear evaluation metric because the puzzle texts are designed to yield exactly one correct answer to each multiplechoice question. Moreover, the domain is another example of “found test material” in the sense of (Hirschman et al., 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. While our current system is not a real world application, we believe that the methods being developed could be used in applications such as a computerized office assistant that must understand requests such as: “Put each file containing a task description in a different directory.” 2 (B) Sculptures E and H (C). . . System Overview This section explains the languages we use to represent"
W04-0902,P03-1054,1,0.0301978,"luable hints to the system whether it understood the meaning of the text correctly. 4 Morpho-Syntactic Analysis While traditional hand-built grammars often include a rich semantics, we have found their coverage inadequate for the logic puzzles task. For example, the English Resource Grammar (Copestake and Flickinger, 2000) fails to parse any of the sentences in Figure 1 for lack of coverage of some words and of several different syntactic structures; and parsable simplified versions of the text produce dozens of unranked parse trees. For this reason, we use a broadcoverage statistical parser (Klein and Manning, 2003) trained on the Penn Treebank. In addition to robustness, treebank-trained statistical parsers have the benefit of extensive research on accurate ambiguity resolution. Qualitatively, we have found that the output of the parser on logic puzzles is quite good (see §10). After parsing, each word in the resulting parse trees is converted to base form by a stemmer. A few tree-transformation rules are applied on the parse trees to make them more convenient for combinatorial semantics. Most of them are general, e.g. imposing a binary branching structure on verb phrases, and grouping expressions like"
W04-0902,N03-1022,0,0.076694,"Missing"
W04-0902,J82-3002,0,0.389051,"nford University Stanford, CA 94305-9040, USA {iddolev|wcmac|manning}@cs.stanford.edu Abstract This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE). We highlight key challenges, and discuss the representations and performance of the prototype system. 1 Introduction Traditional approaches to natural language understanding (Woods, 1973; Warren and Pereira, 1982; Alshawi, 1992) provided a good account of mapping from surface forms to semantic representations, when confined to a very limited vocabulary, syntax, and world model, and resulting low levels of syntactic/semantic ambiguity. It is, however, difficult to scale these methods to unrestricted, general-domain natural language input because of the overwhelming problems of grammar coverage, unknown words, unresolvable ambiguities, and incomplete domain knowledge. Recent work in NLP has consequently focused on more robust, broadcoverage techniques, but with the effect of overall shallower levels of"
W04-0902,J03-4003,0,\N,Missing
W11-1421,P06-1032,0,0.0272706,"mostly focused on rule-based methods and error identification rather than correction. However, there has been a recent outgrowth in the application of machine translation (MT) techniques to address the problem of single-language grammar correction. Park and Levy (2011) propose a noisy channel model for learning to correct various types of errors, including article and preposition errors, wordform errors, and spelling mistakes, to which this paper is an extension. As the present work builds on Park and Levy’s basic model, we will reserve a more detailed discussion of their work for Section 3. Brockett et al. (2006) use phrasal SMT techniques to identify and correct mass noun errors of ESL students with some success, but they correct no other production error classes to our knowledge. Lee and Seneff (2006) learn a method to aid ESL students in language acquisition by reducing sentences to their canonical form, i.e. a lemmatized form devoid of articles, prepositions, and auxiliaries, and then building an over-specified lattice by reinserting all word inflections and removed word classes. They then score this lattice using a trigram model and PCFG. While this method has many advantages, it does not take in"
W11-1421,J93-2003,0,0.0345978,"stic context-free grammar, we can “correct” an observed sentence by inferring the most likely unobserved sentence from which it originated. More concretely, given that we know f , we can compute argmaxw′ p(w ′ |w, f, θ), where w is the observed sentence, θ is the language model, and w ′ is the “corrected,” unobserved sentence. Under this view, some w ′ drawn from the distribution θ is subjected to some noise process f , which perturbs the sentence author’s intended meaning and outputs w. We perform this computation in the standard way from the statistical machine translation (SMT) literature (Brown et al., 1993), namely by using Bayes’ theorem to write p(w ′ |θ)p(w|w ′ , f, θ) p(w ′ |w, f, θ) = p(w|θ) Since the denominator of the RHS is independent of w ′ , we can rewrite our argmax as argmax p(w ′ |θ)p(w|w ′ , f, θ) w′ We have now decomposed our original equation into two manageable parts, a prior belief about the grammaticality of an unobserved sentence w ′ , which we can compute using a language model θ learned separately using standard supervised techniques (in particular, n-gram estimation), and the probability of the observed sentence w given w ′ , f , and θ. Together, these constitute a noisy"
W11-1421,P96-1041,0,0.0302705,"ree major components to the model of a sentence: a language prior, a noise model, and an observed sentence. Each of these is implemented as a wFST and composed 172 주요한 together into a single transducer whose accepting paths represent all possibilities of transducing from an (unobserved) input sentence to the (observed) output sentence, with the path weight being associated probability. See Figure 2 for an example. 3.2.2 Language Model For our language model, we use a Kneser-Ney smoothed trigram model learned from a version of the British National Corpus modified to use Americanized spellings (Chen and Goodman, 1996; Burnard, 1995). The implementation of an n-gram model as a wFST requires that each state represent a context, and so one must necessarily instantiate arcs for all words in the alphabet from each state. In order to reduce model size and minimize memory usage, it is standard practice to remove relatively uninformative higher-order n-grams from the model, but under the wFST regime one cannot, for example, remove some trigrams from a bigram context without removing all of them. Instead, we retain only the 1,000 most informative bigram contexts, as measured by the Kullback-Leibler divergence betw"
W11-1421,2009.mtsummit-posters.3,0,0.0766875,"Missing"
W11-1421,D08-1113,0,0.0585058,"Missing"
W11-1421,P02-1001,0,0.00993839,"frequency end if return C 3.2.4 Sentence Models Sentences are simply identity transducers, i.e. wFSTs with n + 1 states for a sentence of length n and a single arc between each state 0 ≤ i &lt; n and state i+1 labeled with input and output token i from the sentence and weight 1. 3.2.5 Training and Decoding For training, we hold language model parameters constant and use expectation maximization (Dempster et al., 1977) to learn noise model parameters as follows. We replace language model input symbols and sentence model output symbols with the empty symbol ǫ and use the V-expectation semiring of Eisner (2002) to annotate noise model arcs with initial parameter values. This is our M-step. Then, we compose the language, noise, and sentence models, which produces a transducer with only ǫ-labeled arcs, and use ǫ-removal to move expectation information into a single state from which we can easily read off expected noise model parameter counts thanks to the V-expectation semiring’s bookkeeping (Eisner, 2002; Mohri, 2001). We repeat this process over a batch of training sentences and add the results together to yield a final vector of expected counts. This is our E-step. Finally, we normalize the expecte"
W11-1421,N10-1017,0,0.0141413,"ss noun errors of ESL students with some success, but they correct no other production error classes to our knowledge. Lee and Seneff (2006) learn a method to aid ESL students in language acquisition by reducing sentences to their canonical form, i.e. a lemmatized form devoid of articles, prepositions, and auxiliaries, and then building an over-specified lattice by reinserting all word inflections and removed word classes. They then score this lattice using a trigram model and PCFG. While this method has many advantages, it does not take into account the full context of the original sentence. Kok and Brockett (2010) use random walks over bi- and multilingual graphs generated by aligning English sentences with translations in 10 other European languages to learn paraphrases, which they then evaluate in the context of the original sentence. While their approach shares many high-level similarities with ours, both their task, paraphrasing correct sentences, and the details of their methodology are divergent from the present work. D´esilets and Hermet (2009) employ round-trip machine translation from L1 to L2 and back again to correct second language learner text by keeping track of the word alignments betwee"
W11-1421,W07-0734,0,0.129909,"rformed using OpenFST (Allauzen et al., 2007), an open source weighted finite-state transducer library written in C++. Additionally, we use the V-expectation semiring code of Dreyer et al. (2008) for training. 3.2.7 Evaluation The most probable unobserved sentence w ′ from which the observed sentence w was generated under our model, argmaxw′ p(w ′ |θ)p(w|w ′ , f, θ), can be read off from the input of the transducer produced 174 during the decoding process. In order to evaluate its quality versus the observed ESL sentence, we use the METEOR2 and BLEU evaluation metrics for machine translation (Lavie and Agarwal, 2007; Papineni et al., 2002). This evaluation is performed using a set of human-corrected sentences gathered via Amazon Mechanical Turk, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant. 8 workers were assigned to correct each sentence from the development and evaluation sets described in Section 3.1, and so after filtering we had 8 or fewer unique corrected versions per sentence available for evaluation. We note that the use of METEOR and BLEU is justified inasmuch as the process of grammar correction is"
W11-1421,P02-1040,0,0.0800712,"lauzen et al., 2007), an open source weighted finite-state transducer library written in C++. Additionally, we use the V-expectation semiring code of Dreyer et al. (2008) for training. 3.2.7 Evaluation The most probable unobserved sentence w ′ from which the observed sentence w was generated under our model, argmaxw′ p(w ′ |θ)p(w|w ′ , f, θ), can be read off from the input of the transducer produced 174 during the decoding process. In order to evaluate its quality versus the observed ESL sentence, we use the METEOR2 and BLEU evaluation metrics for machine translation (Lavie and Agarwal, 2007; Papineni et al., 2002). This evaluation is performed using a set of human-corrected sentences gathered via Amazon Mechanical Turk, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant. 8 workers were assigned to correct each sentence from the development and evaluation sets described in Section 3.1, and so after filtering we had 8 or fewer unique corrected versions per sentence available for evaluation. We note that the use of METEOR and BLEU is justified inasmuch as the process of grammar correction is translation from an ungr"
W11-1421,P11-1094,1,0.808438,"{rdwest,yapark}@cs.ucsd.edu Abstract We present a novel noisy channel model for correcting text produced by English as a second language (ESL) authors. We model the English word choices made by ESL authors as a random walk across an undirected bipartite dictionary graph composed of edges between English words and associated words in an author’s native language. We present two such models, using cascades of weighted finitestate transducers (wFSTs) to model language model priors, random walk-induced noise, and observed sentences, and expectation maximization (EM) to learn model parameters after Park and Levy (2011). We show that such models can make intelligent word substitutions to improve grammaticality in an unsupervised setting. 1 Introduction How do language learners make word choices as they compose text in a language in which they are not fluent? Anyone who has attempted to learn a foreign language can attest to spending a great deal of time leafing through the pages of a bilingual dictionary. However, dictionaries, especially those without a wealth of example sentences or accompanying word sense information, can often lead even the most scrupulous of language learners in the wrong direction. Con"
W12-1703,P10-1119,1,0.666308,"quency, predictability, and length of words. Both models assume that word frequency, predictability, and length affect eye movements in reading by affecting word recognition, yet neither one models the process of identifying words from visual information. Rather, each of these models directly specifies the effects of these variables on exogenous word processing functions, and the eye movements the models produce are sensitive to these functions’ output. Thus, this approach cannot answer the question of why these linguistic variables have the effects they do on eye movement behavior. Recently, Bicknell and Levy (2010) presented a model of eye movement control in reading that directly models the process of identifying the text from visual input, and makes eye movements to maximize the efficiency of the identification process. Bicknell and Levy (2012) demonstrated that this rational model produces effects of word frequency and predictability that qualitatively match those of humans: words that are less frequent and less predictable receive more and longer fixations. Because this model makes eye movements to maximize the efficiency of the identification process, this result gives an answer for the reason why"
W12-1703,J97-2003,0,0.0180197,"are given by the probability of each possible identity under the language model. On each timestep, the model obtains a visual input string as described above and calculates the likelihood of generating that string from each possible identity of the text. The model then updates its beliefs about the text via standard Bayesian inference: multiplying the probability of each text identity under its prior beliefs by the likelihood of generating the visual input string from that text identity and normalizing. We compactly represent all of these distributions using weighted finite-state transducers (Mohri, 1997) using the OpenFST library (Allauzen, Riley, Schalkwyk, Skut, & Mohri, 2007), and implement belief update with transducer composition and weight pushing. 3.5 Behavior policy The model uses a simple policy with two parameters, α and β , to decide between actions based on the marginal probability m of the most likely character c in each position j, m( j) = max p(w j = c) c where w j indicates the character in the jth position. A high value of m indicates relative confidence about the character’s identity, and a low value relative uncertainty. Because our extension has uncertainty about the absol"
W12-1703,J00-4006,0,\N,Missing
W12-1706,J95-4004,0,0.193579,"probabilities using the algorithm of Stolcke (1995). We evaluate each hierarchical model under each type of surprisal (POS, total, lexical-only, and syntactic-only), where possible. 4.1 Data Sets Each syntactic model is trained on sections 2-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1994), and tested on the Dundee Corpus (Kennedy and Pynte, 2005), which contains reading time measures for 10 subjects over a corpus of 2,391 sentences of naturally occurring text. Gold-standard POS tags for the Dundee corpus are obtained automatically using the Brill tagger (Brill, 1995). Frank and Bod (2011) exclude subject/word pairs from evaluation if any of the following conditions hold true: “the word was not fixated, was presented as the first or last on a line, was attached to punctuation, contained more than one capital letter, or contained a non-letter (this included clitics)”. This leaves 191,380 subject/word pairs in the data set published by Frank and Bod (2011). Because we consider lexicalized hierarchical models in addition to unlexicalized ones, we additionally exclude subject/word pairs where the word is “unknown” to the model.5 This leaves us with a total of"
W12-1706,N01-1021,0,0.943412,"on richer contexts, and show that this model performs still better. Our findings demonstrate that Frank and Bod (2011)’s strong claim that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization improves the psychological accuracy of hierarchical models. 2 Related Work Several broad-coverage experimental studies demonstrate that surprisal under a hierarchical syntactic model predicts human processing difficulty on a corpus of naturally occurring text, even after word-level factors have been taken into account. Under surprisal theory (Hale, 2001; Levy, 2008), processing difficulty at word wi is proportional to reading time at wi , which in turn is proportional to the surprisal of wi in the context in which it is observed: surprisal(wi ) = −log(pr(wi |context)). Typically, context ≈ w1 ...wi−1 . Computing surprisal(wi ) thus reduces to computing −log(pr(wi |w1 ...wi − 1)). Henceforth, we refer to this original formulation of surprisal as total surprisal. Boston et al. (2008) show that surprisal estimates from a lexicalized dependency parser (Nivre, 2006) and an unlexicalized PCFG are significant predictors of reading times on the Germ"
W12-1706,J98-4004,0,0.0244651,"lities. Frank and Bod (2011) use PSG’s that generate POS tag sequences, not words. Under such grammars, the prefix probability of a tag sequence t is the sum of the probabilities of all trees T : yield(T ) = t1 ...ti , where the probability of each tree T is the product of the probabilities of the rules used in the derivation of T . Vanilla PCFG’s, a special case of PSG’s in which the probability of a rule depends only on the identity of the parent node, achieve sub-optimal parsing accuracy relative to grammars in which the probability of each rule depends on a richer context (Charniak, 1996; Johnson, 1998; Klein and Manning, 2003). To this end, Frank and Bod (2011) explore several variants of PSG’s conditioned on successively richer contexts, including ancestor models (which condition rule expansions on ancestor nodes from 1-4 levels up in the tree) and ancestor+sibling models (which condition rule expansions on the ancestor’s left sibling as well). Both sets of grammars also conAuthors Model Surprisal POS Observed Context ti ....ti−1 Latent Context Trees T with yield t1 ...ti−1 Predicted Event ti Boston et al. (2008) Demberg and Keller (2008) Roark et al. (2009) Frank and Bod (2011) This Work"
W12-1706,P03-1054,0,0.0446652,"nd Bod (2011) use PSG’s that generate POS tag sequences, not words. Under such grammars, the prefix probability of a tag sequence t is the sum of the probabilities of all trees T : yield(T ) = t1 ...ti , where the probability of each tree T is the product of the probabilities of the rules used in the derivation of T . Vanilla PCFG’s, a special case of PSG’s in which the probability of a rule depends only on the identity of the parent node, achieve sub-optimal parsing accuracy relative to grammars in which the probability of each rule depends on a richer context (Charniak, 1996; Johnson, 1998; Klein and Manning, 2003). To this end, Frank and Bod (2011) explore several variants of PSG’s conditioned on successively richer contexts, including ancestor models (which condition rule expansions on ancestor nodes from 1-4 levels up in the tree) and ancestor+sibling models (which condition rule expansions on the ancestor’s left sibling as well). Both sets of grammars also conAuthors Model Surprisal POS Observed Context ti ....ti−1 Latent Context Trees T with yield t1 ...ti−1 Predicted Event ti Boston et al. (2008) Demberg and Keller (2008) Roark et al. (2009) Frank and Bod (2011) This Work Demberg and Keller (2008)"
W12-1706,H94-1020,0,0.0733054,"prisals under the hierarchical models. For the PSG’s available under the Roark et al. (2009) parser, we use that parser to calculate approximate prefix probabilities using beam search. For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy4 to calculate exact prefix probabilities using the algorithm of Stolcke (1995). We evaluate each hierarchical model under each type of surprisal (POS, total, lexical-only, and syntactic-only), where possible. 4.1 Data Sets Each syntactic model is trained on sections 2-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1994), and tested on the Dundee Corpus (Kennedy and Pynte, 2005), which contains reading time measures for 10 subjects over a corpus of 2,391 sentences of naturally occurring text. Gold-standard POS tags for the Dundee corpus are obtained automatically using the Brill tagger (Brill, 1995). Frank and Bod (2011) exclude subject/word pairs from evaluation if any of the following conditions hold true: “the word was not fixated, was presented as the first or last on a line, was attached to punctuation, contained more than one capital letter, or contained a non-letter (this included clitics)”. This leave"
W12-1706,N07-1051,0,0.0444615,"...wi−1 ti w1 ...wi−1 Trees T with yield w1 ...wi−1 ; ti wi Seq. SyntacticOnly LexicalOnly POS ti ....ti−1 – ti Seq. Total w1 ...wi−1 t1 ...ti−1 with yield w1 ...wi−1 wi Hier. Table 1: Contexts and events used to produce surprisal measures under various probabilistic syntactic models. T refers to trees; t refers to POS tags; and w refers to words. dition rule expansions on the current head node2 . 3.3 Echo State Networks In addition to the grammars over POS tag sequences used by Frank and Bod (2011), we evaluate PSG’s over word sequences. We also include the state-of-the-art Berkeley grammar (Petrov and Klein, 2007) in our comparison. Syntactic categories in the Berkeley grammar are automatically split into fine-grained subcategories to improve the likelihood of the training corpus under the model. This increased expressivity allows the parser to achieve state-of-the-art automatic parsing accuracy, but increases grammar size considerably.3 Unlike Markov models, ESN’s (J¨ager, 2001) can capture long-distance dependencies. ESN’s are a type of recurrent neural network (Elman, 1991) in which only the weights from the hidden layer to the output layer are trained; the weights from the input layer to the hidden"
W12-1706,D09-1034,0,0.362784,"periments carefully designed to isolate a particular processing phenomenon. Several broad-coverage experimental studies have shown that surprisal under hierarchical syntactic models predicts human processing difficulty on large corpora of naturally occurring text, even after word-level factors have been taken into 61 In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 61–69, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics account (Boston et al., 2008; Demberg and Keller, 2008; Roark et al., 2009). Despite this evidence, in recent work Frank and Bod (2011) challenge the notion that hierarchical syntactic structure is strictly necessary to predict reading times. They compare per-word surprisal predictions from unlexicalized hierarchical and sequential models of syntactic structure along two axes: linguistic accuracy (how well the model predicts the test corpus) and psychological accuracy (how well the model predicts observed reading times on the test corpus). They find that, while hierarchical phrase-structure grammars (PSG’s) achieve better linguistic accuracy, sequential echo state ne"
W12-1706,J01-2004,0,0.744924,"hical and sequential models used in Frank and Bod (2011) vanish. Second, while they restrict their comparisons to un1 By robustly estimated, we mean that these probabilities are estimated from larger corpora and use a better smoothing method (Kneser-Ney) than the lexical n-grams of Frank and Bod (2011). 62 lexicalized models over part-of-speech (POS) tags, we investigate the lexicalized versions of each hierarchical model, and show that lexicalization significantly improves psychological accuracy. Third, while they explore only a subset of the PSG’s implemented under the incremental parser of Roark (2001), we explore a state-of-the-art lexicalized hierarchical model that conditions on richer contexts, and show that this model performs still better. Our findings demonstrate that Frank and Bod (2011)’s strong claim that sequential models predict reading times better than hierarchical models is premature, and also that lexicalization improves the psychological accuracy of hierarchical models. 2 Related Work Several broad-coverage experimental studies demonstrate that surprisal under a hierarchical syntactic model predicts human processing difficulty on a corpus of naturally occurring text, even a"
W12-1706,J95-2002,0,0.22171,"child node, if the head node is not yet available(Roark, 2001). 3 To make parsing with the Berkeley grammar tractable under the prefix probability parser, we prune away all rules with probability less than 10−4 . 64 Methods We use two incremental parsers to calculate surprisals under the hierarchical models. For the PSG’s available under the Roark et al. (2009) parser, we use that parser to calculate approximate prefix probabilities using beam search. For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy4 to calculate exact prefix probabilities using the algorithm of Stolcke (1995). We evaluate each hierarchical model under each type of surprisal (POS, total, lexical-only, and syntactic-only), where possible. 4.1 Data Sets Each syntactic model is trained on sections 2-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1994), and tested on the Dundee Corpus (Kennedy and Pynte, 2005), which contains reading time measures for 10 subjects over a corpus of 2,391 sentences of naturally occurring text. Gold-standard POS tags for the Dundee corpus are obtained automatically using the Brill tagger (Brill, 1995). Frank and Bod (2011) exclude subject/"
W18-5423,Q16-1037,0,0.144913,"Missing"
W18-5423,C18-1012,0,0.453841,"ava model). 5 While the Google model was trained on ten times more data, contained ten times as many hidden units and uses character CNN embeddings, its performance was not qualitatively more human-like than the Gulordava model. Both models failed to correctly generalize island constraints in two conditions: The Google model failed to learn that-headed Complex-NP Islands, the Gulordava model to learn Wh-Islands, and both failed to learn Subject Islands. These results indicate that— beyond a certain point—increased model size and training regimen give diminishing returns. In other recent work, Chowdhury and Zamparelli (2018) tested the ability of neural networks to separate grammatical from ungrammatical extractions using similar metrics to ours, finding that their neural networks do not represent the unboundedness of filler–gap dependencies nor certain strong island constraints. We believe the difference between our results and theirs is due to experimental design: They choose to measure the probability of the question mark punctuation as a proxy for the RNNs gap expectation, and use sentence schemata instead of hand-engineered experimental items. While Chowdhury and Zamparelli (2018) conclude that the networks"
W18-5423,N18-1108,0,0.0582694,".1 Language models We study the behavior of two pre-existing LSTMs trained on a language modeling objective over English text. Our first model is presented in Jozefowicz et al. (2016) under the name BIG LSTM+CNN Inputs; we call it the Google model. It was trained on the One Billion Word Benchmark (Chelba et al., 2013) and has two hidden layers with 8196 units each. It uses the output of a character-level Convolutional Neural Network (CNN) as input to the LSTM. This model has the best published perplexity for English text. Our second model is the one presented in the supplementary materials of Gulordava et al. (2018), which we call the Gulordava model. Trained on 90 million tokens of English Wikipedia, it has two hidden layers of 650 units each. Our goal in using these models is to provide two samples of the state-of-the-art. As a baseline, we also study an n-gram model trained on the One Billion Word Benchmark (a 5-gram model with modified Kneser-Ney interpolation, fit by KenLM with default parameters) (Heafield et al., 2013). 2.2 Dependent variable: Surprisal We investigate RNN behavior primarily by studying the surprisal values that an RNN assigns to words and sentences. Surprisal is log inverse probab"
W18-5423,N01-1021,0,0.792933,"s are sensitive to various constraints: wh-islands, adjunct islands, complex NP islands, and subject islands. We find that the language models are sensitive to some but not all of these constraints. Section 5 concludes. the probability is calculated from the RNN’s softmax activation. The logarithm is taken in base 2, so that surprisal is measured in bits. The degree of surprisal for a word or sentence tells us the extent to which that word or sentence is unexpected under the language model’s probability distribution. It is known to correlate directly with human sentence processing difficulty (Hale, 2001; Levy, 2008; Smith and Levy, 2013). In this paper, we look for cases where the surprisal associated with an an unusual construction—such as a gap—is ameliorated by the presence of a licensor, such as a wh-word. If the models learn that syntactic gaps require licensing, then sentences with licensors should exhibit lower surprisal than minimally different pairs that lack a proper licensor. 2 We test whether the LSTM language models have learned filler–gap dependencies by looking for a 2x2 interaction between the presence of a gap and the presence of a wh-licensor. This interaction indicates the"
W18-5423,P13-2121,0,0.0303394,"al Neural Network (CNN) as input to the LSTM. This model has the best published perplexity for English text. Our second model is the one presented in the supplementary materials of Gulordava et al. (2018), which we call the Gulordava model. Trained on 90 million tokens of English Wikipedia, it has two hidden layers of 650 units each. Our goal in using these models is to provide two samples of the state-of-the-art. As a baseline, we also study an n-gram model trained on the One Billion Word Benchmark (a 5-gram model with modified Kneser-Ney interpolation, fit by KenLM with default parameters) (Heafield et al., 2013). 2.2 Dependent variable: Surprisal We investigate RNN behavior primarily by studying the surprisal values that an RNN assigns to words and sentences. Surprisal is log inverse probability: S(xi ) = − log2 p(xi |hi−1 ), 1 We indicate the gap position with underscores for exposwhere xi is the current word or character, hi−1 is the RNN’s hidden state before consuming xi , and itory purposes, but these underscores were not included in experimental items. 212 d. I know what the lion devoured [wh-licensor, gap] at sunrise. sary because we do not have repeated observations within items and conditions"
W19-0106,N18-1108,0,0.151503,"earn the abstract features of weight, animacy, and definiteness which underlie soft constraints on syntactic alternations. The best-performing models for many natural language processing tasks in recent years have been recurrent neural networks (RNNs) (Elman, 1990; Sutskever et al., 2014; Goldberg, 2017), but the black-box nature of these models makes it hard to know exactly what generalizations they have learned about their linguistic input: Have they learned generalizations stated over hierarchical structures, or only dependencies among relatively local groups of words (Linzen et al., 2016; Gulordava et al., 2018; Futrell et al., 2018)? Do they represent structures analogous to syntactic dependency trees (Williams et al., 2018), and can they represent complex relationships such as filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018)? In order to make progress with RNNs, it is crucial to determine what RNNs actually learn given currently standard practices; then we can design network architectures, objective functions, and training practices to build on strengths and alleviate weaknesses (Linzen, 2018). In this work, we investigate whether RNNs trained on a language modeling ob"
W19-0106,N01-1021,0,0.365064,"vation for xi given its hidden state after consuming xij=11 . Surprisal has a number of interpretations that make it convenient as a dependent variable for examining language model behavior. First, surprisal is equivalent to the contribution of a sentence to a language model’s cross-entropy loss: effectively, our RNN language models are trained with the sole objective of minimizing the average surprisal of training sentences, so surprisal is directly related to the model’s performance. Second, wordby-word surprisal has been found to be an effective predictor of human comprehension difficulty (Hale, 2001; Levy, 2008; Smith and Levy, 2013); interpreting surprisal as metric of “difficulty” allows us to analyze RNN behavior analogously to human processing behavior (van Schijndel and Linzen, 2018; Futrell et al., 2018). Third, surprisal more generally reflects the dispreference or markedness of a sequence according to a language model. High surprisal for a sentence corresponds to a relative dispreference for that sentence. When the logarithm is taken to base 2, surprisal is equivalent to the bits of information required to encode a sentence under a model. In the studies below, we test hypotheses"
W19-0106,P13-2121,0,0.0615885,"Missing"
W19-0106,C18-1012,0,0.0537956,"recurrent neural networks (RNNs) (Elman, 1990; Sutskever et al., 2014; Goldberg, 2017), but the black-box nature of these models makes it hard to know exactly what generalizations they have learned about their linguistic input: Have they learned generalizations stated over hierarchical structures, or only dependencies among relatively local groups of words (Linzen et al., 2016; Gulordava et al., 2018; Futrell et al., 2018)? Do they represent structures analogous to syntactic dependency trees (Williams et al., 2018), and can they represent complex relationships such as filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018)? In order to make progress with RNNs, it is crucial to determine what RNNs actually learn given currently standard practices; then we can design network architectures, objective functions, and training practices to build on strengths and alleviate weaknesses (Linzen, 2018). In this work, we investigate whether RNNs trained on a language modeling objective learn certain syntactic preferences exhibited by humans, especially those involving word order. We draw on a rich literature from quantitative linguistics that has investigated these preferences in corpora and experimen"
W19-0106,E17-1065,1,0.843468,"studied in quantitative syntax, and the abstract features that have been discovered to underly these preferences, finding that RNNs are able to represent many of the required features. The same features underlying these soft preferences in English often play a role in hard constraints in other languages (Bresnan et al., 2001): thus our findings indicate that RNNs can learn crosslinguistically useful abstractions. words and constituents which are easier to produce come earlier (Bock, 1982). It is possible that these same cognitively motivated biases might also be present in RNNs. For example, Futrell and Levy (2017) have argued that there should be a preference for short dependencies in any system that predicts words incrementally given lossy representations of the preceding context: since RNNs represent context using fixedlength vectors, their context representations must be lossy in this way. Furthermore, Chang (2009) has shown that the preference to place animate words earlier can arise in simple recurrent networks without this bias being present in training data, suggesting that RNNs may be subject to similar pressures to produce certain kinds of words earlier. More generally, we have treated RNN lan"
W19-0106,Q16-1037,0,0.336618,"ults show that RNNs learn the abstract features of weight, animacy, and definiteness which underlie soft constraints on syntactic alternations. The best-performing models for many natural language processing tasks in recent years have been recurrent neural networks (RNNs) (Elman, 1990; Sutskever et al., 2014; Goldberg, 2017), but the black-box nature of these models makes it hard to know exactly what generalizations they have learned about their linguistic input: Have they learned generalizations stated over hierarchical structures, or only dependencies among relatively local groups of words (Linzen et al., 2016; Gulordava et al., 2018; Futrell et al., 2018)? Do they represent structures analogous to syntactic dependency trees (Williams et al., 2018), and can they represent complex relationships such as filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018)? In order to make progress with RNNs, it is crucial to determine what RNNs actually learn given currently standard practices; then we can design network architectures, objective functions, and training practices to build on strengths and alleviate weaknesses (Linzen, 2018). In this work, we investigate whether RNNs trained o"
W19-0106,D18-1151,0,0.0180185,"y and definiteness. Much recent work has focused on whether RNNs can learn to represent discrete syntactic 5 For both constructions to be legitimate syntactic options, the possesssum must be definite and unmodified by relative clauses. 6 The preregistration for this experiment can be viewed at https://aspredicted.org/f2sk8.pdf. 56 structures such as long-distance number agreement (Linzen et al., 2016), wh-dependencies (McCoy et al., 2018; Chowdhury and Zamparelli, 2018; Wilcox et al., 2018), anaphora, negative polarity item licensing, and garden path sentences (van Schijndel and Linzen, 2018; Marvin and Linzen, 2018; Futrell et al., 2018). The current work focuses on soft preferences which have been studied in quantitative syntax, and the abstract features that have been discovered to underly these preferences, finding that RNNs are able to represent many of the required features. The same features underlying these soft preferences in English often play a role in hard constraints in other languages (Bresnan et al., 2001): thus our findings indicate that RNNs can learn crosslinguistically useful abstractions. words and constituents which are easier to produce come earlier (Bock, 1982). It is possible that"
W19-0106,D12-1023,0,0.0293143,"ch words linked in syntactic dependencies are close to each other: such sentences are produced more frequently and comprehended more easily (Hawkins, 1994; Futrell et al., 2015; Temperley and Gildea, 2018). We are interested in whether RNNs learn abstract word order preferences as a way of probing their syntactic knowledge. If RNNs exhibit these preferences for appropriately controlled stimuli, then on some level they have learned the abstractions required to state them. Knowing whether RNNs show human-like word order preferences also bears on their suitability as language generation systems. White and Rajkumar (2012) have shown that language gener50 Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 50-59. New York City, New York, January 3-6, 2019 ation systems produce better output when humanlike word order preferecences are built in; it may turn out that RNN language models reproduce such preferences such that they do not need to be built in explicitly. As part of this work, we validate and quantify these word order preferences for humans by collecting acceptability ratings for English sentences with different word orders. To our knowledge, this is the first experimental accep"
W19-0106,W18-5423,1,0.613538,"(Elman, 1990; Sutskever et al., 2014; Goldberg, 2017), but the black-box nature of these models makes it hard to know exactly what generalizations they have learned about their linguistic input: Have they learned generalizations stated over hierarchical structures, or only dependencies among relatively local groups of words (Linzen et al., 2016; Gulordava et al., 2018; Futrell et al., 2018)? Do they represent structures analogous to syntactic dependency trees (Williams et al., 2018), and can they represent complex relationships such as filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018)? In order to make progress with RNNs, it is crucial to determine what RNNs actually learn given currently standard practices; then we can design network architectures, objective functions, and training practices to build on strengths and alleviate weaknesses (Linzen, 2018). In this work, we investigate whether RNNs trained on a language modeling objective learn certain syntactic preferences exhibited by humans, especially those involving word order. We draw on a rich literature from quantitative linguistics that has investigated these preferences in corpora and experiments (e.g., McDonald et"
W19-0106,Q18-1019,0,0.0235115,"ions. The best-performing models for many natural language processing tasks in recent years have been recurrent neural networks (RNNs) (Elman, 1990; Sutskever et al., 2014; Goldberg, 2017), but the black-box nature of these models makes it hard to know exactly what generalizations they have learned about their linguistic input: Have they learned generalizations stated over hierarchical structures, or only dependencies among relatively local groups of words (Linzen et al., 2016; Gulordava et al., 2018; Futrell et al., 2018)? Do they represent structures analogous to syntactic dependency trees (Williams et al., 2018), and can they represent complex relationships such as filler–gap dependencies (Chowdhury and Zamparelli, 2018; Wilcox et al., 2018)? In order to make progress with RNNs, it is crucial to determine what RNNs actually learn given currently standard practices; then we can design network architectures, objective functions, and training practices to build on strengths and alleviate weaknesses (Linzen, 2018). In this work, we investigate whether RNNs trained on a language modeling objective learn certain syntactic preferences exhibited by humans, especially those involving word order. We draw on a"
W19-2111,P14-2134,0,\N,Missing
W19-2111,N18-1202,0,\N,Missing
W19-4819,P82-1020,0,0.707445,"Missing"
W19-4819,C18-1012,0,0.164728,"Missing"
W19-4819,P19-1285,0,0.0502133,"predict words in these structures, a model must be able to temporarily suppress certain expectations and then recover those expectations later, essentially pushing and popping these expectations on a stack. Our results provide evidence that models can successfully suppress and recover expectations in many cases, but do not fully recover their previous grammatical state. 1 Introduction Deep learning sequence models such as RNNs (Elman, 1990; Hochreiter and Schmidhuber, 1997) have led to a marked increase in performance for a range of Natural Language Processing tasks (Jozefowicz et al., 2016; Dai et al., 2019), but it remains an open question whether they are able to induce hierarchical generalizations from linear input alone. Answering this question is important both for technical outcomes—models with explicit hierarchical structure show performance 181 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 181–190 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics S within a syntactic structure. Center embeddings are sentences in which a clause is embedded within the center of another clause, such that the expecta"
W19-4819,J13-2004,0,0.0319227,"ittered CP The diamond S that VP2 NP2 the thief CP stole ... The diamond that thief ↑ PUSH ... ↑ PUSH stole ↑POP glittered. ↑POP Figure 1: Anatomy of a center embedding sentence. At each point marked PUSH, comprehenders need to push the expectations generated by the subject noun onto a stack-like data structure, and suppress those expectations going forward. At the points marked POP, they must recover those expectations. it. The hierarchical structures of natural language are widely believed to be mildly context-sensitive (Shieber, 1985; Weir, 1988; Seki et al., 1991; Joshi and Schabes, 1997; Kuhlmann, 2013), so this result shows that LSTMs are practically capable of inducing the proper data structures to handle the hierarchical structure of natural language. What remains to be seen in a general way is that LSTMs induce and use these structures when trained on natural language input, rather than artificial language input. In this work, we present two suites of experiments that probe for evidence of hierarchical generalizations using two linguistic structures: center embedding sentences and syntactic island constraints on the filler–gap dependency. These structures exemplify context-free hierarchi"
W19-4819,N16-1024,0,0.0933756,"Missing"
W19-4819,Q16-1037,0,0.118228,"wilcoxeg@g.harvard.edu of Brain and Cognitive Sciences, MIT, rplevy@mit.edu 3 Department of Language Science, UC Irvine, rfutrell@uci.edu 2 Department Abstract gains, at least when training on relatively small datasets (Choe and Charniak, 2016; Dyer et al., 2016; Kuncoro et al., 2016)—and for the scientific aim of understanding what biases, learning objectives and training regimes led to humanlike linguistic knowledge. Previous work has approached this question by either examining models’ internal state (Weiss et al., 2018; Mareˇcek and Rosa, 2018) or by studying model behavior (Elman, 1991; Linzen et al., 2016; Futrell et al., 2019; McCoy et al., 2018). For this latter approach, much work has assessed sensitivity to hierarchy by examining whether the expectations associated with longdistance dependencies can be maintained even in the presence of intervening distractor words (Gulordava et al., 2018; Marvin and Linzen, 2018). For example, Linzen et al. (2016) fed RNNs with the prefix The keys to the cabinet. . . . If models assigned higher probability to the grammatical continuation are over the ungrammatical continuation is, they can be said to have learned the correct structural relationship betwee"
W19-4819,N19-1004,1,0.884357,"Missing"
W19-4819,W18-5444,0,0.0443262,"Missing"
W19-4819,W18-5426,0,0.104329,"Missing"
W19-4819,D18-1151,0,0.0769184,"of understanding what biases, learning objectives and training regimes led to humanlike linguistic knowledge. Previous work has approached this question by either examining models’ internal state (Weiss et al., 2018; Mareˇcek and Rosa, 2018) or by studying model behavior (Elman, 1991; Linzen et al., 2016; Futrell et al., 2019; McCoy et al., 2018). For this latter approach, much work has assessed sensitivity to hierarchy by examining whether the expectations associated with longdistance dependencies can be maintained even in the presence of intervening distractor words (Gulordava et al., 2018; Marvin and Linzen, 2018). For example, Linzen et al. (2016) fed RNNs with the prefix The keys to the cabinet. . . . If models assigned higher probability to the grammatical continuation are over the ungrammatical continuation is, they can be said to have learned the correct structural relationship between the subject and the verb, ignoring the syntactically-irrelevant singular distractor, the cabinet. Work in this paradigm has uncovered a complex pattern in terms of what specific hierarchical structures are and are not represented by neural language models. At the same time, work using artificial languages as input h"
W19-4819,W18-0102,0,0.0894,"Missing"
W19-4819,N18-1108,0,0.0438861,"for the scientific aim of understanding what biases, learning objectives and training regimes led to humanlike linguistic knowledge. Previous work has approached this question by either examining models’ internal state (Weiss et al., 2018; Mareˇcek and Rosa, 2018) or by studying model behavior (Elman, 1991; Linzen et al., 2016; Futrell et al., 2019; McCoy et al., 2018). For this latter approach, much work has assessed sensitivity to hierarchy by examining whether the expectations associated with longdistance dependencies can be maintained even in the presence of intervening distractor words (Gulordava et al., 2018; Marvin and Linzen, 2018). For example, Linzen et al. (2016) fed RNNs with the prefix The keys to the cabinet. . . . If models assigned higher probability to the grammatical continuation are over the ungrammatical continuation is, they can be said to have learned the correct structural relationship between the subject and the verb, ignoring the syntactically-irrelevant singular distractor, the cabinet. Work in this paradigm has uncovered a complex pattern in terms of what specific hierarchical structures are and are not represented by neural language models. At the same time, work using artif"
W19-4819,N01-1021,0,0.786288,"Missing"
W19-4819,P18-2117,0,0.066808,"ox1 , Roger Levy2 , and Richard Futrell3 1 Department of Linguistics, Harvard University, wilcoxeg@g.harvard.edu of Brain and Cognitive Sciences, MIT, rplevy@mit.edu 3 Department of Language Science, UC Irvine, rfutrell@uci.edu 2 Department Abstract gains, at least when training on relatively small datasets (Choe and Charniak, 2016; Dyer et al., 2016; Kuncoro et al., 2016)—and for the scientific aim of understanding what biases, learning objectives and training regimes led to humanlike linguistic knowledge. Previous work has approached this question by either examining models’ internal state (Weiss et al., 2018; Mareˇcek and Rosa, 2018) or by studying model behavior (Elman, 1991; Linzen et al., 2016; Futrell et al., 2019; McCoy et al., 2018). For this latter approach, much work has assessed sensitivity to hierarchy by examining whether the expectations associated with longdistance dependencies can be maintained even in the presence of intervening distractor words (Gulordava et al., 2018; Marvin and Linzen, 2018). For example, Linzen et al. (2016) fed RNNs with the prefix The keys to the cabinet. . . . If models assigned higher probability to the grammatical continuation are over the ungrammatical co"
W19-4819,W18-5423,1,0.902574,"Missing"
W19-4819,N19-1334,1,0.837376,"Missing"
W19-4819,D16-1257,0,\N,Missing
